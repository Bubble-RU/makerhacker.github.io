<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2008-Domain Adaptation with Multiple Sources</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-65" href="#">nips2008-65</a> knowledge-graph by maker-knowledge-mining</p><h1>65 nips-2008-Domain Adaptation with Multiple Sources</h1>
<br/><p>Source: <a title="nips-2008-65-pdf" href="http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf">pdf</a></p><p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>Reference: <a title="nips-2008-65-reference" href="../nips2008_reference/nips-2008-Domain_Adaptation_with_Multiple_Sources_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('di', 0.331), ('mixt', 0.313), ('hi', 0.294), ('hypothes', 0.269), ('target', 0.246), ('dt', 0.229), ('hz', 0.207), ('sourc', 0.197), ('loss', 0.187), ('rul', 0.182), ('lz', 0.153), ('domain', 0.149), ('zi', 0.138), ('hu', 0.135), ('brouw', 0.125), ('dvd', 0.125), ('dz', 0.125), ('adapt', 0.11), ('weight', 0.107), ('dj', 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="65-tfidf-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>2 0.21754193 <a title="65-tfidf-2" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>3 0.19130452 <a title="65-tfidf-3" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>4 0.16053353 <a title="65-tfidf-4" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>5 0.15618177 <a title="65-tfidf-5" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>6 0.1394269 <a title="65-tfidf-6" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>7 0.13769361 <a title="65-tfidf-7" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>8 0.12272394 <a title="65-tfidf-8" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>9 0.11629729 <a title="65-tfidf-9" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>10 0.1028482 <a title="65-tfidf-10" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>11 0.09730231 <a title="65-tfidf-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.093058638 <a title="65-tfidf-12" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>13 0.088626966 <a title="65-tfidf-13" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>14 0.087715894 <a title="65-tfidf-14" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>15 0.087548316 <a title="65-tfidf-15" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>16 0.086931631 <a title="65-tfidf-16" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>17 0.085227571 <a title="65-tfidf-17" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>18 0.082475677 <a title="65-tfidf-18" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>19 0.081623286 <a title="65-tfidf-19" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>20 0.077798679 <a title="65-tfidf-20" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.234), (1, 0.043), (2, 0.052), (3, -0.058), (4, -0.009), (5, -0.083), (6, 0.005), (7, 0.007), (8, -0.048), (9, -0.022), (10, -0.06), (11, 0.272), (12, 0.033), (13, 0.085), (14, -0.023), (15, 0.059), (16, 0.023), (17, -0.146), (18, -0.063), (19, -0.003), (20, -0.26), (21, 0.051), (22, 0.044), (23, -0.015), (24, 0.038), (25, 0.026), (26, -0.124), (27, -0.04), (28, 0.031), (29, 0.137), (30, 0.018), (31, 0.014), (32, 0.022), (33, -0.141), (34, -0.156), (35, -0.104), (36, -0.001), (37, 0.081), (38, 0.099), (39, 0.087), (40, 0.069), (41, -0.089), (42, 0.115), (43, 0.103), (44, -0.079), (45, -0.08), (46, 0.044), (47, 0.039), (48, -0.045), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96759444 <a title="65-lsi-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>2 0.73055983 <a title="65-lsi-2" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>3 0.60720426 <a title="65-lsi-3" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>4 0.58711654 <a title="65-lsi-4" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>5 0.5509367 <a title="65-lsi-5" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>6 0.48349604 <a title="65-lsi-6" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>7 0.48296893 <a title="65-lsi-7" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>8 0.45918807 <a title="65-lsi-8" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>9 0.45662591 <a title="65-lsi-9" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>10 0.43032113 <a title="65-lsi-10" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>11 0.42683423 <a title="65-lsi-11" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>12 0.39347246 <a title="65-lsi-12" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>13 0.38800758 <a title="65-lsi-13" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>14 0.38521212 <a title="65-lsi-14" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>15 0.37812725 <a title="65-lsi-15" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>16 0.36709422 <a title="65-lsi-16" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>17 0.36426079 <a title="65-lsi-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.35823756 <a title="65-lsi-18" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<p>19 0.35697809 <a title="65-lsi-19" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>20 0.3529914 <a title="65-lsi-20" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(30, 0.635), (38, 0.012), (40, 0.028), (63, 0.055), (64, 0.05), (71, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97668886 <a title="65-lda-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>2 0.9585892 <a title="65-lda-2" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>3 0.91735131 <a title="65-lda-3" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>4 0.90744531 <a title="65-lda-4" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>5 0.84215426 <a title="65-lda-5" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>6 0.83721042 <a title="65-lda-6" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>7 0.81833452 <a title="65-lda-7" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>8 0.77837336 <a title="65-lda-8" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>9 0.7700994 <a title="65-lda-9" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>10 0.75463694 <a title="65-lda-10" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>11 0.7527262 <a title="65-lda-11" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>12 0.73940098 <a title="65-lda-12" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>13 0.73663926 <a title="65-lda-13" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>14 0.73218656 <a title="65-lda-14" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>15 0.72066224 <a title="65-lda-15" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>16 0.70535636 <a title="65-lda-16" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>17 0.70194995 <a title="65-lda-17" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>18 0.70150554 <a title="65-lda-18" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>19 0.7005918 <a title="65-lda-19" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>20 0.69094813 <a title="65-lda-20" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
