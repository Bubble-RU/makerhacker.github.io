<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-74" href="#">nips2008-74</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</h1>
<br/><p>Source: <a title="nips-2008-74-pdf" href="http://papers.nips.cc/paper/3609-estimating-the-location-and-orientation-of-complex-correlated-neural-activity-using-meg.pdf">pdf</a></p><p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>Reference: <a title="nips-2008-74-reference" href="../nips2008_reference/nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nagarajan Biomagnetic Imaging Laboratory University of California, San Francisco  Abstract The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. [sent-10, score-0.663]
</p><p>2 Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. [sent-11, score-1.144]
</p><p>3 The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. [sent-13, score-0.308]
</p><p>4 1  Introduction  Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sensors to take electromagnetic ﬁeld (or voltage potential) measurements from on or near the scalp surface with excellent temporal resolution. [sent-17, score-0.176]
</p><p>5 In both cases, the observed ﬁeld is generated by the same synchronous, compact current sources located within the brain. [sent-18, score-0.174]
</p><p>6 Although useful for research and clinical purposes, accurately determining the spatial distribution of these unknown sources is an open problem. [sent-19, score-0.289]
</p><p>7 The relevant estimation problem can be posed as follows: The measured electromagnetic signal is B ∈ Rdb ×dt , where db equals the number of sensors and dt is the number of time points at which measurements are made. [sent-20, score-0.295]
</p><p>8 Each unknown source Si ∈ Rdc ×dt is a dc -dimensional neural current dipole , at dt timepoints, projecting from the i-th (discretized) voxel or candidate location distributed throughout the cortex. [sent-21, score-0.821]
</p><p>9 B and each Si are related by the likelihood model ds  Li Si + E,  B=  (1)  i=1  where ds is the number of voxels under consideration, Li ∈ Rdb ×dc is the so-called lead-ﬁeld matrix for the i-th voxel. [sent-23, score-0.544]
</p><p>10 The k-th column of Li represents the signal vector that would be observed at the scalp given a unit current source/dipole at the i-th vertex with a ﬁxed orientation in the k-th direction. [sent-24, score-0.193]
</p><p>11 It is common to assume dc = 2 (for MEG) or dc = 3 (for EEG), which allows ﬂexible source orientations to be estimated in 2D or 3D space. [sent-25, score-0.654]
</p><p>12 However, temporal correlations can easily be incorporated if desired using a simple transformation outlined in [3]. [sent-28, score-0.175]
</p><p>13 To obtain reasonable spatial resolution, the number of candidate source locations will necessarily be much larger than the number of sensors (ds db ). [sent-29, score-0.38]
</p><p>14 The salient inverse problem then becomes the ill-posed estimation of regions with signiﬁcant brain activity, which are reﬂected by voxels i such that Si > 0; we refer to these as active dipoles or sources. [sent-30, score-0.575]
</p><p>15 Because the inverse model is severely underdetermined (the mapping from source activity conﬁguration S [S 1 , . [sent-31, score-0.405]
</p><p>16 , Sds ]T to sensor measurement B is many to one), all efforts at source reconstruction are heavily dependent on prior assumptions, which in a Bayesian framework are embedded in the distribution p(S). [sent-34, score-0.362]
</p><p>17 While advantageous in many respects, all of these methods retain substantial weaknesses estimating complex, correlated source conﬁgurations with unknown orientation in the presence of background interference (e. [sent-38, score-0.746]
</p><p>18 There are two types of correlations that can potentially disrupt the source localization process. [sent-42, score-0.632]
</p><p>19 First, there are correlations within dipole components (meaning the individual rows of S i are correlated), which always exists to a high degree in real data with unknown orientation (i. [sent-43, score-0.526]
</p><p>20 Secondly, there are correlations between different dipoles that are simultaneously active (meaning rows of S i are correlated with rows of Sj for some voxels i = j). [sent-46, score-0.702]
</p><p>21 These correlations are more application speciﬁc and may or may not exist. [sent-47, score-0.175]
</p><p>22 The larger the number of active sources, the greater the chance that both types or correlation can disrupt the estimation process. [sent-48, score-0.199]
</p><p>23 First, failure to accurately account for unknown orientations or correlations can severely disrupt the localization process, leading to a very misleading impression of which brain areas are active. [sent-50, score-0.738]
</p><p>24 Secondly, the orientations and correlations themselves may have clinical signiﬁcance. [sent-51, score-0.334]
</p><p>25 In this paper, we present an alternative empirical Bayesian scheme that attempts to improve upon existing methods in terms of source reconstruction accuracy and/or computational robustness and efﬁciency. [sent-52, score-0.321]
</p><p>26 Section 3 derives a robust algorithm for estimating the sources using this model and proves that each iteration is guaranteed to reduce the associated cost function. [sent-54, score-0.314]
</p><p>27 It also describes how interference suppression can be naturally incorporated. [sent-55, score-0.244]
</p><p>28 Section 4 then provides a theoretical analysis of the bias involved in estimating both the location and orientation of active sources, demonstrating that the proposed method has substantial advantages over existing approaches. [sent-56, score-0.377]
</p><p>29 2  Modeling Assumptions  To begin we invoke the noise model from (1), which fully deﬁnes the assumed likelihood   2 ds 1 , p(B|S) ∝ exp − B − Li S i 2 −1 i=1  (2)  Σ  where X W denotes the weighted matrix norm trace[X T W X]. [sent-58, score-0.282]
</p><p>30 The unknown noise covariance Σ will be estimated from the data using a variational Bayesian factor analysis (VBFA) model as discussed in Section 3. [sent-59, score-0.214]
</p><p>31 Next we adopt the following source prior for S: 1 p (S|Γ) ∝ exp − trace 2  ds T Si Γ−1 Si i  . [sent-61, score-0.485]
</p><p>32 (3)  i=1  This is equivalent to applying independently, at each time point, a zero-mean Gaussian distribution with covariance Γi to each source Si . [sent-62, score-0.298]
</p><p>33 We deﬁne Γ to be the ds dc × ds dc block-diagonal matrix formed by ordering each Γi along the diagonal of an otherwise zero-valued matrix. [sent-63, score-0.713]
</p><p>34 One principled way to accomplish this is to integrate out the sources S and then maximize p(B|Γ) =  1 p(B|S)p(S|Γ)dS ∝ exp − B T Σ−1 B , b 2  Σb  Σ + LΓLT . [sent-67, score-0.174]
</p><p>35 (6)  This is equivalent to minimizing the cost function L(Γ)  −2 log p(B|Γ)p(Γ) ≡ trace Cb Σ−1 + log |Σb , | , b  (7)  where Cb n−1 BB T is the empirical covariance, and is sometimes referred to as type-II maximum likelihood, evidence maximization, or empirical Bayes [1]. [sent-68, score-0.213]
</p><p>36 The ﬁrst term of (7) is a measure of the dissimilarity between the empirical data covariance C b and the model data covariance Σb ; in general, this factor encourages Γ to be large. [sent-69, score-0.186]
</p><p>37 To the extent ˆ quantiﬁes regions of signiﬁcant that this ‘learned’ prior is realistic, the resulting posterior p(S|B, Γ) current density and point estimates for the unknown source dipoles Si can be obtained by evaluating ˆ the posterior mean computed using (4). [sent-74, score-0.535]
</p><p>38 To begin, we note that L(Γ) only depends on the data B through the db ×db sample correlation matrix Cb . [sent-88, score-0.162]
</p><p>39 First, the data ﬁt term can be expressed as   2  ds  trace Cb Σ−1 = min  B − b  ds  +  Li X i  X  i=1  Σ  Xi i=1  −1  T    (8)  2 , Γ−1 i  T T is a matrix of auxiliary variables. [sent-94, score-0.478]
</p><p>40 , X d s determinant term of L(Γ) is concave in Γ, it can be expressed as a minimum over upper-bounding hyperplanes via ds  (9)  T trace Zi Γi − h∗ (Z) ,  log |Σb | = min Z  i=1  T  T T and h∗ (Z) is the concave conjugate of log |Σb |. [sent-98, score-0.306]
</p><p>41 , Z ds below, we will never actually have to compute h∗ (Z). [sent-102, score-0.187]
</p><p>42 Dropping the minimizations and combining terms from (8) and (9) leads to the modiﬁed cost function 2  ds  L(Γ, X, Z) = B −  ds  +  Li X i i=1  Σ  Xi i=1  −1  2 Γ−1 i  T + trace Zi Γi  (10)  − h∗ (Z),  where by construction L(Γ) = minX minZ L(Γ, X, Z). [sent-103, score-0.507]
</p><p>43 i b  With Z and X ﬁxed, computing the minimizing Γ is a bit more difﬁcult because of the constraint Γi ∈ H + for all i, where H + is the set of positive-semideﬁnite, symmetric dc × dc covariance matrices. [sent-112, score-0.381]
</p><p>44 The per-iteration cost is linear in the number of voxels ds so the computational cost is relatively modest (it is quadratic in db , and cubic in dc , but these quantities are relatively small). [sent-119, score-0.695]
</p><p>45 2  Learning the Interference Σ  The learning procedure described in the previous section boils down to ﬁtting a structured maximum likelihood covariance estimate Σb = Σ + F ΓF T to the data covariance Cb . [sent-122, score-0.146]
</p><p>46 The idea here is that F ΓF T will reﬂect the brain signals of interest while Σ will capture all interfering factors, e. [sent-123, score-0.144]
</p><p>47 , data assumed to have no signal/sources of interest), stimulus evoked factor analysis (SEFA) provides a powerful means of decomposing a data covariance matrix Cb into signal and interference components. [sent-129, score-0.33]
</p><p>48 While details can be found in [4], SEFA computes the approximation Cb ≈ Λ + EE T + AAT ,  (18)  where E represents a matrix of learned interference factors, Λ is a diagonal noise matrix, and A is a matrix of signal factors. [sent-130, score-0.3]
</p><p>49 4  Analysis of Theoretical Localization/Orientation Bias  Theoretical support for the proposed algorithm is possible in the context of estimation bias assuming simpliﬁed source conﬁgurations. [sent-138, score-0.281]
</p><p>50 For example, substantial import has been devoted to quantifying localization bias when estimating a single dipolar source. [sent-139, score-0.275]
</p><p>51 However, these results assume a single dipole with ﬁxed, known orientation (or alternatively, that dc = 1), and therefore do not formally handle source correlations or multi-component dipoles. [sent-142, score-0.878]
</p><p>52 In contrast, despite being a complex, non-convex function, we now demonstrate that L(Γ) has very attractive bias properties regarding both localization and orientation. [sent-144, score-0.195]
</p><p>53 , LTs represents a sufﬁciently high sampling of the source space such that 1 d any active dipole component aligns with some lead-ﬁeld columns. [sent-148, score-0.417]
</p><p>54 We deﬁne the empirical intra-dipole corre1 T lation matrix at the i-th voxel as Cii dt Si Si ; non-zero off-diagonal elements imply that correlations are present. [sent-151, score-0.423]
</p><p>55 1 T The empirical inter-dipole correlation matrix between voxels i and j is Cij dt Si Sj ; any nonzero element implies the existence of a correlation. [sent-153, score-0.311]
</p><p>56 In the limit as Σ → 0 (high SNR) and assuming da dc < spark(L) − 1, the cost function L(Γ) maintains the following two properties: 1. [sent-161, score-0.256]
</p><p>57 For arbitrary Cii and Cij , the unique global minimum Γ∗ produces a source estimate S ∗ = Ep(S|B,Γ∗ ) [S] computed using (4) that equals the generating source matrix S, i. [sent-162, score-0.563]
</p><p>58 , it is  unbiased in both location and orientation for all active dipoles and correctly zeros out the inactive ones. [sent-164, score-0.521]
</p><p>59 If Cij = 0 for all active dipoles (although Cii is still arbitrary), then there are no local minima, i. [sent-166, score-0.303]
</p><p>60 In words, this theorem says that intra-dipole correlations do not disrupt the estimation process by creating local minima, and that the global minimum is always unbiased. [sent-170, score-0.35]
</p><p>61 In contrast, inter-dipole correlations can potentially create local minima, but they do not affect the global minimum. [sent-171, score-0.211]
</p><p>62 This is a very signiﬁcant failing because, as mentioned previously, intra-dipole correlations are always present in each active dipole. [sent-175, score-0.245]
</p><p>63 Consequently, localization and orientation bias can occur because of convergence to a local minimum. [sent-176, score-0.347]
</p><p>64 The iterative Bayesian scheme from [13], while very different in structure, also directly attempts to estimate ﬂexible orientations and handle, to some extent, source correlations. [sent-177, score-0.346]
</p><p>65 However, the popular sLORETA and MVAB solutions will in general display a bias for multi-component dipoles (dc > 1) or when multiple dipoles (da > 1) are present, regardless of correlations. [sent-180, score-0.522]
</p><p>66 We focus here on localization accuracy assuming strong source correlations and unknown orientations. [sent-182, score-0.616]
</p><p>67 While orientation estimates themselves are not shown for space considerations, accurate localization implicitly indicates that this confound has been adequately handled. [sent-183, score-0.291]
</p><p>68 Simulated Data: We ﬁrst conducted tests using simulated data with realistic source conﬁgurations. [sent-185, score-0.267]
</p><p>69 The brain volume was segmented into 5mm voxels and a two orientation (dc = 2) forward leadﬁeld was calculated using a spherical-shell model [7]. [sent-186, score-0.394]
</p><p>70 In the pre-stimulus period (263 samples) there is only noise and interfering brain activity, while in the post-stimulus period (437 samples) there is the same (statistically) noise and interference factors plus source activity of interest. [sent-188, score-0.791]
</p><p>71 In the former case, we seeded voxels with Gaussian noise in each orientation and then projected the activity to the sensors using the leadﬁeld, producing colored Gaussian noise at the sensors. [sent-190, score-0.692]
</p><p>72 For the real-brain noise case, we used resting-state data collected from a human subject that is presumed to have ongoing and spontaneous activity and sensor noise. [sent-192, score-0.325]
</p><p>73 In both the Gaussian and real-brain noise cases, the pre-stimulus activity was on-going and continued into the post-stimulus period, where the simulated source signals were added. [sent-193, score-0.451]
</p><p>74 Sources were seeded at locations in the brain as damped-sinusoids and this voxel activity was projected to the sensors. [sent-194, score-0.428]
</p><p>75 We could adjust both the signal-to-noise-plusinterefence ratio (SNIR) and the correlations between the different voxel time-courses to examine the algorithm performance on correlated sources and unknown dipole orientations. [sent-195, score-0.745]
</p><p>76 We ran 100 simulations of three randomly seeded sources at different SNIR levels (-5, 0, 5, 10dB). [sent-196, score-0.267]
</p><p>77 The sources in these simulations always had an inter-dipole correlation coefﬁcient of 0. [sent-197, score-0.21]
</p><p>78 We ran the simulation with both Gaussian-noise and real brain noise using a MVAB and our proposed method. [sent-199, score-0.167]
</p><p>79 We drew spheres around each seeded source location and obtained the maximum voxel value in each sphere. [sent-201, score-0.537]
</p><p>80 Our method quite signiﬁcantly outperforms the MVAB, which is designed to handle unknown orientations but has difﬁculty with  source correlations. [sent-206, score-0.473]
</p><p>81 Figure 1 (middle) shows a sample reconstruction on a much more complex source conﬁguration composed of 10 dipolar sources. [sent-207, score-0.328]
</p><p>82 We also wanted to test the performance on perfectly correlated sources with unknown orientations and compare it to other state-of-the-art Bayesian methods. [sent-209, score-0.457]
</p><p>83 An example using three such sources and 5 dB SNIR is given in Figure 2. [sent-210, score-0.174]
</p><p>84 Middle: Example reconstruction of 10 relatively shallow sources (green circles) using proposed method (MVAB performs poorly on this task). [sent-215, score-0.23]
</p><p>85 The second data set analyzed was an auditory evoked ﬁeld (AEF) paradigm. [sent-224, score-0.188]
</p><p>86 There are two typical peaks seen after the presentation of an auditory stimulus, one at 50ms and one at 100ms, called the M50 and M100 respectively. [sent-226, score-0.136]
</p><p>87 The auditory processing of tones is bilateral at early auditory areas and the activations are correlated. [sent-227, score-0.414]
</p><p>88 The algorithm was able to localize activity in both primary auditory cortices and the time courses for these two activations reveal the M50 and M100. [sent-228, score-0.322]
</p><p>89 The analysis of simple auditory paradigms is problematic because many source localization algorithms, such as the MVAB, do not handle the bilateral correlated sources well. [sent-230, score-0.879]
</p><p>90 We also ran MVAB on the AEF data and it localized activity to the center of the head between the two auditory cortices (data not shown). [sent-231, score-0.291]
</p><p>91 6  Discussion  This paper derives a novel empirical Bayesian algorithm for MEG source reconstruction that readily handles multiple correlated sources with unknown orientations, a situation that commonly arises even with simple imaging tasks. [sent-232, score-0.704]
</p><p>92 Right: Recovered timecourse from left auditory cortex (right auditory cortex, not shown, is similar). [sent-235, score-0.346]
</p><p>93 For example, on a real-world passive visual task where subjects viewed ﬂashing foreground/background textured images, our method correctly localizes activity to the lateral occipital cortex while two state-of-the-art beamformers fail. [sent-238, score-0.198]
</p><p>94 Sekihara, “A probabilistic algorithm for robust interference suppression in bioelectromagnetic sensor data,” Stat Med. [sent-270, score-0.325]
</p><p>95 Pascual-Marqui, “Standardized low resolution brain electromagnetic tomography (sloreta): Technical details,” Methods and Findings in Experimental and Clinical Pharmacology, vol. [sent-278, score-0.178]
</p><p>96 Nagarajan, “Reconstructing MEG sources with unknown correlations,” Advances in Neural Information Processing Systems 16, 2004. [sent-285, score-0.251]
</p><p>97 Sarvas, “Basic methematical and electromagnetic concepts of the biomagnetic inverse problem,” Phys. [sent-287, score-0.146]
</p><p>98 Nagarajan, “Localization bias and spatial resolution of adaptive and non-adaptive spatial ﬁlters for MEG source reconstruction,” NeuroImage, vol. [sent-307, score-0.281]
</p><p>99 Rao, “Analysis of empirical ı Bayesian methods for neuroelectromagnetic source localization,” Advances in Neural Information Processing Systems 19, 2007. [sent-335, score-0.265]
</p><p>100 Nagarajan, “A probabilistic algorithm for interference suppression and source reconstruction from MEG/EEG data,” Advances in Neural Information Processing System 19, 2007. [sent-343, score-0.525]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mvab', 0.279), ('dipoles', 0.233), ('source', 0.225), ('meg', 0.224), ('ds', 0.187), ('correlations', 0.175), ('interference', 0.174), ('sources', 0.174), ('zi', 0.157), ('cb', 0.157), ('dc', 0.154), ('orientation', 0.152), ('nagarajan', 0.14), ('voxels', 0.139), ('localization', 0.139), ('auditory', 0.136), ('dipole', 0.122), ('orientations', 0.121), ('activity', 0.12), ('sekihara', 0.116), ('mm', 0.112), ('voxel', 0.112), ('lt', 0.105), ('brain', 0.103), ('sloreta', 0.102), ('si', 0.101), ('db', 0.095), ('disrupt', 0.093), ('seeded', 0.093), ('snir', 0.093), ('correlated', 0.085), ('attias', 0.081), ('sensor', 0.081), ('unknown', 0.077), ('electromagnetic', 0.075), ('covariance', 0.073), ('trace', 0.073), ('active', 0.07), ('bilateral', 0.07), ('cii', 0.07), ('rdb', 0.07), ('spark', 0.07), ('suppression', 0.07), ('neuroimage', 0.07), ('location', 0.066), ('dt', 0.065), ('noise', 0.064), ('eeg', 0.063), ('spontaneous', 0.06), ('sensors', 0.06), ('eld', 0.06), ('cost', 0.06), ('reconstruction', 0.056), ('bias', 0.056), ('somatosensory', 0.056), ('sj', 0.055), ('bayesian', 0.055), ('li', 0.053), ('xi', 0.053), ('minima', 0.053), ('evoked', 0.052), ('handle', 0.05), ('wipf', 0.049), ('cij', 0.047), ('derives', 0.047), ('aef', 0.047), ('dipolar', 0.047), ('owen', 0.047), ('sefa', 0.047), ('minimum', 0.046), ('simulated', 0.042), ('da', 0.042), ('tones', 0.041), ('aat', 0.041), ('beamformers', 0.041), ('beamforming', 0.041), ('biomagnetic', 0.041), ('interfering', 0.041), ('scalp', 0.041), ('spheres', 0.041), ('empirical', 0.04), ('clinical', 0.038), ('timecourse', 0.037), ('cortex', 0.037), ('correlation', 0.036), ('global', 0.036), ('displays', 0.036), ('sahani', 0.035), ('cortices', 0.035), ('middle', 0.033), ('estimating', 0.033), ('synchronous', 0.033), ('hit', 0.033), ('ee', 0.031), ('activations', 0.031), ('matrix', 0.031), ('em', 0.03), ('inverse', 0.03), ('severely', 0.03), ('purposes', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="74-tfidf-1" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>2 0.16124064 <a title="74-tfidf-2" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>3 0.11653211 <a title="74-tfidf-3" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>4 0.10431311 <a title="74-tfidf-4" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><p>5 0.10202099 <a title="74-tfidf-5" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>6 0.091928326 <a title="74-tfidf-6" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>7 0.090084173 <a title="74-tfidf-7" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>8 0.084662594 <a title="74-tfidf-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.079796746 <a title="74-tfidf-9" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>10 0.075895146 <a title="74-tfidf-10" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>11 0.074581318 <a title="74-tfidf-11" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>12 0.074326403 <a title="74-tfidf-12" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>13 0.068659708 <a title="74-tfidf-13" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>14 0.067964971 <a title="74-tfidf-14" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>15 0.06387271 <a title="74-tfidf-15" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>16 0.062086672 <a title="74-tfidf-16" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>17 0.058893252 <a title="74-tfidf-17" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>18 0.058658641 <a title="74-tfidf-18" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>19 0.057596207 <a title="74-tfidf-19" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>20 0.056816239 <a title="74-tfidf-20" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.212), (1, -0.002), (2, 0.081), (3, 0.063), (4, 0.002), (5, 0.036), (6, -0.052), (7, 0.024), (8, 0.035), (9, 0.098), (10, -0.006), (11, 0.076), (12, -0.008), (13, 0.04), (14, -0.007), (15, 0.016), (16, 0.081), (17, 0.129), (18, -0.074), (19, -0.079), (20, 0.223), (21, 0.053), (22, -0.04), (23, 0.063), (24, -0.039), (25, -0.07), (26, -0.011), (27, 0.031), (28, 0.002), (29, 0.021), (30, -0.011), (31, -0.028), (32, -0.049), (33, 0.002), (34, 0.06), (35, 0.075), (36, 0.013), (37, -0.047), (38, -0.13), (39, -0.047), (40, -0.129), (41, -0.121), (42, -0.068), (43, -0.014), (44, -0.048), (45, 0.105), (46, -0.014), (47, -0.008), (48, 0.033), (49, -0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93486422 <a title="74-lsi-1" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>2 0.65326071 <a title="74-lsi-2" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>3 0.62601703 <a title="74-lsi-3" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>4 0.48446739 <a title="74-lsi-4" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>Author: Matthias Krauledat, Konrad Grzeska, Max Sagebaum, Benjamin Blankertz, Carmen Vidaurre, Klaus-Robert Müller, Michael Schröder</p><p>Abstract: Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for precisely timed control tasks. In the present study, however, we demonstrate and report on the interaction of subjects with a real device: a pinball machine. Results of this study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires precisely timed and complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the ﬁrst session without the necessity to employ lengthy subject training. The current study shows clearly that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI. 1</p><p>5 0.45384812 <a title="74-lsi-5" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>6 0.43742761 <a title="74-lsi-6" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>7 0.43612695 <a title="74-lsi-7" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>8 0.43574524 <a title="74-lsi-8" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>9 0.43484128 <a title="74-lsi-9" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>10 0.42778385 <a title="74-lsi-10" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>11 0.41342467 <a title="74-lsi-11" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>12 0.39593393 <a title="74-lsi-12" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>13 0.3938899 <a title="74-lsi-13" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>14 0.39310798 <a title="74-lsi-14" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>15 0.38954276 <a title="74-lsi-15" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>16 0.38113564 <a title="74-lsi-16" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>17 0.36716342 <a title="74-lsi-17" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>18 0.35692614 <a title="74-lsi-18" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>19 0.35681471 <a title="74-lsi-19" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>20 0.34025627 <a title="74-lsi-20" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.573), (7, 0.057), (12, 0.027), (28, 0.1), (57, 0.045), (63, 0.017), (77, 0.038), (83, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96568078 <a title="74-lda-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>2 0.93663585 <a title="74-lda-2" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>same-paper 3 0.91828561 <a title="74-lda-3" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>4 0.89909703 <a title="74-lda-4" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>5 0.8345381 <a title="74-lda-5" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>Author: Jooseuk Kim, Clayton Scott</p><p>Abstract: We provide statistical performance guarantees for a recently introduced kernel classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference of densities. The classiﬁer is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike SVMs, however, the L2 kernel classiﬁer does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE, and apply this result to deduce an oracle inequality and consistency of the classiﬁer on the sense of both ISE and probability of error. Our results also specialize to give performance guarantees for an existing method of L2 kernel density estimation. 1</p><p>6 0.73728335 <a title="74-lda-6" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>7 0.72981447 <a title="74-lda-7" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>8 0.67206329 <a title="74-lda-8" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>9 0.61938143 <a title="74-lda-9" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>10 0.61085892 <a title="74-lda-10" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>11 0.59451526 <a title="74-lda-11" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>12 0.59211862 <a title="74-lda-12" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>13 0.58507127 <a title="74-lda-13" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>14 0.58191341 <a title="74-lda-14" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>15 0.58185512 <a title="74-lda-15" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>16 0.58050609 <a title="74-lda-16" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>17 0.57634729 <a title="74-lda-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.57575089 <a title="74-lda-18" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>19 0.57341969 <a title="74-lda-19" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>20 0.56123275 <a title="74-lda-20" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
