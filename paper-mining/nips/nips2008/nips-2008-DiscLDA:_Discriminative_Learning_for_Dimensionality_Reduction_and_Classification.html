<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-64" href="#">nips2008-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</h1>
<br/><p>Source: <a title="nips-2008-64-pdf" href="http://papers.nips.cc/paper/3599-disclda-discriminative-learning-for-dimensionality-reduction-and-classification.pdf">pdf</a></p><p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>Reference: <a title="nips-2008-64-reference" href="../nips2008_reference/nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 of EECS and Statistics UC Berkeley Berkeley, CA 94720  Abstract Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. [sent-4, score-0.569]
</p><p>2 These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. [sent-5, score-0.081]
</p><p>3 In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. [sent-6, score-0.323]
</p><p>4 Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. [sent-7, score-0.467]
</p><p>5 By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. [sent-9, score-0.787]
</p><p>6 We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics. [sent-10, score-0.599]
</p><p>7 1 Introduction Dimensionality reduction is a common and often necessary step in most machine learning applications and high-dimensional data analyses. [sent-11, score-0.071]
</p><p>8 A recent trend in dimensionality reduction is to focus on probabilistic models. [sent-13, score-0.151]
</p><p>9 These models, which include generative topological mapping, factor analysis, independent component analysis and probabilistic latent semantic analysis (pLSA), are generally speciﬁed in terms of an underlying independence assumption or low-rank assumption. [sent-14, score-0.096]
</p><p>10 , a document) as a collection of draws from a mixture model in which each mixture component is known as a topic [3]. [sent-18, score-0.333]
</p><p>11 The mixing proportions across topics are document-speciﬁc, and the posterior distribution across these mixing proportions provides a reduced representation of the document. [sent-19, score-0.38]
</p><p>12 The dimensionality reduction methods that we have discussed thus far are entirely unsupervised. [sent-21, score-0.151]
</p><p>13 Another branch of research, known as sufﬁcient dimension reduction (SDR), aims at making use of  supervisory data in dimension reduction [4, 7]. [sent-22, score-0.173]
</p><p>14 For example, we may have class labels or regression responses at our disposal. [sent-23, score-0.085]
</p><p>15 Having reduced dimensionality in this way, one may wish to subsequently build a classiﬁer or regressor in the reduced representation. [sent-25, score-0.128]
</p><p>16 But there are other goals for the dimension reduction as well, including visualization, domain understanding, and domain transfer (i. [sent-26, score-0.071]
</p><p>17 In particular, we wish to incorporate side information such as class labels into LDA, while retaining its favorable unsupervised dimensionality reduction abilities. [sent-30, score-0.335]
</p><p>18 The goal is to develop parameter estimation procedures that yield LDA topics that characterize the corpus and maximally exploit the predictive power of the side information. [sent-31, score-0.325]
</p><p>19 As a parametric generative model, parameters in LDA are typically estimated with maximum likelihood estimation or Bayesian posterior inference. [sent-32, score-0.081]
</p><p>20 In this paper, we use a discriminative learning criterion—conditional likelihood—to train a variant of the LDA model. [sent-34, score-0.088]
</p><p>21 Moreover, we augment the LDA parameterization by introducing class-label-dependent auxiliary parameters that can be tuned by the discriminative criterion. [sent-35, score-0.127]
</p><p>22 By retaining the original LDA parameters and introducing these auxiliary parameters, we are able to retain the advantages of the likelihood-based training procedure and provide additional freedom for tracking the side information. [sent-36, score-0.115]
</p><p>23 In Section 4, we report empirical results on applying DiscLDA to model text documents. [sent-40, score-0.062]
</p><p>24 2 Model We start by reviewing the LDA model [3] for topic modeling. [sent-42, score-0.267]
</p><p>25 1 LDA The LDA model is a generative process where each document in the text corpus is modeled as a set of draws from a mixture distribution over a set of hidden topics. [sent-46, score-0.288]
</p><p>26 A topic is modeled as a probability distribution over words. [sent-47, score-0.24]
</p><p>27 Let the vector wd be the bag-of-words representation of document d. [sent-48, score-0.193]
</p><p>28 In both the maximum likelihood and Bayesian framework it is necessary to integrate over θ d to obtain the marginal likelihood, and this is accomplished either using variational inference or Gibbs sampling [3, 8]. [sent-56, score-0.067]
</p><p>29 2 DiscLDA In our setting, each document is additionally associated with a categorical variable or class label yd ∈ {1, 2, . [sent-58, score-0.305]
</p><p>30 Speciﬁcally, for each class label y, we introduce a linear transformation T y : ℜK → ℜL , which transforms a K-dimensional Dirichlet variable θd to +  α α  α  π  π yd  θd  θd  T  zdn β  β  β  wdn  wdn  N  Φ  θd  zdn  T  zdn  yd  D  Figure 1: LDA model. [sent-67, score-1.273]
</p><p>31 To generate a word wdn , we draw its topic zdn from T yd θ d . [sent-71, score-0.755]
</p><p>32 Note that these points can not be placed arbitrarily, as all documents—whether they have the same class labels or they do not— share the parameter Φ ∈ ℜV ×L . [sent-74, score-0.085]
</p><p>33 The graphical model in Figure 2 shows the new generative process. [sent-75, score-0.065]
</p><p>34 Compared to standard LDA, we have added the nodes for the variable yd (and its prior distribution π), the transformation matrices T y and the corresponding edges. [sent-76, score-0.223]
</p><p>35 An alternative to DiscLDA would be a model in which there are class-dependent topic parameters φy which determine the conditional distribution of the words: k d wdn | zdn , yd , Φ ∼ Multi(φydn ). [sent-77, score-0.768]
</p><p>36 z The problem with this approach is that the posterior p(y|w, Φ) is a highly non-convex function of Φ which makes its optimization very challenging given the high dimensionality of the parameter space in typical applications. [sent-78, score-0.08]
</p><p>37 Our approach circumvents this difﬁculty by learning a low-dimensional transformation of the φk ’s in a discriminative manner instead. [sent-79, score-0.194]
</p><p>38 Indeed, transforming the topic mixture vector θ is actually equivalent to transforming the Φ matrix. [sent-80, score-0.273]
</p><p>39 To see this, note that by marginalizing out the hidden topic vector z, we get the following distribution for the word wdn given θ: wdn | yd , θd , T ∼ Mult (ΦT y θ d ) . [sent-81, score-0.794]
</p><p>40 By the associativity of the matrix product, we see that we obtain an equivalent probabilistic model by applying the linear transformation to Φ instead, and, in effect, deﬁning the class-dependent topic parameters as follows: y φy = φl Tlk . [sent-82, score-0.405]
</p><p>41 k l  Another motivation for our approach is that it gives the model the ability to distinguish topics which are shared across different classes versus topics which are class-speciﬁc. [sent-83, score-0.516]
</p><p>42 In this case, the last K topics are shared by both classes, whereas the two ﬁrst groups of K topics are exclusive to one class or the other. [sent-85, score-0.531]
</p><p>43 Note that we can give a generative interpretation to the transformation by augmenting the model with a hidden topic vector variable u, as shown in Fig. [sent-87, score-0.411]
</p><p>44 By including a Dirichlet prior on the T parameters, the DiscLDA model can be related to the authortopic model [10], if we restrict to the special case in which there is only one author per document. [sent-90, score-0.087]
</p><p>45 In the author-topic model, the bag-of-words representation of a document is augmented by a list of the authors of the document. [sent-91, score-0.146]
</p><p>46 To generate a word in a document, one ﬁrst picks at random the author associated with this document. [sent-92, score-0.08]
</p><p>47 Given the author (y in our notation), a topic is chosen according to corpus-wide author-speciﬁc topic-mixture proportions (which is a column vector T y in our notation). [sent-93, score-0.315]
</p><p>48 The word is then generated from the corresponding topic distribution as usual. [sent-94, score-0.287]
</p><p>49 According to this analogy, we see that our model not only enables us to predict the author of a document (assuming a small set of possible authors), but we also capture the content of documents (using θ) as well as the corpus-wide class properties (using T ). [sent-95, score-0.376]
</p><p>50 The focus of the author-topic model was to model the interests of authors, not the content of documents, explaining why there was no need to add document-speciﬁc topic-mixture proportions. [sent-96, score-0.099]
</p><p>51 Because we want to predict the class for a speciﬁc document, it is crucial that we also model the content of a document. [sent-97, score-0.114]
</p><p>52 Recently, there has been growing interest in topic modeling with supervised information. [sent-98, score-0.279]
</p><p>53 Blei and McAuliffe [2] proposed a supervised LDA model where the empirical topic vector z (sampled from θ) is used as a covariate for a regression on y (see also [6]). [sent-99, score-0.306]
</p><p>54 Mimno and McCallum [9] proposed a Dirichlet-multinomial regression which can handle various types of side information, including the case in which this side information is an indicator variable of the class (y)1 . [sent-100, score-0.134]
</p><p>55 Our work differs from theirs, however, in that we train the transformation parameter by maximum conditional likelihood instead of a generative criterion. [sent-101, score-0.22]
</p><p>56 3 Inference and learning Given a corpus of documents and their labels, we estimate the parameters {T y } by maximizing the conditional likelihood d log p(yd | wd ; {T y }, Φ) while holding Φ ﬁxed. [sent-102, score-0.281]
</p><p>57 To estimate the parameters Φ, we hold the transformation matrices ﬁxed and maximize the posterior of the model, in much the same way as in standard LDA models. [sent-103, score-0.106]
</p><p>58 We maximize the conditional likelihood objective with respect to T by using gradient ascent, for a ﬁxed Φ. [sent-108, score-0.076]
</p><p>59 1 Dimensionality reduction We can obtain a supervised dimensionality reduction method by using the average transformed topic vector as the reduced representation of a test document. [sent-114, score-0.588]
</p><p>60 The ﬁrst term on the right-hand side of this equation can 1 In this case, their model is actually the same as Model 1 in [5] with an additional prior on the classdependent parameters for the Dirichlet distribution on the topics. [sent-116, score-0.073]
</p><p>61 Figure 5: t-SNE 2D embedding of the E [θ|Φ, w, T ] representation of Newsgroups documents, after ﬁtting to the standard unsupervised LDA model. [sent-120, score-0.141]
</p><p>62 Our experiments aimed to demonstrate the beneﬁts of discriminative training of LDA for discovering a compact latent representation that contains both predictive and shared components across different types of data. [sent-124, score-0.315]
</p><p>63 1 Text modeling The 20 Newsgroups dataset contains postings to Usenet newsgroups. [sent-127, score-0.078]
</p><p>64 The postings are organized by content into 20 related categories and are therefore well suited for topic modeling. [sent-128, score-0.363]
</p><p>65 We ﬁt the dataset to both a standard 110-topic LDA model and a DiscLDA model with restricted forms of the transformation matrices {T y }y=20 . [sent-130, score-0.16]
</p><p>66 Speciﬁcally, the transformation matrix T y for class y=1 label c is ﬁxed and given by the following blocked matrix   0 0 . [sent-131, score-0.245]
</p><p>67 At the ﬁrst column and the row y, the block matrix is an identity matrix with dimensionality of K0 × K0 . [sent-146, score-0.168]
</p><p>68 The last element of T y is another identity matrix with dimensionality K1 . [sent-147, score-0.112]
</p><p>69 Intuitively, the shared components should use all class labels to model common latent structures, while nonoverlapping components should model speciﬁc characteristics of data from each class. [sent-149, score-0.308]
</p><p>70 We then estimated a new representation for test documents by taking the conditional expectation of T y θ with y marginalized out as explained in Section 3. [sent-187, score-0.182]
</p><p>71 To obtain an embedding, we ﬁrst tried standard multidimensional scaling (MDS), using the symmetrical KL divergence between pairs of θtr topic vectors as a dissimilarity metric, but the results were hard to visualize. [sent-190, score-0.24]
</p><p>72 A more interpretable embedding was obtained using a modiﬁed version of the t-SNE stochastic neighborhood embedding presented by van der Maaten and Hinton [11]. [sent-191, score-0.11]
</p><p>73 4 shows a scatter plot of the 2D–embedding of the topic representation of the 20 Newsgroups test documents, where the colors of the dots, each corresponding to a document, encode class labels. [sent-193, score-0.315]
</p><p>74 Clearly, the documents are well separated in this space. [sent-194, score-0.116]
</p><p>75 It is also instructive to examine in detail the topic structures of the ﬁtted DiscLDA model. [sent-198, score-0.24]
</p><p>76 Given the speciﬁc setup of our transformation matrix T , each component of the topic vector u is either associated with a class label or shared across all class labels. [sent-199, score-0.606]
</p><p>77 For each component, we can compute the most popular words associated from the word-topic distribution Φ. [sent-200, score-0.068]
</p><p>78 In Table 1, we list these words and group them under each class labels and a special bucket “shared. [sent-201, score-0.157]
</p><p>79 ” We see that the words are highly indicative of their associated class labels. [sent-202, score-0.083]
</p><p>80 Additionally, the words in the “shared” category are “neutral,” neither positively nor negatively suggesting proper class labels where they are likely  LDA+SVM 20%  DiscLDA+SVM 17%  discLDA alone 17%  Table 2: Binary classiﬁcation error rates for two newsgroups to appear. [sent-203, score-0.237]
</p><p>81 2 Document classiﬁcation It is also of interest to consider the classiﬁcation problem more directly and ask whether the features delivered by DiscLDA are more useful for classiﬁcation than those delivered by LDA. [sent-207, score-0.068]
</p><p>82 Of course, we can also use DiscLDA as a classiﬁcation method per se, by marginalizing over the latent variables and computing the probability of the label y given the words in a test document. [sent-208, score-0.132]
</p><p>83 Speciﬁcally, we constructed multiclass linear SVM classiﬁers using the expected topic proportion vectors from unsupervised LDA and DiscLDA models as features as described in Section 3. [sent-213, score-0.293]
</p><p>84 Using the topic vectors from standard LDA the error rate of classiﬁcation was 25%. [sent-216, score-0.24]
</p><p>85 When the topic vectors from the DiscLDA model were used we obtained an error rate of 20%. [sent-217, score-0.267]
</p><p>86 We also computed the MAP estimate of the class label y ∗ = arg max p(y|w) from DiscLDA and used this estimate directly as a classiﬁer. [sent-219, score-0.075]
</p><p>87 In a second experiment, we considered the fully adaptive setting in which the transformation matrix T y is learned in a discriminative fashion as described in Section 3. [sent-221, score-0.252]
</p><p>88 We initialized the matrix T to a smoothed block diagonal matrix having a pattern similar to (1), with 20 shared topics and 20 class-dependent topics per class. [sent-222, score-0.577]
</p><p>89 This was followed by the discriminative learning process in which we iteratively ran batch gradient (in the log domain, so that T remained normalized) using Monte Carlo EM with a constant step size for 10 epochs. [sent-224, score-0.088]
</p><p>90 This discriminative learning process was repeated until there was no improvement on a validation data set. [sent-226, score-0.088]
</p><p>91 In this experiment, we considered the binary classiﬁcation problem of distinguishing postings of the newsgroup alt. [sent-228, score-0.125]
</p><p>92 Table 2 summarizes the results of our experiment, where we have used topic vectors from unsupervised LDA and DiscLDA as input features to binary linear SVM classiﬁers. [sent-232, score-0.293]
</p><p>93 We also computed the prediction of the label of a document directly with DiscLDA. [sent-233, score-0.146]
</p><p>94 As shown in the table, the DiscLDA model clearly generates topic vectors with better predictive power than unsupervised LDA. [sent-234, score-0.368]
</p><p>95 In Table 3 we present the ten most probable words for a subset of topics learned using the discriminative DiscLDA approach. [sent-235, score-0.344]
</p><p>96 In particular, although we started with 20 shared topics the learned T had only 12 shared topics. [sent-237, score-0.437]
</p><p>97 We have grouped the topics in Table 3 according to whether they were class-speciﬁc or shared, uncovering an interesting latent structure which appears more discriminating than the topics presented in Table 1. [sent-238, score-0.436]
</p><p>98 5 Discussion We have presented DiscLDA, a variation on LDA in which the LDA parametrization is augmented to include a transformation matrix and in which this matrix is learned via a conditional likelihood criterion. [sent-239, score-0.272]
</p><p>99 low-dimensional representations of documents, but to also make use of discriminative side information (labels) in forming these representations. [sent-244, score-0.134]
</p><p>100 Given the high dimensionality of such models, it may be intractable to train all of the parameters via a discriminative criterion such as conditional likelihood. [sent-247, score-0.201]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disclda', 0.645), ('lda', 0.331), ('topic', 0.24), ('wdn', 0.195), ('topics', 0.189), ('zdn', 0.156), ('yd', 0.117), ('documents', 0.116), ('document', 0.113), ('shared', 0.111), ('newsgroups', 0.111), ('transformation', 0.106), ('god', 0.098), ('discriminative', 0.088), ('dimensionality', 0.08), ('bible', 0.078), ('jesus', 0.078), ('postings', 0.078), ('reduction', 0.071), ('dirichlet', 0.066), ('christ', 0.059), ('religion', 0.059), ('latent', 0.058), ('embedding', 0.055), ('unsupervised', 0.053), ('word', 0.047), ('classi', 0.047), ('wd', 0.047), ('multi', 0.047), ('newsgroup', 0.047), ('side', 0.046), ('government', 0.046), ('gibbs', 0.045), ('content', 0.045), ('labels', 0.043), ('likelihood', 0.043), ('class', 0.042), ('corpus', 0.042), ('proportions', 0.042), ('words', 0.041), ('atheism', 0.039), ('atheists', 0.039), ('christians', 0.039), ('dos', 0.039), ('morality', 0.039), ('scsi', 0.039), ('supervised', 0.039), ('ca', 0.039), ('auxiliary', 0.039), ('generative', 0.038), ('ik', 0.036), ('text', 0.035), ('plsa', 0.034), ('maaten', 0.034), ('church', 0.034), ('delivered', 0.034), ('fda', 0.034), ('sdr', 0.034), ('season', 0.034), ('conditional', 0.033), ('author', 0.033), ('drive', 0.033), ('representation', 0.033), ('label', 0.033), ('mixture', 0.033), ('le', 0.033), ('matrix', 0.032), ('men', 0.031), ('mimno', 0.031), ('bucket', 0.031), ('mail', 0.031), ('supervisory', 0.031), ('transformed', 0.03), ('retain', 0.03), ('moral', 0.029), ('team', 0.029), ('svm', 0.028), ('win', 0.028), ('players', 0.028), ('berkeley', 0.027), ('blei', 0.027), ('popular', 0.027), ('model', 0.027), ('card', 0.026), ('christian', 0.026), ('thing', 0.026), ('dir', 0.026), ('learned', 0.026), ('experiment', 0.026), ('graphics', 0.025), ('berg', 0.025), ('predictive', 0.025), ('mixing', 0.025), ('reduced', 0.024), ('block', 0.024), ('health', 0.024), ('accomplished', 0.024), ('mb', 0.024), ('people', 0.023), ('power', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="64-tfidf-1" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>2 0.22301522 <a title="64-tfidf-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.17890713 <a title="64-tfidf-3" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>4 0.16581002 <a title="64-tfidf-4" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>5 0.15024893 <a title="64-tfidf-5" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>6 0.14937286 <a title="64-tfidf-6" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>7 0.14267674 <a title="64-tfidf-7" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>8 0.13728862 <a title="64-tfidf-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.12008278 <a title="64-tfidf-9" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>10 0.11855578 <a title="64-tfidf-10" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>11 0.098778695 <a title="64-tfidf-11" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>12 0.088609137 <a title="64-tfidf-12" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>13 0.08690919 <a title="64-tfidf-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.082076497 <a title="64-tfidf-14" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>15 0.075695619 <a title="64-tfidf-15" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>16 0.073330082 <a title="64-tfidf-16" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>17 0.071938202 <a title="64-tfidf-17" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>18 0.067069978 <a title="64-tfidf-18" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>19 0.063835032 <a title="64-tfidf-19" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>20 0.061110899 <a title="64-tfidf-20" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.194), (1, -0.144), (2, 0.081), (3, -0.168), (4, -0.056), (5, -0.012), (6, 0.115), (7, 0.184), (8, -0.253), (9, 0.025), (10, -0.01), (11, -0.088), (12, -0.062), (13, 0.099), (14, -0.029), (15, -0.019), (16, 0.098), (17, -0.011), (18, -0.088), (19, -0.082), (20, -0.015), (21, -0.051), (22, -0.004), (23, 0.057), (24, 0.067), (25, -0.01), (26, -0.043), (27, 0.078), (28, 0.007), (29, -0.076), (30, 0.032), (31, 0.055), (32, 0.108), (33, -0.06), (34, -0.047), (35, 0.011), (36, -0.054), (37, 0.067), (38, 0.015), (39, -0.014), (40, 0.044), (41, -0.116), (42, 0.073), (43, -0.041), (44, 0.038), (45, -0.027), (46, 0.055), (47, -0.019), (48, 0.024), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94148612 <a title="64-lsi-1" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>2 0.87420368 <a title="64-lsi-2" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>Author: Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with signiﬁcant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced. 1</p><p>3 0.84602547 <a title="64-lsi-3" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>4 0.83755219 <a title="64-lsi-4" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>5 0.759161 <a title="64-lsi-5" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>6 0.7241317 <a title="64-lsi-6" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>7 0.6036973 <a title="64-lsi-7" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>8 0.54877073 <a title="64-lsi-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.44591936 <a title="64-lsi-9" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>10 0.4251461 <a title="64-lsi-10" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>11 0.41591865 <a title="64-lsi-11" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>12 0.40014797 <a title="64-lsi-12" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>13 0.39148822 <a title="64-lsi-13" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>14 0.37636021 <a title="64-lsi-14" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>15 0.36114758 <a title="64-lsi-15" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>16 0.35849139 <a title="64-lsi-16" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>17 0.3542673 <a title="64-lsi-17" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>18 0.35388044 <a title="64-lsi-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.3214795 <a title="64-lsi-19" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>20 0.28566846 <a title="64-lsi-20" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.055), (7, 0.091), (8, 0.242), (12, 0.041), (28, 0.141), (57, 0.09), (59, 0.012), (63, 0.031), (71, 0.016), (77, 0.047), (78, 0.018), (83, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79759222 <a title="64-lda-1" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>2 0.72067785 <a title="64-lda-2" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and indicate important underlying structure in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA, which is able to avoid the local optima of typical EM learning. Moreover, by introducing a sample-based approximation to exponential family models, it overcomes the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained using a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data. 1</p><p>3 0.66446543 <a title="64-lda-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.6640799 <a title="64-lda-4" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>5 0.66006869 <a title="64-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.65781122 <a title="64-lda-6" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>7 0.65687674 <a title="64-lda-7" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>8 0.65681362 <a title="64-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.65303135 <a title="64-lda-9" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>10 0.65266454 <a title="64-lda-10" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>11 0.65223837 <a title="64-lda-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.65174901 <a title="64-lda-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.65114701 <a title="64-lda-13" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>14 0.64852071 <a title="64-lda-14" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>15 0.64812934 <a title="64-lda-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.64753664 <a title="64-lda-16" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>17 0.64740139 <a title="64-lda-17" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>18 0.64375043 <a title="64-lda-18" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>19 0.64216036 <a title="64-lda-19" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>20 0.64210975 <a title="64-lda-20" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
