<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-149" href="#">nips2008-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</h1>
<br/><p>Source: <a title="nips-2008-149-pdf" href="http://papers.nips.cc/paper/3424-near-minimax-recursive-density-estimation-on-the-binary-hypercube.pdf">pdf</a></p><p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>Reference: <a title="nips-2008-149-reference" href="../nips2008_reference/nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. [sent-8, score-0.397]
</p><p>2 For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. [sent-9, score-0.069]
</p><p>3 1 Introduction Multivariate binary data arise in a variety of ﬁelds, such as biostatistics [1], econometrics [2] or artiﬁcial intelligence [3]. [sent-12, score-0.08]
</p><p>4 In these and other settings, it is often necessary to estimate a probability density from a number of independent observations. [sent-13, score-0.117]
</p><p>5 samples from a probability density f (with respect to the counting measure) on the d-dimensional bi△ nary hypercube B d, B = {0, 1}, and seek an estimate f of f with a small mean-squared error 2 MSE(f, f ) = E x∈Bd (f (x) − f (x)) . [sent-17, score-0.179]
</p><p>6 In many cases of practical interest, the number of covariates d is much larger than log n, so direct estimation of f as a multinomial density with 2d parameters is both unreliable and impractical. [sent-18, score-0.214]
</p><p>7 Thus, one has to resort to “nonparametric” methods and search for good estimators in a suitably deﬁned class whose complexity grows with n. [sent-19, score-0.139]
</p><p>8 Some nonparametric methods proposed in the literature, such as kernels [4] and orthogonal expansions [5, 6], either have very slow rates of MSE convergence or are computationally prohibitive for large d. [sent-20, score-0.107]
</p><p>9 For example, the kernel method [4] requires O(n2 d) operations to compute the estimate at any x ∈ B d , yet its MSE decays as O(n−4/(4+d) ) [7], which is extremely slow when d is large. [sent-21, score-0.05]
</p><p>10 In contrast, orthogonal function methods generally have much better MSE decay rates, but rely on estimating 2d coefﬁcients in a ﬁxed basis, which requires enormous computational resources for large d. [sent-22, score-0.106]
</p><p>11 For instance, using the Fast Hadamard Transform to estimate the coefﬁcients in the so-called Walsh basis using n samples requires O(nd2d ) operations [5]. [sent-23, score-0.05]
</p><p>12 In this paper we take up the problem of accurate, computationally tractable estimation of a density on the binary hypercube. [sent-24, score-0.16]
</p><p>13 We take the minimax point of view, where we assume that f comes from a particular function class F and seek an estimator that approximately attains the minimax MSE △  ∗ Rn (F ) = inf sup MSE(f, f ), b f f ∈F  where the inﬁmum is over all estimators based on n i. [sent-25, score-0.361]
</p><p>14 We will deﬁne our function class to reﬂect another feature often encountered in situations involving multivariate binary data: namely,  that the shape of the underlying density is strongly inﬂuenced by small constellations of the d covariates. [sent-29, score-0.188]
</p><p>15 To model such “constellation effects” mathematically, we will consider classes of densities that satisfy a particular sparsity condition. [sent-31, score-0.15]
</p><p>16 The algorithm entails recursively examining empirical estimates of whole blocks of the 2d basis coefﬁcients. [sent-33, score-0.049]
</p><p>17 An additional attractive feature of our approach is that it gives us a principled way of trading off MSE against computational complexity by controlling the decay of the threshold as a function of the recursion depth. [sent-36, score-0.186]
</p><p>18 Let µd denote the counting measure on the d-dimensional binary hypercube B d . [sent-41, score-0.108]
</p><p>19 When η = 1/2, we get the standard Walsh system used in [5, 6]; in that case, we shall omit the index η = 1/2 for simplicity. [sent-57, score-0.086]
</p><p>20 The product structure of the biased Walsh bases makes them especially convenient for statistical applications as it allows for a computationally efﬁcient recursive method for computing accurate estimates of squared coefﬁcients in certain hierarchically structured sets. [sent-58, score-0.273]
</p><p>21 We are interested in densities whose representations in some biased Walsh basis satisfy a certain sparsity constraint. [sent-60, score-0.207]
</p><p>22 (3)  It is not hard to show that the coefﬁcients of any probability density on B d in Φd,η are bounded by R(η) = [η ∨ (1 − η)]d/2 . [sent-71, score-0.09]
</p><p>23 When η = 1/2, with R(η) = 2−d/2 , we shall write simply Fd (p). [sent-74, score-0.057]
</p><p>24 Given any f ∈ Fd (p, η) and denoting by fk the function obtained from it by retaining only the k largest coefﬁcients, we get from Parseval’s identity that f − fk  L2 (µd )  ≤ CRk −r . [sent-84, score-0.099]
</p><p>25 Other densities in {Φd (p, η) : 0 < p < 2} include, for example, mixtures of components that, up to a permutation of {1, . [sent-89, score-0.094]
</p><p>26 , d}, can be written as a tensor product of a large number of Bernoulli(η ∗ ) densities and some other density. [sent-92, score-0.147]
</p><p>27 3 Density estimation via recursive Walsh thresholding We now turn to our problem of estimating a density f on B d from a sample {Xi }n when f ∈ Fd (p) i=1 for some unknown 0 < p < 2. [sent-105, score-0.467]
</p><p>28 The minimax theory for weak-ℓp balls [10] says that d  ∗ Rn (Fd (p)) ≥ CM −p/2 n−2r/(2r+1) ,  r = 1/p − 1/2  where M = 2 . [sent-106, score-0.111]
</p><p>29 We shall construct an estimator that adapts to unknown sparsity of f in the sense that it achieves this minimax rate up to a logarithmic factor without prior knowledge of p and that its computational complexity improves as p → 0. [sent-107, score-0.502]
</p><p>30 Our method is based on the thresholding of empirical Walsh coefﬁcients. [sent-108, score-0.166]
</p><p>31 A thresholding estimator is any estimator of the form I{T (θs )≥λn } θs ϕs , f= b s∈Bd n where θs = (1/n) i=1 ϕs (Xi ) are some statistic, and I{·} is an indicator  empirical estimates of the Walsh coefﬁcients of f , T (·) is function. [sent-109, score-0.378]
</p><p>32 For 2 example, in [5, 6] the statistic T (θs ) = θs was used with the threshold λn = 1/M (n + 1). [sent-111, score-0.072]
</p><p>33 While this is not an issue when d ≍ log n, it is clearly impractical when d ≫ log n. [sent-114, score-0.134]
</p><p>34 To deal with this issue, we will consider a recursive thresholding approach that will allow us to reject whole groups of coefﬁcients based on efﬁciently computable statistics. [sent-115, score-0.398]
</p><p>35 For any 1 ≤ k ≤ d, we can write any f ∈ L2 (µd ) with the Walsh coefﬁcients θ(f ) as θuv ϕuv =  f= u∈Bk v∈Bd−k  u∈Bk  fu ⊗ ϕu ,  △  where uv denotes the concatenation of u ∈ B k and v ∈ B d−k and, for each u ∈ B k , fu = △ 2 2 2 v∈Bd−k θuv ϕv lies in L (µd−k ). [sent-117, score-0.509]
</p><p>36 By Parseval’s identity, Wu = fu L2 (µd−k ) = v∈Bd−k θuv . [sent-118, score-0.162]
</p><p>37 We will use the following fact, easily proved using the deﬁnitions (1) and (2) of the Walsh functions: for any density f on B d , any k and u ∈ B k , we have fu (y) = Ef ϕu (πk (X))I{σk (X)=y} , ∀y ∈ B d−k △  and Wu = Ef {ϕu (πk (X))fu (σk (X))} ,  △  where πk (x) = (x(1), . [sent-127, score-0.252]
</p><p>38 Now we can deﬁne our density estimation procedure. [sent-140, score-0.09]
</p><p>39 Instead of using a single threshold for all 1 ≤ k ≤ d, we consider a more ﬂexible strategy: for every k, we shall compare each Wu to a threshold that depends not only on n, but also on k. [sent-141, score-0.127]
</p><p>40 Speciﬁcally, we will let λk,n =  αk log n , n  1≤k≤d  (7)  where α = {αk }d satisﬁes α1 ≥ αk ≥ αd > 0. [sent-142, score-0.067]
</p><p>41 ) Given λ = {λk,n }d , deﬁne the set A(λ) = {s ∈ B d : k=1 Wπk (s) ≥ λk,n , ∀1 ≤ k ≤ d} and the corresponding estimator △  fRWT =  I{s∈A(λ)} θs ϕs ,  (8)  s∈Bd  where RWT stands for “recursive Walsh thresholding. [sent-144, score-0.106]
</p><p>42 We now turn to the asymptotic analysis of the MSE and the computational complexity of fRWT . [sent-147, score-0.104]
</p><p>43 We ﬁrst prove that fRWT adapts to unknown sparsity of f : Theorem 3. [sent-148, score-0.142]
</p><p>44 1 Suppose the threshold sequence λ = {λk }d is such that αd ≥ (20d + 25)2 /2d. [sent-149, score-0.046]
</p><p>45 k=1 Then for all 0 < p < 2 the estimator (8) satisﬁes sup MSE(f, fRWT ) = f ∈Fd (p)  sup Ef f − fRWT  f ∈Fd (p)  2 L2 (µd )  ≤  C 2d  2d α1 log n n  2r/(2r+1)  where the constant C depends only on p. [sent-150, score-0.242]
</p><p>46 Deﬁning the sets A1 = {s ∈ B d : θs ≥ λd,n } and d 2 2 2 A2 = {s ∈ B : θs < λ1,n }, we get T1 ≤ s I{s∈A1 } (θs − θs ) and T2 ≤ s I{s∈A2 } θs . [sent-153, score-0.051]
</p><p>47 Applying (4), (5) and a bit of algebra, we get E T12 ≤ E T22 ≤  1 Mn  s∈Bd  2 s : θs ≥ λd,n /2  ≤  1 Mn  2 2 I{θs <(3α1 /2) log n/n} θs ≤  C M  2 M λd,n  p/2  ≤  M α1 log n n  1 −2r/(2r+1) n , M  (10)  2r/(2r+1)  . [sent-156, score-0.185]
</p><p>48 Using Cauchy–Schwarz, we get E T11 ≤  s  1/2  E(θs − θs )4 · P(s ∈ A1 ∩ B)  . [sent-158, score-0.051]
</p><p>49 (12)  To estimate the fourth moment in (12), we use Rosenthal’s inequality [14] to get E(θs − θs )4 ≤ c/M 2 n2 . [sent-159, score-0.105]
</p><p>50 To bound the probability that s ∈ A1 ∩ B, we observe that s ∈ A1 ∩ B implies that |θs − θs | ≥ (1/5) λd,n , and then use Bernstein’s inequality [14] to get P |θs − θs | ≥ (1/5) λd,n ≤ 2 exp −  β 2 log n 2(1 + 2β/3)  = 2n−β  2  /[2(1+2β/3)]  √ with β = (1/5) M αd ≥ 4d + 5. [sent-160, score-0.145]
</p><p>51 Using the same argument as above, we get P(s ∈ A2 ∩ S) ≤ √ 2 2n−(γ−1)/2 , where γ = (1/5) M α1 . [sent-163, score-0.051]
</p><p>52 (10), (11), (13), and (14), we get (9), and the theorem is proved. [sent-166, score-0.051]
</p><p>53 2 Given any δ ∈ (0, 1), provided each αk is chosen so that √ 2k αk n log n ≥ 5 C2 n + (log(d/δ) + k)/ log e , 2  p/2  Algorithm 1 runs in O(n d(n/M log n)  K(α, p)) time with probability at least 1 − δ. [sent-169, score-0.23]
</p><p>54 (15)  Proof: The complexity is determined by the number of calls to R ECURSIVE WALSH. [sent-171, score-0.216]
</p><p>55 Let us say that a call to R ECURSIVE WALSH(u, λ) is correct if Wu ≥ λk,n /2. [sent-173, score-0.06]
</p><p>56 We will show that, with probability at least 1 − δ, only the correct calls are made. [sent-174, score-0.161]
</p><p>57 For a given u ∈ B k , Wu ≥ λk,n and Wu < λk,n /2 together imply that fu − fu △  2 L2 (µd−k ) k  ≥  (1/5) λk,n , where fu = v∈Bd−k θuv ϕv . [sent-176, score-0.486]
</p><p>58 Now, it can be shown that, for every u ∈ B , the norm fu − fu L2 (µd−k ) can be expressed as a supremum of an empirical process [15] over a certain function class that depends on k (details are omitted for lack of space). [sent-177, score-0.35]
</p><p>59 We can then use Talagrand’s concentration-of-measure inequality for empirical processes [16] to get P(Wu ≥ λk,n , Wu < λk,n /2) ≤ exp − nC1 (2k a2 ∧ 2k/2 ak,n ) , k,n √ k n, and C , C are the absolute constants in Talagrand’s where ak,n = (1/5) αk log n/n − C2 / 2 1 2 bound. [sent-178, score-0.193]
</p><p>60 Summing over k, u ∈ B k , we see that, with probability ≥ 1 − δ, only the correct calls will be made. [sent-180, score-0.161]
</p><p>61 Hence, the number of correct d recursive calls is bounded by N = k=1 (2/M λk,n )p/2 = (2n/M log n)p/2 K(α, p). [sent-184, score-0.394]
</p><p>62 Therefore, with probability at least 1 − δ, the time complexity will be as stated in the theorem. [sent-186, score-0.079]
</p><p>63 By controlling the rate at which the sequence αk decays with k, we can trade off MSE against complexity. [sent-189, score-0.084]
</p><p>64 However, it has K(α, p) = O(M p/2 d), resulting in O(d2 n2 (n/ log n)p/2 ) complexity. [sent-195, score-0.067]
</p><p>65 The second case, which leads to a very severe estimator that will tend to reject a lot of coefﬁcients, has MSE of O((log n/n)2r/(2r+1) M −1/(2r+1) ), but K(α, p) = O(M p/2 ), leading to a considerably better O(dn2 (n/ log n)p/2 ) complexity. [sent-196, score-0.244]
</p><p>66 However, this reduction in complexity will be offset by a corresponding increase in MSE. [sent-198, score-0.079]
</p><p>67 In fact, using exponentially decaying αk ’s in practice is not advisable as its low complexity is mainly due to the fact that it will tend to reject even the big coefﬁcients very early on, especially when d is large. [sent-199, score-0.189]
</p><p>68 To achieve a good balance between complexity and MSE, a moderately decaying threshold sequence might be best, e. [sent-200, score-0.164]
</p><p>69 As p → 0, the effect of λ on complexity becomes negligible, and the complexity tends to O(n2 d). [sent-203, score-0.158]
</p><p>70 In practice renormalization may be computationally expensive when d is very large. [sent-208, score-0.07]
</p><p>71 If the estimate is suitably sparse, however, the renormalization can be carried out approximately using Monte-Carlo methods. [sent-209, score-0.104]
</p><p>72 4 Simulations The focus of our work is theoretical, consisting in the derivation of a recursive thresholding procedure for estimating multivariate binary densities (Algorithm 1), with a proof of its near-minimaxity  and an asymptotic analysis of its complexity. [sent-210, score-0.551]
</p><p>73 Although an extensive empirical evaluation is outside the scope of this paper, we have implemented the proposed estimator, and now present some simulation results to demonstrate its small-sample performance. [sent-211, score-0.052]
</p><p>74 We generated synthetic observations from a mixture density f on a 15-dimensional binary hypercube. [sent-212, score-0.136]
</p><p>75 The mixture has 10 components, where each component is a product density with 12 randomly chosen covariates having Bernoulli(1/2) distributions, and the other three having Bernoulli(0. [sent-213, score-0.172]
</p><p>76 As can be seen from the coefﬁcient proﬁle in the bottom of the ﬁgure, this density is clearly sparse. [sent-218, score-0.09]
</p><p>77 1 also shows the estimated probabilities and the Walsh coefﬁcients for sample sizes n = 5000 (middle) and n = 10000 (right). [sent-220, score-0.046]
</p><p>78 b fRWT , n = 5000  Ground truth (f )  b fRWT , n = 10000  Figure 1: Ground truth (left) and estimated density for n = 5000 (middle) and n = 10000 (right) with constant thresholding. [sent-221, score-0.187]
</p><p>79 Top: true and estimated probabilities (clipped at zero and renormalized) arranged in lexicographic order. [sent-222, score-0.104]
</p><p>80 Bottom: absolute values of true and estimated Walsh coefﬁcients arranged in lexicographic order. [sent-223, score-0.126]
</p><p>81 For the estimated densities, the coefﬁcient plots also show the threshold level (dotted line) and absolute values of the rejected coefﬁcients (lighter color). [sent-224, score-0.092]
</p><p>82 5  1400  Time (s)  MSE (× 2d)  40  constant log linear  0. [sent-231, score-0.088]
</p><p>83 All results are averaged over ﬁve independent runs for each sample size (the error bars show the standard deviations). [sent-233, score-0.051]
</p><p>84 To study the trade-off between MSE and complexity, we implemented three different thresholding schemes: (1) constant, λk,n = 2 log n/(2d n), (2) logarithmic, λk,n = 2 log(d − k + 2) log n/(2d n), and (3) linear, λk,n = 2(d − k + 1) log n/(2d n). [sent-234, score-0.367]
</p><p>85 Up to the log n factor (dictated by the theory), the thresholds at k = d are set to twice the variance of the empirical estimate of any coefﬁcient whose value is zero; this forces the estimator to reject empirical coefﬁcients whose values cannot be reliably distinguished from zero. [sent-235, score-0.323]
</p><p>86 Occasionally, spurious coefﬁcients get retained, as can be seen in Fig. [sent-236, score-0.051]
</p><p>87 In agreement with the theory, MSE is the smallest for the constant thresholding scheme [which is simply an efﬁcient recursive implementation of a term-by-term thresholding estimator with λn ∼ log n/(M n)], and then it increases for the logarithmic and for the linear schemes. [sent-242, score-0.723]
</p><p>88 2(b,c) shows the running time (in seconds) and the number of recursive  calls made to R ECURSIVE WALSH vs. [sent-244, score-0.33]
</p><p>89 The number of recursive calls is a platformindependent way of gauging the computational complexity of the algorithm, although it should be kept in mind that each recursive call has O(n2 d) overhead. [sent-246, score-0.584]
</p><p>90 The running time increases polynomially with n, and is the largest for the constant scheme, followed by the logarithmic and the linear schemes. [sent-247, score-0.103]
</p><p>91 We see that, while the MSE of the logarithmic scheme is fairly close to that of the constant scheme, its complexity is considerably lower, in terms of both the number of recursive calls and the running time. [sent-248, score-0.513]
</p><p>92 In all three cases, the number of recursive calls decreases with n due to the fact that weight estimates become increasingly accurate with n, which causes the expected number of false discoveries (i. [sent-249, score-0.303]
</p><p>93 , making a recursive call at an internal node of the tree only to reject its descendants later) to decrease. [sent-251, score-0.273]
</p><p>94 2(d) shows the number of coefﬁcients retained in the estimate. [sent-253, score-0.049]
</p><p>95 This number grows with n as a consequence of the fact that the threshold decreases with n, while the number of accurately estimated coefﬁcients increases. [sent-254, score-0.07]
</p><p>96 The true density f has 40 parameters: 9 to specify the weights of the components, 3 per component to locate the indices of the nonuniform covariates, and the single Bernoulli parameter of the nonuniform covariates. [sent-255, score-0.17]
</p><p>97 Overall, these preliminary simulation results show that our implemented estimator behaves in accordance with the theory even in the small-sample regime. [sent-257, score-0.132]
</p><p>98 The performance of the logarithmic thresholding scheme is especially encouraging, suggesting that it may be possible to trade off MSE against complexity in a way that will scale to large values of d. [sent-258, score-0.341]
</p><p>99 To model their densities, we plan to experiment with Walsh bases with η biased toward unity. [sent-266, score-0.08]
</p><p>100 Some classiﬁcation procedures for multivariate binary data using orthogonal functions. [sent-297, score-0.137]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('walsh', 0.479), ('frwt', 0.342), ('mse', 0.313), ('wu', 0.278), ('bd', 0.188), ('ecursive', 0.183), ('fd', 0.17), ('recursive', 0.166), ('coef', 0.163), ('uv', 0.163), ('fu', 0.162), ('cients', 0.152), ('thresholding', 0.14), ('calls', 0.137), ('estimator', 0.106), ('densities', 0.094), ('density', 0.09), ('minimax', 0.085), ('complexity', 0.079), ('reject', 0.071), ('log', 0.067), ('adapts', 0.065), ('bk', 0.057), ('nc', 0.057), ('covariates', 0.057), ('sparsity', 0.056), ('logarithmic', 0.055), ('multivariate', 0.052), ('duke', 0.051), ('durham', 0.051), ('get', 0.051), ('retained', 0.049), ('bernoulli', 0.047), ('threshold', 0.046), ('chapel', 0.046), ('crk', 0.046), ('goldreich', 0.046), ('lexicographic', 0.046), ('parseval', 0.046), ('renormalization', 0.046), ('binary', 0.046), ('nonuniform', 0.04), ('lazebnik', 0.04), ('hypercube', 0.04), ('orthogonal', 0.039), ('decay', 0.039), ('decaying', 0.039), ('trade', 0.039), ('talagrand', 0.037), ('willett', 0.037), ('call', 0.036), ('shall', 0.035), ('econometrics', 0.034), ('arranged', 0.034), ('biased', 0.034), ('attains', 0.032), ('sparser', 0.032), ('suitably', 0.031), ('hill', 0.03), ('runs', 0.029), ('estimators', 0.029), ('tensor', 0.028), ('scheme', 0.028), ('estimating', 0.028), ('participants', 0.027), ('running', 0.027), ('estimate', 0.027), ('inequality', 0.027), ('truth', 0.026), ('empirical', 0.026), ('implemented', 0.026), ('statistic', 0.026), ('balls', 0.026), ('ground', 0.025), ('asymptotic', 0.025), ('product', 0.025), ('orthonormal', 0.024), ('sup', 0.024), ('panel', 0.024), ('fk', 0.024), ('mn', 0.024), ('bases', 0.024), ('computationally', 0.024), ('estimated', 0.024), ('correct', 0.024), ('cm', 0.023), ('decays', 0.023), ('fourier', 0.023), ('basis', 0.023), ('nonparametric', 0.022), ('write', 0.022), ('plan', 0.022), ('prohibitive', 0.022), ('controlling', 0.022), ('counting', 0.022), ('sample', 0.022), ('absolute', 0.022), ('constant', 0.021), ('groups', 0.021), ('unknown', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="149-tfidf-1" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>2 0.092378609 <a title="149-tfidf-2" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>3 0.074748904 <a title="149-tfidf-3" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>4 0.070813209 <a title="149-tfidf-4" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><p>5 0.067794159 <a title="149-tfidf-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.065005019 <a title="149-tfidf-6" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>7 0.064226419 <a title="149-tfidf-7" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>8 0.061586015 <a title="149-tfidf-8" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>9 0.060636591 <a title="149-tfidf-9" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>10 0.056349158 <a title="149-tfidf-10" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>11 0.054576166 <a title="149-tfidf-11" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>12 0.05360695 <a title="149-tfidf-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.053115226 <a title="149-tfidf-13" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>14 0.05226393 <a title="149-tfidf-14" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>15 0.051361006 <a title="149-tfidf-15" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>16 0.050747279 <a title="149-tfidf-16" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>17 0.05073059 <a title="149-tfidf-17" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>18 0.050125547 <a title="149-tfidf-18" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>19 0.047882847 <a title="149-tfidf-19" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>20 0.047394991 <a title="149-tfidf-20" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.148), (1, 0.011), (2, -0.036), (3, 0.063), (4, 0.047), (5, -0.01), (6, -0.031), (7, 0.045), (8, 0.032), (9, 0.054), (10, -0.044), (11, -0.018), (12, 0.046), (13, -0.024), (14, -0.129), (15, -0.005), (16, 0.036), (17, 0.028), (18, 0.006), (19, 0.056), (20, -0.013), (21, 0.087), (22, 0.068), (23, -0.055), (24, 0.022), (25, 0.095), (26, -0.007), (27, -0.086), (28, 0.09), (29, 0.001), (30, 0.036), (31, -0.015), (32, 0.033), (33, 0.043), (34, -0.0), (35, 0.055), (36, 0.007), (37, 0.022), (38, 0.088), (39, -0.009), (40, -0.072), (41, -0.117), (42, 0.005), (43, -0.134), (44, -0.171), (45, -0.114), (46, 0.022), (47, -0.002), (48, -0.057), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93173146 <a title="149-lsi-1" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>2 0.63036537 <a title="149-lsi-2" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>3 0.61971432 <a title="149-lsi-3" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>Author: Guangzhi Cao, Charles Bouman</p><p>Abstract: Covariance estimation for high dimensional vectors is a classically difﬁcult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More speciﬁcally, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efﬁciently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efﬁciently computed using a cross-validation procedure. The resulting estimator is positive deﬁnite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes. 1</p><p>4 0.60070437 <a title="149-lsi-4" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><p>5 0.59449941 <a title="149-lsi-5" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>6 0.52547008 <a title="149-lsi-6" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>7 0.51721275 <a title="149-lsi-7" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>8 0.47252896 <a title="149-lsi-8" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>9 0.44841912 <a title="149-lsi-9" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>10 0.44416511 <a title="149-lsi-10" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>11 0.43289858 <a title="149-lsi-11" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>12 0.41757569 <a title="149-lsi-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.41739258 <a title="149-lsi-13" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>14 0.41500059 <a title="149-lsi-14" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>15 0.38234776 <a title="149-lsi-15" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>16 0.36549219 <a title="149-lsi-16" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>17 0.36277574 <a title="149-lsi-17" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>18 0.36103767 <a title="149-lsi-18" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>19 0.35979149 <a title="149-lsi-19" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>20 0.35341099 <a title="149-lsi-20" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (6, 0.104), (7, 0.088), (12, 0.048), (28, 0.158), (57, 0.08), (59, 0.035), (63, 0.021), (71, 0.025), (75, 0.239), (77, 0.033), (78, 0.017), (83, 0.043), (94, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82328093 <a title="149-lda-1" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>2 0.78112304 <a title="149-lda-2" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>3 0.75830835 <a title="149-lda-3" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil, Sergio R. Galeano</p><p>Abstract: Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classiﬁcation tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information. 1</p><p>4 0.6883384 <a title="149-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.68100446 <a title="149-lda-5" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>6 0.67553943 <a title="149-lda-6" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>7 0.67294693 <a title="149-lda-7" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>8 0.6703862 <a title="149-lda-8" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>9 0.67000622 <a title="149-lda-9" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>10 0.66961384 <a title="149-lda-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.66679448 <a title="149-lda-11" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>12 0.66657865 <a title="149-lda-12" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>13 0.66625351 <a title="149-lda-13" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>14 0.66607594 <a title="149-lda-14" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>15 0.66536766 <a title="149-lda-15" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>16 0.66474515 <a title="149-lda-16" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>17 0.66290867 <a title="149-lda-17" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>18 0.66254866 <a title="149-lda-18" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>19 0.66248697 <a title="149-lda-19" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>20 0.66238052 <a title="149-lda-20" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
