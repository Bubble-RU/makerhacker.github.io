<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-14" href="#">nips2008-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</h1>
<br/><p>Source: <a title="nips-2008-14-pdf" href="http://papers.nips.cc/paper/3586-adaptive-forward-backward-greedy-algorithm-for-sparse-learning-with-linear-models.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>Reference: <a title="nips-2008-14-reference" href="../nips2008_reference/nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. [sent-3, score-0.181]
</p><p>2 We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. [sent-4, score-0.141]
</p><p>3 Two heuristics that are widely used in practice are forward and backward greedy algorithms. [sent-5, score-0.941]
</p><p>4 Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. [sent-7, score-0.973]
</p><p>5 If we know the sparsity parameter k, a good learning method is L0 regularization: ˆ w = arg min  w∈Rd  1 n  n  φ(wT xi , yi ) subject to w  0  ≤ k. [sent-29, score-0.199]
</p><p>6 Generally speaking, one is interested in two closely related themes: feature selection, or identifying the basis functions with non-zero coefﬁcients; estimation accuracy, or reconstructing the target function from noisy observations. [sent-33, score-0.209]
</p><p>7 It can be shown that the solution of the L0 regularization problem in (1) achieves ¯ good prediction accuracy if the target function can be approximated by a sparse w. [sent-34, score-0.262]
</p><p>8 It can also solve the feature selection problem under extra identiﬁability assumptions. [sent-35, score-0.159]
</p><p>9 It is known that L1 regularization often leads to sparse solutions. [sent-45, score-0.14]
</p><p>10 For example, if the target is truly sparse, then it was shown in [10] that under some restrictive conditions referred to as irrepresentable conditions, L1 regularization solves the feature selection problem. [sent-47, score-0.343]
</p><p>11 The second approach to approximately solve the subset selection problem is forward greedy algorithm, which we will describe in details in Section 2. [sent-51, score-0.66]
</p><p>12 In this paper, we are particularly interested in greedy algorithms because they have been widely used but the effectiveness has not been well analyzed. [sent-56, score-0.376]
</p><p>13 As we shall explain later, neither the standard forward greedy idea nor th standard backward greedy idea is adequate for our purpose. [sent-57, score-1.268]
</p><p>14 This leads to a novel adaptive forward-backward greedy algorithm which we present in Section 3. [sent-59, score-0.304]
</p><p>15 For least squares loss, we obtain strong theoretical results showing that the method can solve the feature selection problem under moderate conditions. [sent-61, score-0.29]
</p><p>16 Instead of working with n input data vectors xi ∈ Rd , we work with d feature vectors fj ∈ Rn (j = 1, . [sent-64, score-0.205]
</p><p>17 Each fj corresponds to the j-th feature component of xi for i = 1, . [sent-68, score-0.205]
</p><p>18 For least squares 2 d regression, we have R(w) = n−1 j wj fj − y 2 . [sent-77, score-0.314]
</p><p>19 1 Deﬁne supp(w) = {j : wj = 0} as the set of nonzero coefﬁcients of a vector w = [w1 , . [sent-81, score-0.125]
</p><p>20 For a weight vector w ∈ Rd , we deﬁne mapping f : Rd → Rn as: d d ˆ f (w) = j=1 wj fj . [sent-85, score-0.223]
</p><p>21 , d}, let w(F, f ) = minw∈Rd f (w) − 2 ˆ ˆ f 2 subject to supp(w) ⊂ F , and let w(F ) = w(F, y) be the solution of the least squares problem using features F . [sent-89, score-0.234]
</p><p>22 2  Forward and Backward Greedy Algorithms  Forward greedy algorithms have been widely used in applications. [sent-90, score-0.304]
</p><p>23 Although a number of variations exist, they all share the basic form of greedily picking an additional feature at every step to aggressively reduce the cost function. [sent-92, score-0.168]
</p><p>24 Therefore using the forward greedy algorithm, we will ﬁnd f3 ﬁrst, then f1 and f2 . [sent-98, score-0.537]
</p><p>25 The above argument implies that forward greedy method is inadequate for feature selection. [sent-100, score-0.634]
</p><p>26 let i(k) = arg mini minα R(w(k−1) + αei ) let F (k) = {i(k) } ∪ F (k−1) ˆ let w(k) = w(F (k) ) if (R(w(k−1) ) − R(w(k) ) ≤ ) break end Figure 1: Forward Greedy Algorithm f1  f5  f4  f3  y  f2  Figure 2: Failure of Forward Greedy Algorithm  [7]. [sent-107, score-0.152]
</p><p>27 In general, Figure 2 (which is the case we are interested in in this paper) shows that forward greedy algorithm will make errors that are not corrected later on. [sent-108, score-0.599]
</p><p>28 In order to remedy the problem, the so-called backward greedy algorithm has been widely used by practitioners. [sent-109, score-0.683]
</p><p>29 The idea is to train a full model with all the features, and greedily remove one feature (with the smallest increase of cost function) at a time. [sent-110, score-0.187]
</p><p>30 Although at the ﬁrst sight, backward greedy method appears to be a reasonable idea that addresses the problem of forward greedy algorithm, it is computationally very costly because it starts with a full model with all features. [sent-111, score-1.175]
</p><p>31 In this case, the method has no ability to tell which feature is irrelevant and which feature is relevant because removing any feature still completely overﬁts the data. [sent-115, score-0.226]
</p><p>32 3  Adaptive Forward-Backward Greedy Algorithm  The main strength of forward greedy algorithm is that it always works with a sparse solution explicitly, and thus computationally efﬁcient. [sent-117, score-0.64]
</p><p>33 On the other hand, backward greedy steps can potentially correct such an error, but need to start with a good model that does not completely overﬁt the data — it can only correct errors with a small amount of overﬁtting. [sent-120, score-0.711]
</p><p>34 However, a key design issue is how to implement a backward greedy strategy that is provably effective. [sent-122, score-0.68]
</p><p>35 For example, the standard heuristics, described in [5] and implemented in SAS, includes another threshold in addition to : a feature is deleted if the cost-function increase by performing the deletion is no more than . [sent-124, score-0.15]
</p><p>36 Unfortunately we cannot provide an effectiveness proof for this heuristics: if the threshold is too small, then it cannot delete any spurious features introduced in the forward steps; if it is too large, then one cannot make progress because good features are also deleted. [sent-125, score-0.474]
</p><p>37 3  This paper takes a more principled approach, where we speciﬁcally design a forward-backward greedy procedure with adaptive backward steps that are carried out automatically. [sent-127, score-0.73]
</p><p>38 The procedure has provably good performance and ﬁxes the drawbacks of forward greedy algorithm illustrated in Figure 2. [sent-128, score-0.587]
</p><p>39 Note that we only take a backward step when the increase of cost function is no more than half of the decrease of cost function in earlier forward steps. [sent-132, score-0.723]
</p><p>40 This implies that if we take forward steps, then no matter how many backward steps are performed, the cost function is decreased by at least an amount of /2. [sent-133, score-0.709]
</p><p>41 Again, in the ﬁrst three forward steps, we will be able to pick f3 , followed by f1 and f2 . [sent-141, score-0.283]
</p><p>42 After the third step, since we are able to express y using f1 and f2 only, by removing f3 in the backward step, we do not increase the cost. [sent-142, score-0.363]
</p><p>43 Therefore at this stage, we are able to successfully remove the incorrect basis f3 while keeping the good features f1 and f2 . [sent-143, score-0.162]
</p><p>44 In the following, we formally characterize this intuitive example, and prove the effectiveness of FoBa for feature selection as well as parameter estimation. [sent-145, score-0.171]
</p><p>45 For example, for random basis functions fj , we may take ln d = O(n/k) and still have ρ(k) to be bounded away from zero. [sent-150, score-0.237]
</p><p>46 This quantity is the smallest eigenvalue of the k × k diagonal blocks of the d × d design matrix [fiT fj ]i,j=1,. [sent-151, score-0.221]
</p><p>47 We shall refer it to as the sparse eigenvalue condition. [sent-155, score-0.151]
</p><p>48 Assume that the basis 2 1 2 functions are normalized such that n fj 2 = 1 for all j = 1, . [sent-161, score-0.183]
</p><p>49 σ k/(nρ(k)) ¯ ¯ The result shows that one can identify the correct set of features F as long as the weights wj are ¯ . [sent-184, score-0.154]
</p><p>50 This condition is necessary for all feature selection algorithms not close to zero when j ∈ F including previous analysis of Lasso. [sent-185, score-0.163]
</p><p>51 The theorem can be applied as long as eigenvalues of small s × s diagonal blocks of the design matrix [fiT fj ]i,j=1,. [sent-186, score-0.171]
</p><p>52 This is the situation under which the forward greedy step can make mistakes, but such mistakes can be corrected using FoBa. [sent-192, score-0.616]
</p><p>53 Because the conditions of the theorem do not prevent forward steps from making errors, the example described in Figure 2 indicates that it is not possible to prove a similar result for the forward greedy algorithm. [sent-193, score-0.849]
</p><p>54 It is known that the sparse eigenvalue condition considered here is generally weaker [8, 1]. [sent-195, score-0.154]
</p><p>55 ¯ ¯ Our result relies on the assumption that |wj | (j ∈ F ) is larger than the noise level O(σ ln d/n) in order to select features effectively. [sent-196, score-0.142]
</p><p>56 That is, in this case, one cannot reliably perform feature selection due to the noise. [sent-198, score-0.137]
</p><p>57 They show that in practice, FoBa is closer to subset selection than the other two approaches, in the sense that FoBa achieves smaller training error given any sparsity level. [sent-205, score-0.359]
</p><p>58 } means that in the ﬁst three steps, feature 1, 3, 5 are added; and the next step removes feature 3. [sent-210, score-0.159]
</p><p>59 Similar to the Lasso path, FoBa also generates a path with both addition and deletion operations, while forward-greedy algorithm only adds features without deletion. [sent-212, score-0.164]
</p><p>60 We are interested in features selected by the three algorithms at any sparsity level k, where k is the desired number of features presented in the ﬁnal solution. [sent-214, score-0.371]
</p><p>61 Given a path, we can keep an active feature set by adding or deleting features along the path. [sent-215, score-0.165]
</p><p>62 For example, for path {1, 3, 5, −3}, we have two potential active feature sets of size k = 2: {1, 3} (after two steps) and {1, 5} (after four steps). [sent-216, score-0.126]
</p><p>63 We then deﬁne the k best features as the active feature set of size k with the smallest least squares error because this is the best approximation to subset selection (along the path generated by the algorithm). [sent-217, score-0.424]
</p><p>64 Instead, we just generate a solution path which is ﬁve times as long as the maximum desired sparsity k, and then generate the best k features for any sparsity level using the above described procedure. [sent-219, score-0.502]
</p><p>65 1  Simulation Data  ¯ Since for real data, we do not know the true feature set F , simulation is needed to compare feature selection performance. [sent-221, score-0.226]
</p><p>66 The target vector ¯ ¯ w is truly sparse with k = 5 nonzero coefﬁcients generated uniformly from 0 to 10. [sent-223, score-0.185]
</p><p>67 The basis functions fj are randomly generated with moderate correlation: that is, some basis functions are correlated to the basis functions spanning the true target. [sent-226, score-0.315]
</p><p>68 , fj are independent random vectors), then both forward-greedy and L1 -regularization work well because the basis functions are near orthogonal (this is the well-known case considered in the compressed sensing literature). [sent-229, score-0.183]
</p><p>69 Such moderate correlation does not violate the sparse eigenvalue condition in our analysis, but violates the more restrictive conditions for forward-greedy method and Lasso. [sent-231, score-0.236]
</p><p>70 least squares training error parameter estimation error feature selection error  FoBa 0. [sent-232, score-0.367]
</p><p>71 77  Table 1: Performance comparison on simulation data at sparsity level k = 5 Table 1 shows the performance of the three methods (including two versions of FoBa), where we repeat the experiments 50 times, and report the average ± standard-deviation. [sent-249, score-0.218]
</p><p>72 Training error is the squared error of the least squares solution with the selected ﬁve features. [sent-252, score-0.22]
</p><p>73 It is clear from the table that for this data, FoBa achieves signiﬁcantly smaller training error than the other two methods, which implies that it is closest to subset selection. [sent-255, score-0.136]
</p><p>74 Moreover, the parameter estimation performance and feature selection performance are also better. [sent-256, score-0.137]
</p><p>75 In this study, we use the standard Boston Housing data, which is the housing data for 506 census tracts of Boston from the 1970 census, available from the UCI Machine Learning Database Repository: http://archive. [sent-262, score-0.134]
</p><p>76 Each census tract is a data-point, with 13 features (we add a constant offset one as the 14th feature), and the desired output is the housing price. [sent-266, score-0.202]
</p><p>77 We perform the experiments 50 times, and for each sparsity level from 1 to 10, we report the average training and test squared error. [sent-268, score-0.25]
</p><p>78 From the results, we can see that FoBa achieves 6  q  FoBa forward−greedy L1  q  q  50  test error  55  q  45  30  training error  40  60  50  FoBa forward−greedy L1  65  60  q q  70  better training error for any given sparsity, which is consistent with the theory and the design goal of FoBa. [sent-270, score-0.239]
</p><p>79 Moreover, it achieves better test accuracy with small sparsity level (corresponding to a more sparse solution). [sent-271, score-0.316]
</p><p>80 With large sparsity level (corresponding to a less sparse solution), the test error increase more quickly with FoBa. [sent-272, score-0.335]
</p><p>81 However, at the best sparsity level of 2 or 3 (for aggressive and conservative FoBa, respectively), FoBa achieves signiﬁcantly better test error. [sent-274, score-0.31]
</p><p>82 Moreover, we can observe with small sparsity level (a more sparse solution), L1 regularization performs poorly, due to the bias caused by using a large L1 -penalty. [sent-275, score-0.337]
</p><p>83 As we have pointed out, there is no theory for the SAS version of forward-backward greedy algorithm. [sent-278, score-0.276]
</p><p>84 It is difﬁcult to select an appropriate backward threshold : a too small value leads to few backward steps, and a too large value leads to overly aggressive deletion, and the procedure terminates very early. [sent-279, score-0.79]
</p><p>85 From the results, we can see that backward greedy algorithm performs reasonably well on this problem. [sent-282, score-0.638]
</p><p>86 Note that for this data, d n, which is the scenario that backward does not start with a completely overﬁtted full model. [sent-283, score-0.363]
</p><p>87 Still, it is inferior to FoBa at small sparsity level, which means that some degree of overﬁtting still occurs. [sent-284, score-0.154]
</p><p>88 From the graph, we also see that FoBa is more effective than the SAS implementation of forward-backward greedy algorithm. [sent-286, score-0.276]
</p><p>89 Unfortunately, using a larger backward threshold will lead to an undesirable early termination of the algorithm. [sent-288, score-0.391]
</p><p>90 This is why the provably effective adaptive backward strategies introduced in this paper are superior. [sent-289, score-0.398]
</p><p>91 5  Discussion  This paper investigates the problem of learning sparse representations using greedy algorithms. [sent-290, score-0.354]
</p><p>92 We showed that neither forward greedy nor backward greedy algorithms are adequate by themselves. [sent-291, score-1.203]
</p><p>93 However, through a novel combination of the two ideas, we showed that an adaptive forward-back greedy algorithm, referred to as FoBa, can effectively solve the problem under reasonable conditions. [sent-292, score-0.348]
</p><p>94 Under the sparse eigenvalue condition, we obtained strong performance bounds for FoBa for feature selection and parameter estimation. [sent-294, score-0.265]
</p><p>95 Left: average training squared error versus sparsity; Right: average test squared error versus sparsity Our experiments also showed that FoBa achieves its design goal: that is, it gives smaller training error than either forward-greedy or L1 regularization for any given level of sparsity. [sent-297, score-0.554]
</p><p>96 In real data, better sparsity helps on some data such as Boston Housing. [sent-299, score-0.154]
</p><p>97 However, we shall point out that while FoBa always achieves better training error for a given sparsity in our experiments on other datasets (thus it achieves our design goal), L1 regularization some times achieves better test performance. [sent-300, score-0.459]
</p><p>98 This is not surprising because sparsity is not always the best complexity measure for all problems. [sent-301, score-0.154]
</p><p>99 In particular, the prior knowledge of using small weights, which is encoded in the L1 regularization formulation but not in greedy algorithms, can lead to better generalization performance on some data (when such a prior is appropriate). [sent-302, score-0.338]
</p><p>100 On the optimality of the backward greedy algorithm for the subset selection problem. [sent-312, score-0.718]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('foba', 0.703), ('backward', 0.341), ('greedy', 0.276), ('forward', 0.261), ('sparsity', 0.154), ('fj', 0.137), ('sas', 0.117), ('wj', 0.086), ('housing', 0.083), ('sparse', 0.078), ('selection', 0.069), ('feature', 0.068), ('boston', 0.068), ('features', 0.068), ('rd', 0.065), ('lasso', 0.064), ('squares', 0.062), ('regularization', 0.062), ('path', 0.058), ('supp', 0.055), ('steps', 0.051), ('census', 0.051), ('aggressive', 0.051), ('eigenvalue', 0.05), ('basis', 0.046), ('eyi', 0.044), ('level', 0.043), ('restrictive', 0.042), ('achieves', 0.041), ('moderate', 0.04), ('aws', 0.039), ('nonzero', 0.039), ('deletion', 0.038), ('ej', 0.038), ('remedy', 0.038), ('interested', 0.038), ('error', 0.038), ('terminates', 0.035), ('heuristics', 0.035), ('target', 0.035), ('effectiveness', 0.034), ('irrepresentable', 0.034), ('design', 0.034), ('truly', 0.033), ('mistakes', 0.032), ('subset', 0.032), ('ln', 0.031), ('fit', 0.031), ('alexandre', 0.029), ('deleting', 0.029), ('inadequate', 0.029), ('minj', 0.029), ('break', 0.029), ('provably', 0.029), ('least', 0.029), ('squared', 0.028), ('adaptive', 0.028), ('termination', 0.028), ('adequate', 0.028), ('aggressively', 0.028), ('widely', 0.028), ('remove', 0.027), ('cients', 0.027), ('cost', 0.027), ('fd', 0.026), ('rutgers', 0.026), ('shrinks', 0.026), ('condition', 0.026), ('rn', 0.025), ('solution', 0.025), ('let', 0.025), ('training', 0.025), ('corrected', 0.024), ('mini', 0.024), ('minw', 0.024), ('arg', 0.024), ('wd', 0.024), ('away', 0.023), ('wt', 0.023), ('moreover', 0.023), ('tong', 0.023), ('step', 0.023), ('shall', 0.023), ('completely', 0.022), ('pick', 0.022), ('adaptively', 0.022), ('reconstructing', 0.022), ('greedily', 0.022), ('solve', 0.022), ('coef', 0.022), ('threshold', 0.022), ('increase', 0.022), ('earlier', 0.022), ('combination', 0.022), ('good', 0.021), ('reasonably', 0.021), ('simulation', 0.021), ('conservative', 0.021), ('neither', 0.021), ('idea', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="14-tfidf-1" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>2 0.19865537 <a title="14-tfidf-2" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>3 0.17035225 <a title="14-tfidf-3" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>4 0.13959102 <a title="14-tfidf-4" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><p>5 0.10062741 <a title="14-tfidf-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.098253384 <a title="14-tfidf-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.089781016 <a title="14-tfidf-7" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>8 0.087694213 <a title="14-tfidf-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.072143964 <a title="14-tfidf-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.071727119 <a title="14-tfidf-10" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>11 0.067918189 <a title="14-tfidf-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.066273041 <a title="14-tfidf-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.063840151 <a title="14-tfidf-13" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>14 0.06302125 <a title="14-tfidf-14" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>15 0.059667539 <a title="14-tfidf-15" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>16 0.051095653 <a title="14-tfidf-16" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>17 0.050849468 <a title="14-tfidf-17" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>18 0.050597381 <a title="14-tfidf-18" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>19 0.048349097 <a title="14-tfidf-19" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>20 0.048334707 <a title="14-tfidf-20" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.169), (1, -0.018), (2, -0.096), (3, 0.08), (4, 0.077), (5, 0.022), (6, -0.113), (7, 0.003), (8, -0.076), (9, 0.087), (10, -0.097), (11, -0.019), (12, -0.091), (13, -0.082), (14, 0.024), (15, -0.052), (16, -0.023), (17, -0.086), (18, -0.049), (19, 0.009), (20, -0.007), (21, 0.109), (22, 0.028), (23, -0.056), (24, 0.059), (25, -0.068), (26, 0.087), (27, -0.037), (28, -0.101), (29, 0.032), (30, 0.088), (31, -0.018), (32, 0.086), (33, 0.134), (34, -0.025), (35, -0.002), (36, 0.047), (37, -0.046), (38, 0.076), (39, 0.071), (40, -0.056), (41, -0.075), (42, -0.03), (43, -0.046), (44, -0.09), (45, 0.027), (46, -0.148), (47, -0.064), (48, 0.024), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92604578 <a title="14-lsi-1" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>2 0.67496127 <a title="14-lsi-2" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>3 0.66079873 <a title="14-lsi-3" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><p>4 0.61927104 <a title="14-lsi-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.57155782 <a title="14-lsi-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.55487639 <a title="14-lsi-6" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>7 0.53433675 <a title="14-lsi-7" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>8 0.52446997 <a title="14-lsi-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.4712424 <a title="14-lsi-9" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>10 0.433148 <a title="14-lsi-10" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>11 0.43198785 <a title="14-lsi-11" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>12 0.39929444 <a title="14-lsi-12" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>13 0.39562982 <a title="14-lsi-13" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>14 0.39159477 <a title="14-lsi-14" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>15 0.38438842 <a title="14-lsi-15" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>16 0.35405195 <a title="14-lsi-16" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>17 0.35173091 <a title="14-lsi-17" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>18 0.35095868 <a title="14-lsi-18" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>19 0.34705672 <a title="14-lsi-19" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>20 0.34576511 <a title="14-lsi-20" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.118), (7, 0.063), (12, 0.026), (16, 0.081), (28, 0.16), (57, 0.046), (59, 0.022), (63, 0.039), (71, 0.027), (77, 0.042), (83, 0.106), (90, 0.161)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84365851 <a title="14-lda-1" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>2 0.80862546 <a title="14-lda-2" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>3 0.79447275 <a title="14-lda-3" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>Author: Paul L. Ruvolo, Ian Fasel, Javier R. Movellan</p><p>Abstract: Many popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA), use heuristic-based “controllers” that modulate the behavior of the optimizer during the optimization process. For example, in the LMA a damping parameter λ is dynamically modiﬁed based on a set of rules that were developed using heuristic arguments. Reinforcement learning (RL) is a machine learning approach to learn optimal controllers from examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms. Improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems. For example the LMA algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame. Here we show that a popular modern reinforcement learning technique using a very simple state space can dramatically improve the performance of general purpose optimizers, like the LMA. Surprisingly the controllers learned for a particular domain also work well in very different optimization domains. For example we used RL methods to train a new controller for the damping parameter of the LMA. This controller was trained on a collection of classic, relatively small, non-linear regression problems. The modiﬁed LMA performed better than the standard LMA on these problems. This controller also dramatically outperformed the standard LMA on a difﬁcult computer vision problem for which it had not been trained. Thus the controller appeared to have extracted control rules that were not just domain speciﬁc but generalized across a range of optimization domains. 1</p><p>4 0.7933622 <a title="14-lda-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.78925687 <a title="14-lda-5" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>6 0.78279692 <a title="14-lda-6" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>7 0.77799439 <a title="14-lda-7" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>8 0.77269 <a title="14-lda-8" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>9 0.77184725 <a title="14-lda-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.7717185 <a title="14-lda-10" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>11 0.7694906 <a title="14-lda-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.76655096 <a title="14-lda-12" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>13 0.76549888 <a title="14-lda-13" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>14 0.7652148 <a title="14-lda-14" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>15 0.76502842 <a title="14-lda-15" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>16 0.76377565 <a title="14-lda-16" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>17 0.76306677 <a title="14-lda-17" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>18 0.76244926 <a title="14-lda-18" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>19 0.76209247 <a title="14-lda-19" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>20 0.75988024 <a title="14-lda-20" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
