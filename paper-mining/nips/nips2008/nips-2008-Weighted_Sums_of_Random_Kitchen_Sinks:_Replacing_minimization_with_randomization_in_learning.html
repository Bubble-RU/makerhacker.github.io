<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-250" href="#">nips2008-250</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</h1>
<br/><p>Source: <a title="nips-2008-250-pdf" href="http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf">pdf</a></p><p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>Reference: <a title="nips-2008-250-reference" href="../nips2008_reference/nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adaboost', 0.452), ('fw', 0.344), ('fp', 0.313), ('rks', 0.295), ('sink', 0.247), ('remp', 0.242), ('kitch', 0.236), ('wk', 0.219), ('yy', 0.139), ('lemm', 0.139), ('sussm', 0.134), ('rademach', 0.133), ('iid', 0.133), ('lc', 0.114), ('risk', 0.106), ('bound', 0.088), ('shallow', 0.087), ('tting', 0.069), ('nonlinear', 0.067), ('stump', 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="250-tfidf-1" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>2 0.28693289 <a title="250-tfidf-2" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>3 0.25837564 <a title="250-tfidf-3" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>4 0.13575545 <a title="250-tfidf-4" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>5 0.12031527 <a title="250-tfidf-5" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>6 0.11845972 <a title="250-tfidf-6" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>7 0.093498774 <a title="250-tfidf-7" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>8 0.082560614 <a title="250-tfidf-8" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>9 0.07265947 <a title="250-tfidf-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.071236275 <a title="250-tfidf-10" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>11 0.066975676 <a title="250-tfidf-11" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>12 0.064666644 <a title="250-tfidf-12" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>13 0.064643897 <a title="250-tfidf-13" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>14 0.063480884 <a title="250-tfidf-14" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>15 0.061922301 <a title="250-tfidf-15" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>16 0.0613074 <a title="250-tfidf-16" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>17 0.06097563 <a title="250-tfidf-17" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>18 0.060736947 <a title="250-tfidf-18" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>19 0.060712483 <a title="250-tfidf-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.060316753 <a title="250-tfidf-20" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, 0.048), (2, 0.085), (3, -0.095), (4, -0.005), (5, -0.046), (6, -0.063), (7, 0.047), (8, -0.044), (9, 0.016), (10, -0.001), (11, 0.185), (12, -0.005), (13, 0.122), (14, -0.056), (15, -0.027), (16, 0.046), (17, 0.007), (18, 0.084), (19, 0.1), (20, 0.078), (21, -0.054), (22, -0.16), (23, 0.022), (24, -0.091), (25, -0.281), (26, 0.052), (27, -0.034), (28, -0.106), (29, 0.147), (30, 0.004), (31, 0.189), (32, -0.05), (33, -0.087), (34, 0.116), (35, -0.017), (36, -0.112), (37, -0.023), (38, 0.088), (39, -0.159), (40, 0.028), (41, 0.04), (42, 0.002), (43, 0.036), (44, 0.043), (45, -0.017), (46, -0.159), (47, -0.068), (48, -0.084), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8775903 <a title="250-lsi-1" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>2 0.85924882 <a title="250-lsi-2" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>3 0.61588991 <a title="250-lsi-3" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>4 0.5948633 <a title="250-lsi-4" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>5 0.55655795 <a title="250-lsi-5" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>6 0.47952402 <a title="250-lsi-6" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>7 0.47223148 <a title="250-lsi-7" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>8 0.45322672 <a title="250-lsi-8" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>9 0.43815583 <a title="250-lsi-9" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>10 0.41084394 <a title="250-lsi-10" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>11 0.40140197 <a title="250-lsi-11" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>12 0.39935571 <a title="250-lsi-12" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>13 0.39748794 <a title="250-lsi-13" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>14 0.3886151 <a title="250-lsi-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.37298101 <a title="250-lsi-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.37169975 <a title="250-lsi-16" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>17 0.36640382 <a title="250-lsi-17" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>18 0.3471739 <a title="250-lsi-18" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>19 0.32014689 <a title="250-lsi-19" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>20 0.31532732 <a title="250-lsi-20" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.018), (30, 0.54), (38, 0.042), (40, 0.056), (60, 0.01), (63, 0.079), (64, 0.051), (71, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98297536 <a title="250-lda-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>2 0.9714914 <a title="250-lda-2" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>same-paper 3 0.94048983 <a title="250-lda-3" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>4 0.93401504 <a title="250-lda-4" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>5 0.87749249 <a title="250-lda-5" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>6 0.87644637 <a title="250-lda-6" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>7 0.85464972 <a title="250-lda-7" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>8 0.82148266 <a title="250-lda-8" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>9 0.81973946 <a title="250-lda-9" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>10 0.80302411 <a title="250-lda-10" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>11 0.79977733 <a title="250-lda-11" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>12 0.78658026 <a title="250-lda-12" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>13 0.78605759 <a title="250-lda-13" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>14 0.78252232 <a title="250-lda-14" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>15 0.76593834 <a title="250-lda-15" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>16 0.75272089 <a title="250-lda-16" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>17 0.74931669 <a title="250-lda-17" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>18 0.74235666 <a title="250-lda-18" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>19 0.74134642 <a title="250-lda-19" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>20 0.73376715 <a title="250-lda-20" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
