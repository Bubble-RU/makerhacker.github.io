<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-250" href="#">nips2008-250</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</h1>
<br/><p>Source: <a title="nips-2008-250-pdf" href="http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf">pdf</a></p><p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>Reference: <a title="nips-2008-250-reference" href="../nips2008_reference/nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning  Paper #858  Abstract Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. [sent-1, score-0.088]
</p><p>2 “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. [sent-4, score-0.145]
</p><p>3 “So that the room will be empty,” replied Minsky. [sent-11, score-0.05]
</p><p>4 We analyze shallow random networks with the help of concentration of measure inequalities. [sent-13, score-0.173]
</p><p>5 Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. [sent-14, score-0.141]
</p><p>6 We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. [sent-15, score-0.198]
</p><p>7 1  Introduction  In the earliest days of artiﬁcial intelligence, the bottom-most layer of neural networks consisted of randomly connected “associator units” that computed random binary functions of their inputs [1]. [sent-16, score-0.15]
</p><p>8 These randomized shallow networks have largely been superceded by optimally, or nearly optimally, tuned shallow architectures such as weighted sums of positive deﬁnite kernels (as in Support Vector Machines), or weigted sums of weak classiﬁers (as in Adaboost). [sent-17, score-0.492]
</p><p>9 But recently, architectures that randomly transform their inputs have been resurfacing in the machine learning community [2, 3, 4, 5], largely motivated by the fact that randomization is computationally cheaper than optimization. [sent-18, score-0.132]
</p><p>10 With the help of concentration of measure inequalities on function spaces, we show that training a shallow architecture by randomly choosing the nonlinearities in the ﬁrst layer results in a classiﬁer that is not much worse than one constructed by optimally tuning the nonlinearities. [sent-19, score-0.358]
</p><p>11 The main technical contributions of the paper are an approximation error bound (Lemma 1), and a synthesis of known techniques from learning theory to analyze random shallow networks. [sent-20, score-0.264]
</p><p>12 Consider the problem of ﬁtting a function f : X → R to a training data set of m input-output pairs {xi , yi }i=1. [sent-21, score-0.077]
</p><p>13 m , drawn iid from some unknown distribution P (x, y), with xi ∈ X and yi ∈ ±1. [sent-24, score-0.241]
</p><p>14 The ﬁtting problem consists of ﬁnding an f that minimizes the empirical risk Remp [f ] ≡  1 m  m  c(f (xi ), yi ). [sent-25, score-0.135]
</p><p>15 Popular choices for c are the hinge loss, max(0, 1 − yy ), used in the Support Vector Machine [6], the exponential loss, e−yy , used in Adaboost [7, 8], and the quadratic loss, (y − y )2 , used in matching pursuit [9] and regularized least squares classiﬁcation [10]. [sent-27, score-0.284]
</p><p>16 1  Similarly to kernel machines and Adaboost, we will consider functions of the form f (x) = ∞ α(w)φ(x; w) dw, where feature functions φ : X × Ω → R, i=1 α(wi )φ(x; wi ) or f (x) = parameterized by some vector w ∈ Ω, are weighted by a function α : Ω → R. [sent-28, score-0.169]
</p><p>17 In kernel machines, the feature functions φ are the eigenfunctions of a positive deﬁnite kernel k, and in Adaboost they are typically decision trees or stumps. [sent-29, score-0.068]
</p><p>18 Adaboost [8, 7] and matching pursuit [11, 9] ﬁnd approximate empirical risk minimizer over this class of functions by greedily minimizing over a ﬁnite number of scalar weights α and parameter vectors w jointly: K  minimize Remp w1 , . [sent-30, score-0.301]
</p><p>19 Rather than jointly optimizing over α and w, the following algorithm ﬁrst draws the parameters of the nonlinearities randomly from a pre-speciﬁcied distribution p. [sent-35, score-0.157]
</p><p>20 m of m points, a bounded feature function |φ(x; w)| ≤ 1, an integer K, a scalar C, and a probability distribution p(w) on the parameters of φ. [sent-40, score-0.111]
</p><p>21 With w ﬁxed, solve the empirical risk minimization problem minimize α∈RK  s. [sent-50, score-0.099]
</p><p>22 The when c is the quadratic loss, the minimization (3) is simple linear least squares, and when c is the hinge loss, it amounts of ﬁtting a linear SVM to a dataset of m K-dimensional feature vectors. [sent-54, score-0.137]
</p><p>23 Randomly setting the nonlinearities is appealing for several reasons. [sent-55, score-0.13]
</p><p>24 First, the ﬁtting procedure is simple: Algorithm 1 can be implemented in a few lines of MATLAB code even for complex feature functions φ, whereas ﬁtting nonlinearities with Adaboost requires much more care. [sent-56, score-0.198]
</p><p>25 The true risk of a function f is R[f ] ≡ E c(f (x), y), (5) (x,y)∼P  and measures the expected loss of f on as-yet-unseen test points, assuming these test points are generated from the same distribution that generated the training data. [sent-62, score-0.179]
</p><p>26 The following theorem states that with very high probability, Algorithm 1 returns a function whose true risk is near the lowest true risk attainable by functions in the class Fp deﬁned below: Theorem 1 (Main result). [sent-63, score-0.426]
</p><p>27 Deﬁne the set Fp ≡  α(w)φ(x; w) dw |α(w)| ≤ Cp(w) . [sent-65, score-0.051]
</p><p>28 Then for any δ > 0, if the training data ˆ {xi , yi }i=1. [sent-67, score-0.077]
</p><p>29 m are drawn iid from some distribution P , Algorithm 1 returns a function f that satisﬁes 1 1 ˆ √ +√ R[f ] − min R[f ] ≤ O LC log 1 (7) δ f ∈Fp m K with probability at least 1 − 2δ over the training dataset and the choice of the parameters w1 , . [sent-70, score-0.323]
</p><p>30 Note that the dependence on δ in the bound is logarithmic, so even small δ’s do not cause the bound to blow up. [sent-74, score-0.108]
</p><p>31 It consists of functions whose weights α(w) decays more rapidly than the given sampling distribution p. [sent-76, score-0.105]
</p><p>32 For example, when φ(x; w) are sinusoids with frequency w, Fp is the set of all functions whose Fourier transforms decay faster than C p(w). [sent-77, score-0.086]
</p><p>33 We prove the theorem in the next section, and demonstrate the algorithm on some sample datasets in Section 4. [sent-78, score-0.047]
</p><p>34 2  Proof of the Main Theorem  Algorithm 1 returns a function that lies in the random set K  ˆ Fw ≡  αk φ(x; wk ) |αk | ≤  f (x) =  C K  . [sent-80, score-0.268]
</p><p>35 (8)  k=1  The bound in the main theorem can be decomposed in a standard way into two bounds: 1. [sent-81, score-0.101]
</p><p>36 An approximation error bound that shows that the lowest true risk attainable by a function ˆ in Fw is not much larger than the lowest true risk attainable in Fp (Lemma 2). [sent-82, score-0.558]
</p><p>37 An estimation error bound that shows that the true risk of every function in Fw is close to its empirical risk (Lemma 3). [sent-84, score-0.301]
</p><p>38 The following Lemma is helpful in bounding the approximation error: Lemma 1. [sent-85, score-0.049]
</p><p>39 , wK are drawn iid from p, ˆ ˆ then for any δ > 0, with probability at least 1 − δ over w1 , . [sent-90, score-0.185]
</p><p>40 (9)  The proof relies on Lemma 4 of the Appendix, which states that the average of bounded vectors in a Hilbert space concentrates towards its expectation in the Hilbert norm exponentially fast. [sent-94, score-0.068]
</p><p>41 Construct the functions fk = K k) ˆ βk φ(·; wk ), k = 1 . [sent-97, score-0.283]
</p><p>42 Also, under the inner product f, g = f (x)g(x) dµ(x), βk φ(·; wk ) ≤ C. [sent-103, score-0.204]
</p><p>43 , wK are drawn iid from p, then for any δ > 0, with ˆ ˆ probability at least 1 − δ over w1 , . [sent-114, score-0.185]
</p><p>44 For any two functions f and g, the Lipschitz condition on c followed by the concavity of square root gives R[f ] − R[g] = E c(f (x), y) − c(g(x), y) ≤ E |c(f (x), y) − c(g(x), y)| ≤ L E |f (x) − g(x)| ≤ L E(f (x) −  g(x))2 . [sent-119, score-0.084]
</p><p>45 , wK the empirical risk of every function in Fw is close to its true risk. [sent-124, score-0.099]
</p><p>46 m are drawn iid from a ﬁxed distribution, for any δ > 0, with probability at least 1 − δ over the dataset, we have ∀f ∈Fw ˆ  1 |R[f ] − Remp [f ]| ≤ √ m  4LC + 2|c(0)| + LC  1 2  log  1 δ  . [sent-131, score-0.208]
</p><p>47 By H¨ lder, the functions in Fw are bounded above by C. [sent-133, score-0.081]
</p><p>48 The Rademacher complexity o √ ˆ of Fw can be shown to be bounded above by C/ m (see the Appendix). [sent-134, score-0.065]
</p><p>49 Let f ∗ be a minimizer of R over Fp , f a minimizer of Remp over Fw (the ˆ∗ a minimizer of R over Fw . [sent-137, score-0.186]
</p><p>50 (14) (15)  The ﬁrst term in the right side is an estimation error: By Lemma 3, with probability at least ˆ ˆ ˆ ˆ 1 − δ, |R[f ∗ ] − Remp [f ∗ ]| ≤ est and simultaneously, |R[f ] − Remp [f ]| ≤ est , where est ˆ ˆ ˆ is the right side of the bound in Lemma 3. [sent-139, score-0.457]
</p><p>51 ˆ] − R[f ∗ ]| ≤ 2 est = ˆ Combining these facts gives that with probability at least 1 − δ, |R[f 2 √ m  4LC + 2|c(0)| + LC  1 2  log  1 δ  . [sent-141, score-0.16]
</p><p>52 The second term in Equation (15) is the approximation error, and by Theorem 1, with probability at least 1 − δ, it is bounded above by  app  =  LC √ K  1+  2 log  1 δ  . [sent-142, score-0.19]
</p><p>53 By the union bound, with probability at least 1−2δ, the right side of Equation (15) is bounded above by 2 est + app . [sent-143, score-0.251]
</p><p>54 Zhang analyzed greedy algorithms and a randomized algorithm similar to Algorithm 1 for ﬁtting sparse Gaussian processes to data, a more narrow setting than we consider here. [sent-145, score-0.093]
</p><p>55 He obtained bounds on the expected error for this sparse approximation problem by viewing these methods as stochastic gradient descent. [sent-146, score-0.175]
</p><p>56 Approximation error bounds such as that of Maurey [11][Lemma 1], Girosi [13] and Gnecco and Sanguineti [14] rely on random sampling to guarantee the existence of good parameters w1 , . [sent-147, score-0.205]
</p><p>57 , wk , but they require access to the representation of f ∗ to actually produce these parameters. [sent-150, score-0.204]
</p><p>58 These approximation bounds cannot be used to guarantee the performance of Algorithm 1 because Algorithm 1 is oblivious of the data when it generates the parameters. [sent-151, score-0.194]
</p><p>59 Lemma 2 differs from these bounds in that it relies on f ∗ only to generate the weights α1 , . [sent-152, score-0.077]
</p><p>60 , αK , but it remains oblivious to f ∗ when ˆ generating the parameters by sampling them from p instead. [sent-155, score-0.072]
</p><p>61 Furthermore, because Fw is smaller than the classes considered by [11, 14], the approximation error rate in Lemma 1 matches those of existing approximation error bounds. [sent-156, score-0.196]
</p><p>62 The ﬁrst column plots test error of each classiﬁer as a function of K. [sent-158, score-0.049]
</p><p>63 The second column plots the total training and testing time as a function of K. [sent-160, score-0.091]
</p><p>64 For a given K, Random Kitchen Sinks is between two and three orders of magnitude faster than Adaboost. [sent-161, score-0.085]
</p><p>65 It plots testing+training time required to achieve a desired error rate. [sent-163, score-0.049]
</p><p>66 For a given error rate, Random Kitchen Sinks is between one and three orders of magnitude faster than Adaboost. [sent-164, score-0.134]
</p><p>67 We compared Random Kitchen Sinks with Adaboost on three classiﬁcation problems: The adult dataset has roughly 32,000 training instances. [sent-166, score-0.131]
</p><p>68 activity is a human activity recognition dataset with 20,0000 223-dimensional instance, of which about 200 are irrelevant for classiﬁcation. [sent-171, score-0.083]
</p><p>69 The feature functions in these experiments were decision stumpsφ(x; w) = sign(xwd − wt ), which simply determine whether the wd th dimension of x is smaller or greater than the threshold wt . [sent-174, score-0.158]
</p><p>70 The sampling distribution p for Random Kitchen Sinks drew the threshold parameter wt from a normal distribution and the coordinate wd from a uniform distribution over the coorindates. [sent-175, score-0.088]
</p><p>71 We used the quadratic loss, but ﬁnd no substantial differences in quality under the hinge loss (though there is degradation in speed by a factor of 2-10). [sent-177, score-0.074]
</p><p>72 Adaboost expends considerable effort in choosing the decision stumps and achieves good test accuracy with a few of them. [sent-180, score-0.06]
</p><p>73 α ∞ decays with K, which justiﬁes dropping the constraint (4) in practice. [sent-188, score-0.101]
</p><p>74 But because it is faster than Adaboost, it can produce classiﬁers that are just as accurate as Adaboost’s with more nonlinearities in less total time. [sent-190, score-0.176]
</p><p>75 In these experiments, Random Kitchen Sinks is almost as accurate as Adaboost but faster by one to three orders of magnitude. [sent-191, score-0.085]
</p><p>76 We obtain similar results as those of Figure 1 with the random features of [4], and random sigmoidal ridge functions φ(x; w) = σ(w x), To simplify the implementation of Random Kitchen Sinks, we ignore the constraint (4) in practice. [sent-193, score-0.171]
</p><p>77 ˆ The scalar C controls the size of Fw and Fp , and to eliminate the constraint, we implicitly set C it to a large value so that the constraint is never tight. [sent-194, score-0.079]
</p><p>78 Figure 2 shows that the L∞ norm of the unconstrained optimum of (3) for the adult dataset does decays linearly with K, so that there exists a C that does not grow with K for which the constraint is never tight, thereby justifying dropping the constraint. [sent-196, score-0.191]
</p><p>79 5  Discussion and Conclusions  Various hardness of approximation lower bounds for ﬁxed basis functions exist (see, for example [11]). [sent-197, score-0.166]
</p><p>80 The guarantee in Lemma 1 avoids running afoul of these lower bounds because it does not seek to approximate every function in Fp simultaneously, but rather only the true risk minimizer with high probability. [sent-198, score-0.262]
</p><p>81 It may be surprising that Theorem 1 holds even when the feature functions φ are nearly orthogonal. [sent-199, score-0.068]
</p><p>82 The result works because the importance sampling constraint |α(w)| ≤ Cp(w) ensures that a feature function does not receive a large weight if it is unlikely to be sampled by p. [sent-200, score-0.093]
</p><p>83 When the feature functions are highly linearly dependent, better bounds can be obtained because any f (x) = α(w)φ(x; w) can be rewritten as f (x) = α (w)φ(x; w) with |α |/p ≤ |α|/p, improving the importance ratio C. [sent-201, score-0.145]
</p><p>84 Indeed, when φ are the Fourier bases, |α|/p ≤ C implies |α(w)| dw ≤ C, so every function in Fp has an absolutely integrable Frourier transform. [sent-204, score-0.051]
</p><p>85 Thus Ω Fp is smaller than the set considered by Jones [9] for greedy matching pursuit, and for which he √ obtained an approximation rate of O(1/ K). [sent-205, score-0.08]
</p><p>86 The convergence rate for Adaboost [7] is exponentially fast in K, which at ﬁrst appears to be much √ faster than 1/ K. [sent-208, score-0.073]
</p><p>87 However, the base of the exponent is the minimum weighted margin encountered by the algorithm through all iterations, a quantity that is difﬁcult to bound a priori. [sent-209, score-0.084]
</p><p>88 This makes a direct comparison of the bounds difﬁcult, though we have tried to provide empirical comparisons. [sent-210, score-0.077]
</p><p>89 Let X = {x1 , · · · , xK } be iid random variables in a ball H of radius M centered K 1 around the origin in a Hilbert space. [sent-212, score-0.151]
</p><p>90 Then for any δ > 0, with probability at least 1 − δ, M X − EX ≤ √ K  1+  2 log  1 δ  . [sent-214, score-0.06]
</p><p>91 We use McDiarmid’s inequality to show that the scalar function f (X) = X − EX X is √ concentrated about its mean, which shrinks as O(1/ K). [sent-216, score-0.079]
</p><p>92 Let X = {x1 , · · · , xi , · · · , xK } ˜ be a copy of X with the ith element replaced by an arbitrary element of H. [sent-218, score-0.057]
</p><p>93 Applying the triangle inequality twice gives xi − xi ˜ 2M ˜ ˜ ˜ ≤ . [sent-219, score-0.151]
</p><p>94 K  (19)  This bound for the expectation of f and McDiarmid’s inequality give M Pr f (X) − √ ≥ X K  ≤ Pr f (X) − E f (X) ≥ X  ≤ exp −  K 2 2M 2  (20)  To get the ﬁnal result, set δ to the right hand side, solve for , and rearrange. [sent-221, score-0.091]
</p><p>95 ˆ The Rademacher complexity of Fw can be bounded as follows. [sent-226, score-0.065]
</p><p>96 Let F be a class of bounded functions so that supx |f (x)| ≤ C for all f ∈ F, and suppose c(y, y ) = c(yy ), with c(yy ) L-Lipschitz. [sent-229, score-0.081]
</p><p>97 Then with probability at least 1 − δ with respect to training samples {xi , yi }m drawn from a probabilisty distribution P on X × {−1, +1}, every function in F satisﬁes 2|c(0)| R[f ] ≤ Remp [f ] + 4LRm [F] + √ + LC m  1 log 1 . [sent-230, score-0.161]
</p><p>98 A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. [sent-286, score-0.268]
</p><p>99 Universal approximation bounds for superpositions of a sigmoidal function. [sent-297, score-0.166]
</p><p>100 Rademacher and Gaussian complexities: Risk bounds and structural results. [sent-303, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adaboost', 0.406), ('fw', 0.321), ('fp', 0.292), ('sinks', 0.285), ('rks', 0.276), ('remp', 0.226), ('kitchen', 0.22), ('wk', 0.204), ('nonlinearities', 0.13), ('yy', 0.13), ('lemma', 0.13), ('sussman', 0.125), ('rademacher', 0.124), ('iid', 0.124), ('lc', 0.107), ('est', 0.1), ('risk', 0.099), ('shallow', 0.085), ('bounds', 0.077), ('attainable', 0.075), ('learners', 0.075), ('ex', 0.071), ('tting', 0.066), ('minimizer', 0.062), ('randomized', 0.062), ('weak', 0.062), ('stumps', 0.06), ('pursuit', 0.058), ('xi', 0.057), ('seconds', 0.055), ('bound', 0.054), ('adult', 0.053), ('dw', 0.051), ('gnecco', 0.05), ('replied', 0.05), ('wired', 0.05), ('testing', 0.05), ('approximation', 0.049), ('hilbert', 0.049), ('architectures', 0.049), ('error', 0.049), ('theorem', 0.047), ('faster', 0.046), ('optimally', 0.045), ('oblivious', 0.044), ('minsky', 0.044), ('concavity', 0.044), ('scalar', 0.042), ('bounded', 0.041), ('training', 0.041), ('app', 0.04), ('sigmoidal', 0.04), ('functions', 0.04), ('fk', 0.039), ('orders', 0.039), ('loss', 0.039), ('cp', 0.038), ('mcdiarmid', 0.038), ('decays', 0.037), ('returns', 0.037), ('dataset', 0.037), ('constraint', 0.037), ('least', 0.037), ('inequality', 0.037), ('yi', 0.036), ('asked', 0.036), ('hinge', 0.035), ('classi', 0.035), ('lder', 0.034), ('side', 0.033), ('randomization', 0.032), ('sums', 0.032), ('networks', 0.031), ('greedy', 0.031), ('machines', 0.031), ('wd', 0.03), ('weighted', 0.03), ('wt', 0.03), ('concentration', 0.03), ('lowest', 0.029), ('feature', 0.028), ('sampling', 0.028), ('xk', 0.028), ('randomly', 0.027), ('random', 0.027), ('net', 0.027), ('dropping', 0.027), ('exponentially', 0.027), ('sup', 0.027), ('days', 0.025), ('fourier', 0.025), ('squares', 0.024), ('complexity', 0.024), ('largely', 0.024), ('guarantee', 0.024), ('drawn', 0.024), ('sciences', 0.023), ('activity', 0.023), ('log', 0.023), ('rk', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="250-tfidf-1" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>2 0.26790547 <a title="250-tfidf-2" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><p>3 0.23064879 <a title="250-tfidf-3" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>4 0.1375373 <a title="250-tfidf-4" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>5 0.10723001 <a title="250-tfidf-5" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>6 0.10641775 <a title="250-tfidf-6" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>7 0.098161653 <a title="250-tfidf-7" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>8 0.077464342 <a title="250-tfidf-8" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>9 0.069402575 <a title="250-tfidf-9" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>10 0.066901036 <a title="250-tfidf-10" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>11 0.066421606 <a title="250-tfidf-11" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>12 0.06583333 <a title="250-tfidf-12" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>13 0.065300085 <a title="250-tfidf-13" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>14 0.065031983 <a title="250-tfidf-14" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>15 0.063359961 <a title="250-tfidf-15" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>16 0.062597826 <a title="250-tfidf-16" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>17 0.061009642 <a title="250-tfidf-17" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>18 0.060823262 <a title="250-tfidf-18" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>19 0.059030235 <a title="250-tfidf-19" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>20 0.058673926 <a title="250-tfidf-20" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.197), (1, -0.019), (2, -0.146), (3, 0.038), (4, -0.097), (5, 0.056), (6, -0.007), (7, -0.062), (8, -0.003), (9, 0.035), (10, 0.095), (11, 0.153), (12, 0.151), (13, -0.014), (14, -0.077), (15, 0.109), (16, 0.068), (17, -0.072), (18, 0.012), (19, -0.014), (20, -0.068), (21, 0.029), (22, 0.126), (23, -0.151), (24, 0.136), (25, 0.225), (26, 0.015), (27, -0.068), (28, -0.142), (29, -0.105), (30, 0.194), (31, 0.082), (32, 0.033), (33, 0.133), (34, -0.021), (35, 0.02), (36, 0.052), (37, -0.033), (38, 0.004), (39, 0.068), (40, 0.102), (41, -0.05), (42, -0.088), (43, -0.089), (44, 0.009), (45, -0.078), (46, 0.049), (47, 0.014), (48, -0.099), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91872764 <a title="250-lsi-1" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>2 0.84343839 <a title="250-lsi-2" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><p>3 0.62764233 <a title="250-lsi-3" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>4 0.57651067 <a title="250-lsi-4" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><p>5 0.56418866 <a title="250-lsi-5" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>Author: Richard Nock, Frank Nielsen</p><p>Abstract: Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classiﬁcation calibration, ties up the minimization of the surrogates and classiﬁcation risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classiﬁcation calibrated surrogate strictly convex and differentiable — a set whose losses span the exponential, logistic and squared losses —, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zerosum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 ﬂavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates. 1</p><p>6 0.56112009 <a title="250-lsi-6" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>7 0.51874423 <a title="250-lsi-7" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>8 0.50576812 <a title="250-lsi-8" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>9 0.47186089 <a title="250-lsi-9" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>10 0.4300923 <a title="250-lsi-10" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>11 0.40989339 <a title="250-lsi-11" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>12 0.36312532 <a title="250-lsi-12" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>13 0.35316414 <a title="250-lsi-13" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>14 0.35062018 <a title="250-lsi-14" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>15 0.35028851 <a title="250-lsi-15" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>16 0.34794328 <a title="250-lsi-16" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>17 0.34223461 <a title="250-lsi-17" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>18 0.33721569 <a title="250-lsi-18" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>19 0.32477754 <a title="250-lsi-19" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>20 0.31648305 <a title="250-lsi-20" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.106), (7, 0.067), (12, 0.033), (28, 0.165), (32, 0.315), (57, 0.048), (59, 0.015), (63, 0.032), (71, 0.037), (77, 0.032), (83, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8700273 <a title="250-lda-1" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>Author: Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with signiﬁcant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced. 1</p><p>same-paper 2 0.76857907 <a title="250-lda-2" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>3 0.71942323 <a title="250-lda-3" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary deﬁnitions. The deﬁnitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classiﬁer is trained on the resulting sense-speciﬁc images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classiﬁcation experiments show that our dictionarybased approach outperforms baseline methods. 1</p><p>4 0.71550041 <a title="250-lda-4" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>Author: Vinod Nair, Geoffrey E. Hinton</p><p>Abstract: We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures threeway interactions among visible units, hidden units, and a single hidden discrete variable that represents the cluster label. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are deﬁned implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reﬂect the class structure in the data. 1</p><p>5 0.61005372 <a title="250-lda-5" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>Author: Tanya Schmah, Geoffrey E. Hinton, Steven L. Small, Stephen Strother, Richard S. Zemel</p><p>Abstract: Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overﬁtting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classiﬁcation task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by ﬁtting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training. 1</p><p>6 0.58022171 <a title="250-lda-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.57821208 <a title="250-lda-7" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>8 0.57613462 <a title="250-lda-8" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>9 0.57311517 <a title="250-lda-9" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>10 0.5727632 <a title="250-lda-10" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>11 0.57177305 <a title="250-lda-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.57165432 <a title="250-lda-12" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>13 0.57143766 <a title="250-lda-13" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>14 0.57066679 <a title="250-lda-14" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>15 0.57038581 <a title="250-lda-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.57022148 <a title="250-lda-16" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>17 0.56976283 <a title="250-lda-17" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>18 0.56940454 <a title="250-lda-18" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>19 0.56880456 <a title="250-lda-19" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>20 0.56869084 <a title="250-lda-20" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
