<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 nips-2008-An improved estimator of Variance Explained in the presence of noise</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-24" href="#">nips2008-24</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>24 nips-2008-An improved estimator of Variance Explained in the presence of noise</h1>
<br/><p>Source: <a title="nips-2008-24-pdf" href="http://papers.nips.cc/paper/3461-an-improved-estimator-of-variance-explained-in-the-presence-of-noise.pdf">pdf</a></p><p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>Reference: <a title="nips-2008-24-reference" href="../nips2008_reference/nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An improved estimator of Variance Explained in the presence of noise  Ralf. [sent-1, score-0.519]
</p><p>2 One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. [sent-11, score-0.215]
</p><p>3 We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. [sent-13, score-0.407]
</p><p>4 Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. [sent-14, score-0.871]
</p><p>5 We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. [sent-15, score-1.103]
</p><p>6 A fundamental problem of the traditional estimator for VE is its bias in the presence of noise in the data. [sent-24, score-0.84]
</p><p>7 This noise may be due to measurement error or sampling noise owing to the high intrinsic variability in the underlying data. [sent-25, score-0.593]
</p><p>8 This is especially important when trying to model cortical neurons where variability is ubiquitous. [sent-26, score-0.162]
</p><p>9 Either kind of noise is in principle unexplainable by the model and hence needs to be accounted for when evaluating the quality of the model. [sent-27, score-0.26]
</p><p>10 Since the total variance in the data consists of the true underlying variance plus that due to noise, the traditional estimator yields a systematic underestimation of the true VE of the model in the absence of noise [1][2][3]. [sent-28, score-1.111]
</p><p>11 This has been noted by several authors before us; David & Gallant compute the traditional measure at several noise levels and extrapolate it to the noise-free condition [1]. [sent-29, score-0.459]
</p><p>12 Sahani & Linden add an analytical correction to the traditional formula in order to reduce its bias [2]. [sent-31, score-0.586]
</p><p>13 com)  1  & Linden’s formula in three ways: 1) most importantly by accounting for the number of parameters in the model, 2) adding a correction term for the uncertainty in the noise estimation, and 3) including a conditioning term to improve the performance in the presence of excessive noise. [sent-37, score-0.687]
</p><p>14 We propose a principled method to choose the conditioning term in order to electively minimize either the bias or the mean-square-error (MSE) of the estimator. [sent-38, score-0.321]
</p><p>15 In numerical simulations we ﬁnd that the analytical correction alone is capable of drastically reducing the bias at moderate and high noise levels while maintaining a mean-square-error about as good as the traditional formula. [sent-39, score-0.829]
</p><p>16 Only for very high levels of noise is it advantageous to make use of the conditioning term. [sent-40, score-0.459]
</p><p>17 We test the effect of our improved formula on a data set of disparity selective macaque V1 neurons and ﬁnd that for many cells noise accounts for most of the unexplained variance. [sent-41, score-0.804]
</p><p>18 On a population level we ﬁnd that after adjusting for the noise, Gabor functions can explain about 98% of the underlying response variance. [sent-42, score-0.194]
</p><p>19 It is usually reported as a fraction of total variance: N  var(di ) − var(di − mi ) var(di − mi ) ν= =1− =1− var(di ) var(di )  (di − mi )2  i=1 N  . [sent-45, score-0.462]
</p><p>20 (1)  ¯ (di − d)2  i=1  In most cases, the di themselves are averages of individual measurements and subject to a sampling error. [sent-46, score-0.452]
</p><p>21 Since the variances of independent random variables add, this measurement noise leads to additive noise terms in both numerator and denominator of equation (1). [sent-47, score-0.724]
</p><p>22 Below we show that as the noise level increases, ν → (n − 1)/(N − 1) with n being the number of model parameters (see equation 8). [sent-48, score-0.251]
</p><p>23 The consequence is a systematic misestimation of the true Variance Explained (typically underestimation since (n − 1)/(N − 1) is usually smaller than the true VE). [sent-49, score-0.168]
</p><p>24 The average bias (estimated VE minus true VE) of the traditional variance explained is shown for 2000 instantiations of each simulation (shown in triangles). [sent-53, score-0.612]
</p><p>25 As we simulate an increase in sampling noise, the variance explained decreases signiﬁcantly, underestimating the true VE by up to 30% in our examples. [sent-54, score-0.292]
</p><p>26 2  Noise bias R  i ¯ Let di = 1/Ri j=1 dij where the Ri are the number of observations for each variable i. [sent-56, score-0.6]
</p><p>27 We further assume that the measured dij are drawn from a Gaussian distribution around the true means Di with ¯ a variance of RΘ2 . [sent-57, score-0.262]
</p><p>28 It follows that N R ¯ σ 2 = 1/(RN (R − 1)) i=1 j=1 (dij − di )2 is an estimate of Θ2 based on measurements with Nσ = N (R − 1) degrees of freedom. [sent-60, score-0.479]
</p><p>29 In the terms of Sahani & Linden [2], σ 2 is the noise power. [sent-61, score-0.218]
</p><p>30 Then the variance explained in the absence of noise becomes: N  var(Mi − Di ) ν0 = 1 − =1− var(Di )  (Di − Mi )2  i=1 N  (2) ¯ (Di − D)2  i=1  2  N  ¯ where D = 1/N i=1 Di . [sent-64, score-0.474]
</p><p>31 Then ν0 is the true value for the Variance Explained that one would like to know: based on the best ﬁt of the model class to the underlying data in the absence of any measurement or sampling noise. [sent-65, score-0.194]
</p><p>32 Normalizing both denominator and numerator of formula (1) by σ 2 leaves ν unchanged. [sent-67, score-0.342]
</p><p>33 (8)  Nσ (N − 1) − Nσ − 2  Note that apart from the difference in noise estimation, the estimator proposed by Sahani & Linden is contained in ours as a special case, becoming identical when there is no uncertainty in the noise estimate (Nσ → ∞) and testing a model with no free parameters (n = 0). [sent-71, score-0.802]
</p><p>34 However, the fact that their noiseterm does not account for overﬁtting due to free parameters in the model means that their formula overestimates the true Variance Explained. [sent-73, score-0.18]
</p><p>35 At this point we wish to note that (5), (7) and (8) readily generalize to cases where the noise level ¯ Σi and the number of observations Ri on which the means di are based (and therefore Nσi ) differ between those data points. [sent-75, score-0.608]
</p><p>36 3  Conditioning term  First it is important to note that while both numerator and denominator in formula (8) are now unbiased, the ratio is generally not. [sent-77, score-0.342]
</p><p>37 In fact, the ratio is not even well-deﬁned for arbitrary measurements since the denominator can become zero and negative. [sent-78, score-0.21]
</p><p>38 ) The effect of such a criterion is to cut off the lower tail of the distribution from which the denominator is drawn to exclude zero. [sent-82, score-0.17]
</p><p>39 This introduces a bias to the denominator the size of which depends on the amount of noise and the strictness of the criterion used. [sent-83, score-0.529]
</p><p>40 We recognize that both biases are strongest when the data is such that the ratio is close to singular and therefore propose an additive conditioning term C in the denominator of (8): N  Υ(C) = 1 − i=1  di − mi σ  2  −  Nσ (N − n) / Nσ − 2  N  i=1  ¯ di − d σ  2  −  Nσ (N − 1) + C . [sent-84, score-1.164]
</p><p>41 (9) Nσ − 2  Depending on the application, the optimal C can be chosen to either minimize the mean-squareerror (MSE) E[Υ(C) − ν0 ] or the bias |E[Υ(C)] − ν0 | of the estimator. [sent-85, score-0.168]
</p><p>42 Generally, the optimal levels of conditioning for the two scenarios are different, i. [sent-86, score-0.241]
</p><p>43 For individual estimates a small bias can be acceptable in order to improve accuracy (and hence minimize MSE). [sent-89, score-0.168]
</p><p>44 from a population of neurons, it becomes important that the estimator is unbiased. [sent-92, score-0.333]
</p><p>45 In the left column we show the results when testing a model that consists of a 3rd degree polynomial that has been ﬁt to noisy data sampled from a Gaussian distribution around an underlying sinefunction. [sent-99, score-0.163]
</p><p>46 The center column simulates Gaussian and the right column Gamma noise (Fano factor of 2). [sent-103, score-0.383]
</p><p>47 We conﬁrm that the traditional VE measure (triangles) has an increasingly negative bias with increasing noise level σ. [sent-104, score-0.599]
</p><p>48 Applying the Sahani-Linden correction (squares) this negative bias is turned into a positive one since the overﬁtting of noise due to the free parameters in the model is not taken into consideration. [sent-105, score-0.546]
</p><p>49 Accounting for the number of parameters greatly reduces the bias to close to zero across a large range of noise levels (dots). [sent-107, score-0.474]
</p><p>50 The bias becomes notable only 4  3  6  10  2  4  9  bias  11  1  2  0. [sent-108, score-0.336]
</p><p>51 1  0  σ  0  σ  Figure 1: Simulation results: Left column: a 3rd degree polynomial is ﬁt to noise data drawn from an underlying sine-function. [sent-143, score-0.313]
</p><p>52 Left & Center: Gaussian noise, Right: Gamma distributed noise (Fano factor of 2). [sent-145, score-0.218]
</p><p>53 Rows 2-5: bias (deﬁned as estimated minus true VE) and RMSE are shown as a function of noise σ. [sent-148, score-0.435]
</p><p>54 The traditional estimator is shown by triangles, the Sahani-Linden correction by squares, our estimator from eq. [sent-149, score-0.795]
</p><p>55 9) optimized for bias (+) and MSE (x), both dashed, are shown. [sent-153, score-0.168]
</p><p>56 Restricting VE to 0 ≤ ν ≤ 1 is the reason for the plateau in the bias of the Sahani-Linden estimator (right column, fourth from the top). [sent-154, score-0.441]
</p><p>57 at the highest noise levels (at which a large number of data samples does not pass the ANOVAtest for signiﬁcant modulation), while still remaining smaller than that of the traditional estimator. [sent-171, score-0.489]
</p><p>58 8) yields values around its noiseless value that can be positive and negative, the estimator can be negative or greater than one. [sent-176, score-0.332]
</p><p>59 We test whether a conditioning term can improve the performance of our estimator and ﬁnd that this is the case for the Gabor ﬁt, but not the polynomial ﬁt. [sent-180, score-0.458]
</p><p>60 In the case of the Gabor ﬁt, the improvement due to the conditioning term is greatest at the highest noise levels as expected. [sent-181, score-0.489]
</p><p>61 The bias is decreased at the highest three noise levels tested and the MSE is slightly decreased (at the highest noise level) or the same as with conditioning. [sent-182, score-0.752]
</p><p>62 Where the purely analytical formula outperforms the one with conditioning that is because the approximations we have to make in determining the optimal C are greater than the inaccuracy in the analytical formula at those noise levels. [sent-183, score-0.709]
</p><p>63 This is especially true in the 3rd column where the strongly non-Gaussian noise is incompatible with the Gaussian assumption in our computation of C. [sent-184, score-0.368]
</p><p>64 We conclude that unless one has to estimate VE in the presence of extremely high noise, and has conﬁrmed that conditioning provides an improvement for the particular situation under consideration, our analytical estimator is preferable. [sent-185, score-0.556]
</p><p>65 ) Using an estimator that accounts for the amount of noise has another major beneﬁt. [sent-187, score-0.491]
</p><p>66 Because the total number of measurements N · R one can make is usually limited, there is a tradeoff between number of conditions N and number of repeats R. [sent-188, score-0.22]
</p><p>67 Everything else being equal the result from the traditional estimator for VE will depend strongly on that choice: the more conditions and the fewer repeats, the higher the standard error of the means σ (noise) and hence the lower the estimated VE will be – regardless of the model. [sent-189, score-0.458]
</p><p>68 Keeping the total number of measurements constant, the traditional VE (triangles) decreases drastically as the number of conditions N is increased. [sent-191, score-0.283]
</p><p>69 The new unbiased estimator (dots) in comparison has a much reduced bias and depends only weakly on R. [sent-192, score-0.572]
</p><p>70 linear in its parameters and (2) unbiasing the denominator is not the same as unbiasing the ratio. [sent-196, score-0.243]
</p><p>71 Both approximations are accurate in the small noise regime. [sent-197, score-0.218]
</p><p>72 However, as noise levels increase they introduce biases that interact depending on the situation. [sent-198, score-0.306]
</p><p>73 8  1  Figure 3: Disparity tuning curves of V1 neurons ﬁt with a Gabor function: A: Data from an example neuron shown by their standard error of the mean (SEM) errorbars. [sent-219, score-0.265]
</p><p>74 Estimate of VE by Gabor ﬁt (solid line) changes from 85% to 93% when noise is adjusted for. [sent-220, score-0.218]
</p><p>75 D: Traditional VE estimate vs unbiased VE with conditioning to minimize MSE. [sent-228, score-0.311]
</p><p>76 1  Application to experimental data Methods  The data are recorded extracellularly from isolated V1 neurons in two awake, ﬁxating rhesus macaque monkeys and have been previously published in [7]. [sent-235, score-0.167]
</p><p>77 The stimulus consisted of dynamic random dots (RDS) with a binocular disparity applied perpendicular to the preferred orientation of the cell. [sent-236, score-0.299]
</p><p>78 We only included neurons in the analysis which were signiﬁcantly modulated by binocular disparity as evaluated by a one-way ANOVA test. [sent-237, score-0.343]
</p><p>79 2  Results  Most disparity tuning curves in V1 are reasonably well-described by Gabor functions, which explain more than 90% of the variance in two thirds of the neurons [8]. [sent-244, score-0.479]
</p><p>80 Whether the remaining third reﬂect a failure of the model or are merely a consequence of noise in the data has been an open question. [sent-245, score-0.218]
</p><p>81 The traditional VE in panel A is only 82% even though the data is not signiﬁcantly different from the model (pχ2 = 0. [sent-247, score-0.189]
</p><p>82 After adjusting for noise, the unbiased VE becomes 92%, i. [sent-249, score-0.196]
</p><p>83 more than half of the unexplained variance can be attributed to the response variability for each measurement. [sent-251, score-0.326]
</p><p>84 Panel B shows the opposite situation: 94% of the variance is explained according to the traditional measure and only an additional 1% can be attributed to noise. [sent-252, score-0.405]
</p><p>85 Panel C shows the unbiased estimate of the VE for the entire population of neurons depending on their noise power relative to signal power. [sent-254, score-0.545]
</p><p>86 At high relative noise levels there is a wide spread of values and for decreasing noise, the VE values asymptote near 1. [sent-255, score-0.306]
</p><p>87 In fact, the overall population mean for the unbiased VE is 98%, compared with the traditional estimate of 82%. [sent-256, score-0.371]
</p><p>88 This means that for the entire population, most of the variance previously deemed unexplained by the model can in fact be accounted for by our uncertainty about the data. [sent-257, score-0.307]
</p><p>89 For the estimation of the true VE for each neuron individually, we incorporate our knowledge about the bounds 0 ≤ ν0 ≤ 1 and optimize the conditioning term for minimum MSE. [sent-261, score-0.258]
</p><p>90 With the exception of two neurons, the new estimate of the true VE is greater than the traditional one. [sent-262, score-0.229]
</p><p>91 On average 40% of the unexplained variance in each individual neuron can be accounted for by noise. [sent-263, score-0.334]
</p><p>92 4  Conclusions  We have derived an new estimator of the variance explained by models describing noisy data. [sent-264, score-0.517]
</p><p>93 Furthermore, our estimator does not rely on a large number of repetitions of the same stimulus in order to perform an extrapolation to zero noise. [sent-266, score-0.33]
</p><p>94 In numerical simulations with Gaussian and strongly skewed noise we have conﬁrmed that our correction is capable of accounting for most noise levels and provides an estimate with greatly improved bias compared to previous estimators. [sent-267, score-0.884]
</p><p>95 We note that where the results from the two simulations differ, it is the more realistic simulation where the new estimator performs best. [sent-268, score-0.3]
</p><p>96 Another important beneﬁt of our new estimator is that it addresses the classical experimenter’s dilemma of a tradeoff between number of conditions N and number of repeats R at each condition. [sent-269, score-0.426]
</p><p>97 While the results from the traditional estimator quickly deteriorate with increasing N and decreasing R, the new estimator is much closer to invariant with respect to both – allowing the experimenter to choose a greater N for higher resolution. [sent-270, score-0.752]
</p><p>98 When applying the new VE estimator to a data set of macaque V1 disparity tuning curves we ﬁnd that almost all of the variance previously unaccounted for by Gabor ﬁts can be attributed to sampling noise. [sent-271, score-0.766]
</p><p>99 For our population of 109 neurons we ﬁnd that 98% of the variance can be explained by a Gabor model. [sent-272, score-0.384]
</p><p>100 The improvement we present is not limited to neuronal tuning curves but will be valuable to any model testing where noise is an important factor. [sent-274, score-0.318]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('di', 0.357), ('estimator', 0.273), ('gabor', 0.264), ('dd', 0.243), ('noise', 0.218), ('ve', 0.21), ('bias', 0.168), ('disparity', 0.159), ('mi', 0.154), ('conditioning', 0.153), ('traditional', 0.153), ('denominator', 0.143), ('mse', 0.14), ('dm', 0.137), ('unbiased', 0.131), ('unexplained', 0.125), ('var', 0.117), ('variance', 0.111), ('neurons', 0.109), ('linden', 0.109), ('numerator', 0.105), ('explained', 0.104), ('triangles', 0.1), ('panova', 0.1), ('correction', 0.096), ('formula', 0.094), ('sahani', 0.093), ('repeats', 0.09), ('rmse', 0.088), ('levels', 0.088), ('tting', 0.085), ('analytical', 0.075), ('binocular', 0.075), ('cumming', 0.075), ('dij', 0.075), ('accounting', 0.069), ('measurements', 0.067), ('column', 0.066), ('adjusting', 0.065), ('population', 0.06), ('macaque', 0.058), ('neuron', 0.056), ('experimenter', 0.053), ('variability', 0.053), ('tuning', 0.052), ('bethesda', 0.05), ('cbias', 0.05), ('fano', 0.05), ('haefner', 0.05), ('noncentrality', 0.05), ('unbiasing', 0.05), ('true', 0.049), ('curves', 0.048), ('carandini', 0.044), ('gallant', 0.044), ('noncentral', 0.044), ('underestimation', 0.044), ('accounted', 0.042), ('estimators', 0.042), ('cells', 0.041), ('absence', 0.041), ('measurement', 0.04), ('equalize', 0.04), ('nih', 0.04), ('attributed', 0.037), ('free', 0.037), ('dots', 0.036), ('underlying', 0.036), ('panel', 0.036), ('anova', 0.035), ('incompatible', 0.035), ('sensorimotor', 0.034), ('level', 0.033), ('center', 0.033), ('ri', 0.033), ('conditions', 0.032), ('noiseless', 0.032), ('md', 0.032), ('polynomial', 0.032), ('spike', 0.032), ('drastically', 0.031), ('rejected', 0.031), ('tradeoff', 0.031), ('eye', 0.03), ('highest', 0.03), ('noisy', 0.029), ('uncertainty', 0.029), ('stimulus', 0.029), ('demonstrates', 0.028), ('degrees', 0.028), ('repetitions', 0.028), ('sampling', 0.028), ('presence', 0.028), ('modulation', 0.027), ('drawn', 0.027), ('estimate', 0.027), ('negative', 0.027), ('simulation', 0.027), ('rmed', 0.026), ('systematic', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="24-tfidf-1" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>2 0.13088799 <a title="24-tfidf-2" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>Author: Iain Murray, Ruslan Salakhutdinov</p><p>Abstract: We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost. 1</p><p>3 0.12639052 <a title="24-tfidf-3" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>4 0.1010971 <a title="24-tfidf-4" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>5 0.092378609 <a title="24-tfidf-5" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>6 0.079874784 <a title="24-tfidf-6" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>7 0.078754716 <a title="24-tfidf-7" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>8 0.077462368 <a title="24-tfidf-8" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>9 0.07496994 <a title="24-tfidf-9" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>10 0.073591232 <a title="24-tfidf-10" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>11 0.073098503 <a title="24-tfidf-11" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>12 0.069791667 <a title="24-tfidf-12" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>13 0.067533307 <a title="24-tfidf-13" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>14 0.066161536 <a title="24-tfidf-14" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>15 0.065172747 <a title="24-tfidf-15" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>16 0.063101418 <a title="24-tfidf-16" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>17 0.062261425 <a title="24-tfidf-17" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>18 0.062026337 <a title="24-tfidf-18" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>19 0.058362294 <a title="24-tfidf-19" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>20 0.05800461 <a title="24-tfidf-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, 0.039), (2, 0.088), (3, 0.115), (4, -0.006), (5, 0.019), (6, -0.025), (7, 0.082), (8, 0.052), (9, 0.049), (10, -0.013), (11, 0.04), (12, 0.011), (13, 0.033), (14, -0.149), (15, -0.01), (16, 0.071), (17, 0.08), (18, 0.033), (19, -0.024), (20, 0.058), (21, 0.141), (22, 0.095), (23, 0.052), (24, -0.03), (25, 0.082), (26, 0.035), (27, -0.118), (28, 0.091), (29, 0.034), (30, -0.061), (31, -0.121), (32, -0.032), (33, -0.109), (34, 0.025), (35, 0.026), (36, -0.049), (37, -0.018), (38, -0.025), (39, -0.059), (40, -0.084), (41, -0.073), (42, 0.122), (43, -0.193), (44, -0.155), (45, -0.119), (46, 0.17), (47, 0.066), (48, 0.153), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97816432 <a title="24-lsi-1" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>2 0.79632437 <a title="24-lsi-2" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>3 0.69182199 <a title="24-lsi-3" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>Author: Guangzhi Cao, Charles Bouman</p><p>Abstract: Covariance estimation for high dimensional vectors is a classically difﬁcult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More speciﬁcally, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efﬁciently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efﬁciently computed using a cross-validation procedure. The resulting estimator is positive deﬁnite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes. 1</p><p>4 0.5819627 <a title="24-lsi-4" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>5 0.49717885 <a title="24-lsi-5" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>6 0.46422142 <a title="24-lsi-6" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>7 0.45920053 <a title="24-lsi-7" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>8 0.43027458 <a title="24-lsi-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.42413107 <a title="24-lsi-9" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>10 0.40151495 <a title="24-lsi-10" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>11 0.37477699 <a title="24-lsi-11" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>12 0.36343652 <a title="24-lsi-12" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>13 0.3276943 <a title="24-lsi-13" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>14 0.3264944 <a title="24-lsi-14" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>15 0.32387954 <a title="24-lsi-15" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>16 0.31365174 <a title="24-lsi-16" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>17 0.308083 <a title="24-lsi-17" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>18 0.2876716 <a title="24-lsi-18" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>19 0.28472295 <a title="24-lsi-19" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>20 0.28323448 <a title="24-lsi-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.109), (7, 0.071), (12, 0.031), (15, 0.015), (25, 0.256), (28, 0.18), (57, 0.098), (59, 0.02), (63, 0.016), (71, 0.018), (77, 0.049), (78, 0.013), (83, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91869903 <a title="24-lda-1" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>Author: Vicençc Gómez, Andreas Kaltenbrunner, Vicente López, Hilbert J. Kappen</p><p>Abstract: Large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics. An example of this phenomenon is the transition from irregular, noise-driven dynamics to regular, self-sustained behavior observed in networks of integrate-and-ﬁre neurons as the interaction strength between the neurons increases. In this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals (dynamic range) is maximized. Self-organization occurs via synaptic dynamics that we analytically derive. The resulting plasticity rule is deﬁned locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses. 1</p><p>same-paper 2 0.81824023 <a title="24-lda-2" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>3 0.81425339 <a title="24-lda-3" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>Author: Zakria Hussain, John S. Shawe-taylor</p><p>Abstract: We analyse matching pursuit for kernel principal components analysis (KPCA) by proving that the sparse subspace it produces is a sample compression scheme. We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al [7] and highly predictive of the size of the subspace needed to capture most of the variance in the data. We analyse a second matching pursuit algorithm called kernel matching pursuit (KMP) which does not correspond to a sample compression scheme. However, we give a novel bound that views the choice of subspace of the KMP algorithm as a compression scheme and hence provide a VC bound to upper bound its future loss. Finally we describe how the same bound can be applied to other matching pursuit related algorithms. 1</p><p>4 0.69348264 <a title="24-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.6824491 <a title="24-lda-5" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>6 0.6808399 <a title="24-lda-6" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>7 0.68063092 <a title="24-lda-7" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>8 0.67739779 <a title="24-lda-8" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>9 0.67700499 <a title="24-lda-9" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>10 0.67691255 <a title="24-lda-10" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>11 0.67671406 <a title="24-lda-11" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>12 0.67495137 <a title="24-lda-12" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>13 0.67449206 <a title="24-lda-13" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>14 0.67412299 <a title="24-lda-14" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>15 0.6730141 <a title="24-lda-15" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>16 0.67287374 <a title="24-lda-16" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>17 0.67280078 <a title="24-lda-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.67224944 <a title="24-lda-18" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>19 0.67170036 <a title="24-lda-19" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>20 0.67106497 <a title="24-lda-20" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
