<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 nips-2008-Learning Transformational Invariants from Natural Movies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-118" href="#">nips2008-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 nips-2008-Learning Transformational Invariants from Natural Movies</h1>
<br/><p>Source: <a title="nips-2008-118-pdf" href="http://papers.nips.cc/paper/3378-learning-transformational-invariants-from-natural-movies.pdf">pdf</a></p><p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>Reference: <a title="nips-2008-118-reference" href="../nips2008_reference/nips-2008-Learning_Transformational_Invariants_from_Natural_Movies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. [sent-4, score-0.671]
</p><p>2 The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. [sent-5, score-1.219]
</p><p>3 The second layer learns the higher-order structure among the time-varying phase variables. [sent-6, score-0.698]
</p><p>4 After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. [sent-7, score-0.567]
</p><p>5 We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). [sent-8, score-1.005]
</p><p>6 The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. [sent-9, score-0.318]
</p><p>7 1  Introduction  A key attribute of visual perception is the ability to extract invariances from visual input. [sent-11, score-0.312]
</p><p>8 In the realm of object recognition, the goal of invariant representation is quite clear: a successful object recognition system must be invariant to image variations resulting from different views of the same object. [sent-12, score-0.436]
</p><p>9 While spatial invariants are essential for forming a useful representation of the natural environment, there is another, equally important form of visual invariance, namely transformational invariance. [sent-13, score-0.688]
</p><p>10 A transformational invariant refers to the dynamic visual structure that remains the same when the spatial structure changes. [sent-14, score-0.61]
</p><p>11 For example, the property that a soccer ball moving through the air shares with a football moving through the air is a transformational invariant; it is speciﬁc to how the ball moves but invariant to the shape or form of the object. [sent-15, score-0.463]
</p><p>12 Here we seek to learn such invariants from the statistics of natural movies. [sent-16, score-0.244]
</p><p>13 There have been numerous efforts to learn spatial invariants [1, 2, 3] from the statistics of natural images, especially with the goal of producing representations useful for object recognition [4, 5, 6]. [sent-17, score-0.471]
</p><p>14 However, there have been few attempts to learn transformational invariants from natural sensory data. [sent-18, score-0.468]
</p><p>15 Furthermore, it is unclear to what extent these models have captured the diversity of transformations in natural visual scenes or to what level of abstraction their representations produce transformational invariants. [sent-20, score-0.544]
</p><p>16 However, this type of model does not capture the abstract property of motion because each unit is bound to a speciﬁc orientation, spatial-frequency and location within the image—i. [sent-22, score-0.318]
</p><p>17 1  Here we describe a hierarchical probabilistic generative model that learns transformational invariants from unsupervised exposure to natural movies. [sent-25, score-0.625]
</p><p>18 The latter approach characterizes most models of form and motion processing in the visual cortical hierarchy [6, 12], but suffers from the fact that information about these properties is not bound together—i. [sent-27, score-0.397]
</p><p>19 , it is not possible to reconstruct an image sequence from a representation in which form and motion have been extracted by separate and independent mechanisms. [sent-29, score-0.439]
</p><p>20 In the model we propose here, form and motion are factorized, meaning that extracting one property depends upon the other. [sent-31, score-0.318]
</p><p>21 We show that when such a model is adapted to natural movies, the top layer units learn to extract transformational invariants. [sent-33, score-0.815]
</p><p>22 The diversity of units in both the intermediate layer and top layer provides a set of testable predictions for representations that might be found in V1 and MT. [sent-34, score-0.949]
</p><p>23 The model consists of an input layer and two hidden layers as shown in Figure 1. [sent-37, score-0.519]
</p><p>24 The input layer represents the time-varying image pixel intensities. [sent-38, score-0.518]
</p><p>25 The ﬁrst hidden layer is a sparse coding model utilizing complex basis functions, and shares many properties with subspace-ICA [13] and the standard energy model of complex cells [14]. [sent-39, score-0.946]
</p><p>26 The second hidden layer models the dynamics of the complex basis function phase variables. [sent-40, score-0.947]
</p><p>27 The sparse coding model imposes a kurtotic, independent prior over the coefﬁcients, and when adapted to natural image patches the Ai (x) converge to a set of localized, oriented, multiscale functions similar to a Gabor wavelet decomposition of images. [sent-43, score-0.546]
</p><p>28 We propose here a generalization of the sparse coding model to complex variables that is primarily motivated from two observations of natural image statistics. [sent-44, score-0.451]
</p><p>29 These are most clearly seen as circularly symmetric, yet kurtotic distributions among pairs of coefﬁcients corresponding to neighboring basis functions, as ﬁrst described by Zetzsche [17]. [sent-46, score-0.312]
</p><p>30 As pointed out by Hyvarinen [3], the temporal evolution of a coefﬁcient in response to a movie, ui (t), can be well described in terms of the product of a smooth amplitude envelope multiplied by a quickly changing variable. [sent-51, score-0.304]
</p><p>31 A similar result from Kording [1] indicates that temporal continuity in amplitude provides a strong cue for learning local invariances. [sent-52, score-0.301]
</p><p>32 Note that the basis functions are only functions of space. [sent-58, score-0.232]
</p><p>33 Therefore, the temporal dynamics within image sequences will be expressed in the temporal dynamics of the amplitude and phase. [sent-59, score-0.572]
</p><p>34 The second term in the exponential imposes temporal stability on the time rate of change of the amplitudes and is given by: Sla (ai (t), ai (t−1)) = (ai (t) − ai (t−1))2 . [sent-63, score-0.82]
</p><p>35 2  Phase Transformations  Given the decomposition into amplitude and phase variables, we now have a non-linear representation of image content that enables us to learn its structure in another linear generative model. [sent-67, score-0.709]
</p><p>36 In particular, the dynamics of objects moving in continuous trajectories through the world over short epochs will be encoded in the population activity of the phase variables φi . [sent-68, score-0.421]
</p><p>37 Furthermore, because we have encoded these trajectories with an angular variable, many transformations in the image domain that would otherwise be nonlinear in the coefﬁcients ui will now be linearized. [sent-69, score-0.342]
</p><p>38 This linear relationship allows us to model the time-rate of change of the phase variables with a simple linear generative model. [sent-70, score-0.329]
</p><p>39 We thus model the ﬁrst-order time derivative of the phase variables as follows: ˙ φi (t) =  Dik wk (t) + νi (t)  (6)  k  ˙ where φi = φi (t) − φi (t−1), and D is the basis function matrix specifying how the high-level ˙ variables wk inﬂuence the phase shifts φi . [sent-71, score-0.925]
</p><p>40 The additive noise term, νi , represents uncertainty or noise in the estimate of the phase time-rate of change. [sent-72, score-0.259]
</p><p>41 As before, we impose a sparse, independent distribution on the coefﬁcients wk , in this case with a sparse cost function given as: Sw (wk (t)) = β log 1 +  wk (t) σ  2  (7)  The uncertainty over the phase shifts is given by a von Mises distribution: p(νi ) ∝ exp(κ cos(νi )). [sent-73, score-0.554]
</p><p>42 Thus, the log-posterior over the second layer units is given by ˙ κ cos(φi − [Dw(t)]i ) +  E2 = − t  Sw (wk (t)) k  i∈{ai (t)>0}  3  (8)  Figure 1: Graph of the hierarchical model showing the relationship among hidden variables. [sent-74, score-0.606]
</p><p>43 Because the angle of a variable with 0 amplitude is undeﬁned, we exclude angles where the corresponding amplitude is 0 from our cost function. [sent-75, score-0.436]
</p><p>44 Note that in the ﬁrst layer we did not introduce any prior on the phase variables. [sent-76, score-0.625]
</p><p>45 With our second hidden layer, E2 can be viewed as a log-prior on the time rate of change of the phase variables: ˙ ˙ φi (t). [sent-77, score-0.311]
</p><p>46 ˙ Activating the w variables moves the prior away from φi (t) = 0, encouraging certain patterns of phase shifting that will in turn produce patterns of motion in the image domain. [sent-79, score-0.788]
</p><p>47 The resulting dynamics for the amplitudes and phases in the ﬁrst layer are given by ∆ai (t) ∝ {bi (t)} − Sp (ai (t)) − Sl (ai (t), ai (t−1)) (9) ˙ i (t) − [Dw(t)]i ) + κ sin(φi (t+1) − [Dw(t+1)]i ) (10) ˙ ∆φi (t) ∝ {bi (t)} ai (t) − κ sin(φ with bi (t) = σ1 e−jφi (t) 2 N  ∗ {zi (t) Ai (x)} . [sent-87, score-1.214]
</p><p>48 The learning rule for the ﬁrst layer basis functions is given by the gradient of E1 with respect to Ai (x), using the values of the complex coefﬁcients inferred in eqs. [sent-92, score-0.632]
</p><p>49 1  Simulation procedures  The model was trained on natural image sequences obtained from Hans van Hateren’s repository at http://hlab. [sent-95, score-0.241]
</p><p>50 They contain a variety of motions due to the movements of animals in the scene, camera motion, tracking (which introduces background motion), and motion borders due to occlusion. [sent-102, score-0.345]
</p><p>51 We trained the ﬁrst layer of the model on 20x20 pixel image patches, using 400 complex basis functions Ai in the ﬁrst hidden layer initialized to random values. [sent-103, score-1.233]
</p><p>52 During this initial phase of learning only the terms in E1 are used to infer the ai and φi . [sent-104, score-0.591]
</p><p>53 Once the ﬁrst layer reaches convergence, we begin training the second layer, using 100 bases, Di , initialized to random values. [sent-105, score-0.366]
</p><p>54 The second ˙ layer bases are initially trained on the MAP estimates of the ﬁrst layer φi inferred using E1 only. [sent-106, score-0.732]
</p><p>55 After the second layer begins to converge we infer coefﬁcients in both the ﬁrst layer and the second layer simultaneously using all terms in E1 + E2 (we observed that this improved convergence in the second layer). [sent-107, score-1.098]
</p><p>56 The bootstrapping of the second layer was used to speed convergence and we did not observe much change in the ﬁrst layer basis functions after the initial convergence. [sent-109, score-0.917]
</p><p>57 2  Learned complex basis functions  After learning, the ﬁrst layer complex basis functions converge to a set of localized, oriented, and bandpass functions with real and imaginary parts roughly in quadrature. [sent-113, score-1.026]
</p><p>58 Figure 2(a) shows the real part, imaginary part, amplitude, and angle of two representative basis functions as a function of space. [sent-116, score-0.266]
</p><p>59 Examining the amplitude of the basis function we see that it is localized and has a roughly Gaussian envelope. [sent-117, score-0.356]
</p><p>60 The angle as a function of space reveals a smooth ramping of the phase in the direction perpendicular to the basis functions’ orientation. [sent-118, score-0.435]
</p><p>61 Figure 2(b) displays the resulting image sequences produced by two representative basis functions as the amplitude and phase follow the indicated time courses. [sent-122, score-0.814]
</p><p>62 The amplitude has the effect of controlling the presence of the feature within the image and the phase is related to the position of the edge within the image. [sent-123, score-0.668]
</p><p>63 Importantly for our hierarchical model, the time derivative, or slope of the phase through time is directly related to the movement of the edge through time. [sent-124, score-0.314]
</p><p>64 Figure 2(c) shows how the population of complex basis functions tiles the space of position (left) and spatial-frequency (right). [sent-125, score-0.345]
</p><p>65 Each dot represents a different basis function according to its maximum amplitude in the space domain, or its maximum amplitude in the frequency domain computed via the 2D Fourier transform of each complex pair (which produces a single peak in the spatialfrequency plane). [sent-126, score-0.797]
</p><p>66 This visualization will be useful for understanding what the phase shifting components D in the second layer have learned. [sent-128, score-0.677]
</p><p>67 Some functions we believe arise from aliased temporal structure in the movies (row 1, column 5), and others are unknown (row 2, column 4). [sent-138, score-0.286]
</p><p>68 Spatial Domain  Frequency Domain  Spatial Domain  Frequency Domain  Figure 3: Learned phase shifting components. [sent-140, score-0.311]
</p><p>69 The phase shift components generate movements within the image that are invariant to aspects of the spatial structure such as orientation and spatial-frequency. [sent-141, score-0.679]
</p><p>70 4  Discussion and conclusions  The computational vision community has spent considerable effort on developing motion models. [sent-146, score-0.335]
</p><p>71 Of particular relevance to our work is the Motion-Energy model [14], which signals motion via the amplitudes of quadrature pair ﬁlter outputs, similar to the responses of complex neurons in V1. [sent-147, score-0.469]
</p><p>72 Simoncelli & Heeger have shown how it is possible to extract motion by pooling over a population of such units lying within a common plane in the 3D Fourier domain [12]. [sent-148, score-0.516]
</p><p>73 Our model addresses each of these problems: it learns from the statistics of natural movies how to best tile the joint domain of position and motion, and it captures complex motion beyond uniform translation. [sent-151, score-0.799]
</p><p>74 The use of phase information for computing motion is not new, and was used by Fleet and Jepson [20] to compute optic ﬂow. [sent-153, score-0.546]
</p><p>75 In addition, as shown in Eero Simoncelli’s Thesis, one can establish a formal equivalence between phase-based methods and motion energy models. [sent-154, score-0.287]
</p><p>76 Here we argue that phase provides a convenient representation as it linearizes trajectories in coefﬁcient space and thus allows one to capture the higher-order structure via a simple linear generative model. [sent-155, score-0.37]
</p><p>77 Whether or how phase is represented in V1 is not known, 6  (a)  (c)  (b)  (d)  Figure 4: Visualization of learned transformational invariants (best viewed as animations in movie TransInv Figure4x. [sent-156, score-0.757]
</p><p>78 Each phase-shift component produces a pattern of motion that is invariant to the spatial structure contained within the image. [sent-158, score-0.573]
</p><p>79 Each panel displays the induced image transformations for a different basis function, Di . [sent-159, score-0.36]
</p><p>80 Induced motions are shown for four different image patches with the original static patch displayed in the center position. [sent-160, score-0.384]
</p><p>81 The ﬁnal image in each sequence shows the pixel-wise variance of the transformation (white values indicate where image pixels are changing through time, which may be difﬁcult to discern in this static presentation). [sent-162, score-0.304]
</p><p>82 The example in (a) produces global motion in the direction of 45 deg. [sent-163, score-0.376]
</p><p>83 The next example (b) produces local vertical motion in the lower portion of the image patch only. [sent-166, score-0.635]
</p><p>84 Note that in the ﬁrst patch the strong edge in the lower portion of the patch moves while the edge in the upper portion remains ﬁxed. [sent-167, score-0.262]
</p><p>85 Again, this component produces similar transformations irrespective of the spatial structure contained in the image. [sent-168, score-0.272]
</p><p>86 The example in (c) produces horizontal motion in the left part of the image in the opposite direction of horizontal motion in the right half (the two halves of the image either converge or diverge). [sent-169, score-1.039]
</p><p>87 Note that the oriented structure in the ﬁrst two patches becomes more closely spaced in the leftmost patch and is more widely spaced in the right most image. [sent-170, score-0.261]
</p><p>88 This is seen clearly in the third image as the spacing between the vertical structure is most narrow in the leftmost image and widest in the rightmost image. [sent-171, score-0.389]
</p><p>89 Instead of independent processing streams focused on form perception and motion perception, the two streams may represent complementary aspects of visual information: spatial invariants and transformational invariants. [sent-177, score-0.975]
</p><p>90 Importantly though, in our model information about form and motion is bound together since it is computed by a process of factorization rather than by independent mechanisms in separate streams. [sent-179, score-0.318]
</p><p>91 Our model also illustrates a functional role for feedback between higher visual areas and primary visual cortex, not unlike the proposed inference pathways suggested by Lee and Mumford [22]. [sent-180, score-0.251]
</p><p>92 The ﬁrst layer units are responsive to visual information in a narrow spatial window and narrow spatial frequency band. [sent-181, score-0.924]
</p><p>93 However, the top layer units receive input from a diverse population of ﬁrst layer units and can thus disambiguate local information by providing a bias to the time rate of change of the phase variables. [sent-182, score-1.305]
</p><p>94 Because the second layer weights D are adapted to the statistics of natural movies, these biases will be consistent with the statistical distribution of motion occurring in the 7  natural environment. [sent-183, score-0.769]
</p><p>95 Most obviously, the graphical model in Figure 1 begs the question of what would be gained by modeling the joint distribution over the amplitudes, ai , in addition to the phases. [sent-186, score-0.363]
</p><p>96 To some degree, this line of approach has already been pursued by Karklin & Lewicki [2], and they have shown that the high level units in this case learn spatial invariants within the image. [sent-187, score-0.398]
</p><p>97 We are thus eager to combine both of these models into a uniﬁed model of higher-order form and motion in images. [sent-188, score-0.318]
</p><p>98 A selection model for motion processing in area MT of primates. [sent-233, score-0.318]
</p><p>99 Invariant global motion recognition in the dorsal visual system: A unifying theory. [sent-248, score-0.445]
</p><p>100 Computation of component image velocity from local phase information. [sent-316, score-0.444]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('layer', 0.366), ('ai', 0.332), ('motion', 0.287), ('phase', 0.259), ('transformational', 0.224), ('amplitude', 0.218), ('invariants', 0.186), ('image', 0.152), ('movies', 0.148), ('basis', 0.138), ('wk', 0.119), ('coef', 0.114), ('visual', 0.11), ('spatial', 0.11), ('cients', 0.103), ('units', 0.102), ('dw', 0.094), ('circularly', 0.093), ('patches', 0.093), ('invariant', 0.084), ('imaginary', 0.081), ('kurtotic', 0.081), ('patch', 0.081), ('complex', 0.081), ('coding', 0.072), ('tile', 0.07), ('amplitudes', 0.07), ('layers', 0.07), ('transformations', 0.07), ('phases', 0.063), ('dik', 0.061), ('natural', 0.058), ('motions', 0.058), ('perception', 0.058), ('sin', 0.058), ('sparse', 0.057), ('america', 0.056), ('sw', 0.056), ('hierarchical', 0.055), ('domain', 0.053), ('hidden', 0.052), ('shifting', 0.052), ('optical', 0.051), ('dynamics', 0.051), ('produces', 0.051), ('temporal', 0.05), ('ar', 0.05), ('recognition', 0.048), ('vision', 0.048), ('invariance', 0.047), ('diversity', 0.047), ('functions', 0.047), ('cadieu', 0.047), ('fleet', 0.047), ('sla', 0.047), ('transinv', 0.047), ('oriented', 0.046), ('translation', 0.045), ('simoncelli', 0.045), ('sp', 0.045), ('learned', 0.044), ('movie', 0.044), ('narrow', 0.044), ('zi', 0.043), ('karklin', 0.041), ('aperture', 0.041), ('dilation', 0.041), ('spin', 0.041), ('zetzsche', 0.041), ('cos', 0.041), ('structure', 0.041), ('images', 0.04), ('moving', 0.04), ('population', 0.04), ('generative', 0.039), ('position', 0.039), ('direction', 0.038), ('moves', 0.038), ('frequency', 0.038), ('shares', 0.037), ('disambiguate', 0.037), ('adelson', 0.037), ('hyvarinen', 0.037), ('horizontal', 0.036), ('ui', 0.036), ('imposes', 0.036), ('representations', 0.035), ('polar', 0.035), ('fourier', 0.035), ('object', 0.034), ('extract', 0.034), ('uence', 0.034), ('local', 0.033), ('testable', 0.033), ('orientation', 0.033), ('learns', 0.032), ('emergence', 0.031), ('portion', 0.031), ('trajectories', 0.031), ('model', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="118-tfidf-1" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>2 0.22266316 <a title="118-tfidf-2" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>Author: Shuang Wu, Hongjing Lu, Alan L. Yuille</p><p>Abstract: Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These ﬁndings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we ﬁrst perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We deﬁne novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments. 1</p><p>3 0.21536122 <a title="118-tfidf-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.17974588 <a title="118-tfidf-4" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>Author: Ijaz Akhter, Yaser Sheikh, Sohaib Khan, Takeo Kanade</p><p>Abstract: Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we propose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this approach is that we do not need to estimate any basis vectors during computation. We show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) basis, can be used to compactly describe most real motions. This results in a signiﬁcant reduction in unknowns, and corresponding stability in estimation. We report empirical performance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing). 1</p><p>5 0.14379662 <a title="118-tfidf-5" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>Author: Viren Jain, Sebastian Seung</p><p>Abstract: We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from speciﬁc noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we ﬁnd that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random ﬁeld (MRF) methods. Moreover, we ﬁnd that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean ﬁeld theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difﬁculties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is signiﬁcantly less than that associated with inference in MRF approaches with even hundreds of parameters. 1 Background Low-level image processing tasks include edge detection, interpolation, and deconvolution. These tasks are useful both in themselves, and as a front-end for high-level visual tasks like object recognition. This paper focuses on the task of denoising, deﬁned as the recovery of an underlying image from an observation that has been subjected to Gaussian noise. One approach to image denoising is to transform an image from pixel intensities into another representation where statistical regularities are more easily captured. For example, the Gaussian scale mixture (GSM) model introduced by Portilla and colleagues is based on a multiscale wavelet decomposition that provides an effective description of local image statistics [1, 2]. Another approach is to try and capture statistical regularities of pixel intensities directly using Markov random ﬁelds (MRFs) to deﬁne a prior over the image space. Initial work used handdesigned settings of the parameters, but recently there has been increasing success in learning the parameters of such models from databases of natural images [3, 4, 5, 6, 7, 8]. Prior models can be used for tasks such as image denoising by augmenting the prior with a noise model. Alternatively, an MRF can be used to model the probability distribution of the clean image conditioned on the noisy image. This conditional random ﬁeld (CRF) approach is said to be discriminative, in contrast to the generative MRF approach. Several researchers have shown that the CRF approach can outperform generative learning on various image restoration and labeling tasks [9, 10]. CRFs have recently been applied to the problem of image denoising as well [5]. 1 The present work is most closely related to the CRF approach. Indeed, certain special cases of convolutional networks can be seen as performing maximum likelihood inference on a CRF [11]. The advantage of the convolutional network approach is that it avoids a general difﬁculty with applying MRF-based methods to image analysis: the computational expense associated with both parameter estimation and inference in probabilistic models. For example, naive methods of learning MRFbased models involve calculation of the partition function, a normalization factor that is generally intractable for realistic models and image dimensions. As a result, a great deal of research has been devoted to approximate MRF learning and inference techniques that meliorate computational difﬁculties, generally at the cost of either representational power or theoretical guarantees [12, 13]. Convolutional networks largely avoid these difﬁculties by posing the computational task within the statistical framework of regression rather than density estimation. Regression is a more tractable computation and therefore permits models with greater representational power than methods based on density estimation. This claim will be argued for with empirical results on the denoising problem, as well as mathematical connections between MRF and convolutional network approaches. 2 Convolutional Networks Convolutional networks have been extensively applied to visual object recognition using architectures that accept an image as input and, through alternating layers of convolution and subsampling, produce one or more output values that are thresholded to yield binary predictions regarding object identity [14, 15]. In contrast, we study networks that accept an image as input and produce an entire image as output. Previous work has used such architectures to produce images with binary targets in image restoration problems for specialized microscopy data [11, 16]. Here we show that similar architectures can also be used to produce images with the analog ﬂuctuations found in the intensity distributions of natural images. Network Dynamics and Architecture A convolutional network is an alternating sequence of linear ﬁltering and nonlinear transformation operations. The input and output layers include one or more images, while intermediate layers contain “hidden</p><p>6 0.14217412 <a title="118-tfidf-6" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>7 0.13499831 <a title="118-tfidf-7" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>8 0.12567566 <a title="118-tfidf-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.12366334 <a title="118-tfidf-9" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>10 0.11162885 <a title="118-tfidf-10" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>11 0.1095089 <a title="118-tfidf-11" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>12 0.1077394 <a title="118-tfidf-12" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>13 0.10625581 <a title="118-tfidf-13" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>14 0.094628647 <a title="118-tfidf-14" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>15 0.092544451 <a title="118-tfidf-15" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>16 0.090752162 <a title="118-tfidf-16" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>17 0.086867064 <a title="118-tfidf-17" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>18 0.083113149 <a title="118-tfidf-18" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>19 0.082630888 <a title="118-tfidf-19" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>20 0.082137898 <a title="118-tfidf-20" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.248), (1, -0.051), (2, 0.206), (3, -0.077), (4, 0.076), (5, 0.022), (6, -0.201), (7, -0.115), (8, 0.089), (9, 0.07), (10, -0.011), (11, 0.015), (12, 0.1), (13, 0.226), (14, 0.012), (15, -0.129), (16, -0.117), (17, 0.009), (18, -0.113), (19, -0.072), (20, -0.117), (21, -0.008), (22, -0.15), (23, -0.191), (24, 0.073), (25, -0.063), (26, -0.03), (27, -0.097), (28, 0.065), (29, -0.096), (30, 0.067), (31, 0.077), (32, -0.008), (33, 0.005), (34, 0.049), (35, -0.053), (36, 0.017), (37, -0.063), (38, 0.126), (39, -0.06), (40, -0.024), (41, 0.019), (42, 0.01), (43, -0.016), (44, -0.036), (45, 0.036), (46, -0.025), (47, 0.008), (48, 0.01), (49, -0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97225726 <a title="118-lsi-1" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>2 0.71660745 <a title="118-lsi-2" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>Author: Shuang Wu, Hongjing Lu, Alan L. Yuille</p><p>Abstract: Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These ﬁndings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we ﬁrst perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We deﬁne novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments. 1</p><p>3 0.70877719 <a title="118-lsi-3" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>Author: Ijaz Akhter, Yaser Sheikh, Sohaib Khan, Takeo Kanade</p><p>Abstract: Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we propose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this approach is that we do not need to estimate any basis vectors during computation. We show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) basis, can be used to compactly describe most real motions. This results in a signiﬁcant reduction in unknowns, and corresponding stability in estimation. We report empirical performance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing). 1</p><p>4 0.60898381 <a title="118-lsi-4" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random ﬁeld (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a ﬂexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs signiﬁcantly better than directly applying hCRF on local patches alone. 1</p><p>5 0.59722716 <a title="118-lsi-5" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>6 0.58314401 <a title="118-lsi-6" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>7 0.55154991 <a title="118-lsi-7" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>8 0.54788005 <a title="118-lsi-8" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>9 0.51000488 <a title="118-lsi-9" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>10 0.49067762 <a title="118-lsi-10" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>11 0.46740246 <a title="118-lsi-11" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>12 0.41040468 <a title="118-lsi-12" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>13 0.40196949 <a title="118-lsi-13" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>14 0.39005411 <a title="118-lsi-14" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>15 0.38828194 <a title="118-lsi-15" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>16 0.38745126 <a title="118-lsi-16" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>17 0.38235503 <a title="118-lsi-17" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>18 0.38075343 <a title="118-lsi-18" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>19 0.37531465 <a title="118-lsi-19" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>20 0.34980851 <a title="118-lsi-20" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.065), (7, 0.1), (12, 0.072), (15, 0.014), (28, 0.174), (57, 0.114), (59, 0.031), (60, 0.195), (63, 0.02), (71, 0.024), (77, 0.055), (78, 0.012), (83, 0.041), (94, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96527785 <a title="118-lda-1" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>Author: Juan Huo, Zhijun Yang, Alan F. Murray</p><p>Abstract: The visual and auditory map alignment in the Superior Colliculus (SC) of barn owl is important for its accurate localization for prey behavior. Prism learning or Blindness may interfere this alignment and cause loss of the capability of accurate prey. However, juvenile barn owl could recover its sensory map alignment by shifting its auditory map. The adaptation of this map alignment is believed based on activity dependent axon developing in Inferior Colliculus (IC). A model is built to explore this mechanism. In this model, axon growing process is instructed by an inhibitory network in SC while the strength of the inhibition adjusted by Spike Timing Dependent Plasticity (STDP). We test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system. 1</p><p>same-paper 2 0.85040498 <a title="118-lda-2" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>3 0.84265804 <a title="118-lda-3" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>Author: Chao Yuan, Claus Neubauer</p><p>Abstract: Mixture of Gaussian processes models extended a single Gaussian process with ability of modeling multi-modal data and reduction of training complexity. Previous inference algorithms for these models are mostly based on Gibbs sampling, which can be very slow, particularly for large-scale data sets. We present a new generative mixture of experts model. Each expert is still a Gaussian process but is reformulated by a linear model. This breaks the dependency among training outputs and enables us to use a much faster variational Bayesian algorithm for training. Our gating network is more ﬂexible than previous generative approaches as inputs for each expert are modeled by a Gaussian mixture model. The number of experts and number of Gaussian components for an expert are inferred automatically. A variety of tests show the advantages of our method. 1</p><p>4 0.77233362 <a title="118-lda-4" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>5 0.76288122 <a title="118-lda-5" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>6 0.76118684 <a title="118-lda-6" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>7 0.76051712 <a title="118-lda-7" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>8 0.76036894 <a title="118-lda-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.75653058 <a title="118-lda-9" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>10 0.75595146 <a title="118-lda-10" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>11 0.75563657 <a title="118-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.75542217 <a title="118-lda-12" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>13 0.75514251 <a title="118-lda-13" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>14 0.75387388 <a title="118-lda-14" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>15 0.75130701 <a title="118-lda-15" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>16 0.75058937 <a title="118-lda-16" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>17 0.74924338 <a title="118-lda-17" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>18 0.74923778 <a title="118-lda-18" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>19 0.74916232 <a title="118-lda-19" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>20 0.74866444 <a title="118-lda-20" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
