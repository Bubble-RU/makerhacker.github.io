<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-177" href="#">nips2008-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</h1>
<br/><p>Source: <a title="nips-2008-177-pdf" href="http://papers.nips.cc/paper/3397-particle-filter-based-policy-gradient-in-pomdps.pdf">pdf</a></p><p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>Reference: <a title="nips-2008-177-reference" href="../nips2008_reference/nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Decisions are based on a Particle Filter for estimating the belief state given past observations. [sent-8, score-0.26]
</p><p>2 We consider a policy gradient approach for parameterized policy optimization. [sent-9, score-0.604]
</p><p>3 We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. [sent-11, score-0.331]
</p><p>4 , 1998)) deﬁned by a state process (Xt )t≥1 ∈ X, an observation process (Yt )t≥1 ∈ Y , a decision (or action) process (At )t≥1 ∈ A which depends on a policy (mapping from all possible observation histories to actions), and a reward function r : X → R. [sent-16, score-0.526]
</p><p>5 Our goal is to ﬁnd a policy π that maximizes a performance measure J(π), function of future rewards, for example in a ﬁnite horizon setting: n def  J(π) = E  r(Xt ) . [sent-17, score-0.47]
</p><p>6 The state process is a Markov decision process taking its values in a (measurable) state space X, with initial probability measure µ ∈ M(X) (i. [sent-20, score-0.23]
</p><p>7 For simplicity, we adopt the notations def  F (x0 , a0 , u) = Fµ (u), where Fµ is the ﬁrst transition function (i. [sent-29, score-0.258]
</p><p>8 The observation process (Yt )t≥1 lies in a (measurable) space Y and is linked with the state process by the conditional probability measure P(Yt ∈ dyt |Xt = xt ) = g(xt , yt ) dyt , where g : X × Y → [0, 1] is the marginal density function of Yt given Xt . [sent-32, score-0.715]
</p><p>9 Now, the action process (At )t≥1 depends on a policy π which assigns to each possible observation history Y1:t (where we adopt the usual notation “1 : t” to denote the collection of integers s such that 1 ≤ s ≤ t), an action At ∈ A. [sent-40, score-0.481]
</p><p>10 In this paper we will consider policies that depend on the belief state (also called ﬁltering distribution) conditionally to past observations. [sent-41, score-0.344]
</p><p>11 The belief state, written bt , belongs to M(X) (the space def  of all probability measures on X) and is deﬁned by bt (dxt , Y1:t ) = P(Xt ∈ dxt |Y1:t ), and will be written bt (dxt ) or even bt for simplicity when there is no risk of confusion. [sent-42, score-1.563]
</p><p>12 Because of the Markov property of the state dynamics, the belief state bt (·, Y1:t ) is the most informative representation about the current state Xt given the history of past observations Y1:t . [sent-43, score-0.779]
</p><p>13 It represents sufﬁcient statistics for designing an optimal policy in the class of observations-based policies. [sent-44, score-0.26]
</p><p>14 The temporal and causal dependencies of the dynamics of a generic POMDP using belief-based policies is summarized in Figure 1 (left): at time t, the state Xt is unknown, only Yt is observed, which enables (at least in theory) to update bt based on the previous belief bt−1 . [sent-45, score-0.671]
</p><p>15 The policy π takes as input the belief state bt and returns an action At (the policy may be deterministic or stochastic). [sent-46, score-1.149]
</p><p>16 We write bt (fk ) for the value of the k-th feature (among K) (where we use the def  usual notation b(f ) = X f (x)b(dx) for any function f deﬁned on X and measure b ∈ M(X)), and denote bt (f ) the vector (of size K) with components bt (fk ). [sent-48, score-1.121]
</p><p>17 Such a policy π : RK → A selects an action At = π(bt (f )), which in turn, yields a new state Xt+1 . [sent-53, score-0.455]
</p><p>18 Except for simple cases, such as in ﬁnite-state ﬁnite-observation processes (where a Viterbi algorithm could be applied (Rabiner, 1989)), and the case of linear dynamics and Gaussian noise (where a Kalman ﬁlter could be used), there is no closed-form representation of the belief state. [sent-54, score-0.183]
</p><p>19 Thus bt must be approximated in our general setting. [sent-55, score-0.289]
</p><p>20 An PF approximates the belief state bt ∈ M(X) by a set of particles (x1:N ) (points of X), which are updated sequentially at each new observation by a transitiont N 1 selection procedure. [sent-63, score-0.833]
</p><p>21 In particular, the belief feature bt (f ) is approximated by N i=1 f (xi ), and t the policy is thus a function that takes as input the activation of the feature f at the position of N 1 the particles: At = π( N i=1 f (xi )). [sent-64, score-0.694]
</p><p>22 We focus on a policy gradient approach: the POMDP is replaced by an optimization problem on the space of policy parameters, and a (stochastic) gradient ascent on J(θ) is considered. [sent-67, score-0.688]
</p><p>23 For that purpose (and this is the object of this work) we investigate the estimation of ∇J(θ) (where the gradient ∇ refers to the derivative w. [sent-68, score-0.158]
</p><p>24 There are many works about such policy gradient approach in the ﬁeld of Reinforcement Learning, see e. [sent-72, score-0.344]
</p><p>25 Here, we explicitly consider a class of policies that are based on a belief state constructed by a PF. [sent-75, score-0.344]
</p><p>26 Our motivations for investigating this case are based on two facts: (1) the belief state represents sufﬁcient statistics for optimality, as mentioned above. [sent-76, score-0.26]
</p><p>27 (2) PFs are a very popular and efﬁcient tool for constructing the belief state in continuous domains. [sent-77, score-0.26]
</p><p>28 After recalling the general approach for evaluating the performance of a PF-based policy (Section 2), we describe (in Section 3. [sent-78, score-0.26]
</p><p>29 We discuss the bias and variance tradeoff and explain the problem of variance explosion when h is small. [sent-80, score-0.28]
</p><p>30 2: We propose a modiﬁed 2  FD estimate for ∇J(θ) which (along the random sample path) has bias O(h2 ) and variance O(1/N ), thus overcomes the drawback of the previous naive method. [sent-86, score-0.28]
</p><p>31 An algorithm is described and illustrated in Section 4 on a simple problem where the optimal policy exhibits a tradeoff between greedy reward optimization and localization. [sent-87, score-0.322]
</p><p>32 Right ﬁgure: PF-based N 1 scheme for POMDPs where the belief feature bt (f ) is approximated by N i=1 f (xi ). [sent-89, score-0.434]
</p><p>33 t  2  Particle Filters (PF)  We ﬁrst describe a generic PF for estimating the belief state based on past observations. [sent-90, score-0.26]
</p><p>34 2 how to estimate the performance of a given policy in simulation. [sent-93, score-0.291]
</p><p>35 , def N i 2001) for details, approximates the belief state bn by an empirical distribution bN = n i=1 wn δxi n 1:N (where δ denotes a Dirac distribution) made of N particles xn . [sent-96, score-0.776]
</p><p>36 It consists in iterating the two following steps: at time t, given observation yt , • Transition step: (also called importance sampling or mutation) a successor particles population x1:N is generated according to the state dynamics from the previous population t def  1:N x1:N . [sent-97, score-0.771]
</p><p>37 The (importance sampling) weights wt = t−1  g(e1:N ,yt ) x PN t xj j=1 g(et ,yt )  are evaluated,  • Selection step: Resample (with replacement) N particles x1:N from the set x1:N according t t k1:N  def  1:N to the weights wt . [sent-98, score-0.646]
</p><p>38 We write x1:N = xt t t  1:N where kt are the selection indices. [sent-99, score-0.472]
</p><p>39 It consists in selecting new particle positions such as to preserve a consistency N N 1 i i i property (i. [sent-103, score-0.328]
</p><p>40 , 1993) chooses the selection indices kt by an independent sampling from the set 1 : N j 1:N i according to a multinomial distribution with parameters wt , i. [sent-107, score-0.319]
</p><p>41 The idea is to replicate the particles in proportion to their weights. [sent-110, score-0.174]
</p><p>42 1 Control of a real system by an PF-based policy We describe in Algorithm 1 how one may use an PF-based policy πθ for the control of a real-world iid system. [sent-120, score-0.575]
</p><p>43 Note that from our deﬁnition of Fµ , the particles are initialized with: x1:N ∼ µ. [sent-121, score-0.174]
</p><p>44 2 Estimation of J(θ) in simulation Now, for the purpose of policy optimization, one should be capable of evaluating the performance of a policy in simulation. [sent-123, score-0.553]
</p><p>45 J(θ), deﬁned by (1), may be estimated in simulation provided that 3  Algorithm 1 Control of a real-world POMDP for t = 1 to n do Observe: yt , Particle transition step: iid 1:N Set x1:N = F (x1:N , at−1 , u1:N ) with u1:N ∼ ν. [sent-124, score-0.276]
</p><p>46 Set wt = t t−1 t−1 t−1  g(e1:N ,yt ) x PN t , xj j=1 g(et ,yt )  Particle resampling step: k1:N 1:N 1:N Set x1:N = xt t where kt are given by the selection step according to the weights wt . [sent-125, score-0.767]
</p><p>47 t N 1 i Select action: at = πθ ( N i=1 f (xt )), end for the dynamics of the state and observation are known. [sent-126, score-0.214]
</p><p>48 the random sample path, written ω (which accounts for the state and observation stochastic dynamics and the random numbers used in the PF-based policy), we write J(θ) = Eω [Jω (θ)], where def  Jω (θ) =  n t=1  r(Xt,ω (θ)), making the dependency of the state w. [sent-130, score-0.693]
</p><p>49 Algorithm 2 describes how to evaluate an PF-based policy in simulation. [sent-134, score-0.26]
</p><p>50 Algorithm 2 Estimation of Jω (θ) in simulation for t = 1 to n do Deﬁne state: xt = F (xt−1 , at−1 , ut−1 ) with ut−1 ∼ ν, Deﬁne observation: yt = G(xt , vt ) with vt ∼ ν, Particle transition step: iid 1:N 1:N Set xt = F (x1:N , at−1 , u1:N ) with u1:N ∼ ν. [sent-142, score-1.066]
</p><p>51 Set wt = t−1 t−1 t−1  g(e1:N ,yt ) x PN t , xj j=1 g(et ,yt )  Particle resampling step: k1:N 1:N 1:N Set x1:N = xt t where kt are given by the selection step according to the weights wt , t N 1 Select action: at = πθ ( N i=1 f (xi )), t end for def n N Return Jω (θ) = t=1 r(xt ). [sent-143, score-0.977]
</p><p>52 3  A policy gradient approach  Now we want to optimize the value of the parameter in simulation. [sent-144, score-0.344]
</p><p>53 Then, once a “good” parameter θ∗ is found, we would use Algorithm 1 to control the real system using the corresponding PF-based policy πθ∗ . [sent-145, score-0.26]
</p><p>54 , 2005) to use a marginal particle ﬁlter instead of a simple path-based particle ﬁlter. [sent-152, score-0.656]
</p><p>55 We start with a naive Finite Difference (FD) approach and show the problem of variance explosion. [sent-160, score-0.158]
</p><p>56 If the parameter θ is multi-dimensional, the derivative will be calculated def in each direction. [sent-172, score-0.251]
</p><p>57 Thus, it seems natural to estimate Ih by N,M def Ih =  1 1 2h M  M N Jωm (θ m=1  1 + h) − M  M N Jωm′ (θ − h) m′ =1  where we used independent random numbers to evaluate J(θ + h) and J(θ − h). [sent-177, score-0.273]
</p><p>58 This naive FD estimate exhibits the following bias-variance tradeoff (see (Coquelin et al. [sent-179, score-0.19]
</p><p>59 Assume that J(θ) is three times continuously differentiable in a small neighborhood of θ, then the asymptotic (when N → ∞) bias of the naive FD estimate N,M Ih is of order O(h2 ) and its variance is O(N −1 M −1 h−2 ). [sent-181, score-0.261]
</p><p>60 Additional computational resource (larger number of particles N ) will help controlling the variance. [sent-183, score-0.174]
</p><p>61 But if the number of particles is bounded, the variance term will diverge, which may prevent the stochastic gradient algorithm from converging to a local optimum. [sent-189, score-0.373]
</p><p>62 In order to reduce the variance of the previous estimator when h is small, one may use common random numbers to estimate both J(θ + h) and J(θ − h) (i. [sent-190, score-0.217]
</p><p>63 Indeed, for a ﬁxed ω, the selection indices kt (taking values in 1:N a ﬁnite set 1 : N ) are usually a non-smooth function of the weights wt , which depend on θ. [sent-204, score-0.355]
</p><p>64 Therefore the naive FD method using PF cannot be applied in general because of variance explosion of the estimate when h is small, even when using common random number. [sent-205, score-0.249]
</p><p>65 2 Common-indices Finite-Difference method n Let us consider Jω (θ) = t=1 r(Xt,ω (θ)) making explicit the dependency of the state w. [sent-207, score-0.152]
</p><p>66 The function θ → Xt (θ) (for any 1 ≤ t < n) is smooth because all transition functions are smooth, the policy is smooth, and the belief state bt is smooth w. [sent-214, score-0.977]
</p><p>67 Underlying the belief feature bt,θ (f ) dependency w. [sent-218, score-0.182]
</p><p>68 θ, we write: smooth smooth smooth θ −→ bt,θ (f ) −→ Xt (θ) −→ Jω (θ). [sent-221, score-0.18]
</p><p>69 As already mentioned, the problem with the naive FD method is that the PF estimate bN (f ) = t,θ N 1 i i=1 f (xt (θ)) of N 1:N k1:t (θ) which, taken  bt,θ (f ) is not smooth w. [sent-222, score-0.175]
</p><p>70 However, using the same indices means using the same weights during the selection procedure for both trajectories. [sent-228, score-0.177]
</p><p>71 But this would lead to a wrong estimator because the weights strongly depends on θ through the observation function g. [sent-229, score-0.177]
</p><p>72 5  Our idea is thus to use the same selection indices but use a likelihood ratio in the belief feature 1:N estimation. [sent-230, score-0.286]
</p><p>73 More precisely, let us write kt (θ) the selection indices obtained for parameter θ, and ′ consider a parameter θ in a small neighborhood of θ. [sent-231, score-0.301]
</p><p>74 t,θ Thus, for any perturbed value θ′ around θ, we may run an PF where in the resampling step, we 1:N use the same selection indices k1:n (θ) as those obtained for θ. [sent-241, score-0.254]
</p><p>75 The algorithm works by updating 3 families of state, observation, and particle populations, denoted by ’+’, ’-’, and ’o’ for the values of the parameter θ + h, θ − h, and θ respectively. [sent-246, score-0.328]
</p><p>76 For the performance measure deﬁned by (1), the algorithm returns the common indices FD estimator: n N def 1 ∂h Jω = 2h t=1 r(x+ ) − r(x− ) where x+ and x− are upper and lower trajectories simulated t t 1:n 1:n under the random sample path ω. [sent-247, score-0.348]
</p><p>77 Note that although the selection indices are the same, the particle populations ’+’, ’-’, and ’o’ are different, but very close (when h is small). [sent-248, score-0.507]
</p><p>78 Hence the likelihood 1:N ratios lt converge to 1 when h → 0, which avoids a source of variance when h is small. [sent-249, score-0.284]
</p><p>79 def  M  1 M N N The resulting estimator ∂h Jω = M m=1 ∂h Jωm for J(θ) would calculate an average over M sample paths ω1:M of the return of Algorithm 3 called M times. [sent-250, score-0.29]
</p><p>80 This estimator overcomes the drawbacks of the naive FD estimate: Its asymptotic bias is of order O(h2 ) (like any centered FD scheme) but its variance is of order O(N −1 M −1 ) (the Central Limit Theorem applies to the belief N feature estimator (3) thus to ∂h Jω as well). [sent-251, score-0.554]
</p><p>81 The complexity of Algorithm 3 is linear in the number of particles N . [sent-253, score-0.174]
</p><p>82 Note that in the current implementation we used 3 populations of particles per derivative. [sent-254, score-0.212]
</p><p>83 Of course, we could consider a non-centered FD scheme approximating the derivative with J(θ+h)−J(θ) , which is of ﬁrst order but h which only requires 2 particle populations. [sent-255, score-0.369]
</p><p>84 If the parameter is multidimensional, the full gradient estimate could be obtained by using K + 1 populations of particles. [sent-256, score-0.153]
</p><p>85 Of course, in gradient ascent methods, such FD gradient estimate may be advantageously combined with clever techniques such as simultaneous perturbation stochastic approximation (Spall, 2000), conjugate or second-order gradient approaches. [sent-257, score-0.363]
</p><p>86 Note that when h → 0, our estimator converges to an Inﬁnitesimal Perturbation Analysis (IPA) estimator (Glasserman, 1991). [sent-258, score-0.16]
</p><p>87 The advantage of IPA is that it would use one population of particles only (for the full gradient) which may be interesting when the number of parameters K is large. [sent-260, score-0.174]
</p><p>88 4  Numerical Experiment  Because of space constraints, our purpose here is simply to illustrate numerically the theoretical ﬁndings of previous FD methods (in terms of bias-variance contributions) rather than to provide a full example of POMDP policy optimization. [sent-265, score-0.293]
</p><p>89 t=1 2h  def  iid  2 2 of the squared distance to the origin (the goal): yt = ||xt ||2 + vt , where vt ∼ N (0, σy ) (σy is the variance of the noise). [sent-269, score-0.71]
</p><p>90 At each time step, the agent may choose a direction at (with ||at || = 1), which results in moving the state, of a step d, in the corresponding direction: xt+1 = xt + dat + ut , i. [sent-270, score-0.411]
</p><p>91 We consider a class of policies that depend on a single feature belief: the mean of the belief state (i. [sent-275, score-0.344]
</p><p>92 The PF-based policy thus uses the barycenter of def 1 N i ⊥ o i=1 xt . [sent-278, score-0.766]
</p><p>93 The chosen action is thus ||−(1−θ)m+θm⊥ ||  the particle population mt =  consider policies πθ (m) = at = πθ (mt ). [sent-281, score-0.526]
</p><p>94 mt close to xt ), then the policy πθ=0 would move the robot towards the direction of the goal, whereas πθ=1 would move it in an orthogonal direction. [sent-284, score-0.642]
</p><p>95 θ = 0 would correspond to the best feed-back policy if the state was perfectly known. [sent-293, score-0.375]
</p><p>96 Here, the optimal policy exhibits a tradeoff between greedy optimization and localization. [sent-295, score-0.293]
</p><p>97 0188  The table above shows the (empirically measured) bias and variance of the naive FD (NFD) (using common random numbers) method and the common indices FD (CIFD) method, for a speciﬁc value θ = 0. [sent-309, score-0.289]
</p><p>98 Stochastic particle methods for linear tangent ﬁltering equations, e 231–240. [sent-371, score-0.328]
</p><p>99 Feynman-kac formulae, genealogical and interacting particle systems with applications. [sent-382, score-0.361]
</p><p>100 Parameter estimation in general state-space models using particle methods. [sent-405, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('particle', 0.328), ('xt', 0.296), ('bt', 0.289), ('fd', 0.284), ('policy', 0.26), ('def', 0.21), ('pf', 0.192), ('lt', 0.181), ('particles', 0.174), ('yt', 0.173), ('belief', 0.145), ('bn', 0.132), ('state', 0.115), ('ut', 0.115), ('resampling', 0.113), ('ih', 0.112), ('glasserman', 0.1), ('vt', 0.099), ('wt', 0.095), ('indices', 0.092), ('coquelin', 0.087), ('doucet', 0.087), ('pomdp', 0.085), ('naive', 0.084), ('policies', 0.084), ('gradient', 0.084), ('kt', 0.083), ('action', 0.08), ('estimator', 0.08), ('moral', 0.075), ('variance', 0.074), ('del', 0.071), ('pn', 0.064), ('observation', 0.061), ('smooth', 0.06), ('ao', 0.06), ('cifd', 0.06), ('deguest', 0.06), ('douc', 0.06), ('explosion', 0.06), ('ipa', 0.06), ('nfd', 0.06), ('iid', 0.055), ('pomdps', 0.054), ('tadic', 0.052), ('moulines', 0.052), ('dxt', 0.052), ('robot', 0.052), ('overcomes', 0.052), ('selection', 0.049), ('transition', 0.048), ('path', 0.046), ('xo', 0.045), ('write', 0.044), ('observable', 0.043), ('finite', 0.042), ('et', 0.042), ('stochastic', 0.041), ('derivative', 0.041), ('capp', 0.04), ('cmap', 0.04), ('fichoud', 0.04), ('kitagawa', 0.04), ('legland', 0.04), ('limh', 0.04), ('lovejoy', 0.04), ('miclo', 0.04), ('poyadjis', 0.04), ('rou', 0.04), ('perturbation', 0.039), ('bias', 0.039), ('dynamics', 0.038), ('populations', 0.038), ('dependency', 0.037), ('weights', 0.036), ('spall', 0.035), ('andrieu', 0.035), ('mobile', 0.035), ('dyt', 0.035), ('rabiner', 0.035), ('gordon', 0.035), ('mt', 0.034), ('purpose', 0.033), ('interacting', 0.033), ('tradeoff', 0.033), ('neighborhood', 0.033), ('formulae', 0.032), ('filters', 0.032), ('numbers', 0.032), ('markov', 0.031), ('estimate', 0.031), ('ltering', 0.03), ('discontinuity', 0.03), ('fox', 0.03), ('carlo', 0.029), ('ratios', 0.029), ('reward', 0.029), ('monte', 0.028), ('polytechnique', 0.028), ('limn', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="177-tfidf-1" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>2 0.23523208 <a title="177-tfidf-2" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>Author: Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu</p><p>Abstract: This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to obtain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods. 1</p><p>3 0.23398224 <a title="177-tfidf-3" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>Author: Amir M. Farahmand, Mohammad Ghavamzadeh, Shie Mannor, Csaba Szepesvári</p><p>Abstract: In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a ﬂexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2 -regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efﬁcient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide ﬁnite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions. 1</p><p>4 0.20602989 <a title="177-tfidf-4" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><p>5 0.20548984 <a title="177-tfidf-5" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>6 0.18260272 <a title="177-tfidf-6" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>7 0.17308463 <a title="177-tfidf-7" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>8 0.14886783 <a title="177-tfidf-8" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>9 0.14484605 <a title="177-tfidf-9" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>10 0.12746143 <a title="177-tfidf-10" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>11 0.12262097 <a title="177-tfidf-11" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>12 0.1165188 <a title="177-tfidf-12" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>13 0.1150647 <a title="177-tfidf-13" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>14 0.10899372 <a title="177-tfidf-14" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>15 0.10834466 <a title="177-tfidf-15" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>16 0.10333227 <a title="177-tfidf-16" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>17 0.10109226 <a title="177-tfidf-17" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>18 0.099878214 <a title="177-tfidf-18" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>19 0.096070692 <a title="177-tfidf-19" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>20 0.090911955 <a title="177-tfidf-20" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.232), (1, 0.302), (2, -0.064), (3, -0.112), (4, 0.021), (5, 0.305), (6, 0.064), (7, 0.137), (8, 0.095), (9, -0.119), (10, -0.068), (11, -0.169), (12, 0.062), (13, -0.059), (14, -0.036), (15, -0.017), (16, -0.032), (17, -0.079), (18, 0.002), (19, -0.004), (20, 0.016), (21, 0.008), (22, -0.03), (23, -0.031), (24, 0.065), (25, 0.06), (26, 0.051), (27, -0.059), (28, -0.007), (29, 0.017), (30, -0.015), (31, 0.011), (32, -0.009), (33, 0.095), (34, 0.032), (35, -0.057), (36, -0.069), (37, -0.04), (38, -0.028), (39, 0.036), (40, -0.074), (41, -0.077), (42, -0.012), (43, -0.047), (44, -0.092), (45, -0.05), (46, 0.065), (47, -0.07), (48, -0.1), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97251236 <a title="177-lsi-1" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>2 0.71607673 <a title="177-lsi-2" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>Author: Amir M. Farahmand, Mohammad Ghavamzadeh, Shie Mannor, Csaba Szepesvári</p><p>Abstract: In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a ﬂexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2 -regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efﬁcient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide ﬁnite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions. 1</p><p>3 0.67699194 <a title="177-lsi-3" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>Author: Lester W. Mackey</p><p>Abstract: In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets. 1</p><p>4 0.66585046 <a title="177-lsi-4" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>5 0.65711236 <a title="177-lsi-5" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>Author: Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu</p><p>Abstract: This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to obtain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods. 1</p><p>6 0.61229736 <a title="177-lsi-6" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>7 0.61138976 <a title="177-lsi-7" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>8 0.53567523 <a title="177-lsi-8" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>9 0.51996362 <a title="177-lsi-9" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>10 0.50499022 <a title="177-lsi-10" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>11 0.47971436 <a title="177-lsi-11" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>12 0.45986882 <a title="177-lsi-12" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>13 0.45524308 <a title="177-lsi-13" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>14 0.4408147 <a title="177-lsi-14" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>15 0.43717638 <a title="177-lsi-15" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>16 0.42866358 <a title="177-lsi-16" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>17 0.40812755 <a title="177-lsi-17" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>18 0.3975299 <a title="177-lsi-18" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>19 0.34772518 <a title="177-lsi-19" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>20 0.3444483 <a title="177-lsi-20" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (6, 0.06), (7, 0.057), (12, 0.081), (28, 0.193), (38, 0.283), (57, 0.042), (59, 0.02), (63, 0.028), (71, 0.024), (77, 0.057), (83, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80724502 <a title="177-lda-1" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>2 0.79733545 <a title="177-lda-2" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>Author: Jens Kober, Jan R. Peters</p><p>Abstract: Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAMTM robot arm. 1</p><p>3 0.77423882 <a title="177-lda-3" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>Author: Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu</p><p>Abstract: This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to obtain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods. 1</p><p>4 0.73059791 <a title="177-lda-4" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>Author: Ofer Dekel</p><p>Abstract: We present cutoff averaging, a technique for converting any conservative online learning algorithm into a batch learning algorithm. Most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted. An attractive property of our technique is that it preserves the efﬁciency of the original online algorithm, making it appropriate for large-scale learning problems. We provide a statistical analysis of our technique and back our theoretical claims with experimental results. 1</p><p>5 0.63504887 <a title="177-lda-5" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>Author: Amir M. Farahmand, Mohammad Ghavamzadeh, Shie Mannor, Csaba Szepesvári</p><p>Abstract: In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a ﬂexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2 -regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efﬁcient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide ﬁnite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions. 1</p><p>6 0.63261658 <a title="177-lda-6" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>7 0.63052958 <a title="177-lda-7" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>8 0.62908512 <a title="177-lda-8" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>9 0.62670064 <a title="177-lda-9" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>10 0.62632644 <a title="177-lda-10" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>11 0.6260581 <a title="177-lda-11" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>12 0.62588173 <a title="177-lda-12" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>13 0.62582737 <a title="177-lda-13" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>14 0.62527746 <a title="177-lda-14" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>15 0.62462175 <a title="177-lda-15" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>16 0.62419909 <a title="177-lda-16" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>17 0.62339765 <a title="177-lda-17" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>18 0.62332404 <a title="177-lda-18" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>19 0.62320614 <a title="177-lda-19" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>20 0.62212116 <a title="177-lda-20" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
