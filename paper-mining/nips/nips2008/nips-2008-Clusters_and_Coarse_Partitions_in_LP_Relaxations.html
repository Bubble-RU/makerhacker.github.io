<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-49" href="#">nips2008-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</h1>
<br/><p>Source: <a title="nips-2008-49-pdf" href="http://papers.nips.cc/paper/3537-clusters-and-coarse-partitions-in-lp-relaxations.pdf">pdf</a></p><p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>Reference: <a title="nips-2008-49-reference" href="../nips2008_reference/nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. [sent-8, score-0.215]
</p><p>2 Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. [sent-9, score-0.447]
</p><p>3 By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. [sent-10, score-0.383]
</p><p>4 We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. [sent-11, score-0.475]
</p><p>5 We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. [sent-12, score-0.774]
</p><p>6 One promising approach is based on linear programming relaxations, solved via message passing algorithms akin to belief propagation [2, 3]. [sent-23, score-0.464]
</p><p>7 Relaxations can be made increasingly tight by introducing LP variables that correspond to clusters of variables in the original model. [sent-27, score-0.333]
</p><p>8 In fact, in recent work [6] we have shown that by adding a set of clusters over three variables, complex problems such as protein-design and stereo-vision may be solved exactly. [sent-28, score-0.202]
</p><p>9 The problem with adding clusters over variables is that computational cost scales exponentially with the cluster size. [sent-29, score-0.434]
</p><p>10 Using clusters of s variables means adding 100s LP variables, which is computationally demanding even for clusters of size three. [sent-32, score-0.352]
</p><p>11 The key observation is that it may not be necessary to represent all the possible joint states of a cluster of variables. [sent-35, score-0.255]
</p><p>12 Following the approach of [2], we formulate a dual LP for the partition-based LP relaxations and derive a message passing algorithm for optimizing the dual LP based on block coordinate descent. [sent-38, score-0.74]
</p><p>13 Unlike standard message passing algorithms, the algorithm we derive involves passing messages between coarse and ﬁne representations of the same set of variables. [sent-39, score-0.662]
</p><p>14 This is equivalent to ﬁnding the assignment xM that maximizes the function f (x; θ) = ij∈E θij (xi , xj ). [sent-43, score-0.218]
</p><p>15 Deﬁne µ to be a vector of marginal probabilities associated with the interacting pairs of variables (edges) {µij (xi , xj )}ij∈E as well as {µi (xi )}i∈V for the nodes. [sent-45, score-0.308]
</p><p>16 The MAP problem is then equivalent to the following linear program: max f (x; θ) = max µ · θ , x  µ∈M(G)  (2)  where µ · θ = ij∈E xi ,xj θij (xi , xj )µij (xi , xj ). [sent-47, score-0.635]
</p><p>17 The extreme points of the marginal polytope are integral and correspond one-to-one with assignments x. [sent-48, score-0.256]
</p><p>18 Although the number of variables in this LP is only O(|E| + |V |), the difﬁculty comes from an exponential number of linear inequalities typically required to describe the marginal polytope M(G). [sent-50, score-0.219]
</p><p>19 LP relaxations replace the difﬁcult global constraint that the marginals in µ must arise from some common joint distribution by ensuring only that the marginals are locally consistent with one another. [sent-51, score-0.305]
</p><p>20 The most common such relaxation, pairwise consistency, enforces that the edge marginals are consistent with the node marginals, {µ | xj µij (xi , xj ) = µi (xi )}. [sent-52, score-0.617]
</p><p>21 The integral extreme points of this local marginal polytope also correspond to assignments. [sent-53, score-0.256]
</p><p>22 However, the local marginal polytope also contains fractional extreme points, and, as a relaxation, will in general not be tight. [sent-55, score-0.209]
</p><p>23 However, perhaps the most straightforward approach corresponds to lifting the relaxation by adding marginals over clusters of nodes to the model (cf. [sent-58, score-0.342]
</p><p>24 However, each cluster comes with a computational cost that grows as k s , where s is the number of variables in the cluster and k is the number of states for each variable. [sent-60, score-0.525]
</p><p>25 We seek to offset this exponential cost by introducing coarsened clusters, as we show next. [sent-61, score-0.347]
</p><p>26 2  Coarsened clusters and consistency constraints  We begin with an illustrative example. [sent-62, score-0.258]
</p><p>27 We can recover the exact marginal polytope in this case by forcing the pairwise marginals µij (xi , xj ) to be consistent with some distribution µ123 (x1 , x2 , x3 ). [sent-64, score-0.464]
</p><p>28 However, when k is large, introducing the corresponding k 3 variables to our LP may be too costly and perhaps unnecessary, if a weaker consistency constraint would already lead to an integral extreme point. [sent-65, score-0.287]
</p><p>29 zk  xk  xi  zk  zi  zj  zi  Figure 1: A graphical illustration of the consistency constraint between the original (ﬁne granularity) edge (xi xk ) and the coarsened triplet (zi zj zk ). [sent-69, score-1.681]
</p><p>30 Given such a partitioning scheme, we can introduce a distribution over coarsened variables 123 (z1 z2 z3 ) and constrain it to agree with ik (xi xk ) in the sense that they both yield the same marginals for zi zk . [sent-75, score-0.885]
</p><p>31 , 1 2 3 4 , we recover the usual cluster consistency constraint. [sent-80, score-0.276]
</p><p>32 We use the above idea to construct tighter outer bounds on the marginal polytope and incorporate them into the MAP-LP relaxation. [sent-81, score-0.202]
</p><p>33 For each cluster c c C and variable i c we also have a partition Zi as in the above example2 (the choice of clusters and partitions will be discussed later). [sent-83, score-0.409]
</p><p>34 Our LP optimizes over the following marginal variables: ij (xi xj ) i (xi ) for the edges and nodes of the original graph, and c (zc ) for the coarse-grained clusters. [sent-88, score-0.73]
</p><p>35 However, a more efﬁcient and scalable approach is to solve it via message passing in the dual LP, which we show how to do in the next section. [sent-91, score-0.492]
</p><p>36 Our approach for choosing the coarsenings is to iteratively solve the LP using an initial relaxation (beginning with the pairwise consistency constraints), then to introduce additional cluster constraints, letting the current solution guide how to coarsen the variables. [sent-93, score-0.463]
</p><p>37 As we showed in earlier work [6], solving with the dual LP gives us a simple method for “warm starting” the new LP (the tighter relaxation) using the previous solution, and also results in an algorithm for which every step monotonically decreases an upper bound on the MAP assignment. [sent-94, score-0.205]
</p><p>38 3  Dual linear program and a message passing algorithm  In this section we give the dual of the partition-based LP from Eq. [sent-98, score-0.492]
</p><p>39 5, and use it to obtain a message passing algorithm to efﬁciently optimize this relaxation. [sent-99, score-0.345]
</p><p>40 Our approach extends earlier work by Globerson and Jaakkola [2] who gave the generalized max-product linear programming (MPLP) algorithm to solve the usual (non-coarsened) cluster LP relaxation in the dual. [sent-100, score-0.3]
</p><p>41 The dual formulation in [2] was derived by adding auxiliary variables to the primal. [sent-101, score-0.255]
</p><p>42 The dual variables are as follows: βij→i (xi , xj ), βij→j (xi , xj ), βij→ij (xi , xj ) for every edge ij ∈ E, and βc→ij (zc ) for every coarsened cluster c and edge ij ∈ c. [sent-104, score-2.341]
</p><p>43 Thus λij→i (xi ) should be read as the message sent from edge c c ij to node i, and λc→ij (zi , zj ) is the message from the coarsened cluster to one of its intersection edges. [sent-106, score-1.654]
</p><p>44 Finally, λij→ij (xi , xj ) is the message sent from an edge to itself. [sent-107, score-0.506]
</p><p>45 By convex duality, the dual objective evaluated at a dual feasible point upper bounds the primal LP optimum, which in turn upper bounds the value of the MAP assignment. [sent-112, score-0.355]
</p><p>46 It is illustrative to compare this dual LP with [2] where the cluster dual variables were βc→ij (xc ). [sent-113, score-0.563]
</p><p>47 Our dual corresponds to introducing the additional constraint that βc→ij (xc ) = βc→ij (xc ) whenever zc [xc ] = zc [xc ]. [sent-114, score-0.798]
</p><p>48 The advantage of the above dual is that it can be optimized via a simple message passing algorithm that corresponds to block coordinate descent. [sent-115, score-0.492]
</p><p>49 It then turns out that one does not need to work with β variables directly, but can keep only the λ message variables. [sent-117, score-0.26]
</p><p>50 2 provides the form of the updates for all three message types. [sent-119, score-0.194]
</p><p>51 Importantly, all messages outgoing from a cluster or edge must be sent simultaneously. [sent-123, score-0.372]
</p><p>52 Here we derive the cluster to edge updates, which differ from [2]. [sent-124, score-0.258]
</p><p>53 Assume that all values of β are c c ﬁxed except for βc→ij (zi , zj ) for all ij ∈ c in some cluster c. [sent-125, score-0.806]
</p><p>54 The term in the dual objective that c c depends on βc→ij (zi , zj ) can be written equivalently as c c c c λc →ij (zi [xi ], zj [xj ]) + λc→ij (zi [xi ], zj [xj ])  max λij→ij (xi , xj ) + xi ,xj  =  c :c =c,ij∈c  c c c c max bij (zi , zj ) + λc→ij (zi [xi ], zj [xj ]) . [sent-126, score-1.526]
</p><p>55 c c zi ,zj  (11)  Due to the constraint ij∈c βc→ij (zc ) = 0, all of the βc→ij need to be updated simultaneously. [sent-127, score-0.282]
</p><p>56 It can be easily shown (using an equalization argument as in [2]) that the βc→ij (zc ) that satisfy the constraint and minimize the objective are given by 1 c c c c βc→ij (zc ) = −bij (zi , zj ) + bst (zs , zt ). [sent-128, score-0.324]
</p><p>57 (12) |S(c)| st∈c The message update given in Fig. [sent-129, score-0.194]
</p><p>58 Note that none of the cluster messages involve the original cluster variables xc , but rather only zc . [sent-131, score-0.869]
</p><p>59 • Edge to Node: For every edge ij ∈ E and node i (or j) in the edge: 1 2 λij→i (xi )←− λ−j (xi )+ max 3 i 3 xj where  λ−j (xi ) i  =  k∈N (i)\j  c c λc→ij (zi [xi ], zj [xj ])+λij→ij (xi , xj )+λ−i (xj )+θij (xi , xj ) j c:ij∈c  λik→i (xi ). [sent-133, score-1.348]
</p><p>60 2 solves the dual for a given choice of coarsened clusters. [sent-137, score-0.446]
</p><p>61 Our overall algorithm is thus similar in structure to [6] and proceeds as follows (we denote the message passing algorithm from Fig. [sent-140, score-0.345]
</p><p>62 Add a new coarsened cluster c using the strategy given in Sec. [sent-146, score-0.503]
</p><p>63 Initialize messages going out of the new cluster c to zero, and keep all the previous message values (this will not change the bound value), 6. [sent-148, score-0.477]
</p><p>64 4  Choosing coarse partitions  Until now we have not discussed how to choose the clusters to add and their partitionings. [sent-150, score-0.334]
</p><p>65 , the set of all triplets in the graph as in [6]), we would like to add a cluster that would result in the maximum decrease of the dual bound on the MAP. [sent-154, score-0.584]
</p><p>66 In principle such a cluster could be found by optimizing the dual for each candidate cluster, then choosing the best one. [sent-155, score-0.351]
</p><p>67 However, this is computationally costly, so in [6] we instead use the bound decrease resulting from just once sending messages from the candidate cluster to its intersection edges. [sent-156, score-0.36]
</p><p>68 If we were to add the full (un-coarsened) cluster, this bound decrease would be: max bij (xi , xj ) − max  d(c) = ij∈c  xi ,xj  where bij (xi , xj ) = λij→ij (xi , xj ) +  xc  c:ij∈c  bij (xi , xj ),  (13)  ij∈c  c c λc→ij (zi [xi ], zj [xj ]). [sent-157, score-1.625]
</p><p>69 Our strategy now is as follows: we add the cluster c that maximizes d(c), and then choose a partic tioning Zi for all i ∈ c that is guaranteed to achieve a decrease that is close to d(c). [sent-158, score-0.306]
</p><p>70 We will therefore consider partitions where the k states with lowest belief values are put into the same “catch-all” coarse state sc , and all other states of xi get their own coarse state. [sent-164, score-0.768]
</p><p>71 Formally, a partition i c Zi is characterized by a value κi such that sc is the set of all xi with bi (xi ) < κi . [sent-165, score-0.342]
</p><p>72 Since each subsequent value of sc differs by one additional state xi , we can re-use the maximizations over zc\i for the i previous value of sc in evaluating the constraint for the current sc . [sent-172, score-0.546]
</p><p>73 Setting γ < 0 would give a partitioning with fewer coarse states at the cost of a smaller guaranteed bound decrease. [sent-174, score-0.312]
</p><p>74 On the other hand, setting γ > 0 results in a margin between the value of the dual objective (after sending the coarsened cluster message) and its value if we were to ﬁx xi in the max terms of Eq. [sent-175, score-0.902]
</p><p>75 This makes it less likely that a state i in sc will become important again in subsequent message passing iterations. [sent-177, score-0.474]
</p><p>76 Note that this greedy algorithm does not necessarily ﬁnd the partitioning with the fewest number of coarse states that achieves the bound decrease. [sent-179, score-0.26]
</p><p>77 Recently we showed [6] that all but one of the problems described in [9] can be solved exactly by using a LP relaxation with clusters on three variables. [sent-187, score-0.257]
</p><p>78 In both experimental setups we ﬁrst run the standard edge-based message passing algorithm for 1000 iterations. [sent-191, score-0.345]
</p><p>79 In the ﬁrst experiment, we add all triplets that correspond to variables whose single node beliefs are tied (within 10−5 ) at the maximum after running the edge-based algorithm. [sent-192, score-0.481]
</p><p>80 Since tied beliefs correspond to fractional LP solutions, it is natural to consider these in tighter relaxations. [sent-193, score-0.207]
</p><p>81 The triplets correspond to partitioned variables, as explained in Sec. [sent-194, score-0.196]
</p><p>82 Speciﬁcally, for each variable Xi we ﬁnd states whose single node beliefs are tied at the maximum. [sent-197, score-0.249]
</p><p>83 The triplets are then constructed over the coarsened c variables Zi and the message passing algorithm of Sec. [sent-207, score-0.86]
</p><p>84 We add new triplets corresponding to the new variables and re-run. [sent-211, score-0.26]
</p><p>85 We repeat until the dual-LP bound is sufﬁciently close to the value of the integral assignment obtained from the messages (note that these values would not coincide if the relaxation were not tight; in these experiments they do, so the ﬁnal relaxation is tight). [sent-212, score-0.373]
</p><p>86 We applied the above scheme to the ten smallest proteins in the dataset used in [6] (for the larger proteins we used a different strategy described next). [sent-213, score-0.206]
</p><p>87 4 This big factor comes about because a very small number of states are tied per variable, thus increasing the efﬁciency of our method where the number of partitions is equal to the number of tied states. [sent-217, score-0.279]
</p><p>88 While running on full triplets was completely impractical, the coarsened message passing algorithm is very practical and achieves the exact MAP assignments. [sent-218, score-0.818]
</p><p>89 3), alternating between adding 5 triplets to the relaxation and running MPLP for 20 more iterations. [sent-220, score-0.313]
</p><p>90 6 To compare the running times on all 15 proteins, we checked how long it took for the difference between the dual and primal objectives to be less than . [sent-230, score-0.2]
</p><p>91 The cost per iteration increases very little after adding each triplet, showing that our algorithm signiﬁcantly coarsened the clusters. [sent-237, score-0.364]
</p><p>92 Two triplet clusters were added twice using different coarsenings, but otherwise each triplet only needed to be added once, demonstrating that our algorithm chose the right coarsenings. [sent-239, score-0.24]
</p><p>93 In applying the method, we chose to cluster variables’ states based a bound minimization criterion after solving using a looser constraint on the polytope. [sent-245, score-0.33]
</p><p>94 One of the key technical differences is that in our formulation the setting of coarse and ﬁne variables are reﬁned iteratively whereas in [1], once a coarse MRF has been solved, it is not revisited. [sent-252, score-0.26]
</p><p>95 Using the same ideas as in this paper, one can introduce coarsened pairwise consistency constraints in addition the full pairwise consistency constraints. [sent-254, score-0.566]
</p><p>96 Although this would not tighten the relaxation, by passing messages more frequently in the coarsened space, and only occasionally revisiting the full edges, this could give signiﬁcant computational beneﬁts when the nodes have large numbers of states. [sent-255, score-0.519]
</p><p>97 With the coarsening strategy used here, the number of variables still grows exponentially with the cluster size, albeit at a lower rate. [sent-257, score-0.343]
</p><p>98 One way to avoid the exponential growth is to partition the states of a cluster into a ﬁxed number of states (e. [sent-258, score-0.364]
</p><p>99 Such a process may be repeated recursively, generating a hierarchy of coarsened variables. [sent-261, score-0.299]
</p><p>100 Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. [sent-279, score-0.345]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ij', 0.462), ('coarsened', 0.299), ('zc', 0.292), ('zi', 0.24), ('lp', 0.223), ('message', 0.194), ('xj', 0.19), ('xi', 0.181), ('cluster', 0.181), ('zj', 0.163), ('passing', 0.151), ('triplets', 0.15), ('dual', 0.147), ('clusters', 0.122), ('polytope', 0.101), ('relaxations', 0.101), ('coarse', 0.097), ('relaxation', 0.097), ('sc', 0.097), ('bij', 0.087), ('mplp', 0.083), ('marginals', 0.081), ('xc', 0.08), ('edge', 0.077), ('proteins', 0.075), ('states', 0.074), ('consistency', 0.073), ('coarsening', 0.073), ('map', 0.072), ('partitions', 0.071), ('beliefs', 0.069), ('messages', 0.069), ('tied', 0.067), ('sontag', 0.067), ('variables', 0.066), ('protein', 0.061), ('triplet', 0.059), ('bst', 0.058), ('partitioning', 0.056), ('marginal', 0.052), ('coarsenings', 0.05), ('integral', 0.049), ('zk', 0.048), ('belief', 0.045), ('sent', 0.045), ('globerson', 0.045), ('add', 0.044), ('constraint', 0.042), ('adding', 0.042), ('constraints', 0.041), ('pairwise', 0.04), ('node', 0.039), ('solved', 0.038), ('max', 0.037), ('propagation', 0.036), ('mrf', 0.036), ('partition', 0.035), ('constrain', 0.034), ('coarser', 0.034), ('zs', 0.034), ('meltzer', 0.033), ('tightening', 0.033), ('bound', 0.033), ('scheme', 0.033), ('tight', 0.032), ('extreme', 0.032), ('state', 0.032), ('objective', 0.032), ('ik', 0.031), ('agree', 0.03), ('primal', 0.029), ('xm', 0.029), ('tommi', 0.029), ('decrease', 0.029), ('zt', 0.029), ('guaranteed', 0.029), ('bi', 0.029), ('assignment', 0.028), ('edges', 0.026), ('granularity', 0.025), ('sending', 0.025), ('partitionings', 0.025), ('introducing', 0.025), ('tighter', 0.025), ('outer', 0.024), ('running', 0.024), ('partitioned', 0.024), ('fractional', 0.024), ('csail', 0.024), ('st', 0.023), ('candidate', 0.023), ('strategy', 0.023), ('cost', 0.023), ('illustrative', 0.022), ('mrfs', 0.022), ('usual', 0.022), ('guide', 0.022), ('correspond', 0.022), ('ner', 0.022), ('uai', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="49-tfidf-1" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>2 0.17276636 <a title="49-tfidf-2" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><p>3 0.15730946 <a title="49-tfidf-3" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>4 0.15194163 <a title="49-tfidf-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>5 0.14594682 <a title="49-tfidf-5" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>6 0.12518138 <a title="49-tfidf-6" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>7 0.11998736 <a title="49-tfidf-7" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>8 0.11583425 <a title="49-tfidf-8" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>9 0.11520814 <a title="49-tfidf-9" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>10 0.11203903 <a title="49-tfidf-10" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>11 0.10422265 <a title="49-tfidf-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.10334872 <a title="49-tfidf-12" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>13 0.095943607 <a title="49-tfidf-13" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>14 0.092522688 <a title="49-tfidf-14" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>15 0.088734612 <a title="49-tfidf-15" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>16 0.086520508 <a title="49-tfidf-16" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>17 0.085810237 <a title="49-tfidf-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.079388306 <a title="49-tfidf-18" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>19 0.079242319 <a title="49-tfidf-19" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>20 0.077459112 <a title="49-tfidf-20" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.234), (1, -0.055), (2, -0.088), (3, 0.078), (4, 0.038), (5, -0.065), (6, 0.012), (7, -0.085), (8, -0.067), (9, -0.144), (10, -0.05), (11, 0.055), (12, 0.174), (13, 0.024), (14, -0.104), (15, 0.124), (16, -0.005), (17, 0.04), (18, 0.043), (19, -0.097), (20, 0.132), (21, -0.116), (22, -0.171), (23, 0.076), (24, 0.076), (25, -0.231), (26, 0.055), (27, 0.111), (28, 0.045), (29, 0.006), (30, -0.14), (31, 0.051), (32, -0.088), (33, 0.025), (34, 0.044), (35, 0.012), (36, -0.121), (37, 0.066), (38, 0.003), (39, 0.057), (40, -0.048), (41, 0.141), (42, -0.063), (43, -0.114), (44, 0.008), (45, -0.031), (46, 0.016), (47, -0.086), (48, -0.012), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96987021 <a title="49-lsi-1" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>2 0.56800425 <a title="49-lsi-2" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>Author: Tom Minka, John Winn</p><p>Abstract: Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates. 1</p><p>3 0.54672951 <a title="49-lsi-3" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>4 0.54317242 <a title="49-lsi-4" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>5 0.53600717 <a title="49-lsi-5" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>Author: Michael Isard, John MacCormick, Kannan Achan</p><p>Abstract: Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a ﬁxed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense. 1</p><p>6 0.4992457 <a title="49-lsi-6" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>7 0.498216 <a title="49-lsi-7" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>8 0.4882417 <a title="49-lsi-8" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>9 0.45510072 <a title="49-lsi-9" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>10 0.44183809 <a title="49-lsi-10" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>11 0.42099896 <a title="49-lsi-11" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>12 0.42050546 <a title="49-lsi-12" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>13 0.40926403 <a title="49-lsi-13" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>14 0.40541324 <a title="49-lsi-14" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>15 0.37436149 <a title="49-lsi-15" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>16 0.37104616 <a title="49-lsi-16" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>17 0.35640264 <a title="49-lsi-17" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>18 0.34976935 <a title="49-lsi-18" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>19 0.34637263 <a title="49-lsi-19" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>20 0.34265175 <a title="49-lsi-20" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.069), (7, 0.068), (12, 0.044), (28, 0.197), (34, 0.214), (57, 0.064), (59, 0.012), (63, 0.032), (71, 0.02), (77, 0.109), (78, 0.017), (83, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84649962 <a title="49-lda-1" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>2 0.83921933 <a title="49-lda-2" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>3 0.80788285 <a title="49-lda-3" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>4 0.76729977 <a title="49-lda-4" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>5 0.74818164 <a title="49-lda-5" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>Author: Gerhard Neumann, Jan R. Peters</p><p>Abstract: Recently, ﬁtted Q-iteration (FQI) based methods have become more popular due to their increased sample efﬁciency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simpliﬁed to an inexpensive advantageweighted regression. With this result, we are able to derive a new, computationally efﬁcient FQI algorithm which can even deal with high dimensional action spaces. 1</p><p>6 0.74478209 <a title="49-lda-6" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>7 0.74439305 <a title="49-lda-7" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>8 0.74408752 <a title="49-lda-8" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>9 0.7398349 <a title="49-lda-9" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>10 0.73921669 <a title="49-lda-10" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>11 0.73916042 <a title="49-lda-11" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>12 0.73855937 <a title="49-lda-12" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>13 0.73760909 <a title="49-lda-13" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>14 0.73700625 <a title="49-lda-14" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>15 0.73659372 <a title="49-lda-15" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>16 0.73618037 <a title="49-lda-16" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>17 0.73547709 <a title="49-lda-17" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>18 0.73526007 <a title="49-lda-18" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>19 0.73510158 <a title="49-lda-19" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>20 0.73475033 <a title="49-lda-20" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
