<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-6" href="#">nips2008-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</h1>
<br/><p>Source: <a title="nips-2008-6-pdf" href="http://papers.nips.cc/paper/3533-a-shape-aware-model-for-semi-supervised-learning-of-objects-and-its-context.pdf">pdf</a></p><p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>Reference: <a title="nips-2008-6-reference" href="../nips2008_reference/nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 1  Abstract We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. [sent-12, score-0.674]
</p><p>2 We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. [sent-13, score-0.866]
</p><p>3 Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). [sent-14, score-0.47]
</p><p>4 We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. [sent-15, score-0.604]
</p><p>5 Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. [sent-16, score-1.051]
</p><p>6 Syntactical rules are generally not required for extracting knowledge about context - a topic model is generally sufﬁcient for contextual analysis in text [14, 15]. [sent-23, score-0.384]
</p><p>7 We use analogous reasoning to suggest a similar dichotomy in representing object structure and context in vision. [sent-24, score-0.43]
</p><p>8 Our approach combines bag-of-words and spatial models to capture semantics and syntactic rules, respectively, that are employed for recognizing an object using its appearance, structure and context. [sent-25, score-0.641]
</p><p>9 We treat an object and a scene analogous to a sentence and a document respectively. [sent-26, score-0.416]
</p><p>10 Similar to documents, object recognition in natural scenes requires modeling spatial relationships of image features(words) within the object but for representing context in a scene, a bag-of-words approach sufﬁces (See Figure 1 (a) and (b)). [sent-27, score-1.155]
</p><p>11 Learning such a model from weakly labeled data requires labeling the features in an image as belonging to an object or its context (informative background). [sent-28, score-0.955]
</p><p>12 Approaches for learning a dense bag-of-features model with spatial constraints from weakly labeled data have also been proposed. [sent-31, score-0.446]
</p><p>13 The red color shows the features on the foreground car. [sent-34, score-0.533]
</p><p>14 (b) We use a spatial model of the object and a bag-of-words approach for context representation. [sent-36, score-0.667]
</p><p>15 (c) Importance of using contour information: Objects such as signs become part of the foreground since they occur at consistent relative location to the car. [sent-37, score-0.786]
</p><p>16 If shape and contour information is combined with co-occurrence and spatial structure of image features, then such mis-labellings can be reduced. [sent-38, score-0.789]
</p><p>17 Problem: Learn the parameters of object model given the images (I1 , . [sent-40, score-0.455]
</p><p>18 Approach: Simultaneous localization the object in training images and estimation of model parameters. [sent-45, score-0.503]
</p><p>19 Feature Statistics: The image features satisfy the co-occurrence and spatial statistics of the model. [sent-48, score-0.442]
</p><p>20 Shape Similarity: The shape of the foreground object is similar to the shape of the sketch of the object. [sent-50, score-1.196]
</p><p>21 Separation: The object and background features should be separated by the object boundary contours. [sent-52, score-1.074]
</p><p>22 We overcome this problem by applying shape based constraints while constructing the foreground model. [sent-54, score-0.628]
</p><p>23 We add two constraints to the labeling problem using the contour information: (a) The ﬁrst constraint requires the presence of strong intervening contours between foreground and background features. [sent-56, score-1.272]
</p><p>24 (b) The second constraint requires the shape of boundary contours be similar to the shape of the exemplar/sketch provided with the weakly labeled dataset. [sent-57, score-0.978]
</p><p>25 This allows us to learn object models from images where there is signiﬁcant clutter and in which the object does not cover a signiﬁcant part of the image. [sent-58, score-0.841]
</p><p>26 Our approach ﬁrst labels the image features based on co-occurrence and spatial statistics - the features that occur in positive images and exhibit strong spatial relationships are labeled as foreground features. [sent-60, score-1.407]
</p><p>27 Based on the labels of image features, object boundaries are identiﬁed based on how well they separate foreground and background features. [sent-61, score-1.022]
</p><p>28 This is followed by a shape matching step which identiﬁes the object boundary contours based on their expected shape. [sent-62, score-1.021]
</p><p>29 This step prunes many contours and provides a better estimate of object boundaries. [sent-63, score-0.663]
</p><p>30 1 Related Work Many graphical models for object recognition [11] have been inspired by models of text documents such as LDA [6] and pLSA [7]. [sent-68, score-0.414]
</p><p>31 These models are computationally efﬁcient because they ignore the spatial relationships amongst image features (or parts) and use a dense object representation. [sent-69, score-0.835]
</p><p>32 However, ignoring spatial relationships between features leads to problems (See Figure 1(a)). [sent-70, score-0.395]
</p><p>33 In contrast, approaches that model spatial relationships [9, 5] between object parts/features are com-  Figure 2: Shape-Aware Learning (Overview): We ﬁrst compute feature labels using the Gibbs sampling approach on the Spatial Author Topic model. [sent-71, score-0.745]
</p><p>34 The features labeled foreground and background are drawn in red and yellow respectively. [sent-72, score-0.697]
</p><p>35 The object boundaries are identiﬁed based on how well they separate foreground and background features. [sent-74, score-0.901]
</p><p>36 Likely object boundary contours are then matched to the sketch using a voting-based approach and the contours consistent with the shape of the sketch are identiﬁed. [sent-75, score-1.366]
</p><p>37 These contours are then used to relabel the features using the same separation principle. [sent-76, score-0.488]
</p><p>38 These approaches fail under occlusion due to their sparse representation and their stringent requirement of a one-one correspondence between image and object features. [sent-79, score-0.44]
</p><p>39 There has been recent work in applying spatial constraints to topic models which enforce neighboring features to belong to similar topics [10, 2] for the purpose of segmentation. [sent-80, score-0.711]
</p><p>40 Our work is more related to classiﬁcation based approaches [8, 3] that model spatial locations of detected features based on a reference location in the image. [sent-81, score-0.541]
</p><p>41 This was achieved by marginalizing object locations and scale. [sent-86, score-0.4]
</p><p>42 Each object location hypothesis provides a foreground segmentation which can be used for learning the model. [sent-87, score-0.834]
</p><p>43 Additionally, they are subject to modeling errors if the object of interest is small in the training images. [sent-89, score-0.383]
</p><p>44 Our goal is to simultaneously learn an object model and its context model from weakly labeled images. [sent-90, score-0.631]
</p><p>45 To learn context we require real world scenes of object and their natural surrounding environment (high clutter and small objects). [sent-91, score-0.496]
</p><p>46 Our approach resolves the foreground/background labeling ambiguities by requiring that the shapes of the foreground object across the training images to be similar to a sketch exemplar. [sent-93, score-1.014]
</p><p>47 However, contour matching is an expensive(exponential) problem due to the need to select the best subset of contours from the set of all edges that match the shape model. [sent-95, score-0.822]
</p><p>48 We propose an efﬁcient approach that iterates between an co-occurence based labeling and contour based labeling of features. [sent-97, score-0.57]
</p><p>49 2  Our Approach - Integrating feature and contour based cues  We assume the availability of a database of weakly labeled images which specify the presence of an object, but not its location. [sent-98, score-0.569]
</p><p>50 Each word is associated with a topic and an author (the object). [sent-101, score-0.469]
</p><p>51 The topic distribution depends on the associated author and the word distribution depends on the assigned topic (Section 2. [sent-102, score-0.71]
</p><p>52 These assignments are then used to obtain a set of likely object boundary contours in each image. [sent-107, score-0.837]
</p><p>53 These contours are subsequently analyzed to identify the object “centers” and ﬁnal object contours by matching with the shape exemplar(Section 2. [sent-108, score-1.561]
</p><p>54 We ﬁrst provide a brief description of the author topic model, shown in ﬁgure 3(a). [sent-113, score-0.395]
</p><p>55 A topic (zi ) is chosen from a distribution of topics speciﬁc to the selected author and a word (wi ) is generated from that topic. [sent-116, score-0.561]
</p><p>56 The distribution of topics (θ) for each author is chosen from a symmetric Dirichlet(α) prior and the distribution of words (φ) for a topic is chosen from symmetric Dirichlet (β) prior. [sent-117, score-0.554]
</p><p>57 Our model extends the author topic model by including the spatial(syntactical) relationship between features. [sent-119, score-0.467]
</p><p>58 Our goal is not only to model the distribution of type of features but also to model the distribution of spatial locations of the subset of these features that are associated with the foreground object. [sent-121, score-1.05]
</p><p>59 Each feature (wi , li ) is ‘authored’ by an author xi which is described by its type oi 2 and its location ri . [sent-123, score-0.703]
</p><p>60 Topic zi for each word is chosen from a distribution of topic speciﬁc to the type of object oi and a word wi is generated from that topic. [sent-125, score-1.17]
</p><p>61 The distribution of topics (θ) for each object type is chosen from a symmetric Dirichlet (α) distribution3 . [sent-126, score-0.52]
</p><p>62 The location of each feature, li , is sampled from the distribution p(li |oi , zi , ri ) using the following distribution: p(li |oi , zi , ri ) = exp(  −||li − ri ||2 oi ,zi )ζri (li ) 2 σs  (1)  2 For an image with label car, the possible object types are car, and context of car. [sent-128, score-1.277]
</p><p>63 The second term enforces spatial constraints on the location of the feature that is generated by topic (zi ). [sent-132, score-0.583]
</p><p>64 Each feature in the foreground can lie in B possible bins with respect to the reference location. [sent-134, score-0.511]
</p><p>65 The distribution of the spatial location of a feature is speciﬁc to the topic zi and the type of object oi . [sent-135, score-1.295]
</p><p>66 Since we do not want to enforce spatial constraints on the locations of the features generated by topics from context, we set ζ to a constant when oi corresponds to the context of some object. [sent-137, score-0.816]
</p><p>67 noi represents the number of features z oi that are assigned to topic zi and author of type oi and n represents the total number of features assigned to author oi . [sent-141, score-2.047]
</p><p>68 Bi represents the spatial bin in which feature i lies in when the reference is ri , noii,zi represents the number of features from object type oi and topic zi which lie in bin Bi , noi ,zi B represents the total number of features from object type oi and topic zi . [sent-142, score-2.954]
</p><p>69 However, this approach has some shortcomings: (a) If there are features in the background that exhibit a strong spatial relationship with the object, they can be labeled as foreground. [sent-150, score-0.528]
</p><p>70 (b) In clutter, the labeling performance diminishes as the discriminability of the object is lower. [sent-151, score-0.504]
</p><p>71 The labeling performance can, however, be improved if contour cues are utilized. [sent-152, score-0.453]
</p><p>72 We do this by requiring that the shape of the object boundary contours extracted based on feature labeling should be similar to a sketch of the object provided in the dataset. [sent-153, score-1.603]
</p><p>73 Thus, the labeling of features into foreground and background is not only governed by co-occurrence and structural information, but also by shape similarity. [sent-154, score-0.963]
</p><p>74 Shape matching using contours has, in the worst case, exponential complexity since it requires selection of the subset of contours that best constitute the foreground boundary. [sent-156, score-1.029]
</p><p>75 The spatial author-topic model is used to attend to the contours which are likely to be object boundaries. [sent-158, score-0.923]
</p><p>76 Our shape matching module has three steps: (a) Extracting object boundaries based on labels extracted from the spatial author topic model. [sent-159, score-1.342]
</p><p>77 Figure 4: Extraction of object boundaries consistent with the shape of exemplar. [sent-162, score-0.622]
</p><p>78 The ﬁrst step is extraction of contours which separate foreground and background features. [sent-163, score-0.781]
</p><p>79 Each contour in the image is matched to every contour in the model to extract the center of the object. [sent-165, score-0.72]
</p><p>80 The votes are then traced back to identify the contours consistent with the shape model. [sent-166, score-0.582]
</p><p>81 Our goal is to extract boundary contours of the object using the feature labels. [sent-173, score-0.835]
</p><p>82 Since, the boundary contours separates foreground and background features, an estimate of the number of foreground and background features on each side of an image contour provides evidence as to whether that image contour is part of the object boundary. [sent-174, score-2.647]
</p><p>83 For each contour, we measure the number of foreground and background features that lie on each side of the contour within some ﬁxed distance of the contour. [sent-175, score-0.961]
</p><p>84 Shape Matching: Given the probabilities of each contour being a part of the object boundary, we estimate the object center using a voting-based approach [18]. [sent-177, score-1.044]
</p><p>85 Each contour votes for the center of the object where the weight of the vote is determined based on how well the contour matches the sketch. [sent-178, score-1.048]
</p><p>86 Once the candidate location of the center of object is selected, we trace back the votes to estimate the new boundary of the object. [sent-180, score-0.655]
</p><p>87 Figure 4 shows an example of the voting process and boundary contours extracted using this approach. [sent-181, score-0.471]
</p><p>88 Extracting New Labels: These boundaries are then used to relabel the image features into foreground and background. [sent-182, score-0.717]
</p><p>89 Each boundary contour votes as to whether a feature should be labeled foreground or background. [sent-184, score-1.008]
</p><p>90 If the feature lies on the same side as the object center, then the contour votes for the feature as foreground. [sent-185, score-0.859]
</p><p>91 Votes are weighted based on the probability of a contour being an object boundary. [sent-186, score-0.648]
</p><p>92 At each iteration, the author topic distribution changes, which requires retraining the model using Gibbs sampling. [sent-194, score-0.431]
</p><p>93 Figure 6 show some of the cases where both author-topic and author-topic model with spatial constraints fail due to high clutter or the foreground object being too small in the training dataset. [sent-202, score-1.13]
</p><p>94 t=0  t=2  t=0  t=2  Figure 6: Two examples of how the “shape aware” model provides better localization compared to spatial author topic models. [sent-204, score-0.682]
</p><p>95 The odd columns show the results of the author topic model (the initialization point of iterative approach). [sent-205, score-0.49]
</p><p>96 Recall ratio is deﬁned as the ratio of features labeled as foreground to the total number of foreground features. [sent-224, score-1.025]
</p><p>97 Precision is deﬁned as the ratio of features correctly labeled as foreground to the total number of features labeled as foreground. [sent-225, score-0.849]
</p><p>98 In the case of labeling in training data, our approach outperforms both author-topic and spatial author-topic model. [sent-226, score-0.387]
</p><p>99 Low recall rates in our model and the spatial author-topic model is because some foreground features do not satisfy the spatial  Figure 8: Example of performance of three models on a test image. [sent-231, score-1.078]
</p><p>100 Fei-Fei, Spatially coherent latent topic model for concurrent object segmentation and classiﬁcation, ICCV 2007. [sent-275, score-0.641]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('foreground', 0.393), ('object', 0.362), ('contours', 0.301), ('contour', 0.286), ('oi', 0.242), ('spatial', 0.224), ('topic', 0.22), ('shape', 0.201), ('noi', 0.197), ('author', 0.175), ('aware', 0.144), ('labeling', 0.142), ('features', 0.14), ('boundary', 0.123), ('zi', 0.099), ('background', 0.087), ('votes', 0.08), ('ri', 0.079), ('image', 0.078), ('labeled', 0.077), ('weakly', 0.075), ('word', 0.074), ('topics', 0.071), ('clutter', 0.06), ('gibbs', 0.059), ('boundaries', 0.059), ('li', 0.059), ('images', 0.057), ('location', 0.056), ('dirichlet', 0.054), ('assignments', 0.051), ('signs', 0.051), ('feature', 0.049), ('relabel', 0.047), ('reference', 0.047), ('extracting', 0.045), ('context', 0.045), ('syntactical', 0.043), ('labels', 0.043), ('type', 0.043), ('sketch', 0.039), ('locations', 0.038), ('contextual', 0.038), ('disambiguation', 0.038), ('model', 0.036), ('clj', 0.036), ('nbi', 0.036), ('nzi', 0.036), ('nzii', 0.036), ('od', 0.036), ('objects', 0.035), ('wi', 0.035), ('precision', 0.035), ('iccv', 0.034), ('matching', 0.034), ('constraints', 0.034), ('represents', 0.034), ('center', 0.034), ('fergus', 0.033), ('syntactic', 0.033), ('zisserman', 0.033), ('iterative', 0.033), ('side', 0.033), ('steyvers', 0.032), ('sentence', 0.031), ('relationships', 0.031), ('semantic', 0.031), ('bar', 0.03), ('scenes', 0.029), ('intervening', 0.029), ('jack', 0.029), ('documents', 0.028), ('car', 0.028), ('freeman', 0.027), ('localization', 0.027), ('initialization', 0.026), ('expensive', 0.026), ('hit', 0.025), ('authors', 0.025), ('cues', 0.025), ('recall', 0.025), ('recognition', 0.024), ('perona', 0.024), ('syntax', 0.024), ('extracted', 0.024), ('analogous', 0.023), ('segmentation', 0.023), ('sudderth', 0.023), ('voting', 0.023), ('ths', 0.023), ('symmetric', 0.023), ('lie', 0.022), ('labelme', 0.022), ('semantics', 0.022), ('total', 0.022), ('enforce', 0.022), ('grif', 0.022), ('assigned', 0.021), ('chosen', 0.021), ('training', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="6-tfidf-1" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>2 0.39855045 <a title="6-tfidf-2" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>3 0.24662219 <a title="6-tfidf-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.21376471 <a title="6-tfidf-4" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>Author: Longin J. Latecki, Chengen Lu, Marc Sobel, Xiang Bai</p><p>Abstract: We introduce a new interpretation of multiscale random ﬁelds (MSRFs) that admits efﬁcient optimization in the framework of regular (single level) random ﬁelds (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to ﬁnd optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.</p><p>5 0.17598492 <a title="6-tfidf-5" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>6 0.17027469 <a title="6-tfidf-6" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>7 0.15687847 <a title="6-tfidf-7" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>8 0.13658836 <a title="6-tfidf-8" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>9 0.13604011 <a title="6-tfidf-9" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>10 0.13549814 <a title="6-tfidf-10" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>11 0.12967254 <a title="6-tfidf-11" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>12 0.12798698 <a title="6-tfidf-12" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>13 0.12559442 <a title="6-tfidf-13" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>14 0.12008278 <a title="6-tfidf-14" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>15 0.10809702 <a title="6-tfidf-15" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>16 0.10801467 <a title="6-tfidf-16" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>17 0.097714439 <a title="6-tfidf-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.087193415 <a title="6-tfidf-18" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>19 0.084883437 <a title="6-tfidf-19" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>20 0.084037349 <a title="6-tfidf-20" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.217), (1, -0.212), (2, 0.193), (3, -0.333), (4, -0.049), (5, -0.002), (6, -0.086), (7, -0.187), (8, -0.099), (9, -0.012), (10, -0.125), (11, -0.096), (12, 0.068), (13, -0.071), (14, 0.123), (15, 0.058), (16, 0.127), (17, 0.168), (18, -0.064), (19, -0.032), (20, -0.04), (21, -0.02), (22, 0.026), (23, 0.083), (24, 0.064), (25, 0.136), (26, 0.062), (27, 0.081), (28, 0.03), (29, 0.103), (30, -0.044), (31, 0.101), (32, 0.1), (33, 0.011), (34, -0.059), (35, -0.011), (36, -0.087), (37, -0.067), (38, -0.082), (39, 0.088), (40, -0.092), (41, -0.024), (42, 0.002), (43, -0.051), (44, -0.018), (45, -0.034), (46, 0.054), (47, -0.019), (48, -0.067), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96624893 <a title="6-lsi-1" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>2 0.87762606 <a title="6-lsi-2" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>3 0.80081171 <a title="6-lsi-3" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>Author: Longin J. Latecki, Chengen Lu, Marc Sobel, Xiang Bai</p><p>Abstract: We introduce a new interpretation of multiscale random ﬁelds (MSRFs) that admits efﬁcient optimization in the framework of regular (single level) random ﬁelds (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to ﬁnd optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.</p><p>4 0.69102907 <a title="6-lsi-4" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>5 0.60547841 <a title="6-lsi-5" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>6 0.60377163 <a title="6-lsi-6" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>7 0.52640063 <a title="6-lsi-7" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>8 0.4894591 <a title="6-lsi-8" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>9 0.46565685 <a title="6-lsi-9" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>10 0.44583261 <a title="6-lsi-10" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>11 0.4362762 <a title="6-lsi-11" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>12 0.42650142 <a title="6-lsi-12" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>13 0.42044973 <a title="6-lsi-13" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>14 0.40062815 <a title="6-lsi-14" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>15 0.38897538 <a title="6-lsi-15" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>16 0.38299796 <a title="6-lsi-16" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>17 0.36999491 <a title="6-lsi-17" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>18 0.35790879 <a title="6-lsi-18" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>19 0.33584282 <a title="6-lsi-19" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>20 0.33541465 <a title="6-lsi-20" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.047), (7, 0.051), (12, 0.065), (28, 0.082), (35, 0.011), (57, 0.063), (59, 0.012), (71, 0.013), (77, 0.023), (83, 0.537)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93343925 <a title="6-lda-1" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>2 0.9220072 <a title="6-lda-2" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>3 0.89602405 <a title="6-lda-3" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>Author: Yoshihiro Yamanishi</p><p>Abstract: We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a uniﬁed Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data. 1</p><p>4 0.87532389 <a title="6-lda-4" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>5 0.86409247 <a title="6-lda-5" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>6 0.73895723 <a title="6-lda-6" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>7 0.62300152 <a title="6-lda-7" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>8 0.58270335 <a title="6-lda-8" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>9 0.57560402 <a title="6-lda-9" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>10 0.57142949 <a title="6-lda-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.56826466 <a title="6-lda-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.56606907 <a title="6-lda-12" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>13 0.56426322 <a title="6-lda-13" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>14 0.55000329 <a title="6-lda-14" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>15 0.54851937 <a title="6-lda-15" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>16 0.52656114 <a title="6-lda-16" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>17 0.5242002 <a title="6-lda-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.52413476 <a title="6-lda-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.51178652 <a title="6-lda-19" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>20 0.50911504 <a title="6-lda-20" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
