<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-68" href="#">nips2008-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</h1>
<br/><p>Source: <a title="nips-2008-68-pdf" href="http://papers.nips.cc/paper/3387-efficient-direct-density-ratio-estimation-for-non-stationarity-adaptation-and-outlier-detection.pdf">pdf</a></p><p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>Reference: <a title="nips-2008-68-reference" href="../nips2008_reference/nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. [sent-13, score-0.271]
</p><p>2 In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. [sent-14, score-0.18]
</p><p>3 1  Introduction  In the context of importance sampling, the ratio of two probability density functions is called the importance. [sent-18, score-0.163]
</p><p>4 The problem of estimating the importance is gathering a lot of attention these days since the importance can be used for various succeeding tasks, e. [sent-19, score-0.288]
</p><p>5 , Covariate shift adaptation: Covariate shift is a situation in supervised learning where the distributions of inputs change between the training and test phases but the conditional distribution of outputs given inputs remains unchanged [8]. [sent-21, score-0.219]
</p><p>6 Under covariate shift, standard learning techniques such as maximum likelihood estimation or cross-validation are biased and therefore unreliable—the bias caused by covariate shift can be compensated by weighting the training samples according to the importance [8, 5, 1, 9]. [sent-23, score-0.552]
</p><p>7 Outlier detection: The outlier detection task addressed here is to identify irregular samples in an evaluation dataset based on a model dataset that only contains regular samples [7, 3]. [sent-24, score-0.285]
</p><p>8 The importance values for regular samples are close to one, while those for outliers tend to be signiﬁcantly deviated from one. [sent-25, score-0.254]
</p><p>9 Thus the values of the importance could be used as an index of the degree of outlyingness. [sent-26, score-0.12]
</p><p>10 Below, we refer to the two sets of samples as the training and test sets. [sent-27, score-0.114]
</p><p>11 A naive approach to estimating the importance is to ﬁrst estimate the training and test densities from the sets of training and test samples separately, and then take the ratio of the estimated densities. [sent-28, score-0.325]
</p><p>12 1  To cope with this problem, we propose a direct importance estimation method that does not involve density estimation. [sent-31, score-0.22]
</p><p>13 The proposed method, which we call least-squares importance ﬁtting (LSIF), is formulated as a convex quadratic program and therefore the unique global solution can be obtained. [sent-32, score-0.167]
</p><p>14 We give a cross-validation method for model selection and a regularization path tracking algorithm for efﬁcient computation [4]. [sent-33, score-0.221]
</p><p>15 This regularization path tracking algorithm is turned out to be computationally very efﬁcient since the entire solution path can be traced without a quadratic program solver. [sent-34, score-0.281]
</p><p>16 However, it tends to share a common weakness of path tracking algorithms, i. [sent-35, score-0.112]
</p><p>17 We experimentally show that the accuracy of uLSIF is comparable to the best existing method while its computation is much faster than the others in covariate shift adaptation and outlier detection. [sent-41, score-0.376]
</p><p>18 ) training samples {xtr }ntr from a training distribution i i=1 with density ptr (x) and i. [sent-45, score-0.286]
</p><p>19 test samples {xte }nte from a test distribution with density pte (x). [sent-48, score-0.296]
</p><p>20 The goal of this paper is to estimate the importance w(x) =  pte (x) ptr (x)  from {xtr }ntr and {xte }nte . [sent-50, score-0.388]
</p><p>21 Our key restriction is that we want to avoid estimating densities i i=1 j j=1 pte (x) and ptr (x) when estimating the importance w(x). [sent-51, score-0.446]
</p><p>22 Least-squares Approach: Let us model the importance w(x) by the following linear model: w(x) = α⊤ ϕ(x),  (1)  where ⊤ denotes the transpose, α = (α1 , . [sent-52, score-0.12]
</p><p>23 ℓ=1 We determine the parameter α so that the following squared error is minimized: 1 J0 (α) = 2  where C =  1 2  w(x) −  pte (x) ptr (x)  2  ptr (x)dx =  1 2  w(x)2 ptr (x)dx −  w(x)pte (x)dx + C,  w(x)pte (x)dx is a constant and therefore can be safely ignored. [sent-63, score-0.518]
</p><p>24 Let 1 J(α) = J0 (α) − C = 2 α⊤ Hα − h⊤ α,  (2)  where H = ϕ(x)ϕ(x)⊤ ptr (x)dx, h = ϕ(x)pte (x)dx. [sent-64, score-0.125]
</p><p>25 Using the empirical approximation and taking into account the non-negativity of the importance function w(x), we obtain minα∈Rb  1 ⊤ 2 α Hα  ⊤  − h α + λ1⊤ α b  n  s. [sent-65, score-0.12]
</p><p>26 α ≥ 0b ,  (3)  n  tr te ⊤ tr tr ⊤ te h = n1 where H = n1 i=1 ϕ(xi )ϕ(xi ) , j=1 ϕ(xj ). [sent-67, score-0.589]
</p><p>27 λ1b α is a regularization term tr te for avoiding overﬁtting, λ ≥ 0, and 1b is the b-dimensional vector with all ones. [sent-68, score-0.285]
</p><p>28 Theorem 1 Assume that (a) the optimal solution of the problem (4) satisﬁes the strict complementarity condition, and (b) ntr and nte satisfy nte = ω(n2 ). [sent-79, score-1.177]
</p><p>29 Then we have E[J(α)] = tr J(α∗ ) + O n−1 , where E denotes the expectation over all possible training samples of size ntr tr and all possible test samples of size nte . [sent-80, score-1.24]
</p><p>30 It is possible to tr explicitly obtain the coefﬁcient of the term of order n−1 , but we omit the detail due to lack of space. [sent-82, score-0.152]
</p><p>31 tr Model Selection for LSIF: The performance of LSIF depends on the choice of the regularization parameter λ and basis functions {ϕℓ (x)}b (which we refer to as a model). [sent-83, score-0.196]
</p><p>32 Here, we employ cross-validation for estimating J(α), which has an accuracy guarantee for ﬁnite ntr samples: First, the training samples {xtr }i=1 and test samples {xte }nte are divided into R disjoint i j j=1 te tr subsets {Xr }R and {Xr }R , respectively. [sent-85, score-0.87]
</p><p>33 Then an importance estimate wr (x) is obtained using r=1 r=1 tr te {Xjtr }j=r and {Xjte }j=r , and the cost J is approximated using the held-out samples Xr and Xr (CV)  as Jr  =  1 tr 2|Xr |  tr xtr ∈Xr  wr (xtr )2 −  1 te |Xr |  te xte ∈Xr  wr (xte ). [sent-86, score-1.408]
</p><p>34 , j j=1 w(x) =  nte ℓ=1  αℓ Kσ (x, xte ), ℓ  Kσ (x, x′ ) = exp − x − x′ 2 /(2σ 2 ) . [sent-95, score-0.57]
</p><p>35 where  (5)  The reason why we chose the test input points {xte }nte as the Gaussian centers, not the training j j=1 input points {xtr }ntr , is as follows. [sent-96, score-0.144]
</p><p>36 By deﬁnition, the importance w(x) tends to take large values i i=1 if the training input density ptr (x) is small and the test input density pte (x) is large; conversely, w(x) tends to be small (i. [sent-97, score-0.644]
</p><p>37 , close to zero) if ptr (x) is large and pte (x) is small. [sent-99, score-0.268]
</p><p>38 Following this heuristic, we allocate many kernels at high test input density regions, which can be achieved by setting the Gaussian centers at the test input points {xte }nte . [sent-101, score-0.226]
</p><p>39 j j=1 Alternatively, we may locate (ntr + nte ) Gaussian kernels at both {xtr }ntr and {xte }nte . [sent-102, score-0.369]
</p><p>40 When nte is large, just using all the test input points {xte }nte as j j=1 Gaussian centers is already computationally rather demanding. [sent-104, score-0.469]
</p><p>41 , j j=1 w(x) =  b ℓ=1  αℓ Kσ (x, cℓ ),  (6)  where cℓ is a template point randomly chosen from {xte }nte and b (≤ nte ) is a preﬁxed number. [sent-107, score-0.367]
</p><p>42 j j=1 In the experiments shown later, we ﬁx the number of template points at b = min(100, nte ), and optimize the kernel width σ and the regularization parameter λ by cross-validation with grid search. [sent-108, score-0.509]
</p><p>43 A basic idea of regularization path tracking is to check the violation of the Karush-KuhnTucker (KKT) conditions—which are necessary and sufﬁcient conditions for optimality of convex programs—when the regularization parameter λ is changed. [sent-113, score-0.203]
</p><p>44 Although the detail of the algorithm is omitted due to lack of space, we can show that a quadratic programming solver is no longer needed for obtaining the entire solution path of LSIF—just computing matrix inverses is enough. [sent-114, score-0.141]
</p><p>45 However, in our preliminary experiments, the regularization path tracking algorithm is turned out to be numerically rather unreliable since the numerical errors tend to be accumulated when tracking the regularization path. [sent-116, score-0.303]
</p><p>46 This seems to be a common pitfall of solution path tracking algorithms in general. [sent-117, score-0.111]
</p><p>47 tr  Theorem 2 guarantees that uLSIF converges to the ideal solution with order n−1 . [sent-139, score-0.182]
</p><p>48 It is possible to tr explicitly obtain the coefﬁcient of the term of order n−1 , but we omit the detail due to lack of space. [sent-140, score-0.152]
</p><p>49 tr We can also derive upper bounds on the difference between LSIF and uLSIF and show that uLSIF gives a good approximation to LSIF. [sent-141, score-0.137]
</p><p>50 For simi i=1 j j=1 plicity, we assume that ntr < nte and the i-th training sample xtr and the i-th test sample xte are i i held out at the same time; the test samples {xte }nte tr +1 are always used for importance estimation. [sent-145, score-1.671]
</p><p>51 j j=n 4  (i)  Let β λ be a parameter learned without the i-th training sample xtr and the i-th test sample xte . [sent-146, score-0.533]
</p><p>52 i i n  (i)  (i)  tr 1 tr ⊤ 2 te ⊤ Then the LOOCV score is expressed as n1 i=1 [ 2 (ϕ(xi ) β λ ) − ϕ(xi ) β λ ]. [sent-147, score-0.382]
</p><p>53 KDE can be used for importance estimation by ﬁrst estimating ptr (x) and pte (x) separately from {xtr }ntr and {xte }nte and then estimating the importance by w(x) = pte (x)/ptr (x). [sent-154, score-0.736]
</p><p>54 The kernel mean matching (KMM) method allows us to directly obtain an estimate of the importance values at training points without going through density estimation [5]. [sent-157, score-0.281]
</p><p>55 KMM can overcome the curse of dimensionality by directly estimating the importance using a special property of the Gaussian reproducing kernel Hilbert space. [sent-158, score-0.179]
</p><p>56 However, there is no objective model selection method for the regularization parameter and kernel width. [sent-159, score-0.13]
</p><p>57 Other approaches to directly estimating the importance is to directly ﬁt an importance model to the true importance—a method based on logistic regression (LogReg) [1], or a method based on the kernel model (6) (which is called the Kullback-Leibler importance estimation procedure, KLIEP) [9, 6]. [sent-163, score-0.474]
</p><p>58 However, LSIF is advantageous over LogReg and KLIEP in that it is equipped with a regularization path tracking algorithm. [sent-169, score-0.144]
</p><p>59 However, the regularization path tracking algorithm tends to be numerically unstable. [sent-171, score-0.197]
</p><p>60 The proposed uLSIF inherits good properties of existing methods such as no density estimation involved and a build-in model selection method equipped. [sent-172, score-0.111]
</p><p>61 Furthermore, the closed-form solution of uLSIF allows us to compute the LOOCV score analytically without repeating hold-out loops, which highly contributes to reducing the computation time in the model selection phase. [sent-174, score-0.13]
</p><p>62 5  Experiments  Importance Estimation: Let ptr (x) be the d-dimensional normal distribution with mean zero and covariance identity; let pte (x) be the d-dimensional normal distribution with mean (1, 0, . [sent-175, score-0.268]
</p><p>63 The task is to estimate the importance at training input points: {w(xtr )}ntr . [sent-179, score-0.18]
</p><p>64 i i=1 We ﬁxed the number of test input points at nte = 1000 and consider the following two settings for the number ntr of training samples and the input dimension d: (a) ntr = 100 and d = 1, 2, . [sent-180, score-1.431]
</p><p>65 We run the experiments 100 times for each d, each ntr , and each method, and evaluate the quality of the importance estimates {wi }ntr by the normalized mean i=1 5  −5  10  0. [sent-187, score-0.569]
</p><p>66 02  −6  10  5  10 15 d (Input Dimension)  0  20  (a) When d is changed  5  10 15 d (Input Dimension)  LogReg uLSIF 15  10  5  0  20  (a) When d is changed  5  10 15 d (Input Dimension)  20  (a) When d is changed  0. [sent-192, score-0.105]
</p><p>67 2  n  tr tr tr squared error (NMSE): n1 i=1 (w(xi ) − w(xi )) , where tr normalized to be one, respectively. [sent-199, score-0.548]
</p><p>68 ntr i=1  w(xtr ) and i  ntr i=1  w(xtr ) are i  NMSEs averaged over 100 trials (a) as a function of input dimension d and (b) as a function of the training sample size ntr are plotted in log scale in Figure 1. [sent-200, score-1.436]
</p><p>69 This would be the fruit of directly estimating the importance without going through density estimation. [sent-203, score-0.192]
</p><p>70 Figure 1(b) shows that the errors of all methods tend to decrease as the number of training samples grows. [sent-207, score-0.106]
</p><p>71 , KMM does not involve any cross-validation, KDE and KLIEP involve cross-validation over the kernel width, and LogReg and uLSIF involve cross-validation over both the kernel width and the regularization parameter. [sent-212, score-0.206]
</p><p>72 For this reason, we ﬁrst investigate the computation time of each importance estimation method after the model parameters are ﬁxed. [sent-214, score-0.197]
</p><p>73 Figure 2(b) shows that the computation time of LogReg, KLIEP, and uLSIF is nearly independent of the training sample size ntr , while that of KDE and KMM sharply increase as ntr increases. [sent-218, score-0.983]
</p><p>74 Covariate Shift Adaptation in Regression and Classiﬁcation: Next, we illustrate how the importance estimation methods could be used in covariate shift adaptation [8, 5, 1, 9]. [sent-225, score-0.37]
</p><p>75 Covariate shift is a situation in supervised learning where the input distributions change between the training and test phases but the conditional distribution of outputs given inputs remains unchanged. [sent-226, score-0.175]
</p><p>76 Under covariate shift, standard learning techniques such as maximum likelihood estimation or cross-validation are biased; the bias caused by covariate shift can be asymptotically canceled by weighting the samples according to the importance. [sent-227, score-0.383]
</p><p>77 In addition to training input samples {xtr }ntr following a training i i=1 input density ptr (x) and test input samples {xte }nte following a test input density pte (x), suppose j j=1 tr that training output samples {yi }ntr at the training input points {xtr }ntr are given. [sent-228, score-0.986]
</p><p>78 We learn the parameter θ by j j=1 importance weighted regularized least-squares (IWRLS): minθ  ntr i=1  tr w(xtr ) f (xtr ; θ) − yi i i  2  +γ θ  2  . [sent-232, score-0.706]
</p><p>79 (9)  It is known that IWRLS is consistent when the true importance w(xtr ) is used as weights— i unweighted RLS is not consistent due to covariate shift, given that the true learning target function f (x) is not realizable by the model f (x) [8]. [sent-233, score-0.236]
</p><p>80 The kernel width h and the regularization parameter γ in IWRLS (9) are chosen by importance tr tr weighted CV (IWCV) [9]. [sent-234, score-0.522]
</p><p>81 More speciﬁcally, we ﬁrst divide the training samples {zi | zi = tr tr ntr tr R tr (xi , yi )}i=1 into R disjoint subsets {Zr }r=1 . [sent-235, score-1.082]
</p><p>82 Then a function fr (x) is learned using {Zj }j=r tr by IWRLS and its mean test error for the remaining samples Zr is computed: 1 tr |Zr |  tr (x,y)∈Zr  w(x)loss fr (x), y ,  (10)  1 where loss (y, y) is (y − y)2 in regression and 2 (1 − sign{yy}) in classiﬁcation. [sent-236, score-0.52]
</p><p>83 , R and choose the kernel width h and the regularization parameter γ so that the average of the above mean test error over all r is minimized. [sent-240, score-0.157]
</p><p>84 IWCV is shown to be an (almost) unbiased estimator of the generalization error, while unweighted CV with misspeciﬁed models is biased due to covariate shift. [sent-242, score-0.132]
</p><p>85 We set the number of samples at ntr = 100 and nte = 500 for all datasets. [sent-244, score-0.852]
</p><p>86 The experiments are nte te te repeated 100 times for each dataset and evaluate the mean test error: n1 j=1 loss(f (xj ), yj ). [sent-248, score-0.586]
</p><p>87 Thus, proposed uLSIF is overall shown to work well in covariate shift adaptation with low computational cost. [sent-255, score-0.223]
</p><p>88 Outlier Detection: Here, we consider an outlier detection problem of ﬁnding irregular samples in a dataset (“evaluation dataset”) based on another dataset (“model dataset”) that only contains 7  Table 2: Outlier detection. [sent-256, score-0.214]
</p><p>89 Deﬁning the importance over two sets of samples, we can see that the importance values for regular samples are close to one, while those for outliers tend to be signiﬁcantly deviated from one. [sent-483, score-0.374]
</p><p>90 Thus the importance values could be used as an index of the degree of outlyingness in this scenario. [sent-484, score-0.12]
</p><p>91 Then outliers tend to have smaller importance values (i. [sent-490, score-0.161]
</p><p>92 We again test KMM, LogReg, KLIEP, and uLSIF for importance estimation; in addition, we test native outlier detection methods such as the one-class support vector machine (OSVM) [7], the local outlier factor (LOF) [3], and the kernel density estimator (KDE). [sent-493, score-0.439]
</p><p>93 We allocate all positive training samples for the “model” set, while all positive test samples and 1% of negative test samples are assigned in the “evaluation” set. [sent-496, score-0.265]
</p><p>94 Thus, we regard the positive samples as regular and the negative samples as irregular. [sent-497, score-0.123]
</p><p>95 Thus, proposed uLSIF is overall shown to work well and computationally efﬁcient also in outlier detection. [sent-503, score-0.109]
</p><p>96 6  Conclusions  We proposed a new method for importance estimation that can avoid solving a substantially more difﬁcult task of density estimation. [sent-504, score-0.204]
</p><p>97 We are currently exploring various possible applications of important estimation methods beyond covariate shift adaptation and outlier detection, e. [sent-505, score-0.332]
</p><p>98 , feature selection, conditional distribution estimation, and independent component analysis—we believe that importance estimation could be used as a new versatile tool in machine learning. [sent-507, score-0.147]
</p><p>99 The entire regularization path for the support vector machine. [sent-523, score-0.122]
</p><p>100 Improving predictive inference under covariate shift by weighting the log-likelihood function. [sent-539, score-0.188]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ulsif', 0.46), ('ntr', 0.449), ('nte', 0.351), ('xtr', 0.252), ('logreg', 0.23), ('xte', 0.219), ('kliep', 0.197), ('lsif', 0.175), ('kde', 0.163), ('pte', 0.143), ('tr', 0.137), ('kmm', 0.132), ('ptr', 0.125), ('importance', 0.12), ('covariate', 0.102), ('te', 0.089), ('outlier', 0.082), ('shift', 0.071), ('loocv', 0.067), ('xr', 0.062), ('regularization', 0.059), ('samples', 0.052), ('adaptation', 0.05), ('path', 0.047), ('iwrls', 0.044), ('density', 0.043), ('cv', 0.041), ('width', 0.039), ('tracking', 0.038), ('atr', 0.038), ('lof', 0.038), ('computation', 0.036), ('changed', 0.035), ('auc', 0.033), ('iwcv', 0.033), ('osvm', 0.033), ('zr', 0.033), ('rb', 0.033), ('training', 0.033), ('kernel', 0.03), ('sec', 0.03), ('trials', 0.029), ('estimating', 0.029), ('test', 0.029), ('wr', 0.029), ('dataset', 0.028), ('computationally', 0.027), ('tends', 0.027), ('estimation', 0.027), ('selection', 0.027), ('input', 0.027), ('ate', 0.026), ('solution', 0.026), ('unconstrained', 0.026), ('numerically', 0.026), ('nmse', 0.025), ('detection', 0.024), ('analytically', 0.022), ('dx', 0.022), ('deviated', 0.022), ('hido', 0.022), ('kanamori', 0.022), ('nagoya', 0.022), ('centers', 0.021), ('comparable', 0.021), ('tend', 0.021), ('japan', 0.021), ('quadratic', 0.021), ('datasets', 0.02), ('outliers', 0.02), ('score', 0.019), ('regular', 0.019), ('succeeding', 0.019), ('delve', 0.019), ('ida', 0.019), ('nmses', 0.019), ('ideal', 0.019), ('kernels', 0.018), ('allocate', 0.018), ('sugiyama', 0.018), ('sharply', 0.016), ('biased', 0.016), ('template', 0.016), ('involve', 0.016), ('ef', 0.016), ('inverses', 0.016), ('entire', 0.016), ('detail', 0.015), ('tokyo', 0.015), ('bickel', 0.015), ('unreliable', 0.015), ('phases', 0.015), ('weighting', 0.015), ('caused', 0.014), ('kh', 0.014), ('fr', 0.014), ('unweighted', 0.014), ('points', 0.014), ('theoretically', 0.014), ('method', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="68-tfidf-1" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>2 0.046205558 <a title="68-tfidf-2" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>3 0.043551907 <a title="68-tfidf-3" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>4 0.040745761 <a title="68-tfidf-4" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>5 0.040565025 <a title="68-tfidf-5" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>6 0.040133107 <a title="68-tfidf-6" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>7 0.038582563 <a title="68-tfidf-7" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>8 0.038272012 <a title="68-tfidf-8" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>9 0.036803517 <a title="68-tfidf-9" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>10 0.03652991 <a title="68-tfidf-10" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>11 0.0336993 <a title="68-tfidf-11" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>12 0.03352432 <a title="68-tfidf-12" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>13 0.033283032 <a title="68-tfidf-13" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>14 0.032535329 <a title="68-tfidf-14" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>15 0.032434508 <a title="68-tfidf-15" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>16 0.031849943 <a title="68-tfidf-16" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>17 0.03167443 <a title="68-tfidf-17" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>18 0.03137235 <a title="68-tfidf-18" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>19 0.03043269 <a title="68-tfidf-19" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>20 0.029825799 <a title="68-tfidf-20" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.098), (1, -0.02), (2, -0.021), (3, 0.034), (4, 0.03), (5, 0.014), (6, -0.016), (7, 0.016), (8, 0.013), (9, 0.012), (10, 0.074), (11, -0.027), (12, -0.03), (13, 0.01), (14, 0.008), (15, 0.0), (16, 0.014), (17, 0.029), (18, -0.042), (19, 0.029), (20, 0.039), (21, 0.035), (22, -0.01), (23, -0.003), (24, -0.003), (25, 0.033), (26, 0.01), (27, 0.034), (28, -0.046), (29, 0.044), (30, -0.004), (31, -0.044), (32, -0.054), (33, 0.008), (34, 0.026), (35, 0.018), (36, 0.052), (37, 0.05), (38, 0.065), (39, -0.01), (40, -0.031), (41, -0.021), (42, -0.005), (43, 0.011), (44, -0.02), (45, -0.051), (46, -0.007), (47, -0.006), (48, -0.058), (49, -0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85355896 <a title="68-lsi-1" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>2 0.54552782 <a title="68-lsi-2" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>Author: Novi Quadrianto, Le Song, Alex J. Smola</p><p>Abstract: Object matching is a fundamental operation in data analysis. It typically requires the deﬁnition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for ﬁnding a locally optimal solution. 1</p><p>3 0.51720506 <a title="68-lsi-3" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>4 0.5171926 <a title="68-lsi-4" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>5 0.50511718 <a title="68-lsi-5" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>6 0.4991118 <a title="68-lsi-6" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>7 0.48741582 <a title="68-lsi-7" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>8 0.48532629 <a title="68-lsi-8" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>9 0.48077175 <a title="68-lsi-9" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>10 0.47370562 <a title="68-lsi-10" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>11 0.47228161 <a title="68-lsi-11" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>12 0.4636966 <a title="68-lsi-12" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>13 0.45880157 <a title="68-lsi-13" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>14 0.45840865 <a title="68-lsi-14" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>15 0.45481575 <a title="68-lsi-15" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>16 0.45442167 <a title="68-lsi-16" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>17 0.45096305 <a title="68-lsi-17" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>18 0.44314942 <a title="68-lsi-18" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>19 0.43983144 <a title="68-lsi-19" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>20 0.43830365 <a title="68-lsi-20" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.054), (7, 0.049), (12, 0.022), (28, 0.114), (57, 0.036), (59, 0.015), (63, 0.02), (71, 0.012), (77, 0.027), (83, 0.54)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94355869 <a title="68-lda-1" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>2 0.94304627 <a title="68-lda-2" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>3 0.91779464 <a title="68-lda-3" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>Author: Yoshihiro Yamanishi</p><p>Abstract: We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a uniﬁed Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data. 1</p><p>4 0.90164703 <a title="68-lda-4" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>same-paper 5 0.88909584 <a title="68-lda-5" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>6 0.76591849 <a title="68-lda-6" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>7 0.6270014 <a title="68-lda-7" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>8 0.60658544 <a title="68-lda-8" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>9 0.60292524 <a title="68-lda-9" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>10 0.59601074 <a title="68-lda-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.58833611 <a title="68-lda-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.5809961 <a title="68-lda-12" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>13 0.57818234 <a title="68-lda-13" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>14 0.57425904 <a title="68-lda-14" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>15 0.57378256 <a title="68-lda-15" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>16 0.56110823 <a title="68-lda-16" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>17 0.54900813 <a title="68-lda-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.54857016 <a title="68-lda-18" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>19 0.54652309 <a title="68-lda-19" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>20 0.53667969 <a title="68-lda-20" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
