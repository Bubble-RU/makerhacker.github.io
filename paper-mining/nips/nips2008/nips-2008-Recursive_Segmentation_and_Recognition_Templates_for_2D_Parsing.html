<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-191" href="#">nips2008-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</h1>
<br/><p>Source: <a title="nips-2008-191-pdf" href="http://papers.nips.cc/paper/3450-recursive-segmentation-and-recognition-templates-for-2d-parsing.pdf">pdf</a></p><p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>Reference: <a title="nips-2008-191-reference" href="../nips2008_reference/nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. [sent-10, score-0.775]
</p><p>2 Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. [sent-11, score-0.603]
</p><p>3 By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. [sent-12, score-0.521]
</p><p>4 In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. [sent-14, score-1.392]
</p><p>5 This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. [sent-15, score-0.784]
</p><p>6 Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. [sent-17, score-0.522]
</p><p>7 We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. [sent-19, score-0.322]
</p><p>8 Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. [sent-20, score-0.272]
</p><p>9 1  Introduction  Language and image understanding are two major tasks in artiﬁcial intelligence. [sent-21, score-0.272]
</p><p>10 Natural language researchers have formalized this task in terms of parsing an input signal into a hierarchical representation. [sent-22, score-0.609]
</p><p>11 Secondly, they have exploited the one-dimensional structure of language to obtain efﬁcient polynomial-time parsing algorithms (e. [sent-32, score-0.456]
</p><p>12 By contrast, the nature of images makes it much harder to design efﬁcient image parsers which are capable of simultaneously performing segmentation (parsing an image into regions) and recognition (labeling the regions). [sent-35, score-1.143]
</p><p>13 Firstly, it is unclear what hierarchical representations should be used to model images and there are no direct analogies to the syntactic categories and phrase structures that occur in speech. [sent-36, score-0.253]
</p><p>14 Secondly, the inference problem is formidable due to the well-known complexity 1  and ambiguity of segmentation and recognition. [sent-37, score-0.448]
</p><p>15 Unlike most languages (Chinese is an exception), whose constituents are well-separated words, the boundaries between different image regions are usually highly unclear. [sent-38, score-0.421]
</p><p>16 Exploring all the different image partitions results in combinatorial explosions because of the two-dimensional nature of images (which makes it impossible to order these partitions to enable dynamic programming). [sent-39, score-0.332]
</p><p>17 Overall it has been hard to adapt methods from natural language parsing and apply them to vision despite the high-level conceptual similarities (except for restricted problems such as text [4]). [sent-40, score-0.541]
</p><p>18 Attempts at image parsing must make trade-offs between the complexity of the models and the complexity of the computation (for inference and learning). [sent-41, score-0.676]
</p><p>19 This yields simpler (discriminative) models which are less capable of representing complex image structures and long range interactions. [sent-48, score-0.315]
</p><p>20 In this paper, we introduce Hierarchical Image Models (HIM)’s for image parsing. [sent-57, score-0.272]
</p><p>21 In particular, we introduce recursive segmentation and recognition templates which represent complex image knowledge and serve as elementary constituents analogous to those used in speech. [sent-59, score-1.163]
</p><p>22 As in speech, we can recursively compose these constituents at lower levels to form more complex constituents at higher level. [sent-60, score-0.268]
</p><p>23 Each node of the hierarchy corresponds to an image region (whose size depends on the level in the hierarchy). [sent-61, score-0.557]
</p><p>24 The state of each node represents both the partitioning of the corresponding region into segments and the labeling of these segments (i. [sent-62, score-0.259]
</p><p>25 Segmentations at the top levels of the hierarchy give coarse descriptions of the image which are reﬁned by the segmentations at the lower levels. [sent-65, score-0.55]
</p><p>26 Learning and inference (parsing) are made efﬁcient by exploiting the hierarchical structure (and the absence of loops). [sent-66, score-0.248]
</p><p>27 To illustrate the HIM we implement it for parsing images and we evaluate it on the public MSRC image dataset [8]. [sent-70, score-0.732]
</p><p>28 We discuss ways that HIM’s can be extended naturally to model more complex image phenomena. [sent-72, score-0.272]
</p><p>29 1 The Model We represent an image by a hierarchical graph deﬁned by parent-child relationships. [sent-74, score-0.425]
</p><p>30 The hierarchy corresponds to the image pyramid (with 5 layers in this paper). [sent-76, score-0.424]
</p><p>31 The leaf nodes represent local image patches (27 × 27 in this paper). [sent-79, score-0.332]
</p><p>32 Thus, the hierarchy is a quad tree and Ch(a) encodes all its vertical edges. [sent-82, score-0.228]
</p><p>33 The image region represented by node a is denoted by R(a). [sent-83, score-0.405]
</p><p>34 A pixel in R(a), indexed by r, corresponds to an image pixel. [sent-84, score-0.351]
</p><p>35 A conﬁguration of the hierarchy is an assignment of state variables y = {ya } with ya = (sa , ca ) at each node a, where s and c denote region partition and object labeling, respectively and (s, c) is called the “Segmentation and Recognition” pair, which we call an S-R pair. [sent-86, score-0.545]
</p><p>36 The middle panel shows a dictionary of 30 segmentation templates. [sent-91, score-0.473]
</p><p>37 The last panel shows that the ground truth pixel labels (upper right panel) can be well approximated by composing a set of labeled segmentation templates (bottom right panel). [sent-95, score-0.974]
</p><p>38 Figure 2:  This ﬁgure illustrates how the segmentation templates and object labels (S-R pair) represent image regions in a coarse-to-ﬁne way. [sent-96, score-1.155]
</p><p>39 The left ﬁgure is the input image which is followed by global, mid-level and local S-R pairs. [sent-97, score-0.272]
</p><p>40 The global S-R pair gives a coarse description of the object identity (horse), its background (grass), and its position in the image (central). [sent-98, score-0.503]
</p><p>41 More precisely, each region R(a) is described by a segmentation templates which is selected from a dictionary DS . [sent-103, score-0.779]
</p><p>42 Each segmentation template consists of a partition of the region into K non-overlapping sub-parts, see ﬁgure 1. [sent-104, score-0.524]
</p><p>43 In this paper K ≤ 3, |Ds | = 30, and the segmentation templates are designed by hand to cover the taxonomy of shape segmentations that happen in images, such as T-junctions, Y-junctions, and so on. [sent-105, score-0.741]
</p><p>44 The variable s refers to the indexes of the segmentation templates in the dictionary, i. [sent-106, score-0.664]
</p><p>45 The labeling of a pixel r in region R(a) is denoted by or ∈ {1. [sent-118, score-0.281]
</p><p>46 In other words, each level of the hierarchy has a separate labeling ﬁeld. [sent-123, score-0.278]
</p><p>47 a A novel feature of this hierarchical representation is the multi-level S-R pairs which explicitly model both the segmentation and labeling of its corresponding region, while traditional vision approaches [8, 10, 11] use labeling only. [sent-125, score-0.927]
</p><p>48 The S-R pairs deﬁned in a hierarchical form provide a coarse-to-ﬁne representation which captures the “gist” (semantical meaning) of image regions. [sent-126, score-0.468]
</p><p>49 As one can see in ﬁgure 2, the global S-R pair gives a coarse description (the identities of objects and their spatial layout) of the whole image which is accurate enough to encode high level image properties in a compact form. [sent-127, score-0.715]
</p><p>50 The four templates at the lower level further reﬁne the interpretations. [sent-129, score-0.27]
</p><p>51 The ﬁrst term E1 (x, s, c) is an object speciﬁc data term which represents image features of regions. [sent-134, score-0.376]
</p><p>52 We use 55 types of shape (spatial) ﬁlters (similar to [8]) to calculate the responses of 47 image features. [sent-140, score-0.272]
</p><p>53 The second term (segmentation speciﬁc) E2 (x, s, c) = − a α2 ψ2 (x, sa , ca ) is designed to favor the segmentation templates in which the pixels belonging to the same partitions (i. [sent-142, score-1.125]
</p><p>54 The third term, deﬁned as E3 (s, c) = − a,b=P a(a) α3 ψ3 (sa , ca , sb , cb ) (i. [sent-149, score-0.273]
</p><p>55 ψ3 (sa , ca , sb , cb ) is deﬁned by the Hamming distance: ψ3 (sa , ca , sb , cb ) =  1 |R(a)|  δ(or , or ) a b  (5)  r∈R(a)  where δ(or , or ) is the Kronecker delta, which equals one whenever or = or and zero otherwise. [sent-152, score-0.546]
</p><p>56 The a b a b hamming function ensures to glue the segmentation templates (and their labels) at different levels together in a consistent hierarchical form. [sent-153, score-0.871]
</p><p>57 , a cow is unlikely to appear next to an aeroplane): E4 (c) = −  α4 (i, j)ψ4 (i, j, ca , ca ) − a i,j=1. [sent-158, score-0.323]
</p><p>58 M  where ψ4 (i, j, ca , cb ) is an indicator function which equals one while i ≡ ca and j ≡ cb (i ≡ ca means i is a component of ca ) hold true and zero otherwise. [sent-162, score-0.68]
</p><p>59 s encodes the classes in a single template while the second term encodes the classes in two templates of the parent-child nodes. [sent-166, score-0.47]
</p><p>60 4  The ﬁfth term E5 (s) = − a α5 ψ5 (sa ), where ψ5 (sa ) = log p(sa ) encode the generic prior of the segmentation template. [sent-168, score-0.394]
</p><p>61 Similarly the sixth term E6 (s, c) = − a j≡ca α6 ψ6 (sa , j), where ψ6 (sa , j) = log p(sa , j), models the co-occurrence of the segmentation templates and the object classes. [sent-169, score-0.768]
</p><p>62 HIM is a coarse-to-ﬁne representation which captures the “gist” of image regions by using the S-R pairs at multiple levels. [sent-174, score-0.357]
</p><p>63 But the traditional concept of “gist” [14] relies only on image features and does not include segmentation templates. [sent-175, score-0.666]
</p><p>64 Levin and Weiss [15] use a segmentation mask which is more object-speciﬁc than our segmentation templates (and they do not have a hierarchy). [sent-176, score-1.058]
</p><p>65 Our approach has some similarities to some hierarchical models (which have two-layers only) [10],[11] – but these models also lack segmentation templates. [sent-178, score-0.547]
</p><p>66 2 Parsing by Dynamic Programming Parsing an image is performed as inference of the HIM. [sent-181, score-0.326]
</p><p>67 More precisely, the task of parsing is to obtain the maximum a posterior (MAP): y ∗ = arg max p(y|x; α) = arg max ψ(x, y) · α y  y  (7)  The size of the states of each node is O(M K |Ds |) where K = 3, M = 21, |Ds | = 30 in our case. [sent-182, score-0.407]
</p><p>68 The ﬁnal output of the model for segmentation is the pixel labeling determined by the (s, c) of the lowest level. [sent-186, score-0.599]
</p><p>69 In our case, X is a set of images, with Y being a set of possible parse trees which specify the labels of image regions in a hierarchical form. [sent-201, score-0.669]
</p><p>70 It seems that the ground truth of parsing trees needs all labels of both segmentation template and pixel labelings. [sent-202, score-1.068]
</p><p>71 In our experiment, we will show that how to obtain the ground truth directly from the segmentation labels without extra human labeling. [sent-203, score-0.585]
</p><p>72 N • ﬁnd the best state of the model on the i’th training image with current parameter setting, i. [sent-219, score-0.272]
</p><p>73 This dataset is designed to evaluate scene labeling including both image segmentation and multi-class object recognition. [sent-236, score-0.967]
</p><p>74 The ground truth only gives the labeling of the image pixels. [sent-237, score-0.516]
</p><p>75 To supplement this ground truth (to enable learning), we estimate the true labels (states of the S-R pair ) of the nodes in the ﬁve-layer hierarchy of HIM by selecting the S-R pairs which have maximum overlap with the labels of the image pixels. [sent-238, score-0.827]
</p><p>76 This approximation only results in 2% error in labeling image pixels. [sent-239, score-0.398]
</p><p>77 For a given image x, the parsing result is obtained by estimating the best conﬁguration y ∗ of the HIM. [sent-246, score-0.622]
</p><p>78 To evaluate the performance of parsing we use the global accuracy measured in terms of all pixels and the average accuracy over the 21 object classes (global accuracy pays most attention to frequently occurring objects and penalizes infrequent objects). [sent-247, score-0.671]
</p><p>79 The boosting learning takes about 35 hours of which 27 hours are spent on I/O processing and 8 hours on computing. [sent-251, score-0.256]
</p><p>80 In the testing stage, it takes 30 seconds to parse an image with size of 320 × 200 (6s for extracting image features, 9s for computing the strong classiﬁer of boosting and 15s for parsing the HIM). [sent-253, score-1.12]
</p><p>81 Figure 4 (best viewed in color) shows several parsing results obtained by the HIM and by the classiﬁer by itself (i. [sent-255, score-0.35]
</p><p>82 One can see that the HIM is able to roughly a capture different shaped segmentation boundaries (see the legs of the cow and sheep in rows 1 and 3, and the boundary curve between sky and building in row 4). [sent-258, score-0.477]
</p><p>83 Our boosting results are better than Textonboost [8] because of image features. [sent-267, score-0.369]
</p><p>84 The colors indicate the labels of 21 object classes as in [8]. [sent-271, score-0.243]
</p><p>85 The columns (except the fourth “accuracy” column) show the input images, ground truth, the labels obtained by HIM and the boosting classiﬁer respectively. [sent-272, score-0.228]
</p><p>86 “Classiﬁer only” are the results where the pixel labels are predicted by the classiﬁer obtained by boosting only. [sent-286, score-0.249]
</p><p>87 Figure 5 shows how the S-R pairs (which include the segmentation templates) can be used to (partially) parse an object into its constituent parts, by the correspondence between S-R pairs and speciﬁc parts of objects. [sent-290, score-0.795]
</p><p>88 4  Conclusion  This paper describes a novel hierarchical image model (HIM) for 2D image parsing. [sent-296, score-0.697]
</p><p>89 The hierarchical nature of the model, and the use of recursive segmentation and recognition templates, enables the HIM to represent complex image structures in a coarse-to-ﬁne manner. [sent-297, score-0.939]
</p><p>90 We can perform inference (parsing) rapidly in polynomial time by exploiting the hierarchical structure. [sent-298, score-0.283]
</p><p>91 We demonstrated the effectiveness of HIM’s by applying them to the challenging task of segmentation and labeling of the public MSRC image database. [sent-300, score-0.842]
</p><p>92 7  Figure 5:  The S-R pairs can be used to parse the object into parts. [sent-302, score-0.276]
</p><p>93 The shapes (spacial layout) of the segmentation templates distinguish the constituent parts of the object. [sent-304, score-0.746]
</p><p>94 We have attempted to capture the underlying spirit of the successful language processing approaches – the hierarchical representations based on the recursive composition of constituents and efﬁcient inference and learning algorithms. [sent-309, score-0.474]
</p><p>95 Zhu, “Image segmentation by data-driven markov chain monte carlo,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. [sent-338, score-0.394]
</p><p>96 Carreira-Perpi˜ an, “Multiscale conditional random ﬁelds for image labeling,” in Proceedings of IEEE n´ Computer Society Conference on Computer Vision and Pattern Recognition, 2004, pp. [sent-373, score-0.272]
</p><p>97 Jolly, “Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images,” in Proceedings of IEEE International Conference on Computer Vision, 2001, pp. [sent-390, score-0.514]
</p><p>98 Torralba, “Building the gist of a scene: the role of global image features in recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. [sent-394, score-0.409]
</p><p>99 Zhang, “Rapid inference on a novel and/or graph for object detection, segmentation and parsing,” in Advances in Neural Information Processing Systems, 2007. [sent-417, score-0.552]
</p><p>100 Triggs, “Scene segmentation with crfs learned from partially labeled images,” in Advances in Neural Information Processing Systems, vol. [sent-437, score-0.394]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('segmentation', 0.394), ('parsing', 0.35), ('image', 0.272), ('templates', 0.27), ('sa', 0.254), ('hierarchical', 0.153), ('hierarchy', 0.152), ('parse', 0.129), ('labeling', 0.126), ('ca', 0.12), ('textonboost', 0.117), ('oq', 0.111), ('constituents', 0.107), ('language', 0.106), ('object', 0.104), ('cb', 0.1), ('crf', 0.098), ('boosting', 0.097), ('xr', 0.094), ('vision', 0.085), ('cow', 0.083), ('grass', 0.083), ('pixel', 0.079), ('msrc', 0.078), ('gist', 0.078), ('region', 0.076), ('labels', 0.073), ('yuille', 0.072), ('ds', 0.071), ('oa', 0.067), ('horse', 0.067), ('recognition', 0.066), ('grammars', 0.063), ('nodes', 0.06), ('truth', 0.06), ('images', 0.06), ('zhu', 0.059), ('global', 0.059), ('ground', 0.058), ('dp', 0.058), ('node', 0.057), ('er', 0.057), ('xq', 0.055), ('inference', 0.054), ('levels', 0.054), ('template', 0.054), ('recursive', 0.054), ('hours', 0.053), ('sb', 0.053), ('proceedings', 0.05), ('public', 0.05), ('constituent', 0.05), ('pixels', 0.05), ('collins', 0.049), ('conference', 0.049), ('style', 0.048), ('objects', 0.044), ('pairs', 0.043), ('gure', 0.043), ('capable', 0.043), ('classi', 0.042), ('regions', 0.042), ('exploiting', 0.041), ('lin', 0.04), ('segmentations', 0.04), ('firstly', 0.04), ('phrase', 0.04), ('tu', 0.04), ('panel', 0.04), ('encodes', 0.039), ('stomach', 0.039), ('levin', 0.039), ('leg', 0.039), ('dictionary', 0.039), ('energy', 0.038), ('designed', 0.037), ('tree', 0.037), ('chen', 0.036), ('pair', 0.036), ('linguistics', 0.036), ('contextual', 0.036), ('parsers', 0.036), ('ya', 0.036), ('ch', 0.036), ('dist', 0.036), ('emphasizes', 0.036), ('polynomial', 0.035), ('computer', 0.035), ('ieee', 0.035), ('scene', 0.034), ('classes', 0.034), ('triggs', 0.033), ('potts', 0.033), ('attempts', 0.033), ('coarse', 0.032), ('rapid', 0.032), ('colors', 0.032), ('parts', 0.032), ('maxy', 0.031), ('pays', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="191-tfidf-1" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>2 0.26273584 <a title="191-tfidf-2" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>3 0.22506131 <a title="191-tfidf-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.19901776 <a title="191-tfidf-4" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>5 0.1801351 <a title="191-tfidf-5" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>6 0.13658836 <a title="191-tfidf-6" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>7 0.13549665 <a title="191-tfidf-7" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>8 0.12103796 <a title="191-tfidf-8" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>9 0.12061167 <a title="191-tfidf-9" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>10 0.12023581 <a title="191-tfidf-10" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>11 0.11990166 <a title="191-tfidf-11" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>12 0.11800595 <a title="191-tfidf-12" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>13 0.11440451 <a title="191-tfidf-13" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>14 0.11014877 <a title="191-tfidf-14" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>15 0.10846043 <a title="191-tfidf-15" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>16 0.10625581 <a title="191-tfidf-16" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>17 0.10448453 <a title="191-tfidf-17" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>18 0.10440172 <a title="191-tfidf-18" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>19 0.10420186 <a title="191-tfidf-19" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>20 0.10222589 <a title="191-tfidf-20" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.262), (1, -0.164), (2, 0.187), (3, -0.24), (4, -0.038), (5, -0.028), (6, -0.055), (7, -0.136), (8, 0.044), (9, -0.055), (10, -0.096), (11, -0.02), (12, 0.12), (13, -0.051), (14, -0.022), (15, 0.033), (16, -0.054), (17, -0.13), (18, -0.041), (19, 0.012), (20, 0.111), (21, 0.075), (22, 0.073), (23, 0.019), (24, -0.064), (25, 0.013), (26, 0.017), (27, -0.052), (28, -0.064), (29, -0.044), (30, 0.064), (31, -0.165), (32, -0.069), (33, 0.012), (34, -0.027), (35, -0.137), (36, 0.055), (37, 0.145), (38, 0.021), (39, -0.132), (40, 0.063), (41, 0.102), (42, -0.103), (43, 0.078), (44, -0.035), (45, -0.023), (46, 0.077), (47, 0.095), (48, 0.075), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95943338 <a title="191-lsi-1" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>2 0.78694326 <a title="191-lsi-2" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>3 0.67390674 <a title="191-lsi-3" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>4 0.61343759 <a title="191-lsi-4" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>5 0.56870472 <a title="191-lsi-5" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>6 0.56171322 <a title="191-lsi-6" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>7 0.55436563 <a title="191-lsi-7" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>8 0.55209875 <a title="191-lsi-8" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>9 0.54345131 <a title="191-lsi-9" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>10 0.52796978 <a title="191-lsi-10" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>11 0.52324617 <a title="191-lsi-11" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>12 0.49915522 <a title="191-lsi-12" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>13 0.47968012 <a title="191-lsi-13" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>14 0.46686339 <a title="191-lsi-14" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>15 0.44749042 <a title="191-lsi-15" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>16 0.44381219 <a title="191-lsi-16" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>17 0.43673256 <a title="191-lsi-17" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>18 0.43140081 <a title="191-lsi-18" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>19 0.40319267 <a title="191-lsi-19" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>20 0.39857832 <a title="191-lsi-20" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.042), (7, 0.055), (12, 0.029), (28, 0.109), (57, 0.538), (59, 0.015), (63, 0.018), (77, 0.022), (78, 0.012), (81, 0.01), (83, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94513822 <a title="191-lda-1" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>2 0.92807293 <a title="191-lda-2" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>3 0.9271493 <a title="191-lda-3" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>Author: Viren Jain, Sebastian Seung</p><p>Abstract: We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from speciﬁc noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we ﬁnd that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random ﬁeld (MRF) methods. Moreover, we ﬁnd that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean ﬁeld theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difﬁculties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is signiﬁcantly less than that associated with inference in MRF approaches with even hundreds of parameters. 1 Background Low-level image processing tasks include edge detection, interpolation, and deconvolution. These tasks are useful both in themselves, and as a front-end for high-level visual tasks like object recognition. This paper focuses on the task of denoising, deﬁned as the recovery of an underlying image from an observation that has been subjected to Gaussian noise. One approach to image denoising is to transform an image from pixel intensities into another representation where statistical regularities are more easily captured. For example, the Gaussian scale mixture (GSM) model introduced by Portilla and colleagues is based on a multiscale wavelet decomposition that provides an effective description of local image statistics [1, 2]. Another approach is to try and capture statistical regularities of pixel intensities directly using Markov random ﬁelds (MRFs) to deﬁne a prior over the image space. Initial work used handdesigned settings of the parameters, but recently there has been increasing success in learning the parameters of such models from databases of natural images [3, 4, 5, 6, 7, 8]. Prior models can be used for tasks such as image denoising by augmenting the prior with a noise model. Alternatively, an MRF can be used to model the probability distribution of the clean image conditioned on the noisy image. This conditional random ﬁeld (CRF) approach is said to be discriminative, in contrast to the generative MRF approach. Several researchers have shown that the CRF approach can outperform generative learning on various image restoration and labeling tasks [9, 10]. CRFs have recently been applied to the problem of image denoising as well [5]. 1 The present work is most closely related to the CRF approach. Indeed, certain special cases of convolutional networks can be seen as performing maximum likelihood inference on a CRF [11]. The advantage of the convolutional network approach is that it avoids a general difﬁculty with applying MRF-based methods to image analysis: the computational expense associated with both parameter estimation and inference in probabilistic models. For example, naive methods of learning MRFbased models involve calculation of the partition function, a normalization factor that is generally intractable for realistic models and image dimensions. As a result, a great deal of research has been devoted to approximate MRF learning and inference techniques that meliorate computational difﬁculties, generally at the cost of either representational power or theoretical guarantees [12, 13]. Convolutional networks largely avoid these difﬁculties by posing the computational task within the statistical framework of regression rather than density estimation. Regression is a more tractable computation and therefore permits models with greater representational power than methods based on density estimation. This claim will be argued for with empirical results on the denoising problem, as well as mathematical connections between MRF and convolutional network approaches. 2 Convolutional Networks Convolutional networks have been extensively applied to visual object recognition using architectures that accept an image as input and, through alternating layers of convolution and subsampling, produce one or more output values that are thresholded to yield binary predictions regarding object identity [14, 15]. In contrast, we study networks that accept an image as input and produce an entire image as output. Previous work has used such architectures to produce images with binary targets in image restoration problems for specialized microscopy data [11, 16]. Here we show that similar architectures can also be used to produce images with the analog ﬂuctuations found in the intensity distributions of natural images. Network Dynamics and Architecture A convolutional network is an alternating sequence of linear ﬁltering and nonlinear transformation operations. The input and output layers include one or more images, while intermediate layers contain “hidden</p><p>4 0.90782082 <a title="191-lda-4" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>Author: Daniel M. Roy, Yee W. Teh</p><p>Abstract: We describe a novel class of distributions, called Mondrian processes, which can be interpreted as probability distributions over kd-tree data structures. Mondrian processes are multidimensional generalizations of Poisson processes and this connection allows us to construct multidimensional generalizations of the stickbreaking process described by Sethuraman (1994), recovering the Dirichlet process in one dimension. After introducing the Aldous-Hoover representation for jointly and separately exchangeable arrays, we show how the process can be used as a nonparametric prior distribution in Bayesian models of relational data. 1</p><p>5 0.89547056 <a title="191-lda-5" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>Author: Jihun Hamm, Daniel D. Lee</p><p>Abstract: Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of afﬁne as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classiﬁcation and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases. 1</p><p>6 0.79854637 <a title="191-lda-6" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>7 0.77766895 <a title="191-lda-7" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>8 0.72707248 <a title="191-lda-8" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>9 0.6801582 <a title="191-lda-9" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>10 0.64054054 <a title="191-lda-10" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>11 0.63538909 <a title="191-lda-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.6249184 <a title="191-lda-12" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>13 0.61024392 <a title="191-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.59690076 <a title="191-lda-14" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>15 0.59678602 <a title="191-lda-15" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>16 0.59610641 <a title="191-lda-16" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>17 0.59325486 <a title="191-lda-17" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>18 0.58968496 <a title="191-lda-18" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>19 0.58635694 <a title="191-lda-19" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>20 0.58631426 <a title="191-lda-20" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
