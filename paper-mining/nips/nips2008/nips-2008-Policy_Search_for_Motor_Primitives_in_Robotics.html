<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2008-Policy Search for Motor Primitives in Robotics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-181" href="#">nips2008-181</a> knowledge-graph by maker-knowledge-mining</p><h1>181 nips-2008-Policy Search for Motor Primitives in Robotics</h1>
<br/><p>Source: <a title="nips-2008-181-pdf" href="http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf">pdf</a></p><p>Author: Jens Kober, Jan R. Peters</p><p>Abstract: Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAMTM robot arm. 1</p><p>Reference: <a title="nips-2008-181-reference" href="../nips2008_reference/nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.575), ('rollout', 0.279), ('primit', 0.278), ('episod', 0.259), ('robot', 0.252), ('imit', 0.229), ('reinforc', 0.208), ('st', 0.165), ('cup', 0.154), ('mot', 0.143), ('underactu', 0.139), ('enac', 0.122), ('fdg', 0.105), ('vpg', 0.105), ('wamtm', 0.105), ('kinesthet', 0.087), ('rwr', 0.087), ('barret', 0.084), ('return', 0.083), ('torqu', 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="181-tfidf-1" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>2 0.47060961 <a title="181-tfidf-2" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>3 0.36376005 <a title="181-tfidf-3" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>4 0.27972308 <a title="181-tfidf-4" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>5 0.26954177 <a title="181-tfidf-5" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>6 0.25245094 <a title="181-tfidf-6" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>7 0.23004064 <a title="181-tfidf-7" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>8 0.21184888 <a title="181-tfidf-8" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>9 0.19027448 <a title="181-tfidf-9" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>10 0.18926474 <a title="181-tfidf-10" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>11 0.16485341 <a title="181-tfidf-11" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>12 0.16138826 <a title="181-tfidf-12" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>13 0.15439312 <a title="181-tfidf-13" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>14 0.12667881 <a title="181-tfidf-14" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>15 0.10361792 <a title="181-tfidf-15" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>16 0.09709318 <a title="181-tfidf-16" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>17 0.094612978 <a title="181-tfidf-17" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>18 0.09363471 <a title="181-tfidf-18" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>19 0.092365243 <a title="181-tfidf-19" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>20 0.082931608 <a title="181-tfidf-20" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.198), (1, -0.457), (2, 0.181), (3, 0.176), (4, 0.291), (5, 0.064), (6, -0.16), (7, -0.04), (8, 0.006), (9, 0.04), (10, -0.061), (11, -0.032), (12, -0.006), (13, 0.004), (14, 0.084), (15, -0.067), (16, 0.052), (17, 0.041), (18, -0.027), (19, 0.11), (20, -0.057), (21, 0.114), (22, 0.045), (23, 0.024), (24, 0.015), (25, 0.043), (26, 0.005), (27, -0.004), (28, -0.088), (29, -0.008), (30, 0.028), (31, 0.08), (32, -0.03), (33, -0.044), (34, 0.05), (35, -0.027), (36, -0.072), (37, 0.058), (38, -0.003), (39, 0.003), (40, -0.021), (41, 0.023), (42, 0.084), (43, 0.004), (44, 0.021), (45, 0.017), (46, -0.029), (47, 0.042), (48, 0.007), (49, -0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96492225 <a title="181-lsi-1" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>2 0.91242361 <a title="181-lsi-2" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>3 0.7597658 <a title="181-lsi-3" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>4 0.75939691 <a title="181-lsi-4" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>5 0.75189209 <a title="181-lsi-5" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>6 0.71975797 <a title="181-lsi-6" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>7 0.67614508 <a title="181-lsi-7" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>8 0.6307748 <a title="181-lsi-8" href="./nips-2008-Bounding_Performance_Loss_in_Approximate_MDP_Homomorphisms.html">39 nips-2008-Bounding Performance Loss in Approximate MDP Homomorphisms</a></p>
<p>9 0.63032103 <a title="181-lsi-9" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>10 0.58893758 <a title="181-lsi-10" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>11 0.57316333 <a title="181-lsi-11" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>12 0.54436767 <a title="181-lsi-12" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>13 0.43125561 <a title="181-lsi-13" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>14 0.39178771 <a title="181-lsi-14" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>15 0.38934353 <a title="181-lsi-15" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>16 0.3673678 <a title="181-lsi-16" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>17 0.36706999 <a title="181-lsi-17" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>18 0.35085338 <a title="181-lsi-18" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>19 0.29943275 <a title="181-lsi-19" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>20 0.29940557 <a title="181-lsi-20" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.021), (29, 0.014), (30, 0.051), (40, 0.049), (60, 0.071), (61, 0.031), (63, 0.215), (64, 0.075), (71, 0.102), (82, 0.249)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80936372 <a title="181-lda-1" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>same-paper 2 0.78278124 <a title="181-lda-2" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>3 0.70384413 <a title="181-lda-3" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>4 0.69724429 <a title="181-lda-4" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>5 0.69632602 <a title="181-lda-5" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>6 0.69533002 <a title="181-lda-6" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>7 0.69284552 <a title="181-lda-7" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>8 0.69208401 <a title="181-lda-8" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>9 0.68920964 <a title="181-lda-9" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>10 0.68804139 <a title="181-lda-10" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>11 0.67748398 <a title="181-lda-11" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>12 0.67617607 <a title="181-lda-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.67604291 <a title="181-lda-13" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>14 0.67598343 <a title="181-lda-14" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>15 0.67514205 <a title="181-lda-15" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>16 0.67476463 <a title="181-lda-16" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>17 0.67205501 <a title="181-lda-17" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>18 0.67016888 <a title="181-lda-18" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>19 0.6695779 <a title="181-lda-19" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>20 0.6662879 <a title="181-lda-20" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
