<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-244" href="#">nips2008-244</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</h1>
<br/><p>Source: <a title="nips-2008-244-pdf" href="http://papers.nips.cc/paper/3587-unifying-the-sensory-and-motor-components-of-sensorimotor-adaptation.pdf">pdf</a></p><p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>Reference: <a title="nips-2008-244-reference" href="../nips2008_reference/nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Adaptation of visually guided reaching movements in novel visuomotor environments (e. [sent-16, score-0.428]
</p><p>2 wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. [sent-18, score-1.767]
</p><p>3 Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. [sent-19, score-2.106]
</p><p>4 We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. [sent-20, score-1.737]
</p><p>5 Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. [sent-21, score-0.858]
</p><p>6 This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. [sent-22, score-1.261]
</p><p>7 Such visuomotor adaptation is multifaceted, comprising both sensory and motor components [5]. [sent-25, score-1.322]
</p><p>8 The sensory components of adaptation can be measured through alignment tests in which subjects are asked to localize either a visual target or their unseen ﬁngertip, with their other (also unseen) ﬁngertip (without being able to make contact between hands). [sent-26, score-1.174]
</p><p>9 These tests reveal substantial shifts in the perceived spatial location of both visual and proprioceptive cues, following adaptation to shifted visual feedback [7]. [sent-27, score-1.338]
</p><p>10 There must therefore be some additional motor component of adaptation, i. [sent-29, score-0.323]
</p><p>11 some change in the relationship between the planned movement and the  disturbances  v rt  }  y rt  p rt  ut  yt  vt  motor command  hand position  pt  proprioceptive observation  Figure 1: Graphical model of a single reach in a motor adaptation experiment. [sent-31, score-3.146]
</p><p>12 Motor command ut , and visual and proprioceptive observations of hand position, vt and pt , are available to the subject. [sent-32, score-0.9]
</p><p>13 Three distinct disturbances aﬀect observations: A motor disy turbance rt may aﬀect the hand position yt given the motor command ut . [sent-33, score-1.514]
</p><p>14 Visual and p v proprioceptive disturbances, rt and rt , may aﬀect the respective observations given hand position. [sent-34, score-0.899]
</p><p>15 This argument is reinforced by the ﬁnding that patterns of reach aftereﬀects following visuomotor adaptation depend strongly on the motor task performed during adaptation [5]. [sent-36, score-1.579]
</p><p>16 From a modelling point of view, the sensory and motor components of adaptation have previously only been addressed in isolation of one another. [sent-37, score-1.094]
</p><p>17 Previously proposed models of sensory adaptation have assumed that it is driven purely by discrepancies between hand position estimates from diﬀerent sensory modalities. [sent-38, score-1.244]
</p><p>18 On its own, this sensory adaptation model cannot provide a complete description of visuomotor adaptation since it does not fully account for improvements in performance from trial to trial. [sent-41, score-1.703]
</p><p>19 It can, however, be plausibly combined with a conventional error-driven motor adaptation model in which the performance error is calculated using the maximum likelihood estimate of hand position. [sent-42, score-0.994]
</p><p>20 The resulting composite model could plausibly account for both performance improvements and perceptual shifts during visuomotor adaptation. [sent-43, score-0.476]
</p><p>21 According to this view, sensory and motor adaptation are very much independent processes, one driven by sensory discrepancy and the other driven by (estimated) task performance error. [sent-44, score-1.494]
</p><p>22 This uniﬁed sensory and motor adaptation model is also able to account for both performance improvements and perceptual shifts during visuomotor adaptation. [sent-46, score-1.504]
</p><p>23 However, our uniﬁed model also makes the surprising prediction that a motor disturbance, e. [sent-47, score-0.327]
</p><p>24 an external force applied to hand via a manipulandum, will also elicit sensory adaptation. [sent-49, score-0.459]
</p><p>25 The MLE-based model predicts no such sensory adaptation, since there is never any discrepancy between sensory modalities. [sent-50, score-0.591]
</p><p>26 We test this prediction directly with an experiment (Section 5) and ﬁnd that force ﬁeld adaptation does indeed lead to sensory as well as motor adaptation. [sent-51, score-1.181]
</p><p>27 2  Modelling framework  Before describing the details of the models, we ﬁrst outline a basic mathematical framework for describing reaching movements in the context of a motor adaptation experiment, representing the assumptions common to both the MLE-based and the Bayesian adaptation models. [sent-52, score-1.527]
</p><p>28 Figure 1 illustrates a graphical model of a single reaching movement during an adaptation experiment, from the subject’s point of view. [sent-53, score-0.726]
</p><p>29 The multiple components of visuomotor adaptation described above correspond to three distinct potential sources of observed outcome error (across both observation) modalities in a single reaching trial. [sent-54, score-0.961]
</p><p>30 On trial t, the subject generates a (known) motor command ut . [sent-55, score-0.651]
</p><p>31 This motor command ut leads to a ﬁnal hand position yt , which also depends on some (unknown) motor disturbance  rv  rp  Figure 2: MLE-based sensor adaptation model. [sent-56, score-1.833]
</p><p>32 Visual and proprioceptive disturbances rv , rp are treated as parameters of the model. [sent-57, score-0.619]
</p><p>33 Estimates rt ˆv and rt of these parameters are maintained via an ˆp online EM-like procedure. [sent-58, score-0.502]
</p><p>34 an external force applied to the hand) and motor noise ǫu . [sent-61, score-0.439]
</p><p>35 We assume the ﬁnal t hand position yt is given by y yt = ut + rt + ǫu , (1) t u 2 where ǫt ∼ N (0, σu ). [sent-62, score-0.768]
</p><p>36 Instead, noisy and potentially shifted observations are available through visual and proprioceptive modalities, vt pt  = =  v yt + rt + ǫv , t p yt + rt + ǫp , t  (2) (3)  2 where the observation noises ǫv and ǫp are zero-mean and Gaussian with variances σv and t t 2 σp , respectively. [sent-67, score-1.445]
</p><p>37 We denote the full set of potential disturbances on trial t by p y v rt = (rt , rt , rt )T . [sent-68, score-1.08]
</p><p>38 (4) (ˆt , rt , rt )T rv ˆp ˆy  We assume that the subject maintains an internal estimate ˆt = r of the total disturbance rt and selects his motor commands on each trial accordingly. [sent-69, score-1.469]
</p><p>39 For reaches to a ∗ visual target located at vt , the appropriate motor command is given by ∗ ut = vt − rt − rt . [sent-70, score-1.343]
</p><p>40 ˆv ˆy  (5)  Adaptation can be viewed as a process of iteratively updating the disturbance estimate, ˆt , r following each trial given the new (noisy) observations vt and pt and the motor command ut . [sent-71, score-0.965]
</p><p>41 3  Existing sensory adaptation models  The prevailing view of sensory adaptation centres around the principle of maximum likelihood estimation and was ﬁrst proposed by Ghahramani et al. [sent-73, score-1.556]
</p><p>42 It has nevertheless been wideley accepted as a model of how the nervous system deals with visual and proprioceptive cues. [sent-75, score-0.5]
</p><p>43 [7], for instance, based an analysis of the relative uncertainty of visual and proprioceptive estimates of hand location on this principle. [sent-77, score-0.582]
</p><p>44 We suppose that, given the subject’s current estimate of the visual and proprioceptive disturbance, rt and rt , the visual and proprioceptive estimates of hand position are given ˆv ˆp by yt ˆv yt ˆp  = vt − rt , ˆv = pt − rt ˆp  (6) (7)  respectively. [sent-78, score-2.528]
</p><p>45 The maximum likelihood estimate (MLE) of the true hand position yt is given by yt ˆMLE =  2 σp σ2 yt + 2 v 2 yt . [sent-80, score-0.699]
</p><p>46 ˆv ˆp 2 2 σv + σp σv + σp  (8)  v rt+1  v rt  y rt+1  y rt  p rt+1  p rt  ut  yt  vt  ut+1  yt+1  vt+1  pt  Figure 3: Bayesian combined sensory and motor adaptation model. [sent-81, score-2.253]
</p><p>47 The subject assumes that disturbances vary randomly, but smoothly, from trial to trial. [sent-82, score-0.364]
</p><p>48 pt+1  The MLE-based sensory adaptation model states that subjects adapt their future visual and proprioceptive estimates of hand location towards the MLE in such a way that the MLE itself remains unchanged. [sent-83, score-1.471]
</p><p>49 The corresponding updates are given by rt+1 ˆv  =  rt + ηwp [ˆt − yt ] , ˆv y p ˆv  (9)  rt+1 ˆp  =  rt + ηwv [ˆt − yt ] , ˆp y v ˆp  (10)  where η is some ﬁxed adaptation rate. [sent-84, score-1.314]
</p><p>50 This adaptation principle can be interpreted as an online expectation-maximization (EM) procedure in the graphical model shown in Figure 2. [sent-85, score-0.548]
</p><p>51 1  Extending the MLE model to account for motor component of adaptation  As it stands, the MLE-based model described above only accounts for sensory adaptation and does not provide a complete description of sensorimotor adaptation. [sent-89, score-1.776]
</p><p>52 Visual adaptation will aﬀect the estimated location of a visual target, and therefore also the planned movement, but the eﬀect on performance will not be enough to account for complete (or nearly complete) adaptation. [sent-90, score-0.771]
</p><p>53 The performance gain from this component of adaptation will be equal to the discrepancy between the initial visual setimate of hand posion and the MLE - which will be substantially less than the experimentally imposed shift. [sent-91, score-0.818]
</p><p>54 This sensory adaptation model can, however, be plausibly combined with a conventional error-driven state space model [6, 1] of motor adaptation to yield an additional motor component of adaptation rt . [sent-92, score-2.804]
</p><p>55 The resulting update for the estimated motor disturbance rt on trial t is given by ˆy rt+1 = rt + γ(ˆt − yt ˆy ˆy y ∗ ˆMLE ), where rate. [sent-94, score-1.225]
</p><p>56 yt ˆ∗  = (v  ∗  − rt ) ˆv  (11)  is the estimated desired hand location, and γ is some ﬁxed adaptation  This combined model reﬂects the view that sensory and motor adaptation are distinct processes. [sent-95, score-2.092]
</p><p>57 The sensory adaptation component is driven purely by discrepancy between the senses, while the motor adaptation component only has access to a single, fused estimate of hand position and is driven purely by estimated performance error. [sent-96, score-2.034]
</p><p>58 4  Uniﬁed Bayesian sensory and motor adapatation model  We propose an alternative approach to solving the sensorimotor adaptation problem. [sent-97, score-1.173]
</p><p>59 Rather than treat the visual shifts rv and rp as parameters, we consider all the disturbances (iny cluding rt ) as dynamic random variables. [sent-98, score-0.787]
</p><p>60 We assume that the subject’s beliefs about how  Directional Error/o  30 Data Bayesian Model MLE Model  Figure 4: Model comparison with visuomotor adaptation data. [sent-99, score-0.752]
</p><p>61 The Bayesian model (solid blue line) and MLE-based model (dashed red line) were ﬁtted to performance data (ﬁlled circles) from a visuomotor adaptation experiment [4]. [sent-100, score-0.8]
</p><p>62 Both models made qualitatively similar predictions about how adaptation was distributed across components. [sent-101, score-0.524]
</p><p>63 20  10  0  0  5  10  15 20 Trial Number  25  30  these disturbances evolve over time are characterised by a trial-to-trial disturbance dynamics model given by rt+1 = Art + η t , (12) where A is some diagonal matrix and η t is a random drift term with zero mean and diagonal covariance matrix Q, i. [sent-102, score-0.403]
</p><p>64 These parameters reﬂect the statistics of the usual ﬂuctuations in sensory calibration errors and motor plant dynamics, which the sensorimotor system must adapt to on an ongoing basis. [sent-108, score-0.625]
</p><p>65 Combining these assumptions with the statistical model of each individual trial described in Section 2 (and Figure 1), gives rise to a dynamical model of the disturbances and their impact on reaching movements, across all trials. [sent-110, score-0.498]
</p><p>66 We propose that the patterns of adaptation and the sensory aftereﬀects exhibited by subjects correspond to optimal inference of the disturbances rt within this model, given the observations on each trial. [sent-112, score-1.365]
</p><p>67 The latent state tracked by the Kalman v p y ﬁlter is the vector of disturbances rt = (rt , rt , rt )T , with state dynamics given by (12). [sent-114, score-0.98]
</p><p>68 The observations vt and pt are related to the disturbances via vt pt  =  ut ut  +  1 0 0 1  1 1  (rt + ǫt ) ,  (14)  where ǫt = (ǫv , ǫp , ǫu )T . [sent-115, score-0.793]
</p><p>69 We can write this in a more conventional form as t t t zt = Hrt + Hǫt ,  (15)  T  where zt = (vt − ut , pt − ut ) and H is the matrix of 1’s and 0’s in equation (14). [sent-116, score-0.317]
</p><p>70 (16)  The standard Kalman ﬁlter update equations can be used to predict how a subject will update estimates of the disturbances following each trial and therefore how he will select his actions on the next trial, leading to a full prediction of performance from the ﬁrst trial onwards. [sent-118, score-0.51]
</p><p>71 5  Model comparison and experiments  We have described two alternative models of visuomotor adaptation which we have claimed can account for both the motor and sensory components of adaptation. [sent-119, score-1.357]
</p><p>72 We ﬁtted both  (a)  (b)  Error  Target  Adapted trajectory  Catch trial trajectory  y x Start  Figure 5: (a) Experimental Setup, (b) Sample trajectories and performance error measure  models to performance data from a visuomotor adaptation experiment [4] to validate this claim. [sent-120, score-0.974]
</p><p>73 In this study in which this data was taken from, subjects performed visually guided reaching movements to a number of targets. [sent-121, score-0.297]
</p><p>74 The spread of adaptation across components of the model was qualitatively similar between the two models, although no data on perceptual aftereﬀects was available from this study for quantitative comparison. [sent-130, score-0.618]
</p><p>75 While the MLE-based model predicts there will be sensory adaptation only when there is a discrepancy between the senses, the Bayesian model predicts that there will also be sensory adaptation in response to a motor disturbance such as an external force applied to the hand). [sent-137, score-2.254]
</p><p>76 Just as a purely visual disturbance can lead to a multifaceted adaptive response, so can a purely motor disturbance, with both motor and sensory components predicted, even though there is never any discrepancy between the senses. [sent-138, score-1.344]
</p><p>77 1  Experimental Methods  We experimentally tested the hypothesis that force ﬁeld adaptation would lead to sensory adaptation. [sent-141, score-0.878]
</p><p>78 We tested 11 subjects who performed a series of trials consisting of reaching movements interleaved with perceptual alignment tests. [sent-142, score-0.433]
</p><p>79 In the movement phase, subjects made an out-and-back reaching movement towards a visual target with their right hand. [sent-146, score-0.493]
</p><p>80 In the visual localization phase, a visual target was displayed pseudorandomly in one of 5 positions and the subjects moved their left ﬁngertip to the perceived location of the target. [sent-147, score-0.581]
</p><p>81 In the proprioceptive localization phase, the right hand was passively moved to a random target location, with no visual cue of its position, and subjects moved their left ﬁngertip to the perceived location of the right hand. [sent-148, score-0.849]
</p><p>82 A leftward lateral force Fx was applied to the right hand during the reaching phase. [sent-164, score-0.316]
</p><p>83 ˙ After steadily incrementing a during 50 adaptation trials, the force ﬁeld was then kept constant at a = 0. [sent-171, score-0.634]
</p><p>84 All subjects received a catch trial at the very end in which the force ﬁeld was turned oﬀ. [sent-173, score-0.37]
</p><p>85 The particular force ﬁeld used was chosen so that the cursor trajectories (and motor commands required to counter the perturbation) would be as close as possible to those used to generate the linear trajectories required when exposed to a visuomotor shift (such as that described in [7]). [sent-174, score-0.801]
</p><p>86 The initial outward part of the catch trial trajectory, the initial movement is very straight, implying that similar motor commands were used to those required by a visuomotor shift. [sent-177, score-0.816]
</p><p>87 2  Results  We compared the average performance in the visual and proprioceptive alignment tests before and after adaptation in the velocity-dependent force ﬁeld. [sent-179, score-1.213]
</p><p>88 Most subjects exhibited small but signiﬁcant shifts in performance in both the visual and proprioceptive alignment tests. [sent-181, score-0.77]
</p><p>89 We found signiﬁcant lateral shifts in both visual and proprioceptive localization error in the direction of the perturbation (both p < . [sent-183, score-0.688]
</p><p>90 Since there was never any sensory discrepancy, the MLE-based model predicted no change in the localization task. [sent-198, score-0.333]
</p><p>91 6  Conclusions and discussion  Our experimental results demonstrate that adaptation of reaching movements in a force ﬁeld results in shifts in visual and proprioceptive spatial perception. [sent-199, score-1.365]
</p><p>92 Several recent models have similarly described motor adaptation as a process of Bayesian inference of the potential causes of observed error. [sent-202, score-0.827]
</p><p>93 [3] proposed a model of o saccade adaptation and Krakauer et al. [sent-204, score-0.548]
</p><p>94 Our work extends the framework of these models to include multiple observation modalities instead of just one, and multiple classes of disturbances which aﬀect the diﬀerent observation modalities in diﬀerent, experimentally measurable ways. [sent-206, score-0.285]
</p><p>95 Overall, our results suggest that the nervous system solves the problems of sensory and motor adaptation in a principled and uniﬁed manner, supporting the view that sensorimotor adaptation proceeds according to optimal estimation of encountered disturbances. [sent-207, score-1.713]
</p><p>96 Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control. [sent-209, score-0.303]
</p><p>97 The dynamics of o memory as a consequence of optimal adaptation to a changing body. [sent-226, score-0.548]
</p><p>98 Generalization of motor learning depends on the history of prior action. [sent-229, score-0.303]
</p><p>99 Visual-shift adaptation is composed of separable sensory and task-dependent eﬀects. [sent-238, score-0.768]
</p><p>100 Learning of action through adaptive combination of motor primitives. [sent-241, score-0.303]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adaptation', 0.524), ('proprioceptive', 0.319), ('motor', 0.303), ('rt', 0.251), ('sensory', 0.244), ('visuomotor', 0.228), ('disturbances', 0.203), ('disturbance', 0.152), ('yt', 0.144), ('visual', 0.137), ('trial', 0.124), ('reaching', 0.123), ('force', 0.11), ('ut', 0.106), ('mle', 0.103), ('shifts', 0.099), ('subjects', 0.097), ('vt', 0.094), ('alignment', 0.092), ('pt', 0.085), ('command', 0.081), ('discrepancy', 0.079), ('sensorimotor', 0.078), ('ngertip', 0.072), ('position', 0.065), ('localization', 0.065), ('rv', 0.062), ('aftere', 0.058), ('hand', 0.058), ('movement', 0.055), ('movements', 0.053), ('driven', 0.05), ('perceptual', 0.047), ('location', 0.046), ('perceived', 0.045), ('plausibly', 0.043), ('modalities', 0.041), ('bayesian', 0.041), ('catch', 0.039), ('uni', 0.039), ('reza', 0.038), ('commands', 0.038), ('subject', 0.037), ('cm', 0.037), ('purely', 0.037), ('rp', 0.035), ('account', 0.035), ('shift', 0.035), ('aic', 0.035), ('cursor', 0.035), ('ect', 0.034), ('eld', 0.034), ('tests', 0.031), ('beers', 0.029), ('birmingham', 0.029), ('goggles', 0.029), ('krakauer', 0.029), ('manipulandum', 0.029), ('multifaceted', 0.029), ('outward', 0.029), ('planned', 0.029), ('proprioception', 0.029), ('directional', 0.028), ('moved', 0.028), ('erent', 0.027), ('kalman', 0.027), ('exhibited', 0.026), ('tted', 0.026), ('trajectories', 0.026), ('target', 0.026), ('external', 0.026), ('prism', 0.025), ('wearing', 0.025), ('rding', 0.025), ('modality', 0.025), ('lateral', 0.025), ('trajectory', 0.025), ('model', 0.024), ('guided', 0.024), ('dynamics', 0.024), ('fused', 0.023), ('wolpert', 0.023), ('oct', 0.023), ('components', 0.023), ('ects', 0.023), ('phase', 0.022), ('error', 0.022), ('estimates', 0.022), ('trials', 0.021), ('di', 0.021), ('perturbation', 0.021), ('senses', 0.021), ('elicit', 0.021), ('view', 0.02), ('component', 0.02), ('conventional', 0.02), ('observations', 0.02), ('fx', 0.02), ('nervous', 0.02), ('post', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="244-tfidf-1" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>2 0.17957172 <a title="244-tfidf-2" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>3 0.14167836 <a title="244-tfidf-3" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>Author: Jens Kober, Jan R. Peters</p><p>Abstract: Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAMTM robot arm. 1</p><p>4 0.13682148 <a title="244-tfidf-4" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>5 0.1328387 <a title="244-tfidf-5" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>6 0.11862244 <a title="244-tfidf-6" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>7 0.10340969 <a title="244-tfidf-7" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>8 0.10339432 <a title="244-tfidf-8" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>9 0.10150441 <a title="244-tfidf-9" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>10 0.093516231 <a title="244-tfidf-10" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>11 0.088644773 <a title="244-tfidf-11" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>12 0.08585529 <a title="244-tfidf-12" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>13 0.082231492 <a title="244-tfidf-13" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>14 0.079523943 <a title="244-tfidf-14" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>15 0.074193478 <a title="244-tfidf-15" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>16 0.070477001 <a title="244-tfidf-16" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>17 0.069692306 <a title="244-tfidf-17" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>18 0.069688104 <a title="244-tfidf-18" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>19 0.066415757 <a title="244-tfidf-19" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>20 0.064338557 <a title="244-tfidf-20" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.142), (1, 0.093), (2, 0.078), (3, -0.019), (4, -0.002), (5, 0.155), (6, -0.039), (7, 0.11), (8, 0.166), (9, -0.003), (10, -0.009), (11, 0.086), (12, -0.129), (13, 0.1), (14, -0.028), (15, 0.067), (16, -0.057), (17, 0.113), (18, -0.245), (19, 0.109), (20, 0.245), (21, 0.004), (22, -0.088), (23, -0.002), (24, 0.004), (25, 0.056), (26, -0.005), (27, 0.168), (28, -0.045), (29, 0.118), (30, 0.082), (31, 0.083), (32, -0.026), (33, 0.077), (34, 0.014), (35, 0.047), (36, -0.038), (37, -0.002), (38, 0.074), (39, -0.137), (40, 0.145), (41, -0.005), (42, -0.071), (43, -0.05), (44, 0.074), (45, -0.061), (46, -0.007), (47, 0.102), (48, -0.05), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9829734 <a title="244-lsi-1" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>2 0.57307166 <a title="244-lsi-2" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>3 0.51993179 <a title="244-lsi-3" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>4 0.51424569 <a title="244-lsi-4" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>5 0.51203156 <a title="244-lsi-5" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>6 0.49432176 <a title="244-lsi-6" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>7 0.47100797 <a title="244-lsi-7" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>8 0.46581036 <a title="244-lsi-8" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>9 0.42280811 <a title="244-lsi-9" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>10 0.38914427 <a title="244-lsi-10" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>11 0.37968338 <a title="244-lsi-11" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>12 0.36713135 <a title="244-lsi-12" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>13 0.3450937 <a title="244-lsi-13" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>14 0.34501943 <a title="244-lsi-14" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>15 0.33941212 <a title="244-lsi-15" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>16 0.33741876 <a title="244-lsi-16" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>17 0.32888272 <a title="244-lsi-17" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>18 0.32019013 <a title="244-lsi-18" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>19 0.29628038 <a title="244-lsi-19" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>20 0.28085598 <a title="244-lsi-20" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.057), (7, 0.075), (12, 0.027), (15, 0.029), (28, 0.237), (41, 0.302), (57, 0.031), (59, 0.013), (63, 0.02), (77, 0.034), (78, 0.011), (83, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86632586 <a title="244-lda-1" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>Author: Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling</p><p>Abstract: In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efﬁcient ﬁltering in a range of multi-agent domains. 1</p><p>same-paper 2 0.8193844 <a title="244-lda-2" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>3 0.78009033 <a title="244-lda-3" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>Author: Silvia Chiappa, Jens Kober, Jan R. Peters</p><p>Abstract: Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efﬁcient inference algorithms. On recorded human Balero movements, this method is not only capable of ﬁnding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.</p><p>4 0.65279591 <a title="244-lda-4" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>Author: Tao Qin, Tie-yan Liu, Xu-dong Zhang, De-sheng Wang, Hang Li</p><p>Abstract: This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for ‘local ranking’, in the sense that the ranking model is deﬁned on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to deﬁne the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is deﬁned as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two speciﬁc information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.</p><p>5 0.65246332 <a title="244-lda-5" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>6 0.6523909 <a title="244-lda-6" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>7 0.65205926 <a title="244-lda-7" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>8 0.65191859 <a title="244-lda-8" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>9 0.64961886 <a title="244-lda-9" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>10 0.64955759 <a title="244-lda-10" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>11 0.64926332 <a title="244-lda-11" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>12 0.64924073 <a title="244-lda-12" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>13 0.64762473 <a title="244-lda-13" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>14 0.64762038 <a title="244-lda-14" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>15 0.64740509 <a title="244-lda-15" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>16 0.6473152 <a title="244-lda-16" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>17 0.64711821 <a title="244-lda-17" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>18 0.64711422 <a title="244-lda-18" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>19 0.64679569 <a title="244-lda-19" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>20 0.64621806 <a title="244-lda-20" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
