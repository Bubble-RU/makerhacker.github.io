<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-42" href="#">nips2008-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</h1>
<br/><p>Source: <a title="nips-2008-42-pdf" href="http://papers.nips.cc/paper/3472-cascaded-classification-models-combining-models-for-holistic-scene-understanding.pdf">pdf</a></p><p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>Reference: <a title="nips-2008-42-reference" href="../nips2008_reference/nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. [sent-5, score-0.188]
</p><p>2 We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. [sent-9, score-0.162]
</p><p>3 We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. [sent-11, score-0.82]
</p><p>4 1  Introduction  The problem of “holistic scene understanding” encompasses a number of notoriously difﬁcult computer vision tasks. [sent-12, score-0.376]
</p><p>5 Presented with an image, scene understanding involves processing the image to answer a number of questions, including: (i) What type of scene is it (e. [sent-13, score-0.847]
</p><p>6 , a car present in the image indicates that the scene is likely to be urban, which in turn makes it more likely to ﬁnd road or building regions. [sent-22, score-0.555]
</p><p>7 Indeed, this idea of communicating information between tasks is not new and dates back to some of the earliest work in computer vision (e. [sent-23, score-0.175]
</p><p>8 While our focus will be on image understanding, the goal of combining related classiﬁers is relevant to many other machine learning domains where several related tasks operate on the same (or related) raw data and provide correlated outputs. [sent-27, score-0.285]
</p><p>9 In the problem of scene understanding (as in many others), state-of-the-art models already exist for many of the tasks of interest. [sent-30, score-0.484]
</p><p>10 Speciﬁcally, the CCM framework creates multiple instantiations of each classiﬁer, and organizes them into tiers where models in the ﬁrst tier learn in isolation, processing the data to produce the best classiﬁcations given only the raw instance features. [sent-37, score-0.379]
</p><p>11 Lower tiers accept as input both the features from the data instance, as well as features computed from the output classiﬁcations of the models at the previous tier. [sent-38, score-0.331]
</p><p>12 We apply our model to the scene understanding task by combining scene categorization, object detection, multi-class segmentation, and 3d reconstruction. [sent-40, score-0.889]
</p><p>13 Importantly, in extensive experiments on large image databases, we show that our combined model yields superior results on all tasks considered. [sent-42, score-0.229]
</p><p>14 In computer vision, this idea has been very successfully applied to the task of face detection using the so-called Cascade of Boosted Ensembles (CoBE) [18, 2] framework. [sent-48, score-0.146]
</p><p>15 While similar to our work in constructing a cascade of classiﬁers, their motivation was computational efﬁciency, rather than a consideration of contextual beneﬁts. [sent-49, score-0.168]
</p><p>16 Kumar and Hebert [9], for example, develop a large MRF-based probabilistic model linking multiclass segmentation and object detection. [sent-53, score-0.423]
</p><p>17 For example, in our work, we build on the prior work in scene categorization of Li and Perona [10], object detection of Dalal and Triggs [4], multi-class image segmentation of Gould et al. [sent-60, score-1.119]
</p><p>18 [16] use context to signiﬁcantly boost object detection performance, and Sudderth et al. [sent-65, score-0.291]
</p><p>19 [8] propose an innovative system for integrating the tasks of object recognition, surface orientation estimation, and occlusion boundary detection. [sent-68, score-0.244]
</p><p>20 However, their work has a strong leaning towards 3d scene 2  reconstruction rather than understanding, and their algorithms contain many steps that have been specialized for this purpose. [sent-70, score-0.407]
</p><p>21 Their training also requires intimate knowledge of the implementation of each module, while ours is more ﬂexible allowing integration of many related vision tasks regardless of their implementation details. [sent-71, score-0.137]
</p><p>22 Furthermore, we consider a broader class of images and object types, and label regions with speciﬁc classes, rather than generic properties. [sent-72, score-0.348]
</p><p>23 These maps can easily be fed into the other components as additional image channels. [sent-81, score-0.215]
</p><p>24 Most algorithms begin by processing I to produce a set of features, and then learn a function that maps these features into a predicted label (and in some cases also a conﬁdence estimate). [sent-85, score-0.188]
</p><p>25 1: An L-tier Cascaded Classiﬁcation Model (L-CCM) is a cascade of classiﬁers of the target labels Y = {Y1 , . [sent-88, score-0.185]
</p><p>26 , YK }L (L “copies” of each label) consisting of independent classiﬁers ℓ−1 ˆ0 ˆℓ fk,0 (φk (I); θk,0 ) → Yk and a series of conditional classiﬁers fk,ℓ (φk (I, y−k ); θc,ℓ ) → Yk , indexed by ℓ, indicating the “tier” of the model, where y−k indicates the assignment to all labels other than yk . [sent-91, score-0.139]
</p><p>27 The labels at the ﬁnal tier (L − 1) represent the ﬁnal classiﬁcation outputs. [sent-92, score-0.183]
</p><p>28 One copy of each model lies in the ﬁrst tier, and learns with only the image features, φk (I), as input. [sent-94, score-0.189]
</p><p>29 ℓ−1 Subsequent tiers of models accepts a feature vector, φk (I, y−k ), containing the original image features and additional features computed from the outputs of models in the preceeding tier. [sent-95, score-0.562]
</p><p>30 It is not necessary for every instance to have groundtruth labels for every component, and our method works even when the training sets are disjoint. [sent-103, score-0.311]
</p><p>31 4  CCM for Holistic Scene Understanding  Our scene understanding model uses a CCM to combine various subsets of four computer vision tasks: scene categorization, multi-class image segmentation, object detection, and 3d reconstruction. [sent-107, score-1.059]
</p><p>32 Our scene categorization classiﬁer produces a scene label C from one of a small number of classes. [sent-110, score-0.822]
</p><p>33 Our multi-class segmentation model produces a class label Sj 3  Figure 2: (left,middle) Two exmaple features used by the “context” aware object detector. [sent-111, score-0.493]
</p><p>34 (right) Relative location maps showing the relative location of regions (columns) to objects (rows). [sent-112, score-0.343]
</p><p>35 For example, the top row shows that cars are likely to have road beneath and sky above, while the bottom rows show that cows and sheep are often surrounded by grass. [sent-114, score-0.29]
</p><p>36 The base object detectors produce a set of scored windows (Wc,i ) that potentially contain an object of type c. [sent-116, score-0.336]
</p><p>37 Our last component module is monocular 3d reconstruction, which produces a depth Zi for every pixel i in the image. [sent-118, score-0.214]
</p><p>38 Scene Categorization Our scene categorization module is a simple multi-class logistic model that classiﬁes the entire scene into one of a small number of classes. [sent-119, score-0.87]
</p><p>39 In the conditional model, we include features that indicate the relative proportions of each region label (a histogram of Sj values) in the image, plus counts of the number of objects of each type detected, producing a ﬁnal feature vector of length 26. [sent-121, score-0.254]
</p><p>40 Multiclass Image Segmentation The segmentation module aims to assign a label to each pixel. [sent-122, score-0.384]
</p><p>41 , SN } of regions (superpixels) and the pixels are labeled by assigning a class to each region Sj . [sent-130, score-0.142]
</p><p>42 In our work we wish to model the relative location between detected objects and region labels. [sent-132, score-0.25]
</p><p>43 The right side of Figure 2 shows the relative location maps learned by our model. [sent-134, score-0.161]
</p><p>44 These maps model the spatial location of all classes given the location and scale of detected objects. [sent-135, score-0.297]
</p><p>45 Because the detection model provides probabilities for each detection, we actually use the relative location maps multiplied by the probability that each detection is a true detection. [sent-136, score-0.453]
</p><p>46 Preliminary results showed an improvement in using these soft detections over hard (thresholded) detections. [sent-137, score-0.152]
</p><p>47 Object Detectors Our detection module builds on the HOG detector of Dalal and Triggs [4]. [sent-138, score-0.335]
</p><p>48 For each class, the HOG detector is trained on a set of images disjoint from our datasets below. [sent-139, score-0.194]
</p><p>49 This detector is then applied to all images in our dataset with a low threshold that produces an overdetection. [sent-140, score-0.222]
</p><p>50 For each image I, and each object class c, we typically ﬁnd 10-100 candidate detection windows Wc,i . [sent-141, score-0.45]
</p><p>51 Our independent detector model learns a logistic model over a small feature vector φc,i that can be extracted directly from the candidate window. [sent-142, score-0.144]
</p><p>52 Thus, the output from other modules and the image are combined into a feature vector φk (I, C, S, Z). [sent-144, score-0.197]
</p><p>53 Reconstruction Module Our reconstruction module is based on the work of Saxena et al. [sent-148, score-0.206]
</p><p>54 Our Markov Random Field (MRF) approach models the 3d reconstruction (i. [sent-150, score-0.133]
</p><p>55 , depths Z at each point in the image) as a function of the image features and also models the relations between depths at 4  HOG Independent 2-CCM 5-CCM Ground Ideal Input  C AR 0. [sent-152, score-0.44]
</p><p>56 For example, unless there is occlusion, it is more likely that two nearby regions in the image would have similar depths. [sent-209, score-0.217]
</p><p>57 The ﬁrst terms model the depth at each point as a linear function of the local image features, and the second type models relationships between neighboring points, encouraging smoothness. [sent-214, score-0.3]
</p><p>58 Our conditional model includes an additional set of terms that models the depth at each point as a function of the features computed from an image segmentation S in the neighborhood of a point. [sent-215, score-0.588]
</p><p>59 By including this third term, our model beneﬁts from the segmentation outputs in various ways. [sent-216, score-0.253]
</p><p>60 For example, a classiﬁcation of grass implies a horizontal surface, and a classiﬁcation of sky correlates with distant image points. [sent-217, score-0.27]
</p><p>61 While detection outputs might also help reconstruction, we found that most of the signal was present in the segmentation maps, and therefore dropped the detection features for simplicity. [sent-218, score-0.617]
</p><p>62 The ﬁrst subset DS1 contains 422 fully-labeled images of urban and rural outdoor scenes. [sent-220, score-0.218]
</p><p>63 Each image is assigned a category (urban, rural, water, other). [sent-221, score-0.159]
</p><p>64 The foreground class captures detectable objects, and a void class (not used during training or evaluation) allows for the small number of regions not ﬁtting into one of these classes (e. [sent-223, score-0.127]
</p><p>65 We also annotate the location of six different object categories (car, pedestrian, motorcycle, boat, sheep, and cow) by drawing a tight bounding box around each object. [sent-229, score-0.255]
</p><p>66 We use this dataset to demonstrate the combining of three vision tasks: object detection, multi-class segmentation, and scene categorization using the models described above. [sent-230, score-0.812]
</p><p>67 This results in 1749 images with disjoint labelings (no image contains groundtruth labels for more than one task). [sent-232, score-0.523]
</p><p>68 We compare two levels of complexity for our method, a 2-CCM and a 5-CCM to test how the depth of the cascade affects performance. [sent-240, score-0.239]
</p><p>69 The last two training/testing regimes involve using groundtruth information at every stage for training and for both training and testing, respectively. [sent-241, score-0.269]
</p><p>70 Groundtruth trains a 5-CCM using groundtruth inputs for the feature construction (i. [sent-242, score-0.227]
</p><p>71 , as if each tier received perfect inputs from above), but is evaluated with real inputs. [sent-244, score-0.131]
</p><p>72 (a-c,e-g) show precision-recall curves for the six object classes that we consider, while (d) shows our accuracy on the scene categorization task and (h) our accuracy in labeling regions in one of seven classes. [sent-246, score-0.694]
</p><p>73 Input experiment uses the Groundtruth model and also uses the groundtruth input to each tier at testing time. [sent-247, score-0.358]
</p><p>74 For scene categorization, we report an overall accuracy for assigning the correct scene label to an image. [sent-251, score-0.678]
</p><p>75 For segmentation, we compute a per-segment accuracy, where each segment is assigned the groundtruth label that occurs for the majority of pixels in the region. [sent-252, score-0.357]
</p><p>76 For detection, we consider a particular detection correct if the overlap score is larger than 0. [sent-253, score-0.146]
</p><p>77 2 (overlap score equals the area of intersection divided by the area of union between the detected bounding box and the groundtruth). [sent-254, score-0.148]
</p><p>78 It is interesting to note that a large gain was achieved by adding the independent features to the object detectors. [sent-259, score-0.25]
</p><p>79 While the HOG score looks at only the pixels inside the target window, the other features take into account the size and location of the window, allowing our model to capture the fact that foreground object tend to occur in the middle of the image and at a relatively small range of scales. [sent-260, score-0.523]
</p><p>80 For the categorization task, we gained 7% using the CCM framework, and for segmentation, CCM afforded a 3% improvement in accuracy. [sent-262, score-0.144]
</p><p>81 This shows that it is better to train the models using input features that are closer to the features it will see at test time. [sent-265, score-0.179]
</p><p>82 In this way, the downstream tiers can learn to ignore signals that the upstream tiers are bad at capturing, or even take advantage of consistent upstream bias. [sent-266, score-0.392]
</p><p>83 2  DS2 Dataset  For this dataset we combine the three subtasks of reconstruction, segmentation, and object detection. [sent-269, score-0.234]
</p><p>84 Quantitatively, 2-CCM outperformed Independent on segmentation by 2% (75% vs. [sent-272, score-0.216]
</p><p>85 31 mean average precision), and on depth reconstruction by 1. [sent-277, score-0.204]
</p><p>86 The next four rows show four examples where detections are improved and four examples where segmentations are improved. [sent-284, score-0.183]
</p><p>87 In the top left our detectors removed some false boat detections which were out of context and determined that the watery appearance of the bottom of the car was actually foreground. [sent-287, score-0.308]
</p><p>88 Also by providing a sky segment, our method allowed the 3d reconstruction model to infer that those pixels must be very distant (red). [sent-288, score-0.201]
</p><p>89 The next two examples show similar improvement for detections of boats and water. [sent-289, score-0.196]
</p><p>90 The ﬁrst four examples show that our method was able to make correct detections whereas the independent model could not. [sent-292, score-0.185]
</p><p>91 The last examples show improvements in multi-class image segmentation. [sent-293, score-0.159]
</p><p>92 We demonstrated our method on the task of holistic scene understanding by combining scene categorization, object detection, multi-class segmentation and depth reconstruction, and improving on all. [sent-295, score-1.316]
</p><p>93 [8], which uses different components and a smaller number of object classes. [sent-297, score-0.145]
</p><p>94 In addition, we showed that CCMs can beneﬁt from the cascade even with disjoint training data, e. [sent-302, score-0.133]
</p><p>95 , no images containing labels for more than one subtask. [sent-304, score-0.137]
</p><p>96 , a high probability of an ocean scene is a surrogate for a large portion of the image containing water regions). [sent-308, score-0.507]
</p><p>97 Microsoft research cambridge object recognition image database (version 1. [sent-339, score-0.304]
</p><p>98 Labelme: A database and web-based tool for image annotation. [sent-406, score-0.159]
</p><p>99 Learning 3-d scene structure from a single still image. [sent-413, score-0.309]
</p><p>100 Contextual models for object detection using boosted random ﬁelds. [sent-434, score-0.375]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ccm', 0.435), ('scene', 0.309), ('groundtruth', 0.227), ('segmentation', 0.216), ('image', 0.159), ('cascaded', 0.152), ('detections', 0.152), ('hog', 0.152), ('tiers', 0.152), ('classi', 0.149), ('detection', 0.146), ('object', 0.145), ('categorization', 0.144), ('ers', 0.141), ('cascade', 0.133), ('tier', 0.131), ('module', 0.108), ('depth', 0.106), ('holistic', 0.105), ('reconstruction', 0.098), ('ccms', 0.087), ('depths', 0.087), ('saxena', 0.087), ('images', 0.085), ('detector', 0.081), ('urban', 0.076), ('location', 0.075), ('features', 0.072), ('tasks', 0.07), ('understanding', 0.07), ('gould', 0.07), ('vision', 0.067), ('boat', 0.065), ('cascades', 0.065), ('hoiem', 0.065), ('sky', 0.062), ('label', 0.06), ('cars', 0.059), ('regions', 0.058), ('rural', 0.057), ('combining', 0.056), ('dataset', 0.056), ('maps', 0.056), ('yk', 0.054), ('detected', 0.053), ('dalal', 0.052), ('sheep', 0.052), ('labels', 0.052), ('torralba', 0.051), ('objects', 0.049), ('boosted', 0.049), ('grass', 0.049), ('stanford', 0.047), ('detectors', 0.046), ('car', 0.045), ('er', 0.044), ('barrow', 0.044), ('boats', 0.044), ('cows', 0.044), ('upstream', 0.044), ('region', 0.043), ('regimes', 0.042), ('road', 0.042), ('sj', 0.041), ('pixels', 0.041), ('water', 0.039), ('modules', 0.038), ('dates', 0.038), ('classes', 0.038), ('outputs', 0.037), ('bene', 0.036), ('box', 0.035), ('models', 0.035), ('contextual', 0.035), ('ijcv', 0.035), ('prevalence', 0.035), ('contemporary', 0.035), ('superpixels', 0.035), ('cations', 0.035), ('window', 0.035), ('cvpr', 0.034), ('subtasks', 0.033), ('triggs', 0.033), ('yd', 0.033), ('independent', 0.033), ('multiclass', 0.033), ('instance', 0.032), ('rows', 0.031), ('foreground', 0.031), ('intrinsic', 0.03), ('researchers', 0.03), ('area', 0.03), ('relative', 0.03), ('learns', 0.03), ('occlusion', 0.029), ('instantiations', 0.029), ('linking', 0.029), ('penn', 0.029), ('segment', 0.029), ('datasets', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="42-tfidf-1" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>2 0.26273584 <a title="42-tfidf-2" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>3 0.18675013 <a title="42-tfidf-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.17756696 <a title="42-tfidf-4" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>5 0.17356896 <a title="42-tfidf-5" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>6 0.1601692 <a title="42-tfidf-6" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>7 0.15687847 <a title="42-tfidf-7" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>8 0.1283213 <a title="42-tfidf-8" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>9 0.11780126 <a title="42-tfidf-9" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>10 0.11022159 <a title="42-tfidf-10" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>11 0.102582 <a title="42-tfidf-11" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>12 0.095219791 <a title="42-tfidf-12" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>13 0.089981109 <a title="42-tfidf-13" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>14 0.086892992 <a title="42-tfidf-14" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>15 0.081293449 <a title="42-tfidf-15" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>16 0.078739546 <a title="42-tfidf-16" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>17 0.076961234 <a title="42-tfidf-17" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>18 0.074901216 <a title="42-tfidf-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.074185759 <a title="42-tfidf-19" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>20 0.073439837 <a title="42-tfidf-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.233), (1, -0.173), (2, 0.14), (3, -0.237), (4, -0.043), (5, 0.044), (6, -0.094), (7, -0.182), (8, 0.03), (9, 0.017), (10, 0.006), (11, 0.002), (12, 0.033), (13, -0.149), (14, -0.011), (15, 0.004), (16, 0.002), (17, -0.074), (18, 0.03), (19, 0.001), (20, 0.055), (21, 0.022), (22, 0.097), (23, 0.035), (24, -0.089), (25, -0.012), (26, -0.048), (27, 0.029), (28, -0.033), (29, -0.008), (30, 0.058), (31, -0.082), (32, -0.044), (33, -0.026), (34, 0.069), (35, -0.064), (36, 0.004), (37, 0.126), (38, 0.008), (39, -0.107), (40, 0.024), (41, 0.022), (42, -0.034), (43, 0.026), (44, -0.065), (45, 0.029), (46, 0.03), (47, 0.07), (48, 0.02), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96036708 <a title="42-lsi-1" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>2 0.8930366 <a title="42-lsi-2" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>3 0.73766291 <a title="42-lsi-3" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>4 0.72679812 <a title="42-lsi-4" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>5 0.71587348 <a title="42-lsi-5" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>6 0.67901629 <a title="42-lsi-6" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>7 0.66646951 <a title="42-lsi-7" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>8 0.66080236 <a title="42-lsi-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.6462422 <a title="42-lsi-9" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>10 0.59405452 <a title="42-lsi-10" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>11 0.5752238 <a title="42-lsi-11" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>12 0.53180701 <a title="42-lsi-12" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>13 0.4807063 <a title="42-lsi-13" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>14 0.47042716 <a title="42-lsi-14" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>15 0.46616894 <a title="42-lsi-15" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>16 0.45504194 <a title="42-lsi-16" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>17 0.45446065 <a title="42-lsi-17" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>18 0.44025537 <a title="42-lsi-18" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>19 0.41933823 <a title="42-lsi-19" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>20 0.41631234 <a title="42-lsi-20" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.062), (7, 0.058), (12, 0.065), (15, 0.011), (28, 0.115), (35, 0.281), (57, 0.125), (59, 0.015), (63, 0.019), (71, 0.019), (77, 0.036), (78, 0.014), (83, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77828783 <a title="42-lda-1" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>2 0.71900553 <a title="42-lda-2" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>Author: Kevyn Collins-thompson</p><p>Abstract: Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user’s original query with additional related words. Current algorithms for automatic query expansion can often improve retrieval accuracy on average, but are not robust: that is, they are highly unstable and have poor worst-case performance for individual queries. To address this problem, we introduce a novel formulation of query expansion as a convex optimization problem over a word graph. The model combines initial weights from a baseline feedback algorithm with edge weights based on word similarity, and integrates simple constraints to enforce set-based criteria such as aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and signiﬁcant reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm. Our approach does not assume a particular retrieval model, making it applicable to a broad class of existing expansion algorithms. 1</p><p>3 0.6043545 <a title="42-lda-3" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>4 0.59455991 <a title="42-lda-4" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>5 0.59081513 <a title="42-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.58078229 <a title="42-lda-6" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>7 0.56635749 <a title="42-lda-7" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>8 0.56551224 <a title="42-lda-8" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>9 0.5626874 <a title="42-lda-9" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>10 0.56076288 <a title="42-lda-10" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>11 0.55891407 <a title="42-lda-11" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>12 0.55886662 <a title="42-lda-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.55866826 <a title="42-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.55857319 <a title="42-lda-14" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>15 0.55804431 <a title="42-lda-15" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>16 0.5568074 <a title="42-lda-16" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>17 0.55670202 <a title="42-lda-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.55619776 <a title="42-lda-18" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>19 0.55469579 <a title="42-lda-19" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>20 0.55443269 <a title="42-lda-20" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
