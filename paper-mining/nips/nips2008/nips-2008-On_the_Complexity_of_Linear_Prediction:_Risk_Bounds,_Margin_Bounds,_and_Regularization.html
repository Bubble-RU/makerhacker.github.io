<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-161" href="#">nips2008-161</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</h1>
<br/><p>Source: <a title="nips-2008-161-pdf" href="http://papers.nips.cc/paper/3510-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization.pdf">pdf</a></p><p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>Reference: <a title="nips-2008-161-reference" href="../nips2008_reference/nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. [sent-5, score-0.131]
</p><p>2 To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. [sent-6, score-0.829]
</p><p>3 In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. [sent-8, score-0.352]
</p><p>4 Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. [sent-9, score-0.769]
</p><p>5 A paramount question is to understand the generalization ability of these algorithms in terms of the attendant complexity restrictions imposed by the algorithm. [sent-11, score-0.255]
</p><p>6 regularizing based on L1 norm of the weight vector) we seek generalization bounds in terms of the sparsity level. [sent-14, score-0.644]
</p><p>7 SVMs or boosting), we seek generalization bounds in terms of either the L2 or L1 margins. [sent-17, score-0.538]
</p><p>8 More speciﬁcally, n  ˆ w = argmin w  1 ℓ( w, xi , yi ) + λF (w) n i=1  (1)  where ℓ is the loss function, F is the regularizer, and w, x is the inner product between vectors x and w. [sent-20, score-0.254]
</p><p>9 In a formulation closely related to the dual problem, we have: n  1 ℓ( w, xi , yi ) n i=1 w:F (w)≤c  ˆ w = argmin  (2)  where, instead of regularizing, a hard restriction over the parameter space is imposed (by the constant c). [sent-21, score-0.294]
</p><p>10 This works provides generalization bounds for an extensive family of regularization functions F . [sent-22, score-0.586]
</p><p>11 Rademacher complexities (a measure of the complexity of a function class) provide a direct route to obtaining such generalization bounds, and this is the route we take. [sent-23, score-0.489]
</p><p>12 Such bounds are analogous to VC dimensions bounds, but they are typically much sharper and allow for distribution dependent bounds. [sent-24, score-0.442]
</p><p>13 There are a number of methods in the literature to use Rademacher complexities to obtain either generalization bounds or margin bounds. [sent-25, score-0.898]
</p><p>14 Bartlett and Mendelson [2002] provide a generalization bound for Lipschitz loss functions. [sent-26, score-0.327]
</p><p>15 For binary prediction, the results in Koltchinskii and Panchenko [2002] provide means to obtain margin bounds through Rademacher complexities. [sent-27, score-0.618]
</p><p>16 In this work, we provide sharp bounds for Rademacher and Gaussian complexities of linear classes, with respect to a strongly convex complexity function F (as in Equation 1). [sent-28, score-0.936]
</p><p>17 Our bounds are often tighter than previous results and our proofs are all under this more uniﬁed methodology. [sent-30, score-0.436]
</p><p>18 Our proof techniques — reminiscent of those techniques for deriving regret bounds for online learning algorithms — are rooted in convex duality (following Meir and Zhang [2003]) and use a more general notion of strong convexity (as in Shalev-Shwartz and Singer [2006]). [sent-31, score-0.818]
</p><p>19 Perhaps the most extensively studied are margin bounds for the 0-1 loss. [sent-36, score-0.586]
</p><p>20 ), the sharpest bounds are those provided by Bartlett and Mendelson [2002] (using Rademacher complexities) and Langford and Shawe-Taylor [2003], McAllester [2003] (using the PAC-Bayes theorem). [sent-38, score-0.491]
</p><p>21 For L1 -margins (relevant for Boosting, winnow, etc), bounds are provided by Schapire et al. [sent-39, score-0.435]
</p><p>22 For L1 regularization, Ng [2004] provides generalization bounds for this case, which follow from the covering number bounds of Zhang [2002]. [sent-43, score-1.105]
</p><p>23 However, these bounds are only stated as polynomial in the relevant quantities (dependencies are not provided). [sent-44, score-0.407]
</p><p>24 Previous to this work, the most uniﬁed framework for providing generalization bounds for linear prediction stem from the covering number bounds in Zhang [2002]. [sent-45, score-1.156]
</p><p>25 Using these covering number bounds, Zhang [2002] derives margin bounds in a variety of cases. [sent-46, score-0.746]
</p><p>26 However, providing sharp generalization bounds for problems with L1 regularization (or L1 constraints in the dual) requires more delicate arguments. [sent-47, score-0.701]
</p><p>27 As mentioned, Ng [2004] provides bounds for this case, but the techniques used by Ng [2004] would result in rather loose dependencies (the dependence on the sample size n would be n−1/4 rather than n−1/2 ). [sent-48, score-0.437]
</p><p>28 A norm of a vector x is denoted by x , and the dual norm is deﬁned as w ⋆ = sup{ w, x : x ≤ 1}. [sent-53, score-0.188]
</p><p>29 Let ℓ : R × Y → R+ be our loss function of interest. [sent-55, score-0.082]
</p><p>30 The expected of loss of w is denoted by L(w) = E[ℓ( w, x , y)]. [sent-57, score-0.082]
</p><p>31 We denote the empirical loss as L(w) = n i=1 ℓ( w, xi , yi ). [sent-62, score-0.166]
</p><p>32 The restriction we make on our complexity function F is that it is a strongly convex function. [sent-63, score-0.297]
</p><p>33 In particular, we assume it is strongly convex with respect to our dual norm: a function F : S → R is said to be σ-strongly convex w. [sent-64, score-0.318]
</p><p>34 As mentioned in the Introduction, there are number of methods in the literature to use Rademacher complexities to obtain either generalization bounds or margin bounds. [sent-76, score-0.898]
</p><p>35 First, Bartlett and Mendelson [2002] provides the following generalization bound for Lipschitz loss functions. [sent-78, score-0.295]
</p><p>36 Here, L(f ) = E[ℓ(f (x), y)] is the expected of loss of f : X → R, and n 1 ˆ L(f ) = n i=1 ℓ(f (xi ), yi ) is the empirical loss. [sent-79, score-0.122]
</p><p>37 (Bartlett and Mendelson [2002]) Assume the loss ℓ is Lipschitz (with respect to its ﬁrst argument) with Lipschitz constant Lℓ and that ℓ is bounded by c. [sent-81, score-0.082]
</p><p>38 ˆ L(f ) ≤ L(f ) + 2Lℓ Rn (F) + c  The second result, for binary prediction, from Koltchinskii and Panchenko [2002] provides a margin bound in terms of the Rademacher complexity. [sent-83, score-0.261]
</p><p>39 (Koltchinskii and Panchenko [2002]) The zero-one loss function is given by ℓ(f (x), y) = 1[yf (x) ≤ 0], where y ∈ {+1, −1}. [sent-85, score-0.082]
</p><p>40 n Then, with probability at least 1 − δ over the sample, for all margins γ > 0 and all f ∈ F we have, L(f ) ≤ Kγ (f ) + 4  log(log2 4C ) γ  Rn (F) + γ  n  +  log(1/δ) . [sent-88, score-0.122]
</p><p>41 ) The above results show that if we provide sharp bounds on the Rademacher complexities then we obtain sharp generalization bounds. [sent-90, score-0.907]
</p><p>42 Typically, we desire upper bounds on the Rademacher complexity that decrease with n. [sent-91, score-0.482]
</p><p>43 Our main theorem bounds the complexity of FW for certain sets W. [sent-93, score-0.596]
</p><p>44 (Complexity Bounds) Let S be a closed convex set and let F : S → R be σ-strongly convex w. [sent-95, score-0.289]
</p><p>45 σn  The restriction inf w∈S F (w) = 0 is not a signiﬁcant one since adding a constant to F still keeps it strongly convex. [sent-104, score-0.166]
</p><p>46 Interestingly, the complexity bounds above precisely match the regret bounds for online learning algorithms (for linear prediction), a point which we return to in the Discussion. [sent-105, score-1.097]
</p><p>47 Take · , · where x  p  :=  d  d j=1  p  |xi |  1/p  ∗  to be the Lp , Lq norms for p ∈ [2, ∞), 1/p+1/q = 1,  . [sent-110, score-0.098]
</p><p>48 Choose F (w) = ·  2 q  and note that it is 2(q −1)-strongly convex  on R w. [sent-111, score-0.111]
</p><p>49 For any µ, 2 entroµ (w) is 1/W1 -strongly convex on S w. [sent-121, score-0.111]
</p><p>50 (4) n Note that if we take µ to be the uniform distribution then for any w ∈ S we have that trivial upper bound of entroµ (w) ≤ log d. [sent-127, score-0.164]
</p><p>51 (5) n The restriction wj ≥ 0 can be removed in the deﬁnition of S by the standard trick of doubling the dimension of x to include negated copies of each coordinate. [sent-130, score-0.114]
</p><p>52 Rn (FW ) ≤ XW1  In this way, even though the L1 norm is not strongly convex (so our previous Theorem does not directly apply to it), the class of functions imposed by this L1 norm restriction is equivalent to that imposed by the above entropy restriction. [sent-132, score-0.515]
</p><p>53 Hence, we are able to analyze the generalization properties of the optimization problem in Equation 2. [sent-133, score-0.131]
</p><p>54 Lemma 11 in the appendix proves that · is 2/D2 -strongly convex w. [sent-137, score-0.111]
</p><p>55 For a strongly convex F , deﬁne the Bregman divergence ∆F (w v) := F (w) − F (v) − ∇F (v), w − v . [sent-144, score-0.163]
</p><p>56 Except for (5), none of the above bounds depend explicitly on the dimension of the underlying space and hence can be easily extended to inﬁnite dimensional spaces under appropriate assumptions. [sent-147, score-0.407]
</p><p>57 2  The Proof  First, some background on convex duality is in order. [sent-149, score-0.149]
</p><p>58 The Fenchel conjugate of F : S → R is deﬁned as: F ∗ (θ) := sup w, θ − F (w) . [sent-150, score-0.08]
</p><p>59 Let S be a closed convex set and let F : S → R be σ-strongly convex w. [sent-156, score-0.289]
</p><p>60 Like Meir and Zhang [2003] (see Section 5 therein), we begin by using conjugate duality to bound the Rademacher complexity. [sent-178, score-0.12]
</p><p>61 λ λ  2 Since, F (w) ≤ W∗ for all w ∈ W, we have  sup w, θ ≤  w∈W  2 W∗ F ∗ (λθ) + . [sent-191, score-0.08]
</p><p>62 ǫi ’s), we get E sup w, θ w∈W  ≤  2 W∗ 1 + E [F ∗ (λθ)] . [sent-195, score-0.08]
</p><p>63 1  Corollaries Risk Bounds  We now provide generalization error bounds for any Lipschitz loss function ℓ, with Lipschitz constant Lℓ . [sent-201, score-0.652]
</p><p>64 Based on the Rademacher generalization bound provided in the Introduction (see Theorem 1) and the bounds on Rademacher complexity proved in previous section, we obtain the following corollaries. [sent-202, score-0.723]
</p><p>65 Each of the following statements holds with probability at least 1 − δ over the sample: • Let W be as in the Lp /Lq norms example. [sent-204, score-0.133]
</p><p>66 For all w ∈ W, ˆ L(w) ≤ L(w) + 2Lℓ XW∗  p−1 + Lℓ XW∗ n  log(1/δ) 2n  • Let W be as in the L∞ /L1 norms example. [sent-205, score-0.098]
</p><p>67 For all w ∈ W, ˆ ˆ L(w) ≤ L(w) + 2Lℓ XW1  2 log(d) + Lℓ XW1 n  log(1/δ) 2n  Ng [2004] provides bounds for methods which use L1 regularization. [sent-206, score-0.407]
</p><p>68 These bounds are only stated as polynomial bounds, and, the methods used (covering number techniques from Pollard [1984] and covering number bounds from Zhang [2002]) would provide rather loose bounds (the n dependence would be n−1/4 ). [sent-207, score-1.443]
</p><p>69 In fact, even a more careful analysis via Dudley’s entropy integral using the covering numbers from Zhang [2002] would result in a worse bound (with additional log n factors). [sent-208, score-0.375]
</p><p>70 The zero-one loss function is given by ℓ( w, x , y) = 1[y w, x ≤ 0]. [sent-213, score-0.082]
</p><p>71 We n now demonstrate how to get improved margin bounds using the upper bounds for the Rademacher complexity derived in Section 3. [sent-215, score-1.068]
</p><p>72 Based on the Rademacher margin bound provided in the Introduction (see Theorem 2), we get the following corollary which will directly imply the margin bounds we are aiming for. [sent-216, score-0.978]
</p><p>73 The bound for the p = 2 case has been used to explain the performance of SVMs. [sent-217, score-0.082]
</p><p>74 Our bound essentially matches the best known bound [Bartlett and Mendelson, 2002] which was an improvement over previous bounds [Bartlett and Shawe-Taylor, 1999] proved using fat-shattering dimension estimates. [sent-218, score-0.571]
</p><p>75 For the L∞ /L1 case, our bound improves the best known bound [Schapire et al. [sent-219, score-0.164]
</p><p>76 (Lp Margins) Each of the following statements holds with probability at least 1 − δ over the sample: • Let W be as in the Lp /Lq norms example. [sent-222, score-0.133]
</p><p>77 For all γ > 0, w ∈ W, L(w) ≤ Kγ (w) + 4  XW∗ γ  p−1 + n  log(log2 4XW∗ ) γ n  +  log(1/δ) 2n  • Let W be as in the L∞ /L1 norms example. [sent-223, score-0.098]
</p><p>78 , 2001, Theorem 5] and [Zhang, 2002, Theorem 7], by removing a factor of log n. [sent-225, score-0.109]
</p><p>79 We have that with probability at least 1 − δ over the sample, for all margins γ > 0 and all weight vector w ∈ W, XW1 L(w) ≤ Kγ (w) + 8. [sent-231, score-0.122]
</p><p>80 3  PAC-Bayes Theorem  We now show that (a form of) the PAC Bayesian theorem [McAllester, 1999] is a consequence of Theorem 3. [sent-236, score-0.114]
</p><p>81 We choose some prior distribution over this hypothesis set say µ, and after observing the training data, we choose any arbitrary posterior ν and the loss we are interested in is ℓν (x, y) = Ec∼ν ℓ(c, x, y) that is basically the expectation of the loss when hypothesis c ∈ C are drawn i. [sent-238, score-0.228]
</p><p>82 The key observation as that we can view ℓν (x) as the inner product dν(·), ℓ(·, x, y) between the measure dν(·) and the loss ℓ(·, x). [sent-243, score-0.112]
</p><p>83 (PAC-Bayes) For a ﬁxed prior µ over the hypothesis set C, and any loss bounded by 1, with probability at least 1 − δ over the sample, simultaneously for all choice of posteriors ν over C we have that, ˆ Lν ≤ Lν + 4. [sent-246, score-0.114]
</p><p>84 Our bound removes this extra log(n) factor, so, in the regime where we ﬁx ν and examine large n, this bound is sharper. [sent-250, score-0.164]
</p><p>85 4  Covering Number Bounds  It is worth noting that using Sudakov’s minoration results we can obtain upper bound on the L2 (and hence also L1 ) covering numbers using the Gaussian complexities. [sent-253, score-0.298]
</p><p>86 The following is a direct corollary of the Sudakov minoration theorem for Gaussian complexities (Theorem 3. [sent-254, score-0.454]
</p><p>87 There exists a universal constant K > 0 such that its L2 covering number is bounded as follows: ∀ǫ > 0 log(N2 (FW , ǫ, n)) ≤  2 2K 2 X 2 W∗ 2 σǫ  This bound is sharper than those that could be derived from the N∞ covering number bounds of Zhang [2002]. [sent-258, score-0.844]
</p><p>88 5  Discussion: Relations to Online, Regret Minimizing, Algorithms  In this section, we make a further assumption that loss ℓ( w, x , y) is convex in its ﬁrst argument. [sent-259, score-0.193]
</p><p>89 We now show that in the online setting that the regret bounds for linear prediction closely match our risk bounds. [sent-260, score-0.751]
</p><p>90 The algorithm we consider performs the update, wt+1 = ∇F −1 (∇F (wt ) − η∇w ℓ( wt , xt , yt ))  (9)  This algorithm captures both gradient updates, multiplicative updates, and updates based on the Lp norms, through appropriate choices of F . [sent-261, score-0.127]
</p><p>91 For the algorithm given by the above update, the following theorem is a bound on the cumulative regret. [sent-263, score-0.196]
</p><p>92 It is a corollary of Theorem 1 in Shalev-Shwartz and Singer [2006] (and also of Corollary 1 in Shalev-Shwartz [2007]), applied to our linear case. [sent-264, score-0.103]
</p><p>93 (Shalev-Shwartz and Singer [2006]) Let S be a closed convex set and let F : S → R be σ-strongly convex w. [sent-266, score-0.289]
</p><p>94 Then for the update given by Equation 9 if we start with w1 = argmin F (w), we have that for all sequences {(xt , yt )}n , t=1 n  t=1  n  ℓ( wt , xt , yt ) − argmin w∈W  t=1  ℓ( w, xt , yt ) ≤ Lℓ XW∗  2n σ  For completeness, we provide a direct proof in the Appendix. [sent-271, score-0.461]
</p><p>95 Interestingly, the regret above is precisely our complexity bounds (when Lℓ = 1). [sent-272, score-0.593]
</p><p>96 Also, our risk bounds are a factor of 2 worse, essentially due to the symmetrization step used in proving Theorem 1. [sent-273, score-0.552]
</p><p>97 Rademacher and Gaussian complexities: Risk bounds and structural results. [sent-278, score-0.407]
</p><p>98 Empirical margin distributions and bounding the generalization error of combined classiﬁers. [sent-301, score-0.31]
</p><p>99 Continuous versus discrete-time non-linear gradient descent: Relative loss bounds and convergence. [sent-360, score-0.489]
</p><p>100 Covering number bounds of certain regularized linear function classes. [sent-364, score-0.407]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bounds', 0.407), ('rademacher', 0.377), ('fw', 0.269), ('complexities', 0.181), ('margin', 0.179), ('xw', 0.178), ('entro', 0.168), ('covering', 0.16), ('mendelson', 0.14), ('generalization', 0.131), ('koltchinskii', 0.122), ('margins', 0.122), ('theorem', 0.114), ('panchenko', 0.112), ('regret', 0.111), ('bartlett', 0.111), ('convex', 0.111), ('lp', 0.109), ('chicago', 0.109), ('corollary', 0.103), ('norms', 0.098), ('zi', 0.094), ('langford', 0.091), ('lipschitz', 0.088), ('zhang', 0.087), ('risk', 0.085), ('corollaries', 0.084), ('loss', 0.082), ('log', 0.082), ('bound', 0.082), ('sup', 0.08), ('sharp', 0.078), ('complexity', 0.075), ('rn', 0.072), ('norm', 0.072), ('si', 0.071), ('tti', 0.067), ('fenchel', 0.063), ('restriction', 0.059), ('argmin', 0.058), ('meir', 0.057), ('minoration', 0.056), ('sharpest', 0.056), ('sudakov', 0.056), ('inf', 0.055), ('online', 0.055), ('proof', 0.055), ('wj', 0.055), ('bregman', 0.052), ('strongly', 0.052), ('entropy', 0.051), ('prediction', 0.051), ('interestingly', 0.05), ('ei', 0.049), ('imposed', 0.049), ('singer', 0.049), ('karthik', 0.049), ('ledoux', 0.049), ('il', 0.049), ('yt', 0.048), ('regularization', 0.048), ('wt', 0.044), ('boosting', 0.044), ('dual', 0.044), ('fix', 0.044), ('schapire', 0.044), ('xi', 0.044), ('uni', 0.043), ('match', 0.042), ('tightly', 0.042), ('tewari', 0.042), ('ng', 0.041), ('convexity', 0.041), ('yi', 0.04), ('let', 0.039), ('sham', 0.038), ('duality', 0.038), ('constraints', 0.037), ('gn', 0.036), ('xt', 0.035), ('statements', 0.035), ('pac', 0.035), ('route', 0.035), ('sharper', 0.035), ('mcallester', 0.035), ('rd', 0.034), ('regularizing', 0.034), ('inequality', 0.033), ('proving', 0.033), ('hypothesis', 0.032), ('provide', 0.032), ('mistakes', 0.031), ('loose', 0.03), ('inner', 0.03), ('sn', 0.029), ('proofs', 0.029), ('david', 0.029), ('closed', 0.028), ('provided', 0.028), ('factor', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="161-tfidf-1" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>2 0.39493349 <a title="161-tfidf-2" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>3 0.23064879 <a title="161-tfidf-3" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>4 0.18574648 <a title="161-tfidf-4" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>Author: Shai Shalev-shwartz, Sham M. Kakade</p><p>Abstract: We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in Hazan et al. [2006]. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions. 1</p><p>5 0.1613111 <a title="161-tfidf-5" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>Author: Karthik Sridharan, Shai Shalev-shwartz, Nathan Srebro</p><p>Abstract: We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with ﬁxed regularization parameter) to its inﬁnite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other p norms. 1</p><p>6 0.15652716 <a title="161-tfidf-6" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>7 0.15196386 <a title="161-tfidf-7" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>8 0.13609488 <a title="161-tfidf-8" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>9 0.13448852 <a title="161-tfidf-9" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>10 0.13225378 <a title="161-tfidf-10" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>11 0.12946147 <a title="161-tfidf-11" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>12 0.11543137 <a title="161-tfidf-12" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>13 0.11527205 <a title="161-tfidf-13" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>14 0.11173663 <a title="161-tfidf-14" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>15 0.10938454 <a title="161-tfidf-15" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>16 0.1066789 <a title="161-tfidf-16" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>17 0.10175269 <a title="161-tfidf-17" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>18 0.098946132 <a title="161-tfidf-18" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>19 0.096682474 <a title="161-tfidf-19" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>20 0.096397042 <a title="161-tfidf-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.259), (1, -0.0), (2, -0.323), (3, 0.07), (4, -0.196), (5, 0.07), (6, -0.053), (7, -0.079), (8, -0.051), (9, 0.011), (10, -0.006), (11, 0.224), (12, 0.215), (13, 0.069), (14, -0.043), (15, 0.169), (16, 0.108), (17, 0.037), (18, 0.002), (19, -0.043), (20, -0.122), (21, -0.029), (22, 0.073), (23, -0.123), (24, 0.052), (25, 0.087), (26, -0.01), (27, 0.118), (28, -0.136), (29, -0.108), (30, 0.012), (31, 0.126), (32, -0.013), (33, 0.064), (34, -0.067), (35, 0.031), (36, 0.051), (37, -0.077), (38, -0.112), (39, -0.042), (40, 0.043), (41, 0.001), (42, 0.049), (43, 0.093), (44, 0.004), (45, 0.055), (46, -0.017), (47, 0.043), (48, 0.087), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97017998 <a title="161-lsi-1" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>2 0.8927598 <a title="161-lsi-2" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>3 0.78280419 <a title="161-lsi-3" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>Author: Karthik Sridharan, Shai Shalev-shwartz, Nathan Srebro</p><p>Abstract: We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with ﬁxed regularization parameter) to its inﬁnite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other p norms. 1</p><p>4 0.69550997 <a title="161-lsi-4" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>5 0.60903275 <a title="161-lsi-5" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><p>6 0.54650807 <a title="161-lsi-6" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>7 0.5256086 <a title="161-lsi-7" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>8 0.52184063 <a title="161-lsi-8" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>9 0.50591487 <a title="161-lsi-9" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>10 0.48007661 <a title="161-lsi-10" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>11 0.45546576 <a title="161-lsi-11" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>12 0.45148167 <a title="161-lsi-12" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>13 0.4463082 <a title="161-lsi-13" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>14 0.44127667 <a title="161-lsi-14" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>15 0.42905802 <a title="161-lsi-15" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>16 0.41802225 <a title="161-lsi-16" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>17 0.41582313 <a title="161-lsi-17" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>18 0.41274258 <a title="161-lsi-18" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>19 0.41268891 <a title="161-lsi-19" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>20 0.40559018 <a title="161-lsi-20" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.075), (7, 0.049), (12, 0.039), (28, 0.143), (57, 0.047), (59, 0.012), (63, 0.024), (71, 0.451), (77, 0.041), (83, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8941161 <a title="161-lda-1" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>same-paper 2 0.84518176 <a title="161-lda-2" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>3 0.81420547 <a title="161-lda-3" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>Author: Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe</p><p>Abstract: Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method. 1</p><p>4 0.78895253 <a title="161-lda-4" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><p>5 0.53688055 <a title="161-lda-5" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>6 0.52016509 <a title="161-lda-6" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>7 0.5165472 <a title="161-lda-7" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>8 0.50402927 <a title="161-lda-8" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>9 0.49948281 <a title="161-lda-9" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>10 0.48643741 <a title="161-lda-10" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>11 0.47981906 <a title="161-lda-11" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>12 0.47897783 <a title="161-lda-12" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>13 0.46003166 <a title="161-lda-13" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>14 0.45578307 <a title="161-lda-14" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>15 0.4522073 <a title="161-lda-15" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>16 0.45140153 <a title="161-lda-16" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>17 0.4502849 <a title="161-lda-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.4471989 <a title="161-lda-18" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>19 0.44581926 <a title="161-lda-19" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>20 0.44433916 <a title="161-lda-20" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
