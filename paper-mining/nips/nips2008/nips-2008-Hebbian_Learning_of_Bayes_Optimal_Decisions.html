<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-96" href="#">nips2008-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</h1>
<br/><p>Source: <a title="nips-2008-96-pdf" href="http://papers.nips.cc/paper/3391-hebbian-learning-of-bayes-optimal-decisions.pdf">pdf</a></p><p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>Reference: <a title="nips-2008-96-reference" href="../nips2008_reference/nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. [sent-5, score-0.188]
</p><p>2 We present a concrete Hebbian learning rule operating on log-probability ratios. [sent-6, score-0.25]
</p><p>3 Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. [sent-7, score-0.407]
</p><p>4 In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. [sent-8, score-0.165]
</p><p>5 Various attempts to relate these theoretically optimal models to experimentally supported models for computation and plasticity in networks of neurons in the brain have been made. [sent-12, score-0.146]
</p><p>6 For reduced classes of probability distributions, [4] proposed a method for spiking network models to learn Bayesian inference with an online approximation to an EM algorithm. [sent-14, score-0.119]
</p><p>7 The approach of [5] interprets the weight wji of a synaptic connection between neurons p(xi ,xj ) representing the random variables xi and xj as log p(xi )·p(xj ) , and presents algorithms for learning these weights. [sent-15, score-0.258]
</p><p>8 In their study they found that ﬁring rates of neurons in area LIP of macaque monkeys reﬂect the log-likelihood ratio (or logodd) of the outcome of a binary decision, given visual evidence. [sent-19, score-0.118]
</p><p>9 The learning of such log-odds for Bayesian decision making can be reduced to learning weights for a linear classiﬁer, given an appropriate but ﬁxed transformation from the input to possibly nonlinear features [6]. [sent-20, score-0.168]
</p><p>10 1  that the optimal weights for the linear decision function are actually log-odds themselves, and the deﬁnition of the features determines the assumptions of the learner about statistical dependencies among inputs. [sent-22, score-0.156]
</p><p>11 In this work we show that simple Hebbian learning [7] is sufﬁcient to implement learning of Bayes optimal decisions for arbitrarily complex probability distributions. [sent-23, score-0.124]
</p><p>12 In combination with appropriate preprocessing networks this implements learning of different probabilistic decision making processes like e. [sent-25, score-0.224]
</p><p>13 Finally we show that a reward-modulated version of this Hebbian learning rule can solve simple reinforcement learning tasks, and also provides a model for the experimental results of [1]. [sent-28, score-0.312]
</p><p>14 2  A Hebbian rule for learning log-odds  We consider the model of a linear threshold neuron with output y0 , where y0 = 1 means that the neuron is ﬁring and y0 = 0 means non-ﬁring. [sent-29, score-0.322]
</p><p>15 The neuron’s current decision y0 whether to ﬁre or not ˆ n is given by a linear decision function y0 = sign(w0 · constant + i=1 wi yi ), where the yi are the ˆ current ﬁring states of all presynaptic neurons and wi are the weights of the corresponding synapses. [sent-30, score-1.212]
</p><p>16 We propose the following learning rule, which we call the Bayesian Hebb rule: ∆wi =  η (1 + e−wi ), −η (1 + ewi ), 0,  if y0 = 1 and yi = 1 if y0 = 0 and yi = 1 if yi = 0. [sent-31, score-0.513]
</p><p>17 it depends only on the binary ﬁring state of the pre- and postsynaptic neuron yi and y0 , the current weight wi and a learning rate η. [sent-34, score-0.731]
</p><p>18 , yn the Bayesian Hebb rule learns log-probability ratios of the postsynaptic ﬁring state y0 , conditioned on a corresponding presynaptic ﬁring state yi . [sent-38, score-0.506]
</p><p>19 We consider in this article the use of the rule in a supervised, teacher forced mode (see Section 3), and also in a reinforcement learning mode (see ∗ Section 4). [sent-39, score-0.311]
</p><p>20 We will prove that the rule converges globally to the target weight value wi , given by ∗ wi = log  p(y0 = 1|yi = 1) p(y0 = 0|yi = 1)  . [sent-40, score-1.142]
</p><p>21 (2)  ∗ We ﬁrst show that the expected update E[∆wi ] under (1) vanishes at the target value wi : ∗  ∗  ∗ E[∆wi ] = 0 ⇔ p(y0 =1, yi =1)η(1 + e−wi ) − p(y0 =0, yi =1)η(1 + ewi ) = 0 ∗  ⇔ ⇔  1 + ewi p(y0 =1, yi =1) ∗ = p(y0 =0, yi =1) 1 + e−wi p(y0 =1|yi =1) ∗ wi = log p(y0 =0|yi =1)  . [sent-41, score-1.558]
</p><p>22 (3)  ∗ Since the above is a chain of equivalence transformations, this proves that wi is the only equilibrium value of the rule. [sent-42, score-0.365]
</p><p>23 The weight vector w∗ is thus a global point-attractor with regard to expected weight changes of the Bayesian Hebb rule (1) in the n-dimensional weight-space Rn . [sent-43, score-0.32]
</p><p>24 Furthermore we show, using the result from (3), that the expected weight change at any current value ∗ ∗ of wi points in the direction of wi . [sent-44, score-0.785]
</p><p>25 Consider some arbitrary intermediate weight value wi = wi +2ǫ: ∗ E[∆wi ]|wi +2ǫ  =  ∗ ∗ E[∆wi ]|wi +2ǫ − E[∆wi ]|wi ∗  ∗  ∝ p(y0 =1, yi =1)e−wi (e−2ǫ − 1) − p(y0 =0, yi =1)ewi (e2ǫ − 1) = (p(y0 =0, yi =1)e−ǫ + p(y0 =1, yi =1)eǫ )(e−ǫ − eǫ ) . [sent-45, score-1.325]
</p><p>26 The Bayesian Hebb rule is therefore always expected to perform updates in the right direction, and the initial weight values or perturbations of the weights decay exponentially fast. [sent-47, score-0.318]
</p><p>27 , yn ∈ {0, 1}n+1 , the Bayesian Hebb rule closely approximates the optimal weight vector w that can be inferred from the data. [sent-51, score-0.332]
</p><p>28 A traditional ˆ frequentist’s approach would use counters ai = #[y0 =1 ∧ yi =1] and bi = #[y0 =0 ∧ yi =1] to ∗ estimate every wi by ai wi = log ˆ . [sent-52, score-1.507]
</p><p>29 (5) bi A Bayesian approach would model p(y0 |yi ) with an (initially ﬂat) Beta-distribution, and use the counters ai and bi to update this belief [3], leading to the same MAP estimate wi . [sent-53, score-0.842]
</p><p>30 Consequently, in ˆ both approaches a new example with y0 = 1 and yi = 1 leads to the update wi ˆ new = log  ai + 1 ai = log bi bi  1+  1 ai  = wi + log(1 + ˆ  1 ˆ (1 + e−wi )) , Ni  where Ni := ai + bi is the number of previously processed examples with yi = 1, thus bi 1 Ni (1 + ai ). [sent-54, score-2.244]
</p><p>31 Analogously, a new example with y0 = 0 and yi = 1 gives rise to the update wi ˆ new = log  ai ai = log bi + 1 bi  1 1 1 + bi  ˆ = wi − log(1 +  1 ˆ (1 + ewi )). [sent-55, score-1.719]
</p><p>32 Ni  (6) 1 ai  =  (7)  Furthermore, wi ˆ new = wi for a new example with yi = 0. [sent-56, score-0.979]
</p><p>33 Using the approximation log(1 + α) ≈ α ˆ 1 the update rules (6) and (7) yield the Bayesian Hebb rule (1) with an adaptive learning rate ηi = Ni for each synapse. [sent-57, score-0.302]
</p><p>34 Learning rate adaptation One can see from the above considerations that the Bayesian Hebb rule with a constant learning rate η converges globally to the desired log-odds. [sent-60, score-0.365]
</p><p>35 A too small constant learning rate, however, tends to slow down the initial convergence of the weight vector, and a too large constant learning rate produces larger ﬂuctuations once the steady state is reached. [sent-61, score-0.125]
</p><p>36 (N )  1 (6) and (7) suggest a decaying learning rate ηi i = Ni , where Ni is the number of preceding examples with yi = 1. [sent-62, score-0.183]
</p><p>37 We will present a learning rate adaptation mechanism that avoids biologically implausible counters, and is robust enough to deal even with non-stationary distributions. [sent-63, score-0.116]
</p><p>38 The parameters ai and bi in this case are not exact counters anymore but correspond to virtual sample sizes, depending on the current learning rate. [sent-65, score-0.362]
</p><p>39 We formalize this statistical model of wi by σ(wi ) =  1 Γ(ai + bi ) ∼ Beta(ai , bi ) ⇐⇒ wi ∼ σ(wi )ai σ(−wi )bi , 1 + e−wi Γ(ai )Γ(bi )  In practice this model turned out to capture quite well the actually observed quasi-stationary distri1 1 i bution of wi . [sent-66, score-1.363]
</p><p>40 In [9] we show analytically that E[wi ] ≈ log ai and Var[wi ] ≈ ai + bi . [sent-67, score-0.418]
</p><p>41 A learning b rate adaptation mechanism at the synapse that keeps track of the observed mean and variance of the synaptic weight can therefore recover estimates of the virtual sample sizes ai and bi . [sent-68, score-0.508]
</p><p>42 The following mechanism, which we call variance tracking implements this by computing running averages of the weights and the squares of weights in wi and qi : ¯ ¯ new ηi wi ¯ new qi ¯new  ← ← ←  qi −wi ¯ ¯2 1+cosh wi ¯ (1 − ηi ) wi ¯  + ηi wi 2 (1 − ηi ) qi + ηi wi . [sent-69, score-2.448]
</p><p>43 3  Hebbian learning of Bayesian decisions  We now show how the Bayesian Hebb rule can be used to learn Bayes optimal decisions. [sent-72, score-0.341]
</p><p>44 , xm be represented through the binary ﬁring states y1 , . [sent-81, score-0.11]
</p><p>45 , yn ∈ {0, 1} of the n presynaptic neurons in a population coding manner. [sent-84, score-0.181]
</p><p>46 More precisely, let each input variable xk ∈ {1, . [sent-85, score-0.179]
</p><p>47 , mk } be represented by mk neurons, where each neuron ﬁres only for one of the mk possible values of xk . [sent-88, score-0.358]
</p><p>48 Formally we deﬁne the simple preprocessing (SP) yT = φ(x1 )T , . [sent-89, score-0.118]
</p><p>49 (10)  The binary target variable x0 is represented directly by the binary state y0 of the postsynaptic neuron. [sent-96, score-0.159]
</p><p>50 , yn in (9) and taking the logarithm leads to n  log  p(y0 = 1) p(yi = 1|y0 = 1) p(y0 = 1|y) = (1 − m) log + yi log . [sent-100, score-0.337]
</p><p>51 p(y0 = 0|y) p(y0 = 0) i=1 p(yi = 1|y0 = 0)  Hence the optimal decision under the Naive Bayes assumption is n ∗ y0 = sign((1 − m)w0 + ˆ  ∗ wi yi ) . [sent-101, score-0.576]
</p><p>52 i=1  ∗ ∗ The optimal weights w0 and wi ∗ w0 = log  p(y0 = 1) p(y0 = 0)  and  ∗ wi = log  p(y0 = 1|yi = 1) p(y0 = 0|yi = 1)  for  i = 1, . [sent-102, score-0.928]
</p><p>53 are obviously log-odds which can be learned by the Bayesian Hebb rule (the bias weight w0 is simply learned as an unconditional log-odd). [sent-106, score-0.326]
</p><p>54 This implies that the BN can be described by m + 1 (possibly empty) parent sets deﬁned by Pk  = {i | a directed edge xi → xk exists in BN and i ≥ 1}  . [sent-119, score-0.151]
</p><p>55 We refer to the resulting preprocessing circuit as generalized preprocessing (GP). [sent-128, score-0.268]
</p><p>56 , yn (with n ≫ m) such that the decision function (11) can be written as a weighted sum, and the weights correspond to conditional log-odds of yi ’s. [sent-136, score-0.265]
</p><p>57 Figure 1 B illustrates such a sparse code: One binary variable is created for every possible value assignment to a variable and all its parents, and one additional binary variable is created for every possible value assignment to the parent nodes only. [sent-137, score-0.142]
</p><p>58 Formally, the previously introduced population coding operator φ is generalized such that φ(xi1 , xi2 , . [sent-138, score-0.106]
</p><p>59 Inserting the sparse coding (12) into (11) allows writing the Bayes optimal decision function (11) as a pure sum of log-odds of the target variable: n ∗ wi yi ),  x0 = y0 = sign( ˆ ˆ  with  i=1  ∗ wi = log  p(y0 =1|yi =0) . [sent-150, score-1.082]
</p><p>60 p(y0 =0|yi =0)  Every synaptic weight wi can be learned efﬁciently by the Bayesian Hebb rule (1) with the formal modiﬁcation that the update is not only triggered by yi =1 but in general whenever yi =0 (which obviously does not change the behavior of the learning process). [sent-151, score-1.056]
</p><p>61 4  The Bayesian Hebb rule in reinforcement learning  We show in this section that a reward-modulated version of the Bayesian Hebb rule enables a learning agent to solve simple reinforcement learning tasks. [sent-153, score-0.626]
</p><p>62 , xm , chooses an action α out of a set of possible actions A, and receives a binary reward signal r ∈ {0, 1} with probability p(r|x, a). [sent-157, score-0.263]
</p><p>63 The learner’s goal is to learn (as fast as possible) a policy π(x, a) so that action selection according to this policy maximizes the average reward. [sent-158, score-0.225]
</p><p>64 In contrast to the previous 5  learning tasks, the learner has to explore different actions for the same input to learn the rewardprobabilities for all possible actions. [sent-159, score-0.106]
</p><p>65 p(r=1|x,a) The goal is to infer the probability of binary reward, so it sufﬁces to learn the log-odds log p(r=0|x,a) for every action, and choose the action that is most likely to yield reward (e. [sent-162, score-0.3]
</p><p>66 If the reward probability for an action a = α is deﬁned by some Bayesian network BN, one can rewrite this log-odd as  log  p(r = 1|x, a = α) p(r = 1|a = α) = log + p(r = 0|x, a = α) p(r = 0|a = α)  m  log k=1  p(xk |xPk , r = 1, a = α) . [sent-165, score-0.355]
</p><p>67 Both a simple population code such as (10), or generalized preprocessing as in (12) and Figure 1B can be used, depending on the assumed dependency structure. [sent-167, score-0.214]
</p><p>68 The reward log-odd (13) for the preprocessed input vector y can then be written as a linear sum log  p(r = 1|y, a = α) p(r = 0|y, a = α)  n ∗ = wα,0 +  ∗ wα,i yi  ,  i=1  p(r=1|yi =0,a=α) ∗ ∗ where the optimal weights are wα,0 = log p(r=1|a=α) and wα,i = log p(r=0|yi =0,a=α) . [sent-168, score-0.518]
</p><p>69 The weights corresponding to the optimal policy are the only equilibria under the reward-modulated Bayesian Hebb rule, and are also global attractors in weight space, independently of the exploration policy (see [9]). [sent-170, score-0.217]
</p><p>70 1  Experimental Results Results for prediction tasks  We have tested the Bayesian Hebb rule on 400 different prediction tasks, each of them deﬁned by a general (non-Naive) Bayesian network of 7 binary variables. [sent-172, score-0.319]
</p><p>71 Figure 2A shows that the Bayesian Hebb rule with the simple preprocessing (10) generalizes better from a few training examples, but is outperformed by logistic regression in the long run, since the Naive Bayes assumption is not met. [sent-177, score-0.328]
</p><p>72 With the generalized preprocessing (12), the Bayesian Hebb rule learns fast and converges to the Bayes optimum (see Figure 2B). [sent-178, score-0.526]
</p><p>73 In Figure 2C we show that the Bayesian Hebb rule is robust to noisy updates - a condition very likely to occur in biological systems. [sent-179, score-0.21]
</p><p>74 Even such imprecise implementations of the Bayesian Hebb rule perform very well. [sent-181, score-0.244]
</p><p>75 2  Results for action selection tasks  The reward-modulated version (14), of the Bayesian Hebb rule was tested on 250 random action selection tasks with m = 6 binary input attributes, and 4 possible actions. [sent-184, score-0.516]
</p><p>76 A) The Bayesian Hebb rule with simple preprocessing (SP) learns as fast as Naive Bayes, and faster than logistic regression (with optimized constant learning rate). [sent-206, score-0.438]
</p><p>77 B) The Bayesian Hebb rule with generalized preprocessing (GP) learns fast and converges to the Bayes optimal prediction performance. [sent-207, score-0.538]
</p><p>78 C) Even a very imprecise implementation of the Bayesian Hebb rule (noisy updates, uniformly distributed in ∆wi ± γ%) yields almost the same learning performance. [sent-208, score-0.266]
</p><p>79 random Bayesian network [11] was drawn to model the input and reward distributions (see [9] for details). [sent-209, score-0.13]
</p><p>80 The agent received stochastic binary rewards for every chosen action, updated the weights wα,i according to (14), and measured the average reward on 500 independent test trials. [sent-210, score-0.229]
</p><p>81 In Figure 3A we compare the reward-modulated Bayesian Hebb rule with simple population coding (10) (Bayesian Hebb SP), and generalized preprocessing (12) (Bayesian Hebb GP), to the standard learning model for simple conditioning tasks, the non-Hebbian Rescorla-Wagner rule [12]. [sent-211, score-0.701]
</p><p>82 The reward-modulated Bayesian Hebb rule learns as fast as the Rescorla-Wagner rule, and achieves in combination with generalized preprocessing a higher performance level. [sent-212, score-0.448]
</p><p>83 The widely used tabular Q-learning algorithm, in comparison is slower than the other algorithms, since it does not generalize, but it converges to the optimal policy in the long run. [sent-213, score-0.128]
</p><p>84 3  A model for the experiment of Yang and Shadlen  In the experiment by Yang and Shadlen [1], a monkey had to choose between gazing towards a red target R or a green target G. [sent-215, score-0.115]
</p><p>85 The sum of the four weights yielded the log-odd of obtaining a reward at the red target, and a reward for each trial was assigned accordingly to one of the targets. [sent-218, score-0.216]
</p><p>86 The monkey thus had to combine the evidence from four visual stimuli to optimize its action selection behavior. [sent-219, score-0.105]
</p><p>87 In the model of the task it is sufﬁcient to learn weights only for the action a = R, and select this action whenever the log-odd using the current weights is positive, and G otherwise. [sent-220, score-0.305]
</p><p>88 A simple population code as in (10) encoded the 4-dimensional visual stimulus into a 40-dimensional binary vector y. [sent-221, score-0.106]
</p><p>89 In our experiments, the reward-modulated Bayesian Hebb rule learns this task as fast and with similar quality as the non-Hebbian Rescorla-Wagner rule. [sent-222, score-0.298]
</p><p>90 6  Discussion  We have shown that the simplest and experimentally best supported local learning mechanism, Hebbian learning, is sufﬁcient to learn Bayes optimal decisions. [sent-224, score-0.105]
</p><p>91 We have introduced and analyzed the Bayesian Hebb rule, a training method for synaptic weights, which converges fast and robustly to optimal log-probability ratios, without requiring any communication between plasticity mechanisms for different synapses. [sent-225, score-0.243]
</p><p>92 We have shown how the same plasticity mechanism can learn Bayes optimal decisions under different statistical independence assumptions, if it is provided with an appropriately preprocessed input. [sent-226, score-0.238]
</p><p>93 We have demonstrated on a variety of prediction tasks that the Bayesian Hebb rule learns very fast, and with an appropriate sparse preprocessing mechanism for groups of statistically dependent features its performance converges to the Bayes optimum. [sent-227, score-0.533]
</p><p>94 Our approach therefore suggests that sparse, redundant codes of input features may simplify synaptic learning processes in spite of strong statistical dependencies. [sent-228, score-0.134]
</p><p>95 With generalized preprocessing (GP), the rule converges to the optimal action-selection policy. [sent-235, score-0.45]
</p><p>96 B, C) Action selection policies learned by the reward-modulated Bayesian Hebb rule in the task by Yang and Shadlen [1] after 100 (B), and 1000 (C) trials are qualitatively similar to the policies adopted by monkeys H and J in [1] after learning. [sent-236, score-0.265]
</p><p>97 The Bayesian Hebb rule, modulated by a signal related to rewards, enables fast learning of optimal action selection. [sent-238, score-0.194]
</p><p>98 Experimental results of [1] on reinforcement learning of probabilistic inference in primates can be partially modeled in this way with regard to resulting behaviors. [sent-239, score-0.165]
</p><p>99 An attractive feature of the Bayesian Hebb rule is its ability to deal with the addition or removal of input features through the creation or deletion of synaptic connections, since no relearning of weights is required for the other synapses. [sent-240, score-0.357]
</p><p>100 Therefore the learning rule may be viewed as a potential building block for models of the brain as a self-organizing and fast adapting probabilistic inference machine. [sent-242, score-0.318]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hebb', 0.675), ('wi', 0.365), ('rule', 0.21), ('xpk', 0.189), ('bayesian', 0.184), ('xk', 0.151), ('yi', 0.135), ('bi', 0.134), ('preprocessing', 0.118), ('ai', 0.114), ('ni', 0.106), ('hebbian', 0.1), ('ewi', 0.086), ('bayes', 0.086), ('action', 0.085), ('sp', 0.078), ('shadlen', 0.069), ('counters', 0.069), ('reward', 0.068), ('xm', 0.068), ('synaptic', 0.066), ('bn', 0.062), ('ring', 0.058), ('reinforcement', 0.058), ('gp', 0.057), ('converges', 0.057), ('log', 0.056), ('weight', 0.055), ('mk', 0.054), ('weights', 0.053), ('learns', 0.053), ('plasticity', 0.052), ('decisions', 0.047), ('yang', 0.046), ('neuron', 0.045), ('mechanism', 0.044), ('decision', 0.043), ('binary', 0.042), ('postsynaptic', 0.041), ('population', 0.041), ('neurons', 0.04), ('naive', 0.04), ('policy', 0.038), ('monkeys', 0.036), ('conditioning', 0.035), ('fast', 0.035), ('imprecise', 0.034), ('loglr', 0.034), ('nessler', 0.034), ('pfeiffer', 0.034), ('primates', 0.034), ('rescorla', 0.034), ('xil', 0.034), ('xpm', 0.034), ('network', 0.034), ('yn', 0.034), ('target', 0.034), ('qi', 0.033), ('presynaptic', 0.033), ('preprocessed', 0.033), ('tasks', 0.033), ('optimal', 0.033), ('coding', 0.033), ('generalized', 0.032), ('inference', 0.03), ('correctness', 0.029), ('learn', 0.029), ('input', 0.028), ('red', 0.027), ('learner', 0.027), ('update', 0.026), ('graz', 0.026), ('rate', 0.026), ('spiking', 0.026), ('agent', 0.024), ('adaptation', 0.024), ('code', 0.023), ('virtual', 0.023), ('obviously', 0.023), ('rewards', 0.022), ('learning', 0.022), ('percentage', 0.022), ('optimum', 0.021), ('experimentally', 0.021), ('probabilistic', 0.021), ('article', 0.021), ('every', 0.02), ('implements', 0.02), ('monkey', 0.02), ('learned', 0.019), ('sign', 0.019), ('variables', 0.019), ('modulated', 0.019), ('rao', 0.019), ('beta', 0.019), ('redundant', 0.018), ('concrete', 0.018), ('rules', 0.018), ('predictor', 0.018), ('sparse', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="96-tfidf-1" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>2 0.15815097 <a title="96-tfidf-2" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>3 0.14111488 <a title="96-tfidf-3" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>Author: Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter</p><p>Abstract: In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons. 1</p><p>4 0.10857394 <a title="96-tfidf-4" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>5 0.099147581 <a title="96-tfidf-5" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>6 0.098681241 <a title="96-tfidf-6" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>7 0.086795315 <a title="96-tfidf-7" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>8 0.084761493 <a title="96-tfidf-8" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>9 0.08038687 <a title="96-tfidf-9" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>10 0.079824567 <a title="96-tfidf-10" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>11 0.077070937 <a title="96-tfidf-11" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>12 0.074056037 <a title="96-tfidf-12" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>13 0.074002527 <a title="96-tfidf-13" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>14 0.07386744 <a title="96-tfidf-14" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>15 0.073763579 <a title="96-tfidf-15" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>16 0.072200552 <a title="96-tfidf-16" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>17 0.070220515 <a title="96-tfidf-17" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>18 0.068523824 <a title="96-tfidf-18" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>19 0.066943288 <a title="96-tfidf-19" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>20 0.064649731 <a title="96-tfidf-20" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.205), (1, 0.12), (2, 0.005), (3, 0.022), (4, 0.036), (5, -0.01), (6, -0.005), (7, -0.019), (8, -0.037), (9, 0.049), (10, 0.024), (11, 0.142), (12, -0.005), (13, -0.033), (14, 0.046), (15, -0.066), (16, 0.018), (17, -0.007), (18, 0.017), (19, -0.024), (20, 0.018), (21, -0.084), (22, -0.174), (23, 0.083), (24, 0.064), (25, 0.069), (26, 0.072), (27, -0.142), (28, -0.017), (29, 0.04), (30, -0.1), (31, 0.0), (32, 0.081), (33, 0.065), (34, 0.079), (35, -0.131), (36, -0.009), (37, -0.101), (38, 0.072), (39, -0.132), (40, 0.072), (41, -0.013), (42, 0.003), (43, -0.029), (44, 0.005), (45, 0.022), (46, -0.042), (47, 0.001), (48, -0.093), (49, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95938283 <a title="96-lsi-1" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>2 0.59439659 <a title="96-lsi-2" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>Author: Gerhard Neumann, Jan R. Peters</p><p>Abstract: Recently, ﬁtted Q-iteration (FQI) based methods have become more popular due to their increased sample efﬁciency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simpliﬁed to an inexpensive advantageweighted regression. With this result, we are able to derive a new, computationally efﬁcient FQI algorithm which can even deal with high dimensional action spaces. 1</p><p>3 0.59094274 <a title="96-lsi-3" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>Author: Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter</p><p>Abstract: In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons. 1</p><p>4 0.51980972 <a title="96-lsi-4" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>Author: Dotan D. Castro, Dmitry Volkinshtein, Ron Meir</p><p>Abstract: Actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail (e.g., when function approximation is involved). Interestingly, there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through cortical and basal ganglia loops. We derive a temporal difference based actor critic learning algorithm, for which convergence can be proved without assuming widely separated time scales for the actor and the critic. The approach is demonstrated by applying it to networks of spiking neurons. The established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms. 1</p><p>5 0.50776219 <a title="96-lsi-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.50723243 <a title="96-lsi-6" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>7 0.48475471 <a title="96-lsi-7" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>8 0.47325602 <a title="96-lsi-8" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>9 0.47110498 <a title="96-lsi-9" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>10 0.45987374 <a title="96-lsi-10" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>11 0.45147318 <a title="96-lsi-11" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>12 0.43624517 <a title="96-lsi-12" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>13 0.42932105 <a title="96-lsi-13" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>14 0.42266911 <a title="96-lsi-14" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>15 0.42102459 <a title="96-lsi-15" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>16 0.40254557 <a title="96-lsi-16" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>17 0.3988454 <a title="96-lsi-17" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>18 0.39059788 <a title="96-lsi-18" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>19 0.38530341 <a title="96-lsi-19" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>20 0.38088864 <a title="96-lsi-20" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.033), (6, 0.124), (7, 0.076), (9, 0.046), (12, 0.024), (25, 0.015), (28, 0.195), (47, 0.013), (57, 0.05), (59, 0.021), (63, 0.042), (71, 0.148), (77, 0.068), (83, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94433874 <a title="96-lda-1" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>2 0.93833506 <a title="96-lda-2" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><p>3 0.92489541 <a title="96-lda-3" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>4 0.91583359 <a title="96-lda-4" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>Author: Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe</p><p>Abstract: Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method. 1</p><p>same-paper 5 0.90490639 <a title="96-lda-5" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>6 0.89519411 <a title="96-lda-6" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>7 0.89097214 <a title="96-lda-7" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>8 0.88488293 <a title="96-lda-8" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>9 0.87761879 <a title="96-lda-9" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>10 0.87150162 <a title="96-lda-10" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>11 0.8660652 <a title="96-lda-11" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>12 0.85941571 <a title="96-lda-12" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>13 0.85875523 <a title="96-lda-13" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>14 0.85710859 <a title="96-lda-14" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>15 0.85688072 <a title="96-lda-15" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>16 0.85575062 <a title="96-lda-16" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>17 0.85512739 <a title="96-lda-17" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>18 0.85494888 <a title="96-lda-18" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>19 0.8536244 <a title="96-lda-19" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>20 0.84913141 <a title="96-lda-20" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
