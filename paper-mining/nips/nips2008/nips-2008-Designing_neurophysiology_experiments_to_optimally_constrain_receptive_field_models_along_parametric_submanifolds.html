<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-60" href="#">nips2008-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</h1>
<br/><p>Source: <a title="nips-2008-60-pdf" href="http://papers.nips.cc/paper/3509-designing-neurophysiology-experiments-to-optimally-constrain-receptive-field-models-along-parametric-submanifolds.pdf">pdf</a></p><p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>Reference: <a title="nips-2008-60-reference" href="../nips2008_reference/nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Designing neurophysiology experiments to optimally constrain receptive ﬁeld models along parametric submanifolds. [sent-1, score-0.455]
</p><p>2 edu  Abstract Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. [sent-12, score-0.275]
</p><p>3 However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e. [sent-13, score-0.171]
</p><p>4 Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. [sent-16, score-0.324]
</p><p>5 For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc. [sent-17, score-0.284]
</p><p>6 More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. [sent-19, score-0.817]
</p><p>7 1  Introduction  A long standing problem in neuroscience has been collecting enough data to robustly estimate the response function of a neuron. [sent-21, score-0.206]
</p><p>8 To make optimizing the design tractable, we typically need to assume our knowledge has some nice mathematical representation. [sent-23, score-0.212]
</p><p>9 This restriction often makes it difﬁcult to include the types of prior beliefs held by neurophysiologists; for example that the receptive ﬁeld has some parametric form such as a Gabor function [7]. [sent-24, score-0.264]
</p><p>10 edu/∼liam/  1  p(θ|µt , Ct )  p(θ|µb , Cb ) TµM,t M  θ2  µM,t  M  µt θ2  θ2 θ1  θ1  θ1  Figure 1: A schematic illustrating how we use the manifold to improve stimulus design. [sent-30, score-0.449]
</p><p>11 Our method begins with a Gaussian approximation of the posterior on the full model space after t trials, p(θ|µt , C t ). [sent-31, score-0.249]
</p><p>12 The next step involves constructing the tangent space approximation of the manifold M on which θ is believed to lie, as illustrated in the middle plot; M is indicated in blue. [sent-33, score-0.858]
</p><p>13 The MAP estimate (blue dot) is projected onto the manifold to obtain µM,t (green dot). [sent-34, score-0.396]
</p><p>14 We then compute the tangent space (dashed red line) by taking the derivative of the manifold at µM,t . [sent-35, score-0.846]
</p><p>15 The tangent space is the space spanned by vectors in the direction parallel to M at µM,t . [sent-36, score-0.541]
</p><p>16 By deﬁnition, in the neighborhood of µM,t , moving along the manifold is roughly equivalent to moving along the tangent space. [sent-37, score-0.813]
</p><p>17 Thus, the tangent space provides a good local approximation of M. [sent-38, score-0.514]
</p><p>18 In the right panel we compute p(θ|µb,t , Cb,t ) by evaluating p(θ|µt , C t ) on the tangent space. [sent-39, score-0.407]
</p><p>19 the problem of incorporating this strong prior knowledge into an existing algorithm for optimizing neurophysiology experiments [8]. [sent-41, score-0.327]
</p><p>20 We use the manifold to design an experiment which will provide the largest reduction in our uncertainty about the unknown parameters. [sent-45, score-0.545]
</p><p>21 To make the computations tractable we approximate the manifold using the tangent space evaluated at the maximum a posteriori (MAP) estimate of the parameters projected onto the manifold. [sent-46, score-0.918]
</p><p>22 Despite this rather crude approximation of the geometry of the manifold, our simulations show that this method can signiﬁcantly improve the informativeness of our experiments. [sent-47, score-0.16]
</p><p>23 2  Methods  We begin by summarizing the three key elements of an existing algorithm for optimizing neurophysiology experiments. [sent-49, score-0.204]
</p><p>24 We model the neuron’s response function as a mapping between the neuron’s input at time t, st , and its response, rt . [sent-51, score-0.33]
</p><p>25 In this context, optimizing the experimental design means picking the input for which observing the response will provide the most information about the parameters θ deﬁning the conditional response function. [sent-60, score-0.391]
</p><p>26 The likelihood of the response depends on the ﬁring rate, λt , which is a function of the input, λt = E(rt ) = f θT st ,  (1)  where f () is some nonlinear function which is assumed known1 . [sent-62, score-0.265]
</p><p>27 This approximation is justiﬁed by the log-concavity of the likelihood function and asymptotic normality of the posterior distribution given sufﬁcient data [12]. [sent-66, score-0.137]
</p><p>28 Since the purpose of an experiment is to identify the best model, we optimize the design by maximizing the conditional mutual information between rt+1 and θ given st+1 , I(θ; rt+1 |st+1 ). [sent-70, score-0.276]
</p><p>29 The mutual information measures how much we expect observing the response to st+1 will reduce our uncertainty about θ. [sent-71, score-0.263]
</p><p>30 We pick the optimal input by maximizing the mutual information with respect to st+1 ; as discussed in [8], this step, along with the updating of the posterior mean and covariance (µt , C t ), may be computed efﬁciently enough for real-time implementation in many cases. [sent-72, score-0.245]
</p><p>31 For the computation of the mutual information to be tractable, the space of candidate models, Θ, must have some convenient form so that we can derive a suitable expression for the mutual information. [sent-75, score-0.239]
</p><p>32 The problem with incorporating prior knowledge is that if we restrict the model to some complicated subset of model space we will no longer be able to efﬁciently integrate over the set of candidate models. [sent-78, score-0.23]
</p><p>33 We address this problem by showing how local geometric approximations to the parameter sub-manifold can be used to guide optimal sampling while still maintaining a ﬂexible, tractable representation of the posterior distribution on the full model space. [sent-79, score-0.19]
</p><p>34 In many experiments, neurophysiologists expect a-priori that the receptive ﬁeld of a neuron will have some low-dimensional parametric structure; e. [sent-80, score-0.402]
</p><p>35 g the receptive ﬁeld might be well-approximated by a Gabor function [13], or by a difference of Gaussians [14], or by a low rank spatiotemporal matrix [15, 13]. [sent-81, score-0.253]
</p><p>36 (2)  The vector, φ, essentially enumerates the points on the manifold and Ψ() is a function which maps these points into Θ space. [sent-83, score-0.344]
</p><p>37 The basic idea is that we want to run experiments which can identify exactly where on the manifold the optimal model lies. [sent-87, score-0.344]
</p><p>38 Since M can have some arbitrary nonlinear shape, computing the informativeness of a stimulus using just the models on the manifold is not easy. [sent-88, score-0.613]
</p><p>39 Rather, we maintain a Gaussian approximation of the posterior on the full space, Θ. [sent-91, score-0.182]
</p><p>40 However, when optimizing our stimuli we combine our posterior with our knowledge of M in order to do a better job of maximizing the informativeness of each experiment. [sent-92, score-0.363]
</p><p>41 3  Computing the mutual information I(rt+1 ; θ|st+1 , s1:t , r1:t ) entails an integral over model space weighted by the posterior probability on each model. [sent-94, score-0.283]
</p><p>42 We integrate over model space because the informativeness of an experiment clearly depends on what we already know (i. [sent-95, score-0.223]
</p><p>43 Furthermore, the informativeness of an experiment will depend on the outcome. [sent-98, score-0.156]
</p><p>44 Unfortunately, since the manifold in general has some arbitrary nonlinear shape we cannot easily compute integrals over the manifold. [sent-100, score-0.388]
</p><p>45 Furthermore, we do not want to continue to restrict ourselves to models on the manifold if the data indicates our prior knowledge is wrong. [sent-101, score-0.46]
</p><p>46 We can solve both problems by making use of the tangent space of the manifold, as illustrated in Figure 1 [16]. [sent-102, score-0.474]
</p><p>47 The tangent space is a linear space which provides a local approximation of the manifold. [sent-103, score-0.581]
</p><p>48 Since the tangent space is a linear subspace of Θ, integrating over θ in the tangent space is much easier than integrating over all θ on the manifold; in fact, the methods introduced in [8] may be applied directly to this case. [sent-104, score-1.049]
</p><p>49 The tangent space is a local linear approximation evaluated at a particular point, µM,t , on the manifold. [sent-105, score-0.514]
</p><p>50 For µM,t we use the projection of µt onto the manifold (i. [sent-106, score-0.427]
</p><p>51 To ﬁnd models on the manifold close to µM,t we want to perturb the parameters φ about the values corresponding to µM,t . [sent-111, score-0.41]
</p><p>52 Thus, the subspace formed by linear combinations of the partial derivatives approximates the set of models on the manifold close to µM,t . [sent-115, score-0.472]
</p><p>53 This subspace is the tangent space,  TµM,t M = {θ : θ = µM,t + B b, ∀b ∈ Rdim(M) }  B = orth  ∂Ψ ∂Ψ . [sent-116, score-0.501]
</p><p>54 ∂φ1 ∂φd  ,  (3)  where orth is an orthonormal basis for the column space of its argument. [sent-119, score-0.151]
</p><p>55 Here Tx M denotes the tangent space at the point x. [sent-120, score-0.474]
</p><p>56 ) We now use our Gaussian posterior on the full parameter space to compute the posterior likelihood of the models in the tangent space. [sent-123, score-0.713]
</p><p>57 Since the tangent space is a subspace of Θ, restricting our Gaussian approximation, p(θ|µt , C t ), to the tangent space means we are taking a slice through our Gaussian approximation of the posterior. [sent-124, score-1.027]
</p><p>58 The result is a Gaussian distribution on the tangent space whose parameters may be obtained using the standard Gaussian conditioning formula: ptan (θ|µb,t , Cb,t ) = µb,t = −Cb,t B T C −1 (µM,t − µt ) t  N (b; µb,t , Cb,t ) if 0 if Cb,t = (B T C −1 B)−1 t  ∃ b s. [sent-126, score-0.59]
</p><p>59 Now, rather than optimizing the stimulus by trying to squeeze the uncertainty p(θ|r1:t , s1:t , M) on the nonlinear manifold M down as much as possible (a very difﬁcult task in general), we pick the stimulus which best reduces the uncertainty ptan (θ|µb,t , Cb,t ) on the vector space TµM,t . [sent-128, score-0.911]
</p><p>60 Finally, to handle the possibility that θ ∈ M, every so / often we optimize the stimulus using the full posterior p(θ|µt , C t ). [sent-130, score-0.275]
</p><p>61 tangent space design described in the text; an i. [sent-143, score-0.6]
</p><p>62 design which did not use the assumption that θ corresponds to a low rank STRF. [sent-148, score-0.211]
</p><p>63 We see that using the tangent space to optimize the design leads to much faster convergence to the true parameters; in addition, either infomax design signiﬁcantly outperforms the iid design here. [sent-152, score-0.946]
</p><p>64 In this case the true STRF did not in fact lie on the manifold M (chosen to be the set of rank-2 matrices here); thus, these results also show that our knowledge of M does not need to be exact in order to improve the experimental design. [sent-153, score-0.45]
</p><p>65 Initial conditions: start with a log-concave (approximately Gaussian) posterior given t previous trials, summarized by the posterior mean, µt and covariance, C t . [sent-157, score-0.194]
</p><p>66 Compute the tangent space of M at µM,t using Eqn. [sent-162, score-0.474]
</p><p>67 Compute the posterior restricted to the tangent space, ptan (θ|µb,t , Cb,t ), using the standard Gaussian conditioning formula (Eqn. [sent-165, score-0.62]
</p><p>68 Update the posterior by recursively updating the posterior mean and covariance: µt → µt+1 and C t → C t+1 (again, as in [8]), and return to step 1. [sent-170, score-0.194]
</p><p>69 1  Results Low rank models  To test our methods in a realistic, high-dimensional setting, we simulated a typical auditory neurophysiology [17, 15, 18] experiment. [sent-172, score-0.282]
</p><p>70 Here, the objective is to to identify the spectro-temporal receptive ﬁeld (STRF) of the neuron. [sent-173, score-0.168]
</p><p>71 The input and receptive ﬁeld of the neuron are usually represented in the frequency domain because the cochlea is known to perform a frequency decomposition of sound. [sent-174, score-0.396]
</p><p>72 Several researchers, however, have shown that low-rank assumptions can be used to produce accurate approximations of the receptive ﬁeld while signiﬁcantly reducing the number of unknown parameters [19, 13, 15, 20]. [sent-178, score-0.168]
</p><p>73 A low rank assumption is a more general version of the space-time separable assumption that is often used when studying visual receptive ﬁelds [21]. [sent-179, score-0.253]
</p><p>74 Mathematically, a low-rank assumption means that the matrix corresponding to the STRF can be written as a sum of rank one matrices, Θ = M at θ = U V T  (6)  where M at indicates the matrix formed by reshaping the vector θ to form the STRF. [sent-180, score-0.161]
</p><p>75 The columns of U and V are the principal components of the column and row spaces of Θ respectively, and encode the spectral and temporal properties of the STRF, respectively. [sent-182, score-0.126]
</p><p>76 We simulated an auditory experiment using an STRF ﬁtted to the actual response of a neuron in the Mesencephalicus lateralis pars dorsalis (MLd) of an adult male zebra ﬁnch [18]. [sent-183, score-0.363]
</p><p>77 For the manifold we used the set of θ corresponding to rank-2 matrices. [sent-186, score-0.344]
</p><p>78 The manifold of rank r matrices is convenient because we can easily project any θ onto M by reshaping θ as a matrix and then computing its singular-value-decomposition (SVD). [sent-189, score-0.57]
</p><p>79 The results clearly show that using the tangent space to design the experiments leads to much faster convergence to the true parameters. [sent-196, score-0.636]
</p><p>80 2  Real birdsong data  We also tested our method by using it to reshufﬂe the data collected during an actual experiment to ﬁnd an ordering which provided a faster decrease in the error of the ﬁtted model. [sent-199, score-0.127]
</p><p>81 We compared a design which randomly shufﬂed the trials to a design which used our info. [sent-201, score-0.309]
</p><p>82 To constrain the models we assume the STRF is low-rank and that its principal components are smooth. [sent-206, score-0.135]
</p><p>83 The smoothing prior means that if we take the Fourier transform of the principal components, the Fourier coefﬁcients of high frequencies should be zero with high probability. [sent-207, score-0.179]
</p><p>84 In other words, each principal component (the columns of U and V ) should be a linear combination of sinusoidal functions with low frequencies. [sent-208, score-0.17]
</p><p>85 (7)  Each column of F and T is a sine or cosine function representing one of the basis functions of the principal spectral (columns of F ) or temporal (columns of T ) components of the STRF. [sent-210, score-0.151]
</p><p>86 Each column of ν and η determines how we form one of the principal components by combining sine and cosine functions. [sent-211, score-0.151]
</p><p>87 ω is a diagonal matrix which speciﬁes the projection of Θ onto each principal 6  Eθlog p(r|st,θt)  0  −0. [sent-212, score-0.162]
</p><p>88 design which uses the tangent space, and a shufﬂed design. [sent-221, score-0.533]
</p><p>89 The plot shows the expected log-likelihood (prediction accuracy) of the spike trains in response to a birdsong in the test set. [sent-223, score-0.16]
</p><p>90 Using a rank 2 manifold to constrain the model produces slightly better ﬁts of the data. [sent-224, score-0.485]
</p><p>91 The sinusoidal functions corresponding to the columns of F and T should have frequencies {0, . [sent-227, score-0.146]
</p><p>92 mf and mt are the largest integers such that fo,f mf and fo,t mt are less than the Nyquist frequency. [sent-235, score-0.238]
</p><p>93 Now to enforce a smoothing prior we can simply restrict the columns of F and T to sinusoids with low frequencies. [sent-236, score-0.132]
</p><p>94 To project Θ onto the manifold we simply need to compute ν, ω and η by evaluating the SVD of F T ΘT . [sent-237, score-0.396]
</p><p>95 Furthermore, incorporating the low-rank assumption using the tangent space improves the info. [sent-241, score-0.521]
</p><p>96 Thus, the different designs could choose radically different stimulus sets; in contrast, when re-analyzing the data ofﬂine, all we can do is reshufﬂe the trials, but the stimulus sets remain the same in the info. [sent-247, score-0.317]
</p><p>97 4  Conclusion  We have provided a method for incorporating detailed prior information in existing algorithms for the information-theoretic optimal design of neurophysiology experiments. [sent-250, score-0.367]
</p><p>98 These methods use realistic assumptions about the neuron’s response function and choose signiﬁcantly more informative stimuli, leading to faster convergence to the true response function using fewer experimental trials. [sent-251, score-0.246]
</p><p>99 We expect that the inclusion of this strong prior information will help experimentalists contend with the high dimensionality of neural response functions. [sent-252, score-0.183]
</p><p>100 We plot µt for trials in the interval over which the expected log-likelihood of the different designs differed the most in Fig. [sent-261, score-0.164]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('strf', 0.467), ('tangent', 0.407), ('manifold', 0.344), ('receptive', 0.168), ('neurophysiology', 0.149), ('design', 0.126), ('glm', 0.12), ('informativeness', 0.12), ('st', 0.116), ('rt', 0.109), ('designs', 0.107), ('stimulus', 0.105), ('response', 0.105), ('neuron', 0.102), ('posterior', 0.097), ('paninski', 0.096), ('trial', 0.092), ('mutual', 0.086), ('gabor', 0.086), ('rank', 0.085), ('nch', 0.082), ('ptan', 0.082), ('shuf', 0.082), ('principal', 0.079), ('eld', 0.078), ('zebra', 0.072), ('mf', 0.072), ('space', 0.067), ('perturb', 0.066), ('frequency', 0.063), ('et', 0.062), ('georgia', 0.062), ('stimuli', 0.06), ('neuroscience', 0.06), ('trials', 0.057), ('constrain', 0.056), ('birdsong', 0.055), ('khz', 0.055), ('lewi', 0.055), ('liam', 0.055), ('mld', 0.055), ('orth', 0.055), ('shuffled', 0.055), ('woolley', 0.055), ('frequencies', 0.055), ('optimizing', 0.055), ('onto', 0.052), ('neurophysiol', 0.051), ('parametric', 0.051), ('tractable', 0.048), ('neurophysiologists', 0.048), ('reshaping', 0.048), ('reshuf', 0.048), ('auditory', 0.048), ('incorporating', 0.047), ('columns', 0.047), ('mt', 0.047), ('columbia', 0.045), ('full', 0.045), ('prior', 0.045), ('sinusoidal', 0.044), ('bird', 0.044), ('jeremy', 0.044), ('nonlinear', 0.044), ('tan', 0.041), ('robustly', 0.041), ('matrices', 0.041), ('approximation', 0.04), ('restrict', 0.04), ('subspace', 0.039), ('sine', 0.039), ('svd', 0.039), ('uncertainty', 0.039), ('tted', 0.037), ('experiment', 0.036), ('faster', 0.036), ('ring', 0.035), ('lie', 0.034), ('conditioning', 0.034), ('expect', 0.033), ('entails', 0.033), ('cosine', 0.033), ('manifolds', 0.033), ('derivatives', 0.032), ('integrating', 0.031), ('pick', 0.031), ('projection', 0.031), ('furthermore', 0.031), ('knowledge', 0.031), ('along', 0.031), ('dot', 0.03), ('iid', 0.03), ('gaussian', 0.03), ('map', 0.03), ('partial', 0.029), ('orthonormal', 0.029), ('mathematically', 0.029), ('formed', 0.028), ('derivative', 0.028), ('optimize', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="60-tfidf-1" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>2 0.21074036 <a title="60-tfidf-2" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>3 0.14672862 <a title="60-tfidf-3" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>4 0.11034069 <a title="60-tfidf-4" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>5 0.092538595 <a title="60-tfidf-5" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>6 0.082877003 <a title="60-tfidf-6" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>7 0.081202999 <a title="60-tfidf-7" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>8 0.075023562 <a title="60-tfidf-8" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>9 0.073098503 <a title="60-tfidf-9" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>10 0.072929285 <a title="60-tfidf-10" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>11 0.070880689 <a title="60-tfidf-11" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>12 0.069823027 <a title="60-tfidf-12" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>13 0.067809701 <a title="60-tfidf-13" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>14 0.063243881 <a title="60-tfidf-14" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>15 0.062945768 <a title="60-tfidf-15" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>16 0.062073871 <a title="60-tfidf-16" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>17 0.060732294 <a title="60-tfidf-17" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>18 0.059442054 <a title="60-tfidf-18" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>19 0.058178499 <a title="60-tfidf-19" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>20 0.05809366 <a title="60-tfidf-20" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.191), (1, 0.039), (2, 0.098), (3, 0.099), (4, 0.023), (5, 0.015), (6, 0.007), (7, 0.054), (8, 0.059), (9, 0.045), (10, 0.057), (11, -0.014), (12, -0.125), (13, 0.037), (14, -0.056), (15, 0.067), (16, 0.072), (17, 0.138), (18, 0.028), (19, -0.063), (20, -0.061), (21, -0.049), (22, 0.008), (23, -0.123), (24, -0.091), (25, 0.069), (26, 0.032), (27, 0.065), (28, 0.043), (29, 0.153), (30, -0.04), (31, -0.077), (32, 0.15), (33, 0.016), (34, 0.012), (35, -0.285), (36, -0.165), (37, 0.094), (38, 0.069), (39, -0.04), (40, 0.051), (41, -0.089), (42, -0.037), (43, 0.092), (44, -0.014), (45, 0.084), (46, -0.011), (47, 0.005), (48, -0.007), (49, -0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94619495 <a title="60-lsi-1" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>2 0.79869735 <a title="60-lsi-2" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>3 0.61911339 <a title="60-lsi-3" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>4 0.52205414 <a title="60-lsi-4" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>5 0.52130479 <a title="60-lsi-5" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>Author: Michael P. Holmes, Jr. Isbell, Charles Lee, Alexander G. Gray</p><p>Abstract: The Singular Value Decomposition is a key operation in many machine learning methods. Its computational cost, however, makes it unscalable and impractical for applications involving large datasets or real-time responsiveness, which are becoming increasingly common. We present a new method, QUIC-SVD, for fast approximation of the whole-matrix SVD based on a new sampling mechanism called the cosine tree. Our empirical tests show speedups of several orders of magnitude over exact SVD. Such scalability should enable QUIC-SVD to accelerate and enable a wide array of SVD-based methods and applications. 1</p><p>6 0.51553035 <a title="60-lsi-6" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>7 0.46246696 <a title="60-lsi-7" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>8 0.45397335 <a title="60-lsi-8" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>9 0.39908183 <a title="60-lsi-9" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>10 0.39879793 <a title="60-lsi-10" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>11 0.39186102 <a title="60-lsi-11" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>12 0.37136066 <a title="60-lsi-12" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>13 0.36214927 <a title="60-lsi-13" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>14 0.3596696 <a title="60-lsi-14" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>15 0.341934 <a title="60-lsi-15" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>16 0.33537969 <a title="60-lsi-16" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>17 0.32768229 <a title="60-lsi-17" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>18 0.3245399 <a title="60-lsi-18" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>19 0.3150512 <a title="60-lsi-19" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>20 0.31390849 <a title="60-lsi-20" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.061), (7, 0.135), (12, 0.039), (15, 0.021), (25, 0.011), (28, 0.181), (57, 0.063), (59, 0.019), (63, 0.027), (71, 0.019), (77, 0.046), (83, 0.035), (94, 0.277)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84455538 <a title="60-lda-1" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>Author: Shuang Wu, Hongjing Lu, Alan L. Yuille</p><p>Abstract: Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These ﬁndings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we ﬁrst perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We deﬁne novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments. 1</p><p>same-paper 2 0.80802393 <a title="60-lda-2" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>3 0.80673313 <a title="60-lda-3" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>Author: Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk</p><p>Abstract: Compressive Sensing (CS) combines sampling and compression into a single subNyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</p><p>4 0.76102078 <a title="60-lda-4" href="./nips-2008-Algorithms_for_Infinitely_Many-Armed_Bandits.html">17 nips-2008-Algorithms for Infinitely Many-Armed Bandits</a></p>
<p>Author: Yizao Wang, Jean-yves Audibert, Rémi Munos</p><p>Abstract: We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-conﬁdence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matches (up to a logarithmic factor) the upper-bound in some cases. 1</p><p>5 0.65758157 <a title="60-lda-5" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>6 0.65161014 <a title="60-lda-6" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>7 0.64820254 <a title="60-lda-7" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>8 0.64801073 <a title="60-lda-8" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>9 0.64555532 <a title="60-lda-9" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>10 0.64490032 <a title="60-lda-10" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>11 0.64475393 <a title="60-lda-11" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>12 0.64390701 <a title="60-lda-12" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>13 0.64283758 <a title="60-lda-13" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>14 0.64280182 <a title="60-lda-14" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>15 0.64191729 <a title="60-lda-15" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>16 0.64180171 <a title="60-lda-16" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>17 0.64135003 <a title="60-lda-17" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>18 0.64089167 <a title="60-lda-18" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>19 0.64053619 <a title="60-lda-19" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>20 0.64028609 <a title="60-lda-20" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
