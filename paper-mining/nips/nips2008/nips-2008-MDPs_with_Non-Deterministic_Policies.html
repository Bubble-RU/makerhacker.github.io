<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2008-MDPs with Non-Deterministic Policies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-131" href="#">nips2008-131</a> knowledge-graph by maker-knowledge-mining</p><h1>131 nips-2008-MDPs with Non-Deterministic Policies</h1>
<br/><p>Source: <a title="nips-2008-131-pdf" href="http://papers.nips.cc/paper/3504-mdps-with-non-deterministic-policies.pdf">pdf</a></p><p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><p>Reference: <a title="nips-2008-131-reference" href="../nips2008_reference/nips-2008-MDPs_with_Non-Deterministic_Policies_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.759), ('mip', 0.25), ('mdp', 0.242), ('cit', 0.159), ('ct', 0.15), ('eqn', 0.141), ('qid', 0.132), ('mdps', 0.121), ('search', 0.117), ('reward', 0.114), ('bup', 0.11), ('vm', 0.109), ('conserv', 0.092), ('paty', 0.082), ('vmax', 0.082), ('med', 0.067), ('getoptim', 0.066), ('nonaug', 0.066), ('tg', 0.066), ('vmin', 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="131-tfidf-1" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>2 0.47237647 <a title="131-tfidf-2" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>3 0.47060961 <a title="131-tfidf-3" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>4 0.37976506 <a title="131-tfidf-4" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>5 0.29247785 <a title="131-tfidf-5" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>6 0.26506954 <a title="131-tfidf-6" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>7 0.24830064 <a title="131-tfidf-7" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>8 0.23194532 <a title="131-tfidf-8" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>9 0.21223772 <a title="131-tfidf-9" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>10 0.20153528 <a title="131-tfidf-10" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>11 0.17001712 <a title="131-tfidf-11" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>12 0.16091074 <a title="131-tfidf-12" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>13 0.15391749 <a title="131-tfidf-13" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>14 0.15018946 <a title="131-tfidf-14" href="./nips-2008-Bounding_Performance_Loss_in_Approximate_MDP_Homomorphisms.html">39 nips-2008-Bounding Performance Loss in Approximate MDP Homomorphisms</a></p>
<p>15 0.13295348 <a title="131-tfidf-15" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>16 0.12672208 <a title="131-tfidf-16" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>17 0.10785209 <a title="131-tfidf-17" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>18 0.081455179 <a title="131-tfidf-18" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>19 0.067123987 <a title="131-tfidf-19" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>20 0.067107327 <a title="131-tfidf-20" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.191), (1, -0.506), (2, 0.218), (3, 0.181), (4, 0.319), (5, 0.059), (6, -0.22), (7, -0.053), (8, -0.026), (9, 0.047), (10, -0.055), (11, -0.051), (12, 0.004), (13, 0.002), (14, 0.002), (15, -0.02), (16, 0.011), (17, 0.099), (18, 0.029), (19, 0.093), (20, 0.003), (21, 0.119), (22, -0.011), (23, 0.029), (24, 0.021), (25, 0.0), (26, 0.021), (27, -0.031), (28, 0.0), (29, -0.026), (30, -0.012), (31, 0.014), (32, 0.043), (33, -0.054), (34, -0.003), (35, -0.016), (36, -0.058), (37, 0.105), (38, -0.028), (39, 0.057), (40, -0.016), (41, 0.008), (42, 0.039), (43, -0.004), (44, -0.021), (45, -0.014), (46, -0.021), (47, 0.008), (48, -0.062), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97582924 <a title="131-lsi-1" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>2 0.88485563 <a title="131-lsi-2" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>3 0.81839228 <a title="131-lsi-3" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>4 0.81270063 <a title="131-lsi-4" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>5 0.78412706 <a title="131-lsi-5" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>6 0.70598066 <a title="131-lsi-6" href="./nips-2008-Bounding_Performance_Loss_in_Approximate_MDP_Homomorphisms.html">39 nips-2008-Bounding Performance Loss in Approximate MDP Homomorphisms</a></p>
<p>7 0.68140548 <a title="131-lsi-7" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>8 0.68036228 <a title="131-lsi-8" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>9 0.66317296 <a title="131-lsi-9" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>10 0.65818042 <a title="131-lsi-10" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>11 0.59127122 <a title="131-lsi-11" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>12 0.5866344 <a title="131-lsi-12" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>13 0.39808843 <a title="131-lsi-13" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>14 0.39013991 <a title="131-lsi-14" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>15 0.38808876 <a title="131-lsi-15" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>16 0.33113709 <a title="131-lsi-16" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>17 0.32083464 <a title="131-lsi-17" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>18 0.2969254 <a title="131-lsi-18" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>19 0.26152214 <a title="131-lsi-19" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>20 0.24043195 <a title="131-lsi-20" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.012), (25, 0.046), (30, 0.051), (40, 0.057), (60, 0.019), (63, 0.204), (64, 0.088), (71, 0.114), (75, 0.287)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77990669 <a title="131-lda-1" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>2 0.65899384 <a title="131-lda-2" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>3 0.65695608 <a title="131-lda-3" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>4 0.6560238 <a title="131-lda-4" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>5 0.65244293 <a title="131-lda-5" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>6 0.65202397 <a title="131-lda-6" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>7 0.65175951 <a title="131-lda-7" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>8 0.65165174 <a title="131-lda-8" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>9 0.64920527 <a title="131-lda-9" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>10 0.6487689 <a title="131-lda-10" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>11 0.64656913 <a title="131-lda-11" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>12 0.64414155 <a title="131-lda-12" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>13 0.64075512 <a title="131-lda-13" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>14 0.63968027 <a title="131-lda-14" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>15 0.63908219 <a title="131-lda-15" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>16 0.63747084 <a title="131-lda-16" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>17 0.63741058 <a title="131-lda-17" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>18 0.63687664 <a title="131-lda-18" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>19 0.63334793 <a title="131-lda-19" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>20 0.63156682 <a title="131-lda-20" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
