<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-53" href="#">nips2008-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</h1>
<br/><p>Source: <a title="nips-2008-53-pdf" href="http://papers.nips.cc/paper/3512-counting-solution-clusters-in-graph-coloring-problems-using-belief-propagation.pdf">pdf</a></p><p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>Reference: <a title="nips-2008-53-reference" href="../nips2008_reference/nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ∗  Abstract We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. [sent-6, score-0.977]
</p><p>2 This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. [sent-7, score-0.625]
</p><p>3 Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. [sent-8, score-0.346]
</p><p>4 Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. [sent-10, score-0.373]
</p><p>5 Recently, these techniques have also been applied to compute properties of discrete spaces, in particular, properties of the space of solutions of combinatorial problems. [sent-13, score-0.223]
</p><p>6 For example, for propositional satisﬁability (SAT) and graph coloring (COL) problems, marginal probability information about the uniform distribution over solutions (or similar combinatorial objects) has been the key ingredient in the success of BP-like algorithms. [sent-14, score-0.77]
</p><p>7 Most notably, the survey propagation (SP) algorithm utilizes this information to solve very large hard random instances of these problems [3, 11]. [sent-15, score-0.231]
</p><p>8 Earlier work on random ensembles of Constraint Satisfaction Problems (CSPs) has shown that the computationally hardest instances occur near phase boundaries, where instances go from having many globally satisfying solutions to having no solution at all (a “solution-focused picture”). [sent-16, score-0.454]
</p><p>9 In recent years, this picture has been reﬁned and it was found that a key factor in determining the hardness of instances in terms of search algorithm (or sampling algorithm) is the question: how are the solutions spatially distributed within the search space? [sent-17, score-0.349]
</p><p>10 This has made the structure of the solution space in terms of its clustering properties a key factor in determining the performance of combinatorial search methods (a “cluster-focused picture”). [sent-18, score-0.213]
</p><p>11 For example, how many clusters are there in a solution space? [sent-20, score-0.366]
</p><p>12 Answers to such questions will shed further light into our understanding of these hard combinatorial problems and lead to better algorithmic approaches for reasoning about them, be it for ﬁnding one solution or answering queries of probabilistic inference about the set of solutions. [sent-23, score-0.192]
</p><p>13 We provide a purely combinatorial method for counting the number of clusters, which is applicable even to small size problems and can be approximated very well by message passing techniques. [sent-28, score-0.213]
</p><p>14 Solutions can be thought of as ‘neighbors’ if they differ in the value of one variable, and the transitive closure of the neighbor relation deﬁnes clusters in a natural manner. [sent-29, score-0.326]
</p><p>15 Counting the number of clusters is a challenging problem. [sent-30, score-0.287]
</p><p>16 One relatively crude but useful way is to represent a cluster by the set of ‘backbone’ variables in that cluster, i. [sent-32, score-0.17]
</p><p>17 , variables that take a ﬁxed value in all solutions within the cluster. [sent-34, score-0.178]
</p><p>18 Interestingly, while it is easy (polynomial time) to verify whether a variable assignment is indeed a solution of CSP, the same check is much harder for a candidate cluster represented by the set of its backbone variables. [sent-35, score-0.318]
</p><p>19 We propose one of the ﬁrst scalable methods for estimating the number of clusters of solutions of graph coloring problems using a belief propagation like algorithm. [sent-36, score-1.138]
</p><p>20 We validate the accuracy of our approach by also providing a fairly non-trivial exact counting method for clusters, utilizing advanced knowledge compilation techniques. [sent-38, score-0.226]
</p><p>21 Our approach works with the factor graph representation of the graph coloring problem. [sent-39, score-0.707]
</p><p>22 [12] showed that if one can write the so-called “partition function”, Z, for a quantity of interest in a factor graph with non-negative weights, then there is a fairly mechanical variational method derivation that yields belief propagation equations for estimating Z. [sent-41, score-0.418]
</p><p>23 Our experiments with random graph coloring problems show that Z(−1) itself is an extremely accurate estimate of the number of clusters, and so is its approximation, ZBP(−1) , obtained from our BP equations. [sent-44, score-0.542]
</p><p>24 2  Preliminaries  The graph coloring problem can be expressed in the form of a factor graph, a bipartite graph with two kinds of nodes. [sent-45, score-0.707]
</p><p>25 Each factor function is a Boolean function with arguments xα (a subset of variables from x) and range {0, 1}, and evaluates to 1 if and only if (iff) the associated constraint is satisﬁed. [sent-60, score-0.158]
</p><p>26 An edge connects a variable xi with factor fα iff the variable appears in the constraint represented by the factor node, which we denote by i ∈ α. [sent-61, score-0.22]
</p><p>27 In the graph coloring problem, each factor function has exactly two variables. [sent-62, score-0.563]
</p><p>28 The assignments with weight 1 correspond precisely to legal colorings, or solutions to the problem. [sent-66, score-0.225]
</p><p>29 We denote this quantity by Z, the so-called partition function: Z :=  F (x) = x∈Domn  fα (xα ) x∈Domn  (1)  α  We deﬁne the solution space of a graph coloring problem to be the set of all its legal colorings. [sent-68, score-0.704]
</p><p>30 Two legal colorings (or solutions) are called neighbors if they differ in the color of one vertex. [sent-69, score-0.229]
</p><p>31 A set of solutions C ⊆ S of a solution space S is a cluster if it is a maximal subset such that any two solutions in C can be connected by a sequence from C where consecutive solutions are neighbors. [sent-71, score-0.635]
</p><p>32 In other words, clusters are connected components of the “solution graph” which has solutions as nodes and an edge between two solutions if they differ in the value of exactly one variable. [sent-72, score-0.646]
</p><p>33 3  A Partition Function Style Expression for Counting Clusters  In this section we consider a method for estimating the number of solution clusters of a graph coloring problem. [sent-73, score-0.877]
</p><p>34 This means that F ′ evaluates to 1 on a hypercube iff F evaluates to 1 on all points within that hypercube. [sent-84, score-0.261]
</p><p>35 Let us ﬁrst assume that the solution space we work with decomposes into a set of separated hypercubes, so clusters correspond exactly to the hypercubes; by separated hypercubes, we mean that points in one hypercube differ from points in others in at least two values. [sent-85, score-0.562]
</p><p>36 , y1 = ({c1 } , {c1 } , {c1 }) and y2 = ({c2 } , {c3 } , {c1 , c2 }) are separated hypercubes in three dimensions. [sent-88, score-0.167]
</p><p>37 This allows us to develop a surprisingly simple expression for counting the number of clusters, and we will later see that the same expression applies with high precision also to solution spaces of much more complex instances of graph coloring problems. [sent-89, score-0.936]
</p><p>38 Note that if the solution clusters are in fact hypercubes, then variable values that can be “extended” independently can also be extended all at once, that is, F ′ (y[yi ← yi ∪ {vi }]) = 1 and F ′ (y[yj ← yj ∪ {vj }]) = 1 implies F (y[yi ← yi ∪ {vi } , yj ← yj ∪ {vj }]) = 1. [sent-91, score-0.89]
</p><p>39 Moreover, any F ′ (y[yi ← yi ∪ {vi }]) implies F (y). [sent-92, score-0.171]
</p><p>40 It is easily seen that if the solution space consists of a set of separated hypercubes, then Z(−1) exactly captures the number of clusters (each separated hypercube is a cluster). [sent-96, score-0.523]
</p><p>41 Surprisingly, this number is remarkably accurate even for random coloring problems as we will see in Section 6, Figure 1. [sent-97, score-0.398]
</p><p>42 4  Exact Computation of the Number of Clusters and Z(−1)  Obtaining the exact number of clusters for reasonable size problems is crucial for evaluating our proposed approach based on Z(−1) and the corresponding BP equations to follow in Section 5. [sent-98, score-0.399]
</p><p>43 Not surprisingly, this method does not scale well because the number of solutions typically grows exponentially as the number of variables of the graph coloring problems increases. [sent-100, score-0.72]
</p><p>44 Our method scales to graph coloring problems with a few hundred variables (see experimental results) for computing both the exact number of clusters and the exact value of Z(−1) . [sent-102, score-0.978]
</p><p>45 Both DNNF [6] and BDD [4] are graph based data structures that have proven to be very effective in “knowledge compilation”, i. [sent-103, score-0.144]
</p><p>46 For our purposes, we use DNNF to succinctly represent all solutions of F and a set of BDDs to represent solution clusters that we create as we traverse the DNNF representation. [sent-106, score-0.569]
</p><p>47 , by replacing each variable xi of F with domain size t with t Boolean variables x′ , 1 ≤ j ≤ t, respecting the semantics: xi = j iff xi,j = 1. [sent-111, score-0.149]
</p><p>48 This generalization has the following useful property: if two solutions x(1) and x(2) are neighbors in the solution space of F , then the corresponding solutions x′(1) and x′(2) are in the same cluster in the solution space of F ′ . [sent-117, score-0.607]
</p><p>49 Next, we traverse F ′′ from the leaf nodes up, creating clusters as we go along. [sent-120, score-0.385]
</p><p>50 Speciﬁcally, with each node U of F ′′ , we associate a set SU of BDDs, one for each cluster in the sub-formula contained under U . [sent-121, score-0.212]
</p><p>51 The set of BDDs for the root node of F ′′ then corresponds precisely to the set of solution clusters of F ′ , and thus of F . [sent-122, score-0.445]
</p><p>52 If U is a leaf node of F ′′ , it represents a Boolean variable or its negation and SU consists of the single one-node BDD corresponding to this Boolean literal. [sent-124, score-0.199]
</p><p>53 Similarly, if U is an “and” node, then SU is constructed by considering the cross product {bL and bR | bL ∈ SL , bR ∈ SR } of the two sets of BDDs and merging adjacent resulting BDDs as before; in the worst case, this leads to |SL | · |SR | cluster BDDs in SU . [sent-129, score-0.158]
</p><p>54 In fact, as is reﬂected in our experimental results, evaluation of Z(−1) is a much more scalable process than counting clusters because it requires a simple traversal of F ′′ without the need for maintaining BDDs. [sent-132, score-0.414]
</p><p>55 Suppose L has VL solutions with an even number of o positive literals and VL solutions with an odd number of positive literals; similarly for R. [sent-141, score-0.374]
</p><p>56 vertex) fα ; y−i are values of all variables in y except yi . [sent-153, score-0.208]
</p><p>57 • Consistency: ∀α, i ∈ α, yi : bi (yi ) =  yα\i bα (yα ). [sent-163, score-0.245]
</p><p>58 • Tree-like decomposition: says that the weights b(y) of each conﬁguration can be obtained from the marginal sums as follows (di is the degree of the variable node yi in the factor graph): Q α |bα (yα )| |b(y)| = Q |bi (yi )|di −1 . [sent-165, score-0.441]
</p><p>59 • Sign-alternation: bi (yi ) is negative iff |yi | is even, and bα (yα ) is negative iff #e (yα ) is odd. [sent-174, score-0.236]
</p><p>60 This is also a built-in assumption, but not necessarily legitimate; whether or not it is legitimate depends on the structure of the solution space of a particular problem. [sent-175, score-0.196]
</p><p>61 The Sign-alternation assumption can be viewed as an application of the inclusion-exclusion principle, and is easy to illustrate on a graph coloring problem with only two colors. [sent-176, score-0.511]
</p><p>62 In this case, if F ′ (y) = 1, then yi = {c1 } means that yi can have color 1, yi = {c2 } that yi can have color 2, and yi = {c1 , c2 } that yi can have both colors. [sent-177, score-1.094]
</p><p>63 BP equations: The resulting modiﬁed BP updates (denoted BP(−1) ) are, for yi ∈ DomExt: ni→α (yi )  =  mβ→i (yi )  (3)  β∋i\α ′ fα (yα )  mα→i (yi ) ∝ yα\i ∈DomExt|α|−1  (−1)δ(|yj | is even) nj→α (yj )  (4)  j∈α\i  (Almost equivalent to standard BP, except for the (−1) term. [sent-197, score-0.171]
</p><p>64 The left panel of Figure 1 compares the number of clusters (on the x-axis, log-scale) with Z(−1) (on the y-axis, log-scale) for 2, 500 colorable random 3-COL instances on graphs with 20, 50, and 100 vertices with average vertex degree ranging between 1. [sent-206, score-0.768]
</p><p>65 As can be seen, the Z(−1) expression captures the number of clusters almost exactly. [sent-209, score-0.324]
</p><p>66 The inaccuracies come mostly from low graph density regions; in all instances we tried with density > 3. [sent-210, score-0.261]
</p><p>67 It is worth noting that for tree-structured graphs (with more than one vertex), the Z(−1) expression gives 0 for any k ≥ 3 colors although there is exactly one solution cluster. [sent-213, score-0.219]
</p><p>68 Moreover, given a disconnected graph with at least one tree component, Z(−1) also evaluates to 0 as it is the product of Z(−1) values over different components. [sent-214, score-0.213]
</p><p>69 For low graph densities, there are still some instances  0. [sent-216, score-0.261]
</p><p>70 00  5 5  20 50 200 1000 Number of clusters  5000  ZBP(−1), |V|=100K ZBP(−1), |V|=100 Z(−1), |V|=100  0. [sent-223, score-0.287]
</p><p>71 number of clusters in random 3-COL problems with 20, 50 and 100 vertices, and average vertex degree between 1. [sent-230, score-0.441]
</p><p>72 for which Z(−1) evaluates to 0; these instances are not visible in Figure 1 due to the log-log scale. [sent-237, score-0.186]
</p><p>73 In fact, all our instances with fewer than 5 clusters have Z(−1) = 0. [sent-238, score-0.404]
</p><p>74 For a given problem instance, we can deﬁne the cluster marginal of a variable xi to be the fraction of solution clusters in which xi only appears with one particular value (i. [sent-245, score-0.565]
</p><p>75 Recall that the semantics of factors in the extended domain is such that a variable can assume a set of values only if every value in the set yields a solution to the problem. [sent-250, score-0.161]
</p><p>76 This extends to the Z(−1) estimate of the number of clusters, and one can therefore use the principle of inclusion-exclusion to compute the number of clusters where a variable can only assume one particular value. [sent-251, score-0.317]
</p><p>77 The deﬁnition of Z(−1) conveniently provides for correct signs, and the number of clusters where xi is ﬁxed to vi is thus estimated by yi ∋vi Z(−1) (yi ), where Z(−1) (yi ) is the marginal sum of Z(−1) . [sent-252, score-0.559]
</p><p>78 The plot shows cluster marginals and Z(−1) -marginals for one color; the points correspond to individual variables. [sent-255, score-0.207]
</p><p>79 They are merely an estimate of the true cluster marginals, and how well they work depends on the solution space structure at hand. [sent-258, score-0.212]
</p><p>80 They are exact if the solution space decomposes into separated hypercubes and, as the ﬁgure shows, remarkably accurate also for random coloring instances. [sent-259, score-0.653]
</p><p>81 Figure 3 depicts a comparison between ZBP(−1) and Z(−1) for the 3-COL problem on colorable random graphs of various sizes and graph densities. [sent-262, score-0.315]
</p><p>82 It compares Z(−1) (on the x-axis, log-scale) with ZBP(−1) (y-axis, log-scale) for 1, 300 colorable 3-COL instances on random graphs with 50, 100, and 200 vertices, with average vertex degree ranging from 1. [sent-263, score-0.411]
</p><p>83 Instances which are not 3-colorable are not shown, and BP in general incorrectly estimates a non-zero number of clusters for them. [sent-267, score-0.287]
</p><p>84 Estimates on very large graphs and for various graph densities. [sent-268, score-0.219]
</p><p>85 Figure 2 shows similar data from a different perspective: what is shown is a rescaled average estimate of the number of clusters (y-axis) for average vertex degrees 1. [sent-269, score-0.378]
</p><p>86 The average is taken across different colorable instances of a given size, and the rescaling assumes that the number of clusters = exp(|V |·Σ) where Σ is a constant independent of the number of vertices [3]. [sent-272, score-0.57]
</p><p>87 The three curves show, respectively, BP’s estimate for graphs with 100, 000 vertices, BP’s estimate for graphs with 100 vertices, and Z(−1) for the same graphs of size 100. [sent-273, score-0.225]
</p><p>88 The averages are computed across 3, 000 instances of the small graphs, and only 10 instances of the large ones where the instance-to-instance variability is practically nonexistent. [sent-274, score-0.234]
</p><p>89 on average for colorable instances (where we can compare it with exact values), and that the estimate remains accurate for large problems. [sent-278, score-0.253]
</p><p>90 [3] also aims at computing the number of certain clusters in the solution space. [sent-280, score-0.366]
</p><p>91 However, SP counts only the number of clusters with a “typical size”, and would show non-zero values in Figure 2 only for average vertex degrees between 4. [sent-281, score-0.416]
</p><p>92 Our algorithm counts clusters of all sizes, and is very accurate in the entire range of graph densities. [sent-284, score-0.469]
</p><p>93 7  Conclusion  We discuss a purely combinatorial construction for estimating the number of solution clusters in graph coloring problems with very high accuracy. [sent-285, score-0.99]
</p><p>94 The technique uses a hypercube-based inclusionexclusion argument coupled with solution counting, and lends itself to an application of a modiﬁed belief propagation algorithm. [sent-286, score-0.22]
</p><p>95 This way, the number of clusters in huge random graph coloring instances can be accurately and efﬁciently estimated. [sent-287, score-0.915]
</p><p>96 Our preliminary investigation has revealed that it is possible to use combinatorial arguments to formally prove that the cluster counts estimated by Z(−1) are exact on certain kinds of solution spaces (not necessarily only for graph coloring). [sent-288, score-0.547]
</p><p>97 We hope that such insights and the cluster-focused picture will lead to new techniques for solving hard combinatorial problems and for bounding solvability transitions in random problem ensembles. [sent-289, score-0.152]
</p><p>98 Polynomial iterative algorithms for coloring and analyzing random graphs. [sent-308, score-0.367]
</p><p>99 Counting solution clusters of combinatorial problems using belief propagation, 2008. [sent-336, score-0.537]
</p><p>100 Gibbs states and the set of solutions of random constraint satisfaction problems. [sent-344, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coloring', 0.367), ('bp', 0.322), ('bdds', 0.306), ('clusters', 0.287), ('zbp', 0.248), ('dnnf', 0.191), ('yi', 0.171), ('vl', 0.168), ('domextn', 0.153), ('graph', 0.144), ('solutions', 0.141), ('cluster', 0.133), ('vr', 0.129), ('hypercubes', 0.122), ('instances', 0.117), ('legitimate', 0.117), ('counting', 0.1), ('colorable', 0.096), ('signs', 0.095), ('vertex', 0.091), ('legal', 0.084), ('propagation', 0.083), ('combinatorial', 0.082), ('solution', 0.079), ('node', 0.079), ('domext', 0.076), ('graphs', 0.075), ('marginals', 0.074), ('bi', 0.074), ('boolean', 0.074), ('vu', 0.072), ('su', 0.071), ('vertices', 0.07), ('evaluates', 0.069), ('hypercube', 0.067), ('col', 0.067), ('literals', 0.067), ('vi', 0.065), ('sr', 0.062), ('sl', 0.062), ('negation', 0.061), ('belief', 0.058), ('compilation', 0.057), ('yedidia', 0.057), ('iff', 0.056), ('factor', 0.052), ('backbone', 0.05), ('satisfaction', 0.05), ('separated', 0.045), ('sign', 0.043), ('yj', 0.042), ('equations', 0.041), ('sums', 0.041), ('derivation', 0.04), ('exact', 0.04), ('differ', 0.039), ('picture', 0.039), ('bdd', 0.038), ('colorings', 0.038), ('domn', 0.038), ('kroc', 0.038), ('sabharwal', 0.038), ('counts', 0.038), ('nodes', 0.038), ('expression', 0.037), ('variables', 0.037), ('marginal', 0.036), ('color', 0.034), ('neighbors', 0.034), ('braunstein', 0.033), ('count', 0.033), ('degree', 0.032), ('hundred', 0.032), ('spaces', 0.031), ('succinctly', 0.031), ('traverse', 0.031), ('problems', 0.031), ('partition', 0.03), ('variable', 0.03), ('leaf', 0.029), ('cornell', 0.029), ('decomposable', 0.029), ('advanced', 0.029), ('di', 0.028), ('colors', 0.028), ('ck', 0.027), ('sat', 0.027), ('dom', 0.027), ('scalable', 0.027), ('assignment', 0.026), ('br', 0.026), ('bl', 0.026), ('extended', 0.026), ('domain', 0.026), ('adjacent', 0.025), ('odd', 0.025), ('sp', 0.025), ('negative', 0.025), ('children', 0.024), ('surprisingly', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="53-tfidf-1" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>2 0.15427199 <a title="53-tfidf-2" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>3 0.139865 <a title="53-tfidf-3" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>Author: Michael Isard, John MacCormick, Kannan Achan</p><p>Abstract: Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a ﬁxed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense. 1</p><p>4 0.13728155 <a title="53-tfidf-4" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>Author: Mark Herbster, Guy Lever, Massimiliano Pontil</p><p>Abstract: We continue our study of online prediction of the labelling of a graph. We show a fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this drawback by means of an efﬁcient algorithm which achieves a logarithmic mistake bound. It is based on the notion of a spine, a path graph which provides a linear embedding of the original graph. In practice, graphs may exhibit cluster structure; thus in the last part, we present a modiﬁed algorithm which achieves the “best of both worlds”: it performs well locally in the presence of cluster structure, and globally on large diameter graphs. 1</p><p>5 0.12975274 <a title="53-tfidf-5" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>6 0.11520814 <a title="53-tfidf-6" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>7 0.11333024 <a title="53-tfidf-7" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>8 0.10552513 <a title="53-tfidf-8" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>9 0.097294591 <a title="53-tfidf-9" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>10 0.095115036 <a title="53-tfidf-10" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>11 0.09486571 <a title="53-tfidf-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.093906589 <a title="53-tfidf-12" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>13 0.089822322 <a title="53-tfidf-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.08900921 <a title="53-tfidf-14" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>15 0.085462615 <a title="53-tfidf-15" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>16 0.07987643 <a title="53-tfidf-16" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>17 0.075975537 <a title="53-tfidf-17" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>18 0.074513242 <a title="53-tfidf-18" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>19 0.070649594 <a title="53-tfidf-19" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>20 0.062506169 <a title="53-tfidf-20" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, -0.037), (2, -0.033), (3, 0.086), (4, 0.053), (5, -0.15), (6, 0.1), (7, -0.077), (8, 0.027), (9, -0.225), (10, -0.057), (11, 0.06), (12, -0.019), (13, -0.068), (14, -0.119), (15, 0.016), (16, -0.089), (17, -0.012), (18, 0.002), (19, 0.01), (20, 0.088), (21, -0.122), (22, -0.075), (23, 0.009), (24, -0.0), (25, -0.022), (26, 0.095), (27, -0.057), (28, -0.019), (29, -0.038), (30, -0.035), (31, 0.037), (32, 0.085), (33, 0.079), (34, -0.005), (35, 0.12), (36, -0.084), (37, 0.054), (38, 0.051), (39, 0.027), (40, -0.079), (41, -0.065), (42, -0.012), (43, -0.063), (44, -0.037), (45, 0.018), (46, 0.041), (47, -0.062), (48, 0.101), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95973873 <a title="53-lsi-1" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>2 0.65212673 <a title="53-lsi-2" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>Author: Deli Zhao, Xiaoou Tang</p><p>Abstract: Detecting underlying clusters from large-scale data plays a central role in machine learning research. In this paper, we tackle the problem of clustering complex data of multiple distributions and multiple scales. To this end, we develop an algorithm named Zeta l-links (Zell) which consists of two parts: Zeta merging with a similarity graph and an initial set of small clusters derived from local l-links of samples. More speciﬁcally, we propose to structurize a cluster using cycles in the associated subgraph. A new mathematical tool, Zeta function of a graph, is introduced for the integration of all cycles, leading to a structural descriptor of a cluster in determinantal form. The popularity character of a cluster is conceptualized as the global fusion of variations of such a structural descriptor by means of the leave-one-out strategy in the cluster. Zeta merging proceeds, in the hierarchical agglomerative fashion, according to the maximum incremental popularity among all pairwise clusters. Experiments on toy data clustering, imagery pattern clustering, and image segmentation show the competitive performance of Zell. The 98.1% accuracy, in the sense of the normalized mutual information (NMI), is obtained on the FRGC face data of 16028 samples and 466 facial clusters. 1</p><p>3 0.61606187 <a title="53-lsi-3" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>4 0.61120474 <a title="53-lsi-4" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>Author: Michael Isard, John MacCormick, Kannan Achan</p><p>Abstract: Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a ﬁxed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense. 1</p><p>5 0.57086408 <a title="53-lsi-5" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>Author: Philip Torr, M. P. Kumar</p><p>Abstract: We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-MINCUT based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or treereweighted message passing (TRW), our method is faster as it uses only the efﬁcient st-MINCUT algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efﬁcient algorithms for more complex relaxations.</p><p>6 0.57084632 <a title="53-lsi-6" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>7 0.56776404 <a title="53-lsi-7" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>8 0.56511277 <a title="53-lsi-8" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>9 0.56409711 <a title="53-lsi-9" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>10 0.56265908 <a title="53-lsi-10" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>11 0.54399925 <a title="53-lsi-11" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>12 0.54109937 <a title="53-lsi-12" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>13 0.50716305 <a title="53-lsi-13" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>14 0.50215405 <a title="53-lsi-14" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>15 0.44727743 <a title="53-lsi-15" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>16 0.44418323 <a title="53-lsi-16" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>17 0.435202 <a title="53-lsi-17" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>18 0.3888756 <a title="53-lsi-18" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>19 0.38069376 <a title="53-lsi-19" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>20 0.38055223 <a title="53-lsi-20" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.04), (7, 0.084), (12, 0.046), (15, 0.01), (28, 0.227), (57, 0.047), (59, 0.029), (63, 0.023), (71, 0.014), (77, 0.07), (78, 0.019), (83, 0.052), (89, 0.265)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84661603 <a title="53-lda-1" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>same-paper 2 0.82133222 <a title="53-lda-2" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>3 0.70056409 <a title="53-lda-3" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>4 0.69808471 <a title="53-lda-4" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>5 0.69715905 <a title="53-lda-5" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>6 0.69590008 <a title="53-lda-6" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>7 0.69510204 <a title="53-lda-7" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>8 0.69467747 <a title="53-lda-8" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>9 0.69447833 <a title="53-lda-9" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>10 0.69426984 <a title="53-lda-10" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>11 0.69380355 <a title="53-lda-11" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>12 0.6930089 <a title="53-lda-12" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>13 0.69299865 <a title="53-lda-13" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>14 0.69230664 <a title="53-lda-14" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>15 0.69141948 <a title="53-lda-15" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>16 0.69028372 <a title="53-lda-16" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>17 0.68991268 <a title="53-lda-17" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>18 0.68922263 <a title="53-lda-18" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>19 0.68919563 <a title="53-lda-19" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>20 0.68894374 <a title="53-lda-20" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
