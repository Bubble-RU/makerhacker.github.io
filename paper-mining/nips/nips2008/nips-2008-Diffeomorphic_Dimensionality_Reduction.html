<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 nips-2008-Diffeomorphic Dimensionality Reduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-61" href="#">nips2008-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 nips-2008-Diffeomorphic Dimensionality Reduction</h1>
<br/><p>Source: <a title="nips-2008-61-pdf" href="http://papers.nips.cc/paper/3542-diffeomorphic-dimensionality-reduction.pdf">pdf</a></p><p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>Reference: <a title="nips-2008-61-reference" href="../nips2008_reference/nips-2008-Diffeomorphic_Dimensionality_Reduction_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('diffeomorph', 0.505), ('diffeomap', 0.372), ('ra', 0.299), ('dimend', 0.179), ('map', 0.177), ('tk', 0.165), ('dupu', 0.159), ('grenand', 0.159), ('lawr', 0.153), ('ud', 0.149), ('candel', 0.149), ('josh', 0.116), ('upward', 0.113), ('vowel', 0.106), ('eul', 0.106), ('rb', 0.1), ('downward', 0.094), ('warp', 0.09), ('usp', 0.09), ('cox', 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="61-tfidf-1" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>2 0.13747846 <a title="61-tfidf-2" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>3 0.084873103 <a title="61-tfidf-3" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>4 0.074405998 <a title="61-tfidf-4" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>Author: Novi Quadrianto, Le Song, Alex J. Smola</p><p>Abstract: Object matching is a fundamental operation in data analysis. It typically requires the deﬁnition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for ﬁnding a locally optimal solution. 1</p><p>5 0.072795458 <a title="61-tfidf-5" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>6 0.072162904 <a title="61-tfidf-6" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>7 0.069157936 <a title="61-tfidf-7" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>8 0.067079075 <a title="61-tfidf-8" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>9 0.066886157 <a title="61-tfidf-9" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>10 0.060180403 <a title="61-tfidf-10" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>11 0.060147483 <a title="61-tfidf-11" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>12 0.054243699 <a title="61-tfidf-12" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>13 0.05407628 <a title="61-tfidf-13" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>14 0.053024009 <a title="61-tfidf-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.052528363 <a title="61-tfidf-15" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>16 0.052191507 <a title="61-tfidf-16" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>17 0.052069969 <a title="61-tfidf-17" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>18 0.051736366 <a title="61-tfidf-18" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>19 0.051458508 <a title="61-tfidf-19" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<p>20 0.050975781 <a title="61-tfidf-20" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.171), (1, 0.044), (2, -0.014), (3, 0.026), (4, 0.013), (5, 0.057), (6, 0.029), (7, -0.018), (8, -0.005), (9, 0.084), (10, -0.005), (11, -0.014), (12, -0.009), (13, -0.054), (14, 0.048), (15, -0.032), (16, 0.037), (17, -0.058), (18, -0.015), (19, 0.049), (20, 0.08), (21, 0.026), (22, 0.037), (23, 0.087), (24, -0.037), (25, -0.059), (26, 0.051), (27, -0.006), (28, -0.048), (29, -0.009), (30, 0.157), (31, -0.015), (32, -0.067), (33, -0.02), (34, -0.098), (35, -0.169), (36, 0.109), (37, -0.135), (38, -0.04), (39, -0.017), (40, 0.038), (41, 0.006), (42, -0.047), (43, 0.02), (44, 0.024), (45, 0.026), (46, 0.07), (47, -0.026), (48, 0.007), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89372444 <a title="61-lsi-1" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>2 0.74974591 <a title="61-lsi-2" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>3 0.59655201 <a title="61-lsi-3" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>4 0.56600803 <a title="61-lsi-4" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>Author: Ali Nouri, Michael L. Littman</p><p>Abstract: The essence of exploration is acting to try to decrease uncertainty. We propose a new methodology for representing uncertainty in continuous-state control problems. Our approach, multi-resolution exploration (MRE), uses a hierarchical mapping to identify regions of the state space that would beneﬁt from additional samples. We demonstrate MRE’s broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method. Empirical results show that MRE improves upon state-of-the-art exploration approaches. 1</p><p>5 0.5526405 <a title="61-lsi-5" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and indicate important underlying structure in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA, which is able to avoid the local optima of typical EM learning. Moreover, by introducing a sample-based approximation to exponential family models, it overcomes the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained using a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data. 1</p><p>6 0.53342491 <a title="61-lsi-6" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>7 0.51547754 <a title="61-lsi-7" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>8 0.50482982 <a title="61-lsi-8" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>9 0.4968991 <a title="61-lsi-9" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>10 0.47430626 <a title="61-lsi-10" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>11 0.46770018 <a title="61-lsi-11" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>12 0.46708807 <a title="61-lsi-12" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>13 0.46140125 <a title="61-lsi-13" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>14 0.46108186 <a title="61-lsi-14" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>15 0.4543415 <a title="61-lsi-15" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>16 0.44805577 <a title="61-lsi-16" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>17 0.44465968 <a title="61-lsi-17" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>18 0.43811071 <a title="61-lsi-18" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>19 0.43648636 <a title="61-lsi-19" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>20 0.4318645 <a title="61-lsi-20" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(19, 0.341), (20, 0.022), (30, 0.052), (40, 0.11), (60, 0.032), (63, 0.092), (64, 0.102), (71, 0.092), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68616456 <a title="61-lda-1" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>2 0.64400101 <a title="61-lda-2" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>3 0.49857485 <a title="61-lda-3" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>Author: Kevyn Collins-thompson</p><p>Abstract: Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user’s original query with additional related words. Current algorithms for automatic query expansion can often improve retrieval accuracy on average, but are not robust: that is, they are highly unstable and have poor worst-case performance for individual queries. To address this problem, we introduce a novel formulation of query expansion as a convex optimization problem over a word graph. The model combines initial weights from a baseline feedback algorithm with edge weights based on word similarity, and integrates simple constraints to enforce set-based criteria such as aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and signiﬁcant reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm. Our approach does not assume a particular retrieval model, making it applicable to a broad class of existing expansion algorithms. 1</p><p>4 0.49680188 <a title="61-lda-4" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>5 0.49327141 <a title="61-lda-5" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>Author: Christopher G. Lucas, Thomas L. Griffiths, Fei Xu, Christine Fawcett</p><p>Abstract: Young children demonstrate the ability to make inferences about the preferences of other agents based on their choices. However, there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge. We use a rational model of preference learning, drawing on ideas from economics and computer science, to explain the behavior of children in several recent experiments. Speciﬁcally, we show how a simple econometric model can be extended to capture two- to four-year-olds’ use of statistical information in inferring preferences, and their generalization of these preferences. 1</p><p>6 0.49100125 <a title="61-lda-6" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>7 0.49080867 <a title="61-lda-7" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>8 0.49006873 <a title="61-lda-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.48865193 <a title="61-lda-9" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>10 0.4879455 <a title="61-lda-10" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>11 0.48715076 <a title="61-lda-11" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>12 0.48664057 <a title="61-lda-12" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>13 0.48421782 <a title="61-lda-13" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>14 0.48404929 <a title="61-lda-14" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>15 0.48352718 <a title="61-lda-15" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>16 0.48262554 <a title="61-lda-16" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>17 0.48246256 <a title="61-lda-17" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>18 0.48223251 <a title="61-lda-18" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>19 0.48207998 <a title="61-lda-19" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>20 0.48198169 <a title="61-lda-20" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
