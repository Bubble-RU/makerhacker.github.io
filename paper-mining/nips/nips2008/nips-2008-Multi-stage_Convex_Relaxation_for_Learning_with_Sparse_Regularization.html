<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-145" href="#">nips2008-145</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</h1>
<br/><p>Source: <a title="nips-2008-145-pdf" href="http://papers.nips.cc/paper/3526-multi-stage-convex-relaxation-for-learning-with-sparse-regularization.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>Reference: <a title="nips-2008-145-reference" href="../nips2008_reference/nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study learning formulations with non-convex regularizaton that are natural for sparse linear models. [sent-3, score-0.15]
</p><p>2 A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. [sent-5, score-0.13]
</p><p>3 • Convex relaxation such as L1 -regularization that solves the problem under some conditions. [sent-6, score-0.395]
</p><p>4 In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. [sent-9, score-0.688]
</p><p>5 Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. [sent-10, score-0.425]
</p><p>6 Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. [sent-11, score-0.904]
</p><p>7 We assume that φ(f, y) is convex in f throughout the paper. [sent-25, score-0.236]
</p><p>8 An important target constraint is sparsity, which corresponds to the (non-convex) L0 regularization, deﬁned as w 0 = |{j : wj = 0}| = k. [sent-32, score-0.251]
</p><p>9 If we know the sparsity parameter k for the target vector, then a good learning method is L0 regularization: ˆ w = arg min  w∈Rd  1 n  n  φ(wT xi , yi ) subject to w  0  ≤ k. [sent-33, score-0.295]
</p><p>10 It can be shown that the solution of the L0 regularization problem in (1) achieves good prediction accuracy 1  ¯ if the target function can be approximated by a sparse w. [sent-37, score-0.57]
</p><p>11 Due to the computational difﬁcult, in practice, it is necessary to replace (1) by some easier to solve formulations below: ˆ w = arg min  w∈Rd  1 n  n  φ(wT xi , yi ) + λg(w),  (2)  i=1  where λ > 0 is an appropriately chosen regularization condition. [sent-42, score-0.578]
</p><p>12 We obtain a formulation equivalent to (2) by choosing the regularization function as g(w) = w 0 . [sent-43, score-0.365]
</p><p>13 In particular, by choosing the closest approximation with p = 1, one obtain Lasso, which is the standard convex relaxation formulation for sparse learning. [sent-47, score-0.779]
</p><p>14 With p ∈ (0, 1), the Lp regularization w p is non-convex but continuous. [sent-48, score-0.336]
</p><p>15 Therefore when α → 0, this regularization condition is equivalent to the sparse L0 regularization upto a rescaling of λ. [sent-50, score-0.808]
</p><p>16 It is related to the so-called SCAD regularization in statistics, which is a smoother version. [sent-52, score-0.336]
</p><p>17 We use the simpler capped-L1 regularization because the extra smoothness does not affect our algorithm or theory. [sent-53, score-0.336]
</p><p>18 For a non-convex but smooth regularization condition such as capped-L1 or Lp with p ∈ (0, 1), standard numerical techniques such as gradient descent leads to a local minimum solution. [sent-54, score-0.56]
</p><p>19 Although in practice, such a local minimum solution may outperform the Lasso solution, the lack of theoretical (and practical) performance guarantee prevents the more wide-spread applications of such algorithms. [sent-56, score-0.202]
</p><p>20 As a matter of fact, results with non-convex regularization are difﬁcult to reproduce because different numerical optimization procedures can lead to different local minima. [sent-57, score-0.442]
</p><p>21 Therefore the quality of the solution heavily depend on the numerical procedure used. [sent-58, score-0.194]
</p><p>22 The situation is very difﬁcult for a convex relaxation formulation such as L1 -regularization (Lasso). [sent-59, score-0.66]
</p><p>23 The global optimum can be easily computed using standard convex programming techniques. [sent-60, score-0.33]
</p><p>24 It is known that in practice, 1-norm regularization often leads to sparse solutions (although often suboptimal). [sent-61, score-0.458]
</p><p>25 For example, it is known from the compressed sensing literature that under certain conditions, the solution of L1 relaxation may be equivalent to L0 regularization asymptotically even when noise is present (e. [sent-63, score-0.803]
</p><p>26 If the target is truly sparse, then it was shown in [9] that under some restrictive conditions referred to as irrepresentable conditions, 1-norm regularization solves the feature selection problem. [sent-66, score-0.538]
</p><p>27 Statistically, this means that even though it converges to the true sparse target when n → ∞ (consistency), the rate of convergence can be suboptimal. [sent-69, score-0.162]
</p><p>28 The only way to ﬁx this problem is to employ a non-convex regularization condition that is closer to L0 regularization, such as the capped-L1 regularization. [sent-70, score-0.382]
</p><p>29 Because of the above gap between practice and theory, it is important to study direct solutions of non-convex regularization beyond the standard L1 relaxation. [sent-72, score-0.365]
</p><p>30 Our goal is to design a numerical procedure that leads to a reproducible solution with better theoretical behavior than L1 -regularization. [sent-73, score-0.303]
</p><p>31 Speciﬁcally, we consider a general multi-stage convex relaxation method for solving learning formulations with non-convex regularization. [sent-75, score-0.718]
</p><p>32 In this scheme, concave duality is used to construct a sequence of convex relaxations that give better and better approximations to the original non-convex problem. [sent-76, score-0.361]
</p><p>33 Moreover, using the capped-L1 regularization, we show that after only two stages, the solution gives better statistical performance than standard Lasso when the target is approximately sparse. [sent-77, score-0.173]
</p><p>34 In essence, this paper establishes a performance guarantee for non-convex formulations using a multi-stage convex relaxation approach that is more sophisticated than the standard one-stage convex relaxation (which is the standard approach com2  monly studied in the current literature). [sent-78, score-1.409]
</p><p>35 2  Concave Duality  Given a continuous regularization function g(w) in (2) which may be non-convex, we are interested in rewriting it using concave duality. [sent-80, score-0.42]
</p><p>36 However, we assume that there exists a function gh (u) deﬁned on Ω such that ¯ g(w) = gh (h(w)) holds. [sent-83, score-0.686]
</p><p>37 ¯ We assume that we can ﬁnd h so that the function gh (u) is a concave function of u on Ω. [sent-84, score-0.427]
</p><p>38 Under ¯ this assumption, we can rewrite the regularization function g(w) as: g(w) = inf  v∈Rd  ∗ vT h(w) + gh (v)  (3)  ∗ using concave duality [6]. [sent-85, score-0.84]
</p><p>39 In this case, gh (v) is the concave dual of gh (u) given below ¯ ∗ gh (v) = inf −vT u + gh (u) . [sent-86, score-1.492]
</p><p>40 ¯ u∈Ω  Moreover, it is well-known that the minimum of the right hand side of (3) is achieved at ˆ v=  ¯ u gh (u)|u=h(w) . [sent-87, score-0.383]
</p><p>41 For illustration, we include two example non-convex sparse regularization conditions discussed in the introduction. [sent-89, score-0.466]
</p><p>42 d  p Lp regularization We consider the regularization condition g(w) = j=1 |wj | for some q q ∗ p ∈ (0, 1). [sent-90, score-0.718]
</p><p>43 , |wd | ] and gh (v) = p/(p−q) c(p, q) j vj deﬁned on the domain {v : vj ≥ 0}, where c(p, q) = (q − p)pp/(q−p) q q/(p−q) . [sent-94, score-0.641]
</p><p>44 The solution in (4) is given by  Capped-L1 regularization We consider the regularization condition g(w) = ∗ gh (v)  d j=1  min(|wj |, α). [sent-97, score-1.133]
</p><p>45 , |wd |] and = α(1 − vj )I(vj ∈ [0, 1]) deﬁned on the domain {v : vj ≥ 0}, where I(·) is the set indicator function. [sent-101, score-0.298]
</p><p>46 The solution in (4) is ˆ given by vj = I(|wj | ≤ α). [sent-102, score-0.221]
</p><p>47 3  Multi-stage Convex Relaxation  We consider a general procedure for solving (2) with convex loss and non-convex regularization g(w). [sent-103, score-0.709]
</p><p>48 Let h(w) = j hj (w) be a convex relaxation of g(w) that dominates g(w) (for example, it can be the smallest convex upperbound (i. [sent-104, score-0.903]
</p><p>49 A simple convex relaxation of (2) becomes   n d 1 ˆ w = arg min  φ(wT xi , yi ) + λ hj (w) . [sent-107, score-0.849]
</p><p>50 (5) w∈Rd n i=1 j=1 This simple relaxation can yield a solution that is not close to the solution of (2). [sent-108, score-0.539]
</p><p>51 Now, with this new representation, we can rewrite (2) as ˆ ˆ [w, v] = arg min  w,v∈Rd  1 n  n ∗ φ(wT xi , yi ) + λvT h(w) + λgh (v), ,  (6)  i=1  ˆ This is clearly equivalent to (2) because of (3). [sent-110, score-0.182]
</p><p>52 , 1], then the above formulation can lead to a reﬁned convex problem in w that is a better convex relaxation than (5). [sent-114, score-0.925]
</p><p>53 3  Our numerical procedure exploits the above fact, which tries to improve the estimation of vj over the initial choice of vj = 1 in (5) using an iterative algorithm. [sent-115, score-0.468]
</p><p>54 This can be done using an alternating optimization procedure, which repeatedly applies the following two steps: • First we optimize w with v ﬁxed: this is a convex problem in w with appropriately chosen h(w). [sent-116, score-0.263]
</p><p>55 By repeatedly reﬁning the parameter v, we can potentially obtain better and better convex relaxation, leading to a solution superior to that of the initial convex relaxation. [sent-120, score-0.609]
</p><p>56 Note that using the Lp and capped-L1 regularization conditions in Section 2, this procedure lead to more speciﬁc multi-stage convex relaxation algorithms. [sent-121, score-1.114]
</p><p>57 In contrast, the simple one-stage L1 relaxation is known to perform reasonably well under certain assumptions. [sent-127, score-0.395]
</p><p>58 Therefore unless we can develop a theory to show the effectiveness of the multi-stage procedure in Figure 1, our proposal is mere yet another local minimum ﬁnding scheme that may potentially stuck into a bad local solution. [sent-128, score-0.31]
</p><p>59 In particular, if the target function is sparse, then the performance of the solution after merely twostages of our procedure is superior to that of Lasso. [sent-131, score-0.26]
</p><p>60 Since the analysis is rather complicated, we focus on the least squares loss only, and only for the solution after two-stages of the algorithm. [sent-133, score-0.143]
</p><p>61 For a complete theory, the following questions are worth asking: • Under what conditions, the global solution with non-convex penalty is statistically better than the (one-stage) convex relaxation solution? [sent-134, score-0.766]
</p><p>62 • Under what conditions, there is only one local minimum solution close to the solution of the initial convex relaxation, and it is also the global optimum? [sent-136, score-0.48]
</p><p>63 The second question answers whether we can effectively solve the resulting non-convex problem using multistage convex relaxation. [sent-139, score-0.265]
</p><p>64 The combination of the two questions leads to a satisfactory theoretical answer to the effectiveness of the multi-stage procedure. [sent-140, score-0.187]
</p><p>65 In the following, instead of trying to answer the above questions separately, we provide a uniﬁed ﬁnite sample analysis for the procedure that directly addresses the combined effect of the two questions. [sent-142, score-0.141]
</p><p>66 The result is adopted 4  from [8], which justiﬁes the multi-stage convex relaxation approach by showing that the two-stage procedure using capped-L1 regularization can lead to better generalization than the standard one stage L1 regularization. [sent-143, score-1.228]
</p><p>67 The procedure we shall analyze, which is a special case of the multi-stage algorithm in Figure 1 with capped-L1 regularization and only two stages, is described in Figure 2. [sent-144, score-0.449]
</p><p>68 The result is reproducible when the solution of the ﬁrst stage is unique because it involves two well-deﬁned convex programming problems. [sent-146, score-0.482]
</p><p>69 Note that it is described with least squares loss only because our analysis assumes least squares loss: a more general analysis for other loss functions is possible but would lead to extra complications that are not central to our interests. [sent-147, score-0.171]
</p><p>70 , (xn , yn ) ˆ Output: weight vector w ˆ Stage 1: Compute w by solving the L1 penalization problem: ˆ w = arg min  w∈Rd  1 n  n  (wT xi − yi )2 + λ w  1  . [sent-151, score-0.32]
</p><p>71 i=1  Stage 2: Solving the following selective L1 penalization problem:  n 1 ˆ w = arg min  (wT xi − yi )2 + λ w∈Rd n i=1   |wj | . [sent-152, score-0.244]
</p><p>72 ˆ j:|wj |≤α  Figure 2: Two-stage capped-L1 Regularization This particular two-stage procedure also has an intuitive interpretation (besides treating it as a special case of multi-stage convex relaxation). [sent-153, score-0.314]
</p><p>73 We shall refer to the feature components corresponding to the large weights as relevant features, and the feature components smaller the cut-off threshold α as irrelevant features. [sent-154, score-0.186]
</p><p>74 We observe that as an estimation method, L1 regularization has two important properties: shrink estimated weights corresponding to irrelevant features toward zero; shrink estimated weights corresponding to relevant features toward zero. [sent-155, score-0.649]
</p><p>75 In fact, we should avoid shrinking the weights corresponding to the relevant features if we can identify these features. [sent-157, score-0.168]
</p><p>76 This is why the standard L1 regularization may have suboptimal performance. [sent-158, score-0.41]
</p><p>77 However, after the ﬁrst stage of L1 regularization, we can identify the relevant features by picking the components corresponding to the largest weights; in the second stage of L1 regularization, we do not have to penalize the features selected in the ﬁrst stage, as in Figure 2. [sent-159, score-0.493]
</p><p>78 Their idea differs from our proposal in that in the second ˆ stage, the weight coefﬁcients wj are forced to be zero when j ∈ supp0 (w). [sent-161, score-0.211]
</p><p>79 It was pointed out / ˆ in [5] that if supp0 (w) can exactly identify all non-zero components of the target vector, then in the second stage, the relaxed Lasso can asymptotically remove the bias in the ﬁrst stage Lasso. [sent-162, score-0.318]
</p><p>80 On the contrary, the two-stage penalization procedure in Figure 2, which is based on the capped-L1 regularization, does not require that all relevant features are identiﬁed. [sent-165, score-0.252]
</p><p>81 , wd ] ∈ Rd and α ≥ 0, we deﬁne the set of relevant features with threshold α as: suppα (w) = {j : |wj | > α}. [sent-171, score-0.167]
</p><p>82 1 can be signiﬁcantly better than that of the standard √ ¯ Lasso result if the sparse target satisﬁes δk (w) kλ and k − q k. [sent-206, score-0.191]
</p><p>83 That is, when ¯ the target w can be decompose as a sparse vector with large coefﬁcients plus another (less sparse) ¯ vector with small coefﬁcients. [sent-210, score-0.162]
</p><p>84 In the extreme case when q = k = |supp0 (w)| (that is, all nonzero ¯ ˆ ¯ components of w are large), we obtain w −w 2 = O( k ln(1/η)/n) for the two-stage procedure, ˆ ¯ which is superior to the standard one-stage Lasso bound w − w 2 = O( k ln(d/η)/n). [sent-211, score-0.189]
</p><p>85 We shall also compare it to the two-stage Lp regularization method with p = 0. [sent-215, score-0.371]
</p><p>86 A truly sparse target β, is generated with k i,j 6  q q  6  q  q q  4  q q  3  parameter estimation error  0. [sent-227, score-0.241]
</p><p>87 The average training error and 2-norm parameter estimation error are reported in Figure 3. [sent-237, score-0.124]
</p><p>88 We compare the performance of the two-stage method with different q versus the regularization parameter λ. [sent-238, score-0.399]
</p><p>89 Compared to the standard Lasso (which corresponds to q = 0), substantially smaller estimation error is achieved with q = 3 for Capped-L1 regularization and with p = 0. [sent-240, score-0.409]
</p><p>90 This shows that the multi-stage convex relaxation approach is effective. [sent-242, score-0.631]
</p><p>91 00  lambda  Figure 3: Performance of multi-stage convex relaxation on simulation data. [sent-259, score-0.761]
</p><p>92 Left: average training squared error versus λ; Right: parameter estimation error versus λ. [sent-260, score-0.294]
</p><p>93 This is the housing data for 506 census tracts of Boston from the 1970 census, available from the UCI Machine Learning Database Repository: http://archive. [sent-263, score-0.21]
</p><p>94 Each census tract is a datapoint, with 13 features (we add a constant offset on e as the 14th feature), and the desired output is the housing price. [sent-267, score-0.275]
</p><p>95 We perform the experiments 100 times, and report training and test squared error versus the regularization parameter λ for different q. [sent-269, score-0.523]
</p><p>96 1 and the discussion thereafter, since d becomes large, the multi-stage convex relaxation approach with capped-L1 regularization (q > 0) has signiﬁcant advantage over the standard Lasso (q = 0). [sent-278, score-0.996]
</p><p>97 0  Figure 4: Performance of multi-stage convex relaxation on the original Boston Housing data. [sent-313, score-0.631]
</p><p>98 Left: average training squared error versus λ; Right: test squared error versus λ. [sent-314, score-0.338]
</p><p>99 0  lambda  Figure 5: Performance of multi-stage convex relaxation on the modiﬁed Boston Housing data. [sent-328, score-0.761]
</p><p>100 Left: average training squared error versus λ; Right: test squared error versus λ. [sent-329, score-0.338]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('relaxation', 0.395), ('lasso', 0.357), ('gh', 0.343), ('regularization', 0.336), ('convex', 0.236), ('wj', 0.179), ('lp', 0.159), ('vj', 0.149), ('housing', 0.13), ('lambda', 0.13), ('stage', 0.125), ('rd', 0.111), ('wt', 0.109), ('boston', 0.107), ('sparse', 0.09), ('supp', 0.087), ('concave', 0.084), ('census', 0.08), ('procedure', 0.078), ('ln', 0.074), ('target', 0.072), ('solution', 0.072), ('features', 0.065), ('effectiveness', 0.064), ('versus', 0.063), ('penalization', 0.062), ('formulations', 0.06), ('wd', 0.055), ('relaxed', 0.055), ('yi', 0.055), ('tuning', 0.054), ('arg', 0.052), ('reproducible', 0.049), ('yn', 0.049), ('tries', 0.048), ('relevant', 0.047), ('dantzig', 0.046), ('eyi', 0.046), ('vt', 0.046), ('nonzero', 0.046), ('condition', 0.046), ('suboptimal', 0.045), ('error', 0.044), ('squared', 0.044), ('numerical', 0.044), ('min', 0.042), ('sparsity', 0.041), ('vladimir', 0.041), ('selector', 0.041), ('duality', 0.041), ('minimum', 0.04), ('conditions', 0.04), ('remedy', 0.04), ('shrink', 0.04), ('squares', 0.039), ('optimum', 0.038), ('superior', 0.038), ('components', 0.038), ('bound', 0.038), ('inf', 0.036), ('training', 0.036), ('questions', 0.036), ('tong', 0.036), ('hj', 0.036), ('shall', 0.035), ('truly', 0.035), ('coef', 0.034), ('cients', 0.034), ('uj', 0.034), ('xn', 0.034), ('xi', 0.033), ('oracle', 0.033), ('local', 0.033), ('proposal', 0.032), ('leads', 0.032), ('loss', 0.032), ('theorem', 0.031), ('scheme', 0.03), ('moreover', 0.03), ('princeton', 0.03), ('adaptive', 0.03), ('standard', 0.029), ('formulation', 0.029), ('guarantee', 0.029), ('lead', 0.029), ('answers', 0.029), ('identify', 0.028), ('uci', 0.028), ('stages', 0.028), ('referred', 0.028), ('weights', 0.028), ('theoretical', 0.028), ('repeatedly', 0.027), ('global', 0.027), ('solving', 0.027), ('answer', 0.027), ('annales', 0.027), ('florentina', 0.027), ('henri', 0.027), ('irrepresentable', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="145-tfidf-1" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>2 0.24891238 <a title="145-tfidf-2" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>3 0.24623206 <a title="145-tfidf-3" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>Author: Pierre Garrigues, Laurent E. Ghaoui</p><p>Abstract: It has been shown that the problem of 1 -penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efﬁcient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point. 1</p><p>4 0.21260934 <a title="145-tfidf-4" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>5 0.19865537 <a title="145-tfidf-5" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>6 0.17078024 <a title="145-tfidf-6" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>7 0.16978638 <a title="145-tfidf-7" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>8 0.16947286 <a title="145-tfidf-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.14950949 <a title="145-tfidf-9" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>10 0.14399518 <a title="145-tfidf-10" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>11 0.14209321 <a title="145-tfidf-11" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>12 0.14195277 <a title="145-tfidf-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.12946147 <a title="145-tfidf-13" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>14 0.12909667 <a title="145-tfidf-14" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>15 0.10334872 <a title="145-tfidf-15" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>16 0.10323198 <a title="145-tfidf-16" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>17 0.099545762 <a title="145-tfidf-17" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>18 0.098937079 <a title="145-tfidf-18" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>19 0.094873235 <a title="145-tfidf-19" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>20 0.089092664 <a title="145-tfidf-20" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.274), (1, -0.069), (2, -0.253), (3, 0.161), (4, 0.061), (5, 0.074), (6, -0.195), (7, -0.041), (8, -0.143), (9, 0.088), (10, -0.186), (11, -0.052), (12, -0.138), (13, -0.025), (14, 0.068), (15, -0.009), (16, -0.039), (17, -0.061), (18, -0.04), (19, 0.059), (20, -0.105), (21, 0.003), (22, -0.085), (23, 0.091), (24, -0.004), (25, -0.052), (26, 0.088), (27, 0.043), (28, -0.125), (29, 0.022), (30, 0.002), (31, -0.023), (32, -0.002), (33, 0.078), (34, -0.024), (35, -0.026), (36, -0.001), (37, -0.0), (38, 0.025), (39, 0.05), (40, -0.014), (41, 0.018), (42, -0.059), (43, -0.092), (44, 0.034), (45, 0.035), (46, -0.033), (47, -0.022), (48, 0.024), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96979338 <a title="145-lsi-1" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>2 0.82416368 <a title="145-lsi-2" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>3 0.76707375 <a title="145-lsi-3" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>4 0.71942157 <a title="145-lsi-4" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>Author: Pierre Garrigues, Laurent E. Ghaoui</p><p>Abstract: It has been shown that the problem of 1 -penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efﬁcient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point. 1</p><p>5 0.70838827 <a title="145-lsi-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.62942892 <a title="145-lsi-6" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>7 0.62162441 <a title="145-lsi-7" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>8 0.614847 <a title="145-lsi-8" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>9 0.59540021 <a title="145-lsi-9" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>10 0.56277424 <a title="145-lsi-10" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>11 0.56093079 <a title="145-lsi-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.54511505 <a title="145-lsi-12" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>13 0.52227104 <a title="145-lsi-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.51597935 <a title="145-lsi-14" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>15 0.51588005 <a title="145-lsi-15" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>16 0.49994424 <a title="145-lsi-16" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>17 0.48777539 <a title="145-lsi-17" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>18 0.47302336 <a title="145-lsi-18" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>19 0.46291533 <a title="145-lsi-19" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>20 0.43397948 <a title="145-lsi-20" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.15), (7, 0.101), (12, 0.028), (16, 0.261), (28, 0.154), (57, 0.051), (59, 0.034), (63, 0.029), (71, 0.016), (77, 0.038), (78, 0.011), (83, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81642479 <a title="145-lda-1" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>same-paper 2 0.81357074 <a title="145-lda-2" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><p>3 0.75282407 <a title="145-lda-3" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>Author: Paul L. Ruvolo, Ian Fasel, Javier R. Movellan</p><p>Abstract: Many popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA), use heuristic-based “controllers” that modulate the behavior of the optimizer during the optimization process. For example, in the LMA a damping parameter λ is dynamically modiﬁed based on a set of rules that were developed using heuristic arguments. Reinforcement learning (RL) is a machine learning approach to learn optimal controllers from examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms. Improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems. For example the LMA algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame. Here we show that a popular modern reinforcement learning technique using a very simple state space can dramatically improve the performance of general purpose optimizers, like the LMA. Surprisingly the controllers learned for a particular domain also work well in very different optimization domains. For example we used RL methods to train a new controller for the damping parameter of the LMA. This controller was trained on a collection of classic, relatively small, non-linear regression problems. The modiﬁed LMA performed better than the standard LMA on these problems. This controller also dramatically outperformed the standard LMA on a difﬁcult computer vision problem for which it had not been trained. Thus the controller appeared to have extracted control rules that were not just domain speciﬁc but generalized across a range of optimization domains. 1</p><p>4 0.69869798 <a title="145-lda-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.69701135 <a title="145-lda-5" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>6 0.66901433 <a title="145-lda-6" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>7 0.66557115 <a title="145-lda-7" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>8 0.66285211 <a title="145-lda-8" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>9 0.65703857 <a title="145-lda-9" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>10 0.65660757 <a title="145-lda-10" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>11 0.65616065 <a title="145-lda-11" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>12 0.65391821 <a title="145-lda-12" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>13 0.6538747 <a title="145-lda-13" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>14 0.65248913 <a title="145-lda-14" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>15 0.65164679 <a title="145-lda-15" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>16 0.65144318 <a title="145-lda-16" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>17 0.64964074 <a title="145-lda-17" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>18 0.64920968 <a title="145-lda-18" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>19 0.64813662 <a title="145-lda-19" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>20 0.64799863 <a title="145-lda-20" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
