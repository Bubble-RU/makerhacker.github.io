<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-43" href="#">nips2008-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2008-43-pdf" href="http://papers.nips.cc/paper/3610-cell-assemblies-in-large-sparse-inhibitory-networks-of-biologically-realistic-spiking-neurons.pdf">pdf</a></p><p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>Reference: <a title="nips-2008-43-reference" href="../nips2008_reference/nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. [sent-5, score-0.537]
</p><p>2 Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. [sent-6, score-0.889]
</p><p>3 We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. [sent-8, score-1.032]
</p><p>4 1  Introduction  Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2], but how such correlated activity emerges in neural microcircuits is not well understood. [sent-11, score-0.608]
</p><p>5 Here we address the question of how coherent assemblies can emerge in large inhibitory neural networks and what this implies for the structure and function of one such network, the striatum. [sent-12, score-0.695]
</p><p>6 Furthermore dimensionality reduction of network dynamics revealed functional states deﬁned by cell assemblies that alternated their activity and displayed spatiotemporal pattern generation. [sent-16, score-0.988]
</p><p>7 Recurrent synchronous activity traveled from one cell assembly to the other often returning to the original assembly; suggesting a robust structure. [sent-17, score-0.593]
</p><p>8 Moreover the authors showed that while each cell assembly comprised different cells, a small set of neurons was shared by different assemblies. [sent-19, score-0.606]
</p><p>9 Although the striatum is an inhibitory network composed of GABAergic projection neurons, similar types of cell assemblies have also been observed in excitatory networks such as the hippocampus. [sent-20, score-1.284]
</p><p>10 They found discrete heterogeneous network states deﬁned by active cell ensembles which were stable against external perturbations through synaptic activity. [sent-23, score-0.484]
</p><p>11 We here address the important question of how such assemblies can appear deterministically in biologically realistic cell networks. [sent-27, score-0.906]
</p><p>12 We focus our modeling on the inhibitory network of the striatum, however similar models can be proposed for networks such as CA3 if the cell assembly activity is controlled by the inhibitory CA3 interneurons. [sent-28, score-1.078]
</p><p>13 Here we extend this work focusing on the formation of burst ﬁring cell assemblies The striatum is composed of GABAergic projection neurons with fairly sparse asymmetric inhibitory collaterals which seem quite randomly structured and that receive an excitatory cortical projection[6]. [sent-31, score-1.445]
</p><p>14 This implies that each MSN is actively inhibited by about 50−150 cortically excited cells in general. [sent-34, score-0.52]
</p><p>15 We show by numerical computer simulation that very general random networks of biologically realistic neurons coupled with inhibitory Rall-type synapses[7] and individually driven by excitatory input can show switching assembly dynamics. [sent-36, score-0.773]
</p><p>16 We commonly observed a switching bursting regime in networks with sparse to intermediate connectivity when the level of network inhibition approximately balanced the external excitation so that the individual cells were near a bifurcation point. [sent-37, score-1.166]
</p><p>17 In our simulations, cells and assemblies slowly and spontaneously switch between a depolarized ﬁring state and a more hyperpolarized quiescent state. [sent-38, score-1.058]
</p><p>18 The proportion of switching cells varies with the network connectivity, peaking at low connection probability for ﬁxed total inhibition. [sent-39, score-0.707]
</p><p>19 The sorted cross correlation matrix of the ﬁring rates time series for switching cells shows a fascinating multiscale clustered structure of cell assemblies similar to observations in[1, 2]. [sent-40, score-1.457]
</p><p>20 The origin of the deterministic switching dynamics in our model is related to the principle of winnerless competition (WLC) which has previously been observed by Rabinovich and coworkers[3] in small inhibitory networks with closed loops based on heteroclinic cycles connecting saddle points. [sent-41, score-0.446]
</p><p>21 Networks produce this switching mode of dynamical activity when lateral inhibitory connections are strongly non-symmetric. [sent-43, score-0.496]
</p><p>22 Our study suggests attractor switching may be ubiquitous in biologically realistic large sparse random inhibitory networks. [sent-49, score-0.533]
</p><p>23 2  Model  The network is composed of biologically realistic model neurons in the vicinity of a bifurcation from a stable ﬁxed point to spiking limit cycle dynamical behaviour. [sent-50, score-0.461]
</p><p>24 To describe the cells we use the IN a,p + Ik model described in Izhikevich[13] although any model near such a bifurcation would be appropriate. [sent-51, score-0.534]
</p><p>25 These values of excitatory input current mean that all cells would be on limit cycles and ﬁring with low rates if the network inhibition were not present. [sent-68, score-0.648]
</p><p>26 In fact the inhibitory network may cause some cells to become quiescent by reducing the total input current to below the bifurcation point. [sent-69, score-0.924]
</p><p>27 In the simulations reported here we use random networks where cells i and j are connected with probability p, and there are no self-connections, Xii = 0. [sent-81, score-0.505]
</p><p>28 ksyn is a parameter which is rescaled by the connection probability p so that average total inhibition on each cell is constant independent of p. [sent-82, score-0.485]
</p><p>29 3  Results  Figure 1(a) shows a time series segment of membrane potentials Vi (t) for some randomly selected cells from an N = 100 cell network. [sent-84, score-0.887]
</p><p>30 Since we have set the unit parameters so that all units are near the bifurcation point even weak network inhibition is able to cause the cells to become quiescent at times. [sent-89, score-0.825]
</p><p>31 The parameter settings are biologically realistic[13] and MSN cells are known to show irregular quiescent and ﬁring states in vivo[14]. [sent-90, score-0.604]
</p><p>32 To make this plot the cells have been ordered by the k-means algorithm with ﬁve clusters (see below). [sent-95, score-0.527]
</p><p>33 The cells are coloured according to the cluster assigned to them by the algorithm. [sent-96, score-0.527]
</p><p>34 During the periodic window, most cells are silent however some cells ﬁre continuously 3  Figure 1: (a) Membrane potential Vi (t) time series segment for a few cells from a N = 100 cell network simulation with 20 connections per cell. [sent-97, score-1.879]
</p><p>35 (b) Spike raster plot from an N = 100 cell network simulation with 20 connections per cell. [sent-99, score-0.594]
</p><p>36 Each line is a different cell and the 71 cells which ﬁre at least one spike during the period shown are plotted. [sent-100, score-0.92]
</p><p>37 at ﬁxed frequency and some cells ﬁre in periodic bursts. [sent-102, score-0.467]
</p><p>38 In fact the cells which ﬁre in bursts have been separated into two clusters, as can be seen in Fig. [sent-103, score-0.464]
</p><p>39 Cell assemblies can also be seen in the chaotic regions. [sent-106, score-0.497]
</p><p>40 The cells in the black cluster ﬁre together in a burst around t = 17500 while the cells in the orange cluster ﬁre a burst together around t = 16000. [sent-107, score-1.174]
</p><p>41 2(a) shows another example of a spike raster plot from a N = 100 cell network simulation where again the cells have been ordered by the k-means algorithm with ﬁve clusters. [sent-109, score-1.127]
</p><p>42 Due to the presence of attractor switching where cell assemblies can burst in antiphase we can expect the appearance of strongly positively and strongly negatively correlated cell pairs. [sent-112, score-1.62]
</p><p>43 Each cell is assigned to one of a ﬁxed number of clusters and the cells indices are reordered accordingly. [sent-115, score-0.852]
</p><p>44 Within an assembly cells are positively correlated, while cells in different assemblies often show negative correlation. [sent-119, score-1.505]
</p><p>45 A patch-work of switching cell assembly clusters can be seen in the spike raster plot and corresponding cross-correlation matrix shown in Figs. [sent-121, score-0.966]
</p><p>46 2(c) and (d) respectively for a N = 500 cell system where the cells have been ordered by the k-means algorithm, now with 30 clusters. [sent-122, score-0.855]
</p><p>47 Any particular assembly can seem to be burst ﬁring periodically for a spell before becoming quiescent for long spells. [sent-123, score-0.445]
</p><p>48 Other cell assemblies burst very occasionally for no apparent reason. [sent-124, score-0.93]
</p><p>49 2(b) and (d) that although some cell assemblies are positively correlated with each other, they have different relationships to other cell assemblies, and therefore cannot be combined into a single larger assemblies. [sent-126, score-1.269]
</p><p>50 2(c) reveals many cells switching between a ﬁring state and quiescent state. [sent-128, score-0.749]
</p><p>51 1(b) are three ISI distributions for three 500 cell network simulations in the sparse to intermediate regime with 30 connections per cell. [sent-132, score-0.638]
</p><p>52 It is this distribution which produces the appearance of the complex identitytemporal patterns shown in the 500 cell time series ﬁgure in Fig. [sent-135, score-0.451]
</p><p>53 Power-law distributions are characteristic of systems showing chaotic 4  Figure 2: (a) Spike raster plot from all 69 cells in a 100 cell network with 20 connections per cell which ﬁre at least one spike. [sent-137, score-1.451]
</p><p>54 The cells are ordered by k-means with ﬁve clusters and coloured according to their assigned cluster. [sent-138, score-0.577]
</p><p>55 The cells are ordered by the k-means algorithm the same way as (a). [sent-140, score-0.477]
</p><p>56 (c) Spike raster plot from an N = 500 cell sparse network with 6 connections per cell. [sent-143, score-0.62]
</p><p>57 The 379 cells which ﬁre at least one spike during the period shown are plotted. [sent-144, score-0.542]
</p><p>58 The cells are ordered by k-means with 30 clusters. [sent-145, score-0.477]
</p><p>59 Plenz and Thiagarajan[16] discuss cortical cell assemblies in the framework of scale free avalanches which are associated with intermittency[17]. [sent-151, score-0.85]
</p><p>60 By combining the spikes of cells in a cluster into a “cluster spike train” preserving each spikes’ timing we can study the ISIs of cluster spike time series. [sent-155, score-0.766]
</p><p>61 3(a) show the cluster ISI distribution after cells have been associated to clusters with the k-means algorithm with 10 clusters. [sent-159, score-0.527]
</p><p>62 The cluster ISI distribution, like the individual cell ISI distribution, also shows a powerlaw over several orders of magnitude. [sent-160, score-0.431]
</p><p>63 The slope of the power law is greater than the individual cell result and the cut-off is lower as would be expected when spike trains are combined. [sent-162, score-0.496]
</p><p>64 To demonstrate this we perform a bootstrap type test where rather than making each cluster spike train 5  Figure 3: (a) Green, brown, blue: Three cell cumulative ISI distributions from 500 cell network simulations with 30 connections per cell, all cells combined. [sent-164, score-1.525]
</p><p>65 Black: ISI distribution for clusters formed by k-means algorithm corresponding to green single cell distribution. [sent-168, score-0.464]
</p><p>66 Red: ISI distribution for clusters formed from cells randomly corresponding to green single cell distribution. [sent-171, score-0.888]
</p><p>67 Red: Proportion of cells which ﬁre at least one spike during the period. [sent-175, score-0.542]
</p><p>68 Black: Average absolute cross-correlation |Cij | between all cells in network calculated from rate time series constructed from counting spikes in moving window of size 2000 msec. [sent-177, score-0.548]
</p><p>69 Green: Coefﬁcient of variation CV of ISI distribution averaged across all cells in network rescaled by 1/3. [sent-178, score-0.528]
</p><p>70 from the cells associated to the cluster we perform the same k-means clustering to obtain correct cluster sizes but then scramble the cell indices, associating the cells to the clusters randomly. [sent-179, score-1.382]
</p><p>71 How does the formation of switching assembly dynamics depend on the network connectivity? [sent-188, score-0.505]
</p><p>72 As described above the synaptic efﬁcacy is rescaled by the connection probability so the total inhibition on each cell is ﬁxed and therefore effects arise purely from variations in connectivity. [sent-190, score-0.513]
</p><p>73 3(b) (red) shows the proportion of cells which ﬁre at least one spike versus average connections per cell for 500 cell network simulations. [sent-192, score-1.476]
</p><p>74 This quantity shows a transition around 5 connections per cell to state where almost all the network is burst ﬁring and then decays off to a plateau region at higher connectivity. [sent-193, score-0.654]
</p><p>75 Below the transition a large proportion of cells are not inhibited and ﬁring periodically due to the excitatory cortical drive, while another large proportion are not ﬁring at all, inhibited by the periodically ﬁring group. [sent-197, score-0.779]
</p><p>76 At high connectivities however most cells receive similar inhibition levels which leaves a certain proportion ﬁring. [sent-198, score-0.581]
</p><p>77 3(b) (green) shows the coefﬁcient of variation CV of the single cell ISI distribution averaged across all cells and rescaled by 1/3. [sent-200, score-0.828]
</p><p>78 At high connectivities it is also low and inspection of spike time series shows all cells ﬁring with fairly regular ISIs. [sent-204, score-0.626]
</p><p>79 In intermediate regions however this quantity can become very large reﬂecting long periods of quiescent interrupted by high frequency bursting, as also reﬂected in the single cell ISI distributions in Fig. [sent-205, score-0.577]
</p><p>80 This quantity also shows the low connectivity transition but peaks around 200 connections per cell, where many cells are substantially cross-correlated (both positively and negatively). [sent-209, score-0.582]
</p><p>81 3(b) therefore displays an interesting regime between about 50 and 200 connections per cell where many cells are burst ﬁring with long periods of quiescence but have substantial cross-correlation. [sent-212, score-1.079]
</p><p>82 It is in this regime that spike time series often show the complex identity-temporal patterns and switching cell assemblies exempliﬁed in Fig. [sent-213, score-1.208]
</p><p>83 4  Discussion  We have shown that inhibitory networks of biologically realistic spiking neurons obeying deterministic dynamical equations with sparse to intermediate connectivity can show bursting dynamics, complex identity-temporal patterns and form cell assemblies. [sent-215, score-1.064]
</p><p>84 The cells should be near a bifurcation point where even weak inhibition can cause them to become quiescent. [sent-216, score-0.615]
</p><p>85 This slow change in inhibition allows the bursting assembly dynamics since presynaptic cells do not instantly inhibit postsynaptic cells, but inhibition builds up gradually, allowing the formation of assemblies which eventually becoming strong enough to quench the postsynaptic cell activity. [sent-219, score-1.863]
</p><p>86 At low connectivities sets of cells with sufﬁciently few and/or sufﬁciently weak connections between them will exist and these cells will ﬁre together as an assembly due to the cortical excitation, if the rest of the network which inhibits them is sufﬁciently quiescent for a period. [sent-220, score-1.365]
</p><p>87 Such a set of weakly connected cells can be inhibited by another such set of weakly connected cells if each member of the ﬁrst set is inhibited by a sufﬁcient number of cells of the second set. [sent-221, score-1.404]
</p><p>88 These assemblies can exist in asymmetric closed loops which slowly switch active set. [sent-223, score-0.476]
</p><p>89 Multiple “frustrated” interlocking loops can exist where the slow switching of one loop will interfere with the dynamical switching of another loop; only when inhibition on one member set is removed will the loop be able to continue slow switching, producing a type of neural computation. [sent-224, score-0.464]
</p><p>90 [1] who show some cells ﬁring with only one assembly and other cells ﬁring in multiple assemblies. [sent-228, score-1.025]
</p><p>91 These cross-coupled switching assemblies with partially shared members produce complex multiple timescale dynamics and identity-temporal patterning for appropriate connectivities. [sent-229, score-0.708]
</p><p>92 Switching assemblies are most likely to be observed in networks of sparse to intermediate connectivities. [sent-230, score-0.545]
</p><p>93 2(a) and (c), indicate that cell assemblies switch non-randomly in sequence due to the deterministic attractor switching. [sent-234, score-0.928]
</p><p>94 Our time series and the crosscorrelation matrices demonstrate that while most cells ﬁre with only one particular assembly, some cells are shared between assemblies, as observed by Carrillo-Reid et al. [sent-238, score-0.894]
</p><p>95 We have shown that cells form assemblies of positively correlated cells and assemblies are negatively correlated with each other, in accordance with the similarity matrix results shown in Sasaki et al. [sent-240, score-1.836]
</p><p>96 Very interestingly cell assemblies are predominantly found in a connectivity regime appropriate for the striatum[6], where each cell is likely to be connected to about 100 cortically excited cells, suggesting the striatum may have adapted to be in this regime. [sent-242, score-1.41]
</p><p>97 Since these cells are driven by different levels of cortical excitation, the synchronization can only result from an entrainment produced by the spiking. [sent-250, score-0.454]
</p><p>98 This is possible in cells with close ﬁring rates since the effect an inhibitory spike has on a post-synaptic cell depends on 7  the post-synaptic membrane potential[13, 19]. [sent-251, score-1.139]
</p><p>99 BG may contain central pattern generators (CPGs) that activate innate behavioral routines, procedural memories, and learned motor programs[20] and recurrent alternating bursting is characteristic of cell assemblies included in CPGs[20]. [sent-257, score-0.939]
</p><p>100 Our modeling suggests that complex switching dynamics based in the sparse striatal inhibitory network may allow the generation of cell assemblies which interface sensory driven cortical patterns to dynamical sequence generation. [sent-259, score-1.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('assemblies', 0.442), ('cells', 0.424), ('cell', 0.378), ('inhibitory', 0.18), ('assembly', 0.177), ('switching', 0.167), ('ring', 0.166), ('isi', 0.152), ('quiescent', 0.132), ('rabinovich', 0.131), ('spike', 0.118), ('bifurcation', 0.11), ('burst', 0.11), ('huerta', 0.099), ('striatum', 0.094), ('bursting', 0.088), ('inhibition', 0.081), ('network', 0.078), ('sasaki', 0.076), ('raster', 0.076), ('attractor', 0.074), ('abarbanel', 0.071), ('inhibited', 0.066), ('excitatory', 0.065), ('hdi', 0.063), ('wlc', 0.063), ('laurent', 0.063), ('connections', 0.062), ('connectivity', 0.058), ('striatal', 0.057), ('chaotic', 0.055), ('cluster', 0.053), ('ordered', 0.053), ('dynamics', 0.052), ('neurons', 0.051), ('coloured', 0.05), ('msn', 0.05), ('prl', 0.05), ('clusters', 0.05), ('dynamical', 0.049), ('presynaptic', 0.049), ('biologically', 0.048), ('timescale', 0.047), ('networks', 0.047), ('series', 0.046), ('varona', 0.044), ('spiking', 0.044), ('periodic', 0.043), ('vicinity', 0.043), ('bursts', 0.04), ('membrane', 0.039), ('mi', 0.039), ('activity', 0.038), ('vi', 0.038), ('positively', 0.038), ('bazhenov', 0.038), ('collaterals', 0.038), ('connectivities', 0.038), ('cpgs', 0.038), ('gabaergic', 0.038), ('ibif', 0.038), ('intermittency', 0.038), ('quiescence', 0.038), ('wickens', 0.038), ('chaos', 0.038), ('cv', 0.038), ('realistic', 0.038), ('proportion', 0.038), ('periods', 0.037), ('green', 0.036), ('simulations', 0.034), ('switch', 0.034), ('correlated', 0.033), ('nowotny', 0.033), ('buzsaki', 0.033), ('stopfer', 0.033), ('gj', 0.033), ('synapses', 0.033), ('re', 0.032), ('formation', 0.031), ('recurrent', 0.031), ('postsynaptic', 0.03), ('isis', 0.03), ('excited', 0.03), ('regime', 0.03), ('cortical', 0.03), ('intermediate', 0.03), ('tsuda', 0.028), ('synaptic', 0.028), ('excitation', 0.027), ('patterns', 0.027), ('dynamically', 0.026), ('sparse', 0.026), ('state', 0.026), ('coherent', 0.026), ('rescaled', 0.026), ('periodically', 0.026), ('cij', 0.026), ('afraimovich', 0.025), ('aldridge', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="43-tfidf-1" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>2 0.23734009 <a title="43-tfidf-2" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>Author: Klaus Wimmer, Marcel Stimberg, Robert Martin, Lars Schwabe, Jorge Mariño, James Schummers, David C. Lyon, Mriganka Sur, Klaus Obermayer</p><p>Abstract: The computational role of the local recurrent network in primary visual cortex is still a matter of debate. To address this issue, we analyze intracellular recording data of cat V1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map. For the analysis, we consider a network model of Hodgkin-Huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map. We then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input. Each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances. A quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition. This recurrent regime is close to a regime of “instability”, where strong, self-sustained activity of the network occurs. The ﬁring rate of neurons in the best-ﬁtting network is particularly sensitive to small modulations of model parameters, which could be one of the functional beneﬁts of a network operating in this particular regime. 1</p><p>3 0.14020827 <a title="43-tfidf-3" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>4 0.13337655 <a title="43-tfidf-4" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>5 0.13091184 <a title="43-tfidf-5" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>Author: Vicençc Gómez, Andreas Kaltenbrunner, Vicente López, Hilbert J. Kappen</p><p>Abstract: Large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics. An example of this phenomenon is the transition from irregular, noise-driven dynamics to regular, self-sustained behavior observed in networks of integrate-and-ﬁre neurons as the interaction strength between the neurons increases. In this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals (dynamic range) is maximized. Self-organization occurs via synaptic dynamics that we analytically derive. The resulting plasticity rule is deﬁned locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses. 1</p><p>6 0.11933752 <a title="43-tfidf-6" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>7 0.11139405 <a title="43-tfidf-7" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>8 0.10343286 <a title="43-tfidf-8" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>9 0.10158247 <a title="43-tfidf-9" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>10 0.096477322 <a title="43-tfidf-10" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>11 0.085163601 <a title="43-tfidf-11" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>12 0.081237912 <a title="43-tfidf-12" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>13 0.081077442 <a title="43-tfidf-13" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>14 0.080617875 <a title="43-tfidf-14" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>15 0.069372952 <a title="43-tfidf-15" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>16 0.068243325 <a title="43-tfidf-16" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>17 0.066668987 <a title="43-tfidf-17" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>18 0.06561216 <a title="43-tfidf-18" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>19 0.056930918 <a title="43-tfidf-19" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>20 0.053133499 <a title="43-tfidf-20" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, 0.08), (2, 0.216), (3, 0.183), (4, -0.127), (5, 0.013), (6, -0.006), (7, -0.076), (8, 0.005), (9, 0.002), (10, -0.025), (11, 0.076), (12, -0.097), (13, 0.015), (14, -0.003), (15, -0.04), (16, 0.008), (17, -0.027), (18, 0.072), (19, -0.24), (20, -0.081), (21, 0.106), (22, -0.046), (23, 0.063), (24, 0.066), (25, -0.016), (26, 0.023), (27, 0.038), (28, 0.075), (29, 0.004), (30, 0.039), (31, -0.012), (32, -0.074), (33, 0.182), (34, 0.006), (35, 0.095), (36, -0.068), (37, 0.172), (38, -0.156), (39, 0.068), (40, -0.067), (41, -0.01), (42, -0.021), (43, -0.062), (44, -0.023), (45, 0.094), (46, 0.004), (47, -0.0), (48, -0.02), (49, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98087168 <a title="43-lsi-1" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>2 0.83472282 <a title="43-lsi-2" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>Author: Klaus Wimmer, Marcel Stimberg, Robert Martin, Lars Schwabe, Jorge Mariño, James Schummers, David C. Lyon, Mriganka Sur, Klaus Obermayer</p><p>Abstract: The computational role of the local recurrent network in primary visual cortex is still a matter of debate. To address this issue, we analyze intracellular recording data of cat V1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map. For the analysis, we consider a network model of Hodgkin-Huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map. We then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input. Each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances. A quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition. This recurrent regime is close to a regime of “instability”, where strong, self-sustained activity of the network occurs. The ﬁring rate of neurons in the best-ﬁtting network is particularly sensitive to small modulations of model parameters, which could be one of the functional beneﬁts of a network operating in this particular regime. 1</p><p>3 0.63310486 <a title="43-lsi-3" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>Author: Vicençc Gómez, Andreas Kaltenbrunner, Vicente López, Hilbert J. Kappen</p><p>Abstract: Large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics. An example of this phenomenon is the transition from irregular, noise-driven dynamics to regular, self-sustained behavior observed in networks of integrate-and-ﬁre neurons as the interaction strength between the neurons increases. In this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals (dynamic range) is maximized. Self-organization occurs via synaptic dynamics that we analytically derive. The resulting plasticity rule is deﬁned locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses. 1</p><p>4 0.60911405 <a title="43-lsi-4" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>5 0.59413141 <a title="43-lsi-5" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>6 0.53834301 <a title="43-lsi-6" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>7 0.53426731 <a title="43-lsi-7" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>8 0.51992953 <a title="43-lsi-8" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>9 0.51582921 <a title="43-lsi-9" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>10 0.47045112 <a title="43-lsi-10" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>11 0.41114235 <a title="43-lsi-11" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>12 0.38594437 <a title="43-lsi-12" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>13 0.34751713 <a title="43-lsi-13" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>14 0.34074545 <a title="43-lsi-14" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>15 0.32112604 <a title="43-lsi-15" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>16 0.31994236 <a title="43-lsi-16" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>17 0.31504658 <a title="43-lsi-17" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>18 0.2696203 <a title="43-lsi-18" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>19 0.26109007 <a title="43-lsi-19" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>20 0.23737991 <a title="43-lsi-20" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.047), (6, 0.515), (7, 0.036), (12, 0.022), (15, 0.018), (25, 0.033), (28, 0.088), (57, 0.044), (59, 0.017), (63, 0.016), (71, 0.029), (77, 0.026), (83, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95516092 <a title="43-lda-1" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>same-paper 2 0.93944919 <a title="43-lda-2" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>3 0.90459919 <a title="43-lda-3" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>4 0.88717884 <a title="43-lda-4" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>5 0.82277787 <a title="43-lda-5" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>Author: Jooseuk Kim, Clayton Scott</p><p>Abstract: We provide statistical performance guarantees for a recently introduced kernel classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference of densities. The classiﬁer is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike SVMs, however, the L2 kernel classiﬁer does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE, and apply this result to deduce an oracle inequality and consistency of the classiﬁer on the sense of both ISE and probability of error. Our results also specialize to give performance guarantees for an existing method of L2 kernel density estimation. 1</p><p>6 0.73378038 <a title="43-lda-6" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>7 0.72265011 <a title="43-lda-7" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>8 0.6679765 <a title="43-lda-8" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>9 0.60712963 <a title="43-lda-9" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>10 0.59900349 <a title="43-lda-10" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>11 0.58051008 <a title="43-lda-11" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>12 0.57745188 <a title="43-lda-12" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>13 0.57627702 <a title="43-lda-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.57215548 <a title="43-lda-14" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>15 0.56750435 <a title="43-lda-15" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>16 0.56704968 <a title="43-lda-16" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>17 0.56642365 <a title="43-lda-17" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>18 0.5621897 <a title="43-lda-18" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>19 0.56059653 <a title="43-lda-19" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>20 0.55566114 <a title="43-lda-20" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
