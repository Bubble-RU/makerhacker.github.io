<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>234 nips-2008-The Infinite Factorial Hidden Markov Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-234" href="#">nips2008-234</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>234 nips-2008-The Infinite Factorial Hidden Markov Model</h1>
<br/><p>Source: <a title="nips-2008-234-pdf" href="http://papers.nips.cc/paper/3518-the-infinite-factorial-hidden-markov-model.pdf">pdf</a></p><p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>Reference: <a title="nips-2008-234-reference" href="../nips2008_reference/nips-2008-The_Infinite_Factorial_Hidden_Markov_Model_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ifhm', 0.55), ('dish', 0.223), ('iic', 0.22), ('fhmm', 0.206), ('mibp', 0.206), ('buffet', 0.206), ('ibp', 0.202), ('markov', 0.199), ('st', 0.177), ('cm', 0.175), ('ic', 0.156), ('speak', 0.147), ('hid', 0.135), ('chain', 0.121), ('bm', 0.12), ('yt', 0.119), ('hmm', 0.101), ('wait', 0.1), ('custom', 0.098), ('blind', 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="234-tfidf-1" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>2 0.23832555 <a title="234-tfidf-2" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>3 0.15872273 <a title="234-tfidf-3" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>Author: Thomas L. Griffiths, Joseph L. Austerweil</p><p>Abstract: Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to deﬁne a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features. 1</p><p>4 0.12487574 <a title="234-tfidf-4" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>Author: Iain Murray, Ruslan Salakhutdinov</p><p>Abstract: We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost. 1</p><p>5 0.11139184 <a title="234-tfidf-5" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>6 0.10482426 <a title="234-tfidf-6" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>7 0.073477864 <a title="234-tfidf-7" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>8 0.071700133 <a title="234-tfidf-8" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>9 0.069491319 <a title="234-tfidf-9" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>10 0.06909433 <a title="234-tfidf-10" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>11 0.065799788 <a title="234-tfidf-11" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>12 0.065127 <a title="234-tfidf-12" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>13 0.063000038 <a title="234-tfidf-13" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>14 0.061105113 <a title="234-tfidf-14" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>15 0.060367636 <a title="234-tfidf-15" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>16 0.059609 <a title="234-tfidf-16" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>17 0.057768628 <a title="234-tfidf-17" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>18 0.053015061 <a title="234-tfidf-18" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>19 0.052573793 <a title="234-tfidf-19" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>20 0.052235488 <a title="234-tfidf-20" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.167), (1, -0.023), (2, -0.034), (3, 0.071), (4, 0.019), (5, -0.019), (6, 0.123), (7, -0.017), (8, 0.074), (9, -0.029), (10, -0.032), (11, -0.034), (12, 0.111), (13, 0.077), (14, 0.104), (15, 0.067), (16, 0.054), (17, 0.016), (18, 0.037), (19, 0.152), (20, 0.012), (21, -0.088), (22, 0.031), (23, -0.202), (24, -0.023), (25, 0.092), (26, 0.157), (27, -0.084), (28, 0.045), (29, 0.1), (30, -0.012), (31, -0.01), (32, -0.012), (33, -0.008), (34, 0.114), (35, -0.02), (36, 0.094), (37, -0.014), (38, 0.004), (39, 0.033), (40, 0.043), (41, -0.063), (42, 0.052), (43, -0.121), (44, 0.0), (45, 0.093), (46, -0.001), (47, 0.02), (48, -0.043), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90376973 <a title="234-lsi-1" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>2 0.68533432 <a title="234-lsi-2" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>3 0.58064574 <a title="234-lsi-3" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>Author: Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe</p><p>Abstract: Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method. 1</p><p>4 0.5731585 <a title="234-lsi-4" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>Author: Daniel M. Roy, Yee W. Teh</p><p>Abstract: We describe a novel class of distributions, called Mondrian processes, which can be interpreted as probability distributions over kd-tree data structures. Mondrian processes are multidimensional generalizations of Poisson processes and this connection allows us to construct multidimensional generalizations of the stickbreaking process described by Sethuraman (1994), recovering the Dirichlet process in one dimension. After introducing the Aldous-Hoover representation for jointly and separately exchangeable arrays, we show how the process can be used as a nonparametric prior distribution in Bayesian models of relational data. 1</p><p>5 0.55126977 <a title="234-lsi-5" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>Author: Dilan Gorur, Yee W. Teh</p><p>Abstract: We propose an efﬁcient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model [1]. Our algorithm has a quadratic runtime while those in [1] is cubic. In experiments, we were surprised to ﬁnd that in addition to being more efﬁcient, it is also a better sequential Monte Carlo sampler than the best in [1], when measured in terms of variance of estimated likelihood and effective sample size. 1</p><p>6 0.54865044 <a title="234-lsi-6" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>7 0.51266265 <a title="234-lsi-7" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>8 0.48767927 <a title="234-lsi-8" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>9 0.48024315 <a title="234-lsi-9" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>10 0.41711089 <a title="234-lsi-10" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>11 0.4163138 <a title="234-lsi-11" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>12 0.41546643 <a title="234-lsi-12" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>13 0.39962816 <a title="234-lsi-13" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>14 0.39645401 <a title="234-lsi-14" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>15 0.39551312 <a title="234-lsi-15" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>16 0.3702094 <a title="234-lsi-16" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>17 0.36517051 <a title="234-lsi-17" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>18 0.36358795 <a title="234-lsi-18" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>19 0.36211663 <a title="234-lsi-19" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>20 0.34796819 <a title="234-lsi-20" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.031), (12, 0.013), (28, 0.037), (30, 0.09), (35, 0.014), (38, 0.011), (40, 0.069), (60, 0.026), (63, 0.105), (64, 0.095), (71, 0.136), (92, 0.279)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76005101 <a title="234-lda-1" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>Author: Haixuan Yang, Irwin King, Michael Lyu</p><p>Abstract: Regularized Least Squares (RLS) algorithms have the ability to avoid over-ﬁtting problems and to express solutions as kernel expansions. However, we observe that the current RLS algorithms cannot provide a satisfactory interpretation even on the penalty of a constant function. Based on the intuition that a good kernelbased inductive function should be consistent with both the data and the kernel, a novel learning scheme is proposed. The advantages of this scheme lie in its corresponding Representer Theorem, its strong interpretation ability about what kind of functions should not be penalized, and its promising accuracy improvements shown in a number of experiments. Furthermore, we provide a detailed technical description about heat kernels, which serves as an example for the readers to apply similar techniques for other kernels. Our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions. 1</p><p>2 0.73538649 <a title="234-lda-2" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>same-paper 3 0.72287863 <a title="234-lda-3" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>4 0.6254164 <a title="234-lda-4" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>Author: Kevyn Collins-thompson</p><p>Abstract: Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user’s original query with additional related words. Current algorithms for automatic query expansion can often improve retrieval accuracy on average, but are not robust: that is, they are highly unstable and have poor worst-case performance for individual queries. To address this problem, we introduce a novel formulation of query expansion as a convex optimization problem over a word graph. The model combines initial weights from a baseline feedback algorithm with edge weights based on word similarity, and integrates simple constraints to enforce set-based criteria such as aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and signiﬁcant reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm. Our approach does not assume a particular retrieval model, making it applicable to a broad class of existing expansion algorithms. 1</p><p>5 0.62039179 <a title="234-lda-5" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>6 0.61940163 <a title="234-lda-6" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>7 0.61915934 <a title="234-lda-7" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>8 0.61890161 <a title="234-lda-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.61889243 <a title="234-lda-9" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>10 0.61785901 <a title="234-lda-10" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>11 0.61703753 <a title="234-lda-11" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>12 0.61684006 <a title="234-lda-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.61683661 <a title="234-lda-13" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>14 0.61668539 <a title="234-lda-14" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>15 0.61665964 <a title="234-lda-15" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>16 0.6166296 <a title="234-lda-16" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>17 0.61631757 <a title="234-lda-17" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>18 0.61610925 <a title="234-lda-18" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>19 0.61601925 <a title="234-lda-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.61594003 <a title="234-lda-20" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
