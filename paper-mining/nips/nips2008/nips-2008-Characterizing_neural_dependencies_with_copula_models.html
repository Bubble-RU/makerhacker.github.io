<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2008-Characterizing neural dependencies with copula models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-45" href="#">nips2008-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 nips-2008-Characterizing neural dependencies with copula models</h1>
<br/><p>Source: <a title="nips-2008-45-pdf" href="http://papers.nips.cc/paper/3593-characterizing-neural-dependencies-with-copula-models.pdf">pdf</a></p><p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>Reference: <a title="nips-2008-45-reference" href="../nips2008_reference/nips-2008-Characterizing_neural_dependencies_with_copula_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Characterizing neural dependencies with copula models  Pietro Berkes Volen Center for Complex Systems Brandeis University, Waltham, MA 02454 berkes@brandeis. [sent-1, score-1.042]
</p><p>2 uk  Abstract The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. [sent-5, score-0.225]
</p><p>3 However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. [sent-6, score-0.448]
</p><p>4 Here, we show that both marginal and joint properties of neural responses can be captured using copula models. [sent-7, score-1.104]
</p><p>5 Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. [sent-8, score-0.347]
</p><p>6 Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. [sent-9, score-0.369]
</p><p>7 We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. [sent-10, score-1.019]
</p><p>8 We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. [sent-11, score-0.281]
</p><p>9 We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. [sent-12, score-0.278]
</p><p>10 The stochastic spiking activity of individual neurons in cortex is often well described by a Poisson distribution. [sent-14, score-0.173]
</p><p>11 Responses from multiple neurons also exhibit strong dependencies (i. [sent-15, score-0.228]
</p><p>12 Recent work has focused on the construction of large parametric models that capture inter-neuronal dependencies using generalized linear point-process models [5, 6, 7, 8, 9] and binary second-order maximum-entropy models [10, 11, 12]. [sent-22, score-0.228]
</p><p>13 Although these approaches are quite powerful, they model spike trains only in very ﬁne time bins, and thus describe the dependencies in neural spike count distributions only implicitly. [sent-23, score-0.289]
</p><p>14 Modeling the joint distribution of neural activities is therefore an important open problem. [sent-24, score-0.122]
</p><p>15 Here we show how to construct non-independent joint distributions over ﬁring rates using copulas. [sent-25, score-0.108]
</p><p>16 Top row: The marginal distributions (the leftmost marginal is uniform, by deﬁnition of copula). [sent-28, score-0.196]
</p><p>17 cortex; ﬁnally, in Section 6 we review the insights provided by neural copula models and discuss several extensions and future directions. [sent-31, score-0.913]
</p><p>18 , un ) : [0, 1]n → [0, 1] is a multivariate distribution function on the unit cube with uniform marginals [13, 14]. [sent-35, score-0.265]
</p><p>19 The basic idea behind copulas is quite simple, and is closely related to that of histogram equalization: for a random variable yi with continuous cumulative distribution function (cdf) Fi , the random variable ui := Fi (yi ) is uniformly distributed on the interval [0, 1]. [sent-36, score-0.351]
</p><p>20 , Fn and joint distribution F , there exist a unique copula C such that for all ui : −1 −1 C(u1 , . [sent-44, score-0.965]
</p><p>21 , Fn (yn ))  (2)  is a n-variate distribution function with marginal distribution functions F1 , . [sent-59, score-0.134]
</p><p>22 This result gives a way to derive a copula given the joint and marginal distributions (using Eq. [sent-63, score-1.041]
</p><p>23 1), and also, more importantly here, to construct a joint distribution by specifying the marginal distributions and the dependency structure separately (Eq. [sent-64, score-0.296]
</p><p>24 For example, one can keep the dependency structure ﬁxed and vary the marginals (Fig. [sent-66, score-0.207]
</p><p>25 1), or vice versa given ﬁxed marginal distributions deﬁne new joint distributions using parametrized copula families (Fig. [sent-67, score-1.219]
</p><p>26 Since copulas do not depend on the marginals, one can deﬁne in this way dependency measures that are insensitive to non-linear transformations of the individual variables [14] and generalize correlation coefﬁcients, which are only appropriate for elliptic distributions. [sent-71, score-0.305]
</p><p>27 The copula representation has also been used to estimate the conditional entropy of neural latencies by separating the contribution of the individual latencies from that coming from their correlations [16]. [sent-72, score-1.007]
</p><p>28 One notable example is the Gaussian copula, which generalizes the dependency structure of the multivariate Gaussian distribution to arbitrary marginal distribution (Fig. [sent-74, score-0.28]
</p><p>29 1), and is deﬁned as C(u1 , u2 ; Σ) = ΦΣ φ−1 (u1 ), φ−1 (u2 ) , 2  (3)  Figure 2: Samples drawn from a joint distribution with ﬁxed Gaussian marginals and dependency structure deﬁned by parametric copula families, as indicated by the labels. [sent-75, score-1.176]
</p><p>30 where φ(u) is the cdf of the univariate Gaussian with mean 0 and variance 1, and ΦΣ is the cdf of a standard multivariate Gaussian with mean 0 and covariance matrix Σ. [sent-80, score-0.166]
</p><p>31 Other families derive from the economics literature, and are typically one-parameter families that capture various possible dependencies, for example dependencies only in one of the tails of the distribution. [sent-81, score-0.466]
</p><p>32 Table 1 shows the deﬁnition of the copula distributions used in this paper (see [14], for an overview of known copulas and copula construction methods). [sent-82, score-1.956]
</p><p>33 3  Maximum Likelihood estimation for discrete marginal distributions  In the case where the random variables have discrete distribution functions, as in the case of neural ﬁring rates, only a weaker version of Theorem 1 is valid: there always exists a copula that satisﬁes Eq. [sent-83, score-1.113]
</p><p>34 With discrete data, the probability of a particular outcome is determined by an integral over the region of [0, 1]n corresponding to that outcome; any two copulas that integrate to the same values on all such regions produce the same joint distribution. [sent-85, score-0.291]
</p><p>35 These marginals can be given by the empirical cumulative distribution of ﬁring rates (as in this paper) or by any parametrized family of univariate distributions (such as Poisson). [sent-88, score-0.296]
</p><p>36 , un ) du ,  ···  = argmax F1 (y1 −1)  3  Fn (yn −1)  (5)  θ  u  p(u|θ) = cθ (u1 , . [sent-92, score-0.109]
</p><p>37 3  Frank  Figure 3: Graphical representation of the copula model with discrete marginals. [sent-101, score-0.884]
</p><p>38 Uniform marginals u are drawn from the copula density function cθ (u1 , . [sent-102, score-0.965]
</p><p>39 The discrete marginals are then generated deterministically using the inverse cdf of the marginals, which are parametrized by λ. [sent-106, score-0.25]
</p><p>40 5  2  4  1  3  5  −5  6  0  4  7  8  5  5  9  10  10  Figure 4: Distribution of the maximum likelihood estimation of the parameters of four copula families, for various setting of their parameter (x-axis). [sent-111, score-0.891]
</p><p>41 The last equation is the copula probability mass inside the volume deﬁned by the vertices Fi (yi ) and Fi (yi − 1), and can be readily computed using the copula distribution Cθ (u1 , . [sent-115, score-1.759]
</p><p>42 For example, in the bivariate case one obtains argmax p(y1 , y2 |θ) = argmax Cθ (u1 , u2 ) + Cθ (u− , u− ) − Cθ (u− , u2 ) − Cθ (u1 , u− ) , 1 2 1 2 θ  (6)  θ  where ui = Fi (yi ) and u− = Fi (yi − 1). [sent-119, score-0.145]
</p><p>43 Given neural data in the form of ﬁring rates y1 , y2 from a pair of neurons, we collect the empirical cumulative histogram of responses, Fi (k) = P (yi ≤ k). [sent-122, score-0.15]
</p><p>44 The data is then transformed through the cdfs ui = Fi (yi ), and the copula model is ﬁt according to Eq. [sent-123, score-0.925]
</p><p>45 If a parametric distribution family is used for the marginals, the parameters of the copula θ and those of the marginals λ can be estimated simultaneously, or alternatively λ can be ﬁtted ﬁrst, followed by θ. [sent-125, score-1.04]
</p><p>46 We checked for biases in ML estimation due to a limited amount of data and low ﬁring rate by generating data from the discrete copula model (Fig. [sent-127, score-0.902]
</p><p>47 3), for a number of copula families and Poisson marginals with parameters λ1 = 2, λ2 = 3. [sent-128, score-1.076]
</p><p>48 Inaccuracy in the estimation becomes larger as the copulas approach functional dependency (i. [sent-132, score-0.308]
</p><p>49 , u2 = f (u1 ) for a deterministic function f ), as it is the case for the Gaussian copula when ρ tends to 1, and for the Gumbel copula as θ goes to inﬁnity. [sent-134, score-1.712]
</p><p>50 4                                    Figure 5: Empirical joint distribution and copula ﬁt for two neuron pairs. [sent-138, score-0.985]
</p><p>51 The top row shows two neurons that have dependencies mainly in the upper tails of their marginal distribution. [sent-139, score-0.442]
</p><p>52 4  Results  To demonstrate the ability of copula models to ﬁt joint ﬁring rate distribution, we model neural data recorded using a multi-electrode array implanted in the pre-motor cortex (PMd) area of a macaque monkey [18, 19]. [sent-147, score-1.109]
</p><p>53 We ﬁt the copula model using the marginal distribution of neural activity over the entire recording session, including data recorded between trials (i. [sent-151, score-1.041]
</p><p>54 , the stimulus-conditional response distribution), the marginal response distribution is an important statistical object in its own right, and has been the focus of recent much literature [10, 11]. [sent-156, score-0.147]
</p><p>55 For example, the joint activity across neurons, averaged over stimuli, is the only distribution the brain has access to, and must be sufﬁcient for learning to construct representations of the external world. [sent-157, score-0.099]
</p><p>56 We collected spike responses in 100ms bins, and selected at random, without repetition, a training set of 4000 bins and a test set of 2000 bins. [sent-158, score-0.138]
</p><p>57 Out of a total of 194 neurons we select a subset of 33 neurons that ﬁred a minimum of 2500 spikes over the whole data set. [sent-159, score-0.198]
</p><p>58 For every pair of neurons in this subset (528 pairs), we ﬁt the parameters of several copula families to the joint ﬁring rate. [sent-160, score-1.136]
</p><p>59 Figure 5 shows two examples of the kind of the dependencies present in the data set and how they are ﬁt by different copula families. [sent-161, score-0.985]
</p><p>60 This is conﬁrmed by the empirical copula, which shows the probability mass in the regions deﬁned by the cdfs of the marginal distribution. [sent-163, score-0.156]
</p><p>61 Since the marginal cdfs are discrete, the data is projected on a discrete set of points on the unit cube; the colors in the empirical copula plots represent the probability mass in the region where the marginal cdfs are constant. [sent-164, score-1.182]
</p><p>62 The axis in the empirical copula should be interpreted as the quantiles of the marginal distributions – for example, 0. [sent-165, score-0.986]
</p><p>63 The higher probability mass in the upper right corner of the plot thus means that the two neurons tend to be in the upper tails of the distributions simultaneously, and thus to have higher ﬁring rates together. [sent-167, score-0.302]
</p><p>64 On the right, one can see that this dependency structure is well captured by the Gumbel copula ﬁt. [sent-168, score-0.969]
</p><p>65 Although this is not readily visible in the joint histogram, the dependency becomes clear in the empirical copula plot. [sent-170, score-1.007]
</p><p>66 This structure is captured by the Frank copula ﬁt. [sent-171, score-0.887]
</p><p>67 The goodness-of-ﬁt of the copula families is evaluated by cross-validation: We ﬁt different models on training data, and compute the log-likelihood of test data under the ﬁtted model. [sent-175, score-0.984]
</p><p>68 The models are scored according to the difference between the log-likelihood of a model that assumes independent neurons and the log-likelihood of the copula model. [sent-176, score-0.972]
</p><p>69 This measure (appropriately renormalized) can be interpreted as the number of bits per second that can be saved when coding the ﬁring rate by taking into account the dependencies encoded by the copula family. [sent-177, score-1.012]
</p><p>70 (7) (8) (9)  We took particular care in selecting a small set of copula families that would be able to capture the dependencies occurring in the data. [sent-179, score-1.113]
</p><p>71 Some of the families that we considered at ﬁrst capture similar kind of dependencies, and their scores are highly correlated. [sent-180, score-0.128]
</p><p>72 For example, the Frank and Gaussian copulas are able to represent both positive and negative dependencies in the data, and simultaneously in lower and upper tails, although the dependencies in the tails are less strong for the Frank family (compare the copula densities in Figs. [sent-181, score-1.46]
</p><p>73 An advantage of the Frank copula is that it is much more efﬁcient to ﬁt, since the Gaussian copula requires multiple evaluations of the bivariate Gaussian cdf, which requires expensive numerical calculations. [sent-185, score-1.764]
</p><p>74 In addition, The Gaussian copula was also found to be more prone to overﬁtting on this data set (Fig. [sent-186, score-0.856]
</p><p>75 With similar procedures we shortlisted a total 3 families that cover the vast majority of dependencies in our data set: Frank, Clayton, and Gumbel copulas. [sent-189, score-0.24]
</p><p>76 Examples of the copula density of these families can be found in Figs. [sent-190, score-0.967]
</p><p>77 The Clayton and Gumbel copulas describe dependencies in the lower and upper tails of the distributions, respectively. [sent-192, score-0.444]
</p><p>78 We didn’t ﬁnd any example of neuron pairs where the dependency would be in the upper tail of the distribution for one and in the lower tail for the other distribution, or more complicated dependencies. [sent-193, score-0.236]
</p><p>79 Dependencies in the data set seem thus to be widespread, despite the fact that individual neurons are recorded from electrodes that are up to 4. [sent-196, score-0.139]
</p><p>80 The most common dependencies structures over all neuron pairs are given by the Gaussian-like dependencies of the Frank copula (54% of the pairs). [sent-200, score-1.188]
</p><p>81 Interestingly, a large proportion of the neurons showed dependencies concentrated in the upper tails (Gumbel copula, 22%) or lower tails (Clayton copula, 16%) of the distributions (Fig. [sent-201, score-0.47]
</p><p>82 1 We computed the signiﬁcance level by generating an artiﬁcial data set using independent neurons with the same empirical pdf as the monkey data. [sent-203, score-0.146]
</p><p>83 Right: Pie chart of the copula families that best ﬁt the neuron pairs. [sent-210, score-1.014]
</p><p>84 5  Discussion  The results presented here show that it is possible to represent neuronal spike responses using a model that preserves discrete, non-negative marginals while incorporating various types of dependencies between neurons. [sent-211, score-0.385]
</p><p>85 However, many copula families have only one or two parameters, regardless of the copula dimensionality. [sent-215, score-1.823]
</p><p>86 If the dependency structure across a neural population is relatively homogeneous, then these copulas may be useful in that they can be estimated using far less data than required, e. [sent-216, score-0.384]
</p><p>87 On the other hand, if the dependencies within a population vary markedly for different pairs of neurons (as in the data set examined here), such copulas will lack the ﬂexibility to capture the complicated dependencies within a full population. [sent-219, score-0.647]
</p><p>88 In such cases, we can still apply the Gaussian copula (and other copulas derived from elliptically symmetric distributions), since it is parametrized by the same covariance matrix as a n-dimensional Gaussian. [sent-220, score-1.109]
</p><p>89 However, the Gaussian copula becomes prohibitively expensive to ﬁt in high dimensions, since evaluating the likelihood requires an exponential number of evaluations of the multivariate Gaussian cdf, which itself must be computed numerically. [sent-221, score-0.921]
</p><p>90 One challenge for future work will therefore be to design new parametric families of copulas whose parameters grow with the number of neurons, but remain tractable enough for maximum-likelihood estimation. [sent-222, score-0.35]
</p><p>91 Recently, Kirshner [20] proposed a copula-based representation for multivariate distributions using a model that averages over tree-structured copula distributions. [sent-223, score-0.94]
</p><p>92 The basic idea is that pairwise copulas can be easily combined to produce a tree-structured representation of a multivariate distribution, and that averaging over such trees gives an even more ﬂexible class of multivariate distributions. [sent-224, score-0.304]
</p><p>93 Another future challenge is to combine explicit models of the stimulus-dependence underlying neural responses with models capable of capturing their joint response dependencies. [sent-226, score-0.207]
</p><p>94 The data set analyzed here concerned the distribution over spike responses during all all stimulus conditions (i. [sent-227, score-0.127]
</p><p>95 , the marginal distribution over responses, as opposed to the the conditional response distribution given a stimulus). [sent-229, score-0.154]
</p><p>96 Although this marginal response distribution is interesting in its own right, for many applications one is interested in separating correlations that are induced by external stimuli from internal correlations due to the network interactions. [sent-230, score-0.241]
</p><p>97 One obvious approach is to consider a hybrid model with a Linear-Nonlinear-Poisson model [21] capturing stimulus-induced correlation, adjoined to a copula distribution that models the residual dependencies between neurons (Fig. [sent-231, score-1.142]
</p><p>98 The LNP part of the model removes stimulus-induced correlations from the neural data, so that the copula model can take into account residual network-related dependencies. [sent-237, score-0.943]
</p><p>99 A point process framework for relating neural spiking activity to spiking history, neural ensemble and extrinsic covariate effects. [sent-283, score-0.167]
</p><p>100 Bayesian inference for spiking neuron models with a sparsity prior. [sent-298, score-0.099]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('copula', 0.856), ('copulas', 0.208), ('frank', 0.132), ('dependencies', 0.129), ('families', 0.111), ('marginals', 0.109), ('ring', 0.101), ('neurons', 0.099), ('gumbel', 0.09), ('tails', 0.084), ('dependency', 0.082), ('marginal', 0.08), ('clayton', 0.078), ('fi', 0.067), ('un', 0.062), ('fn', 0.059), ('responses', 0.058), ('fellows', 0.056), ('pindep', 0.056), ('joint', 0.055), ('bivariate', 0.052), ('cdf', 0.051), ('histogram', 0.049), ('multivariate', 0.048), ('neuron', 0.047), ('correlations', 0.047), ('ml', 0.046), ('parametrized', 0.045), ('pillow', 0.042), ('cdfs', 0.042), ('spike', 0.042), ('neural', 0.04), ('population', 0.038), ('paninski', 0.037), ('distributions', 0.036), ('spiking', 0.035), ('argmax', 0.033), ('poisson', 0.033), ('monkey', 0.033), ('parametric', 0.031), ('neuronal', 0.029), ('berkes', 0.028), ('hatsopoulos', 0.028), ('litke', 0.028), ('macke', 0.028), ('shlens', 0.028), ('sklar', 0.028), ('discrete', 0.028), ('row', 0.027), ('pairs', 0.027), ('distribution', 0.027), ('coding', 0.027), ('ui', 0.027), ('gaussian', 0.026), ('yi', 0.025), ('macaque', 0.024), ('improvement', 0.024), ('upper', 0.023), ('latencies', 0.022), ('yn', 0.022), ('cortex', 0.022), ('gauss', 0.021), ('implanted', 0.021), ('bins', 0.021), ('recorded', 0.021), ('colors', 0.02), ('response', 0.02), ('separating', 0.02), ('sher', 0.02), ('mass', 0.02), ('array', 0.02), ('primate', 0.019), ('electrodes', 0.019), ('cube', 0.019), ('estimation', 0.018), ('incorporating', 0.018), ('deterministically', 0.017), ('likelihood', 0.017), ('collected', 0.017), ('capture', 0.017), ('family', 0.017), ('activity', 0.017), ('rates', 0.017), ('models', 0.017), ('univariate', 0.016), ('structure', 0.016), ('uj', 0.015), ('pair', 0.015), ('cumulative', 0.015), ('concentrated', 0.015), ('gatsby', 0.015), ('correlation', 0.015), ('tail', 0.015), ('edition', 0.015), ('captured', 0.015), ('empirical', 0.014), ('simultaneously', 0.014), ('du', 0.014), ('hybrid', 0.014), ('derive', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="45-tfidf-1" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>2 0.52712512 <a title="45-tfidf-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>3 0.080721155 <a title="45-tfidf-3" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>4 0.059300762 <a title="45-tfidf-4" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>Author: Siwei Lyu, Eero P. Simoncelli</p><p>Abstract: We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass ﬁlter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass ﬁlter responses is signiﬁcantly greater than that achieved by PCA or ICA.</p><p>5 0.051982857 <a title="45-tfidf-5" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>6 0.051183183 <a title="45-tfidf-6" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>7 0.050667968 <a title="45-tfidf-7" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>8 0.046811696 <a title="45-tfidf-8" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>9 0.044329394 <a title="45-tfidf-9" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>10 0.043111023 <a title="45-tfidf-10" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>11 0.040294968 <a title="45-tfidf-11" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>12 0.039220773 <a title="45-tfidf-12" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>13 0.038572911 <a title="45-tfidf-13" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>14 0.037762322 <a title="45-tfidf-14" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>15 0.037714843 <a title="45-tfidf-15" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>16 0.037535343 <a title="45-tfidf-16" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>17 0.037414715 <a title="45-tfidf-17" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>18 0.037274458 <a title="45-tfidf-18" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>19 0.035770047 <a title="45-tfidf-19" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>20 0.030815445 <a title="45-tfidf-20" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.109), (1, 0.057), (2, 0.177), (3, 0.199), (4, -0.164), (5, 0.002), (6, 0.04), (7, -0.028), (8, -0.063), (9, 0.007), (10, -0.035), (11, -0.058), (12, 0.043), (13, -0.056), (14, -0.005), (15, -0.018), (16, 0.034), (17, 0.014), (18, -0.02), (19, 0.196), (20, -0.061), (21, -0.043), (22, 0.046), (23, -0.09), (24, -0.049), (25, -0.079), (26, 0.098), (27, 0.082), (28, -0.033), (29, -0.157), (30, -0.154), (31, 0.096), (32, 0.104), (33, 0.024), (34, 0.544), (35, 0.023), (36, 0.115), (37, -0.18), (38, 0.051), (39, -0.072), (40, -0.043), (41, -0.026), (42, -0.056), (43, 0.056), (44, -0.013), (45, -0.039), (46, 0.126), (47, -0.003), (48, -0.004), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95357323 <a title="45-lsi-1" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>2 0.81480211 <a title="45-lsi-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>3 0.37857229 <a title="45-lsi-3" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>4 0.26868317 <a title="45-lsi-4" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>5 0.2500734 <a title="45-lsi-5" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>Author: Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Current methods for extracting neural trajectories involve a two-stage process: the data are ﬁrst “denoised” by smoothing over time, then a static dimensionality reduction technique is applied. We ﬁrst describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which uniﬁes the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-ﬁt metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. 1</p><p>6 0.23259109 <a title="45-lsi-6" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>7 0.2160223 <a title="45-lsi-7" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>8 0.20949748 <a title="45-lsi-8" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>9 0.20698193 <a title="45-lsi-9" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>10 0.19810158 <a title="45-lsi-10" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>11 0.19156286 <a title="45-lsi-11" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>12 0.18989363 <a title="45-lsi-12" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>13 0.1823884 <a title="45-lsi-13" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>14 0.17341426 <a title="45-lsi-14" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>15 0.17248856 <a title="45-lsi-15" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>16 0.1706792 <a title="45-lsi-16" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>17 0.16486824 <a title="45-lsi-17" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>18 0.16184624 <a title="45-lsi-18" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>19 0.15959084 <a title="45-lsi-19" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>20 0.15151493 <a title="45-lsi-20" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.057), (7, 0.428), (12, 0.026), (15, 0.023), (27, 0.053), (28, 0.141), (57, 0.06), (59, 0.014), (63, 0.016), (77, 0.028), (83, 0.023), (94, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9694227 <a title="45-lda-1" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>2 0.96422434 <a title="45-lda-2" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>3 0.95041752 <a title="45-lda-3" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>Author: Kai Yu, Wei Xu, Yihong Gong</p><p>Abstract: In this paper we aim to train deep neural networks for rapid visual recognition. The task is highly challenging, largely due to the lack of a meaningful regularizer on the functions realized by the networks. We propose a novel regularization method that takes advantage of kernel methods, where an oracle kernel function represents prior knowledge about the recognition task of interest. We derive an efﬁcient algorithm using stochastic gradient descent, and demonstrate encouraging results on a wide range of recognition tasks, in terms of both accuracy and speed. 1</p><p>4 0.93313378 <a title="45-lda-4" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>same-paper 5 0.91590416 <a title="45-lda-5" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>6 0.75457543 <a title="45-lda-6" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>7 0.73815733 <a title="45-lda-7" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>8 0.73159063 <a title="45-lda-8" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>9 0.70340902 <a title="45-lda-9" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>10 0.70232439 <a title="45-lda-10" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>11 0.69817823 <a title="45-lda-11" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>12 0.696383 <a title="45-lda-12" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>13 0.68512726 <a title="45-lda-13" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>14 0.66530836 <a title="45-lda-14" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>15 0.66527265 <a title="45-lda-15" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>16 0.66498357 <a title="45-lda-16" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>17 0.66235369 <a title="45-lda-17" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>18 0.66110557 <a title="45-lda-18" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>19 0.65998888 <a title="45-lda-19" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>20 0.65967011 <a title="45-lda-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
