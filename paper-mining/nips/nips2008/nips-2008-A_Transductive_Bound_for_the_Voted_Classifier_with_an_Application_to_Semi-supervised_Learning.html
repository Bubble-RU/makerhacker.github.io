<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-5" href="#">nips2008-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</h1>
<br/><p>Source: <a title="nips-2008-5-pdf" href="http://papers.nips.cc/paper/3444-a-transductive-bound-for-the-voted-classifier-with-an-application-to-semi-supervised-learning.pdf">pdf</a></p><p>Author: Massih Amini, Nicolas Usunier, François Laviolette</p><p>Abstract: We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. The ﬁrst one involves the margin distribution of the classiﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a zone of low margin. In semi-supervised learning, considering the margin as an indicator of conﬁdence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. Following this assumption, we propose to bound the error probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed threshold. As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is ﬁxed manually. 1</p><p>Reference: <a title="nips-2008-5-reference" href="../nips2008_reference/nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. [sent-9, score-0.402]
</p><p>2 The ﬁrst one involves the margin distribution of the classiﬁer and a risk bound on its associate Gibbs classiﬁer. [sent-10, score-0.343]
</p><p>3 The bound is tight when so is the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a zone of low margin. [sent-11, score-0.378]
</p><p>4 In semi-supervised learning, considering the margin as an indicator of conﬁdence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. [sent-12, score-0.201]
</p><p>5 Following this assumption, we propose to bound the error probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed threshold. [sent-13, score-0.169]
</p><p>6 As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. [sent-14, score-0.303]
</p><p>7 Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is ﬁxed manually. [sent-15, score-0.062]
</p><p>8 1  Introduction  Ensemble methods [5] return a weighted vote of baseline classiﬁers. [sent-16, score-0.092]
</p><p>9 It is well known that under the PACBayes framework [9], one can obtain an estimation of the generalization error (also called risk) of such majority votes (referred as Bayes classiﬁer). [sent-17, score-0.086]
</p><p>10 Unfortunately, those bounds are generally not tight, mainly because they are indirectly obtain via a bound on a randomized combination of the baseline classiﬁers (called the Gibbs classiﬁer). [sent-18, score-0.117]
</p><p>11 Although the PAC-Bayes theorem gives tight risk bounds of Gibbs classiﬁers, the bounds of their associate Bayes classiﬁers come at a cost of worse risk (trivially a factor of 2, or under some margin assumption, a factor of 1+ ). [sent-19, score-0.507]
</p><p>12 In practice the Bayes risk is often smaller than the Gibbs risk. [sent-20, score-0.115]
</p><p>13 In this paper we present a transductive bound over the Bayes risk. [sent-21, score-0.195]
</p><p>14 This bound is also based on the risk of the associated Gibbs classiﬁer, but it takes as an additional information the exact knowledge of the margin distribution of unlabeled data. [sent-22, score-0.406]
</p><p>15 This bound is obtained by analytically solving a linear program. [sent-23, score-0.083]
</p><p>16 The intuitive idea here is that given the risk of the Gibbs classiﬁer and the margin distribution, the risk of the majority vote classiﬁer is maximized when all its errors are located on low margin examples. [sent-24, score-0.695]
</p><p>17 We show that our bound is tight when the associated Gibbs risk can accurately be estimated and when the Bayes classiﬁer makes most of its errors on low margin examples. [sent-25, score-0.429]
</p><p>18 The proof of this transductive bound makes use of the (joint) probability over an unlabeled data set that the majority vote classiﬁer makes an error and the margin is above a given threshold. [sent-26, score-0.614]
</p><p>19 This second result naturally leads to consider the conditional probability that the majority vote classiﬁer makes an error knowing that the margin is above a given threshold. [sent-27, score-0.318]
</p><p>20 This conditional probability is related to the concept that the margin is an indicator of conﬁdence which is recurrent in semi-supervised self-learning algorithms [3,6,10,11,12]. [sent-28, score-0.175]
</p><p>21 These methods ﬁrst train a classiﬁer on the labeled training examples. [sent-29, score-0.034]
</p><p>22 The classiﬁer outputs serve then to assign pseudo-class labels to unlabeled data having margin above a given threshold. [sent-30, score-0.239]
</p><p>23 The supervised method is retrained using the initial labeled set and its previous predictions on unlabeled data as additional labeled examples. [sent-31, score-0.149]
</p><p>24 In the second part of the paper, we propose to ﬁnd this margin threshold by minimizing the bound on the conditional probability. [sent-33, score-0.251]
</p><p>25 In section 4, we present experimental results obtained with a self-learning algorithm on different datasets in which we use the bound presented in section 2. [sent-35, score-0.082]
</p><p>26 2 for choosing the threshold which serve in the label assignment step of the algorithm. [sent-36, score-0.049]
</p><p>27 Finally, in section 5 we discuss the outcomes of this study and give some pointers to further research. [sent-37, score-0.036]
</p><p>28 We furthermore suppose that the training set is composed of a l labeled set Z = ((xi , yi ))i=1 ∈ Z l and an unlabeled set XU = (xi )l+u ∈ X u , where Z represents the i=l+1 set of X × Y. [sent-39, score-0.134]
</p><p>29 with respect to a ﬁxed, but unknown, probability distribution D over X × Y and we denote the marginal distribution over X by DX . [sent-43, score-0.012]
</p><p>30 1 In this study, we consider learning algorithms that work in a ﬁxed hypothesis space H of binary classiﬁers (deﬁned without reference to the training data). [sent-45, score-0.021]
</p><p>31 After observing the training set S = Z ∪ XU , the task of the learner is to choose a posterior distribution Q over H such that the Q-weighted majority vote classiﬁer BQ (also called the Bayes classiﬁer) will have the smallest possible risk on examples of XU . [sent-46, score-0.275]
</p><p>32 Recall that the Bayes classiﬁer is deﬁned by BQ (x) = sgn [Eh∼Q h(x)] ∀x ∈ X . [sent-47, score-0.02]
</p><p>33 We accordingly deﬁne the transductive risk of GQ over an unlabeled set by: def 1 Ru (GQ ) = Eh∼Q [[h(x ) = y ]] (2) u x ∈XU  Where, [[π]] = 1 if predicate π holds and 0 otherwise, and for every unlabeled example x ∈ XU we refer to y as its true unknown class label. [sent-50, score-0.48]
</p><p>34 1 we show that if we consider the margin as an indicator of δ conﬁdence and that we dispose a tight upper bound Ru (GQ ) of the risk of GQ which holds with probability 1 − δ over the random choice of Z and XU (for example using Theorem 17 or 18 of Derbelo et al. [sent-52, score-0.424]
</p><p>35 One of the practical issues that arises from this result is the possibility to deﬁne a threshold θ for which the bound is optimal and that we use in a self-learning algorithm by iteratively assigning pseudo-labels to unlabeled examples having margin above this threshold. [sent-54, score-0.35]
</p><p>36 We ﬁnally denote by Eu z the expectation of a random variable z with respect to the uniform distribution over XU and for notation convenience we equivalently deﬁne Pu the uniform 1 probability distribution over XU i. [sent-55, score-0.022]
</p><p>37 1  The proofs can be inferred to the more general noisy case, but one has to replace the summation P(x ,y )∼D (y |x ) in the deﬁnitions of equations (3) and (4). [sent-58, score-0.033]
</p><p>38 1  Main Result  Our main result is the following theorem which provides two bounds on the transductive risks of the Bayes classiﬁer (3) and the joint Bayes risk (4). [sent-61, score-0.312]
</p><p>39 Then for all Q and all δ ∈ (0, 1] with probability at least 1 − δ: Ru (BQ ) ≤ inf  γ∈(0,1]  Pu (mQ (x ) < γ) +  1 δ < Ku (Q) − MQ (γ) γ  (5)  +  1 δ δ Where Ku (Q) = Ru (GQ ) + 2 (Eu mQ (x ) − 1), MQ (t) = Eu mQ (x )[[mQ (x ) and . [sent-63, score-0.035]
</p><p>40 t]] for  being < or ≤  More generally, with probability at least 1 − δ, for all Q and all θ ≥ 0: Ru∧θ (BQ ) ≤ inf  γ∈(θ,1]  Pu (θ < mQ (x ) < γ) +  1 ≤ δ < Ku (Q) + MQ (θ) − MQ (γ) γ  +  (6)  In section 2. [sent-67, score-0.035]
</p><p>41 2 we will prove that the bound (5) simply follows from (6). [sent-68, score-0.067]
</p><p>42 Proposition 2, together with the explanations that follow, makes this idea clearer. [sent-70, score-0.027]
</p><p>43 Then, a Bayes classiﬁer that makes its error mostly on low margin regions will admit a coefﬁcient C in inequality (7) close to 1 and the bound of (5) δ becomes tight (provided we have an accurate upper bound Ru (GQ ) ). [sent-72, score-0.385]
</p><p>44 In the next section we provide proofs of all the statements above and show in lemma 4 a simple way to compute the best margin threshold for which the general bound on the joint Bayes risk is the lowest. [sent-73, score-0.434]
</p><p>45 2  Proofs  All our proofs are based on the relationship between Ru (GQ ) and Ru (BQ ) and the following lemma: Lemma 3 Let (γ1 , . [sent-75, score-0.033]
</p><p>46 , γN ) be the ordered sequence of the different strictly positive values of the margin on XU , that is {γi , i = 1. [sent-77, score-0.14]
</p><p>47 Denote moreover bi = Pu (BQ (x ) = y ∧ mQ (x ) = γi ) for i ∈ {1, . [sent-83, score-0.074]
</p><p>48 Then, N  Ru (GQ ) = i=1  1 (1 − Eu mQ (x )) 2  (8)  bi with k = max{i|γi ≤ θ}  bi γi +  (9)  N  ∀θ ∈ [0, 1], Ru∧θ (BQ ) = i=k+1  Proof Equation (9) follows the deﬁnition Ru∧θ (BQ ) = Pu (BQ (x ) = y ∧ mQ (x ) > θ). [sent-87, score-0.148]
</p><p>49 Recall that the values the x for which mQ (x ) = 0 counts for 0 in the sum that deﬁned the Gibbs risk (see equation 2 and the deﬁnition of mQ ). [sent-89, score-0.142]
</p><p>50 , bN ) such that 0 ≤ bi ≤ Pu (mQ (x ) = γi ) and which satisfy equations (8) and (9). [sent-93, score-0.074]
</p><p>51 δ Let k = max{i | γi ≤ θ}, assuming now that we can obtain an upper bound Ru (GQ ) of Ru (GQ ) which holds with probability 1 − δ over the random choices of Z and XU , from the deﬁnition (4) of Ru∧θ (BQ ) with probability 1 − δ we have then N  N  Ru∧θ (BQ ) ≤ max  b1 ,. [sent-94, score-0.114]
</p><p>52 ∀i, 0 ≤ bi ≤ Pu (mQ (x ) = γi ) and  (11)  i=1  i=k+1  1 δ δ Where Ku (Q) = Ru (GQ ) − 2 (1 − Eu mQ (x )). [sent-98, score-0.074]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mq', 0.596), ('bq', 0.464), ('ru', 0.364), ('pu', 0.232), ('gq', 0.212), ('margin', 0.14), ('xu', 0.139), ('transductive', 0.128), ('eh', 0.121), ('er', 0.121), ('risk', 0.115), ('ku', 0.106), ('unlabeled', 0.084), ('vote', 0.081), ('classi', 0.077), ('eu', 0.076), ('bi', 0.074), ('bayes', 0.074), ('bound', 0.067), ('gibbs', 0.063), ('majority', 0.058), ('voted', 0.057), ('tight', 0.044), ('def', 0.044), ('paris', 0.041), ('fu', 0.04), ('laboratoire', 0.038), ('threshold', 0.034), ('informatique', 0.033), ('tsvm', 0.033), ('proofs', 0.033), ('universit', 0.031), ('marie', 0.03), ('bounds', 0.029), ('curie', 0.028), ('pierre', 0.028), ('votes', 0.028), ('equation', 0.027), ('labeled', 0.024), ('indicator', 0.023), ('inf', 0.023), ('ers', 0.022), ('outcomes', 0.021), ('associate', 0.021), ('sgn', 0.02), ('lemma', 0.02), ('errors', 0.019), ('dence', 0.018), ('france', 0.018), ('low', 0.017), ('makes', 0.017), ('francois', 0.017), ('laviolette', 0.017), ('retrained', 0.017), ('suppose', 0.016), ('analytically', 0.016), ('proposition', 0.015), ('pointers', 0.015), ('qc', 0.015), ('zone', 0.015), ('datasets', 0.015), ('serve', 0.015), ('noticing', 0.014), ('theorem', 0.014), ('iteratively', 0.014), ('writes', 0.013), ('risks', 0.013), ('effectiveness', 0.013), ('joint', 0.013), ('card', 0.013), ('holds', 0.013), ('probability', 0.012), ('nicolas', 0.012), ('predicate', 0.012), ('admit', 0.012), ('margins', 0.012), ('statements', 0.012), ('bn', 0.011), ('examples', 0.011), ('inequality', 0.011), ('baseline', 0.011), ('proving', 0.011), ('hypothesis', 0.011), ('trivially', 0.011), ('propose', 0.01), ('constitutes', 0.01), ('convenience', 0.01), ('explanations', 0.01), ('indirectly', 0.01), ('maximized', 0.01), ('accurately', 0.01), ('upper', 0.01), ('training', 0.01), ('concentrated', 0.01), ('nition', 0.01), ('proof', 0.01), ('side', 0.01), ('attained', 0.01), ('knowing', 0.01), ('con', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="5-tfidf-1" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>Author: Massih Amini, Nicolas Usunier, François Laviolette</p><p>Abstract: We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. The ﬁrst one involves the margin distribution of the classiﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a zone of low margin. In semi-supervised learning, considering the margin as an indicator of conﬁdence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. Following this assumption, we propose to bound the error probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed threshold. As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is ﬁxed manually. 1</p><p>2 0.091744415 <a title="5-tfidf-2" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><p>3 0.07837256 <a title="5-tfidf-3" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>4 0.06920436 <a title="5-tfidf-4" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>5 0.068956338 <a title="5-tfidf-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.067483902 <a title="5-tfidf-6" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>7 0.059090141 <a title="5-tfidf-7" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>8 0.0567366 <a title="5-tfidf-8" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>9 0.05630957 <a title="5-tfidf-9" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>10 0.048629876 <a title="5-tfidf-10" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>11 0.04598432 <a title="5-tfidf-11" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>12 0.045070298 <a title="5-tfidf-12" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>13 0.044786282 <a title="5-tfidf-13" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>14 0.044710446 <a title="5-tfidf-14" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>15 0.042538289 <a title="5-tfidf-15" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>16 0.036679972 <a title="5-tfidf-16" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>17 0.035688799 <a title="5-tfidf-17" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>18 0.03553145 <a title="5-tfidf-18" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>19 0.034474581 <a title="5-tfidf-19" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>20 0.034162924 <a title="5-tfidf-20" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.091), (1, -0.043), (2, -0.064), (3, -0.02), (4, -0.071), (5, 0.017), (6, 0.028), (7, -0.033), (8, -0.008), (9, 0.024), (10, 0.056), (11, 0.084), (12, 0.015), (13, -0.075), (14, -0.073), (15, 0.046), (16, -0.016), (17, 0.043), (18, 0.044), (19, -0.034), (20, 0.007), (21, -0.043), (22, 0.008), (23, -0.081), (24, 0.021), (25, 0.003), (26, -0.078), (27, -0.073), (28, 0.051), (29, -0.05), (30, 0.068), (31, 0.097), (32, -0.033), (33, -0.025), (34, 0.03), (35, -0.001), (36, -0.048), (37, -0.019), (38, -0.051), (39, -0.016), (40, 0.015), (41, 0.016), (42, -0.028), (43, 0.001), (44, 0.022), (45, 0.067), (46, 0.003), (47, -0.069), (48, -0.022), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93228698 <a title="5-lsi-1" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>Author: Massih Amini, Nicolas Usunier, François Laviolette</p><p>Abstract: We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. The ﬁrst one involves the margin distribution of the classiﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a zone of low margin. In semi-supervised learning, considering the margin as an indicator of conﬁdence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. Following this assumption, we propose to bound the error probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed threshold. As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is ﬁxed manually. 1</p><p>2 0.62118781 <a title="5-lsi-2" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>3 0.56906509 <a title="5-lsi-3" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><p>4 0.54548365 <a title="5-lsi-4" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>5 0.52213931 <a title="5-lsi-5" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>6 0.49226466 <a title="5-lsi-6" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>7 0.48842543 <a title="5-lsi-7" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>8 0.48119521 <a title="5-lsi-8" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>9 0.47260872 <a title="5-lsi-9" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>10 0.4467282 <a title="5-lsi-10" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>11 0.44164687 <a title="5-lsi-11" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>12 0.44134626 <a title="5-lsi-12" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>13 0.4237535 <a title="5-lsi-13" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>14 0.41242108 <a title="5-lsi-14" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>15 0.40840551 <a title="5-lsi-15" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>16 0.39972177 <a title="5-lsi-16" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>17 0.38614115 <a title="5-lsi-17" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>18 0.38613051 <a title="5-lsi-18" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>19 0.37401825 <a title="5-lsi-19" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>20 0.37301436 <a title="5-lsi-20" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.369), (6, 0.078), (7, 0.067), (12, 0.013), (28, 0.094), (57, 0.021), (59, 0.013), (63, 0.019), (71, 0.064), (77, 0.01), (78, 0.016), (83, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72589177 <a title="5-lda-1" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>Author: Massih Amini, Nicolas Usunier, François Laviolette</p><p>Abstract: We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. The ﬁrst one involves the margin distribution of the classiﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a zone of low margin. In semi-supervised learning, considering the margin as an indicator of conﬁdence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. Following this assumption, we propose to bound the error probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed threshold. As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is ﬁxed manually. 1</p><p>2 0.52043045 <a title="5-lda-2" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>Author: Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola</p><p>Abstract: Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efﬁcient optimization algorithms, these convex formulations are not tight and sacriﬁce the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classiﬁcation to structured estimation. We show that a small modiﬁcation of existing optimization algorithms sufﬁces to solve this modiﬁed problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy. 1</p><p>3 0.38569549 <a title="5-lda-3" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>4 0.38506901 <a title="5-lda-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.38140291 <a title="5-lda-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.38138092 <a title="5-lda-6" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>7 0.38062209 <a title="5-lda-7" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>8 0.37907889 <a title="5-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.37894168 <a title="5-lda-9" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>10 0.37842977 <a title="5-lda-10" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>11 0.37798387 <a title="5-lda-11" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>12 0.37786487 <a title="5-lda-12" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>13 0.37691027 <a title="5-lda-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.37668842 <a title="5-lda-14" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>15 0.3758986 <a title="5-lda-15" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>16 0.37554586 <a title="5-lda-16" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>17 0.37522399 <a title="5-lda-17" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>18 0.37327254 <a title="5-lda-18" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>19 0.37265745 <a title="5-lda-19" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>20 0.37169611 <a title="5-lda-20" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
