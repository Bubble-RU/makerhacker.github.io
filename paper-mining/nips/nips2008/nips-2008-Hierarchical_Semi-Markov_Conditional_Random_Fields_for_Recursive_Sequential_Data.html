<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-98" href="#">nips2008-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</h1>
<br/><p>Source: <a title="nips-2008-98-pdf" href="http://papers.nips.cc/paper/3405-hierarchical-semi-markov-conditional-random-fields-for-recursive-sequential-data.pdf">pdf</a></p><p>Author: Tran T. Truyen, Dinh Phung, Hung Bui, Svetha Venkatesh</p><p>Abstract: Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efﬁcient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases. 1</p><p>Reference: <a title="nips-2008-98-reference" href="../nips2008_reference/nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. [sent-13, score-0.341]
</p><p>2 Importantly, we develop efﬁcient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. [sent-15, score-0.135]
</p><p>3 We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. [sent-16, score-0.236]
</p><p>4 We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases. [sent-17, score-0.191]
</p><p>5 1  Introduction  Modelling hierarchical aspects in complex stochastic processes is an important research issue in many application domains ranging from computer vision, text information extraction, computational linguistics to bioinformatics. [sent-18, score-0.119]
</p><p>6 For example, in a syntactic parsing task known as noun-phrase chunking, noun-phrases (NPs) and part-of-speech tags (POS) are two layers of semantics associated with words in the sentence. [sent-19, score-0.18]
</p><p>7 Previous approach ﬁrst tags the POS and then feeds these tags as input to the chunker. [sent-20, score-0.282]
</p><p>8 This may not be optimal, as a noun-phrase is often very informative to infer the POS tags belonging to the phrase. [sent-22, score-0.141]
</p><p>9 Thus, it is more desirable to jointly model and infer both the NPs and the POS tags at the same time. [sent-23, score-0.141]
</p><p>10 hierarchical structures, such as the dynamic CRFs (DCRF) [10], and hierarchical CRFs [5]. [sent-33, score-0.238]
</p><p>11 For example, in the noun-phrase chunking problem, no prior hierarchical structures are known. [sent-35, score-0.227]
</p><p>12 In addition, most discriminative structured models are trained in a completely supervised fashion using fully labelled data, and limited research has been devoted to dealing with the partially labelled data (e. [sent-37, score-0.146]
</p><p>13 We term the process of learning with partial labels partial-supervision, and the process of inference with partial labels constrained inference. [sent-42, score-0.2]
</p><p>14 To address the above issues, we propose the Hierarchical Semi-Markov Conditional Random Field (HSCRF), which is a recursive, undirected graphical model that generalises the undirected Markov chains and allows hierarchical decomposition. [sent-45, score-0.201]
</p><p>15 For example, the noun-phrase chunking problem can be modeled as a two level HSCRF, where the top level represents the NP process, the bottom level the POS process. [sent-47, score-0.43]
</p><p>16 Rich contextual information such as starting and ending of the phrase, the phrase length, and the distribution of words falling inside the phrase can be effectively encoded. [sent-50, score-0.475]
</p><p>17 We demonstrate the effectiveness of HSCRFs in two applications: (i) segmenting and labelling activities of daily living (ADLs) in an indoor environment and (ii) jointly modelling noun-phrases and part-of-speeches in shallow parsing. [sent-52, score-0.31]
</p><p>18 Our experimental results in the ﬁrst application show that the HSCRFs are capable of learning rich, hierarchical activities with good accuracy and exhibit better performance when compared to DCRFs and ﬂat-CRFs. [sent-53, score-0.222]
</p><p>19 Applications for recognition of activities and natural language parsing are presented in section 5. [sent-61, score-0.142]
</p><p>20 1  Model Deﬁnition and Parameterisation The Hierarchical Semi-Markov Conditional Random Fields  Consider a hierarchically nested Markov process with D levels where, by convention, the top level is the dummy root level that generates all subsequent Markov chains. [sent-64, score-0.339]
</p><p>21 Then, as in the generative process of the hierarchical HMMs [2], the parent state embeds a child Markov chain whose states may in turn contain grand-child Markov chains. [sent-65, score-0.439]
</p><p>22 The relation among these nested Markov chains is deﬁned via the model topology, which is a state hierarchy of depth D. [sent-66, score-0.211]
</p><p>23 It speciﬁes a set of states S d at each level d, i. [sent-67, score-0.155]
</p><p>24 |S d |}, where |S d | is the number of states at level d and 1 ≤ d ≤ D. [sent-72, score-0.155]
</p><p>25 For each state sd ∈ S d where d = D, the model also deﬁnes a set of children associated with it at the next level ch(sd ) ⊂ S d+1 , and thus conversely, each child sd+1 is associated with a set  of parental states at the upper level pa(sd+1 ) ⊂ S d . [sent-73, score-0.637]
</p><p>26 Unlike the original HHMMs proposed in [2] where tree structure is explicitly enforced on the state hierarchy, the HSCRFs allow arbitrary sharing of children among parental states as addressed in [1]. [sent-74, score-0.185]
</p><p>27 Start with the root node at the top level, as soon as a new state is created at level d = D, it initialises a child state at level d + 1. [sent-77, score-0.575]
</p><p>28 This child process at level d + 1 continues its execution recursively until it terminates, and when it does, the control of execution returns to its parent at the upper level d. [sent-79, score-0.412]
</p><p>29 At this point, the parent makes a decision either to transits to a new state at the same level or returns the control to the grand-parent at the upper level d − 1. [sent-80, score-0.448]
</p><p>30 The key intuition for this hierarchical nesting process is that the lifespan of a child process is a subsegment in the lifespan of its parent. [sent-81, score-0.299]
</p><p>31 To be more precise, consider the case which a parent process sd at level d starts a new state2 at time i and persists until time j. [sent-82, score-0.448]
</p><p>32 At time i, the parent initialises i:j a child state sd+1 which continues until it ends at time k < j, at which the child state transits to i a new child state sd+1 . [sent-83, score-0.838]
</p><p>33 The child process exits at time j, at which the control from the child level k+1 is returned to the parent sd . [sent-84, score-0.542]
</p><p>34 Upon receiving the control, the parent state sd may transit to a new i:j i:j parent state sd j+1:l , or ends at j and returns the control to the grand-parent at level d − 1. [sent-85, score-0.788]
</p><p>35 d=1  xi−1  xj−1  xi  xd−1 i+1  x2 2  d=2  ei−1 = 1  e2 2  ei = 0 xd i  d=D  ed i−1 = 1  1  xj  2  T −1  ej−1 = 0 xd i  ej = 1  d−1 ei = 0  xd i  xd i+1  ed = 1 i  ed = 1 i  T  xd+1 i  xd+1 i  Figure 1: Graphical presentation for HSCRFs (leftmost). [sent-86, score-1.503]
</p><p>36 Graph structures for state-persistence (middle-top), initialisation and ending (middle-bottom), and state-transition (rightmost). [sent-87, score-0.238]
</p><p>37 It starts from the root level indexed as 1, runs for T time slices and at each time slice a hierarchy of D states are generated. [sent-90, score-0.288]
</p><p>38 At each level d and time index i, there is a node representing a state variable xd ∈ S d = {1, 2, . [sent-91, score-0.576]
</p><p>39 Associated with each xd i i is an ending indicator ed which can be either 1 or 0 to signify whether the state xd terminates or i i continues its execution to the next time slice. [sent-95, score-1.069]
</p><p>40 The nesting nature of the HSCRFs is formally realised by imposing the speciﬁc constraints on the value assignment of ending indicators: • The root state persists during the course of evolution, i. [sent-96, score-0.415]
</p><p>41 , ed = 1 implies i ed+1:D = 1; and when a state persists, all of its ancestors must also persist, i. [sent-103, score-0.133]
</p><p>42 i • When a state transits, its parent must remain unchanged, i. [sent-106, score-0.186]
</p><p>43 , ed = 1, ed−1 = 0, and states i i at the bottom level terminates at every single slice, i. [sent-108, score-0.24]
</p><p>44 i Thus, speciﬁc value assignments of ending indicators provide contexts that realise the evolution of the model states in both hierarchical (vertical) and temporal (horizontal) directions. [sent-111, score-0.427]
</p><p>45 Each context at 1 In HHMMs, the bottom level is also called production level, in which the states emit observational symbols. [sent-112, score-0.184]
</p><p>46 2 Our notation sd is to denote the set of variables from time i to j at level d, i. [sent-114, score-0.293]
</p><p>47 i:j i:j i i+1 j  a level and associated state variables form a contextual clique, and here we identify four contextual clique types (cf. [sent-120, score-0.669]
</p><p>48 1): • State-persistence : This corresponds to the life time of a state at a given level Speciﬁcally, persist,d given a context be c = (ed = (xd , c), is a contextual i−1:j = (1, 0, . [sent-122, score-0.378]
</p><p>49 , 0, 1)), then σi:j i:j d clique that speciﬁes the life span [i, j] of any state s = xi:j . [sent-124, score-0.297]
</p><p>50 • State-transition : This corresponds to a state at level d ∈ [2, D] at time i transiting to transit,d a new state. [sent-125, score-0.245]
</p><p>51 Speciﬁcally, given a context c = (ed−1 = 0, ed = 1) then σi = i i d−1 d d d (xi+1 , xi:i+1 , c) is a contextual clique that speciﬁes the transition of xi to xi+1 at time i under the same parent xd−1 . [sent-126, score-0.46]
</p><p>52 i+1 • State-initialisation : This corresponds to a state at level d ∈ [1, D − 1] initialising a new child state at level d + 1 at time i. [sent-127, score-0.543]
</p><p>53 Speciﬁcally, given a context c = (ed = 1), then i−1 init,d σi = (xd , xd+1 , c) is a contextual clique that speciﬁes the initialisation at time i from i i the parent xd to the ﬁrst child xd+1 . [sent-128, score-0.887]
</p><p>54 i i • State-exiting : This corresponds to a state at level d ∈ [1, D−1] to end at time i Speciﬁcally, exit,d given a context c = (ed = 1), then σi = (xd , xd+1 , c) is a contextual clique that i i i d speciﬁes the ending of xi at time i with the last child xd+1 . [sent-129, score-0.861]
</p><p>55 i In the HSCRF, we are interested in the conditional setting, in which the entire state and ending variables (x1:D , e1:D ) are conditioned on an observational sequence z. [sent-130, score-0.324]
</p><p>56 For example, in computa1:T 1:T tional linguistics, the observation is often the sequence of words, and the state variables might be the part-of-speech tags and the phrases. [sent-131, score-0.248]
</p><p>57 To capture the correlation between variables and such conditioning, we deﬁne a non-negative potential function φ(σ, z) over each contextual clique σ. [sent-132, score-0.352]
</p><p>58 Figure 2 shows the notations for potentials that correspond to the four contextual clique types we have identiﬁed above. [sent-133, score-0.377]
</p><p>59 State persistence potential State transition potential State initialization potential State ending potential  d,s,z persist,d Ri:j = φ(σi:j , z) where s = xd . [sent-136, score-0.648]
</p><p>60 i:j d,s,z transit,d Au,v,i = φ(σi , z) where s = xd−1 and u = xd , v = xd . [sent-137, score-0.662]
</p><p>61 i i+1 i+1 d,s,z init,d πu,i = φ(σi , z) where s = xd , u = xd+1 . [sent-138, score-0.331]
</p><p>62 i i d,s,z exit,d Eu,i = φ(σi , z) where s = xd , u = xd+1 . [sent-139, score-0.331]
</p><p>63 i i  Figure 2: Shorthands for contextual clique potentials. [sent-140, score-0.323]
</p><p>64 A conﬁguration ζ of the model is a complete assignment of all the states and i ending indicators (x1:D , e1:D ) which satisﬁes the set of hierarchical constraints described earlier in 1:i 1:T  this section. [sent-142, score-0.415]
</p><p>65 2  (1)  Log-linear Parameterisation  d In our HSCRF setting, there is a feature vector fσ (σ, z) associated with each type of contextual d d d clique σ, in that φ(σ , z) = exp θσ • fσ (σ, z) . [sent-145, score-0.323]
</p><p>66 Thus, the features are active only in the context in which the corresponding contextual cliques appear. [sent-147, score-0.133]
</p><p>67 For the state-persistence contextual clique, the features incorporate state-duration, start time i and end time j of the state. [sent-148, score-0.197]
</p><p>68 The key insight is the context-speciﬁc independence, which is due to hierarchical constraints described in Section 2. [sent-153, score-0.119]
</p><p>69 Given Πi:j , the set of variables inside the blanket is independent of those outside it. [sent-156, score-0.172]
</p><p>70 A similar relation holds with respect to the asymmetric Markov blanket, which includes the set of variable assignments Γd,s (u) = (xd = i:j i:j s, xd+1 = u, ed:D = 1, ed+1:D = 1, ed = 0). [sent-157, score-0.122]
</p><p>71 Figure 3 depicts an asymmetric Markov blanket i−1 i:j−1 j j (the covering arrowed line) containing a smaller asymmetric blanket (the left arrowed line) and a symmetric blanket (the double-arrowed line). [sent-158, score-0.577]
</p><p>72 Denote by ∆d,s the sum of products of all clique i:j  level d level d +1  Figure 3: Decomposition with respect to symmetric/asymmetric Markov blankets. [sent-159, score-0.402]
</p><p>73 potentials falling inside the symmetric Markov blanket Πd,s . [sent-160, score-0.273]
</p><p>74 In the same manner, let αi:j (u) be the sum i:j ˆ of products of all clique potentials falling inside the asymmetric Markov blanket Γd,s (u). [sent-162, score-0.519]
</p><p>75 In general, when we make observations, we observe some states and some ending indicators. [sent-176, score-0.225]
</p><p>76 Let ˜ V = {˜, e} be the set of observed state and end variables respectively. [sent-177, score-0.132]
</p><p>77 For example, computing ∆d,s assumes Πd,s , which implies the constraint to the i:j i:j state s at level d starting at i and persisting till terminating at j. [sent-179, score-0.213]
</p><p>78 , there is an xd = s for k ∈ [i, j]) are made causing this constraint invalid, ∆d,s will be zero. [sent-182, score-0.331]
</p><p>79 The persistent activities share some of the 12 sub-trajectories. [sent-193, score-0.14]
</p><p>80 Thus naturally, the data has a state hierarchy of depth 3: the dummy root for each location sequence, the persistent activities, and the sub-trajectories. [sent-195, score-0.28]
</p><p>81 At each level d and time t we count an error if the predicted state is not the same as the ground-truth. [sent-197, score-0.245]
</p><p>82 Next, we consider partially-supervised learning in which about 50% of start/end times of a state and state labels are observed at the second level. [sent-210, score-0.304]
</p><p>83 All ending indicators are known at the bottom level. [sent-211, score-0.248]
</p><p>84 As can be seen, although only 50% of the state  labels and state start/end times are observed, the model learned is still performing well with accuracy of 80. [sent-213, score-0.279]
</p><p>85 We next consider the issue of partially observing labels during decoding and test the effect using degraded learned models. [sent-216, score-0.137]
</p><p>86 There are 48 POS labels and 3 NP labels (B-NP for beginning of a noun-phrase, I-NP for inside a noun-phrase or O for others). [sent-227, score-0.169]
</p><p>87 We build an HSCRF topology of 3 levels, where the root is just a dummy node, the second level has 2 NP states, and the bottom level has 5 POS states. [sent-234, score-0.339]
</p><p>88 Since the state spaces are relatively small, we are able to run exact inference in the DCRF by collapsing both the NP and POS state spaces to a combined state space of size 3 × 5 = 15. [sent-237, score-0.359]
</p><p>89 The SCRF and Semi-CRF model only the NP process, taking the POS tags and words as input. [sent-238, score-0.141]
</p><p>90 Furthermore, we use the contextual window of 5 instead of 7 as in [10]. [sent-244, score-0.133]
</p><p>91 The model feature is factorised as f (xc , z) = I(xc )gc (z), where I(xc ) is a binary function on the assignment of the clique variables xc , and gc (z) are the raw features. [sent-246, score-0.291]
</p><p>92 At test time, since the SCRF and the Semi-CRF are able to use the POS tags as input, it is not fair for the DCRF and HSCRF to predict those labels during inference. [sent-255, score-0.206]
</p><p>93 Instead, we also give the POS tags to the DCRF and HSCRF and perform constrained inference to predict only the NP labels. [sent-256, score-0.211]
</p><p>94 Without POS tags given at test time, both the HSCRF and the DCRF perform worse than the SCRF. [sent-265, score-0.141]
</p><p>95 This is not surprising because the POS tags are always given in the case of SCRF. [sent-266, score-0.141]
</p><p>96 6  Discussion and Conclusions  The HSCRFs presented here are not a standard graphical model since the clique structures are not predeﬁned. [sent-268, score-0.241]
</p><p>97 The potentials are deﬁned on-the-ﬂy depending on the assignments of the ending indicators. [sent-269, score-0.27]
</p><p>98 Furthermore, the state persistence potentials capture duration information that is not available in the DBN representation of the HHMMs in [6]. [sent-271, score-0.186]
</p><p>99 However, the context-free grammar does not limit the depth of semantic hierarchy, thus making unnecessarily difﬁcult to map many hierarchical problems into its form. [sent-274, score-0.151]
</p><p>100 Learning and detecting activities from movement trajectories using the hierarchical hidden Markov models. [sent-320, score-0.222]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hscrf', 0.464), ('pos', 0.348), ('xd', 0.331), ('dcrf', 0.232), ('hscrfs', 0.216), ('clique', 0.19), ('ending', 0.176), ('sd', 0.155), ('tags', 0.141), ('contextual', 0.133), ('blanket', 0.133), ('scrf', 0.133), ('np', 0.124), ('hierarchical', 0.119), ('xpos', 0.116), ('state', 0.107), ('level', 0.106), ('activities', 0.103), ('child', 0.085), ('chunking', 0.083), ('hhmm', 0.083), ('parent', 0.079), ('indoor', 0.073), ('bui', 0.073), ('markov', 0.071), ('pr', 0.067), ('hhmms', 0.066), ('phung', 0.066), ('labels', 0.065), ('xnp', 0.058), ('asymmetric', 0.056), ('crfs', 0.056), ('potentials', 0.054), ('persist', 0.053), ('transits', 0.05), ('states', 0.049), ('falling', 0.047), ('partially', 0.047), ('ik', 0.046), ('crf', 0.046), ('modelling', 0.045), ('persists', 0.044), ('ej', 0.043), ('indicators', 0.043), ('conditional', 0.041), ('phrase', 0.04), ('xc', 0.04), ('parameterised', 0.04), ('assignments', 0.04), ('inside', 0.039), ('parsing', 0.039), ('inference', 0.038), ('hierarchy', 0.038), ('initialisation', 0.037), ('persistent', 0.037), ('continues', 0.036), ('labelled', 0.036), ('dummy', 0.035), ('nested', 0.034), ('shallow', 0.034), ('adls', 0.033), ('aio', 0.033), ('arrowed', 0.033), ('curtin', 0.033), ('gc', 0.033), ('hung', 0.033), ('initialises', 0.033), ('lifespan', 0.033), ('parameterisation', 0.033), ('recognising', 0.033), ('truyen', 0.033), ('venkatesh', 0.033), ('time', 0.032), ('topology', 0.032), ('depth', 0.032), ('constrained', 0.032), ('root', 0.031), ('terminates', 0.03), ('ei', 0.029), ('jul', 0.029), ('nesting', 0.029), ('nps', 0.029), ('parental', 0.029), ('sentences', 0.029), ('bottom', 0.029), ('potential', 0.029), ('assignment', 0.028), ('undirected', 0.028), ('segmenting', 0.028), ('levels', 0.027), ('discriminative', 0.027), ('jose', 0.027), ('living', 0.027), ('graphical', 0.026), ('ed', 0.026), ('elds', 0.026), ('observed', 0.025), ('structures', 0.025), ('persistence', 0.025), ('degraded', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="98-tfidf-1" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>Author: Tran T. Truyen, Dinh Phung, Hung Bui, Svetha Venkatesh</p><p>Abstract: Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efﬁcient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases. 1</p><p>2 0.10328329 <a title="98-tfidf-2" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>Author: Peter Carbonetto, Mark Schmidt, Nando D. Freitas</p><p>Abstract: The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its farreaching application, there is almost no work on applying stochastic approximation to learning problems with general constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization. 1</p><p>3 0.10222589 <a title="98-tfidf-3" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>4 0.091012798 <a title="98-tfidf-4" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><p>5 0.085860699 <a title="98-tfidf-5" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>Author: Roger P. Levy, Florencia Reali, Thomas L. Griffiths</p><p>Abstract: Language comprehension in humans is signiﬁcantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and ﬁelded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle ﬁlter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the ﬁrst rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information. 1</p><p>6 0.074907042 <a title="98-tfidf-6" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>7 0.067971185 <a title="98-tfidf-7" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>8 0.055793878 <a title="98-tfidf-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.054993629 <a title="98-tfidf-9" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>10 0.048071314 <a title="98-tfidf-10" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>11 0.045843981 <a title="98-tfidf-11" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>12 0.044248436 <a title="98-tfidf-12" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>13 0.043861646 <a title="98-tfidf-13" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>14 0.041112997 <a title="98-tfidf-14" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>15 0.040419493 <a title="98-tfidf-15" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>16 0.039429907 <a title="98-tfidf-16" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>17 0.039290242 <a title="98-tfidf-17" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>18 0.038056687 <a title="98-tfidf-18" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>19 0.037950482 <a title="98-tfidf-19" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>20 0.037859574 <a title="98-tfidf-20" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, -0.009), (2, 0.056), (3, -0.044), (4, 0.004), (5, -0.03), (6, 0.017), (7, 0.032), (8, -0.015), (9, -0.055), (10, -0.049), (11, 0.026), (12, 0.043), (13, 0.004), (14, 0.004), (15, 0.057), (16, 0.033), (17, -0.095), (18, -0.041), (19, -0.008), (20, 0.035), (21, 0.08), (22, 0.002), (23, -0.028), (24, -0.048), (25, 0.01), (26, 0.021), (27, -0.085), (28, -0.004), (29, -0.058), (30, 0.037), (31, -0.083), (32, -0.196), (33, -0.095), (34, -0.069), (35, -0.124), (36, 0.073), (37, 0.033), (38, 0.027), (39, 0.035), (40, -0.066), (41, -0.013), (42, -0.074), (43, 0.106), (44, -0.026), (45, 0.03), (46, 0.023), (47, 0.012), (48, -0.158), (49, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91636932 <a title="98-lsi-1" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>Author: Tran T. Truyen, Dinh Phung, Hung Bui, Svetha Venkatesh</p><p>Abstract: Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efﬁcient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases. 1</p><p>2 0.63560271 <a title="98-lsi-2" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>Author: Roger P. Levy, Florencia Reali, Thomas L. Griffiths</p><p>Abstract: Language comprehension in humans is signiﬁcantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and ﬁelded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle ﬁlter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the ﬁrst rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information. 1</p><p>3 0.54362619 <a title="98-lsi-3" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing, Bo Zhang</p><p>Abstract: Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overﬁtting, but is also built on the desirable discriminative laws resemble those of the M3 N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M3 N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task. 1</p><p>4 0.50679344 <a title="98-lsi-4" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><p>5 0.47637001 <a title="98-lsi-5" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>Author: Liang Zhang, Deepak Agarwal</p><p>Abstract: Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable that is organized hierarchically. Model ﬁtting is challenging, especially for a hierarchy with a large number of nodes. We provide a novel algorithm based on a multi-scale Kalman ﬁlter that is both scalable and easy to implement. For Gaussian response, we show our method provides the maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response, parameter estimation is performed through a Laplace approximation. However, the Laplace approximation provides biased parameter estimates that is corrected through a parametric bootstrap procedure. We illustrate through simulation studies and analyses of real world data sets in health care and online advertising.</p><p>6 0.47262928 <a title="98-lsi-6" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>7 0.46828479 <a title="98-lsi-7" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>8 0.45035461 <a title="98-lsi-8" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>9 0.44519073 <a title="98-lsi-9" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>10 0.44240493 <a title="98-lsi-10" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>11 0.39758316 <a title="98-lsi-11" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>12 0.39578336 <a title="98-lsi-12" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>13 0.39356861 <a title="98-lsi-13" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>14 0.37881589 <a title="98-lsi-14" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>15 0.37520698 <a title="98-lsi-15" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>16 0.35329095 <a title="98-lsi-16" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>17 0.35230118 <a title="98-lsi-17" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>18 0.34015328 <a title="98-lsi-18" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>19 0.33600098 <a title="98-lsi-19" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>20 0.30874181 <a title="98-lsi-20" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.036), (7, 0.051), (12, 0.051), (15, 0.011), (28, 0.141), (57, 0.104), (59, 0.019), (63, 0.016), (71, 0.017), (76, 0.316), (77, 0.057), (78, 0.032), (83, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75786924 <a title="98-lda-1" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>Author: Tran T. Truyen, Dinh Phung, Hung Bui, Svetha Venkatesh</p><p>Abstract: Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efﬁcient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases. 1</p><p>2 0.53765357 <a title="98-lda-2" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>3 0.53212166 <a title="98-lda-3" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>4 0.52989638 <a title="98-lda-4" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>5 0.52532011 <a title="98-lda-5" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>6 0.52453667 <a title="98-lda-6" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>7 0.5241611 <a title="98-lda-7" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>8 0.52361655 <a title="98-lda-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.52347445 <a title="98-lda-9" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>10 0.52317059 <a title="98-lda-10" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>11 0.52277124 <a title="98-lda-11" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>12 0.52198017 <a title="98-lda-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.52073538 <a title="98-lda-13" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>14 0.52044648 <a title="98-lda-14" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>15 0.52014667 <a title="98-lda-15" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>16 0.51885134 <a title="98-lda-16" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>17 0.51853073 <a title="98-lda-17" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>18 0.51830983 <a title="98-lda-18" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>19 0.51821256 <a title="98-lda-19" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>20 0.51756555 <a title="98-lda-20" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
