<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-52" href="#">nips2008-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</h1>
<br/><p>Source: <a title="nips-2008-52-pdf" href="http://papers.nips.cc/paper/3564-correlated-bigram-lsa-for-unsupervised-language-model-adaptation.pdf">pdf</a></p><p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>Reference: <a title="nips-2008-52-reference" href="../nips2008_reference/nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. [sent-5, score-0.764]
</p><p>2 The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. [sent-6, score-0.393]
</p><p>3 We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. [sent-7, score-0.79]
</p><p>4 For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. [sent-8, score-0.923]
</p><p>5 Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2. [sent-9, score-0.788]
</p><p>6 5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. [sent-10, score-0.208]
</p><p>7 On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. [sent-11, score-0.12]
</p><p>8 1  Introduction  Language model (LM) adaptation is crucial to automatic speech recognition (ASR) as it enables higher-level contextual information to be effectively incorporated into a background LM improving recognition performance. [sent-12, score-0.183]
</p><p>9 Exploiting topical context for LM adaptation has shown to be effective for ASR using latent semantic analysis (LSA) such as LSA using singular value decomposition [1], Latent Dirichlet Allocation (LDA) [2, 3, 4] and HMM-LDA [5, 6]. [sent-13, score-0.195]
</p><p>10 For document classiﬁcation, word ordering may not be important. [sent-15, score-0.134]
</p><p>11 But in the LM perspective, word ordering is crucial since a trigram LM normally performs signiﬁcantly better than a unigram LM for word prediction. [sent-16, score-0.335]
</p><p>12 We employ bigram LSA [7] which is a natural extension of LDA to relax the bag-of-word assumption by connecting the adjacent words in a document together to form a Markov chain. [sent-18, score-0.647]
</p><p>13 There are two main challenges in bigram LSA which are not addressed properly in [7] especially for largescale application. [sent-19, score-0.564]
</p><p>14 Secondly, model initialization is important for EM training, especially for bigram LSA due to the model sparsity. [sent-22, score-0.564]
</p><p>15 To tackle the ﬁrst challenge, we represent bigram LSA as a set of K topic-dependent backoff LM. [sent-23, score-0.61]
</p><p>16 We propose fractional Kneser-Ney smoothing 1 which supports ∗ This work is partly supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. [sent-24, score-0.212]
</p><p>17 Prior distribution over topic mixture weights  Latent topics  Observed words    θ  z1  z2  zN  w1  w2  wN  Figure 1: Graphical representation of bigram LSA. [sent-29, score-0.753]
</p><p>18 To address the second challenge, we propose a bootstrapping approach for bigram LSA training using a well-trained unigram LSA as an initial model. [sent-33, score-0.756]
</p><p>19 During unsupervised LM adaptation, word hypotheses from the ﬁrst-pass decoding are used to estimate the topic mixture weight of each test audio to adapt both unigram and bigram LSA. [sent-34, score-1.026]
</p><p>20 The adapted unigram and bigram LSA are combined with the background LM in two stages. [sent-35, score-0.762]
</p><p>21 Firstly, marginal adaptation [10] is applied to integrate unigram LSA into the background LM. [sent-36, score-0.337]
</p><p>22 Then the intermediately adapted LM from the ﬁrst stage is combined with bigram LSA via linear interpolation with the interpolation weights estimated by minimizing the word perplexity on the word hypotheses. [sent-37, score-0.83]
</p><p>23 Related work includes topic mixtures [11] which perform document clustering and train a trigram LM for each document cluster as an initial model. [sent-39, score-0.272]
</p><p>24 Sentence-level topic mixtures are modeled so that the topic label is ﬁxed within a sentence. [sent-40, score-0.252]
</p><p>25 The paper is organized as follows: In Section 2, we describe the bigram LSA training and the fractional Kneser-Ney smoothing algorithm. [sent-43, score-0.775]
</p><p>26 In Section 3, we present the LM adaptation approach based on marginal adaptation and linear interpolation. [sent-44, score-0.285]
</p><p>27 In Section 4, we report LM adaptation results on Mandarin and Arabic ASR, followed by conclusions and future work in Section 5. [sent-45, score-0.126]
</p><p>28 2  Correlated bigram LSA  Latent semantic analysis such as LDA makes a bag-of-word assumption that each word in a document is generated irrespective of its position in a document. [sent-46, score-0.71]
</p><p>29 To relax this assumption, bigram LSA has been proposed [7] to modify the graphical structure of LDA by connecting adjacent words in a document together to form a Markov chain. [sent-47, score-0.647]
</p><p>30 Figure 1 shows the graphical representation of bigram LSA where the top node represents the prior distribution over the topic mixture weights and the middle layer represents the latent topic label associated to each observed word at the bottom layer. [sent-48, score-0.955]
</p><p>31 The document generation procedure of bigram LSA is similar to LDA except that the previous word is taken into consideration for generating the current word: 1. [sent-49, score-0.698]
</p><p>32 Secondly, we propose efﬁcient algorithm for bigram LSA training via variational Bayes approach and model bootstrapping which are scalable to large settings in Section 2. [sent-53, score-0.638]
</p><p>33 Thirdly, we formulate the fractional Kneser-Ney smoothing to generalize the original Kneser-Ney smoothing which supports only integral counts in Section 2. [sent-55, score-0.336]
</p><p>34 Right: Variational E-step as bottom-up propagation and summation of fractional topic counts. [sent-73, score-0.257]
</p><p>35 1  Topic correlation  Modeling topic correlations is motivated by an observation that documents such as newspaper articles are usually organized into main-topic and sub-topic hierarchy for document browsing. [sent-75, score-0.183]
</p><p>36 From this perspective, a Dirichlet prior is not appropriate since it assumes topic independence. [sent-76, score-0.126]
</p><p>37 A DirichletTree prior [13, 14] is employed to capture topic correlations. [sent-77, score-0.141]
</p><p>38 The sampling procedure for the topic mixture weight θ ∼ p(θ) can be described as follows: 1. [sent-80, score-0.139]
</p><p>39 Compute the topic mixture weight as θk = jc bjc where δjc (k) is an indicator function which sets to unity when the c-th branch of the j-th node leads to the leaf node of topic k and zero otherwise. [sent-88, score-0.658]
</p><p>40 The k-th topic weight θk is computed as the product of sampled branch probabilities from the root node to the leaf node corresponding to topic k. [sent-89, score-0.332]
</p><p>41 2  Model training  Gibbs sampling was employed for bigram LSA training [7]. [sent-93, score-0.609]
</p><p>42 Cd (u, v|k) denotes the fractional counts of a bigram (u, v) belonging to topic k in document d. [sent-109, score-0.922]
</p><p>43 Intuitively, Eqn 12 simply computes the relative frequency of the bigram (u, v). [sent-110, score-0.577]
</p><p>44 However, this solution is not practical since bigram LSA assigns zero probability to unseen bigrams. [sent-111, score-0.578]
</p><p>45 However, this approach can lead to worse performance since it will bias the bigram probability towards a uniform distribution when the vocabulary size V gets large. [sent-114, score-0.588]
</p><p>46 Our approach is to represent p(v|u, k) as a standard backoff LM smoothed by fractional Kneser-Ney smoothing as described in Section 2. [sent-115, score-0.264]
</p><p>47 We employ a bootstrapping approach using a well-trained unigram LSA as an initial model for bigram LSA so that p(wi |wi−1 , k) is approximated by p(wi |k) in Eqn 6. [sent-118, score-0.741]
</p><p>48 It saves computation and avoids keeping the full initial bigram LSA in memory during the EM training. [sent-119, score-0.58]
</p><p>49 To make the training procedure more practical, we apply bigram pruning during statistics accumulation in the M-step when the bigram count in a document is less than 0. [sent-120, score-1.22]
</p><p>50 With the sparsity, there is no need to store K copies of accumulators for each bigram and thus reducing the memory requirement signiﬁcantly. [sent-123, score-0.582]
</p><p>51 The pruned bigram counts are re-assigned to the most likely topic of the current document so that the counts are conserved. [sent-124, score-0.835]
</p><p>52 3  Fractional Kneser-Ney smoothing  Standard backoff N-gram LM is widely used in the ASR community. [sent-128, score-0.111]
</p><p>53 The state-of-the-art smoothing for the backoff LM is based on Kneser-Ney smoothing [9]. [sent-129, score-0.176]
</p><p>54 However, the original formulation only works for integral  counts which is not suitable for bigram LSA using fractional counts. [sent-131, score-0.766]
</p><p>55 Therefore, we propose the fractional Kneser-Ney smoothing as a generalization of the original formulation. [sent-132, score-0.196]
</p><p>56 N>D (u, ·) denotes the number of word types following u with the bigram counts bigger than D. [sent-140, score-0.685]
</p><p>57 For the original formulation, C≤D (u, ·) equals to zero since each observed bigram count must be at least one by deﬁnition with D less than one. [sent-143, score-0.584]
</p><p>58 In other words, fractional Kneser-Ney smoothing estimates the lower-order probability distribution using the relative frequency over discounts instead of word counts. [sent-146, score-0.304]
</p><p>59 With this approach, each topic-dependent LM in bigram LSA can be smoothed using our formulation. [sent-147, score-0.586]
</p><p>60 =⇒ pKN (v)  =  C(v) −  + pKN (v) ·  u  3  Unsupervised LM adaptation  Unsupervised LM adaptation is performed by ﬁrst inferring the topic distribution of each test audio using the word hypotheses from the ﬁrst-pass decoding via variational inference in Eqn 6–7. [sent-148, score-0.558]
</p><p>61 The MAP topic ˆ mixture weight θ and the adapted unigram and bigram LSA are computed as follows: ˆ θk  ∝ jc  γjc c γjc  δjc (k)  for k = 1. [sent-150, score-1.13]
</p><p>62 Intuitively, bigram LSA would be integrated in the same fashion by introducing bigram marginal constraints. [sent-154, score-1.161]
</p><p>63 However, we found that integrating bigram features via marginal adaptation did not offer further improvement compared to only integrating unigram features. [sent-155, score-0.872]
</p><p>64 Since marginal adaptation integrates a unigram feature as a likelihood ratio between the adapted marginal pa (v) and the background marginal pbg (v) in Eqn 28, perhaps the unigram and bigram likelihood ratios are very similar and thus the latter does not give extra information. [sent-156, score-1.201]
</p><p>65 Another explanation is that marginal adaptation corresponds to only one iteration of generalized iterative scaling (GIS). [sent-157, score-0.159]
</p><p>66 Due to the large number of bigram features in terms of millions, one GIS iteration may not be sufﬁcient for convergence. [sent-158, score-0.564]
</p><p>67 The ﬁnal LM adaptation formula is provided using results from Eqn 27 and Eqn 28 as a two-stage process: p(2) (v|h) a  = λ · p(1) (v|h) + (1 − λ) · pa (v|u) a  (29)  where λ is tuned to optimize perplexity on word hypotheses from the ﬁrst-pass decoding on a peraudio basis. [sent-160, score-0.31]
</p><p>68 4  Experimental setup  Our LM adaptation approach was evaluated using the RT04 Mandarin Broadcast News evaluation system. [sent-161, score-0.126]
</p><p>69 The system employed context-dependent Initial-Final acoustic models trained using 100hour broadcast news audio from the Mandarin HUB4 1997 training set and a subset of TDT4. [sent-162, score-0.162]
</p><p>70 For the second-pass decoding, we applied standard acoustic model adaptation such as vocal tract length normalization and maximum likelihood linear regression on the feature and model spaces. [sent-165, score-0.143]
</p><p>71 A background 4-gram LM was trained using modiﬁed Kneser-Ney smoothing using the SRILM toolkit [15]. [sent-167, score-0.122]
</p><p>72 The same training corpora were used for unigram and bigram LSA training with 200 topics. [sent-168, score-0.777]
</p><p>73 Discounting factor D for fractional Kneser-Ney smoothing was set to 0. [sent-170, score-0.196]
</p><p>74 Then unsupervised LM adaptation was applied using the automatic transcript to obtain an adapted LM for second-pass decoding using the approach described in Section 3. [sent-173, score-0.22]
</p><p>75 Table 1: Correlated bigram topics extracted from bigram LSA. [sent-176, score-1.164]
</p><p>76 Bigram LSA was applied in addition to unigram LSA. [sent-178, score-0.149]
</p><p>77 1  LM adaptation results  Table 1 shows the correlated bigram topics sorted by the joint bigram probability p(v|u, k) · p(u|k). [sent-200, score-1.311]
</p><p>78 Table 2 shows the LM adaptation results in CER and perplexity. [sent-202, score-0.126]
</p><p>79 Applying both unigram and bigram LSA yields consistent improvement over unigram LSA in the range of 6. [sent-203, score-0.862]
</p><p>80 We compared our proposed fractional Kneser-Ney smoothing with Witten-Bell smoothing which also supports fractional counts. [sent-209, score-0.408]
</p><p>81 Increasing the number of topics in bigram LSA helps despite model sparsity. [sent-211, score-0.6]
</p><p>82 We applied extra EM iterations on top of the bootstrapped bigram LSA but no further performance improvement was observed. [sent-212, score-0.564]
</p><p>83 Unigram and bigram LSA were trained on the same corpora and were applied to lattice rescoring on Dev07 and unseen Dev08 test sets with 2. [sent-216, score-0.665]
</p><p>84 6-hour and 3-hour audio shows containing broadcast news (BN) and broadcast conversation (BC) genre. [sent-217, score-0.158]
</p><p>85 Table 3 shows that bigram LSA rescoring reduces the overall word error rate by more than 3. [sent-218, score-0.669]
</p><p>86 However, degradation is observed using trigram LSA compared to bigram LSA which may be due to data sparseness. [sent-221, score-0.596]
</p><p>87 Table 3: Lattice rescoring results in word error rate on Dev07 (unseen Dev08) using the CMUInterACT Arabic transcription system for the GALE Phase-3 evaluation. [sent-222, score-0.121]
</p><p>88 0 (-)  5  Conclusion  We present a correlated bigram LSA approach for unsupervised LM adaptation for ASR. [sent-242, score-0.736]
</p><p>89 Our contributions include efﬁcient variational EM for model training and fractional Kneser-Ney approach for LM smoothing with fractional counts. [sent-243, score-0.373]
</p><p>90 Bigram LSA yields additional improvement in both perplexity and recognition performance in addition to unigram LSA. [sent-244, score-0.197]
</p><p>91 Increasing the number of topics for bigram LSA helps despite the model sparsity. [sent-245, score-0.6]
</p><p>92 Bootstrapping bigram LSA from unigram LSA saves computation and memory requirement during EM training. [sent-246, score-0.729]
</p><p>93 The improvement from bigram LSA is statistically signiﬁcant compared to the unadapted baseline. [sent-248, score-0.598]
</p><p>94 Acknowledgement We would like to thank Mark Fuhs for help parallelizing the bigram LSA training via condor. [sent-250, score-0.579]
</p><p>95 Schultz, “Language model adaptation using variational Bayes inference,” in Proceedings of Interspeech, 2005. [sent-265, score-0.157]
</p><p>96 Woodland, “Unsupervised language model adaptation for mandarin broadcast conversation transcription,” in Proceedings of Interspeech, 2006. [sent-269, score-0.29]
</p><p>97 Glass, “Style and topic language model adaptation using HMM-LDA,” in Proceedings of Empirical Methods on Natural Language Processing (EMNLP), 2006. [sent-278, score-0.305]
</p><p>98 Klakow, “Language model adaptation using dynamic marginals,” in Proceedings of European Conference on Speech Communication and Technology (EUROSPEECH), 1997, pp. [sent-293, score-0.126]
</p><p>99 Wei, “Topical N-grams: Phrase and topic discovery, with an application to information retrieval,” in IEEE International Conference on Data Mining, 2007. [sent-304, score-0.126]
</p><p>100 Schultz, “Correlated latent semantic model for unsupervised language model adaptation,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2007. [sent-310, score-0.115]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsa', 0.571), ('bigram', 0.564), ('lm', 0.33), ('jc', 0.258), ('unigram', 0.149), ('fractional', 0.131), ('adaptation', 0.126), ('topic', 0.126), ('eqn', 0.118), ('word', 0.077), ('smoothing', 0.065), ('document', 0.057), ('pkn', 0.056), ('bjc', 0.055), ('mandarin', 0.055), ('language', 0.053), ('wi', 0.051), ('perplexity', 0.048), ('backoff', 0.046), ('bigrams', 0.046), ('zi', 0.045), ('counts', 0.044), ('dir', 0.044), ('audio', 0.041), ('asr', 0.04), ('broadcast', 0.04), ('cer', 0.037), ('pbg', 0.037), ('topics', 0.036), ('corpora', 0.034), ('marginal', 0.033), ('branch', 0.032), ('trigram', 0.032), ('arabic', 0.032), ('topical', 0.032), ('dirichlet', 0.031), ('variational', 0.031), ('decoding', 0.031), ('eq', 0.03), ('background', 0.029), ('speech', 0.028), ('pa', 0.028), ('bootstrapping', 0.028), ('rescoring', 0.028), ('unsupervised', 0.025), ('latent', 0.025), ('bj', 0.025), ('tam', 0.024), ('gale', 0.024), ('vocabulary', 0.024), ('node', 0.024), ('lda', 0.023), ('interpolation', 0.022), ('smoothed', 0.022), ('correlated', 0.021), ('em', 0.021), ('news', 0.021), ('adapted', 0.02), ('count', 0.02), ('education', 0.02), ('discounting', 0.019), ('accumulators', 0.018), ('discounts', 0.018), ('gis', 0.018), ('interspeech', 0.018), ('kneser', 0.018), ('srilm', 0.018), ('tanja', 0.018), ('transcript', 0.018), ('unadapted', 0.018), ('schultz', 0.018), ('acoustic', 0.017), ('firstly', 0.017), ('supports', 0.016), ('statistically', 0.016), ('transcription', 0.016), ('employment', 0.016), ('saves', 0.016), ('conversation', 0.016), ('preservation', 0.016), ('unsmoothed', 0.016), ('character', 0.016), ('integral', 0.015), ('cd', 0.015), ('employed', 0.015), ('training', 0.015), ('emnlp', 0.015), ('toolkit', 0.015), ('unseen', 0.014), ('words', 0.014), ('reduction', 0.014), ('mixture', 0.013), ('relative', 0.013), ('trained', 0.013), ('lattice', 0.012), ('formulation', 0.012), ('adjacent', 0.012), ('cance', 0.012), ('proceedings', 0.012), ('semantic', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="52-tfidf-1" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>2 0.11273027 <a title="52-tfidf-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.078312241 <a title="52-tfidf-3" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>Author: Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1</p><p>4 0.073330082 <a title="52-tfidf-4" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>5 0.069688104 <a title="52-tfidf-5" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>6 0.064277329 <a title="52-tfidf-6" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>7 0.064127132 <a title="52-tfidf-7" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>8 0.063130826 <a title="52-tfidf-8" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>9 0.061591856 <a title="52-tfidf-9" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>10 0.056774754 <a title="52-tfidf-10" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>11 0.0561019 <a title="52-tfidf-11" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>12 0.052887052 <a title="52-tfidf-12" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>13 0.051087651 <a title="52-tfidf-13" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>14 0.050627995 <a title="52-tfidf-14" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>15 0.048562121 <a title="52-tfidf-15" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>16 0.043862946 <a title="52-tfidf-16" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>17 0.030871652 <a title="52-tfidf-17" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>18 0.029708289 <a title="52-tfidf-18" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>19 0.02875172 <a title="52-tfidf-19" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>20 0.027636802 <a title="52-tfidf-20" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.085), (1, -0.054), (2, 0.039), (3, -0.066), (4, -0.033), (5, -0.009), (6, 0.065), (7, 0.105), (8, -0.145), (9, 0.011), (10, -0.047), (11, 0.013), (12, -0.033), (13, 0.068), (14, -0.033), (15, 0.05), (16, 0.026), (17, 0.028), (18, -0.113), (19, -0.06), (20, 0.076), (21, -0.005), (22, -0.011), (23, 0.021), (24, -0.01), (25, 0.057), (26, 0.03), (27, 0.033), (28, -0.016), (29, 0.031), (30, 0.042), (31, 0.086), (32, 0.018), (33, 0.024), (34, 0.074), (35, -0.064), (36, 0.002), (37, -0.02), (38, 0.055), (39, -0.025), (40, 0.013), (41, -0.029), (42, -0.006), (43, -0.05), (44, 0.078), (45, -0.073), (46, -0.032), (47, 0.052), (48, 0.023), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.954404 <a title="52-lsi-1" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>2 0.72491527 <a title="52-lsi-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.67871487 <a title="52-lsi-3" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>Author: Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with signiﬁcant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced. 1</p><p>4 0.6351527 <a title="52-lsi-4" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>5 0.60468346 <a title="52-lsi-5" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>6 0.49627155 <a title="52-lsi-6" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>7 0.44897142 <a title="52-lsi-7" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>8 0.43862444 <a title="52-lsi-8" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>9 0.43645671 <a title="52-lsi-9" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>10 0.42518166 <a title="52-lsi-10" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>11 0.39167839 <a title="52-lsi-11" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>12 0.37269533 <a title="52-lsi-12" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>13 0.33958787 <a title="52-lsi-13" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>14 0.3218123 <a title="52-lsi-14" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>15 0.32050565 <a title="52-lsi-15" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>16 0.28724605 <a title="52-lsi-16" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>17 0.28281927 <a title="52-lsi-17" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>18 0.27853546 <a title="52-lsi-18" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>19 0.27215266 <a title="52-lsi-19" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>20 0.26550013 <a title="52-lsi-20" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.06), (7, 0.07), (12, 0.032), (28, 0.078), (32, 0.018), (57, 0.05), (59, 0.013), (63, 0.014), (77, 0.025), (78, 0.44), (83, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81499022 <a title="52-lda-1" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>2 0.76949072 <a title="52-lda-2" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<p>Author: Zaïd Harchaoui, Eric Moulines, Francis R. Bach</p><p>Abstract: We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an unlabelled sample of observations consists in, ﬁrst, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistic based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of a change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs, the test statistic also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented. 1</p><p>3 0.74998999 <a title="52-lda-3" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>4 0.67947978 <a title="52-lda-4" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>Author: Liang Zhang, Deepak Agarwal</p><p>Abstract: Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable that is organized hierarchically. Model ﬁtting is challenging, especially for a hierarchy with a large number of nodes. We provide a novel algorithm based on a multi-scale Kalman ﬁlter that is both scalable and easy to implement. For Gaussian response, we show our method provides the maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response, parameter estimation is performed through a Laplace approximation. However, the Laplace approximation provides biased parameter estimates that is corrected through a parametric bootstrap procedure. We illustrate through simulation studies and analyses of real world data sets in health care and online advertising.</p><p>5 0.37618083 <a title="52-lda-5" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>6 0.36046189 <a title="52-lda-6" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>7 0.35903785 <a title="52-lda-7" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>8 0.35379899 <a title="52-lda-8" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>9 0.35250428 <a title="52-lda-9" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>10 0.34688818 <a title="52-lda-10" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>11 0.34194353 <a title="52-lda-11" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>12 0.33955568 <a title="52-lda-12" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>13 0.33786711 <a title="52-lda-13" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>14 0.33769903 <a title="52-lda-14" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>15 0.33214891 <a title="52-lda-15" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>16 0.33213347 <a title="52-lda-16" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>17 0.32915047 <a title="52-lda-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.32688406 <a title="52-lda-18" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>19 0.32498068 <a title="52-lda-19" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>20 0.32400203 <a title="52-lda-20" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
