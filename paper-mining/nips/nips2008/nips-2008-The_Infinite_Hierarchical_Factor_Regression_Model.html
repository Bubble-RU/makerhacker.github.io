<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2008-The Infinite Hierarchical Factor Regression Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-235" href="#">nips2008-235</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>235 nips-2008-The Infinite Hierarchical Factor Regression Model</h1>
<br/><p>Source: <a title="nips-2008-235-pdf" href="http://papers.nips.cc/paper/3627-the-infinite-hierarchical-factor-regression-model.pdf">pdf</a></p><p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>Reference: <a title="nips-2008-235-reference" href="../nips2008_reference/nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ibp', 0.519), ('coalesc', 0.433), ('load', 0.285), ('bfrm', 0.208), ('apk', 0.182), ('tp', 0.173), ('dish', 0.156), ('nonparamet', 0.148), ('kingm', 0.139), ('spury', 0.121), ('hierarchy', 0.118), ('pathway', 0.115), ('buffet', 0.104), ('pri', 0.104), ('regress', 0.097), ('fact', 0.093), ('idiosyncr', 0.093), ('custom', 0.088), ('mse', 0.084), ('ti', 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="235-tfidf-1" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>2 0.31340578 <a title="235-tfidf-2" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>Author: Dilan Gorur, Yee W. Teh</p><p>Abstract: We propose an efﬁcient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model [1]. Our algorithm has a quadratic runtime while those in [1] is cubic. In experiments, we were surprised to ﬁnd that in addition to being more efﬁcient, it is also a better sequential Monte Carlo sampler than the best in [1], when measured in terms of variance of estimated likelihood and effective sample size. 1</p><p>3 0.2714017 <a title="235-tfidf-3" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>Author: Thomas L. Griffiths, Joseph L. Austerweil</p><p>Abstract: Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to deﬁne a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features. 1</p><p>4 0.23832555 <a title="235-tfidf-4" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>5 0.14040314 <a title="235-tfidf-5" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>6 0.12687033 <a title="235-tfidf-6" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>7 0.11532595 <a title="235-tfidf-7" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>8 0.076160975 <a title="235-tfidf-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.066880248 <a title="235-tfidf-9" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>10 0.064307079 <a title="235-tfidf-10" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>11 0.061998241 <a title="235-tfidf-11" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>12 0.06194948 <a title="235-tfidf-12" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>13 0.061748251 <a title="235-tfidf-13" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>14 0.061087858 <a title="235-tfidf-14" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>15 0.060745314 <a title="235-tfidf-15" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>16 0.060500696 <a title="235-tfidf-16" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>17 0.059988152 <a title="235-tfidf-17" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>18 0.059342146 <a title="235-tfidf-18" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>19 0.058304273 <a title="235-tfidf-19" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>20 0.057521854 <a title="235-tfidf-20" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.183), (1, 0.011), (2, -0.05), (3, 0.064), (4, -0.004), (5, 0.079), (6, 0.186), (7, -0.013), (8, 0.044), (9, -0.071), (10, -0.055), (11, -0.063), (12, 0.114), (13, 0.134), (14, 0.059), (15, -0.055), (16, 0.082), (17, 0.067), (18, 0.036), (19, 0.114), (20, 0.081), (21, -0.137), (22, 0.004), (23, -0.312), (24, -0.17), (25, 0.266), (26, 0.047), (27, -0.138), (28, 0.106), (29, 0.036), (30, -0.182), (31, 0.133), (32, 0.174), (33, -0.074), (34, 0.063), (35, -0.219), (36, -0.019), (37, 0.031), (38, 0.082), (39, 0.004), (40, 0.057), (41, -0.083), (42, -0.071), (43, -0.121), (44, -0.017), (45, 0.055), (46, -0.041), (47, 0.066), (48, 0.059), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89456856 <a title="235-lsi-1" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>2 0.83157247 <a title="235-lsi-2" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>Author: Dilan Gorur, Yee W. Teh</p><p>Abstract: We propose an efﬁcient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model [1]. Our algorithm has a quadratic runtime while those in [1] is cubic. In experiments, we were surprised to ﬁnd that in addition to being more efﬁcient, it is also a better sequential Monte Carlo sampler than the best in [1], when measured in terms of variance of estimated likelihood and effective sample size. 1</p><p>3 0.66485333 <a title="235-lsi-3" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>Author: Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe</p><p>Abstract: Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method. 1</p><p>4 0.61832887 <a title="235-lsi-4" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>5 0.50239795 <a title="235-lsi-5" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>Author: Thomas L. Griffiths, Joseph L. Austerweil</p><p>Abstract: Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to deﬁne a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features. 1</p><p>6 0.49192443 <a title="235-lsi-6" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>7 0.44403949 <a title="235-lsi-7" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>8 0.44310558 <a title="235-lsi-8" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>9 0.35319376 <a title="235-lsi-9" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>10 0.33795494 <a title="235-lsi-10" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>11 0.32444128 <a title="235-lsi-11" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>12 0.31972605 <a title="235-lsi-12" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>13 0.30082816 <a title="235-lsi-13" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>14 0.30043522 <a title="235-lsi-14" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>15 0.28519085 <a title="235-lsi-15" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>16 0.28166428 <a title="235-lsi-16" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>17 0.27197635 <a title="235-lsi-17" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>18 0.27149209 <a title="235-lsi-18" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>19 0.26553336 <a title="235-lsi-19" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>20 0.26389572 <a title="235-lsi-20" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.012), (14, 0.011), (25, 0.021), (28, 0.353), (29, 0.011), (30, 0.043), (38, 0.013), (40, 0.082), (60, 0.048), (63, 0.078), (64, 0.099), (71, 0.107), (92, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79841363 <a title="235-lda-1" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>Author: Sanmay Das, Malik Magdon-Ismail</p><p>Abstract: We study the proﬁt-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market. The sequential decision problem is hard to solve because the state space is a function. We demonstrate that the belief state is well approximated by a Gaussian distribution. We prove a key monotonicity property of the Gaussian state update which makes the problem tractable, yielding the ﬁrst optimal sequential market-making algorithm in an established model. The algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher proﬁts later. 1</p><p>same-paper 2 0.63106215 <a title="235-lda-2" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>3 0.61133373 <a title="235-lda-3" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>Author: Tony Jebara, Pannagadatta K. Shivaswamy</p><p>Abstract: In classiﬁcation problems, Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful, the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be eﬃciently solved and experiments on digit datasets show drastic performance improvements over SVMs. 1</p><p>4 0.58571994 <a title="235-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.47396708 <a title="235-lda-5" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>Author: Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani</p><p>Abstract: We introduce a new probability distribution over a potentially inﬁnite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the inﬁnite factorial hidden Markov model can be used for blind source separation. 1</p><p>6 0.47347683 <a title="235-lda-6" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>7 0.47275519 <a title="235-lda-7" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>8 0.47111627 <a title="235-lda-8" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>9 0.47034931 <a title="235-lda-9" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>10 0.46620846 <a title="235-lda-10" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>11 0.46354425 <a title="235-lda-11" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>12 0.4626382 <a title="235-lda-12" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>13 0.46215004 <a title="235-lda-13" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>14 0.45948482 <a title="235-lda-14" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>15 0.45793515 <a title="235-lda-15" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>16 0.45588422 <a title="235-lda-16" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>17 0.45573527 <a title="235-lda-17" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>18 0.45567656 <a title="235-lda-18" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>19 0.45547473 <a title="235-lda-19" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>20 0.45498633 <a title="235-lda-20" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
