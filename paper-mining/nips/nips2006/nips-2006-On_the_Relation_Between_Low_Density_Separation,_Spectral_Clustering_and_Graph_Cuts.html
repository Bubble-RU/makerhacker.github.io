<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-151" href="#">nips2006-151</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</h1>
<br/><p>Source: <a title="nips-2006-151-pdf" href="http://papers.nips.cc/paper/3000-on-the-relation-between-low-density-separation-spectral-clustering-and-graph-cuts.pdf">pdf</a></p><p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>Reference: <a title="nips-2006-151-reference" href="../nips2006_reference/nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. [sent-7, score-0.346]
</p><p>2 In this paper we provide some formal analysis of that notion for a probability distribution. [sent-8, score-0.148]
</p><p>3 We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. [sent-9, score-0.789]
</p><p>4 We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. [sent-10, score-0.294]
</p><p>5 keywords: Clustering, Semi-Supervised Learning  1  Introduction  Consider the probability distribution with density p(x) depicted in Fig. [sent-11, score-0.174]
</p><p>6 Asked to cluster this probability distribution, we would probably separate it into two roughly Gaussian bumps as shown in the left panel. [sent-13, score-0.147]
</p><p>7 Asked to point out more likely groups of data of the same type, we would be inclined to believe that these two bumps contain data points with the same labels. [sent-15, score-0.116]
</p><p>8 On the other hand, the class boundary shown in the right panel seems rather less likely. [sent-16, score-0.258]
</p><p>9 One way to state this basic intuition is the Low Density Separation assumption [5], saying that the class/cluster boundary tends to pass through regions of low density. [sent-17, score-0.347]
</p><p>10 In this paper we propose a formal measure on the complexity of the boundary, which intuitively corresponds to the Low Density Separation assumption. [sent-18, score-0.11]
</p><p>11 We will show that given a class boundary, this measure can be computed from a ﬁnite sample from the probability distribution. [sent-19, score-0.083]
</p><p>12 Moreover, we show this is done by computing the size of a cut for a partition of a certain standard adjacency graph, deﬁned on that sample, and point out some interesting connections to spectral clustering. [sent-20, score-0.673]
</p><p>13 To ﬁx our intuitions, let us consider the question of what makes the cut in the left panel more intuitively acceptable than the cut in the right. [sent-21, score-0.913]
</p><p>14 Two features of the left cut make it more pleasing: the cut is shorter in length and the cut  Figure 1: A likely cut and a less likely cut. [sent-22, score-1.502]
</p><p>15 Note that a very jagged cut through a low-density area or a short cut through the middle of a high-density bump would be unsatisfactory. [sent-24, score-0.761]
</p><p>16 It will therefore appear reasonable to take the length of the cut as a measure of its complexity but weight it depending on the density of the probability distribution p through which it passes. [sent-25, score-0.656]
</p><p>17 In other words, we propose the weighted length of the boundary, represented by the contour integral along the boundary cut p(s)ds to measure the complexity of a cut. [sent-26, score-0.752]
</p><p>18 It is clear that the boundary in the left panel has a considerably lower weighted length than the boundary in the right panel of our Fig. [sent-27, score-0.635]
</p><p>19 To formalize this notion further consider a (marginal) probability distribution with density p(x) supported on some domain or manifold M . [sent-29, score-0.369]
</p><p>20 Assuming that the boundary S is a smooth hypersurface we deﬁne the weighted volume of the cut to be S p(s)ds. [sent-31, score-0.766]
</p><p>21 Note that just as in the example above, the integral is taken over the surface of the boundary. [sent-32, score-0.052]
</p><p>22 We will show how this quantity can be approximated given empirical data and establish connections with some popular graph-based methods. [sent-33, score-0.105]
</p><p>23 1  Connections and related work Spectral Clustering  Over the last two decades there has been considerable interest in various spectral clustering techniques (see, e. [sent-35, score-0.289]
</p><p>24 The idea of spectral clustering can be expressed very simply. [sent-38, score-0.289]
</p><p>25 Given a graph, we would often like to construct a balanced partitioning of the vertex set, i. [sent-39, score-0.093]
</p><p>26 a partitioning such which minimizes the number (or total weight) of edges across the cut. [sent-41, score-0.045]
</p><p>27 It turns out, however, that a simple real-valued relaxation can be used to reduce it to standard linear algebra, typically to ﬁnding eigenvectors of a certain graph Laplacian. [sent-43, score-0.133]
</p><p>28 We note that the quality of partition is usually measured in terms of the corresponding cut size. [sent-44, score-0.437]
</p><p>29 A critical question, when this notion is applied to general purpose clustering in the context of machine learning is how to construct the graph given data points. [sent-45, score-0.315]
</p><p>30 To summarize, a graph is obtained from a point cloud, using Gaussian or other weights, and partitioned using spectral clustering or a different algorithm, which attempts to approximate the smallest (balanced) cut. [sent-49, score-0.46]
</p><p>31 2  Graph-based semi-supervised learning  Similarly to spectral clustering, graph-based semi-supervised learning constructs a graph from the data. [sent-52, score-0.283]
</p><p>32 The problem is typically to either label the unlabeled points (transduction) or, more generally, to build a classiﬁer deﬁned on the whole space. [sent-54, score-0.062]
</p><p>33 This may be done trying to ﬁnd the 2  Figure 2: Curves of small and high condition number respectively  minimum cut, which respects the labels of the data directly ([3]), or, using graph Laplacian as a penalty functional (e. [sent-55, score-0.133]
</p><p>34 One of the important intuitions of semi-supervised learning is the cluster assumption(e. [sent-58, score-0.11]
</p><p>35 , [4]) or, more speciﬁcally, the low density separation assumption suggested in [5], which states that the class boundary passes through a low density region. [sent-60, score-0.621]
</p><p>36 We argue that this intuition needs to slightly modiﬁed by suggesting that cutting through a high density region may be acceptable as long as the length of the cut is very short. [sent-61, score-0.677]
</p><p>37 Cutting the thread is appropriate as long as the width of the thread is much smaller than the radii of the clusters. [sent-63, score-0.142]
</p><p>38 3  Convergence of Manifold Laplacians  Another closely related line of research is the connections between point-cloud graph Laplacians and Laplace-Beltrami operators on manifolds, which have been explored recently in [9, 2, 10]. [sent-65, score-0.219]
</p><p>39 A typical result in that setting shows that for a ﬁxed function f and points sampled from a probability distribution on a manifold or a domain, the graph Laplacian applied to f converges to the manifold Laplace-Beltrami operator ∆M f . [sent-66, score-0.437]
</p><p>40 Even more importantly, this paper establishes an explicit connection between the point-cloud Laplacian applied to such characteristic functions (weighted graph cuts) and a geometric quantity, which is the weighted volume of the cut boundary. [sent-68, score-0.649]
</p><p>41 3  Summary of the Main Results  Let p be a probability density function on a domain M ⊆ Rd . [sent-70, score-0.218]
</p><p>42 Let S be a smooth hypersurface that separates M into two parts, S1 and S2 . [sent-71, score-0.062]
</p><p>43 The smoothness of S will be quantiﬁed by a condition number 1/τ , where τ is the radius of the largest ball that can be placed tangent to the manifold at any point, intersecting the manifold at only one point. [sent-72, score-0.411]
</p><p>44 Deﬁnition 1 Let Kt (x, y) be the heat kernel in Rd given by Kt (x, y) := Let Mt := Kt (x, x) =  1 e− (4πt)d/2  x−y  2  /4t  . [sent-74, score-0.247]
</p><p>45 , xN } be a set of N points chosen independently at random from p. [sent-79, score-0.033]
</p><p>46 Consider the complete graph whose vertices are associated with the points in X, and where the weight of the edge between xi and xj , i = j is given 3  by Wij = Kt (xi , xj ) Let W be the weight matrix. [sent-80, score-0.197]
</p><p>47 Let X1 = X ∩ S1 and X2 = X ∩ S2 be the data point which land in S1 and S2 respectively. [sent-81, score-0.031]
</p><p>48 p(s)ds, which measures the quality of the partition S in accordance with the weighted volume of the S boundary. [sent-87, score-0.223]
</p><p>49 f T Lf , which measures the quality of the empirical partition in terms of its cut size. [sent-89, score-0.437]
</p><p>50 Our main Theorem shows that after an appropriate scaling, the empirical cut size converges to the volume of the boundary. [sent-90, score-0.439]
</p><p>51 Theorem 1 Let the number of points, |X| = N tend to inﬁnity and {tN }∞ , be a sequence of values of t that tend to 0 zero such that tN > 11 . [sent-91, score-0.064]
</p><p>52 N t S √  π This theorem is proved by ﬁrst relating the empirical quantity N √t f T L(tN , X)f to a heat ﬂow across the relevant cut (on the continuous domain), and then relating the heat ﬂow to the measure of the cut. [sent-93, score-1.135]
</p><p>53 In theorem 2 we show that for a ﬁxed t, as the number of points |X| = N tends to inﬁnity, with probability 1, β(t, X) tends to α(t). [sent-98, score-0.346]
</p><p>54 In theorem 3 we show that α(t) can be made arbitrarily close to the weighted volume of the boundary by making t tend to 0. [sent-99, score-0.487]
</p><p>55 4  S  S 2  S 1  t1/2  Figure 3: Heat ﬂow α tends to  S  p(s)ds  √ Theorem 2 Let 0 < µ < 1. [sent-100, score-0.071]
</p><p>56 Then, there exist positive constants C1 , C2 depending only on p and S such that with probability greater than 1 − exp (−C1 N µ ) |β(t, X) − α(t)| < C2 1 + t  d+1 2  uα(t). [sent-102, score-0.134]
</p><p>57 (2)  S  By letting N → ∞ and tN → 0 at suitable rates and putting together theorems 2 and 3, we obtain the following theorem: √ Theorem 4 Let the number of random data points N → ∞, and tN → 0, at rates so that u := 1/ t2d+1 N 1−µ → 0. [sent-104, score-0.075]
</p><p>58 N0 is chosen to be a large enough integer so that an application of the union bound on all N > N0 , still gives us a probability exp (−C1 (N µ )) < δ, of the rate of convergence being worse than stated in Theorem 1. [sent-106, score-0.094]
</p><p>59 Theorem 2: We prove theorem 2 using a generalization of McDiarmid’s inequality from [7, 8]. [sent-108, score-0.234]
</p><p>60 McDiarmid’s inequality asserts that a function of a large number of independent random variables, that is not very inﬂuenced by the value of any one of these, takes a value close to its mean. [sent-109, score-0.046]
</p><p>61 In the generalization that we use, it is permitted that over a bad set that has a small probability mass, the function is highly inﬂuenced by some of the random variables. [sent-110, score-0.085]
</p><p>62 In our setting, it can be shown that our measure of a cut, f T Lf is such a function of the independent random points in X, and so the result is applicable. [sent-111, score-0.061]
</p><p>63 There is another step involved, since the mean √ f T Lf is not α, the quantity to which we wish to prove of π convergence. [sent-112, score-0.1]
</p><p>64 Therefore we need to prove that the mean E[ N √t f T L(t, X)f ] tends to α(t) as N tends to inﬁnity. [sent-113, score-0.184]
</p><p>65 M  Putting the last two facts together and using the Generalization of McDiarmid’s inequality from [7, 8], the result follows. [sent-117, score-0.046]
</p><p>66 Theorem 3: The quantity π Kt (x, y)ψt (x)ψt (y)dxdy α := t S1 S2  is similar to the heat that would ﬂow in time t from one part to another if the ﬁrst were heated proportional to p. [sent-119, score-0.305]
</p><p>67 Intuitively, the heat that would ﬂow from one part to the other in a small interval ought to be related to the volume of the boundary between these two parts, which in our setting is S p(s)ds. [sent-120, score-0.509]
</p><p>68 To prove this relationship, we bound α both above and below in terms of the weighted volume and condition number of the boundary. [sent-121, score-0.232]
</p><p>69 These bounds are obtained 1 by making comparisons with the “worst case”, given condition number τ , which is when S is a sphere of radius τ . [sent-122, score-0.065]
</p><p>70 In order to obtain a lower bound on α, we observe that if B2 is the nearest ball of radius τ contained in S1 to a point P in S2 that is within τ of S1 , Kt (x, P )ψt (x)ψt (P )dx ≥ S1  Kt (x, P )ψt (x)ψt (P )dx, B2  as in Figure 4. [sent-123, score-0.156]
</p><p>71 Similarly, to obtain an upper bound on α, we observe that if B1 is a ball or radius τ in S2 , tangent to B2 at the point of S nearest to P , Kt (x, P )ψt (x)ψt (P )dx ≤  Kt (x, P )ψt (x)ψt (P )dx. [sent-124, score-0.203]
</p><p>72 c B1  S1  We now indicate how a lower bound is obtained for Kt (x, P )ψt (x)ψt (P )dx. [sent-125, score-0.039]
</p><p>73 For this reason, only the portions of B2  x−P >R  near P contribute to the the integral Kt (x, P )ψt (x)ψt (P )dx. [sent-127, score-0.052]
</p><p>74 B2  It turns out that a good lower bound can be obtained by considering the integral over H2 instead, where H2 is a 2 halfspace whose boundary is at a distance τ − R from the center as in ﬁgure 4. [sent-128, score-0.341]
</p><p>75 7  An upper bound for Kt (x, P )ψt (x)ψt (P )dx c B1  is obtained along similar lines. [sent-131, score-0.039]
</p><p>76 5  Conclusion  In this paper we take a step towards a probabilistic analysis of graph based methods for clustering. [sent-133, score-0.133]
</p><p>77 The nodes of the graph are identiﬁed with data points drawn at random from an underlying probability distribution on a continuous domain. [sent-134, score-0.221]
</p><p>78 For a ﬁxed partition we show that the cut-size of the graph partition converges to the weighted volume of the boundary separating the two regions of the domain. [sent-135, score-0.616]
</p><p>79 If one is able to generalize our result uniformly over all partitions, this allows us to relate ideas around graph based partitioning to ideas surrounding Low Density Separation. [sent-137, score-0.178]
</p><p>80 The most important future direction would be to achieve similar results uniformly over balanced partitions. [sent-138, score-0.048]
</p><p>81 Chawla, “Learning from labeled and unlabeled data using graph mincuts“, ICML 2001. [sent-151, score-0.162]
</p><p>82 Kutin, TR-2002-04, “Extensions to McDiarmid’s inequality when differences are bounded with high probability. [sent-162, score-0.046]
</p><p>83 von Luxburg, From Graphs to Manifolds – Weak and Strong Pointwise Consistency of Graph Laplacians, COLT 2005. [sent-172, score-0.039]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kt', 0.54), ('cut', 0.365), ('heat', 0.247), ('boundary', 0.188), ('spectral', 0.15), ('tn', 0.143), ('clustering', 0.139), ('graph', 0.133), ('ds', 0.125), ('dz', 0.123), ('mcdiarmid', 0.123), ('density', 0.119), ('theorem', 0.116), ('manifold', 0.108), ('niyogi', 0.09), ('ow', 0.083), ('belkin', 0.08), ('laplacians', 0.079), ('weighted', 0.077), ('lf', 0.074), ('volume', 0.074), ('separation', 0.073), ('dx', 0.073), ('partition', 0.072), ('tends', 0.071), ('cuts', 0.071), ('kutin', 0.071), ('thread', 0.071), ('intuitions', 0.07), ('panel', 0.07), ('chicago', 0.068), ('radius', 0.065), ('diffusing', 0.062), ('dxdy', 0.062), ('hypersurface', 0.062), ('halfspace', 0.062), ('laplacian', 0.059), ('quantity', 0.058), ('lafon', 0.056), ('luxburg', 0.056), ('partha', 0.056), ('cutting', 0.056), ('probability', 0.055), ('bumps', 0.052), ('ball', 0.052), ('integral', 0.052), ('formal', 0.05), ('acceptable', 0.049), ('balanced', 0.048), ('tangent', 0.047), ('walks', 0.047), ('connections', 0.047), ('depending', 0.047), ('intuition', 0.046), ('inequality', 0.046), ('partitioning', 0.045), ('domain', 0.044), ('notion', 0.043), ('length', 0.042), ('prove', 0.042), ('low', 0.042), ('putting', 0.042), ('uenced', 0.042), ('cluster', 0.04), ('manifolds', 0.04), ('bound', 0.039), ('diffusion', 0.039), ('operators', 0.039), ('nity', 0.039), ('von', 0.039), ('adjacency', 0.039), ('partitioned', 0.038), ('passes', 0.038), ('wij', 0.037), ('relating', 0.037), ('asked', 0.035), ('il', 0.035), ('graphs', 0.033), ('points', 0.033), ('intuitively', 0.032), ('tend', 0.032), ('constants', 0.032), ('let', 0.032), ('vertices', 0.031), ('colt', 0.031), ('columbus', 0.031), ('mbelkin', 0.031), ('yale', 0.031), ('land', 0.031), ('mincuts', 0.031), ('chawla', 0.031), ('inclined', 0.031), ('intersecting', 0.031), ('jagged', 0.031), ('generalization', 0.03), ('unlabeled', 0.029), ('icml', 0.028), ('measure', 0.028), ('pleasing', 0.028), ('darker', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="151-tfidf-1" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>2 0.30034989 <a title="151-tfidf-2" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>3 0.28272367 <a title="151-tfidf-3" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>Author: Yonatan Amit, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: We describe and analyze an algorithmic framework for online classiﬁcation where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online hypothesis by deﬁning a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution for the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with online multiclass text categorization. Our experiments indicate that a combination of class-dependent features with the simultaneous projection method outperforms previously studied algorithms. 1</p><p>4 0.21618032 <a title="151-tfidf-4" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi</p><p>Abstract: Geometrically based methods for various tasks of machine learning have attracted considerable attention over the last few years. In this paper we show convergence of eigenvectors of the point cloud Laplacian to the eigenfunctions of the Laplace-Beltrami operator on the underlying manifold, thus establishing the ﬁrst convergence results for a spectral dimensionality reduction algorithm in the manifold setting. 1</p><p>5 0.17251666 <a title="151-tfidf-5" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>6 0.15085962 <a title="151-tfidf-6" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>7 0.13784733 <a title="151-tfidf-7" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>8 0.13594262 <a title="151-tfidf-8" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>9 0.12730899 <a title="151-tfidf-9" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>10 0.1239104 <a title="151-tfidf-10" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>11 0.12131169 <a title="151-tfidf-11" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>12 0.09505751 <a title="151-tfidf-12" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>13 0.092547894 <a title="151-tfidf-13" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>14 0.091473699 <a title="151-tfidf-14" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>15 0.081229188 <a title="151-tfidf-15" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>16 0.077444807 <a title="151-tfidf-16" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>17 0.076330893 <a title="151-tfidf-17" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>18 0.074803762 <a title="151-tfidf-18" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>19 0.069785416 <a title="151-tfidf-19" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>20 0.069422282 <a title="151-tfidf-20" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.239), (1, 0.139), (2, -0.095), (3, 0.318), (4, 0.051), (5, -0.081), (6, 0.011), (7, 0.073), (8, 0.039), (9, -0.131), (10, 0.199), (11, -0.058), (12, -0.116), (13, 0.122), (14, -0.067), (15, 0.037), (16, -0.047), (17, -0.04), (18, -0.071), (19, -0.144), (20, -0.168), (21, 0.022), (22, -0.138), (23, -0.135), (24, 0.062), (25, 0.072), (26, -0.044), (27, 0.117), (28, 0.01), (29, -0.134), (30, 0.012), (31, -0.078), (32, -0.139), (33, 0.064), (34, -0.041), (35, 0.015), (36, 0.145), (37, -0.066), (38, -0.15), (39, -0.059), (40, 0.136), (41, -0.004), (42, 0.061), (43, 0.022), (44, -0.089), (45, 0.102), (46, 0.111), (47, 0.004), (48, -0.032), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96033883 <a title="151-lsi-1" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>2 0.60463572 <a title="151-lsi-2" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>3 0.60274339 <a title="151-lsi-3" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>Author: Dengyong Zhou, Jiayuan Huang, Bernhard Schölkopf</p><p>Abstract: We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classiﬁcation on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. 1</p><p>4 0.53379041 <a title="151-lsi-4" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi</p><p>Abstract: Geometrically based methods for various tasks of machine learning have attracted considerable attention over the last few years. In this paper we show convergence of eigenvectors of the point cloud Laplacian to the eigenfunctions of the Laplace-Beltrami operator on the underlying manifold, thus establishing the ﬁrst convergence results for a spectral dimensionality reduction algorithm in the manifold setting. 1</p><p>5 0.50503486 <a title="151-lsi-5" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>Author: Rie K. Ando, Tong Zhang</p><p>Abstract: We consider a general form of transductive learning on graphs with Laplacian regularization, and derive margin-based generalization bounds using appropriate geometric properties of the graph. We use this analysis to obtain a better understanding of the role of normalization of the graph Laplacian matrix as well as the effect of dimension reduction. The results suggest a limitation of the standard degree-based normalization. We propose a remedy from our analysis and demonstrate empirically that the remedy leads to improved classiﬁcation performance.</p><p>6 0.46868491 <a title="151-lsi-6" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>7 0.45972106 <a title="151-lsi-7" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>8 0.40505457 <a title="151-lsi-8" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>9 0.40321249 <a title="151-lsi-9" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>10 0.38841471 <a title="151-lsi-10" href="./nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</a></p>
<p>11 0.37723386 <a title="151-lsi-11" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>12 0.34413067 <a title="151-lsi-12" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>13 0.30318215 <a title="151-lsi-13" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>14 0.29837832 <a title="151-lsi-14" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>15 0.29722747 <a title="151-lsi-15" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>16 0.29693156 <a title="151-lsi-16" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>17 0.2859115 <a title="151-lsi-17" href="./nips-2006-High-Dimensional_Graphical_Model_Selection_Using_%24%5Cell_1%24-Regularized_Logistic_Regression.html">92 nips-2006-High-Dimensional Graphical Model Selection Using $\ell 1$-Regularized Logistic Regression</a></p>
<p>18 0.28211281 <a title="151-lsi-18" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>19 0.28088424 <a title="151-lsi-19" href="./nips-2006-Hyperparameter_Learning_for_Graph_Based_Semi-supervised_Learning_Algorithms.html">93 nips-2006-Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</a></p>
<p>20 0.28047651 <a title="151-lsi-20" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.045), (3, 0.034), (7, 0.633), (9, 0.037), (22, 0.034), (44, 0.044), (57, 0.031), (65, 0.035), (69, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98663324 <a title="151-lda-1" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>2 0.92913687 <a title="151-lda-2" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><p>3 0.91125476 <a title="151-lda-3" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Author: Cheng-tao Chu, Sang K. Kim, Yi-an Lin, Yuanyuan Yu, Gary Bradski, Kunle Olukotun, Andrew Y. Ng</p><p>Abstract: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and uniﬁed way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Speciﬁcally, we show that algorithms that ﬁt the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors. 1</p><p>4 0.88970679 <a title="151-lda-4" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>5 0.83834231 <a title="151-lda-5" href="./nips-2006-The_Neurodynamics_of_Belief_Propagation_on_Binary_Markov_Random_Fields.html">190 nips-2006-The Neurodynamics of Belief Propagation on Binary Markov Random Fields</a></p>
<p>Author: Thomas Ott, Ruedi Stoop</p><p>Abstract: We rigorously establish a close relationship between message passing algorithms and models of neurodynamics by showing that the equations of a continuous Hopﬁeld network can be derived from the equations of belief propagation on a binary Markov random ﬁeld. As Hopﬁeld networks are equipped with a Lyapunov function, convergence is guaranteed. As a consequence, in the limit of many weak connections per neuron, Hopﬁeld networks exactly implement a continuous-time variant of belief propagation starting from message initialisations that prevent from running into convergence problems. Our results lead to a better understanding of the role of message passing algorithms in real biological neural networks.</p><p>6 0.80913228 <a title="151-lda-6" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>7 0.72845221 <a title="151-lda-7" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>8 0.64359218 <a title="151-lda-8" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>9 0.6051017 <a title="151-lda-9" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>10 0.60402822 <a title="151-lda-10" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>11 0.60278004 <a title="151-lda-11" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>12 0.60181457 <a title="151-lda-12" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>13 0.58862638 <a title="151-lda-13" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>14 0.5862506 <a title="151-lda-14" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>15 0.58478022 <a title="151-lda-15" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>16 0.58449018 <a title="151-lda-16" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>17 0.58384776 <a title="151-lda-17" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>18 0.58154935 <a title="151-lda-18" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>19 0.58032602 <a title="151-lda-19" href="./nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</a></p>
<p>20 0.57974792 <a title="151-lda-20" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
