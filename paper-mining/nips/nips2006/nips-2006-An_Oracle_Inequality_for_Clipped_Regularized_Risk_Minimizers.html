<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-30" href="#">nips2006-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</h1>
<br/><p>Source: <a title="nips-2006-30-pdf" href="http://papers.nips.cc/paper/3066-an-oracle-inequality-for-clipped-regularized-risk-minimizers.pdf">pdf</a></p><p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>Reference: <a title="nips-2006-30-reference" href="../nips2006_reference/nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 gov  Abstract We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. [sent-2, score-1.023]
</p><p>2 We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. [sent-3, score-0.6]
</p><p>3 Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. [sent-4, score-0.741]
</p><p>4 1  Introduction  The theoretical understanding of support vector machines (SVMs) and related kernel-based methods has been substantially improved in recent years. [sent-5, score-0.044]
</p><p>5 For example using Talagrand’s concentration inequality and local Rademacher averages it has recently been shown that SVMs for classiﬁcation can learn with rates up to n−1 under somewhat realistic assumptions on the data-generating distribution (see [9, 11] and the related work [2]). [sent-6, score-0.293]
</p><p>6 On the other hand, the oracle inequality in [2] only holds for distributions having Tsybakov noise exponent ∞, and hence it describes a situation which is rarely met in practice. [sent-9, score-0.649]
</p><p>7 The goal of this work is to overcome these shortcomings by establishing a general oracle inequality (see Theorem 3. [sent-10, score-0.564]
</p><p>8 The key ingredient of this oracle inequality is the observation that for most commonly used loss functions it is possible to “clip” the decision function of the algorithm before beginning with the theoretical analysis. [sent-12, score-0.625]
</p><p>9 In addition, a careful choice of the weighted empirical process Talagrand’s inequality is applied to, makes the “shrinking technique” superﬂuous. [sent-13, score-0.246]
</p><p>10 Finally, by explicitly dealing with -approximate minimizers of the regularized risk our results also apply to actual SVM algorithms. [sent-14, score-0.14]
</p><p>11 With the help of the general oracle inequality we then establish an oracle inequality for SVM type algorithms (see Theorem 2. [sent-15, score-1.104]
</p><p>12 1) as well as a simple oracle inequality for model selection (see Theorem 4. [sent-16, score-0.576]
</p><p>13 For the former, we show that it leads to improved rates for e. [sent-18, score-0.058]
</p><p>14 Using the model selection theorem we then show how our new oracle inequality for SVMs can be used to analyze a simple parameter selection procedure based on a validation set that achieves the same learning rates without prior knowledge on the noise exponents. [sent-21, score-0.771]
</p><p>15 The rest of this work is organized as follows: In Section 2 we present our oracle inequality for SVM type algorithms. [sent-22, score-0.542]
</p><p>16 We then discuss its implications and analyze the simple parameter selection  procedure when using Gaussian RBF kernels. [sent-23, score-0.034]
</p><p>17 In Section 3 we then present and prove the general oracle inequality. [sent-24, score-0.327]
</p><p>18 1 as well as the oracle inequality for model selection can be found in Section 4. [sent-26, score-0.576]
</p><p>19 2  Main Results  Throughout this work we assume that X is compact metric space, Y ⊂ [−1, 1] is compact, P is a Borel probability measure on X × Y , and F is a set of functions over X such that 0 ∈ F. [sent-27, score-0.053]
</p><p>20 Often F is a reproducing kernel Hilbert space (RKHS) H of continuous functions over X with closed unit ball BH . [sent-28, score-0.082]
</p><p>21 It is well-known that H can then be continuously embedded into the space of continuous functions C(X) equipped with the usual maximum-norm . [sent-29, score-0.078]
</p><p>22 In order to avoid constants we always assume that this embedding has norm 1, i. [sent-31, score-0.048]
</p><p>23 The functions L will serve as loss functions and consequently let us recall that the associated L-risk of a measurable function f : X → R is deﬁned by RL,P (f ) = E(x,y)∼P L y, f (x) . [sent-37, score-0.288]
</p><p>24 The learning schemes we are mainly interested in are based on an optimization problem of the form fP,λ := arg min λ f f ∈H  2 H  + RL,P (f ) ,  (1)  where λ > 0. [sent-43, score-0.043]
</p><p>25 , (xn , yn )) ∈ (X × Y )n with its empirical measure, then fT,λ denotes the empirical estimators of the above learning scheme. [sent-47, score-0.062]
</p><p>26 Furthermore in order to deal with the complexity of the used RKHSs let us recall that for a subset A ⊂ E of a Banach space E the covering numbers are deﬁned by n  N (A, ε, E) := min n ≥ 1 : ∃x1 , . [sent-54, score-0.103]
</p><p>27 , xn ∈ E with A ⊂  (xi + εBE ) ,  ε > 0,  i=1  where BE denotes the closed unit ball of E. [sent-57, score-0.058]
</p><p>28 For our main results we are particularly interested in covering numbers in the Hilbert space L2 (TX ) which consists of all equivalence classes of functions f : X × Y → R and which is equipped with the norm f  L2 (TX )  1 := n  n  f (xi )  2  1 2  . [sent-65, score-0.127]
</p><p>29 Learning schemes of the form (1) typically produce functions fP,λ with limλ→0 fP,λ ∞ = ∞ (see e. [sent-70, score-0.055]
</p><p>30 Unfortunately, this behaviour has a serious negative impact on the learning rates when directly employing standard tool’s such as Hoeffding’s, Bernstein’s or Talagrand’s inequality. [sent-73, score-0.058]
</p><p>31 the hinge loss it is obvious that clipping the function fP,λ at −1 and 1 does not worsen the corresponding risks. [sent-76, score-0.169]
</p><p>32 Following this simple observation we will consider loss functions L that satisfy the clipping condition L(y, t) ≥  L(y, 1) L(y, −1)  if t ≥ 1 if t ≤ −1 ,  (3)  for all y ∈ Y . [sent-77, score-0.157]
</p><p>33 Recall that this type of loss function was already considered in [4, 11], but the clipping idea actually goes back to [1]. [sent-78, score-0.125]
</p><p>34 Moreover, it is elementary to check that most commonly used loss functions including the hinge loss and the least squares loss satisfy (3). [sent-79, score-0.261]
</p><p>35 Given a function f : X → R ˆ we now deﬁne its clipped version f : X → [−1, 1] by  if f (x) > 1 1 ˆ f (x) := f (x) if f (x) ∈ [−1, 1]  −1 if f (x) < −1 . [sent-80, score-0.101]
</p><p>36 ˆ It is clear from (3) that we always have L(y, f (x)) ≤ L(y, f (x)) and consequently we obtain ˆ) ≤ RL,P (f ) for all distributions P . [sent-81, score-0.047]
</p><p>37 |t1 − t2 | y∈Y,−1≤t1 ,t2 ≤1 sup  (4)  With the help of these deﬁnitions we can now state our main result which establishes an oracle inequality for clipped versions of fT,λ : Theorem 2. [sent-83, score-0.814]
</p><p>38 1 Let P be a distribution on X × Y and let L be a loss function which satisﬁes (3) and (4). [sent-84, score-0.079]
</p><p>39 sup  (5)  x∈X,y∈Y  In addition, we assume that we have a variance bound of the form ∗ ˆ EP L ◦ f − L ◦ fL,P  2  ∗ ˆ ≤ v EP (L ◦ f − L ◦ fL,P )  ϑ  (6)  for constants v ≥ 1, ϑ ∈ [0, 1] and all measurable f : X → R. [sent-87, score-0.313]
</p><p>40 Moreover, suppose that H satisﬁes sup  log N BH , ε, L2 (TX ) ≤ aε−2p ,  ε > 0,  (7)  T ∈(X×Y )n  for some constants p ∈ (0, 1) and a ≥ 1. [sent-88, score-0.219]
</p><p>41 Then there exists a constant Kp,v depending only on p H and v such that for all τ ≥ 1 we have with probability not less than 1 − 3e−τ that ˆ RL,P (fT,λ ) − R∗ L,P  ≤  Kp,v a λp n  1 2−ϑ+p(ϑ−1)  +  Kp,v a 32vτ +5 λp n n  + 8a(f0 ) + 4 . [sent-90, score-0.096]
</p><p>42 1 2−ϑ  +  140τ 14B(f0 )τ + n 3n (8)  The above oracle inequality has some interesting consequences as the following examples illustrate. [sent-91, score-0.542]
</p><p>43 2 (Learning rates for single kernel) Assume that in Theorem 2. [sent-93, score-0.058]
</p><p>44 1 we have a Lipschitz continuous loss function such as the hinge loss. [sent-94, score-0.114]
</p><p>45 In addition assume that the approximation error function satisﬁes a(λ) ≤ cλβ , λ > 0, for some constants c > 0 and β ∈ (0, 1]. [sent-95, score-0.072]
</p><p>46 The next example investigates SVMs that use a Gaussian RBF kernel whose width may vary with the sample size: Example 2. [sent-98, score-0.041]
</p><p>47 Furthermore assume that we are interested in binary classiﬁcation using the hinge  2  2  loss and the Gaussian RKHSs Hσ that belong to the RBF kernels kσ (x1 , x2 ) := e−σ x1 −x2 with width σ > 0. [sent-100, score-0.137]
</p><p>48 If P has geometric noise exponent α ∈ (0, ∞) in the sense of [9] then it was shown in [9] that there exists a function f0 ∈ Hσ with f0 ∞ ≤ 1 and aσ (f0 ) ≤ c σ d λ + σ −αd ,  σ > 0, λ > 0,  where c > 0 is a constant independent of λ and σ. [sent-101, score-0.108]
</p><p>49 Now assume that P has Tsybakov noise exponent q ∈ [0, ∞] in the sense of [9]. [sent-105, score-0.081]
</p><p>50 Note that these rates are superior to those obtained in [9, Theorem 2. [sent-108, score-0.058]
</p><p>51 However, these optimal parameters require us to know certain characteristics of the distribution such as the approximation exponent β or the noise exponents α and q. [sent-111, score-0.137]
</p><p>52 The following example shows that the oracle inequality of Theorem 2. [sent-112, score-0.542]
</p><p>53 We write T0 for the ﬁrst n samples and T1 for the last n samples. [sent-115, score-0.034]
</p><p>54 Moreover, let Σ ⊂ [1, n1/d ) and Λ ⊂ (0, 1] be ﬁnite sets with cardinality mΣ and mΛ , respectively. [sent-117, score-0.052]
</p><p>55 Now using a simple model selection approach (see e. [sent-120, score-0.034]
</p><p>56 With such parameter sets it is then easy to check that we obtain exactly the rates we have found in Example 2. [sent-127, score-0.058]
</p><p>57 3, but without knowing the noise exponents α and q a-priori. [sent-128, score-0.107]
</p><p>58 3  An oracle inequality for clipped penalized ERM  Theorem 2. [sent-129, score-0.662]
</p><p>59 1 is a consequence of a far more general oracle inequality on clipped penalized empirical risk minimizers. [sent-130, score-0.726]
</p><p>60 Since this result is of its own interest we now present it together with its proof in detail. [sent-131, score-0.04]
</p><p>61 To √ end recall that a subroot is a nondecreasing function ϕ : [0, ∞) → [0, ∞) such this that ϕ(r)/ r is nonincreasing in r. [sent-132, score-0.131]
</p><p>62 Now the general oracle inequality is:  Theorem 3. [sent-137, score-0.542]
</p><p>63 1 Let P = ∅ be a set of (hyper)-parameters, F be a set of measurable functions f : X → R with 0 ∈ F, and Ω : P × F → [0, ∞] be a function. [sent-138, score-0.103]
</p><p>64 Let P be a distribution on X × Y and L be a loss function which satisﬁes (3) and (4). [sent-139, score-0.051]
</p><p>65 In addition, we assume that we have a variance bound of the form (6) for constants v ≥ 1, ϑ ∈ [0, 1] and all measurable f : X → R. [sent-142, score-0.142]
</p><p>66 Furthermore, suppose that there exists a subroot ϕn with ∗ ˆ Rσ (L ◦ f − L ◦ fL,P ) ≤ ϕn (r) ,  sup  ET ∼P n Eσ∼ν  r > 0. [sent-143, score-0.283]
</p><p>67 Then for all τ ≥ 1 and all r satisfying r ≥ max 120ϕn (r),  32vτ n  1 2−ϑ  ,  28τ n  (10)  we have with probability not less than 1 − 3e−τ that ˆ Ω(pT,Ω , fT,Ω ) + RL,P (fT,Ω ) − R∗ ≤ 5r + L,P  14B(f0 )τ + 8aΩ (p0 , f0 ) + 4 . [sent-145, score-0.048]
</p><p>68 2] shows that with probability not less than 1 − e−τ we have ET h1 − EP h1 < Now using  √  ab ≤  a 2  +  b 2  we ﬁnd  √  2τ B EP h1 2Bτ + . [sent-154, score-0.076]
</p><p>69 n 3n 1  2τ BEP h1 · n− 2 ≤ EP h1 +  Bτ 2n  , and consequently we have  7Bτ ˆ ˆ ≥ 1−e−τ . [sent-155, score-0.047]
</p><p>70 In addition, our variance bound gives EP (h2 − EP h2 )2 ≤ EP h2 ≤ v(EP h2 )ϑ , and consequently, Bernstein’s inequality shows that with probability not less 2 than 1 − e−τ we have 2τ v(EP h2 )ϑ 4τ ET h2 − EP h2 < + . [sent-159, score-0.286]
</p><p>71 n n −1 −1 Now, for q −1 + (q ) = 1 the elementary inequality ab ≤ aq q −1 + bq (q ) holds, and hence for √ 1 ϑ/2 2 2 q := 2−ϑ , q := ϑ , a := 21−ϑ ϑϑ τ v · n− 2 , and b := 2EP h2 we obtain ϑ 2τ v(EP h2 )ϑ ϑ ≤ 1− n 2  21−ϑ ϑϑ vτ n  1 2−ϑ  + EP h2 . [sent-160, score-0.301]
</p><p>72 1 2−ϑ  Since elementary calculations show that 2−ϑ ϑϑ  2τ v(EP h2 )ϑ ϑ ≤ 1− n 2  ≤ 1 we obtain 2vτ n  1 2−ϑ  + EP h2 . [sent-161, score-0.032]
</p><p>73 Therefore we have with probability not less than 1 − e−τ that 1 2vτ 2−ϑ 4τ + . [sent-162, score-0.048]
</p><p>74 To this end we write hf := L ◦ f − L ◦ fL,P , f ∈ F. [sent-164, score-0.516]
</p><p>75 Moreover, for r > 0 we deﬁne  ∗ ∗ ˆ ˆ RL,T (f0 ) − RL,T (fL,P ) − RL,P (f0 ) + RL,P (fL,P ) < EP h2 + 1 −  EP hf − hf : (p, f ) ∈ P × F . [sent-165, score-0.964]
</p><p>76 Ω(p, f ) + EP (hf ) + r  Gr :=  EP hf −hf Ω(p,f )+EP (hf )+r  Then for gp,f := gp,f  ∞  = sup z∈Z  ϑ 2  ∈ Gr we have EP gp,f = 0 and  EP hf − hf (z) EP hf − hf ∞ 6 = ≤ . [sent-166, score-2.581]
</p><p>77 Ω(p, f ) + EP (hf ) + r Ω(p, f ) + EP (hf ) + r r  In addition, the inequality aϑ b2−ϑ ≤ (a + b)2 and the variance bound assumption (6) implies that 2 EP gp,f ≤  EP h2 EP h2 v f f ≤ 2−ϑ ≤ 2−ϑ . [sent-167, score-0.238]
</p><p>78 (EP (hf ) + r)2 r (EP hf )ϑ r  Now deﬁne Φ(r) := ET ∼P n  sup (p,f )∈P×F  EP hf − ET hf . [sent-168, score-1.617]
</p><p>79 Ω(p, f ) + EP (hf ) + r  Standard symmetrization then yields ET ∼P n  |EP hf − ET hf | ≤ 2ET ∼P n Eσ∼ν  sup (p,f )∈P×F Ω(p,f )+EP (hf )≤r  sup  |Rσ hf | ,  (p,f )∈P×F Ω(p,f )+EP (hf )≤r  and hence Lemma 3. [sent-169, score-1.814]
</p><p>80 Therefore applying Talagrand’s inequality in the version of [3] to the class Gr we obtain P n T ∈ Z n : sup ET g ≤ g∈Gr  30ϕn (r) + r  2τ v 7τ + nr2−ϑ nr  ≥ 1 − e−τ . [sent-171, score-0.428]
</p><p>81 1/2  7τ 2τ v n Let us deﬁne εr := 30ϕr (r) + nr2−ϑ + nr . [sent-172, score-0.042]
</p><p>82 This shows 1 − εr ≥ 1 , and hence we obtain with probability not less than 1 − 3e−τ that nr 4 4  +  1 2−ϑ  2vτ 32τ vrϑ + 2(2 − ϑ) n n  ˆ Ω(pT,Ω , fT,Ω ) + RL,P (fT,Ω ) − R∗ ≤ 120ϕn (r) + L,P  +  44τ n  14Bτ + 4aΩ (p0 , f0 ) + 4 RL,P (f0 )−R∗ L,P + 4 . [sent-176, score-0.116]
</p><p>83 3n ϑ  vr However we also have 120ϕn (r) ≤ r, 32τn r 2(2 − ϑ) 4 ≤ r, and hence we ﬁnd the assertion. [sent-177, score-0.133]
</p><p>84 1/2  44τ n  ≤ r,  5r 3 ,  ≤  and 2(2 − ϑ)  1 2−ϑ  2vτ n  ≤  For the proof of Theorem 3. [sent-178, score-0.04]
</p><p>85 Deﬁne ET W (f ) − EP W (f ) Φ(r) := ET ∼P n sup a(p, f ) + r f ∈P×F and suppose that there exists a subroot Ψ such that sup  ET ∼P n  ET W (f ) − EP W (f ) ≤ Ψ(r) ,  r > 0. [sent-183, score-0.454]
</p><p>86 r xi + 1 i=0  However since Ψ is a subroot we obtain that Ψ(rxi+1 ) ≤ x by setting x := 4. [sent-186, score-0.085]
</p><p>87 1 let us state the following proposition which follows directly from [8] (see also [9, Prop. [sent-189, score-0.066]
</p><p>88 7]) together with simple considerations on covering numbers: Proposition 4. [sent-191, score-0.048]
</p><p>89 1 Let F := H be a RKHS, P := {p0 } be a singleton, and Ω(p0 , f ) := λ f is satisﬁed then there exists a constant cp depending only on p such that (9) is satisﬁed for 1  ϑ  ϕn (r) := cp max v 2 (1−p) r 2 (1−p)  r λ  p 2  a n  1 2  ,  r λ  p 1+p  a n  1 1+p  . [sent-192, score-0.122]
</p><p>90 1: From the covering bound assumption we observe that Proposition 4. [sent-195, score-0.071]
</p><p>91 1 2 (1−p)  ϑ 2 (1−p)  p 1+p  a n  1 1+p  ,  32vτ n  1 2−ϑ  ,  28τ n  (18)  Finally, for the parameter selection approach in Example 2. [sent-199, score-0.034]
</p><p>92 4 we need the following oracle inequality for model selection: Theorem 4. [sent-200, score-0.542]
</p><p>93 2 Let P be a distribution on X × Y and let L be a loss function which satisﬁes (3), (4), and the variance bound (6). [sent-201, score-0.102]
</p><p>94 , fm } be a ﬁnite set of functions mapping X into [−1, 1]. [sent-205, score-0.032]
</p><p>95 f ∈F  Then there exists a universal constant K such that for all τ ≥ 1 we have with probability not less than 1 − 3e−τ that RL,P (fT ) − R∗ L,P  ≤ 5  K log m n  1 2−ϑ  +5  32vτ n  1 2−ϑ  +  5K log m + 154τ n  +8 min(RL,P (f ) − R∗ ) . [sent-207, score-0.098]
</p><p>96 L,P f ∈F  Proof: Since all functions fi already map into [−1, 1] we do not have to consider the clipping operator. [sent-208, score-0.106]
</p><p>97 Then the cardinality of L,P ∗ Fr is smaller than or equal to m and hence we have N (L ◦ Fr − L ◦ fL,P , ε, L2 (T )) ≤ m for all ε > 0. [sent-210, score-0.05]
</p><p>98 7]) we hence obtain that (9) is satisﬁed for c ϕn (r) := √ max n  log m v log m rϑ/2 , √ n  ,  where c is a universal constant. [sent-214, score-0.049]
</p><p>99 A Bennet concentration inequality and its application to suprema of empirical processes. [sent-232, score-0.266]
</p><p>100 Fast rates for support vector machines using Gaussian kernels. [sent-275, score-0.102]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ep', 0.535), ('hf', 0.482), ('oracle', 0.327), ('ft', 0.265), ('inequality', 0.215), ('sup', 0.171), ('rxi', 0.17), ('vr', 0.107), ('fp', 0.104), ('clipped', 0.101), ('subroot', 0.085), ('talagrand', 0.085), ('pt', 0.084), ('tx', 0.074), ('clipping', 0.074), ('theorem', 0.074), ('measurable', 0.071), ('gr', 0.068), ('et', 0.065), ('rates', 0.058), ('exponents', 0.056), ('exponent', 0.052), ('fr', 0.051), ('minimizers', 0.051), ('steinwart', 0.051), ('loss', 0.051), ('satis', 0.049), ('covering', 0.048), ('constants', 0.048), ('consequently', 0.047), ('bernstein', 0.045), ('hinge', 0.044), ('svms', 0.044), ('alamos', 0.043), ('bh', 0.043), ('rkhss', 0.043), ('tsybakov', 0.043), ('nr', 0.042), ('proof', 0.04), ('svm', 0.039), ('proposition', 0.038), ('regularized', 0.038), ('bep', 0.037), ('cp', 0.037), ('ying', 0.034), ('write', 0.034), ('selection', 0.034), ('rbf', 0.034), ('risk', 0.033), ('elementary', 0.032), ('functions', 0.032), ('ball', 0.031), ('empirical', 0.031), ('furthermore', 0.03), ('rkhs', 0.03), ('los', 0.03), ('shrinking', 0.03), ('obviously', 0.029), ('noise', 0.029), ('let', 0.028), ('rademacher', 0.028), ('ab', 0.028), ('lipschitz', 0.028), ('moreover', 0.028), ('exists', 0.027), ('recall', 0.027), ('equipped', 0.027), ('less', 0.027), ('xn', 0.027), ('hence', 0.026), ('support', 0.025), ('cardinality', 0.024), ('addition', 0.024), ('bound', 0.023), ('arbitrarily', 0.023), ('universal', 0.023), ('schemes', 0.023), ('width', 0.022), ('knowing', 0.022), ('establishing', 0.022), ('depending', 0.021), ('probability', 0.021), ('interested', 0.02), ('es', 0.02), ('concentration', 0.02), ('classi', 0.02), ('establish', 0.02), ('wu', 0.019), ('penalized', 0.019), ('machines', 0.019), ('continuous', 0.019), ('hilbert', 0.019), ('nondecreasing', 0.019), ('banach', 0.019), ('attaining', 0.019), ('hoeffding', 0.019), ('hush', 0.019), ('investigates', 0.019), ('lemma', 0.018), ('dealing', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="30-tfidf-1" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>2 0.14902875 <a title="30-tfidf-2" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>Author: Mark Girolami, Mingjun Zhong</p><p>Abstract: By adopting Gaussian process priors a fully Bayesian solution to the problem of integrating possibly heterogeneous data sets within a classiﬁcation setting is presented. Approximate inference schemes employing Variational & Expectation Propagation based methods are developed and rigorously assessed. We demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classiﬁer combination. 1</p><p>3 0.12492105 <a title="30-tfidf-3" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>4 0.12476881 <a title="30-tfidf-4" href="./nips-2006-implicit_Online_Learning_with_Kernels.html">203 nips-2006-implicit Online Learning with Kernels</a></p>
<p>Author: Li Cheng, Dale Schuurmans, Shaojun Wang, Terry Caelli, S.v.n. Vishwanathan</p><p>Abstract: We present two new algorithms for online learning in reproducing kernel Hilbert spaces. Our ﬁrst algorithm, ILK (implicit online learning with kernels), employs a new, implicit update technique that can be applied to a wide variety of convex loss functions. We then introduce a bounded memory version, SILK (sparse ILK), that maintains a compact representation of the predictor without compromising solution quality, even in non-stationary environments. We prove loss bounds and analyze the convergence rate of both. Experimental evidence shows that our proposed algorithms outperform current methods on synthetic and real data. 1</p><p>5 0.092801839 <a title="30-tfidf-5" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><p>6 0.088303357 <a title="30-tfidf-6" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>7 0.084485948 <a title="30-tfidf-7" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>8 0.057191197 <a title="30-tfidf-8" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>9 0.054492462 <a title="30-tfidf-9" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>10 0.050512608 <a title="30-tfidf-10" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>11 0.050232604 <a title="30-tfidf-11" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>12 0.049360801 <a title="30-tfidf-12" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>13 0.049229749 <a title="30-tfidf-13" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>14 0.048710458 <a title="30-tfidf-14" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>15 0.047732685 <a title="30-tfidf-15" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>16 0.046830501 <a title="30-tfidf-16" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<p>17 0.043974753 <a title="30-tfidf-17" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>18 0.043969378 <a title="30-tfidf-18" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>19 0.04194035 <a title="30-tfidf-19" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>20 0.041500505 <a title="30-tfidf-20" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, 0.041), (2, -0.137), (3, 0.011), (4, -0.034), (5, 0.126), (6, 0.013), (7, -0.013), (8, -0.023), (9, 0.021), (10, 0.058), (11, 0.009), (12, -0.006), (13, -0.064), (14, 0.017), (15, 0.051), (16, 0.007), (17, -0.008), (18, -0.034), (19, -0.065), (20, -0.013), (21, 0.191), (22, -0.006), (23, 0.05), (24, -0.045), (25, -0.099), (26, -0.041), (27, -0.024), (28, -0.031), (29, 0.115), (30, 0.068), (31, -0.029), (32, 0.002), (33, 0.063), (34, 0.057), (35, -0.11), (36, -0.072), (37, -0.18), (38, 0.02), (39, -0.018), (40, -0.036), (41, -0.077), (42, -0.098), (43, 0.016), (44, -0.01), (45, 0.103), (46, 0.079), (47, 0.07), (48, 0.01), (49, -0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94796968 <a title="30-lsi-1" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>2 0.68260437 <a title="30-lsi-2" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after nν iterations—for sample size n and ν < 1—the sequence of risks of the classiﬁers it produces approaches the Bayes risk if Bayes risk L∗ > 0. 1</p><p>3 0.52497256 <a title="30-lsi-3" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>4 0.50763452 <a title="30-lsi-4" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><p>5 0.47243071 <a title="30-lsi-5" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><p>6 0.44525599 <a title="30-lsi-6" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<p>7 0.42358071 <a title="30-lsi-7" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>8 0.38852614 <a title="30-lsi-8" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>9 0.3780973 <a title="30-lsi-9" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>10 0.3473323 <a title="30-lsi-10" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<p>11 0.34349144 <a title="30-lsi-11" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>12 0.33983147 <a title="30-lsi-12" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>13 0.32221484 <a title="30-lsi-13" href="./nips-2006-implicit_Online_Learning_with_Kernels.html">203 nips-2006-implicit Online Learning with Kernels</a></p>
<p>14 0.30089715 <a title="30-lsi-14" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>15 0.29955643 <a title="30-lsi-15" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>16 0.29396749 <a title="30-lsi-16" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>17 0.27962869 <a title="30-lsi-17" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>18 0.27470347 <a title="30-lsi-18" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<p>19 0.27027687 <a title="30-lsi-19" href="./nips-2006-Multi-Instance_Multi-Label_Learning_with_Application_to_Scene_Classification.html">136 nips-2006-Multi-Instance Multi-Label Learning with Application to Scene Classification</a></p>
<p>20 0.26722944 <a title="30-lsi-20" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.107), (3, 0.015), (4, 0.342), (7, 0.084), (9, 0.048), (22, 0.069), (44, 0.087), (57, 0.043), (65, 0.049), (69, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7899701 <a title="30-lda-1" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>2 0.48431653 <a title="30-lda-2" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method. In this paper, we present an asymptotic analysis of active learning for generalized linear models. Our analysis holds under the common practical situation of model misspeciﬁcation, and is based on realistic assumptions regarding the nature of the sampling distributions, which are usually neither independent nor identical. We derive unbiased estimators of generalization performance, as well as estimators of expected reduction in generalization error after adding a new training data point, that allow us to optimize its sampling distribution through a convex optimization problem. Our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models (e.g., binary classiﬁcation, multi-class classiﬁcation, regression) and can be applied in non-linear settings through the use of Mercer kernels. 1</p><p>3 0.48359469 <a title="30-lda-3" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>4 0.47948995 <a title="30-lda-4" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>5 0.47875956 <a title="30-lda-5" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>Author: Rie K. Ando, Tong Zhang</p><p>Abstract: We consider a general form of transductive learning on graphs with Laplacian regularization, and derive margin-based generalization bounds using appropriate geometric properties of the graph. We use this analysis to obtain a better understanding of the role of normalization of the graph Laplacian matrix as well as the effect of dimension reduction. The results suggest a limitation of the standard degree-based normalization. We propose a remedy from our analysis and demonstrate empirically that the remedy leads to improved classiﬁcation performance.</p><p>6 0.47830078 <a title="30-lda-6" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>7 0.47689128 <a title="30-lda-7" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>8 0.47431672 <a title="30-lda-8" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>9 0.474213 <a title="30-lda-9" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>10 0.4741132 <a title="30-lda-10" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<p>11 0.47337094 <a title="30-lda-11" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>12 0.4720419 <a title="30-lda-12" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>13 0.47102788 <a title="30-lda-13" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>14 0.47094741 <a title="30-lda-14" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>15 0.47072485 <a title="30-lda-15" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>16 0.47004241 <a title="30-lda-16" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>17 0.46998113 <a title="30-lda-17" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>18 0.46968815 <a title="30-lda-18" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>19 0.4695797 <a title="30-lda-19" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>20 0.469372 <a title="30-lda-20" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
