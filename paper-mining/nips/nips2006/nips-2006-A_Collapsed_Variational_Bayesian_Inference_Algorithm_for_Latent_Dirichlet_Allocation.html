<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-2" href="#">nips2006-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2006-2-pdf" href="http://papers.nips.cc/paper/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><p>Reference: <a title="nips-2006-2-reference" href="../nips2006_reference/nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. [sent-6, score-0.074]
</p><p>2 Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. [sent-7, score-0.271]
</p><p>3 In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA. [sent-8, score-0.852]
</p><p>4 1 Introduction Bayesian networks with discrete random variables form a very general and useful class of probabilistic models. [sent-9, score-0.06]
</p><p>5 In a Bayesian setting it is convenient to endow these models with Dirichlet priors over the parameters as they are conjugate to the multinomial distributions over the discrete random variables [1]. [sent-10, score-0.126]
</p><p>6 This choice has important computational advantages and allows for easy inference in such models. [sent-11, score-0.061]
</p><p>7 A class of Bayesian networks that has gained signiﬁcant momentum recently is latent Dirichlet allocation (LDA) [2], otherwise known as multinomial PCA [3]. [sent-12, score-0.236]
</p><p>8 Training LDA on a large corpus of several million documents can be a challenge and crucially depends on an efﬁcient and accurate inference procedure. [sent-14, score-0.143]
</p><p>9 A host of inference algorithms have been proposed, ranging from variational Bayesian (VB) inference [2], expectation propagation (EP) [7] to collapsed Gibbs sampling [5]. [sent-15, score-0.72]
</p><p>10 Perhaps surprisingly, the collapsed Gibbs sampler proposed in [5] seem to be the preferred choice in many of these large scale applications. [sent-16, score-0.387]
</p><p>11 However, collapsed Gibbs sampling also has its own problems: one needs to assess convergence of the Markov chain and to have some idea of mixing times to estimate the number of samples to collect, and to identify coherent topics across multiple samples. [sent-18, score-0.452]
</p><p>12 In practice one often ignores these issues and collects as many samples as is computationally feasible, while the question of topic identiﬁcation is often sidestepped by using just 1 sample. [sent-19, score-0.079]
</p><p>13 Hence there still seems to be a need for more efﬁcient, accurate and deterministic inference procedures. [sent-20, score-0.069]
</p><p>14 In this paper we will leverage the important insight that a Gibbs sampler that operates in a collapsed space—where the parameters are marginalized out—mixes much better than a Gibbs sampler that samples parameters and latent topic variables simultaneously. [sent-21, score-0.738]
</p><p>15 This suggests that the parameters and latent variables are intimately coupled. [sent-22, score-0.2]
</p><p>16 As we shall see in the following, marginalizing out the parameters induces new dependencies between the latent variables (which are conditionally independent given the parameters), but these dependencies are spread out over many latent variables. [sent-23, score-0.403]
</p><p>17 This implies that the dependency between any two latent variables is expected to be small. [sent-24, score-0.182]
</p><p>18 fully factorized variational) approximation: a particular variable interacts with the remaining variables only through summary statistics called the ﬁeld, and the impact of any single variable on the ﬁeld is very small [9]. [sent-27, score-0.078]
</p><p>19 Note that this is not true in the joint space of parameters and latent variables because ﬂuctuations in parameters can have a signiﬁcant impact on latent variables. [sent-28, score-0.357]
</p><p>20 We thus conjecture that the mean ﬁeld assumptions are much better satisﬁed in the collapsed space of latent variables than in the joint space of latent variables and parameters. [sent-29, score-0.739]
</p><p>21 In this paper we leverage this insight and propose a collapsed variational Bayesian (CVB) inference algorithm. [sent-30, score-0.607]
</p><p>22 Making use of this approximation, the ﬁnal algorithm is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard VB. [sent-33, score-0.058]
</p><p>23 We assume there are K latent topics, each being a multinomial distribution over a vocabulary of size W . [sent-35, score-0.21]
</p><p>24 For document j, we ﬁrst draw a mixing proportion θj = {θjk } over K topics from a symmetric Dirichlet with parameter α. [sent-36, score-0.066]
</p><p>25 For the ith word in the document, a topic zij is drawn with topic k chosen with probability θjk , then word xij is drawn from the zij th topic, with xij taking on value w with probability φkw . [sent-37, score-0.874]
</p><p>26 Finally, a symmetric Dirichlet prior with parameter β is placed on the topic parameters φk = {φkw }. [sent-38, score-0.075]
</p><p>27 Given the observed words x = {xij } the task of Bayesian inference is to compute the posterior distribution over the latent topic indices z = {zij }, the mixing proportions θ = {θj } and the topic parameters φ = {φk }. [sent-40, score-0.392]
</p><p>28 There are three current approaches, variational Bayes (VB) [2], expectation propagation [7] and collapsed Gibbs sampling [5]. [sent-41, score-0.624]
</p><p>29 We review the VB and collapsed Gibbs sampling methods here as they are the most popular methods and to motivate our new algorithm which combines advantages of both. [sent-42, score-0.424]
</p><p>30 Notice that the latent variables z and parameters θ, φ can be strongly dependent in the true posterior p(z, θ, φ|x) through the cross terms in (1). [sent-47, score-0.231]
</p><p>31 This dependence is ignored in VB which assumes that latent variables and parameters are independent instead. [sent-48, score-0.235]
</p><p>32 As a result, the VB upper bound on the negative log marginal likelihood can be very loose, leading to inaccurate estimates of the posterior. [sent-49, score-0.071]
</p><p>33 2 Collapsed Gibbs Sampling Standard Gibbs sampling, which iteratively samples latent variables z and parameters θ, φ, can potentially have slow convergence due again to strong dependencies between the parameters and latent variables. [sent-51, score-0.371]
</p><p>34 Collapsed Gibbs sampling improves upon Gibbs sampling by marginalizing out θ and φ instead, therefore dealing with them exactly. [sent-52, score-0.127]
</p><p>35 The conditional distribution of zij is multinomial with simple to calculate probabilities, so the programming and computational overhead is minimal. [sent-54, score-0.305]
</p><p>36 Collapsed Gibbs sampling has been observed to converge quickly [5]. [sent-55, score-0.053]
</p><p>37 Notice from (8) that zij depends on z ¬ij only through the counts n¬ij , n¬ijij , n¬ij . [sent-56, score-0.272]
</p><p>38 In particular, the dependence of zij on jk· ·kx ·k· any particular other variable zi j is very weak, especially for large datasets. [sent-57, score-0.299]
</p><p>39 As a result we expect the convergence of collapsed Gibbs sampling to be fast [10]. [sent-58, score-0.411]
</p><p>40 However, as with other MCMC samplers, and unlike variational inference, it is often hard to diagnose convergence, and a sufﬁciently large number of samples may be required to reduce sampling noise. [sent-59, score-0.223]
</p><p>41 The argument of rapid convergence of collapsed Gibbs sampling is reminiscent of the argument for when mean ﬁeld algorithms can be expected to be accurate [9]. [sent-60, score-0.449]
</p><p>42 The counts n¬ij , n¬ijij , n¬ij act as jk· ·kx ·k· ﬁelds through which zij interacts with other variables. [sent-61, score-0.291]
</p><p>43 This is the idea behind the collapsed variational Bayesian inference algorithm of the next section. [sent-63, score-0.576]
</p><p>44 3 Collapsed Variational Bayesian Inference for LDA We derive a new inference algorithm for LDA combining the advantages of both standard VB and collapsed Gibbs sampling. [sent-64, score-0.434]
</p><p>45 It is a variational algorithm which, instead of assuming independence, models the dependence of the parameters on the latent variables in an exact fashion. [sent-65, score-0.413]
</p><p>46 On the other hand we still assume that latent variables are mutually independent. [sent-66, score-0.182]
</p><p>47 We call this algorithm collapsed variational Bayesian (CVB) inference. [sent-68, score-0.528]
</p><p>48 There are two ways to deal with the parameters in an exact fashion, the ﬁrst is to marginalize them out of the joint distribution and to start from (7), the second is to explicitly model the posterior of θ, φ given z and x without any assumptions on its form. [sent-69, score-0.117]
</p><p>49 The only assumption we make in CVB is that the latent variables z are mutually independent, thus we approximate the posterior as: q (z, θ, φ) = q (θ, φ|z) ˆ ˆ  q (zij |ˆij ) ˆ γ  (10)  ij  where q (zij |ˆij ) is multinomial with parameters γij . [sent-71, score-0.57]
</p><p>50 As CVB makes a strictly weaker assumption on the variational posterior than standard VB, we have F (ˆ(z)) ≤ F(˜(z)) q q  min F (˜(z)˜(θ)˜(φ)) q q q  q(θ)˜(φ) ˜ q  (13)  and thus CVB is a better approximation than standard VB. [sent-74, score-0.266]
</p><p>51 Finally, we derive the updates for the variational parameters γij . [sent-75, score-0.201]
</p><p>52 1 Gaussian approximation for CVB Inference For completeness, we describe how to compute each expectation term in (15) exactly in the appendix. [sent-77, score-0.05]
</p><p>53 This exact implementation of CVB is computationally too expensive to be practical, and we propose instead to use a simple Gaussian approximation which works very accurately and which requires minimal computational costs. [sent-78, score-0.067]
</p><p>54 In this section we describe the Gaussian approximation applied to Eq [log(α + n¬ij )]; the other ˆ jk· two expectation terms are similarly computed. [sent-79, score-0.05]
</p><p>55 Notice that n¬ij = jk· 1 (zi j = k) is a sum of a large number independent Bernoulli variables 1 (zi j = k) each i =i with mean parameter γi jk , thus it can be accurately approximated by a Gaussian. [sent-81, score-0.466]
</p><p>56 The reason is that we often have γi jk being either close to 0 or 1 thus ˆ the variance of n¬ij is small relative to its mean and the Gaussian approximation will be accurate. [sent-84, score-0.43]
</p><p>57 By keeping track of the mean and variance of njk· , n·kw and n·k· , and subtracting the mean and variance of the corresponding Bernoulli variables whenever we require the terms with xij , zij removed, the computational cost scales only as O(K) for each update to q (zij ). [sent-88, score-0.41]
</p><p>58 In comparison, collapsed Gibbs sampling needs to keep track of the current sample of zij for every word in the corpus, thus the memory requirement is O(N ) while the computational cost scales as O(N K) where N is the total number of words in the corpus—higher than for VB and CVB. [sent-91, score-0.742]
</p><p>59 Note however that the constant factor involved in the O(N K) time cost of collapsed Gibbs sampling is signiﬁcantly smaller than those for VB and CVB. [sent-92, score-0.411]
</p><p>60 4 Experiments We compared the three algorithms described in the paper: standard VB, CVB and collapsed Gibbs sampling. [sent-93, score-0.373]
</p><p>61 com), which has J = 3430 documents, a vocabulary size of W = 6909, a total of N = 467, 714 words in all the documents and on average 136 words per document. [sent-96, score-0.138]
</p><p>62 cc) with J = 1675 documents, a vocabulary size of W = 12419, N = 2, 166, 029 words in the corpus and on average 1293 words per document. [sent-99, score-0.128]
</p><p>63 In both datasets stop words and infrequent words were removed. [sent-100, score-0.054]
</p><p>64 We split both datasets into a training set and a test set by assigning 10% of the words in each document to the test set. [sent-101, score-0.078]
</p><p>65 1, K = 8 number of topics for KOS and K = 40 for NIPS. [sent-104, score-0.026]
</p><p>66 First using variational bounds of the log marginal probabilities on the training set, and secondly using log probabilities on the test set. [sent-107, score-0.337]
</p><p>67 Expressions for the variational bounds are given in (2) for VB and (12) for CVB. [sent-108, score-0.188]
</p><p>68 For both VB and CVB, test set log probabilities are computed as: α + Eq [njk· ] ¯ θjk = Kα + Eq [nj·· ]  ¯ ¯ θjk φkxtest ij  p(xtest ) = ij  k  β + Eq [n·kw ] ¯ φkw = W β + Eq [n·k· ]  (19)  Note that we used estimated mean values of θjk and φkw [11]. [sent-109, score-0.673]
</p><p>69 For collapsed Gibbs sampling, given S samples from the posterior, we used: p(xtest ) = ij  k  1 |S|  S s θjk φs test kx ij  s=1  s θjk =  α + ns jk· Kα + ns j··  φs = kw  β + ns ·kw W β + ns ·k·  (20)  Figure 1 summarizes our results. [sent-110, score-1.294]
</p><p>70 CVB converged faster and to signiﬁcantly better solutions than standard VB; this conﬁrms our intuition that CVB provides much better approximations than VB. [sent-112, score-0.033]
</p><p>71 CVB also converged faster than collapsed Gibbs sampling, but Gibbs sampling attains a better solution in the end; this is reasonable since Gibbs sampling should be exact with  −7. [sent-113, score-0.505]
</p><p>72 First row: per word variational bounds as functions of numbers of iterations of VB and CVB. [sent-161, score-0.254]
</p><p>73 Second row: histograms of converged per word variational bounds across random initializations for VB and CVB. [sent-162, score-0.29]
</p><p>74 Third row: test set per word log probabilities as functions of numbers of iterations for VB, CVB and Gibbs. [sent-163, score-0.14]
</p><p>75 Fourth row: histograms of ﬁnal test set per word log probabilities across 50 random initializations. [sent-164, score-0.158]
</p><p>76 5 0  500  1000  1500  2000  2500  Figure 2: Left: test set per word log probabilities. [sent-176, score-0.12]
</p><p>77 Both as functions of the number of documents for KOS. [sent-178, score-0.042]
</p><p>78 We have also studied the dependence of approximation accuracies on the number of documents in the corpus. [sent-181, score-0.084]
</p><p>79 To conduct this experiment we train on 90% of the words in a (growing) subset of the corpus and test on the corresponding 10% left out words. [sent-182, score-0.072]
</p><p>80 In ﬁgure Figure 2 we show both variational bounds and test set log probabilities as functions of the number of documents J. [sent-183, score-0.304]
</p><p>81 We observe that as expected the variational methods improve as J increases. [sent-184, score-0.17]
</p><p>82 5 Discussion We have described a collapsed variational Bayesian (CVB) inference algorithm for LDA. [sent-186, score-0.576]
</p><p>83 The algorithm is easy to implement, computationally efﬁcient and more accurate than standard VB. [sent-187, score-0.058]
</p><p>84 The central insight of CVB is that instead of assuming parameters to be independent from latent variables, we treat their dependence on the topic variables in an exact fashion. [sent-188, score-0.329]
</p><p>85 Because the factorization assumptions made by CVB are weaker than those made by VB, the resulting approximation is more accurate. [sent-189, score-0.035]
</p><p>86 Computational efﬁciency is achieved in CVB with a Gaussian approximation, which was found to be so accurate that there is never a need for exact summation. [sent-190, score-0.044]
</p><p>87 The idea of integrating out parameters before applying variational inference has been independently proposed by [12]. [sent-191, score-0.236]
</p><p>88 These models all have the property that they consist of discrete random variables with Dirichlet priors on the parameters, which is the property allowing us to use the Gaussian approximation. [sent-195, score-0.06]
</p><p>89 Over the years a variety of inference algorithms have been proposed based on a combination of {maximize, sample, assume independent, marginalize out} applied to both parameters and latent variables. [sent-197, score-0.25]
</p><p>90 We conclude by summarizing these algorithms in Table 1, and note that CVB is located in the marginalize out parameters and assume latent variables are independent cell. [sent-198, score-0.26]
</p><p>91 A  Exact Computation of Expectation Terms in (15)  We can compute the expectation terms in (15) exactly as follows. [sent-199, score-0.028]
</p><p>92 Consider Eq [log(α + n¬ij )], ˆ jk· ¬ij which requires computing q (njk· ) (other expectation terms are similarly computed). [sent-200, score-0.028]
</p><p>93 Note that ˆ  Parameters → maximize sample assume marginalize ↓ Latent variables independent out maximize Viterbi EM ? [sent-201, score-0.103]
</p><p>94 VB CVB marginalize out EM any MCMC EP for LDA intractable Table 1: A variety of inference algorithms for graphical models. [sent-204, score-0.093]
</p><p>95 “ME” is the maximization-expectation algorithm of [15] and “any MCMC” means that we can use any MCMC sampler for the parameters once latent variables have been marginalized out. [sent-207, score-0.245]
</p><p>96 n¬ij = i =i 1 (zi j = k) is a sum of independent Bernoulli variables 1 (zi j = k) each with mean jk· parameter γi jk . [sent-208, score-0.466]
</p><p>97 Deﬁne vectors vi jk = [(1 − γi jk ), γi jk ] , and let vjk = v1jk ⊗ · · · ⊗ vn·j· jk be ˆ ˆ ˆ ¬ij the convolution of all vi jk . [sent-209, score-2.019]
</p><p>98 Then q (n¬ij = m) will ˆ jk· ¬ij ¬ij be the (m+1)st entry in vjk . [sent-211, score-0.064]
</p><p>99 The expectation Eq [log(α+njk· )] can now be computed explicitly. [sent-212, score-0.028]
</p><p>100 At ˆ j·· jk· the expense of complicating the algorithm implementation, this can be improved by sparsifying the vectors vjk (setting small entries to zero) as well as other computational tricks. [sent-214, score-0.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vb', 0.399), ('jk', 0.391), ('cvb', 0.373), ('collapsed', 0.358), ('ij', 0.291), ('zij', 0.257), ('eq', 0.211), ('variational', 0.17), ('gibbs', 0.154), ('kw', 0.143), ('latent', 0.139), ('njk', 0.116), ('ijij', 0.09), ('ijk', 0.09), ('kx', 0.078), ('xij', 0.076), ('lda', 0.073), ('dirichlet', 0.073), ('varq', 0.064), ('vjk', 0.064), ('topic', 0.057), ('sampling', 0.053), ('inference', 0.048), ('multinomial', 0.048), ('word', 0.047), ('marginalize', 0.045), ('bayesian', 0.043), ('variables', 0.043), ('documents', 0.042), ('log', 0.041), ('njkw', 0.039), ('allocation', 0.035), ('bernoulli', 0.032), ('corpus', 0.032), ('posterior', 0.031), ('ns', 0.03), ('sampler', 0.029), ('expectation', 0.028), ('eld', 0.028), ('words', 0.027), ('topics', 0.026), ('ijxij', 0.026), ('kos', 0.026), ('xtest', 0.026), ('document', 0.025), ('em', 0.025), ('ep', 0.023), ('exact', 0.023), ('mcmc', 0.023), ('vocabulary', 0.023), ('zi', 0.022), ('approximation', 0.022), ('computationally', 0.022), ('nj', 0.022), ('accurate', 0.021), ('marginalizing', 0.021), ('opper', 0.02), ('probabilities', 0.02), ('dependence', 0.02), ('uai', 0.019), ('interacts', 0.019), ('per', 0.019), ('london', 0.018), ('bounds', 0.018), ('parameters', 0.018), ('histograms', 0.018), ('notice', 0.018), ('converged', 0.018), ('leverage', 0.017), ('mean', 0.017), ('discrete', 0.017), ('energy', 0.017), ('blei', 0.016), ('factorized', 0.016), ('inaccurate', 0.016), ('marginalized', 0.016), ('gatsby', 0.016), ('mixing', 0.015), ('standard', 0.015), ('counts', 0.015), ('propagation', 0.015), ('independent', 0.015), ('insight', 0.014), ('free', 0.014), ('marginal', 0.014), ('weakly', 0.014), ('gaussian', 0.014), ('ths', 0.014), ('college', 0.014), ('denominator', 0.014), ('gained', 0.014), ('plugging', 0.014), ('dependencies', 0.014), ('equations', 0.014), ('updates', 0.013), ('test', 0.013), ('grif', 0.013), ('weaker', 0.013), ('welling', 0.013), ('advantages', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="2-tfidf-1" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><p>2 0.28776398 <a title="2-tfidf-2" href="./nips-2006-Parameter_Expanded_Variational_Bayesian_Methods.html">159 nips-2006-Parameter Expanded Variational Bayesian Methods</a></p>
<p>Author: Tommi S. Jaakkola, Yuan Qi</p><p>Abstract: Bayesian inference has become increasingly important in statistical machine learning. Exact Bayesian calculations are often not feasible in practice, however. A number of approximate Bayesian methods have been proposed to make such calculations practical, among them the variational Bayesian (VB) approach. The VB approach, while useful, can nevertheless suffer from slow convergence to the approximate solution. To address this problem, we propose Parameter-eXpanded Variational Bayesian (PX-VB) methods to speed up VB. The new algorithm is inspired by parameter-expanded expectation maximization (PX-EM) and parameterexpanded data augmentation (PX-DA). Similar to PX-EM and -DA, PX-VB expands a model with auxiliary variables to reduce the coupling between variables in the original model. We analyze the convergence rates of VB and PX-VB and demonstrate the superior convergence rates of PX-VB in variational probit regression and automatic relevance determination. 1</p><p>3 0.25234532 <a title="2-tfidf-3" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>Author: Mark Girolami, Mingjun Zhong</p><p>Abstract: By adopting Gaussian process priors a fully Bayesian solution to the problem of integrating possibly heterogeneous data sets within a classiﬁcation setting is presented. Approximate inference schemes employing Variational & Expectation Propagation based methods are developed and rigorously assessed. We demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classiﬁer combination. 1</p><p>4 0.15989268 <a title="2-tfidf-4" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose a highly efﬁcient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work. 1</p><p>5 0.13965279 <a title="2-tfidf-5" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>Author: Peter Carbonetto, Nando D. Freitas</p><p>Abstract: Despite all the attention paid to variational methods based on sum-product message passing (loopy belief propagation, tree-reweighted sum-product), these methods are still bound to inference on a small set of probabilistic models. Mean ﬁeld approximations have been applied to a broader set of problems, but the solutions are often poor. We propose a new class of conditionally-speciﬁed variational approximations based on mean ﬁeld theory. While not usable on their own, combined with sequential Monte Carlo they produce guaranteed improvements over conventional mean ﬁeld. Moreover, experiments on a well-studied problem— inferring the stable conﬁgurations of the Ising spin glass—show that the solutions can be signiﬁcantly better than those obtained using sum-product-based methods. 1</p><p>6 0.10993634 <a title="2-tfidf-6" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>7 0.10594467 <a title="2-tfidf-7" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>8 0.10192544 <a title="2-tfidf-8" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>9 0.098797396 <a title="2-tfidf-9" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>10 0.097635336 <a title="2-tfidf-10" href="./nips-2006-Modeling_General_and_Specific_Aspects_of_Documents_with_a_Probabilistic_Topic_Model.html">133 nips-2006-Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model</a></p>
<p>11 0.096121252 <a title="2-tfidf-11" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>12 0.086746059 <a title="2-tfidf-12" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>13 0.070392862 <a title="2-tfidf-13" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>14 0.067799628 <a title="2-tfidf-14" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>15 0.067137852 <a title="2-tfidf-15" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>16 0.060815148 <a title="2-tfidf-16" href="./nips-2006-Modeling_Dyadic_Data_with_Binary_Latent_Factors.html">132 nips-2006-Modeling Dyadic Data with Binary Latent Factors</a></p>
<p>17 0.054939222 <a title="2-tfidf-17" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>18 0.052394483 <a title="2-tfidf-18" href="./nips-2006-A_PAC-Bayes_Risk_Bound_for_General_Loss_Functions.html">11 nips-2006-A PAC-Bayes Risk Bound for General Loss Functions</a></p>
<p>19 0.051053375 <a title="2-tfidf-19" href="./nips-2006-PAC-Bayes_Bounds_for_the_Risk_of_the_Majority_Vote_and_the_Variance_of_the_Gibbs_Classifier.html">157 nips-2006-PAC-Bayes Bounds for the Risk of the Majority Vote and the Variance of the Gibbs Classifier</a></p>
<p>20 0.046259657 <a title="2-tfidf-20" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.148), (1, 0.054), (2, 0.043), (3, -0.109), (4, 0.055), (5, 0.144), (6, 0.365), (7, 0.052), (8, -0.148), (9, 0.03), (10, 0.043), (11, 0.145), (12, 0.049), (13, -0.166), (14, 0.116), (15, 0.286), (16, -0.18), (17, -0.0), (18, 0.047), (19, -0.048), (20, -0.035), (21, -0.006), (22, -0.094), (23, 0.081), (24, 0.016), (25, 0.004), (26, -0.132), (27, -0.037), (28, -0.059), (29, -0.113), (30, 0.065), (31, 0.092), (32, 0.052), (33, -0.099), (34, -0.035), (35, 0.039), (36, -0.05), (37, 0.065), (38, -0.001), (39, -0.04), (40, -0.023), (41, -0.065), (42, 0.021), (43, -0.005), (44, 0.013), (45, -0.011), (46, 0.057), (47, -0.038), (48, 0.068), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96628249 <a title="2-lsi-1" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><p>2 0.68419296 <a title="2-lsi-2" href="./nips-2006-Parameter_Expanded_Variational_Bayesian_Methods.html">159 nips-2006-Parameter Expanded Variational Bayesian Methods</a></p>
<p>Author: Tommi S. Jaakkola, Yuan Qi</p><p>Abstract: Bayesian inference has become increasingly important in statistical machine learning. Exact Bayesian calculations are often not feasible in practice, however. A number of approximate Bayesian methods have been proposed to make such calculations practical, among them the variational Bayesian (VB) approach. The VB approach, while useful, can nevertheless suffer from slow convergence to the approximate solution. To address this problem, we propose Parameter-eXpanded Variational Bayesian (PX-VB) methods to speed up VB. The new algorithm is inspired by parameter-expanded expectation maximization (PX-EM) and parameterexpanded data augmentation (PX-DA). Similar to PX-EM and -DA, PX-VB expands a model with auxiliary variables to reduce the coupling between variables in the original model. We analyze the convergence rates of VB and PX-VB and demonstrate the superior convergence rates of PX-VB in variational probit regression and automatic relevance determination. 1</p><p>3 0.67496121 <a title="2-lsi-3" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>Author: Mark Girolami, Mingjun Zhong</p><p>Abstract: By adopting Gaussian process priors a fully Bayesian solution to the problem of integrating possibly heterogeneous data sets within a classiﬁcation setting is presented. Approximate inference schemes employing Variational & Expectation Propagation based methods are developed and rigorously assessed. We demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classiﬁer combination. 1</p><p>4 0.56676137 <a title="2-lsi-4" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>Author: Kenichi Kurihara, Max Welling, Nikos A. Vlassis</p><p>Abstract: Dirichlet Process (DP) mixture models are promising candidates for clustering applications where the number of clusters is unknown a priori. Due to computational considerations these models are unfortunately unsuitable for large scale data-mining applications. We propose a class of deterministic accelerated DP mixture models that can routinely handle millions of data-cases. The speedup is achieved by incorporating kd-trees into a variational Bayesian algorithm for DP mixtures in the stick-breaking representation, similar to that of Blei and Jordan (2005). Our algorithm differs in the use of kd-trees and in the way we handle truncation: we only assume that the variational distributions are ﬁxed at their priors after a certain level. Experiments show that speedups relative to the standard variational algorithm can be signiﬁcant. 1</p><p>5 0.49071178 <a title="2-lsi-5" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>Author: Peter Carbonetto, Nando D. Freitas</p><p>Abstract: Despite all the attention paid to variational methods based on sum-product message passing (loopy belief propagation, tree-reweighted sum-product), these methods are still bound to inference on a small set of probabilistic models. Mean ﬁeld approximations have been applied to a broader set of problems, but the solutions are often poor. We propose a new class of conditionally-speciﬁed variational approximations based on mean ﬁeld theory. While not usable on their own, combined with sequential Monte Carlo they produce guaranteed improvements over conventional mean ﬁeld. Moreover, experiments on a well-studied problem— inferring the stable conﬁgurations of the Ising spin glass—show that the solutions can be signiﬁcantly better than those obtained using sum-product-based methods. 1</p><p>6 0.36724952 <a title="2-lsi-6" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>7 0.35889328 <a title="2-lsi-7" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>8 0.34971839 <a title="2-lsi-8" href="./nips-2006-Modeling_General_and_Specific_Aspects_of_Documents_with_a_Probabilistic_Topic_Model.html">133 nips-2006-Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model</a></p>
<p>9 0.34689656 <a title="2-lsi-9" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>10 0.32846799 <a title="2-lsi-10" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>11 0.29764658 <a title="2-lsi-11" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>12 0.25585744 <a title="2-lsi-12" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>13 0.24900547 <a title="2-lsi-13" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>14 0.24247545 <a title="2-lsi-14" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>15 0.23845282 <a title="2-lsi-15" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<p>16 0.23551963 <a title="2-lsi-16" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>17 0.23018445 <a title="2-lsi-17" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>18 0.20835599 <a title="2-lsi-18" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>19 0.19956292 <a title="2-lsi-19" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>20 0.19347546 <a title="2-lsi-20" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.131), (7, 0.053), (9, 0.035), (20, 0.049), (22, 0.061), (44, 0.093), (57, 0.106), (65, 0.032), (69, 0.022), (84, 0.29), (90, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86716413 <a title="2-lda-1" href="./nips-2006-A_selective_attention_multi--chip_system_with_dynamic_synapses_and_spiking_neurons.html">18 nips-2006-A selective attention multi--chip system with dynamic synapses and spiking neurons</a></p>
<p>Author: Chiara Bartolozzi, Giacomo Indiveri</p><p>Abstract: Selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity: salient subregions of the input stimuli are serially processed, while non–salient regions are suppressed. We present an mixed mode analog/digital Very Large Scale Integration implementation of a building block for a multi–chip neuromorphic hardware model of selective attention. We describe the chip’s architecture and its behavior, when its is part of a multi–chip system with a spiking retina as input, and show how it can be used to implement in real-time ﬂexible models of bottom-up attention. 1</p><p>same-paper 2 0.83017159 <a title="2-lda-2" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><p>3 0.71226025 <a title="2-lda-3" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>Author: Rie K. Ando, Tong Zhang</p><p>Abstract: We consider a general form of transductive learning on graphs with Laplacian regularization, and derive margin-based generalization bounds using appropriate geometric properties of the graph. We use this analysis to obtain a better understanding of the role of normalization of the graph Laplacian matrix as well as the effect of dimension reduction. The results suggest a limitation of the standard degree-based normalization. We propose a remedy from our analysis and demonstrate empirically that the remedy leads to improved classiﬁcation performance.</p><p>4 0.58144981 <a title="2-lda-4" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>5 0.57702941 <a title="2-lda-5" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>Author: Dong S. Cheng, Vittorio Murino, Mário Figueiredo</p><p>Abstract: This paper proposes a new approach to model-based clustering under prior knowledge. The proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as ﬁnite mixture learning under a grouping prior. To estimate the parameters of the proposed model, we derive a (generalized) EM algorithm with a closed-form E-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require Gibbs sampling or suboptimal shortcuts. We show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature Markov random ﬁeld priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efﬁcient way. Finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes. 1</p><p>6 0.57505155 <a title="2-lda-6" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>7 0.57115585 <a title="2-lda-7" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>8 0.57086396 <a title="2-lda-8" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>9 0.57050061 <a title="2-lda-9" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>10 0.5673123 <a title="2-lda-10" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>11 0.5667997 <a title="2-lda-11" href="./nips-2006-Tighter_PAC-Bayes_Bounds.html">193 nips-2006-Tighter PAC-Bayes Bounds</a></p>
<p>12 0.56462717 <a title="2-lda-12" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>13 0.5637759 <a title="2-lda-13" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>14 0.56218159 <a title="2-lda-14" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>15 0.56199241 <a title="2-lda-15" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>16 0.56177598 <a title="2-lda-16" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>17 0.56025684 <a title="2-lda-17" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>18 0.56025594 <a title="2-lda-18" href="./nips-2006-Isotonic_Conditional_Random_Fields_and_Local_Sentiment_Flow.html">101 nips-2006-Isotonic Conditional Random Fields and Local Sentiment Flow</a></p>
<p>19 0.56017691 <a title="2-lda-19" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>20 0.55935466 <a title="2-lda-20" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
