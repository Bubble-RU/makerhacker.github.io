<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2006-Multiple timescales and uncertainty in motor adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-141" href="#">nips2006-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2006-Multiple timescales and uncertainty in motor adaptation</h1>
<br/><p>Source: <a title="nips-2006-141-pdf" href="http://papers.nips.cc/paper/3045-multiple-timescales-and-uncertainty-in-motor-adaptation.pdf">pdf</a></p><p>Author: Konrad P. Körding, Joshua B. Tenenbaum, Reza Shadmehr</p><p>Abstract: Our motor system changes due to causes that span multiple timescales. For example, muscle response can change because of fatigue, a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower. Here we hypothesize that the nervous system adapts in a way that reﬂects the temporal properties of such potential disturbances. According to a Bayesian formulation of this idea, movement error results in a credit assignment problem: what timescale is responsible for this disturbance? The adaptation schedule inﬂuences the behavior of the optimal learner, changing estimates at different timescales as well as the uncertainty. A system that adapts in this way predicts many properties observed in saccadic gain adaptation. It well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction.</p><p>Reference: <a title="nips-2006-141-reference" href="../nips2006_reference/nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Multiple timescales and uncertainty in motor adaptation  ¨ Konrad P. [sent-1, score-0.925]
</p><p>2 edu  Abstract Our motor system changes due to causes that span multiple timescales. [sent-8, score-0.342]
</p><p>3 For example, muscle response can change because of fatigue, a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower. [sent-9, score-0.792]
</p><p>4 According to a Bayesian formulation of this idea, movement error results in a credit assignment problem: what timescale is responsible for this disturbance? [sent-11, score-0.3]
</p><p>5 The adaptation schedule inﬂuences the behavior of the optimal learner, changing estimates at different timescales as well as the uncertainty. [sent-12, score-0.67]
</p><p>6 A system that adapts in this way predicts many properties observed in saccadic gain adaptation. [sent-13, score-0.642]
</p><p>7 It well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction. [sent-14, score-0.896]
</p><p>8 For that reason, without adaptation any changes in the properties of the oculomotor plant would lead to inaccurate saccades [2]. [sent-17, score-0.768]
</p><p>9 Motor gain is the ratio of actual and desired movement distances. [sent-18, score-0.29]
</p><p>10 If the motor gain decreases to below one then the nervous system must send a stronger command to produce a movement of the same size. [sent-19, score-0.671]
</p><p>11 Indeed, it has been observed that if saccades overshoot the target, the gain tends to decrease and if they undershoot, the gain tends to increase. [sent-20, score-0.689]
</p><p>12 The saccadic jump paradigm [3] is often used to probe such adaptation [4]: while the subject moves its eyes towards a target, the target is moved. [sent-21, score-0.769]
</p><p>13 This is not distinguishable to the subject from a change in the properties of the oculomotor plant [5]. [sent-22, score-0.314]
</p><p>14 Using this paradigm it is possible to probe the mechanism that is normally used to adapt to ongoing changes of the oculomotor plant. [sent-23, score-0.28]
</p><p>15 1 Disturbances to the motor plant Properties of the oculomotor plant may change due to a variety of disturbances, such as various kinds of fatigue and disease. [sent-25, score-0.725]
</p><p>16 Here we model each disturbance as a random walk with a  characteristic timescale (Figures 1A and B) over which the disturbance is expected to go away. [sent-27, score-0.635]
</p><p>17 It seems plausible that disturbances that have a short timescale tend to be more variable than those that have a long timescale, and we choose: σ τ = c/τ where c is one of the two free parameters of our model. [sent-34, score-0.461]
</p><p>18 05 which we estimated from the spread of saccade gains over typical periods of 200 saccades and c = 0. [sent-39, score-0.532]
</p><p>19 3 Inference Given this explicit model, Bayesian statistics allows deriving an optimal adaptation strategy. [sent-43, score-0.268]
</p><p>20 Because the contribution of each timescale can never be known precisely, the Bayesian learner represents what it knows as a probability distribution. [sent-49, score-0.386]
</p><p>21 And so the learner represents what it knows about the contribution of each timescale as a best estimate, but also keeps a measure of uncertainty around this estimate (Fig 1C). [sent-51, score-0.479]
</p><p>22 Any point along the +0% gain line is a point where the fast and slow timescale cancel each other. [sent-52, score-0.708]
</p><p>23 Every timestep the system starts with its belief that it has from the previous timestep (sketched in yellow) and combines this with information from the current saccade (sketched in blue) to come up with a new estimate (sketched in red). [sent-56, score-0.489]
</p><p>24 (1) When time passes, disturbances can be expected to get smaller but at the same time our uncertainty about them increases. [sent-58, score-0.272]
</p><p>25 Normally the adaptation mechanism is responding to the small drifts that happen to the oculomotor plant and the estimate from the saccade is largely overlapping with the prior belief and with the new belief. [sent-61, score-0.911]
</p><p>26 When the light is turned off the estimate of each of the  B disturbance  d τ3  0  -45 -45  error  gain  error  gain  -0. [sent-62, score-0.71]
</p><p>27 disease)  fast disturbance [%]  A  0 45 slow disturbance [%]  1. [sent-70, score-0.547]
</p><p>28 9  prior belief 4000  0  evidence from saccade  time [saccades]  D Normal saccades  In the dark  Saccadic jump  +30 saccades  Washout  Reversal  Figure 1: A generative model for changes in the motor plant and the corresponding optimal inference. [sent-72, score-1.277]
</p><p>29 B) An example of a system with two timescales (fast and slow), and the resulting gain. [sent-76, score-0.418]
</p><p>30 The system observes a saccade with an error of +30%. [sent-80, score-0.41]
</p><p>31 Here we simulated a saccade on every 10th time step of the model. [sent-87, score-0.323]
</p><p>32 Only in the darkness case saccades 1 3 and 50 are shown. [sent-89, score-0.387]
</p><p>33 In a gain increase paradigm, initially most of the error is associated with the fast perturbations. [sent-91, score-0.363]
</p><p>34 After 30 saccades in the gain increase paradigm, most of the error is associated with slow perturbations. [sent-92, score-0.545]
</p><p>35 Washout trials that follow gain increase do not return the system to a naive state. [sent-93, score-0.359]
</p><p>36 Gain decrease following gain increase training will mostly affect the fast system. [sent-95, score-0.363]
</p><p>37 06  800 400 0  Day 1 Day 2  F Day 5  Bayesian adaptation 1200  0. [sent-98, score-0.268]
</p><p>38 Starting at saccade number 0 the target is jumping 30% short of the target giving the impression of muscles that are too strong. [sent-108, score-0.397]
</p><p>39 The gain then decreases until the manipulation is ended at saccade number 1400. [sent-109, score-0.574]
</p><p>40 Normal saccadic gain change paradigm as in Figure 2, however now the monkey spends its nights without vision and the paradigm is continued for many days. [sent-113, score-0.791]
</p><p>41 E) Comparison of the saccadic gain change timecourses obtained by ﬁtting an exponential. [sent-115, score-0.637]
</p><p>42 F) the same ﬁgure as in E) for the Bayesian learner disturbances slowly creeps towards zero. [sent-116, score-0.363]
</p><p>43 In the saccadic jump paradigm the error is much larger than it would be during normal life and this is ﬁrst interpreted by the learner as a fast change and as it persists progressively interpreted as a slow change. [sent-118, score-0.85]
</p><p>44 When the saccadic jumps ends then the fast timescale goes negative fast and the slow timescale slowly approaches zero. [sent-119, score-1.129]
</p><p>45 In a reversal setting the fast timescale becomes very negative and the slow timescale goes towards zero. [sent-120, score-0.759]
</p><p>46 Already with two timescales the optimal learner can thus exhibit a large number of interesting properties. [sent-121, score-0.479]
</p><p>47 1 Saccadic gain adaptation In an impressive range of experiments started by Mclaughlin [3], investigators have examined how monkeys adapt their saccadic gain. [sent-123, score-0.917]
</p><p>48 Figure 2A shows how the gain changes over time so that saccades progressively become more precise. [sent-124, score-0.549]
</p><p>49 The rate of adaptation typically starts fast and then progressively gets slower. [sent-125, score-0.421]
</p><p>50 This is a classic pattern that is reﬂected in numerous motor adaptation paradigms [8, 9]. [sent-126, score-0.499]
</p><p>51 Fast timescale disturbances are assumed to increase and decrease faster than slow timescale disturbances. [sent-128, score-0.858]
</p><p>52 Between trials, the estimates of the fast disturbances decay fast, but this decay is smaller in the slower timescales. [sent-132, score-0.402]
</p><p>53 If the gain change is maintained, the relative contribution of the fast timescales diminishes in comparison to the slow timescales (Fig. [sent-133, score-1.18]
</p><p>54 As fast timescales adapt fast but decay fast as well and slow timescales adapt and decay slowly, this implies that the gain change is driven by progressively slower timescales resulting in the transition from initial fast adapting to a progressively slower adapting. [sent-135, score-2.143]
</p><p>55 2 Saccadic gain adaptation after sensory deprivation The effects of a wide range of timescales and uncertainty about the causes of changes of the oculomotor plant will largely be hidden if experiments are of a relatively short duration and no uncertainty  is produced. [sent-137, score-1.431]
</p><p>56 However, in a recent experiment Robinson et al analyzed saccadic gain adaptation [7] in a way that allowed insight into many timescales as well as insight into the way the nervous system deals with uncertainty. [sent-138, score-1.413]
</p><p>57 The monkey adapted for about 1500 saccades every day for 21 consecutive days. [sent-140, score-0.404]
</p><p>58 Because of the long duration many different timescales are involved in this process. [sent-141, score-0.354]
</p><p>59 (1) There are several timescales during adaptation: there is a fast (100 saccades) and a slow (10 days) timescale. [sent-147, score-0.527]
</p><p>60 During the breaks that are paired with darkness the system is decaying back to a gain of zero, as predicted by the model. [sent-150, score-0.515]
</p><p>61 The ﬁnding that the Bayesian learner seems to change faster than the monkey may be related to the context being somewhat different than in the Hopp and Fuchs experiment. [sent-154, score-0.277]
</p><p>62 The system seems to represent uncertainty and clearly represents the way the motor plant is expected to change in the absence of feedback. [sent-155, score-0.561]
</p><p>63 It has been proposed that the nervous system may use a set of integrators where one is learning fast and the other is learning slowly [10, 11]. [sent-156, score-0.277]
</p><p>64 3 Gain adaptation with reversals  Kojima et al [12] reported a host of surprising behavioral results during saccade adaptation. [sent-160, score-0.72]
</p><p>65 In these experiments the adaptation direction was changed 3 times. [sent-161, score-0.268]
</p><p>66 The saccadic gain was initially increased, then decreased until it reached unity, and ﬁnally increased again (Figure 3A). [sent-162, score-0.584]
</p><p>67 The saccadic gain increased faster during the second gain-up session than during the ﬁrst(Figure 3B). [sent-163, score-0.66]
</p><p>68 The Bayesian learner shows a similar phenomenon and provides a rationale: At the end of the ﬁrst gain-up session for the Bayesian learner, most of the gain change is associated with a slow timescale (Figure 3C). [sent-165, score-0.845]
</p><p>69 In the subsequent gain-down session, errors produce rapid changes in the fast timescales so that by the time the gain estimate reaches unity, the fast and slow timescales have opposite estimates. [sent-166, score-1.313]
</p><p>70 In the subsequent gain-up session, the rate of re-adaptation is faster than initial adaptation because the fast timescales decay upwards in between trials (Figure 3D). [sent-168, score-0.796]
</p><p>71 After about 100 saccades the speed gain from the low frequencies is over and is turned into a slowed increase due to the decreased error term. [sent-169, score-0.461]
</p><p>72 In a second experiment, Kojima et al [12] found that saccade gains could change despite the fact that the animal was provided with no feedback to guide its performance. [sent-170, score-0.502]
</p><p>73 When they come out of the dark their gain had spontaneously increased (Figure 3E). [sent-173, score-0.322]
</p><p>74 However, the estimates are still affected by their timescales of change: the estimate moves up fast along the fast timescales but slowly along the slow timescales. [sent-176, score-1.075]
</p><p>75 At the start of the darkness period there is a positive upward and a negative downward disturbance inferred by the system (Figure 1C, reversal). [sent-177, score-0.451]
</p><p>76 Consequently, by the end of the dark period, the estimate has become gain-up, the gain learned in the initial session. [sent-178, score-0.317]
</p><p>77 Updating without feedback leads the system to infer unobserved dynamics of the oculomotor plant and these dynamics lead to the observed changes. [sent-180, score-0.356]
</p><p>78 A) The gain is ﬁrst adapted up until it reaches about 1. [sent-192, score-0.3]
</p><p>79 Once the gain reaches 1 again it is adapted up with a positive target jump again. [sent-195, score-0.415]
</p><p>80 B) The speed of adaptation is compared between the ﬁrst adaptation and the second positive adaptation. [sent-197, score-0.536]
</p><p>81 The gain used by the monkey is changing during this interval. [sent-201, score-0.353]
</p><p>82 3 Discussion Traditional models of adaptation simply change motor commands to reduce prediction errors [13]. [sent-203, score-0.57]
</p><p>83 (1) The system represents its knowledge of the properties of the motor system at different timescales and explicitly models how these disturbances evolve over time. [sent-205, score-0.913]
</p><p>84 (3) It formulates the computational aim of adaptation in terms of optimally predicting ongoing changes in the properties of the motor plant. [sent-207, score-0.567]
</p><p>85 Two timescales had been proposed in the context of connectionist learning theory [11]. [sent-210, score-0.354]
</p><p>86 [10] proposed a model where the motor system responds to error with two systems: one that is highly sensitive to error but rapidly forgets and another that has poor sensitivity to error but has strong retention. [sent-212, score-0.295]
</p><p>87 Even the earliest studies of oculomotor adaptation realized that the objective of adaptation is to allow precise movement with a relentlessly changing motor plant [3]. [sent-214, score-1.099]
</p><p>88 Multi timescale adaptation and learning is a near universal phenomenon [14, 8, 16, 17]. [sent-216, score-0.558]
</p><p>89 Multiscale learning in cognitive systems may be a result of a system that has originally evolved to deal with ever changing motor problems. [sent-220, score-0.322]
</p><p>90 Multiscale adaptation can also be seen in the way visual neurons adapt to changing visual stimuli [16]. [sent-221, score-0.373]
</p><p>91 The phenomenon of spontaneous recovery in classical conditioning [19, 20] is largely equivalent to the ﬁndings of Kojima et al [12] and can also be explained within the Bayesian multiscale learner framework. [sent-222, score-0.355]
</p><p>92 The presented model obviously does not explain all known effects in motor or even saccadic gain adaptation. [sent-223, score-0.819]
</p><p>93 Moreover it seems that adaptation speed of monkeys can be very different on one day than the other and from one experimental setting to the other (e. [sent-225, score-0.442]
</p><p>94 An important question for further enquiry is how the nervous system solves problems that require multiple timescale adaptation. [sent-231, score-0.411]
</p><p>95 The characteristics and neuronal substrate of saccadic eye movement plasticity. [sent-246, score-0.388]
</p><p>96 Illusory shifts in visual direction accompany adaptation of saccadic eye movements. [sent-263, score-0.639]
</p><p>97 Distinct short-term and long-term adaptation to reduce saccade size in monkey. [sent-276, score-0.591]
</p><p>98 Interacting adaptive processes with different timescales underlie short-term motor learning. [sent-296, score-0.607]
</p><p>99 Memory of learning facilitates saccadic adaptation in the monkey. [sent-307, score-0.575]
</p><p>100 Learning to learn- optimal adjustment of the rate at which the motor system adapts. [sent-381, score-0.315]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('timescales', 0.354), ('saccade', 0.323), ('saccadic', 0.307), ('adaptation', 0.268), ('timescale', 0.261), ('gain', 0.251), ('motor', 0.231), ('disturbances', 0.2), ('darkness', 0.2), ('disturbance', 0.187), ('saccades', 0.187), ('plant', 0.146), ('learner', 0.125), ('oculomotor', 0.12), ('day', 0.117), ('kojima', 0.108), ('hopp', 0.092), ('fast', 0.089), ('nervous', 0.086), ('slow', 0.084), ('jump', 0.078), ('fuchs', 0.077), ('monkey', 0.075), ('uncertainty', 0.072), ('reversal', 0.064), ('progressively', 0.064), ('system', 0.064), ('al', 0.059), ('multiscale', 0.058), ('monkeys', 0.057), ('paradigm', 0.055), ('bayesian', 0.055), ('psychol', 0.053), ('robinson', 0.049), ('change', 0.048), ('session', 0.047), ('changes', 0.047), ('bahrick', 0.046), ('timecourse', 0.046), ('washout', 0.046), ('dark', 0.045), ('sketched', 0.043), ('eye', 0.042), ('deprivation', 0.04), ('movement', 0.039), ('slowly', 0.038), ('target', 0.037), ('reprinted', 0.037), ('decay', 0.035), ('fatigue', 0.034), ('adapt', 0.034), ('belief', 0.033), ('yellow', 0.032), ('adapting', 0.032), ('sensory', 0.031), ('spontaneous', 0.031), ('behav', 0.031), ('sacca', 0.031), ('timecourses', 0.031), ('effects', 0.03), ('ga', 0.029), ('recovery', 0.029), ('faster', 0.029), ('phenomenon', 0.029), ('smith', 0.028), ('reversals', 0.027), ('reza', 0.027), ('changing', 0.027), ('increased', 0.026), ('feedback', 0.026), ('affected', 0.025), ('kalman', 0.025), ('adapted', 0.025), ('movements', 0.025), ('erlbaum', 0.024), ('spacing', 0.024), ('probe', 0.024), ('timestep', 0.024), ('konrad', 0.024), ('neurophysiol', 0.024), ('reaches', 0.024), ('et', 0.024), ('increase', 0.023), ('des', 0.023), ('commands', 0.023), ('cancel', 0.023), ('observes', 0.023), ('adaptive', 0.022), ('visual', 0.022), ('slower', 0.022), ('gains', 0.022), ('neurosci', 0.021), ('estimates', 0.021), ('estimate', 0.021), ('optimally', 0.021), ('trials', 0.021), ('adapts', 0.02), ('adjustment', 0.02), ('disease', 0.02), ('behavioral', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="141-tfidf-1" href="./nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation.html">141 nips-2006-Multiple timescales and uncertainty in motor adaptation</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum, Reza Shadmehr</p><p>Abstract: Our motor system changes due to causes that span multiple timescales. For example, muscle response can change because of fatigue, a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower. Here we hypothesize that the nervous system adapts in a way that reﬂects the temporal properties of such potential disturbances. According to a Bayesian formulation of this idea, movement error results in a credit assignment problem: what timescale is responsible for this disturbance? The adaptation schedule inﬂuences the behavior of the optimal learner, changing estimates at different timescales as well as the uncertainty. A system that adapts in this way predicts many properties observed in saccadic gain adaptation. It well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction.</p><p>2 0.11578614 <a title="141-tfidf-2" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>Author: Gregory Shakhnarovich, Sung-phil Kim, Michael J. Black</p><p>Abstract: Neural motor prostheses (NMPs) require the accurate decoding of motor cortical population activity for the control of an artiﬁcial motor system. Previous work on cortical decoding for NMPs has focused on the recovery of hand kinematics. Human NMPs however may require the control of computer cursors or robotic devices with very different physical and dynamical properties. Here we show that the ﬁring rates of cells in the primary motor cortex of non-human primates can be used to control the parameters of an artiﬁcial physical system exhibiting realistic dynamics. The model represents 2D hand motion in terms of a point mass connected to a system of idealized springs. The nonlinear spring coefﬁcients are estimated from the ﬁring rates of neurons in the motor cortex. We evaluate linear and a nonlinear decoding algorithms using neural recordings from two monkeys performing two different tasks. We found that the decoded spring coefﬁcients produced accurate hand trajectories compared with state-of-the-art methods for direct decoding of hand kinematics. Furthermore, using a physically-based system produced decoded movements that were more “natural” in that their frequency spectrum more closely matched that of natural hand movements. 1</p><p>3 0.098806024 <a title="141-tfidf-3" href="./nips-2006-Adaptive_Spatial_Filters_with_predefined_Region_of_Interest_for_EEG_based_Brain-Computer-Interfaces.html">22 nips-2006-Adaptive Spatial Filters with predefined Region of Interest for EEG based Brain-Computer-Interfaces</a></p>
<p>Author: Moritz Grosse-wentrup, Klaus Gramann, Martin Buss</p><p>Abstract: The performance of EEG-based Brain-Computer-Interfaces (BCIs) critically depends on the extraction of features from the EEG carrying information relevant for the classiﬁcation of different mental states. For BCIs employing imaginary movements of different limbs, the method of Common Spatial Patterns (CSP) has been shown to achieve excellent classiﬁcation results. The CSP-algorithm however suffers from a lack of robustness, requiring training data without artifacts for good performance. To overcome this lack of robustness, we propose an adaptive spatial ﬁlter that replaces the training data in the CSP approach by a-priori information. More speciﬁcally, we design an adaptive spatial ﬁlter that maximizes the ratio of the variance of the electric ﬁeld originating in a predeﬁned region of interest (ROI) and the overall variance of the measured EEG. Since it is known that the component of the EEG used for discriminating imaginary movements originates in the motor cortex, we design two adaptive spatial ﬁlters with the ROIs centered in the hand areas of the left and right motor cortex. We then use these to classify EEG data recorded during imaginary movements of the right and left hand of three subjects, and show that the adaptive spatial ﬁlters outperform the CSP-algorithm, enabling classiﬁcation rates of up to 94.7 % without artifact rejection. 1</p><p>4 0.098750532 <a title="141-tfidf-4" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum</p><p>Abstract: Many recent studies analyze how data from different modalities can be combined. Often this is modeled as a system that optimally combines several sources of information about the same variable. However, it has long been realized that this information combining depends on the interpretation of the data. Two cues that are perceived by different modalities can have different causal relationships: (1) They can both have the same cause, in this case we should fully integrate both cues into a joint estimate. (2) They can have distinct causes, in which case information should be processed independently. In many cases we will not know if there is one joint cause or two independent causes that are responsible for the cues. Here we model this situation as a Bayesian estimation problem. We are thus able to explain some experiments on visual auditory cue combination as well as some experiments on visual proprioceptive cue integration. Our analysis shows that the problem solved by people when they combine cues to produce a movement is much more complicated than is usually assumed, because they need to infer the causal structure that is underlying their sensory experience.</p><p>5 0.069807142 <a title="141-tfidf-5" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>Author: Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira</p><p>Abstract: Discriminative learning methods for classiﬁcation perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classiﬁer which performs well on a target domain with a different distribution. Under what conditions can we adapt a classiﬁer trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justiﬁcation for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set. 1</p><p>6 0.06763909 <a title="141-tfidf-6" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>7 0.06449911 <a title="141-tfidf-7" href="./nips-2006-Learning_Time-Intensity_Profiles_of_Human_Activity_using_Non-Parametric_Bayesian_Models.html">114 nips-2006-Learning Time-Intensity Profiles of Human Activity using Non-Parametric Bayesian Models</a></p>
<p>8 0.061435282 <a title="141-tfidf-8" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>9 0.057640396 <a title="141-tfidf-9" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>10 0.053543113 <a title="141-tfidf-10" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>11 0.051794656 <a title="141-tfidf-11" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>12 0.05085193 <a title="141-tfidf-12" href="./nips-2006-Optimal_Single-Class_Classification_Strategies.html">155 nips-2006-Optimal Single-Class Classification Strategies</a></p>
<p>13 0.050518848 <a title="141-tfidf-13" href="./nips-2006-Aggregating_Classification_Accuracy_across_Time%3A_Application_to_Single_Trial_EEG.html">24 nips-2006-Aggregating Classification Accuracy across Time: Application to Single Trial EEG</a></p>
<p>14 0.050405171 <a title="141-tfidf-14" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>15 0.050175328 <a title="141-tfidf-15" href="./nips-2006-A_selective_attention_multi--chip_system_with_dynamic_synapses_and_spiking_neurons.html">18 nips-2006-A selective attention multi--chip system with dynamic synapses and spiking neurons</a></p>
<p>16 0.044116627 <a title="141-tfidf-16" href="./nips-2006-Reducing_Calibration_Time_For_Brain-Computer_Interfaces%3A_A_Clustering_Approach.html">168 nips-2006-Reducing Calibration Time For Brain-Computer Interfaces: A Clustering Approach</a></p>
<p>17 0.043035854 <a title="141-tfidf-17" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>18 0.038414214 <a title="141-tfidf-18" href="./nips-2006-A_Bayesian_Approach_to_Diffusion_Models_of_Decision-Making_and_Response_Time.html">1 nips-2006-A Bayesian Approach to Diffusion Models of Decision-Making and Response Time</a></p>
<p>19 0.037089452 <a title="141-tfidf-19" href="./nips-2006-Logistic_Regression_for_Single_Trial_EEG_Classification.html">126 nips-2006-Logistic Regression for Single Trial EEG Classification</a></p>
<p>20 0.03708427 <a title="141-tfidf-20" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.108), (1, -0.066), (2, 0.026), (3, -0.051), (4, -0.109), (5, -0.062), (6, 0.03), (7, 0.026), (8, -0.001), (9, -0.047), (10, 0.014), (11, 0.015), (12, -0.035), (13, -0.008), (14, -0.039), (15, -0.046), (16, -0.007), (17, -0.005), (18, -0.087), (19, -0.008), (20, -0.006), (21, 0.014), (22, -0.05), (23, -0.005), (24, -0.096), (25, 0.097), (26, -0.051), (27, 0.013), (28, -0.054), (29, -0.009), (30, -0.095), (31, -0.22), (32, 0.187), (33, -0.052), (34, -0.235), (35, 0.015), (36, -0.021), (37, -0.187), (38, 0.074), (39, -0.017), (40, -0.053), (41, -0.055), (42, -0.109), (43, -0.1), (44, -0.009), (45, 0.055), (46, -0.034), (47, -0.005), (48, -0.013), (49, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97137141 <a title="141-lsi-1" href="./nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation.html">141 nips-2006-Multiple timescales and uncertainty in motor adaptation</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum, Reza Shadmehr</p><p>Abstract: Our motor system changes due to causes that span multiple timescales. For example, muscle response can change because of fatigue, a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower. Here we hypothesize that the nervous system adapts in a way that reﬂects the temporal properties of such potential disturbances. According to a Bayesian formulation of this idea, movement error results in a credit assignment problem: what timescale is responsible for this disturbance? The adaptation schedule inﬂuences the behavior of the optimal learner, changing estimates at different timescales as well as the uncertainty. A system that adapts in this way predicts many properties observed in saccadic gain adaptation. It well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction.</p><p>2 0.75067902 <a title="141-lsi-2" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum</p><p>Abstract: Many recent studies analyze how data from different modalities can be combined. Often this is modeled as a system that optimally combines several sources of information about the same variable. However, it has long been realized that this information combining depends on the interpretation of the data. Two cues that are perceived by different modalities can have different causal relationships: (1) They can both have the same cause, in this case we should fully integrate both cues into a joint estimate. (2) They can have distinct causes, in which case information should be processed independently. In many cases we will not know if there is one joint cause or two independent causes that are responsible for the cues. Here we model this situation as a Bayesian estimation problem. We are thus able to explain some experiments on visual auditory cue combination as well as some experiments on visual proprioceptive cue integration. Our analysis shows that the problem solved by people when they combine cues to produce a movement is much more complicated than is usually assumed, because they need to infer the causal structure that is underlying their sensory experience.</p><p>3 0.67244971 <a title="141-lsi-3" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>Author: Gregory Shakhnarovich, Sung-phil Kim, Michael J. Black</p><p>Abstract: Neural motor prostheses (NMPs) require the accurate decoding of motor cortical population activity for the control of an artiﬁcial motor system. Previous work on cortical decoding for NMPs has focused on the recovery of hand kinematics. Human NMPs however may require the control of computer cursors or robotic devices with very different physical and dynamical properties. Here we show that the ﬁring rates of cells in the primary motor cortex of non-human primates can be used to control the parameters of an artiﬁcial physical system exhibiting realistic dynamics. The model represents 2D hand motion in terms of a point mass connected to a system of idealized springs. The nonlinear spring coefﬁcients are estimated from the ﬁring rates of neurons in the motor cortex. We evaluate linear and a nonlinear decoding algorithms using neural recordings from two monkeys performing two different tasks. We found that the decoded spring coefﬁcients produced accurate hand trajectories compared with state-of-the-art methods for direct decoding of hand kinematics. Furthermore, using a physically-based system produced decoded movements that were more “natural” in that their frequency spectrum more closely matched that of natural hand movements. 1</p><p>4 0.41656867 <a title="141-lsi-4" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>Author: Gediminas Lukšys, Jérémie Knüsel, Denis Sheynikhovich, Carmen Sandi, Wulfram Gerstner</p><p>Abstract: Stress and genetic background regulate different aspects of behavioral learning through the action of stress hormones and neuromodulators. In reinforcement learning (RL) models, meta-parameters such as learning rate, future reward discount factor, and exploitation-exploration factor, control learning dynamics and performance. They are hypothesized to be related to neuromodulatory levels in the brain. We found that many aspects of animal learning and performance can be described by simple RL models using dynamic control of the meta-parameters. To study the effects of stress and genotype, we carried out 5-hole-box light conditioning and Morris water maze experiments with C57BL/6 and DBA/2 mouse strains. The animals were exposed to different kinds of stress to evaluate its effects on immediate performance as well as on long-term memory. Then, we used RL models to simulate their behavior. For each experimental session, we estimated a set of model meta-parameters that produced the best ﬁt between the model and the animal performance. The dynamics of several estimated meta-parameters were qualitatively similar for the two simulated experiments, and with statistically signiﬁcant differences between different genetic strains and stress conditions. 1</p><p>5 0.39836881 <a title="141-lsi-5" href="./nips-2006-Optimal_Single-Class_Classification_Strategies.html">155 nips-2006-Optimal Single-Class Classification Strategies</a></p>
<p>Author: Ran El-Yaniv, Mordechai Nisenson</p><p>Abstract: We consider single-class classiﬁcation (SCC) as a two-person game between the learner and an adversary. In this game the target distribution is completely known to the learner and the learner’s goal is to construct a classiﬁer capable of guaranteeing a given tolerance for the false-positive error while minimizing the false negative error. We identify both “hard” and “soft” optimal classiﬁcation strategies for different types of games and demonstrate that soft classiﬁcation can provide a signiﬁcant advantage. Our optimal strategies and bounds provide worst-case lower bounds for standard, ﬁnite-sample SCC and also motivate new approaches to solving SCC.</p><p>6 0.39791867 <a title="141-lsi-6" href="./nips-2006-Adaptive_Spatial_Filters_with_predefined_Region_of_Interest_for_EEG_based_Brain-Computer-Interfaces.html">22 nips-2006-Adaptive Spatial Filters with predefined Region of Interest for EEG based Brain-Computer-Interfaces</a></p>
<p>7 0.38262224 <a title="141-lsi-7" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>8 0.38188934 <a title="141-lsi-8" href="./nips-2006-Learning_Time-Intensity_Profiles_of_Human_Activity_using_Non-Parametric_Bayesian_Models.html">114 nips-2006-Learning Time-Intensity Profiles of Human Activity using Non-Parametric Bayesian Models</a></p>
<p>9 0.36894503 <a title="141-lsi-9" href="./nips-2006-Aggregating_Classification_Accuracy_across_Time%3A_Application_to_Single_Trial_EEG.html">24 nips-2006-Aggregating Classification Accuracy across Time: Application to Single Trial EEG</a></p>
<p>10 0.33696181 <a title="141-lsi-10" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>11 0.29628605 <a title="141-lsi-11" href="./nips-2006-Combining_causal_and_similarity-based_reasoning.html">53 nips-2006-Combining causal and similarity-based reasoning</a></p>
<p>12 0.29226527 <a title="141-lsi-12" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>13 0.2668193 <a title="141-lsi-13" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>14 0.25981498 <a title="141-lsi-14" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>15 0.2576803 <a title="141-lsi-15" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>16 0.25162446 <a title="141-lsi-16" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>17 0.25136423 <a title="141-lsi-17" href="./nips-2006-Context_Effects_in_Category_Learning%3A_An_Investigation_of_Four_Probabilistic_Models.html">58 nips-2006-Context Effects in Category Learning: An Investigation of Four Probabilistic Models</a></p>
<p>18 0.23902388 <a title="141-lsi-18" href="./nips-2006-A_Bayesian_Approach_to_Diffusion_Models_of_Decision-Making_and_Response_Time.html">1 nips-2006-A Bayesian Approach to Diffusion Models of Decision-Making and Response Time</a></p>
<p>19 0.23760071 <a title="141-lsi-19" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>20 0.23705442 <a title="141-lsi-20" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.075), (3, 0.025), (7, 0.051), (9, 0.039), (20, 0.027), (22, 0.028), (25, 0.01), (44, 0.058), (47, 0.41), (57, 0.074), (65, 0.028), (69, 0.026), (71, 0.033), (82, 0.017), (90, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82005793 <a title="141-lda-1" href="./nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation.html">141 nips-2006-Multiple timescales and uncertainty in motor adaptation</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum, Reza Shadmehr</p><p>Abstract: Our motor system changes due to causes that span multiple timescales. For example, muscle response can change because of fatigue, a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower. Here we hypothesize that the nervous system adapts in a way that reﬂects the temporal properties of such potential disturbances. According to a Bayesian formulation of this idea, movement error results in a credit assignment problem: what timescale is responsible for this disturbance? The adaptation schedule inﬂuences the behavior of the optimal learner, changing estimates at different timescales as well as the uncertainty. A system that adapts in this way predicts many properties observed in saccadic gain adaptation. It well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction.</p><p>2 0.74993932 <a title="141-lda-2" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>Author: Gregory Shakhnarovich, Sung-phil Kim, Michael J. Black</p><p>Abstract: Neural motor prostheses (NMPs) require the accurate decoding of motor cortical population activity for the control of an artiﬁcial motor system. Previous work on cortical decoding for NMPs has focused on the recovery of hand kinematics. Human NMPs however may require the control of computer cursors or robotic devices with very different physical and dynamical properties. Here we show that the ﬁring rates of cells in the primary motor cortex of non-human primates can be used to control the parameters of an artiﬁcial physical system exhibiting realistic dynamics. The model represents 2D hand motion in terms of a point mass connected to a system of idealized springs. The nonlinear spring coefﬁcients are estimated from the ﬁring rates of neurons in the motor cortex. We evaluate linear and a nonlinear decoding algorithms using neural recordings from two monkeys performing two different tasks. We found that the decoded spring coefﬁcients produced accurate hand trajectories compared with state-of-the-art methods for direct decoding of hand kinematics. Furthermore, using a physically-based system produced decoded movements that were more “natural” in that their frequency spectrum more closely matched that of natural hand movements. 1</p><p>3 0.34157097 <a title="141-lda-3" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum</p><p>Abstract: Many recent studies analyze how data from different modalities can be combined. Often this is modeled as a system that optimally combines several sources of information about the same variable. However, it has long been realized that this information combining depends on the interpretation of the data. Two cues that are perceived by different modalities can have different causal relationships: (1) They can both have the same cause, in this case we should fully integrate both cues into a joint estimate. (2) They can have distinct causes, in which case information should be processed independently. In many cases we will not know if there is one joint cause or two independent causes that are responsible for the cues. Here we model this situation as a Bayesian estimation problem. We are thus able to explain some experiments on visual auditory cue combination as well as some experiments on visual proprioceptive cue integration. Our analysis shows that the problem solved by people when they combine cues to produce a movement is much more complicated than is usually assumed, because they need to infer the causal structure that is underlying their sensory experience.</p><p>4 0.3361671 <a title="141-lda-4" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>Author: Angela J. Yu</p><p>Abstract: Survival in a non-stationary, potentially adversarial environment requires animals to detect sensory changes rapidly yet accurately, two oft competing desiderata. Neurons subserving such detections are faced with the corresponding challenge to discern “real” changes in inputs as quickly as possible, while ignoring noisy ﬂuctuations. Mathematically, this is an example of a change-detection problem that is actively researched in the controlled stochastic processes community. In this paper, we utilize sophisticated tools developed in that community to formalize an instantiation of the problem faced by the nervous system, and characterize the Bayes-optimal decision policy under certain assumptions. We will derive from this optimal strategy an information accumulation and decision process that remarkably resembles the dynamics of a leaky integrate-and-ﬁre neuron. This correspondence suggests that neurons are optimized for tracking input changes, and sheds new light on the computational import of intracellular properties such as resting membrane potential, voltage-dependent conductance, and post-spike reset voltage. We also explore the inﬂuence that factors such as timing, uncertainty, neuromodulation, and reward should and do have on neuronal dynamics and sensitivity, as the optimal decision strategy depends critically on these factors. 1</p><p>5 0.33261073 <a title="141-lda-5" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>Author: Thomas Voegtlin</p><p>Abstract: In biological neurons, the timing of a spike depends on the timing of synaptic currents, in a way that is classically described by the Phase Response Curve. This has implications for temporal coding: an action potential that arrives on a synapse has an implicit meaning, that depends on the position of the postsynaptic neuron on the ﬁring cycle. Here we show that this implicit code can be used to perform computations. Using theta neurons, we derive a spike-timing dependent learning rule from an error criterion. We demonstrate how to train an auto-encoder neural network using this rule. 1</p><p>6 0.33239496 <a title="141-lda-6" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>7 0.3297497 <a title="141-lda-7" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>8 0.32917523 <a title="141-lda-8" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>9 0.32559776 <a title="141-lda-9" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>10 0.32527134 <a title="141-lda-10" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>11 0.32483104 <a title="141-lda-11" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>12 0.32427287 <a title="141-lda-12" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>13 0.32379726 <a title="141-lda-13" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>14 0.32379356 <a title="141-lda-14" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>15 0.32297409 <a title="141-lda-15" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>16 0.32217565 <a title="141-lda-16" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>17 0.32125703 <a title="141-lda-17" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>18 0.32117999 <a title="141-lda-18" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>19 0.31995761 <a title="141-lda-19" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>20 0.31971699 <a title="141-lda-20" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
