<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2006-Stability of $K$-Means Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-181" href="#">nips2006-181</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 nips-2006-Stability of $K$-Means Clustering</h1>
<br/><p>Source: <a title="nips-2006-181-pdf" href="http://papers.nips.cc/paper/3116-stability-of-k-means-clustering.pdf">pdf</a></p><p>Author: Alexander Rakhlin, Andrea Caponnetto</p><p>Abstract: We phrase K-means clustering as an empirical risk minimization procedure over a class HK and explicitly calculate the covering number for this class. Next, we show that stability of K-means clustering is characterized by the geometry of HK with respect to the underlying distribution. We prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of Ω(n1/2 ) samples deﬁnes the transition between stability and instability. While for a ﬁnite number of minimizers this result follows from multinomial distribution estimates, the case of inﬁnite minimizers requires more reﬁned tools. We conclude by proving that stability of the functions in HK implies stability of the actual centers of the clusters. Since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for the choice of K. 1</p><p>Reference: <a title="nips-2006-181-reference" href="../nips2006_reference/nips-2006-Stability_of_%24K%24-Means_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We phrase K-means clustering as an empirical risk minimization procedure over a class HK and explicitly calculate the covering number for this class. [sent-8, score-0.44]
</p><p>2 Next, we show that stability of K-means clustering is characterized by the geometry of HK with respect to the underlying distribution. [sent-9, score-0.69]
</p><p>3 We prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of Ω(n1/2 ) samples deﬁnes the transition between stability and instability. [sent-10, score-0.829]
</p><p>4 While for a ﬁnite number of minimizers this result follows from multinomial distribution estimates, the case of inﬁnite minimizers requires more reﬁned tools. [sent-11, score-0.706]
</p><p>5 We conclude by proving that stability of the functions in HK implies stability of the actual centers of the clusters. [sent-12, score-0.963]
</p><p>6 Since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for the choice of K. [sent-13, score-0.5]
</p><p>7 Part of the difﬁculty comes from the absence, in general, of an objective way to assess the clustering quality and to compare two groupings of the data. [sent-16, score-0.239]
</p><p>8 In particular, attempts have been made by [4, 2, 3] to study and theoretically justify the stability-based approach of evaluating the quality of clustering solutions. [sent-18, score-0.214]
</p><p>9 Building upon these ideas, we present a characterization of clustering stability in terms of the geometry of the function class associated with minimizing the objective function. [sent-19, score-0.706]
</p><p>10 To simplify the exposition, we focus on K-means clustering, although the analogous results can be derived for K-medians and other clustering algorithms which minimize an objective function. [sent-20, score-0.239]
</p><p>11 Let us ﬁrst motivate the notion of clustering stability. [sent-21, score-0.186]
</p><p>12 While for a ﬁxed K, two clustering solutions can be compared according to the K-means objective function (see the next section), it is not meaningful to compare the value of the objective function for different K. [sent-22, score-0.312]
</p><p>13 The approach stipulates that, for each K in some range, several clustering solutions should be computed by sub-sampling or perturbing the data. [sent-27, score-0.225]
</p><p>14 The best value of K is that for which the clustering solutions  are most “similar”. [sent-28, score-0.206]
</p><p>15 For instance, Ben-Hur et al [5] randomly choose overlapping portions of the data and evaluate the distance between the resulting clustering solutions on the common samples. [sent-31, score-0.269]
</p><p>16 Similarly, Ben-David et al [3, 2] study stability with respect to complete change of the data (independent draw). [sent-33, score-0.52]
</p><p>17 These different approaches of choosing K prompted us to give a precise characterization of clustering stability with respect to both complete and partial changes of the data. [sent-34, score-0.738]
</p><p>18 It has been noted by [6, 4, 3] that the stability of clustering with respect to complete change of the data is characterized by the uniqueness of the minimum of the objective function with respect to the true distribution. [sent-35, score-0.828]
</p><p>19 Indeed, minimization of the K-means objective function can be phrased as an empirical risk minimization procedure (see [7]). [sent-36, score-0.318]
</p><p>20 The stability follows, under some regularity assumptions, from the convergence of empirical and expected means over a Glivenko-Cantelli class of functions. [sent-37, score-0.439]
</p><p>21 We prove stability in the case of a unique minimizer by explicitly computing the covering number in the next section and noting that the resulting class is VC-type. [sent-38, score-0.642]
</p><p>22 We go further in our analysis by considering the other two interesting cases: ﬁnite and inﬁnite number of minimizers of the objective function. [sent-39, score-0.388]
</p><p>23 With the help of a stability result of [8, 9] for empirical risk minimization, we are able to prove that K-means clustering is stable with respect √ √ to changes of o( n) samples, where n is the total number of samples. [sent-40, score-0.819]
</p><p>24 In fact, the rate of Ω( n) changes is a sharp transition between stability and instability in these cases. [sent-41, score-0.443]
</p><p>25 The goal of clustering is to ﬁnd a good partition based on the sample Z1 , . [sent-51, score-0.206]
</p><p>26 (1)  It is easy to verify that the (scaled) within-point scatter can be rewritten as W (C) =  1 n  K  k=1 i:C(Zi )=k  Zi − ck  2  (2)  where ck is the mean of the k-th cluster based on the assignment C (see Figure 1). [sent-59, score-0.228]
</p><p>27 We are interested in the minimizers of the within-point scatter. [sent-60, score-0.335]
</p><p>28 The K-means clustering algorithm is an alternating procedure minimizing the within-point scatter W (C). [sent-66, score-0.252]
</p><p>29 The centers {ck }K are computed in the ﬁrst step, following by the assignment of each Zi k=1 to its closest center ck ; the procedure is repeated. [sent-67, score-0.318]
</p><p>30 In this paper, we are not concerned with the algorithmic issues of the minimization procedure. [sent-69, score-0.055]
</p><p>31 Rather, we study stability properties of the minimizers of W (C). [sent-70, score-0.706]
</p><p>32 The problem of minimizing W (C) can be phrased as empirical risk minimization [7] over the function class HK = {hA (z) = z − ai 2 , i = argmin z − aj j∈{1. [sent-71, score-0.366]
</p><p>33 , aK } ∈ Z K },  We have scaled the within-point scatter by 1/n if compared to [10]. [sent-77, score-0.066]
</p><p>34 (3)  ck − Zi c1  2  c2  Figure 1: The clustering objective is to place the centers ck to minimize the sum of squared distances from points to their closest centers. [sent-78, score-0.638]
</p><p>35 Functions hA (z) in HK can also be written as K  hA (z) = i=1  z − ai 2 I(z is closest to ai ),  where ties are broken, for instance, in the order of ai ’s. [sent-80, score-0.335]
</p><p>36 Hence, functions hA ∈ HK are K parabolas glued together with centers at a1 , . [sent-81, score-0.221]
</p><p>37 Hence, we will interchangeably use C and hC as minimizers of the within-point scatter. [sent-87, score-0.335]
</p><p>38 One choice is to measure the similarity between the centers {ak }K and {bk }K of clusterings A and B. [sent-92, score-0.351]
</p><p>39 3  Covering Number for HK  The following technical Lemma shows that a covering of the ball B2 (0, R) induces a cover of HK in the L∞ distance because small shifts of the centers imply small changes of the corresponding functions in HK . [sent-95, score-0.441]
</p><p>40 It is well-known that a Euclidean ball of radius R in Rm can be covered by N = 4R+δ δ balls of radius δ (see Lemma 2. [sent-100, score-0.104]
</p><p>41 Consider an arbitrary function hA ∈ HK with centers at {a1 , . [sent-106, score-0.19]
</p><p>42 We iterate through all the ai ’s, replacing them by the members of T . [sent-116, score-0.096]
</p><p>43 After K steps, hA − hAK ∞ ≤ 4RKδ and all centers of AK belong to T . [sent-117, score-0.19]
</p><p>44 Hence, each function hA ∈ H can be approximated to within 4RKδ by functions with centers in a ﬁnite set T . [sent-118, score-0.221]
</p><p>45 The upper bound on the number of functions in mK functions cover HK to within 4RKδ in HK with centers in T is N K . [sent-119, score-0.279]
</p><p>46 Geometry of HK and Stability  4  The above Lemma shows that HK is not too rich, as its covering numbers are polynomial. [sent-122, score-0.076]
</p><p>47 This is the ﬁrst important aspect in the study of clustering stability. [sent-123, score-0.186]
</p><p>48 The second aspect is the geometry of HK with respect to the measure P . [sent-124, score-0.125]
</p><p>49 In particular, stability of K-means clustering depends on the number of functions h ∈ HK with the minimum expectation Eh. [sent-125, score-0.588]
</p><p>50 Note that the number of minimizers depends only on P and K, and not on the data. [sent-126, score-0.335]
</p><p>51 Since Z is closed, the number of minimizers is at least one. [sent-127, score-0.335]
</p><p>52 The three important cases are: unique minimum, a ﬁnite number of minimizers (greater than one), and an inﬁnite number of minimizers. [sent-128, score-0.387]
</p><p>53 In the case of a unique minimum of Eh, one can show that the diameter of Qε tends to zero as P → 0. [sent-133, score-0.079]
</p><p>54 Hence, empirical averages of functions in HK uniformly converge to their expectations: n 1 lim P sup Eh − h(Zi ) > ε = 0. [sent-137, score-0.117]
</p><p>55 → h ∈HK  Assuming the existence of a unique minimizer, i. [sent-150, score-0.052]
</p><p>56 Suppose the clustering A minimizes W (C) over the set {Z1 , . [sent-171, score-0.186]
</p><p>57 →  We have shown that in the case of a unique minimizer of the objective function (with respect to the distribution), two clusterings over independently drawn sets of points become arbitrarily close to each other with increasing probability as the number of points increases. [sent-179, score-0.385]
</p><p>58 If there are ﬁnite (but greater than one) number of minimizers h ∈ HK of Eh, multinomial distri√ bution estimates tell us that we expect stability with respect to o( n) changes of points, while no √ stability is expected for Ω( n) changes, as the next example shows. [sent-180, score-1.212]
</p><p>59 Consider 1-mean minimization over Z = {x1 , x2 }, x1 = x2 , and P = 2 (δx1 + δx2 ). [sent-182, score-0.055]
</p><p>60 , Zn , the center of the minimizer of W (C) is either x1 or x2 , according to the majority vote over the training set. [sent-186, score-0.119]
</p><p>61 , Zn , it is possible to swap the majority vote with constant probability. [sent-190, score-0.063]
</p><p>62 Moreover, with probability approaching one, it is not possible to √ achieve the swap by a change of o( n) points. [sent-191, score-0.058]
</p><p>63 The above example shows that, in general, it is not possible to prove closeness of clusterings over √ two sets of samples differing on Ω( n) elements. [sent-193, score-0.199]
</p><p>64 Indeed, by employing the following Theorem, proven in [8, 9], we can show that even in the case of an inﬁnite √ number of minimizers, clusterings over two sets of samples differing on o( n) elements become arbitrarily close with increasing probability as the number of samples increases. [sent-195, score-0.166]
</p><p>65 This result cannot be deduced from the multinomial estimates, as it relies on the control of ﬂuctuations of empirical means over a Donsker class. [sent-196, score-0.083]
</p><p>66 Assume that the class of functions F over Z is uniformly bounded and P -Donsker, for some probability measure P over Z. [sent-200, score-0.073]
</p><p>67 Let f (S) and f (T ) be minimizers over F of the empirical√ averages with respect to the sets S and T of n points i. [sent-201, score-0.405]
</p><p>68 →  We apply the above theorem to HK which is P -Donsker for any P because its covering numbers in L∞ scale polynomially (see Lemma 3. [sent-206, score-0.076]
</p><p>69 We note that if the class HK were richer than P -Donsker, the stability result would not necessarily hold. [sent-209, score-0.392]
</p><p>70 Suppose the clusterings A and B are minimizers of the K-means objective W (C) √ over the sets S and T , respectively. [sent-212, score-0.528]
</p><p>71 →  The above Corollary holds even if the number of minimizers h ∈ HK of Eh is inﬁnite. [sent-215, score-0.335]
</p><p>72 This concludes the analysis of stability of K-means for the three interesting cases: unique minimizer, ﬁnite number (greater than one) of minimizers, and inﬁnite number of minimizers. [sent-216, score-0.423]
</p><p>73 We have proved that stability of K-means clustering is characterized by the geometry of the class HK with respect to P . [sent-218, score-0.738]
</p><p>74 It is evident that the choice of K maximizing stability of clustering aims to choose K for which there is a unique minimizer. [sent-219, score-0.609]
</p><p>75 Unfortunately, √ “small” n, stability with respect for to a complete change of the data and stability with respect to o( n) changes are indistinguishable, making this rule of thumb questionable. [sent-220, score-0.997]
</p><p>76 Moreover, as noted in [3], small changes of P lead to drastic changes in the number of minimizers. [sent-221, score-0.096]
</p><p>77 5  Stability of the Centers  Intuitively, stability of functions hA with respect to perturbation of the data Z1 , . [sent-222, score-0.453]
</p><p>78 , Zn implies stability of the centers of the clusters. [sent-225, score-0.561]
</p><p>79 Let us ﬁrst deﬁne a notion of distance between centers of two clusterings. [sent-227, score-0.215]
</p><p>80 , bK } are centers of two clusterings A and B, respectively. [sent-236, score-0.33]
</p><p>81 Deﬁne a distance between these clusterings as dmax ({a1 , . [sent-237, score-0.253]
</p><p>82 , bK }) := max  min ( ai − bj + aj − bi )  1≤i≤K 1≤j≤K  Lemma 5. [sent-243, score-0.19]
</p><p>83 Assume the density of P (with respect to the Lebesgue measure λ over Z) is bounded away from 0, i. [sent-245, score-0.072]
</p><p>84 , bK }) ≤ 2 max  max  min  1≤i≤K 1≤j≤K  ai − bj , max  min  1≤i≤K 1≤j≤K  aj − bi  Without loss of generality, assume that the maximum on the right-hand side is attained at a1 and b1 such that b1 is the closest center to a1 out of {b1 , . [sent-263, score-0.257]
</p><p>85 Consider B2 (a1 , d/2), a ball of radius d/2 centered at a1 . [sent-275, score-0.074]
</p><p>86 Also note that for any z ∈ Z, / K  z − a1  2  ≥  i=1  z − ai 2 I(ai is closest to z) = hA (z). [sent-282, score-0.143]
</p><p>87 1 it is enough to show that the shaded area is upperbounded by the L1 (P ) distance between the functions hA and hB and lower-bounded by a power of d. [sent-284, score-0.075]
</p><p>88 Assume the density of P (with respect to the Lebesgue measure λ over Z) is bounded away from 0, i. [sent-291, score-0.072]
</p><p>89 Suppose the clusterings A and B are minimizers of√ K-means objective W (C) over the sets S and T , respectively. [sent-294, score-0.528]
</p><p>90 → Hence, the√ centers of the minimizers of the within-point scatter are stable with respect to perturbations of o( n) points. [sent-303, score-0.67]
</p><p>91 6  Conclusions  We showed that K-means clustering can be phrased as empirical risk minimization over a class HK . [sent-306, score-0.417]
</p><p>92 Furthermore, stability of clustering is determined by the geometry of HK with respect to P . [sent-307, score-0.661]
</p><p>93 We proved that in the case of a unique minimizer, K-means is stable with respect to a complete √ change of the data, while for multiple minimizers, we still expect stability with respect to o( n) changes. [sent-308, score-0.64]
</p><p>94 The rule for choosing K by maximizing stability can be viewed then as an attempt to select K such that HK has a unique minimizer with respect to P . [sent-309, score-0.563]
</p><p>95 We hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for choosing the number of clusters. [sent-311, score-0.129]
</p><p>96 A framework for statistical clustering with a constant time approximation algorithms for k-median clustering. [sent-313, score-0.186]
</p><p>97 A stability based method for discovering structure in clustered data. [sent-329, score-0.371]
</p><p>98 Empirical risk approximation: An induction principle for unsupervised learning. [sent-340, score-0.055]
</p><p>99 Some properties of empirical risk minimization over Donsker classes. [sent-345, score-0.157]
</p><p>100 Stability properties of empirical risk minimization over Donsker classes. [sent-350, score-0.157]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hk', 0.47), ('stability', 0.371), ('minimizers', 0.335), ('ha', 0.328), ('centers', 0.19), ('clustering', 0.186), ('eh', 0.181), ('hb', 0.174), ('ak', 0.142), ('clusterings', 0.14), ('zn', 0.118), ('donsker', 0.111), ('dp', 0.107), ('bk', 0.101), ('zi', 0.101), ('ai', 0.096), ('hc', 0.093), ('minimizer', 0.089), ('dmax', 0.088), ('ck', 0.081), ('covering', 0.076), ('caponnetto', 0.067), ('eha', 0.067), ('scatter', 0.066), ('lemma', 0.064), ('minimization', 0.055), ('risk', 0.055), ('phrased', 0.053), ('objective', 0.053), ('geometry', 0.053), ('unique', 0.052), ('suppose', 0.051), ('respect', 0.051), ('changes', 0.048), ('closest', 0.047), ('empirical', 0.047), ('ehc', 0.045), ('grounded', 0.045), ('rakhlin', 0.045), ('thumb', 0.045), ('ulrike', 0.045), ('ball', 0.044), ('corollary', 0.043), ('shai', 0.041), ('aj', 0.039), ('lange', 0.039), ('al', 0.038), ('qp', 0.038), ('multinomial', 0.036), ('luxburg', 0.035), ('complete', 0.035), ('prove', 0.033), ('recipes', 0.033), ('swap', 0.033), ('bj', 0.032), ('inf', 0.031), ('functions', 0.031), ('radius', 0.03), ('rm', 0.03), ('nite', 0.03), ('zj', 0.03), ('vote', 0.03), ('lebesgue', 0.03), ('characterized', 0.029), ('theoretically', 0.028), ('stable', 0.028), ('cover', 0.027), ('diameter', 0.027), ('uniqueness', 0.027), ('proved', 0.027), ('differing', 0.026), ('distance', 0.025), ('euclidean', 0.025), ('precise', 0.025), ('change', 0.025), ('sharp', 0.024), ('von', 0.024), ('mk', 0.024), ('bi', 0.023), ('starting', 0.023), ('characterization', 0.022), ('assignments', 0.021), ('chicago', 0.021), ('class', 0.021), ('hence', 0.021), ('measure', 0.021), ('solutions', 0.02), ('sup', 0.02), ('proposition', 0.02), ('side', 0.02), ('partition', 0.02), ('averages', 0.019), ('colt', 0.019), ('argminh', 0.019), ('sober', 0.019), ('questionable', 0.019), ('joachim', 0.019), ('axiomatic', 0.019), ('stipulates', 0.019), ('upperbounded', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="181-tfidf-1" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>Author: Alexander Rakhlin, Andrea Caponnetto</p><p>Abstract: We phrase K-means clustering as an empirical risk minimization procedure over a class HK and explicitly calculate the covering number for this class. Next, we show that stability of K-means clustering is characterized by the geometry of HK with respect to the underlying distribution. We prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of Ω(n1/2 ) samples deﬁnes the transition between stability and instability. While for a ﬁnite number of minimizers this result follows from multinomial distribution estimates, the case of inﬁnite minimizers requires more reﬁned tools. We conclude by proving that stability of the functions in HK implies stability of the actual centers of the clusters. Since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for the choice of K. 1</p><p>2 0.1912407 <a title="181-tfidf-2" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>3 0.15146883 <a title="181-tfidf-3" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><p>4 0.13664719 <a title="181-tfidf-4" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>5 0.096918702 <a title="181-tfidf-5" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>6 0.091075741 <a title="181-tfidf-6" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>7 0.072608903 <a title="181-tfidf-7" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>8 0.072411537 <a title="181-tfidf-8" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>9 0.071924269 <a title="181-tfidf-9" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>10 0.070937589 <a title="181-tfidf-10" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>11 0.068078443 <a title="181-tfidf-11" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>12 0.066339687 <a title="181-tfidf-12" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>13 0.065476209 <a title="181-tfidf-13" href="./nips-2006-A_PAC-Bayes_Risk_Bound_for_General_Loss_Functions.html">11 nips-2006-A PAC-Bayes Risk Bound for General Loss Functions</a></p>
<p>14 0.055384517 <a title="181-tfidf-14" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>15 0.053926699 <a title="181-tfidf-15" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>16 0.053671446 <a title="181-tfidf-16" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>17 0.051405314 <a title="181-tfidf-17" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>18 0.05046773 <a title="181-tfidf-18" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>19 0.049229749 <a title="181-tfidf-19" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>20 0.049090546 <a title="181-tfidf-20" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.154), (1, 0.08), (2, -0.037), (3, 0.122), (4, 0.031), (5, 0.06), (6, 0.017), (7, 0.101), (8, 0.06), (9, 0.058), (10, 0.039), (11, 0.089), (12, -0.132), (13, 0.067), (14, -0.025), (15, 0.016), (16, 0.078), (17, 0.043), (18, 0.046), (19, -0.08), (20, -0.08), (21, 0.148), (22, 0.052), (23, 0.084), (24, -0.166), (25, -0.09), (26, 0.172), (27, -0.112), (28, -0.157), (29, 0.108), (30, -0.011), (31, -0.072), (32, 0.003), (33, -0.127), (34, 0.137), (35, -0.042), (36, -0.157), (37, -0.144), (38, -0.06), (39, -0.016), (40, -0.074), (41, 0.084), (42, 0.087), (43, 0.104), (44, -0.089), (45, 0.103), (46, 0.04), (47, -0.014), (48, 0.021), (49, 0.196)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96544498 <a title="181-lsi-1" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>Author: Alexander Rakhlin, Andrea Caponnetto</p><p>Abstract: We phrase K-means clustering as an empirical risk minimization procedure over a class HK and explicitly calculate the covering number for this class. Next, we show that stability of K-means clustering is characterized by the geometry of HK with respect to the underlying distribution. We prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of Ω(n1/2 ) samples deﬁnes the transition between stability and instability. While for a ﬁnite number of minimizers this result follows from multinomial distribution estimates, the case of inﬁnite minimizers requires more reﬁned tools. We conclude by proving that stability of the functions in HK implies stability of the actual centers of the clusters. Since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for the choice of K. 1</p><p>2 0.52495372 <a title="181-lsi-2" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><p>3 0.51541853 <a title="181-lsi-3" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>4 0.40780491 <a title="181-lsi-4" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf</p><p>Abstract: We present a local learning approach for clustering. The basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. An optimization problem is formulated such that its solution has the above property. Relaxation and eigen-decomposition are applied to solve this optimization problem. We also brieﬂy investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. Experimental results are provided to validate the effectiveness of the proposed approach. 1</p><p>5 0.40316519 <a title="181-lsi-5" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>6 0.3528235 <a title="181-lsi-6" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>7 0.34517244 <a title="181-lsi-7" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>8 0.33685499 <a title="181-lsi-8" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>9 0.33443218 <a title="181-lsi-9" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>10 0.3216449 <a title="181-lsi-10" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<p>11 0.30481303 <a title="181-lsi-11" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>12 0.30120873 <a title="181-lsi-12" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>13 0.28659028 <a title="181-lsi-13" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>14 0.27501407 <a title="181-lsi-14" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>15 0.27206308 <a title="181-lsi-15" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>16 0.26916486 <a title="181-lsi-16" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>17 0.26061648 <a title="181-lsi-17" href="./nips-2006-The_Neurodynamics_of_Belief_Propagation_on_Binary_Markov_Random_Fields.html">190 nips-2006-The Neurodynamics of Belief Propagation on Binary Markov Random Fields</a></p>
<p>18 0.25931132 <a title="181-lsi-18" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>19 0.25391027 <a title="181-lsi-19" href="./nips-2006-Adaptor_Grammars%3A_A_Framework_for_Specifying_Compositional_Nonparametric_Bayesian_Models.html">23 nips-2006-Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models</a></p>
<p>20 0.24857171 <a title="181-lsi-20" href="./nips-2006-Hidden_Markov_Dirichlet_Process%3A_Modeling_Genetic_Recombination_in_Open_Ancestral_Space.html">90 nips-2006-Hidden Markov Dirichlet Process: Modeling Genetic Recombination in Open Ancestral Space</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.07), (3, 0.027), (7, 0.085), (9, 0.074), (11, 0.333), (20, 0.017), (22, 0.05), (44, 0.094), (57, 0.05), (65, 0.041), (69, 0.024), (71, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86671323 <a title="181-lda-1" href="./nips-2006-Game_Theoretic_Algorithms_for_Protein-DNA_binding.html">81 nips-2006-Game Theoretic Algorithms for Protein-DNA binding</a></p>
<p>Author: Luis Pérez-breva, Luis E. Ortiz, Chen-hsiang Yeang, Tommi S. Jaakkola</p><p>Abstract: We develop and analyze game-theoretic algorithms for predicting coordinate binding of multiple DNA binding regulators. The allocation of proteins to local neighborhoods and to sites is carried out with resource constraints while explicating competing and coordinate binding relations among proteins with afﬁnity to the site or region. The focus of this paper is on mathematical foundations of the approach. We also brieﬂy demonstrate the approach in the context of the λ-phage switch. 1</p><p>same-paper 2 0.785694 <a title="181-lda-2" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>Author: Alexander Rakhlin, Andrea Caponnetto</p><p>Abstract: We phrase K-means clustering as an empirical risk minimization procedure over a class HK and explicitly calculate the covering number for this class. Next, we show that stability of K-means clustering is characterized by the geometry of HK with respect to the underlying distribution. We prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of Ω(n1/2 ) samples deﬁnes the transition between stability and instability. While for a ﬁnite number of minimizers this result follows from multinomial distribution estimates, the case of inﬁnite minimizers requires more reﬁned tools. We conclude by proving that stability of the functions in HK implies stability of the actual centers of the clusters. Since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for ﬁnding theoretically grounded recipes for the choice of K. 1</p><p>3 0.45748958 <a title="181-lda-3" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>4 0.454357 <a title="181-lda-4" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>Author: Gloria Haro, Gregory Randall, Guillermo Sapiro</p><p>Abstract: The study of point cloud data sampled from a stratiﬁcation, a collection of manifolds with possible different dimensions, is pursued in this paper. We present a technique for simultaneously soft clustering and estimating the mixed dimensionality and density of such structures. The framework is based on a maximum likelihood estimation of a Poisson mixture model. The presentation of the approach is completed with artiﬁcial and real examples demonstrating the importance of extending manifold learning to stratiﬁcation learning. 1</p><p>5 0.45383418 <a title="181-lda-5" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>6 0.45336759 <a title="181-lda-6" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>7 0.45295048 <a title="181-lda-7" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>8 0.45176449 <a title="181-lda-8" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>9 0.45160067 <a title="181-lda-9" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>10 0.45071033 <a title="181-lda-10" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>11 0.45054579 <a title="181-lda-11" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>12 0.44620791 <a title="181-lda-12" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>13 0.44596604 <a title="181-lda-13" href="./nips-2006-Accelerated_Variational_Dirichlet_Process_Mixtures.html">19 nips-2006-Accelerated Variational Dirichlet Process Mixtures</a></p>
<p>14 0.4457902 <a title="181-lda-14" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>15 0.44413757 <a title="181-lda-15" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>16 0.44413599 <a title="181-lda-16" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>17 0.44408932 <a title="181-lda-17" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>18 0.44365245 <a title="181-lda-18" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>19 0.44346014 <a title="181-lda-19" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>20 0.443239 <a title="181-lda-20" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
