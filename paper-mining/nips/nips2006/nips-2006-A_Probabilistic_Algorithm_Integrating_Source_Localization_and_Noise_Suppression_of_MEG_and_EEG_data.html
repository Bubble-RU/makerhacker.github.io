<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-12" href="#">nips2006-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</h1>
<br/><p>Source: <a title="nips-2006-12-pdf" href="http://papers.nips.cc/paper/3061-a-probabilistic-algorithm-integrating-source-localization-and-noise-suppression-of-meg-and-eeg-data.pdf">pdf</a></p><p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>Reference: <a title="nips-2006-12-reference" href="../nips2006_reference/nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. [sent-14, score-0.633]
</p><p>2 Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. [sent-15, score-0.539]
</p><p>3 In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. [sent-16, score-0.172]
</p><p>4 Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. [sent-17, score-0.241]
</p><p>5 Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. [sent-18, score-0.717]
</p><p>6 1  Introduction  Mapping functional brain activity is an important problem in basic neuroscience research as well as clinical use. [sent-19, score-0.33]
</p><p>7 Clinically, such brain mapping procedures are useful to guide neurosurgical planning, navigation, and tumor and epileptic spike removal, as well as guiding the surgeon as to which areas of the brain are still relevant for cognitive and motor function in each patient. [sent-20, score-0.581]
</p><p>8 Many non-invasive techniques have emerged for functional brain mapping, such as functional magnetic resonance imaging (fMRI) and electromagnetic source imaging (ESI). [sent-21, score-0.684]
</p><p>9 Although fMRI is the most popular method for functional brain imaging with high spatial resolution, it suffers from poor temporal resolution since it measures blood oxygenation level signals with ﬂuctuations in the order of seconds. [sent-22, score-0.296]
</p><p>10 However, dynamic neuronal activity has ﬂuctuations in the sub-millisecond time-scale that can only be directly measured with electromagnetic source imaging (ESI). [sent-23, score-0.539]
</p><p>11 ESI refers to imaging of neuronal activity using magnetoencephalography (MEG) and electroencephalography (EEG)  data. [sent-24, score-0.235]
</p><p>12 The past decade has shown rapid development of whole-head MEG/EEG sensor arrays and of algorithms for reconstruction of brain source activity from MEG and EEG data. [sent-26, score-0.813]
</p><p>13 Source localization algorithms, which can be broadly classiﬁed as parametric or tomographic, make assumptions to overcome the ill-posed inverse problem. [sent-27, score-0.16]
</p><p>14 Parametric methods, including equivalent current dipole (ECD) ﬁtting techniques, assume knowledge about the number of sources and their approximate locations. [sent-28, score-0.383]
</p><p>15 A single dipolar source can be localized well, but ECD techniques poorly describe multiple sources or sources with large spatial extent. [sent-29, score-0.759]
</p><p>16 Alternatively, tomographic methods reconstruct an estimate of source activity at every grid point across the whole brain. [sent-30, score-0.523]
</p><p>17 Of many tomographic algorithms, the adaptive beamformer has been shown to have the best spatial resolution and zero localization bias [1, 2]. [sent-31, score-0.297]
</p><p>18 All existing methods for brain source localization are hampered by the many types of noise present in MEG/EEG data. [sent-32, score-0.751]
</p><p>19 The magnitude of the stimulus-evoked neural sources are on the order of noise on a single trial, and so typically 50-200 trials are needed to average in order to distinguish the sources above noise. [sent-33, score-0.53]
</p><p>20 Gaussian thermal noise is present at the sensors themselves. [sent-35, score-0.198]
</p><p>21 Background room interference such as from powerlines and electronic equipment can be problematic. [sent-36, score-0.194]
</p><p>22 Ongoing brain activity itself, including the drowsy-state alpha (∼10Hz) rhythm can drown out evoked brain sources. [sent-38, score-0.663]
</p><p>23 Finally, most localization algorithms have difﬁculty in separating neural sources of interest that have temporally overlapping activity. [sent-39, score-0.37]
</p><p>24 Noise in MEG and EEG data is typically reduced by a variety of preprocessing algorithms before being used by source localization algorithms. [sent-40, score-0.452]
</p><p>25 Simple forms of preprocessing include ﬁltering out frequency bands not containing a brain signal of interest. [sent-41, score-0.189]
</p><p>26 More sophisticated techniques have also recently been developed using graphical models for preprocessing prior to source localization [3, 4]. [sent-43, score-0.484]
</p><p>27 This paper presents a probabilistic modeling framework for MEG/EEG source localization that is robust to interference and noise. [sent-44, score-0.646]
</p><p>28 The framework uses a probabilistic hidden variable model that describes the observed sensor data in terms of activity from unobserved brain and interference sources. [sent-45, score-0.715]
</p><p>29 The unobserved source activities and model parameters are inferred from the data by a VariationalBayes Expectation-Maximization algorithm. [sent-46, score-0.292]
</p><p>30 The algorithm then creates a spatiotemporal image of brain activity by scanning the brain, inferring the model parameters and variables from sensor data, and using them to compute the likelihood of a dipole at each grid location in the brain. [sent-47, score-0.812]
</p><p>31 We also show that an established source localization method, the minimum variance adaptive beamformer (MVAB), is an approximation of our framework. [sent-48, score-0.491]
</p><p>32 2  Probabilistic model integrating source localization and noise suppression  This section describes the generative model for the data. [sent-49, score-0.601]
</p><p>33 Ongoing brain activity, biological noise, background environmental noise, and sensor noise are present in both pre-stimulus and post-stimulus periods; however, the evoked neural sources of interest are only present in the post-stimulus time period. [sent-51, score-0.948]
</p><p>34 We ﬁrst infer the model describing source types (3) and (4) from the pre-stimulus data, then ﬁx certain quantities (described in section 2. [sent-53, score-0.292]
</p><p>35 2) and infer the full model describing the remaining source types (1) and (2) from the post-stimulus data (described in section 2. [sent-54, score-0.292]
</p><p>36 After inference of the model, a map of the source activity is created as well as a map of the likelihood of activity across voxels. [sent-56, score-0.703]
</p><p>37 Let yn denote the K × 1 vector of sensor data for time point n, where K is the number of sensors (typically 200). [sent-57, score-0.349]
</p><p>38 In orange, a post-stimulus source at the voxel of interest, focused on by the lead ﬁeld F. [sent-62, score-0.509]
</p><p>39 In red, other post-stimulus sources not at that particular voxel. [sent-63, score-0.21]
</p><p>40 In green, all background sources, including ongoing brain activity, eyeblinks, heartbeat, and electrical noise. [sent-64, score-0.343]
</p><p>41 , Npost − 1  Bun + vn F r sr + Ar xr + Bun + vn n n  (1)  The K × 3 forward lead ﬁeld matrix F r represents the physical (and linear) relationship between a dipole source at voxel r for each orientation, and its inﬂuence on sensor k = 1 : K [5]. [sent-73, score-0.957]
</p><p>42 The lead ﬁeld F r is calculated from knowing the geometry of the source location to the sensor location, as well as the conducting medium in which the source lies: the human head is most commonly approximated as a single-shell sphere volume conductor. [sent-74, score-0.846]
</p><p>43 The source activity s r is a 3 × 1 vector n of dipole strength in each of the three orientations at time n for the voxel r. [sent-75, score-0.823]
</p><p>44 The K × L matrix A and the L × 1 vector xn represent the post-stimulus mixing matrix and evoked non-localized factors, respectively, corresponding to source type (2) discussed above. [sent-76, score-0.574]
</p><p>45 The K × M matrix B and the M × 1 vector un represent the background mixing matrix and background factors, respectively. [sent-77, score-0.403]
</p><p>46 All quantities depend on r in the post-stimulus period except for B, un and λ (the sensor precision), which will be learned from the pre-stimulus data and ﬁxed as the other quantities are learned for each voxel. [sent-79, score-0.437]
</p><p>47 Note however the posterior update for u n ¯ does depend on the voxel r. [sent-80, score-0.217]
</p><p>48 1  Localization of evoked sources learned from post-stimulus data  In the stimulus-evoked paradigm, the source strength at each voxel is learned from the post-stimulus data. [sent-85, score-0.943]
</p><p>49 The background mixing matrix B and sensor noise precision λ are ﬁxed, after having been learned from the pre-stimulus data, described in section 2. [sent-86, score-0.625]
</p><p>50 We assume those quantities remain constant through the post-stimulus period and are independent of source location. [sent-88, score-0.348]
</p><p>51 We assume Gaussian prior distributions on the source factors and interference factors. [sent-89, score-0.541]
</p><p>52 The source factors have prior precision given by the 3 × 3 matrix Φ, which relates to the strength of the dipole in each of 3 orientations. [sent-94, score-0.615]
</p><p>53 3  p(s) =  p(sn ); n  p(sn ) =  p(sjn ) = N (0, Φ)  (2)  j=1  The interference and background factors are assumed to have identity precision. [sent-96, score-0.353]
</p><p>54 We use a  conjugate prior for the interference mixing matrix A, where the αj is a hyperparameter over the jth column of A and λi is the precision of the ith sensor. [sent-98, score-0.424]
</p><p>55 We update the posterior distribution of the interference mixing matrix A including its precision Ψ AA . [sent-118, score-0.374]
</p><p>56 The sensor noise precision λ is also kept ﬁxed from the prestimulus period. [sent-120, score-0.396]
</p><p>57 The MAP values of the hyperparameter α and source factor precision Φ are learned here from the post-stimulus data. [sent-121, score-0.477]
</p><p>58 Furthermore, we can also plot an image of the source power estimates and the time course of activity at each voxel. [sent-125, score-0.495]
</p><p>59 2 Separation of background sources learned from pre-stimulus data  Localization error of proposed model (blue) relative to beamforming (green) 20 Algortihm, sim. [sent-127, score-0.49]
</p><p>60 inteference MVAB, real brain noise  15 Error (mm)  We note that the computational complexity of the proposed algorithm is on the order O(KLN S), roughly equivalent to a single dipole scan, which is of order O(N (K 2 +S)). [sent-129, score-0.472]
</p><p>61 10  5  0  -10  -5  0  5  SNIR (dB)  We learn the background mixing matrix and sensor noise pre- Figure 2: Performance of algorithm cision from the pre-stimulus data using a variational-Bayes relative to beamforming for simufactor analysis model. [sent-132, score-0.626]
</p><p>62 tions on the background factors and sensor noise, with zero mean and identity precision; we assume a ﬂat prior on the sensor precision. [sent-135, score-0.541]
</p><p>63 We again use a conjugate prior for the background mixing matrix B, where βj is a hyperparameter, similar to the expression for the interference mixing matrix. [sent-136, score-0.468]
</p><p>64 We make the variational-Bayesian approximation for the background mixing matrix and background factors p(u, B|y) ≈ q(u, B|y) = q(u|y)q(B|y). [sent-138, score-0.348]
</p><p>65 3  1 ¯T ¯ B λB + ψ); K  λ−1 =  1 ¯ yu diag(Ryy − BRT ) N  (11)  Relationship to minimum-variance adaptive beamforming  Minimum variance adaptive beamforming (MVAB) is one of the best performing source localization techniques. [sent-146, score-0.724]
</p><p>66 MVAB estimates the dipole source time series by sn = WM V AB yn , where ˆ −1 −1 WM V AB = (F T Ryy F )−1 F T Ryy and Ryy is the measured data covariance matrix. [sent-147, score-0.672]
</p><p>67 MVAB attempts to suppress interference, but recent studies have shown the MVAB is ineffective in cancellation of interference from other brain sources, especially if there are many such sources. [sent-149, score-0.383]
</p><p>68 5  0  -600 1  -20  -20  -200  -50  0 x (mm)  0  50  -1 -600  Figure 3: Example of algorithm and MVAB for correlated source simulation. [sent-153, score-0.365]
</p><p>69 (1) as yn = F sn + zn , where zn is termed the total noise and is given by zn = Axn + Bun + vn . [sent-155, score-0.452]
</p><p>70 Assuming we have estimated the model parameters A, B, λ, Φ, the MAP estimate of the dipole source time series is sn = W yn , where W = Γ−1 F T Υ and Γ = F T ΥF + Φ. [sent-157, score-0.672]
</p><p>71 1  Results Simulations  The proposed method was tested in a variety of realistic source conﬁgurations reconstructed on a 5mm voxel grid. [sent-166, score-0.509]
</p><p>72 Simulated datasets were constructed by placing Gaussian-damped sinusoidal time courses at speciﬁc locations inside a voxel grid based on realistic head geometry. [sent-168, score-0.377]
</p><p>73 Performance of SAKETINI (blue) relative to beamforming (green) in ability to estimate source time course  NA Correlation of estimated with true source  3  IN  1  1  0. [sent-170, score-0.782]
</p><p>74 700 samples of real MEG sensor data averaged over 100 trials collected while a human subject was alert but not performing tasks or receiving stimuli. [sent-186, score-0.191]
</p><p>75 This real background data thus includes real sensor noise plus real ”ongoing” brain activity that could interfere with evoked sources and adds spatial correlation to the sensor data. [sent-187, score-1.327]
</p><p>76 SNIR is calculated from the ratio of the sensor data resulting from sources only to sensor data from noise plus interference. [sent-189, score-0.702]
</p><p>77 For this data, a single dipole was placed randomly within  Norma lized Intens (-1000 to 1000) ity  1000  500  0  -500  -1000 -100  0  100  200 300 Time (ms)  400  Figure 5: Algorithm applied to auditory MEG dataset in patient with temporal lobe tumor. [sent-192, score-0.548]
</p><p>78 The largest peak in the likelihood map was found and the distance from this point to the true source was recorded. [sent-194, score-0.435]
</p><p>79 Each datapoint is an average of 20 realizations of the source conﬁguration, with error bars showing standard error. [sent-195, score-0.366]
</p><p>80 The next set of simulations examines the proposed method’s ability to estimate the source time course sn . [sent-199, score-0.452]
</p><p>81 Three sources were placed in the brain voxel grid. [sent-200, score-0.649]
</p><p>82 The locations of these sources were ﬁxed, but the orientation and time courses were allowed to vary across realizations of the simulations. [sent-201, score-0.339]
</p><p>83 In half the cases, two of the three sources were forced to be perfectly correlated in time (a scenario where the MVAB is known to fail), while the time course of the third source was random relative to the other two. [sent-202, score-0.637]
</p><p>84 An example of the likelihood map and estimated time courses are shown in Fig. [sent-203, score-0.172]
</p><p>85 However, the MVAB (middle plot) largely misses the source on the left. [sent-206, score-0.292]
</p><p>86 While both methods estimate the time courses well, MVAB underestimates the overall strength of the source on the top plot, and exhibits extra noise in the pre-stimulus period for the middle plot. [sent-209, score-0.576]
</p><p>87 The proposed method consistently out-performs the MVAB whether the simulated sources are highly correlated with each other (dashed lines), or uncorrelated (solid), and especially in the RE case. [sent-213, score-0.327]
</p><p>88 2  Real data  Stimulus-evoked data was collected in a 275-channel CTF System MEG device from a patient with temporal lobe tumor near auditory cortex. [sent-217, score-0.271]
</p><p>89 On the right, the likelihood map show a spatial peak in auditory cortex near the tumor. [sent-221, score-0.268]
</p><p>90 The top left subplot shows the raw sensor data for the segment containing the marked spike. [sent-229, score-0.191]
</p><p>91 The bottom left shows the location of the equivalent-current dipole (ECD) ﬁt to several spikes from this patient; this location from the ECD ﬁt would normally be used clinically. [sent-230, score-0.245]
</p><p>92 The middle bottom ﬁgure shows the likelihood map from the proposed model; the peak is in clear agreement with the standard ECD localization. [sent-231, score-0.175]
</p><p>93 The middle top ﬁgure shows the time course estimated for the likelihood spatial peak. [sent-232, score-0.184]
</p><p>94 Finally, the top right plot shows a source time course from a randomly selected location far from the epileptic spike source (shown with cross-hairs on bottom right plot), in order to show the low noise level and to show lack of cross-talk onto source estimates elsewhere. [sent-245, score-1.209]
</p><p>95 4  Extensions  We have described a novel probabilistic algorithm which performs source localization while robust to interference and demonstrated its superior performance over a standard method in a variety of simulations and real datasets. [sent-246, score-0.646]
</p><p>96 The model takes advantage of knowledge of when sources of interest are not occurring (such as in the pre-stimulus period of a evoked response paradigm). [sent-247, score-0.41]
</p><p>97 Furthermore, one is not limited to s n in a single voxel; the above formulation holds for any P arbitrarily chosen dipole components, no matter which voxels they belong to, and for any value of P . [sent-251, score-0.173]
</p><p>98 Nagarajan, “Localization bias and spatial resolution of adaptive and non-adaptive spatial ﬁlters for MEG source reconstruction,” NeuroImage, vol. [sent-257, score-0.386]
</p><p>99 Hild, and Kensuke Sekihara, “A graphical model for estimating stimulus-evoked brain responses from magnetoencephalography data with large background brain activity,” Neuroimage, vol. [sent-271, score-0.548]
</p><p>100 Sekihara, “Stimulus evoked independent factor analysis of MEG data with large background activity,” in Adv. [sent-281, score-0.248]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mvab', 0.371), ('source', 0.292), ('meg', 0.22), ('voxel', 0.217), ('sources', 0.21), ('interference', 0.194), ('sensor', 0.191), ('brain', 0.189), ('ryy', 0.176), ('dipole', 0.173), ('localization', 0.16), ('evoked', 0.144), ('activity', 0.141), ('snir', 0.137), ('beamforming', 0.136), ('patient', 0.115), ('un', 0.11), ('noise', 0.11), ('yn', 0.109), ('background', 0.104), ('sn', 0.098), ('ecd', 0.098), ('precision', 0.095), ('courses', 0.086), ('nagarajan', 0.085), ('sekihara', 0.085), ('mixing', 0.085), ('bun', 0.078), ('epileptic', 0.078), ('auditory', 0.078), ('correlated', 0.073), ('logp', 0.062), ('course', 0.062), ('imaging', 0.06), ('eeg', 0.059), ('biomagnetic', 0.059), ('intens', 0.059), ('lized', 0.059), ('npost', 0.059), ('npre', 0.059), ('peak', 0.057), ('mm', 0.057), ('period', 0.056), ('aa', 0.055), ('factors', 0.055), ('xn', 0.053), ('esi', 0.051), ('ity', 0.051), ('norma', 0.051), ('tomographic', 0.051), ('ongoing', 0.05), ('hyperparameter', 0.05), ('francisco', 0.049), ('sensors', 0.049), ('spatial', 0.047), ('spike', 0.047), ('attias', 0.046), ('dipoles', 0.046), ('electromagnetic', 0.046), ('db', 0.046), ('scan', 0.046), ('uncorrelated', 0.044), ('map', 0.043), ('realizations', 0.043), ('likelihood', 0.043), ('vn', 0.042), ('learned', 0.04), ('grid', 0.039), ('axn', 0.039), ('beamformer', 0.039), ('heartbeat', 0.039), ('hild', 0.039), ('kensuke', 0.039), ('lobe', 0.039), ('neurosurgical', 0.039), ('ryx', 0.039), ('srikantan', 0.039), ('thermal', 0.039), ('tumor', 0.039), ('ab', 0.039), ('suppression', 0.039), ('ms', 0.038), ('magnetic', 0.037), ('stimulus', 0.037), ('location', 0.036), ('san', 0.036), ('na', 0.036), ('head', 0.035), ('wm', 0.034), ('green', 0.034), ('bioengineering', 0.034), ('hagai', 0.034), ('localizing', 0.034), ('magnetoencephalography', 0.034), ('radiology', 0.034), ('placed', 0.033), ('graphical', 0.032), ('middle', 0.032), ('zn', 0.031), ('datapoint', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="12-tfidf-1" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>2 0.25972241 <a title="12-tfidf-2" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>3 0.15156625 <a title="12-tfidf-3" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>4 0.14520375 <a title="12-tfidf-4" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>5 0.14372832 <a title="12-tfidf-5" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>6 0.11975995 <a title="12-tfidf-6" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>7 0.10786258 <a title="12-tfidf-7" href="./nips-2006-Adaptive_Spatial_Filters_with_predefined_Region_of_Interest_for_EEG_based_Brain-Computer-Interfaces.html">22 nips-2006-Adaptive Spatial Filters with predefined Region of Interest for EEG based Brain-Computer-Interfaces</a></p>
<p>8 0.095456287 <a title="12-tfidf-8" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>9 0.09320353 <a title="12-tfidf-9" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>10 0.088085435 <a title="12-tfidf-10" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>11 0.073144786 <a title="12-tfidf-11" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>12 0.070965849 <a title="12-tfidf-12" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>13 0.070109531 <a title="12-tfidf-13" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>14 0.065980487 <a title="12-tfidf-14" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<p>15 0.062959462 <a title="12-tfidf-15" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>16 0.059744466 <a title="12-tfidf-16" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>17 0.058669955 <a title="12-tfidf-17" href="./nips-2006-Dirichlet-Enhanced_Spam_Filtering_based_on_Biased_Samples.html">68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</a></p>
<p>18 0.057319928 <a title="12-tfidf-18" href="./nips-2006-Distributed_Inference_in_Dynamical_Systems.html">69 nips-2006-Distributed Inference in Dynamical Systems</a></p>
<p>19 0.056608569 <a title="12-tfidf-19" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>20 0.054195337 <a title="12-tfidf-20" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, -0.074), (2, 0.077), (3, -0.067), (4, -0.113), (5, -0.061), (6, 0.217), (7, 0.073), (8, -0.07), (9, 0.029), (10, 0.056), (11, -0.046), (12, -0.096), (13, -0.002), (14, 0.122), (15, -0.246), (16, 0.234), (17, -0.249), (18, 0.097), (19, 0.002), (20, -0.047), (21, 0.021), (22, -0.035), (23, 0.042), (24, -0.037), (25, -0.001), (26, 0.041), (27, -0.075), (28, 0.034), (29, 0.004), (30, -0.037), (31, 0.168), (32, -0.029), (33, 0.059), (34, 0.084), (35, 0.102), (36, 0.173), (37, -0.07), (38, -0.012), (39, -0.119), (40, -0.083), (41, 0.035), (42, -0.033), (43, 0.05), (44, -0.011), (45, -0.02), (46, -0.021), (47, 0.009), (48, 0.06), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96884131 <a title="12-lsi-1" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>2 0.83809429 <a title="12-lsi-2" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>3 0.63913107 <a title="12-lsi-3" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>4 0.53052771 <a title="12-lsi-4" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>5 0.49530929 <a title="12-lsi-5" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>Author: Enrico Simonotto, Heather Whalley, Stephen Lawrie, Lawrence Murray, David Mcgonigle, Amos J. Storkey</p><p>Abstract: Structural equation models can be seen as an extension of Gaussian belief networks to cyclic graphs, and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of Gaussian dynamic belief networks. Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. First we demonstrate structure learning on a toy problem. We then show that for particular fMRI data the simple models usually assumed are not supported. We show that is is possible to learn sensible structural equation models that can provide modelling beneﬁts, but that are not necessarily going to be the same as a true causal model, and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more beneﬁts than learning structural equations alone. 1</p><p>6 0.49055129 <a title="12-lsi-6" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>7 0.37517813 <a title="12-lsi-7" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<p>8 0.3731243 <a title="12-lsi-8" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>9 0.34996334 <a title="12-lsi-9" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>10 0.32833269 <a title="12-lsi-10" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>11 0.32047224 <a title="12-lsi-11" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>12 0.31549373 <a title="12-lsi-12" href="./nips-2006-Adaptive_Spatial_Filters_with_predefined_Region_of_Interest_for_EEG_based_Brain-Computer-Interfaces.html">22 nips-2006-Adaptive Spatial Filters with predefined Region of Interest for EEG based Brain-Computer-Interfaces</a></p>
<p>13 0.30826554 <a title="12-lsi-13" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>14 0.30820054 <a title="12-lsi-14" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>15 0.29553509 <a title="12-lsi-15" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>16 0.27004409 <a title="12-lsi-16" href="./nips-2006-Inferring_Network_Structure_from_Co-Occurrences.html">98 nips-2006-Inferring Network Structure from Co-Occurrences</a></p>
<p>17 0.26872563 <a title="12-lsi-17" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>18 0.26559907 <a title="12-lsi-18" href="./nips-2006-Hidden_Markov_Dirichlet_Process%3A_Modeling_Genetic_Recombination_in_Open_Ancestral_Space.html">90 nips-2006-Hidden Markov Dirichlet Process: Modeling Genetic Recombination in Open Ancestral Space</a></p>
<p>19 0.2611891 <a title="12-lsi-19" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>20 0.2597591 <a title="12-lsi-20" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.094), (3, 0.041), (7, 0.064), (9, 0.039), (20, 0.022), (22, 0.045), (34, 0.384), (44, 0.069), (55, 0.011), (57, 0.043), (65, 0.027), (69, 0.031), (71, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82397974 <a title="12-lda-1" href="./nips-2006-Context_Effects_in_Category_Learning%3A_An_Investigation_of_Four_Probabilistic_Models.html">58 nips-2006-Context Effects in Category Learning: An Investigation of Four Probabilistic Models</a></p>
<p>Author: Michael C. Mozer, Michael Shettel, Michael P. Holmes</p><p>Abstract: Categorization is a central activity of human cognition. When an individual is asked to categorize a sequence of items, context effects arise: categorization of one item inﬂuences category decisions for subsequent items. Speciﬁcally, when experimental subjects are shown an exemplar of some target category, the category prototype appears to be pulled toward the exemplar, and the prototypes of all nontarget categories appear to be pushed away. These push and pull effects diminish with experience, and likely reﬂect long-term learning of category boundaries. We propose and evaluate four principled probabilistic (Bayesian) accounts of context effects in categorization. In all four accounts, the probability of an exemplar given a category is encoded as a Gaussian density in feature space, and categorization involves computing category posteriors given an exemplar. The models differ in how the uncertainty distribution of category prototypes is represented (localist or distributed), and how it is updated following each experience (using a maximum likelihood gradient ascent, or a Kalman ﬁlter update). We ﬁnd that the distributed maximum-likelihood model can explain the key experimental phenomena. Further, the model predicts other phenomena that were conﬁrmed via reanalysis of the experimental data. Categorization is a key cognitive activity. We continually make decisions about characteristics of objects and individuals: Is the fruit ripe? Does your friend seem unhappy? Is your car tire ﬂat? When an individual is asked to categorize a sequence of items, context effects arise: categorization of one item inﬂuences category decisions for subsequent items. Intuitive naturalistic scenarios in which context effects occur are easy to imagine. For example, if one lifts a medium-weight object after lifting a light-weight or heavy-weight object, the medium weight feels heavier following the light weight than following the heavy weight. Although the object-contrast effect might be due to fatigue of sensory-motor systems, many context effects in categorization are purely cognitive and cannot easily be attributed to neural habituation. For example, if you are reviewing a set of conference papers, and the ﬁrst three in the set are dreadful, then even a mediocre paper seems like it might be above threshold for acceptance. Another example of a category boundary shift due to context is the following. Suppose you move from San Diego to Pittsburgh and notice that your neighbors repeatedly describe muggy, somewhat overcast days as ”lovely.” Eventually, your notion of what constitutes a lovely day accommodates to your new surroundings. As we describe shortly, experimental studies have shown a fundamental link between context effects in categorization and long-term learning of category boundaries. We believe that context effects can be viewed as a reﬂection of a trial-to-trial learning, and the cumulative effect of these trial-to-trial modulations corresponds to what we classically consider to be category learning. Consequently, any compelling model of category learning should also be capable of explaining context effects. 1 Experimental Studies of Context Effects in Categorization Consider a set of stimuli that vary along a single continuous dimension. Throughout this paper, we use as an illustration circles of varying diameters, and assume four categories of circles deﬁned ranges of diameters; call them A, B, C, and D, in order from smallest to largest diameter. In a classiﬁcation paradigm, experimental subjects are given an exemplar drawn from one category and are asked to respond with the correct category label (Zotov, Jones, & Mewhort, 2003). After making their response, subjects receive feedback as to the correct label, which we’ll refer to as the target. In a production paradigm, subjects are given a target category label and asked to produce an exemplar of that category, e.g., using a computer mouse to indicate the circle diameter (Jones & Mewhort, 2003). Once a response is made, subjects receive feedback as to the correct or true category label for the exemplar they produced. Neither classiﬁcation nor production task has sequential structure, because the order of trial is random in both experiments. The production task provides direct information about the subjects’ internal representations, because subjects are producing exemplars that they consider to be prototypes of a category, whereas the categorization task requires indirect inferences to be made about internal representations from reaction time and accuracy data. Nonetheless, the ﬁndings in the production and classiﬁcation tasks mirror one another nicely, providing converging evidence as to the nature of learning. The production task reveals how mental representations shift as a function of trial-to-trial sequences, and these shifts cause the sequential pattern of errors and response times typically observed in the classiﬁcation task. We focus on the production task in this paper because it provides a richer source of data. However, we address the categorization task with our models as well. Figure 1 provides a schematic depiction of the key sequential effects in categorization. The horizontal line represents the stimulus dimension, e.g., circle diameter. The dimension is cut into four regions labeled with the corresponding category. The category center, which we’ll refer to as the prototype, is indicated by a vertical dashed line. The long solid vertical line marks the current exemplar—whether it is an exemplar presented to subjects in the classiﬁcation task or an exemplar generated by subjects in the production task. Following an experimental trial with this exemplar, category prototypes appear to shift: the target-category prototype moves toward the exemplar, which we refer to as a pull effect, and all nontarget-category prototypes move away from the exemplar, which we refer to as a push effect. Push and pull effects are assessed in the production task by examining the exemplar produced on the following trial, and in the categorization task by examining the likelihood of an error response near category boundaries. The set of phenomena to be explained are as follows, described in terms of the production task. All numerical results referred to are from Jones and Mewhort (2003). This experiment consisted of 12 blocks of 40 trials, with each category label given as target 10 times within a block. • Within-category pull: When a target category is repeated on successive trials, the exemplar generated on the second trial moves toward the exemplar generated on the ﬁrst trial, with respect to the true category prototype. Across the experiment, a correlation coefﬁcient of 0.524 is obtained, and remains fairly constant over trials. • Between-category push: When the target category changes from one trial to the next, the exemplar generated on the second trial moves away from the exemplar generated on the ﬁrst trial (or equivalently, from the prototype of the target category on the ﬁrst trial). Figure 2a summarizes the sequential push effects from Jones and Mewhort. The diameter of the circle produced on trial t is plotted as a function of the target category on trial t − 1, with one line for each of the four trial t targets. The mean diameter for each target category is subtracted out, so the absolute vertical offset of each line is unimportant. The main feature of the data to note is that all four curves have a negative slope, which has the following meaning: the smaller that target t − 1 is (i.e., the further to the left on the x axis in Figure 1), the larger the response to target t is (further to the right in Figure 1), and vice versa, reﬂecting a push away from target t − 1. Interestingly and importantly, the magnitude of the push increases with the ordinal distance between targets t − 1 and t. Figure 2a is based on data from only eight subjects and is therefore noisy, though the effect is statistically reliable. As further evidence, Figure 2b shows data from a categorization task (Zotov et al., 2003), where the y-axis is a different dependent measure, but the negative slope has the same interpretation as in Figure 2a. example Figure 1: Schematic depiction of sequential effects in categorization A B C D stimulus dimension C 0 B −0.02 −0.04 −0.06 −0.08 −0.1 0.06 0.04 0.04 response deviation D 0.02 0.02 0 −0.02 −0.04 −0.06 −0.08 A B C −0.1 D previous category label 0.02 0 −0.02 −0.04 −0.06 −0.08 A B C −0.1 D previous category label (b) humans: classification (d) KFU−distrib 0.08 D previous category label C D 0.06 0.04 0.04 response deviation response deviation C B (f) MLGA−distrib 0.08 0.02 0 −0.02 −0.04 −0.06 −0.08 B A previous category label 0.06 A (e) MLGA−local 0.08 0.06 A 0.04 (c) KFU−local 0.08 response deviation response deviation 0.06 response bias from (a) production task of Jones and Mewhort (2003), (b) classiﬁcation task of Zotov et al. (2003), and (c)-(f) the models proposed in this paper. The y axis is the deviation of the response from the mean, as a proportion of the total category width. The response to category A is solid red, B is dashed magenta, C is dash-dotted blue, and D is dotted green. (a) humans: production 0.08 Figure 2: Push effect data −0.1 0.02 0 −0.02 −0.04 −0.06 −0.08 A B C D previous category label −0.1 A B C D previous category label • Push and pull effects are not solely a consequence of errors or experimenter feedback. In quantitative estimation of push and pull effects, trial t is included in the data only if the response on trial t − 1 is correct. Thus, the effects follow trials in which no error feedback is given to the subjects, and therefore the adjustments are not due to explicit error correction. • Push and pull effects diminish over the course of the experiment. The magnitude of push effects can be measured by the slope of the regression lines ﬁt to the data in Figure 2a. The slopes get shallower over successive trial blocks. The magnitude of pull effects can be measured by the standard deviation (SD) of the produced exemplars, which also decreases over successive trial blocks. • Accuracy increases steadily over the course of the experiment, from 78% correct responses in the ﬁrst block to 91% in the ﬁnal block. This improvement occurs despite the fact that error feedback is relatively infrequent and becomes even less frequent as performance improves. 2 Four Models In this paper, we explore four probabilistic (Bayesian) models to explain data described in the previous section. The key phenomenon to explain turns out to be the push effect, for which three of the four models fail to account. Modelers typically discard the models that they reject, and present only their pet model. In this work, we ﬁnd it useful to report on the rejected models for three reasons. First, they help to set up and motivate the one successful model. Second, they include several obvious candidates, and we therefore have the imperative to address them. Third, in order to evaluate a model that can explain certain data, one needs to know the degree to which the the data constrain the space of models. If many models exist that are consistent with the data, one has little reason to prefer our pet candidate. Underlying all of the models is a generative probabilistic framework in which a category i is represented by a prototype value, di , on the dimension that discriminates among the categories. In the example used throughout this paper, the dimension is the diameter of a circle (hence the notation d for the prototype). An exemplar, E, of category i is drawn from a Gaussian distribution with mean di and variance vi , denoted E ∼ N (di , vi ). Category learning involves determining d ≡ {di }. In this work, we assume that the {vi } are ﬁxed and given. Because d is unknown at the start of the experiment, it is treated as the value of a random vector, D ≡ {Di }. Figure 3a shows a simple graphical model representing the generative framework, in which E is the exemplar and C the category label. To formalize our discussion so far, we adopt the following notation: P (E|C = c, D = d) ∼ N (hc d, vc ), (1) where, for the time being, hc is a unary column vector all of whose elements are zero except for element c which has value 1. (Subscripts may indicate either an index over elements of a vector, or an index over vectors. Boldface is used for vectors and matrices.) Figure 3: (a) Graphical model depicting selection of an exemplar, E, of a category, C, based on the prototype vector, D; (b) Dynamic version of model indexed by trials, t (a) D (b) C Dt-1 Ct-1 E Dt Ct Et-1 Et We assume that the prototype representation, D, is multivariate Gaussian, D ∼ N (Ψ, Σ), where Ψ and Σ encode knowledge—and uncertainty in the knowledge—of the category prototype structure. Given this formulation, the uncertainty in D can be integrated out: P (E|C) ∼ N (hc Ψ, hc ΣhT + vc ). c (2) For the categorization task, a category label can be assigned by evaluating the category posterior, P (C|E), via Bayes rule, Equation 1, and the category priors, P (C). In this framework, learning takes place via trial-to-trial adaptation of the category prototype distribution, D. In Figure 3b, we add the subscript t to each random variable to denote the trial, yielding a dynamic graphical model for the sequential updating of the prototype vector, Dt . (The reader should be attentive to the fact that we use subscripted indices to denote both trials and category labels. We generally use the index t to denote trial, and c or i to denote a category label.) The goal of our modeling work is to show that the sequential updating process leads to context effects, such as the push and pull effects discussed earlier. We propose four alternative models to explore within this framework. The four models are obtained via the Cartesian product of two binary choices: the learning rule and the prototype representation. 2.1 Learning rule The ﬁrst learning rule, maximum likelihood gradient ascent (MLGA), attempts to adjust the prototype representation so as to maximize the log posterior of the category given the exemplar. (The category, C = c, is the true label associated with the exemplar, i.e., either the target label the subject was asked to produce, or—if an error was made—the actual category label the subject did produce.) Gradient ascent is performed in all parameters of Ψ and Σ: ∆ψi = ψ ∂ log(P (c|e)) and ∂ψi ∆σij = σ ∂ log(P (c|e)), ∂σij (3) where ψ and σ are step sizes. To ensure that Σ remains a covariance matrix, constrained gradient 2 steps are applied. The constraints are: (1) diagonal terms are nonnegative, i.e., σi ≥ 0; (2) offdiagonal terms are symmetric, i.e., σij = σji ; and (3) the matrix remains positive deﬁnite, ensured σ by −1 ≤ σiijj ≤ 1. σ The second learning rule, a Kalman ﬁlter update (KFU), reestimates the uncertainty distribution of the prototypes given evidence provided by the current exemplar and category label. To draw the correspondence between our framework and a Kalman ﬁlter: the exemplar is a scalar measurement that pops out of the ﬁlter, the category prototypes are the hidden state of the ﬁlter, the measurement noise is vc , and the linear mapping from state to measurement is achieved by hc . Technically, the model is a measurement-switched Kalman ﬁlter, where the switching is determined by the category label c, i.e., the measurement function, hc , and noise, vc , are conditioned on c. The Kalman ﬁlter also allows temporal dynamics via the update equation, dt = Adt−1 , as well as internal process noise, whose covariance matrix is often denoted Q in standard Kalman ﬁlter notation. We investigated the choice of A and R, but because they did not impact the qualitative outcome of the simulations, we used A = I and R = 0. Given the correspondence we’ve established, the KFU equations—which specify Ψt+1 and Σt+1 as a function of ct , et , Ψt , and Σt —can be found in an introductory text (e.g., Maybeck, 1979). Change to a category prototype for each category following a trial of a given category. Solid (open) bars indicate trials in which the exemplar is larger (smaller) than the prototype. 2.2 prototype mvt. Figure 4: 0.2 trial t −1: A 0 −0.2 0.2 trial t −1: B 0 A B C trial t D −0.2 0.2 trial t −1: C 0 A B C trial t D −0.2 0.2 trial t −1: D 0 A B C trial t D −0.2 A B C trial t D Representation of the prototype The prototype representation that we described is localist: there is a one-to-one correspondence between the prototype for each category i and the random variable Di . To select the appropriate prototype given a current category c, we deﬁned the unary vector hc and applied hc as a linear transform on D. The identical operations can be performed in conjunction with a distributed representation of the prototype. But we step back momentarily to motivate the distributed representation. The localist representation suffers from a key weakness: it does not exploit interrelatedness constraints on category structure. The task given to experimental subjects speciﬁes that there are four categories, and they have an ordering; the circle diameters associated with category A are smaller than the diameters associated with B, etc. Consequently, dA < dB < dC < dD . One might make a further assumption that the category prototypes are equally spaced. Exploiting these two sources of domain knowledge leads to the distributed representation of category structure. A simple sort of distributed representation involves deﬁning the prototype for category i not as di but as a linear function of an underlying two-dimensional state-space representation of structure. In this state space, d1 indicates the distance between categories and d2 an offset for all categories. This representation of state can be achieved by applying Equation 1 and deﬁning hc = (nc , 1), where nc is the ordinal position of the category (nA = 1, nB = 2, etc.). We augment this representation with a bit of redundancy by incorporating not only the ordinal positions but also the reverse ordinal positions; this addition yields a symmetry in the representation between the two ends of the ordinal category scale. As a result of this augmentation, d becomes a three-dimensional state space, and hc = (nc , N + 1 − nc , 1), where N is the number of categories. To summarize, both the localist and distributed representations posit the existence of a hidden-state space—unknown at the start of learning—that speciﬁes category prototypes. The localist model assumes one dimension in the state space per prototype, whereas the distributed model assumes fewer dimensions in the state space—three, in our proposal—than there are prototypes, and computes the prototype location as a function of the state. Both localist and distributed representations assume a ﬁxed, known {hc } that specify the interpretation of the state space, or, in the case of the distributed model, the subject’s domain knowledge about category structure. 3 Simulation Methodology We deﬁned a one-dimensional feature space in which categories A-D corresponded to the ranges [1, 2), [2, 3), [3, 4), and [4, 5), respectively. In the human experiment, responses were considered incorrect if they were smaller than A or larger than D; we call these two cases out-of-bounds-low (OOBL) and out-of-bounds-high (OOBH). OOBL and OOBH were treated as two additional categories, resulting in 6 categories altogether for the simulation. Subjects and the model were never asked to produce exemplars of OOBL or OOBH, but feedback was given if a response fell into these categories. As in the human experiment, our simulation involved 480 trials. We performed 100 replications of each simulation with identical initial conditions but different trial sequences, and averaged results over replications. All prototypes were initialized to have the same mean, 3.0, at the start of the simulation. Because subjects had some initial practice on the task before the start of the experimental trials, we provided the models with 12 initial trials of a categorization (not production) task, two for each of the 6 categories. (For the MLGA models, it was necessary to use a large step size on these trials to move the prototypes to roughly the correct neighborhood.) To perform the production task, the models must generate an exemplar given a category. It seems natural to draw an exemplar from the distribution in Equation 2 for P (E|C). However, this distribu- tion reﬂects the full range of exemplars that lie within the category boundaries, and presumably in the production task, subjects attempt to produce a prototypical exemplar. Consequently, we exclude the intrinsic category variance, vc , from Equation 2 in generating exemplars, leaving variance only via uncertainty about the prototype. Each model involved selection of various parameters and initial conditions. We searched the parameter space by hand, attempting to ﬁnd parameters that satisﬁed basic properties of the data: the accuracy and response variance in the ﬁrst and second halves of the experiment. We report only parameters for the one model that was successful, the MLGA-Distrib: ψ = 0.0075, σ = 1.5 × 10−6 for off-diagonal terms and 1.5 × 10−7 for diagonal terms (the gradient for the diagonal terms was relatively steep), Σ0 = 0.01I, and for all categories c, vc = 0.42 . 4 4.1 Results Push effect The phenomenon that most clearly distinguishes the models is the push effect. The push effect is manifested in sequential-dependency functions, which plot the (relative) response on trial t as a function of trial t − 1. As we explained using Figures 2a,b, the signature of the push effect is a negatively sloped line for each of the different trial t target categories. The sequential-dependency functions for the four models are presented in Figures 2c-f. KFU-Local (Figure 2c) produces a ﬂat line, indicating no push whatsoever. The explanation for this result is straightforward: the Kalman ﬁlter update alters only the variable that is responsible for the measurement (exemplar) obtained on that trial. That variable is the prototype of the target class c, Dc . We thought the lack of an interaction among the category prototypes might be overcome with KFU-Distrib, because with a distributed prototype representation, all of the state variables jointly determine the target category prototype. However, our intuition turned out to be incorrect. We experimented with many different representations and parameter settings, but KFU-Distrib consistently obtained ﬂat or shallow positive sloping lines (Figure 2d). MLGA-Local (Figure 2e) obtains a push effect for neighboring classes, but not distant classes. For example, examining the dashed magenta line, note that B is pushed away by A and C, but is not affected by D. MLGA-Local maximizes the likelihood of the target category both by pulling the classconditional density of the target category toward the exemplar and by pushing the class-conditional densities of the other categories away from the exemplar. However, if a category has little probability mass at the location of the exemplar, the increase in likelihood that results from pushing it further away is negligible, and consequently, so is the push effect. MLGA-Distrib obtains a lovely result (Figure 2f)—a negatively-sloped line, diagnostic of the push effect. The effect magnitude matches that in the human data (Figure 2a), and captures the key property that the push effect increases with the ordinal distance of the categories. We did not build a mechanism into MLGA-Distrib to produce the push effect; it is somewhat of an emergent property of the model. The state representation of MLGA-Distrib has three components: d1 , the weight of the ordinal position of a category prototype, d2 , the weight of the reverse ordinal position, and d3 , an offset. The last term, d3 , cannot be responsible for a push effect, because it shifts all prototypes equally, and therefore can only produce a ﬂat sequential dependency function. Figure 4 helps provide an intuition how d1 and d2 work together to produce the push effect. Each graph shows the average movement of the category prototype (units on the y-axis are arbitrary) observed on trial t, for each of the four categories, following presentation of a given category on trial t − 1. Positve values on the y axis indicate increases in the prototype (movement to the right in Figure 1), and negative values decreases. Each solid vertical bar represents the movement of a given category prototype following a trial in which the exemplar is larger than its current prototype; each open vertical bar represents movement when the exemplar is to the left of its prototype. Notice that all category prototypes get larger or smaller on a given trial. But over the course of the experiment, the exemplar should be larger than the prototype as often as it is smaller, and the two shifts should sum together and partially cancel out. The result is the value indicated by the small horizontal bar along each line. The balance between the shifts in the two directions exactly corresponds to the push effect. Thus, the model produce a push-effect graph, but it is not truly producing a push effect as was originally conceived by the experimentalists. We are currently considering empirical consequences of this simulation result. Figure 5 shows a trial-by-trial trace from MLGA-Distrib. (a) class prototype 2 50 100 150 200 250 300 350 400 6 4 2 0 50 100 150 200 250 300 350 400 450 −4 50 100 150 200 250 50 100 150 200 300 350 400 450 250 300 350 400 450 300 350 400 450 300 350 400 450 0.2 0 −0.2 50 100 150 200 250 (f) 1 −2 −6 0.6 (e) (c) 0 0.8 0.4 450 (b) posterior log(class variance) P(correct) 4 0 (d) 1 shift (+=toward −=away) example 6 0.8 0.6 0.4 50 100 150 200 250 Figure 5: Trial-by-trial trace of MLGA-Distrib. (a) exemplars generated on one run of the simulation; (b) the mean and (c) variance of the class prototype distribution for the 6 classes on one run; (d) mean proportion correct over 100 replications of the simulation; (e) push and pull effects, as measured by changes to the prototype means: the upper (green) curve is the pull of the target prototype mean toward the exemplar, and the lower (red) curve is the push of the nontarget prototype means away from the exemplar, over 100 replications; (f) category posterior of the generated exemplar over 100 replications, reﬂecting gradient ascent in the posterior. 4.2 Other phenomena accounted for MLGA-Distrib captures the other phenomena we listed at the outset of this paper. Like all of the other models, MLGA-Distrib readily produces a pull effect, which is shown in the movement of category prototypes in Figure 5e. More observably, a pull effect is manifested when two successive trials of the same category are positively correlated: when trial t − 1 is to the left of the true category prototype, trial t is likely to be to the left as well. In the human data, the correlation coefﬁcient over the experiment is 0.524; in the model, the coefﬁcient is 0.496. The explanation for the pull effect is apparent: moving the category prototype to the exemplar increases the category likelihood. Although many learning effects in humans are based on error feedback, the experimental studies showed that push and pull effects occur even in the absence of errors, as they do in MLGA-Distrib. The model simply assumes that the target category it used to generate an exemplar is the correct category when no feedback to the contrary is provided. As long as the likelihood gradient is nonzero, category prototypes will be shifted. Pull and push effects shrink over the course of the experiment in human studies, as they do in the simulation. Figure 5e shows a reduction in both pull and push, as measured by the shift of the prototype means toward or away from the exemplar. We measured the slope of MLGA-Distrib’s push function (Figure 2f) for trials in the ﬁrst and second half of the simulation. The slope dropped from −0.042 to −0.025, as one would expect from Figure 5e. (These slopes are obtained by combining responses from 100 replications of the simulation. Consequently, each point on the push function was an average over 6000 trials, and therefore the regression slopes are highly reliable.) A quantitative, observable measure of pull is the standard deviation (SD) of responses. As push and pull effects diminish, SDs should decrease. In human subjects, the response SDs in the ﬁrst and second half of the experiment are 0.43 and 0.33, respectively. In the simulation, the response SDs are 0.51 and 0.38. Shrink reﬂects the fact that the model is approaching a local optimum in log likelihood, causing gradients—and learning steps—to become smaller. Not all model parameter settings lead to shrink; as in any gradient-based algorithm, step sizes that are too large do not lead to converge. However, such parameter settings make little sense in the context of the learning objective. 4.3 Model predictions MLGA-Distrib produces greater pull of the target category toward the exemplar than push of the neighboring categories away from the exemplar. In the simulation, the magnitude of the target pull— measured by the movement of the prototype mean—is 0.105, contrasted with the neighbor push, which is 0.017. After observing this robust result in the simulation, we found pertinent experimental data. Using the categorization paradigm, Zotov et al. (2003) found that if the exemplar on trial t is near a category border, subjects are more likely to produce an error if the category on trial t − 1 is repeated (i.e., a pull effect just took place) than if the previous trial is of the neighboring category (i.e., a push effect), even when the distance between exemplars on t − 1 and t is matched. The greater probability of error translates to a greater magnitude of pull than push. The experimental studies noted a phenomenon termed snap back. If the same target category is presented on successive trials, and an error is made on the ﬁrst trial, subjects perform very accurately on the second trial, i.e., they generate an exemplar near the true category prototype. It appears as if subjects, realizing they have been slacking, reawaken and snap the category prototype back to where it belongs. We tested the model, but observed a sort of anti snap back. If the model made an error on the ﬁrst trial, the mean deviation was larger—not smaller—on the second trial: 0.40 versus 0.32. Thus, MLGA-Distrib fails to explain this phenomenon. However, the phenomenon is not inconsistent with the model. One might suppose that on an error trial, subjects become more attentive, and increased attention might correspond to a larger learning rate on an error trial, which should yield a more accurate response on the following trial. McLaren et al. (1995) studied a phenomenon in humans known as peak shift, in which subjects are trained to categorize unidimensional stimuli into one of two categories. Subjects are faster and more accurate when presented with exemplars far from the category boundary than those near the boundary. In fact, they respond more efﬁciently to far exemplars than they do to the category prototype. The results are characterized in terms of the prototype of one category being pushed away from the prototype of the other category. It seems straightforward to explain these data in MLGA-Distrib as a type of long-term push effect. 5 Related Work and Conclusions Stewart, Brown, and Chater (2002) proposed an account of categorization context effects in which responses are based solely on the relative difference between the previous and present exemplars. No representation of the category prototype is maintained. However, classiﬁcation based solely on relative difference cannot account for a diminished bias effects as a function of experience. A long-term stable prototype representation, of the sort incorporated into our models, seems necessary. We considered four models in our investigation, and the fact that only one accounts for the experimental data suggests that the data are nontrivial. All four models have principled theoretical underpinnings, and they space they deﬁne may suggest other elegant frameworks for understanding mechanisms of category learning. The successful model, MLDA-Distrib, offers a deep insight into understanding multiple-category domains: category structure must be considered. MLGA-Distrib exploits knowledge available to subjects performing the task concerning the ordinal relationships among categories. A model without this knowledge, MLGA-Local, fails to explain data. Thus, the interrelatedness of categories appears to provide a source of constraint that individuals use in learning about the structure of the world. Acknowledgments This research was supported by NSF BCS 0339103 and NSF CSE-SMA 0509521. Support for the second author comes from an NSERC fellowship. References Jones, M. N., & Mewhort, D. J. K. (2003). Sequential contrast and assimilation effects in categorization of perceptual stimuli. Poster presented at the 44th Meeting of the Psychonomic Society. Vancouver, B.C. Maybeck, P.S. (1979). Stochastic models, estimation, and control, Volume I. Academic Press. McLaren, I. P. L., et al. (1995). Prototype effects and peak shift in categorization. JEP:LMC, 21, 662–673. Stewart, N. Brown, G. D. A., & Chater, N. (2002). Sequence effects in categorization of simple perceptual stimuli. JEP:LMC, 28, 3–11. Zotov, V., Jones, M. N., & Mewhort, D. J. K. (2003). Trial-to-trial representation shifts in categorization. Poster presented at the 13th Meeting of the Canadian Society for Brain, Behaviour, and Cognitive Science: Hamilton, Ontario.</p><p>same-paper 2 0.78258443 <a title="12-lda-2" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>3 0.4961893 <a title="12-lda-3" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>4 0.45347452 <a title="12-lda-4" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>5 0.42432657 <a title="12-lda-5" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>6 0.41015187 <a title="12-lda-6" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>7 0.40849462 <a title="12-lda-7" href="./nips-2006-Reducing_Calibration_Time_For_Brain-Computer_Interfaces%3A_A_Clustering_Approach.html">168 nips-2006-Reducing Calibration Time For Brain-Computer Interfaces: A Clustering Approach</a></p>
<p>8 0.39702752 <a title="12-lda-8" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>9 0.38503808 <a title="12-lda-9" href="./nips-2006-Adaptive_Spatial_Filters_with_predefined_Region_of_Interest_for_EEG_based_Brain-Computer-Interfaces.html">22 nips-2006-Adaptive Spatial Filters with predefined Region of Interest for EEG based Brain-Computer-Interfaces</a></p>
<p>10 0.38104004 <a title="12-lda-10" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>11 0.37942475 <a title="12-lda-11" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>12 0.3790178 <a title="12-lda-12" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>13 0.37677252 <a title="12-lda-13" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>14 0.37657908 <a title="12-lda-14" href="./nips-2006-Learning_to_be_Bayesian_without_Supervision.html">121 nips-2006-Learning to be Bayesian without Supervision</a></p>
<p>15 0.37576556 <a title="12-lda-15" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>16 0.37516904 <a title="12-lda-16" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>17 0.37372941 <a title="12-lda-17" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>18 0.37307009 <a title="12-lda-18" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>19 0.37248239 <a title="12-lda-19" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>20 0.37221274 <a title="12-lda-20" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
