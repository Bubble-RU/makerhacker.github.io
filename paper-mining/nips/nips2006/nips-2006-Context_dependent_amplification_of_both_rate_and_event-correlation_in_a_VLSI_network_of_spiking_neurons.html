<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-59" href="#">nips2006-59</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</h1>
<br/><p>Source: <a title="nips-2006-59-pdf" href="http://papers.nips.cc/paper/2986-context-dependent-amplification-of-both-rate-and-event-correlation-in-a-vlsi-network-of-spiking-neurons.pdf">pdf</a></p><p>Author: Elisabetta Chicca, Giacomo Indiveri, Rodney J. Douglas</p><p>Abstract: Cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties. We propose a VLSI implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean ﬁring rate domain and in spike timing correlation space. In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. In the event correlation case, the recurrent network ampliﬁes with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean ﬁring rate unaltered. We describe the network architecture and present experimental data demonstrating its context dependent computation capabilities. 1</p><p>Reference: <a title="nips-2006-59-reference" href="../nips2006_reference/nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Context dependent ampliﬁcation of both rate and event-correlation in a VLSI network of spiking neurons Elisabetta Chicca, Giacomo Indiveri and Rodney J. [sent-1, score-0.714]
</p><p>2 We propose a VLSI implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean ﬁring rate domain and in spike timing correlation space. [sent-6, score-1.037]
</p><p>3 In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. [sent-7, score-1.522]
</p><p>4 In the event correlation case, the recurrent network ampliﬁes with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean ﬁring rate unaltered. [sent-8, score-1.365]
</p><p>5 We describe the network architecture and present experimental data demonstrating its context dependent computation capabilities. [sent-9, score-0.256]
</p><p>6 1  Introduction  There is an increasing body of evidence supporting the hypothesis that recurrent cooperative competitive neural networks play a central role in cortical processing [1]. [sent-10, score-0.676]
</p><p>7 Similarly, it has been shown that neurons with similar functional properties are aggregated together in modules or columns and most connections are made locally within the neighborhood of a 1 mm column [3]. [sent-12, score-0.465]
</p><p>8 From the computational point of view, recurrent cooperative competitive networks have been investigated extensively in the past [4–6]. [sent-13, score-0.579]
</p><p>9 Already in the late 70’s Amari and Arbib [4] applied the concept of dynamic neural ﬁelds1 [7, 8] to develop a unifying mathematical framework to study cooperative competitive neural network models based on a series of detailed models of biological systems [9–11]. [sent-14, score-0.564]
</p><p>10 [5] argued that recurrent cortical circuits restore analog signals on the basis of their connectivity patterns and produce selective neuronal responses while maintaining network stability. [sent-16, score-0.742]
</p><p>11 To support this hypothesis they proposed the cortical ampliﬁer2 model and showed that a network of cortical ampliﬁers performs signal restoration and noise suppression by amplifying the correlated signal in a pattern that was stored in the connectivity of the network, without amplifying the noise. [sent-17, score-0.598]
</p><p>12 2 The cortical ampliﬁer consists of a population of identical neurons, connected to each other with the same excitatory synaptic strength, sharing a common inhibitory feedback and the same input. [sent-20, score-0.424]
</p><p>13 Squares represent excitatory (E) and inhibitory (I) synapses, trapezoids represent I&F; neurons. [sent-22, score-0.312]
</p><p>14 The I&F; neurons can transmit their spikes off-chip and/or to the locally connected synapses (see text for details). [sent-24, score-0.664]
</p><p>15 The local connectivity implements a cooperative competitive network with ﬁrst and secon dneighbors recurrent excitatory connections and global inhibition. [sent-25, score-1.157]
</p><p>16 The ﬁrst and second neighbor connections of the neurons at the edges of the array are connected to pads. [sent-26, score-0.567]
</p><p>17 This allows us to leave the network open, or implement closed boundary conditions (to form a ring of neurons), using off-chip jumpers. [sent-27, score-0.307]
</p><p>18 The global inhibitory neuron (bottom left) receives excitation from all neurons in the array and its output inhibits all of them. [sent-28, score-1.103]
</p><p>19 tical feature selectivity based on recurrent cooperative competitive networks [6] where they showed how these models can account for some of the emergent cooperative cortical properties observed in nature. [sent-29, score-0.899]
</p><p>20 Recently it has been argued that recurrent cooperative competitive networks exhibit at the same time computational properties both in an analog way (e. [sent-30, score-0.643]
</p><p>21 proposed a VLSI chip in which neurons are implemented as linear threshold units and input and output signals encode mean ﬁring rates. [sent-36, score-0.706]
</p><p>22 The recurrent connectivity of the network proposed in [12] comprises self-excitation, ﬁrst and second neighbors recurrent excitatory connections, and global inhibition. [sent-37, score-1.036]
</p><p>23 Here we propose a spike-based VLSI neural network that allows us to explore these computational properties both in the mean rate and time domain. [sent-39, score-0.31]
</p><p>24 The device we propose comprises a ring of excitatory Integrate-and-Fire (I&F;) neurons with ﬁrst and second neighbors recurrent excitatory connections and a global inhibitory neuron which receives excitation from all the neurons in the ring. [sent-40, score-2.188]
</p><p>25 In the next Section we describe the network architecture, and in Section 3 and 4 we show how this network can perform context dependent computation in the mean rate domain and in the time domain respectively. [sent-41, score-0.568]
</p><p>26 2  The VLSI Spiking Cooperative Competitive Network  Several examples of VLSI competitive networks of spiking neurons have already been presented in literature [14–19]. [sent-42, score-0.653]
</p><p>27 [16] proposed a VLSI winner-take-all (WTA) spiking network consisting of 4 neurons with all-to-all inhibitory connections. [sent-44, score-0.834]
</p><p>28 In 1993, a different VLSI WTA chip comprising also 4 neurons was proposed, it used global inhibition to implement the WTA behavior [17]. [sent-45, score-0.773]
</p><p>29 More recent implementations of spiking VLSI cooperative competitive networks consist of larger arrays and show more complex behavior thanks also to more advanced VLSI processes and testing instruments currently available [14, 15, 18, 19]. [sent-46, score-0.451]
</p><p>30 The left panel shows the raster plot of the network activity in response to two Gaussian shaped, Poisson distributed, input spike trains, with mean ﬁring rates ranging from 0 to 120 Hz. [sent-49, score-0.706]
</p><p>31 The right panel shows the mean frequencies of the neurons ion the feed-forward network. [sent-50, score-0.486]
</p><p>32 The feed-forward network response directly reﬂects the applied input: on average, the neurons produce an output spike in response to about every 6 input spikes. [sent-51, score-0.895]
</p><p>33 Note how the global inhibitory neuron (address number 1) is not active. [sent-52, score-0.412]
</p><p>34 The left panel shows the raster plot of the network activity in response to the same input applied to the feed-forward network. [sent-54, score-0.535]
</p><p>35 The right panel shows the mean output frequencies of the neurons in the cooperative-competitive network. [sent-55, score-0.534]
</p><p>36 The recurrent connectivity (lateral excitation and global inhibition) ampliﬁes the activity of the neurons with highest mean output frequency and suppresses the activity of other neurons (compare with right panel of (a)). [sent-56, score-1.939]
</p><p>37 Most of the previously proposed VLSI models focused on hard WTA behaviors (only the neuron that receives the strongest input is active). [sent-57, score-0.293]
</p><p>38 The VLSI network we designed comprises 31 excitatory neurons and 1 global inhibitory neuron [15]. [sent-60, score-1.212]
</p><p>39 Cooperation between neurons is implemented by ﬁrst and second neighbors recurrent excitatory connections. [sent-61, score-0.811]
</p><p>40 Depending on the relative strength of excitation and inhibition the network can be operated either in hard or soft WTA mode. [sent-62, score-0.586]
</p><p>41 On top of the local hardwired recurrent connectivity the network comprises 16 AER3 synapses per neuron. [sent-63, score-0.782]
</p><p>42 The architecture of the VLSI network of I&F; neurons is shown in Fig. [sent-66, score-0.624]
</p><p>43 Each column contains 14 AER excitatory synapses, 2 AER inhibitory synapses and 6 locally connected (hard-wired) synapses. [sent-69, score-0.543]
</p><p>44 The circuits implementing the chip’s I&F; neurons and synapses have been described in [22]. [sent-70, score-0.671]
</p><p>45 If the input address-events routed to the synapse integrate up to the neuron’s spiking threshold, then that neuron generates an output address-event which is transmitted off-chip. [sent-72, score-0.439]
</p><p>46 Arbitrary network architectures can be implemented using off-chip look-up tables and routing the chip’s output address-events to one or more AER input synapses. [sent-73, score-0.331]
</p><p>47 Synapses with local hard-wired connectivity are used to realize the cooperative competitive network with nearest neighbor and second nearest neighbor interactions (see Fig. [sent-75, score-0.647]
</p><p>48 1): 31 neurons of the array 3 In the Address Event Representation (AER) input and output spikes are real-time digital events that carry analog information structure [20]. [sent-76, score-0.696]
</p><p>49 (a) The graph shows the mean ﬁring rate of the neurons for three different connectivity conditions and in response to the same stimulus. [sent-79, score-0.641]
</p><p>50 The continuous line represents the baseline: the activity of the neurons in response to the external stimulus when the local connections are not active (feed-forward network). [sent-80, score-0.759]
</p><p>51 The dashed and dotted lines represent the activity of the feed-back network for weak and strong inhibition respectively and ﬁxed lateral excitation. [sent-81, score-0.744]
</p><p>52 For weak inhibition the neurons that receive the strongest inputs amplify their activity. [sent-82, score-0.702]
</p><p>53 (b) Activity of the network for three different connectivity conditions. [sent-83, score-0.305]
</p><p>54 The continuous line represents the baseline (response of the feed-forward network to the input stimulus). [sent-84, score-0.272]
</p><p>55 The dashed and dotted lines represent the activity of the feed-back network for weak and strong lateral excitation respectively and ﬁxed global inhibition. [sent-85, score-0.822]
</p><p>56 For strong lateral excitation the neurons that receive the highest input amplify their activity. [sent-86, score-0.876]
</p><p>57 The ﬁrst and second neighbor connections of the neurons at the edges of the array are connected to pads. [sent-88, score-0.567]
</p><p>58 This allows us to leave the network open, or implement closed boundary conditions (to form a ring of neurons [12]), using off-chip jumpers. [sent-89, score-0.706]
</p><p>59 All of the synapses on the chip can be switched off. [sent-90, score-0.339]
</p><p>60 In addition, a uniform constant DC current can be injected to all the neurons in the array thus producing a regular “spontaneous” activity throughout the whole array. [sent-92, score-0.641]
</p><p>61 obtained from a silicon retina, a silicon cochlea, or other AER sensory systems) onto the network’s AER synapses in a way to implement different types of feature maps. [sent-97, score-0.259]
</p><p>62 [24] recently presented an orientation selectivity system implemented by properly mapping the activity of a silicon retina onto AER input synapses of our chip. [sent-99, score-0.508]
</p><p>63 Moreover the ﬂexibility of the AER infrastructure, combined with the large number of externally addressable AER synapses of our VLSI device, allows us to perform cooperative competitive computation across different feature spaces in parallel. [sent-100, score-0.503]
</p><p>64 We explored the behavior of the network using synthetic control stimuli: we stimulated the chip via its input AER synapses with Poisson distributed spike trains, using Gaussian shaped mean frequency proﬁles. [sent-101, score-0.88]
</p><p>65 6  5 10  Mean correlation coefficient  Mean correlation coefficient  0. [sent-118, score-0.46]
</p><p>66 01 0  30  5  10  (a)  15 20 Neuron address  25  30  (b)  Figure 4: (a) Mean correlation coefﬁcient among input spike trains used to stimulate the neurons of our network. [sent-130, score-0.922]
</p><p>67 (b) Mean correlation coefﬁcient of output spike trains, when the cooperative-competitive connections of the network are disabled (feed-forward mode). [sent-132, score-0.624]
</p><p>68 5  5 10  Mean correlation coefficient  Mean correlation coefficient  10  −0. [sent-147, score-0.46]
</p><p>69 01 0  5  10  15 20 Neuron address  25  30  (b)  Figure 5: (a) Correlation coefﬁcient of output spike trains, when only global inhibition is enabled; (b) Correlation coefﬁcient of output spike trains when both global inhibition and local excitation are enabled. [sent-160, score-1.135]
</p><p>70 Two examples of raw data for these experiments in the feed-forward and recurrent network conditions are shown in Fig. [sent-162, score-0.416]
</p><p>71 3(a) for two different values of the strength of global inhibition (modulated using the weight of the connection from the excitatory neurons to the global inhibitory neuron) and a ﬁxed strength of lateral excitation. [sent-165, score-1.219]
</p><p>72 The activity of the recurrent network has to be compared with the activity of the feed-forward network (“baseline” activity plotted in Fig. [sent-166, score-1.158]
</p><p>73 3(a)) in response to the same stimulus to easily estimate the effect of the recurrent connectivity. [sent-168, score-0.304]
</p><p>74 The most active neurons cooperatively amplify their activity through lateral excitation and efﬁciently drive the global inhibitory neuron to suppress the activity of other neurons (dashed line in Fig. [sent-169, score-1.94]
</p><p>75 When the strength of global inhibition is high the ampliﬁcation given by the lateral excitatory connections can be completely suppressed (dotted line in Fig. [sent-171, score-0.639]
</p><p>76 A similar behavior is observed when the strength of lateral excitation is modulated (see Fig. [sent-173, score-0.368]
</p><p>77 3(b)) ampliﬁcation is observed for the neurons receiving the input with highest mean frequency and suppression of neurons stimulated by trains with lower mean frequencies occur. [sent-176, score-1.256]
</p><p>78 When lateral excitation is weak (dotted line in Fig. [sent-177, score-0.357]
</p><p>79 3(b)), global inhibition dominates and the activity of all neurons is suppressed. [sent-178, score-0.805]
</p><p>80 The non-linearity of this behavior is evident when we compare the effect of recurrent connectivity on the peak of the lowest hill of activity and on the side of the highest hill of activity (e. [sent-179, score-0.79]
</p><p>81 In the feed-forward network (continuous line) these two neurons have a similar mean out frequency (∼ 12 Hz), nevertheless the effect of recurrent connectivity on their activity is different. [sent-183, score-1.166]
</p><p>82 The activity of neuron 11 is ampliﬁed by a factor of 1. [sent-184, score-0.388]
</p><p>83 24 while the activity of neuron 23 is suppressed by a factor of 0. [sent-185, score-0.388]
</p><p>84 This difference shows that the network is able to act differently on similar mean rates depending on the spatial context, distinguishing the relevant signal from distractors and noise. [sent-187, score-0.289]
</p><p>85 4  Competition in correlation space  Here we test the context-dependent computation properties of the cooperative competitive network, also in the spike-timing domain. [sent-188, score-0.491]
</p><p>86 We stimulated the neurons with correlated, Poisson distributed spike trains and analyzed the network’s response properties in correlation space, as a function of its excitatory/inhibitory connection settings. [sent-189, score-0.936]
</p><p>87 Figure 4(a) shows the mean correlation coefﬁcient between each input spike train with the spike trains sent to all other neurons in the array. [sent-190, score-1.018]
</p><p>88 When used as a plain feed-forward network (with all local connections disabled), the neurons generate output spike trains that reﬂect the distributions of the input signal, both in the mean ﬁring rate domain (see Fig. [sent-193, score-1.124]
</p><p>89 The lower output mean ﬁring rates and smaller amount of correlations among output spikes are due to the integrating properties of the I&F; neuron and of the AER synapses. [sent-196, score-0.396]
</p><p>90 5 we show the response of the network when global inhibition and recurrent local excitation are activated. [sent-198, score-0.898]
</p><p>91 Enabling only global inhibition, without recurrent excitation has no substantial effect with respect to the feed-forward case (compare Fig. [sent-199, score-0.463]
</p><p>92 Given the nature of the connectivity patterns in our chip, the correlation among neighboring neurons is increased throughout the array, independent of the input sources, hence the mean correlation coefﬁcient is higher throughout the whole network. [sent-203, score-0.971]
</p><p>93 However, the difference in correlation between the base level and the group with highest correlation is signiﬁcantly higher when cooperation and competition are enabled, with respect to the feed-forward case. [sent-204, score-0.522]
</p><p>94 At the same time, the difference in correlation between the base level and the group with lowest correlation when cooperation and competition are enabled cannot be distinguished from that of the feed-forward case. [sent-205, score-0.535]
</p><p>95 5  Discussion  We presented a hardware cooperative competitive network composed of spiking VLSI neurons and analog synapses, and used it to simulate in real-time network architectures similar to those studied by Amari and Arbib [4], Douglas et al. [sent-212, score-1.299]
</p><p>96 We pointed out how the recurrent network can act differently on neurons with similar activity depending on the local context (i. [sent-224, score-1.056]
</p><p>97 In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons belonging to distractors or at noise level. [sent-227, score-1.555]
</p><p>98 We believe that this is one of the mechanisms used by biological systems to perform highly reliable computation restoring signals on the basis of cooperative-competitive interaction among elementary units of recurrent networks and hence on the basis of the context of the signal. [sent-229, score-0.296]
</p><p>99 In the mean correlation coefﬁcient case, the recurrent network ampliﬁes more efﬁciently the correlation between neurons which receive highly correlated inputs while keeping the average mean ﬁring rate constant. [sent-230, score-1.425]
</p><p>100 A VLSI recurrent network of integrate–and–ﬁre neurons connected by plastic synapses with long term memory. [sent-408, score-1.046]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurons', 0.399), ('aer', 0.232), ('vlsi', 0.231), ('recurrent', 0.22), ('neuron', 0.206), ('cooperative', 0.197), ('network', 0.196), ('synapses', 0.189), ('excitation', 0.184), ('activity', 0.182), ('correlation', 0.177), ('excitatory', 0.165), ('inhibition', 0.165), ('ampli', 0.161), ('chip', 0.15), ('inhibitory', 0.147), ('lateral', 0.143), ('competitive', 0.117), ('trains', 0.111), ('spike', 0.111), ('ring', 0.111), ('connectivity', 0.109), ('douglas', 0.094), ('stimulated', 0.092), ('wta', 0.092), ('spiking', 0.092), ('circuits', 0.083), ('cooperation', 0.073), ('chicca', 0.072), ('cortical', 0.07), ('inset', 0.066), ('connections', 0.066), ('analog', 0.064), ('competition', 0.064), ('array', 0.06), ('mean', 0.06), ('global', 0.059), ('suppression', 0.055), ('coefficient', 0.053), ('selectivity', 0.053), ('hansel', 0.05), ('input', 0.049), ('output', 0.048), ('response', 0.046), ('address', 0.046), ('networks', 0.045), ('synapse', 0.044), ('enabled', 0.044), ('connected', 0.042), ('digital', 0.042), ('strength', 0.041), ('comprises', 0.04), ('correlated', 0.04), ('amplify', 0.039), ('indiveri', 0.039), ('neuromorphic', 0.039), ('suppresses', 0.039), ('coef', 0.039), ('inputs', 0.038), ('stimulus', 0.038), ('behaviors', 0.038), ('architectures', 0.038), ('raster', 0.035), ('silicon', 0.035), ('spikes', 0.034), ('poisson', 0.033), ('arbib', 0.033), ('distractors', 0.033), ('iger', 0.033), ('sompolinsky', 0.033), ('vertebrate', 0.033), ('shaped', 0.033), ('hill', 0.033), ('amari', 0.033), ('context', 0.031), ('highest', 0.031), ('receive', 0.031), ('weak', 0.03), ('architecture', 0.029), ('domain', 0.029), ('dante', 0.029), ('lichtsteiner', 0.029), ('stimulate', 0.029), ('hahnloser', 0.029), ('stimulates', 0.029), ('amplifying', 0.029), ('dotted', 0.028), ('sources', 0.028), ('local', 0.028), ('panel', 0.027), ('neighbors', 0.027), ('baseline', 0.027), ('hz', 0.027), ('neural', 0.027), ('rate', 0.027), ('neuroscience', 0.027), ('disabled', 0.026), ('oster', 0.026), ('whatley', 0.026), ('mahowald', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="59-tfidf-1" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<p>Author: Elisabetta Chicca, Giacomo Indiveri, Rodney J. Douglas</p><p>Abstract: Cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties. We propose a VLSI implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean ﬁring rate domain and in spike timing correlation space. In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. In the event correlation case, the recurrent network ampliﬁes with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean ﬁring rate unaltered. We describe the network architecture and present experimental data demonstrating its context dependent computation capabilities. 1</p><p>2 0.5003559 <a title="59-tfidf-2" href="./nips-2006-Attentional_Processing_on_a_Spike-Based_VLSI_Neural_Network.html">36 nips-2006-Attentional Processing on a Spike-Based VLSI Neural Network</a></p>
<p>Author: Yingxue Wang, Rodney J. Douglas, Shih-Chii Liu</p><p>Abstract: The neurons of the neocortex communicate by asynchronous events called action potentials (or ’spikes’). However, for simplicity of simulation, most models of processing by cortical neural networks have assumed that the activations of their neurons can be approximated by event rates rather than taking account of individual spikes. The obstacle to exploring the more detailed spike processing of these networks has been reduced considerably in recent years by the development of hybrid analog-digital Very-Large Scale Integrated (hVLSI) neural networks composed of spiking neurons that are able to operate in real-time. In this paper we describe such a hVLSI neural network that performs an interesting task of selective attentional processing that was previously described for a simulated ’pointer-map’ rate model by Hahnloser and colleagues. We found that most of the computational features of their rate model can be reproduced in the spiking implementation; but, that spike-based processing requires a modiﬁcation of the original network architecture in order to memorize a previously attended target. 1</p><p>3 0.39555666 <a title="59-tfidf-3" href="./nips-2006-A_selective_attention_multi--chip_system_with_dynamic_synapses_and_spiking_neurons.html">18 nips-2006-A selective attention multi--chip system with dynamic synapses and spiking neurons</a></p>
<p>Author: Chiara Bartolozzi, Giacomo Indiveri</p><p>Abstract: Selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity: salient subregions of the input stimuli are serially processed, while non–salient regions are suppressed. We present an mixed mode analog/digital Very Large Scale Integration implementation of a building block for a multi–chip neuromorphic hardware model of selective attention. We describe the chip’s architecture and its behavior, when its is part of a multi–chip system with a spiking retina as input, and show how it can be used to implement in real-time ﬂexible models of bottom-up attention. 1</p><p>4 0.34746507 <a title="59-tfidf-4" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>Author: Thomas Voegtlin</p><p>Abstract: In biological neurons, the timing of a spike depends on the timing of synaptic currents, in a way that is classically described by the Phase Response Curve. This has implications for temporal coding: an action potential that arrives on a synapse has an implicit meaning, that depends on the position of the postsynaptic neuron on the ﬁring cycle. Here we show that this implicit code can be used to perform computations. Using theta neurons, we derive a spike-timing dependent learning rule from an error criterion. We demonstrate how to train an auto-encoder neural network using this rule. 1</p><p>5 0.3220385 <a title="59-tfidf-5" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Many neural areas, notably, the hippocampus, show structured, dynamical, population behavior such as coordinated oscillations. It has long been observed that such oscillations provide a substrate for representing analog information in the ﬁring phases of neurons relative to the underlying population rhythm. However, it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture, and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems. Here, we observe that, since neurons in an oscillatory network need not only ﬁre once in each cycle (or even at all), uncertainty about the analog quantities each neuron represents by its ﬁring phase might naturally be reported through the degree of concentration of the spikes that it ﬁres. We apply this theory to memory in a model of oscillatory associative recall in hippocampal area CA3. Although it is not well treated in the literature, representing and manipulating uncertainty is fundamental to competent memory; our theory enables us to view CA3 as an effective uncertainty-aware, retrieval system. 1</p><p>6 0.28928977 <a title="59-tfidf-6" href="./nips-2006-Neurophysiological_Evidence_of_Cooperative_Mechanisms_for_Stereo_Computation.html">145 nips-2006-Neurophysiological Evidence of Cooperative Mechanisms for Stereo Computation</a></p>
<p>7 0.28546378 <a title="59-tfidf-7" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>8 0.1982514 <a title="59-tfidf-8" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>9 0.14805473 <a title="59-tfidf-9" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>10 0.099842168 <a title="59-tfidf-10" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>11 0.099499889 <a title="59-tfidf-11" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>12 0.098854072 <a title="59-tfidf-12" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>13 0.084576607 <a title="59-tfidf-13" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>14 0.081749342 <a title="59-tfidf-14" href="./nips-2006-The_Neurodynamics_of_Belief_Propagation_on_Binary_Markov_Random_Fields.html">190 nips-2006-The Neurodynamics of Belief Propagation on Binary Markov Random Fields</a></p>
<p>15 0.073234938 <a title="59-tfidf-15" href="./nips-2006-A_recipe_for_optimizing_a_time-histogram.html">17 nips-2006-A recipe for optimizing a time-histogram</a></p>
<p>16 0.072736457 <a title="59-tfidf-16" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>17 0.066851363 <a title="59-tfidf-17" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>18 0.065980487 <a title="59-tfidf-18" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>19 0.059493329 <a title="59-tfidf-19" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>20 0.055918287 <a title="59-tfidf-20" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, -0.629), (2, 0.052), (3, 0.151), (4, 0.065), (5, 0.083), (6, -0.004), (7, 0.132), (8, -0.031), (9, -0.047), (10, 0.017), (11, -0.005), (12, -0.077), (13, -0.014), (14, -0.035), (15, 0.008), (16, 0.035), (17, 0.018), (18, -0.01), (19, 0.148), (20, -0.039), (21, 0.013), (22, -0.026), (23, 0.06), (24, -0.022), (25, -0.107), (26, -0.085), (27, 0.043), (28, 0.028), (29, -0.061), (30, 0.058), (31, 0.002), (32, -0.069), (33, 0.012), (34, 0.036), (35, -0.1), (36, 0.007), (37, 0.025), (38, -0.044), (39, -0.034), (40, -0.001), (41, 0.004), (42, -0.048), (43, -0.072), (44, -0.046), (45, 0.047), (46, 0.024), (47, 0.037), (48, -0.004), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98673791 <a title="59-lsi-1" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<p>Author: Elisabetta Chicca, Giacomo Indiveri, Rodney J. Douglas</p><p>Abstract: Cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties. We propose a VLSI implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean ﬁring rate domain and in spike timing correlation space. In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. In the event correlation case, the recurrent network ampliﬁes with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean ﬁring rate unaltered. We describe the network architecture and present experimental data demonstrating its context dependent computation capabilities. 1</p><p>2 0.97985303 <a title="59-lsi-2" href="./nips-2006-Attentional_Processing_on_a_Spike-Based_VLSI_Neural_Network.html">36 nips-2006-Attentional Processing on a Spike-Based VLSI Neural Network</a></p>
<p>Author: Yingxue Wang, Rodney J. Douglas, Shih-Chii Liu</p><p>Abstract: The neurons of the neocortex communicate by asynchronous events called action potentials (or ’spikes’). However, for simplicity of simulation, most models of processing by cortical neural networks have assumed that the activations of their neurons can be approximated by event rates rather than taking account of individual spikes. The obstacle to exploring the more detailed spike processing of these networks has been reduced considerably in recent years by the development of hybrid analog-digital Very-Large Scale Integrated (hVLSI) neural networks composed of spiking neurons that are able to operate in real-time. In this paper we describe such a hVLSI neural network that performs an interesting task of selective attentional processing that was previously described for a simulated ’pointer-map’ rate model by Hahnloser and colleagues. We found that most of the computational features of their rate model can be reproduced in the spiking implementation; but, that spike-based processing requires a modiﬁcation of the original network architecture in order to memorize a previously attended target. 1</p><p>3 0.89787793 <a title="59-lsi-3" href="./nips-2006-A_selective_attention_multi--chip_system_with_dynamic_synapses_and_spiking_neurons.html">18 nips-2006-A selective attention multi--chip system with dynamic synapses and spiking neurons</a></p>
<p>Author: Chiara Bartolozzi, Giacomo Indiveri</p><p>Abstract: Selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity: salient subregions of the input stimuli are serially processed, while non–salient regions are suppressed. We present an mixed mode analog/digital Very Large Scale Integration implementation of a building block for a multi–chip neuromorphic hardware model of selective attention. We describe the chip’s architecture and its behavior, when its is part of a multi–chip system with a spiking retina as input, and show how it can be used to implement in real-time ﬂexible models of bottom-up attention. 1</p><p>4 0.85873085 <a title="59-lsi-4" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Many neural areas, notably, the hippocampus, show structured, dynamical, population behavior such as coordinated oscillations. It has long been observed that such oscillations provide a substrate for representing analog information in the ﬁring phases of neurons relative to the underlying population rhythm. However, it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture, and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems. Here, we observe that, since neurons in an oscillatory network need not only ﬁre once in each cycle (or even at all), uncertainty about the analog quantities each neuron represents by its ﬁring phase might naturally be reported through the degree of concentration of the spikes that it ﬁres. We apply this theory to memory in a model of oscillatory associative recall in hippocampal area CA3. Although it is not well treated in the literature, representing and manipulating uncertainty is fundamental to competent memory; our theory enables us to view CA3 as an effective uncertainty-aware, retrieval system. 1</p><p>5 0.79499787 <a title="59-lsi-5" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>Author: Thomas Voegtlin</p><p>Abstract: In biological neurons, the timing of a spike depends on the timing of synaptic currents, in a way that is classically described by the Phase Response Curve. This has implications for temporal coding: an action potential that arrives on a synapse has an implicit meaning, that depends on the position of the postsynaptic neuron on the ﬁring cycle. Here we show that this implicit code can be used to perform computations. Using theta neurons, we derive a spike-timing dependent learning rule from an error criterion. We demonstrate how to train an auto-encoder neural network using this rule. 1</p><p>6 0.68271852 <a title="59-lsi-6" href="./nips-2006-Neurophysiological_Evidence_of_Cooperative_Mechanisms_for_Stereo_Computation.html">145 nips-2006-Neurophysiological Evidence of Cooperative Mechanisms for Stereo Computation</a></p>
<p>7 0.66712683 <a title="59-lsi-7" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>8 0.44751275 <a title="59-lsi-8" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>9 0.43151265 <a title="59-lsi-9" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>10 0.40629634 <a title="59-lsi-10" href="./nips-2006-The_Neurodynamics_of_Belief_Propagation_on_Binary_Markov_Random_Fields.html">190 nips-2006-The Neurodynamics of Belief Propagation on Binary Markov Random Fields</a></p>
<p>11 0.38590592 <a title="59-lsi-11" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>12 0.35507748 <a title="59-lsi-12" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>13 0.27335608 <a title="59-lsi-13" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>14 0.26466584 <a title="59-lsi-14" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>15 0.25886872 <a title="59-lsi-15" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>16 0.20827733 <a title="59-lsi-16" href="./nips-2006-A_Scalable_Machine_Learning_Approach_to_Go.html">13 nips-2006-A Scalable Machine Learning Approach to Go</a></p>
<p>17 0.16860147 <a title="59-lsi-17" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>18 0.16649804 <a title="59-lsi-18" href="./nips-2006-Multi-dynamic_Bayesian_Networks.html">139 nips-2006-Multi-dynamic Bayesian Networks</a></p>
<p>19 0.16244194 <a title="59-lsi-19" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>20 0.16171998 <a title="59-lsi-20" href="./nips-2006-Inferring_Network_Structure_from_Co-Occurrences.html">98 nips-2006-Inferring Network Structure from Co-Occurrences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.07), (3, 0.015), (7, 0.061), (9, 0.058), (20, 0.013), (22, 0.023), (44, 0.061), (57, 0.036), (65, 0.024), (69, 0.022), (71, 0.182), (84, 0.049), (93, 0.295)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8697108 <a title="59-lda-1" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<p>Author: Elisabetta Chicca, Giacomo Indiveri, Rodney J. Douglas</p><p>Abstract: Cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties. We propose a VLSI implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean ﬁring rate domain and in spike timing correlation space. In the mean rate case the network ampliﬁes the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. In the event correlation case, the recurrent network ampliﬁes with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean ﬁring rate unaltered. We describe the network architecture and present experimental data demonstrating its context dependent computation capabilities. 1</p><p>2 0.6897893 <a title="59-lda-2" href="./nips-2006-Attentional_Processing_on_a_Spike-Based_VLSI_Neural_Network.html">36 nips-2006-Attentional Processing on a Spike-Based VLSI Neural Network</a></p>
<p>Author: Yingxue Wang, Rodney J. Douglas, Shih-Chii Liu</p><p>Abstract: The neurons of the neocortex communicate by asynchronous events called action potentials (or ’spikes’). However, for simplicity of simulation, most models of processing by cortical neural networks have assumed that the activations of their neurons can be approximated by event rates rather than taking account of individual spikes. The obstacle to exploring the more detailed spike processing of these networks has been reduced considerably in recent years by the development of hybrid analog-digital Very-Large Scale Integrated (hVLSI) neural networks composed of spiking neurons that are able to operate in real-time. In this paper we describe such a hVLSI neural network that performs an interesting task of selective attentional processing that was previously described for a simulated ’pointer-map’ rate model by Hahnloser and colleagues. We found that most of the computational features of their rate model can be reproduced in the spiking implementation; but, that spike-based processing requires a modiﬁcation of the original network architecture in order to memorize a previously attended target. 1</p><p>3 0.62504566 <a title="59-lda-3" href="./nips-2006-Neurophysiological_Evidence_of_Cooperative_Mechanisms_for_Stereo_Computation.html">145 nips-2006-Neurophysiological Evidence of Cooperative Mechanisms for Stereo Computation</a></p>
<p>Author: Jason M. Samonds, Brian R. Potetz, Tai S. Lee</p><p>Abstract: Although there has been substantial progress in understanding the neurophysiological mechanisms of stereopsis, how neurons interact in a network during stereo computation remains unclear. Computational models on stereopsis suggest local competition and long-range cooperation are important for resolving ambiguity during stereo matching. To test these predictions, we simultaneously recorded from multiple neurons in V1 of awake, behaving macaques while presenting surfaces of different depths rendered in dynamic random dot stereograms. We found that the interaction between pairs of neurons was a function of similarity in receptive fields, as well as of the input stimulus. Neurons coding the same depth experienced common inhibition early in their responses for stimuli presented at their nonpreferred disparities. They experienced mutual facilitation later in their responses for stimulation at their preferred disparity. These findings are consistent with a local competition mechanism that first removes gross mismatches, and a global cooperative mechanism that further refines depth estimates. 1 In trod u ction The human visual system is able to extract three-dimensional (3D) structures in random noise stereograms even when such images evoke no perceptible patterns when viewed monocularly [1]. Bela Julesz proposed that this is accomplished by a stereopsis mechanism that detects correlated shifts in 2D noise patterns between the two eyes. He also suggested that this mechanism likely involves cooperative neural processing early in the visual system. Marr and Poggio formalized the computational constraints for solving stereo matching (Fig. 1a) and devised an algorithm that can discover the underlying 3D structures in a variety of random dot stereogram patterns [2]. Their algorithm was based on two rules: (1) each element or feature is unique (i.e., can be assigned only one disparity) and (2) surfaces of objects are cohesive (i.e., depth changes gradually across space). To describe their algorithm in neurophysiological terms, we can consider neurons in primary visual cortex as simple element or feature detectors. The first rule is implemented by introducing competitive interactions (mutual inhibition) among neurons of different disparity tuning at each location (Fig. 1b, blue solid horizontal or vertical lines), allowing only one disparity to be detected at each location. The second rule is implemented by introducing cooperative interactions (mutual facilitation) among neurons tuned to the same depth (image disparity) across different spatial locations (Fig. 1b, along the red dashed diagonal lines). In other words, a disparity estimate at one location is more likely to be correct if neighboring locations have similar disparity estimates. A dynamic system under such constraints can relax to a stable global disparity map. Here, we present neurophysiological evidence of interactions between disparity-tuned neurons in the primary visual cortex that is consistent with this general approach. We sampled from a variety of spatially distributed disparity tuned neurons (see electrodes Fig. 1b) while displaying DRDS stimuli defined at various disparities (see stimulus Fig.1b). We then measured the dynamics of interactions by assessing the temporal evolution of correlation in neural responses. a Left Image b Right Image Electrodes Disparity Left Image ? Stimulus Right Image Figure 1: (a) Left and right images of random dot stereogram (right image has been shifted to the right). (b) 1D graphical depiction of competition (blue solid lines) and cooperation (red dashed lines) among disparity-tuned neurons with respect to space as defined by Marr and Poggio’s stereo algorithm [2]. 2 2.1 Methods Recording and stimulation a Posterior - Anterior Recordings were made in V1 of two awake, behaving macaques. We simultaneously recorded from 4-8 electrodes providing data from up to 10 neurons in a single recording session (some electrodes recorded from as many as 3 neurons). We collected data from 112 neurons that provided 224 pairs for cross-correlation analysis. For stimuli, we used 12 Hz dynamic random dot stereograms (DRDS; 25% density black and white pixels on a mean luminance background) presented in a 3.5-degree aperture. Liquid crystal shutter goggles were used to present random dot patterns to each eye separately. Eleven horizontal disparities between the two eyes, ranging from ±0.9 degrees, were tested. Seventy-four neurons (66%) had significant disparity tuning and 99 pairs (44%) were comprised of neurons that both had significant disparity tuning (1-way ANOVA, p<0.05). b 5mm Medial - Lateral 100µV 0.2ms 1° Figure 2: (a) Example recording session from five electrodes in V1. (b) Receptive field (white box—arrow represents direction preference) and random dot stereogram locations for same recording session (small red square is the fixation spot). 2.2 Data analysis Interaction between neurons was described as</p><p>4 0.59201097 <a title="59-lda-4" href="./nips-2006-Inferring_Network_Structure_from_Co-Occurrences.html">98 nips-2006-Inferring Network Structure from Co-Occurrences</a></p>
<p>Author: Michael G. Rabbat, Mário Figueiredo, Robert Nowak</p><p>Abstract: We consider the problem of inferring the structure of a network from cooccurrence data: observations that indicate which nodes occur in a signaling pathway but do not directly reveal node order within the pathway. This problem is motivated by network inference problems arising in computational biology and communication systems, in which it is difﬁcult or impossible to obtain precise time ordering information. Without order information, every permutation of the activated nodes leads to a different feasible solution, resulting in combinatorial explosion of the feasible set. However, physical principles underlying most networked systems suggest that not all feasible solutions are equally likely. Intuitively, nodes that co-occur more frequently are probably more closely connected. Building on this intuition, we model path co-occurrences as randomly shufﬂed samples of a random walk on the network. We derive a computationally efﬁcient network inference algorithm and, via novel concentration inequalities for importance sampling estimators, prove that a polynomial complexity Monte Carlo version of the algorithm converges with high probability. 1</p><p>5 0.58461457 <a title="59-lda-5" href="./nips-2006-A_selective_attention_multi--chip_system_with_dynamic_synapses_and_spiking_neurons.html">18 nips-2006-A selective attention multi--chip system with dynamic synapses and spiking neurons</a></p>
<p>Author: Chiara Bartolozzi, Giacomo Indiveri</p><p>Abstract: Selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity: salient subregions of the input stimuli are serially processed, while non–salient regions are suppressed. We present an mixed mode analog/digital Very Large Scale Integration implementation of a building block for a multi–chip neuromorphic hardware model of selective attention. We describe the chip’s architecture and its behavior, when its is part of a multi–chip system with a spiking retina as input, and show how it can be used to implement in real-time ﬂexible models of bottom-up attention. 1</p><p>6 0.57949686 <a title="59-lda-6" href="./nips-2006-Modelling_transcriptional_regulation_using_Gaussian_Processes.html">135 nips-2006-Modelling transcriptional regulation using Gaussian Processes</a></p>
<p>7 0.56152165 <a title="59-lda-7" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<p>8 0.5359962 <a title="59-lda-8" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>9 0.49631178 <a title="59-lda-9" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>10 0.48014975 <a title="59-lda-10" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>11 0.44803005 <a title="59-lda-11" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>12 0.42104408 <a title="59-lda-12" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>13 0.4209379 <a title="59-lda-13" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>14 0.40113398 <a title="59-lda-14" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>15 0.39323142 <a title="59-lda-15" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>16 0.39262637 <a title="59-lda-16" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<p>17 0.38432017 <a title="59-lda-17" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>18 0.38262606 <a title="59-lda-18" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>19 0.37553352 <a title="59-lda-19" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>20 0.37459141 <a title="59-lda-20" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
