<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-111" href="#">nips2006-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</h1>
<br/><p>Source: <a title="nips-2006-111-pdf" href="http://papers.nips.cc/paper/3109-learning-motion-style-synthesis-from-perceptual-observations.pdf">pdf</a></p><p>Author: Lorenzo Torresani, Peggy Hackney, Christoph Bregler</p><p>Abstract: This paper presents an algorithm for synthesis of human motion in speciﬁed styles. We use a theory of movement observation (Laban Movement Analysis) to describe movement styles as points in a multi-dimensional perceptual space. We cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. We demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data. 1</p><p>Reference: <a title="nips-2006-111-reference" href="../nips2006_reference/nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper presents an algorithm for synthesis of human motion in speciﬁed styles. [sent-6, score-0.633]
</p><p>2 We use a theory of movement observation (Laban Movement Analysis) to describe movement styles as points in a multi-dimensional perceptual space. [sent-7, score-0.613]
</p><p>3 We cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. [sent-8, score-2.21]
</p><p>4 We demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data. [sent-9, score-1.702]
</p><p>5 1  Introduction  Human motion perception can be generally thought of as the result of interaction of two factors, traditionally termed content and style. [sent-10, score-0.653]
</p><p>6 ), while style denotes the particular way that action is performed. [sent-14, score-0.391]
</p><p>7 In computer animation, the separation of the underlying content of a movement from its stylistic characteristics is particularly important. [sent-15, score-0.394]
</p><p>8 For example, a system that can synthesize stylistic variations of a given action would be a useful tool for animators. [sent-16, score-0.319]
</p><p>9 In this work we address such a problem by proposing a system that applies user-speciﬁed styles to motion sequences. [sent-17, score-0.812]
</p><p>10 Speciﬁcally, given as input a target motion style and an arbitrary animation or pre-recorded motion, we want to synthesize a novel sequence that preserves the content of the original input motion but exhibits style similar to the user-speciﬁed target. [sent-18, score-2.406]
</p><p>11 Concatenative synthesis techniques [15, 1, 11] are based on the simple idea of generating novel movements by concatenation of motion capture snippets. [sent-20, score-0.792]
</p><p>12 Since motion is produced by cutting and pasting pre-recorded examples, the resulting animations achieve realism similar to that of pure motion-capture play back. [sent-21, score-0.65]
</p><p>13 Snippet concatenation can produce novel content by generating arbitrarily complex new movements. [sent-22, score-0.32]
</p><p>14 However, this approach is restricted to synthesize only the subset of styles originally contained in the input database. [sent-23, score-0.447]
</p><p>15 Sample-based concatenation techniques are unable to produce novel stylistic variations and cannot generalize style differences from the existing examples. [sent-24, score-0.68]
</p><p>16 Unfortunately, most of these methods learn simple parametric motion models that are unable to fully capture the subtleties and complexities of human movement. [sent-26, score-0.616]
</p><p>17 The aim is to maintain the animated precision of motion capture data, while introducing the ﬂexibility of style changes achievable by learned  parametric models. [sent-29, score-0.983]
</p><p>18 Our system builds on the observation that stylistically novel, yet highly realistic animations can be generated via space-time interpolation of pairs of motion sequences. [sent-30, score-0.992]
</p><p>19 We propose to learn not a parametric function of the motion, but rather a parametric function of how the interpolation or extrapolation weights applied to data snippets relate to the styles of the output sequences. [sent-31, score-0.775]
</p><p>20 This allows us to create motions with arbitrary styles without compromising animation quality. [sent-32, score-0.654]
</p><p>21 Several researchers have previously proposed the use of motion interpolation for synthesis of novel movement [18, 6, 10]. [sent-33, score-1.061]
</p><p>22 These approaches are based on the na¨ve assumption that motion interpolation ı produces styles corresponding precisely to the interpolation of the styles of the original sequences. [sent-34, score-1.613]
</p><p>23 In this paper we experimentally demonstrate that styles generated through motion interpolation are a rather complex function of styles and contents of the original snippets. [sent-35, score-1.365]
</p><p>24 We propose to explicitly learn the mapping between motion blending parameters and resulting animation styles. [sent-36, score-0.724]
</p><p>25 This enables our animation system not only to generate arbitrary stylistic variations of a given action, but, more importantly, to synthesize sequences matching user-speciﬁed stylistic characteristics. [sent-37, score-0.722]
</p><p>26 [16], in which interpolation models parameterized by style attributes are learned for several actions, such as walking or reaching. [sent-39, score-0.727]
</p><p>27 This feature allows our algorithm to be used for style editing of sequences without content speciﬁcation by the user. [sent-41, score-0.604]
</p><p>28 2  The LMA Framework  In computer animation literature motion style is a vaguely deﬁned concept. [sent-43, score-1.068]
</p><p>29 In our work, we describe motion styles according to a movement notation system, called Laban Movement Analysis or LMA [7]. [sent-44, score-0.917]
</p><p>30 Instead, it targets the subtle differences in motion style, e. [sent-49, score-0.523]
</p><p>31 Generally, additional features not present in motion capture data, such as eye gaze, are necessary to detect this factor. [sent-64, score-0.532]
</p><p>32 We model styles as points in a three-dimensional perceptual space derived by translating the LMA-Effort notations for each of these factors into numerical values ranging in the interval [−3, 3]. [sent-66, score-0.333]
</p><p>33 3  Overview of the system  The key-idea of our work is to learn motion style synthesis from a training set of computer-generated animations. [sent-67, score-1.05]
</p><p>34 This set of supervised data is used to learn a mapping between the space of motion styles and the animation system parameters. [sent-69, score-1.043]
</p><p>35 1  Training: Learning the Style of Motion Interpolation  In order to train our system to synthesize motion styles, we employ a corpus of human motion sequences recorded with a motion capture system. [sent-72, score-1.798]
</p><p>36 We represent the motion as a time-varying vector of joint angles. [sent-73, score-0.524]
</p><p>37 In the training stage each motion sequence is manually segmented by an LMA human expert into fragments corresponding to fundamental actions or units of motions. [sent-74, score-0.835]
</p><p>38 We apply a motion matching algorithm to identify fragment pairs (Xi , Xj ) containing similar actions. [sent-77, score-0.578]
</p><p>39 Our motion matching algorithm is based on dynamic-time warping. [sent-78, score-0.541]
</p><p>40 Both the synthesized animations Xα as well as the ”seed” i,j motion capture data Xi are labeled with LMA-Effort values by an LMA expert. [sent-83, score-0.73]
</p><p>41 i,j A non-linear regression model [5] is ﬁtted to the LMA labels and the parameters α of the space-time interpolation algorithm. [sent-85, score-0.336]
</p><p>42 2  Testing: Style Transfer  ¯ At testing stage we are given a motion sequence Y, and a user-speciﬁed motion style e. [sent-88, score-1.428]
</p><p>43 The goal ¯ is to apply style e to the input sequence Y, without modifying the content of the motion. [sent-89, score-0.602]
</p><p>44 First, we use dynamic-time warping to segment the input sequence into snippets Yi , such that each snippet matches the content of a set of analogous motions {Xi1 , . [sent-90, score-0.649]
</p><p>45 , XiK }, we determine the one that ik ¯ provides the best approximation to the target style e. [sent-97, score-0.452]
</p><p>46 4  Matching motion content  The objective of the matching algorithm is to identify pairs of sequences having similar motion content or consisting of analogous activities. [sent-100, score-1.431]
</p><p>47 The method should ignore variations in the style with which movements are performed. [sent-101, score-0.421]
</p><p>48 Previous work [2, 12] has shown that the differences in movement styles can be found by examining the parameters of timing and movement acceleration. [sent-102, score-0.625]
</p><p>49 Thus we compare the content of two motions by identifying similar spatial body poses while allowing for potentially large differences in timing. [sent-104, score-0.385]
</p><p>50 Speciﬁcally, we deﬁne the content similarity between motion snippets Xi and Xj , as the minimum sum of their squared joint angle differences SSD(Xi , Xi ) under a dynamic time warping path. [sent-105, score-0.911]
</p><p>51 5  Space-time interpolation  A time warping strategy is also employed to synthesize novel animations from the pairs of contentmatching examples found by the algorithm outlined in the previous section. [sent-109, score-0.739]
</p><p>52 Given matching snippets Xi and Xj , the objective is to generate a stylistically novel sequence that maintains the content of the two original motions. [sent-110, score-0.44]
</p><p>53 The idea is to induce changes in style by acting primarily on the timings of the motions. [sent-111, score-0.367]
</p><p>54 we estimate joint angles Xj at time steps q∗ (n0 )) then the resampled motion will k correspond to playing sequence j with the timing of sequence i. [sent-127, score-0.708]
</p><p>55 , n1 j can be cho1 T sen, such that q∗ (n1 ) = k, and these parameter values can be used to synthesize motion i with the k timing of motion j. [sent-132, score-1.154]
</p><p>56 It is also possible to smoothly interpolate between these two scenarios according to an interpolation parameter α ∈ [0, 1] to produce intermediate time warps. [sent-133, score-0.338]
</p><p>57 New stylistic versions of motions i and j can be produced by estimating the joint angles Xi and Xj at p∗ (nα ) and q∗ (nα ), respectively. [sent-140, score-0.342]
</p><p>58 From these two synchronized sequences, a novel motion Xα can be generated by averaging the joint angles according i,j to mixing coefﬁcients (1 − α) and α: Xα (k) = (1 − α)Xi (p∗ (nα )) + αXj (q∗ (nα )). [sent-142, score-0.647]
</p><p>59 The synthei,j k k sized motion Xα will display content similar to that of Xi and Xj , but it will have distinct style. [sent-143, score-0.653]
</p><p>60 6  Learning style interpolation  Given a pair of content-matching snippets Xi and Xj , our goal is to determine the parameter α that needs to be applied to space-time interpolation in order to produce a motion Xα exhibiting target i,j ¯ style e. [sent-145, score-1.984]
</p><p>61 The training data for this supervised learning task consists of our seed motion sequences {Xi } in the database, a set of interpolated motions {Xα }, and the i,j corresponding LMA-Effort qualities {ei }, {eα } observed by an LMA human expert. [sent-147, score-0.897]
</p><p>62 In order to avoid overﬁtting, we compress further the motion data by projecting it onto a low-dimensional linear subspace computed using Principal Component Analysis (PCA). [sent-149, score-0.493]
</p><p>63 In many of the test cases, we found it was sufﬁcient to retain only the ﬁrst two or three principal components in order to obtain a discriminative representation of the motion contents. [sent-150, score-0.493]
</p><p>64 While kernel ridge regression requires us to store all training examples in order to evaluate function f at a given input, support vector regression overcomes this limitation by using an ǫ-insensitive loss function [17]. [sent-163, score-0.301]
</p><p>65 7  Testing: Style Transfer  We can restate our initial objective as follows: given an input motion sequence Y in unknown style, ¯ and a target motion style e speciﬁed by LMA-Effort values, we want to synthesize a sequence having ¯ style e and content analogous to that of motion Y. [sent-165, score-2.671]
</p><p>66 A na¨ve approach to this problem is to seek in ı the motion database a pair of sequences having content similar to Y and whose interpolation can ¯ approximate style e. [sent-166, score-1.418]
</p><p>67 The learned function f can be used to determine the pair of motions and the ¯ interpolation parameter α that produce the best approximation to e. [sent-167, score-0.495]
</p><p>68 The idea is to determine the concatenation of database motion examples [X1 , . [sent-172, score-0.651]
</p><p>69 , XN ] determined by the ¯ method outlined in the previous section to synthesize a version of motion Y in style e. [sent-210, score-1.028]
</p><p>70 The ﬁnal goal then is to replace each snippet Yi with a pairwise blend of examples ¯ in its cluster so as to produce a motion exhibiting style e. [sent-226, score-1.027]
</p><p>71 We then select the pair (ik∗ , il∗ ) providing the minimum deviation ¯ from the target style e. [sent-228, score-0.404]
</p><p>72 25] rather than [0,1], we give the algorithm the ability to extrapolate from existing motion styles. [sent-236, score-0.518]
</p><p>73 Given optimal parameters (α∗ , k ∗ , l∗ ), space-time interpolation of fragments Xik∗ and Xil∗ with parameter value α∗ produces an animation with content similar to that of Yi and style approximating ¯ the desired target e. [sent-237, score-1.179]
</p><p>74 The ﬁnal animation is obtained by concatenating all of the fragments generated via interpolation with optimal parameters. [sent-239, score-0.615]
</p><p>75 8  Experiments  The system was tested using a motion database consisting of 12 sequences performed by different professional dancers. [sent-240, score-0.65]
</p><p>76 All fragments were then automatically clustered into 5 content groups using the SSD criterion outlined in section 4. [sent-243, score-0.322]
</p><p>77 The motions were recorded using a marker-based motion capture system. [sent-244, score-0.694]
</p><p>78 The joint angles were represented with exponential maps [13], which have the property of being locally linear and thus particularly suitable for motion interpolation. [sent-246, score-0.579]
</p><p>79 From these 60 motion fragments, 105 novel motions were synthesized with space-time interpolation using random values of α in the range [−0. [sent-247, score-1.015]
</p><p>80 5  α  Figure 1: Sample LMA-Effort attributes estimated by kernel ridge regression on three different pairs of motions (Xi , Xj ) and for α varying in [-0. [sent-259, score-0.385]
</p><p>81 From this set of motions, 85 training examples were randomly selected to train the style regression models. [sent-265, score-0.48]
</p><p>82 We include in our analysis the linear style interpolation model, commonly used in previous work. [sent-269, score-0.643]
</p><p>83 This model assumes that the style of a sequence generated via motion interpolation is equal to the interpolation of the styles of the two seed motions: eα = αei + (1 − α)ej . [sent-270, score-1.782]
</p><p>84 Overall, non-linear regression models proved to be much superior to the linear interpolation function, indicating that the style of sequences generates via space-time interpolation is a complex function of the original styles and motions. [sent-277, score-1.34]
</p><p>85 Figure 1 shows the LMA-Effort qualities predicted by kernel ridge regression while varying α for three different sample values of the inputs (Xi , Xj , ei , ej ). [sent-278, score-0.33]
</p><p>86 Several additional motion examples performed by dancers not included in the training data were used to evaluate the complete pipeline of the motion synthesis algorithm. [sent-282, score-1.148]
</p><p>87 The input sequences were always correctly segmented by the dynamic programming algorithm into the ﬁve fragments associated with the actions in the phrase. [sent-283, score-0.315]
</p><p>88 In order to test the generalization ability of our system, the target styles in this experiment were chosen to be considerably different from those in the training set. [sent-289, score-0.344]
</p><p>89 All of the synthesized sequences were visually inspected by LMA experts and, for the great majority, they were found to be consistent with the style target labels. [sent-290, score-0.522]
</p><p>90 9  Discussions and Future Work  We have presented a novel technique that learns motion style synthesis from artiﬁcially-generated examples. [sent-291, score-1.012]
</p><p>91 Animations produced by our system have quality similar to pure motion capture playback. [sent-292, score-0.567]
</p><p>92 Furthermore, we have shown that, even with a small database, it is possible to use pair-wise interpolation or extrapolation to generate new styles. [sent-293, score-0.299]
</p><p>93 In previous LMA-based animation systems [3], heuristic and hand-designed rules have been adopted to implement the style changes associated to LMA-Effort variations. [sent-294, score-0.575]
</p><p>94 Although  our algorithm has shown to produce good results with small training data, we expect that larger databases with a wider variety of motion contents and styles are needed in order to build an effective animation system. [sent-296, score-1.07]
</p><p>95 Multi-way, as opposed to pair-wise, interpolation might lead to synthesis of more varied motion styles. [sent-297, score-0.878]
</p><p>96 Our future work will focus on the recognition of LMA categories in motion capture data. [sent-299, score-0.532]
</p><p>97 Research in this area might point to methods for learning person-speciﬁc styles and to techniques for transferring individual movement signatures to arbitrary motion sequences. [sent-300, score-0.917]
</p><p>98 Morphable models for the analysis and synthesis of complex motion patterns. [sent-344, score-0.602]
</p><p>99 Interactive control of avatars animated with human motion data. [sent-377, score-0.555]
</p><p>100 Motion texture: A two-level statistical model for character motion synthesis. [sent-384, score-0.493]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('motion', 0.493), ('style', 0.367), ('styles', 0.284), ('interpolation', 0.276), ('animation', 0.208), ('lma', 0.203), ('motions', 0.162), ('content', 0.16), ('animations', 0.157), ('movement', 0.14), ('synthesize', 0.137), ('fragments', 0.131), ('synthesis', 0.109), ('snippets', 0.109), ('ridge', 0.098), ('stylistic', 0.094), ('flow', 0.092), ('graphics', 0.092), ('xik', 0.087), ('concatenation', 0.083), ('snippet', 0.078), ('sequences', 0.077), ('qualities', 0.074), ('xi', 0.074), ('ssd', 0.068), ('warping', 0.065), ('laban', 0.063), ('regression', 0.06), ('xj', 0.057), ('angles', 0.055), ('sequence', 0.049), ('perceptual', 0.049), ('matching', 0.048), ('ik', 0.048), ('july', 0.047), ('concatenative', 0.047), ('poles', 0.047), ('xil', 0.047), ('database', 0.045), ('novel', 0.043), ('opposing', 0.041), ('synthesized', 0.041), ('acm', 0.041), ('path', 0.04), ('capture', 0.039), ('target', 0.037), ('fragment', 0.037), ('seed', 0.037), ('tj', 0.037), ('ei', 0.037), ('frames', 0.036), ('system', 0.035), ('attributes', 0.035), ('frame', 0.034), ('produce', 0.034), ('body', 0.033), ('human', 0.031), ('joint', 0.031), ('adverbs', 0.031), ('animated', 0.031), ('blends', 0.031), ('stylistically', 0.031), ('zj', 0.031), ('ej', 0.031), ('outlined', 0.031), ('il', 0.031), ('timing', 0.031), ('examples', 0.03), ('parametric', 0.03), ('differences', 0.03), ('mse', 0.03), ('kernel', 0.03), ('variations', 0.029), ('segmented', 0.029), ('actions', 0.029), ('smoothly', 0.028), ('contents', 0.028), ('eik', 0.027), ('lorenzo', 0.027), ('rose', 0.027), ('input', 0.026), ('walking', 0.026), ('stage', 0.026), ('movements', 0.025), ('extrapolate', 0.025), ('touch', 0.025), ('synchronized', 0.025), ('exhibiting', 0.025), ('endpoint', 0.025), ('action', 0.024), ('transactions', 0.024), ('ti', 0.024), ('expert', 0.024), ('learn', 0.023), ('learned', 0.023), ('dynamic', 0.023), ('tij', 0.023), ('extrapolation', 0.023), ('yi', 0.023), ('training', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="111-tfidf-1" href="./nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations.html">111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</a></p>
<p>Author: Lorenzo Torresani, Peggy Hackney, Christoph Bregler</p><p>Abstract: This paper presents an algorithm for synthesis of human motion in speciﬁed styles. We use a theory of movement observation (Laban Movement Analysis) to describe movement styles as points in a multi-dimensional perceptual space. We cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. We demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data. 1</p><p>2 0.32339579 <a title="111-tfidf-2" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. After training, the model ﬁnds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line ﬁlling in of data lost during motion capture. Website: http://www.cs.toronto.edu/∼gwtaylor/publications/nips2006mhmublv/</p><p>3 0.26599023 <a title="111-tfidf-3" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>Author: Ce Liu, William T. Freeman, Edward H. Adelson</p><p>Abstract: A reliable motion estimation algorithm must function under a wide range of conditions. One regime, which we consider here, is the case of moving objects with contours but no visible texture. Tracking distinctive features such as corners can disambiguate the motion of contours, but spurious features such as T-junctions can be badly misleading. It is difﬁcult to determine the reliability of motion from local measurements, since a full rank covariance matrix can result from both real and spurious features. We propose a novel approach that avoids these points altogether, and derives global motion estimates by utilizing information from three levels of contour analysis: edgelets, boundary fragments and contours. Boundary fragment are chains of orientated edgelets, for which we derive motion estimates from local evidence. The uncertainties of the local estimates are disambiguated after the boundary fragments are properly grouped into contours. The grouping is done by constructing a graphical model and marginalizing it using importance sampling. We propose two equivalent representations in this graphical model, reversible switch variables attached to the ends of fragments and fragment chains, to capture both local and global statistics of boundaries. Our system is successfully applied to both synthetic and real video sequences containing high-contrast boundaries and textureless regions. The system produces good motion estimates along with properly grouped and completed contours.</p><p>4 0.10096779 <a title="111-tfidf-4" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>Author: Andriy Myronenko, Xubo Song, Miguel Á. Carreira-Perpiñán</p><p>Abstract: We introduce Coherent Point Drift (CPD), a novel probabilistic method for nonrigid registration of point sets. The registration is treated as a Maximum Likelihood (ML) estimation problem with motion coherence constraint over the velocity ﬁeld such that one point set moves coherently to align with the second set. We formulate the motion coherence constraint and derive a solution of regularized ML estimation through the variational approach, which leads to an elegant kernel form. We also derive the EM algorithm for the penalized ML optimization with deterministic annealing. The CPD method simultaneously ﬁnds both the non-rigid transformation and the correspondence between two point sets without making any prior assumption of the transformation model except that of motion coherence. This method can estimate complex non-linear non-rigid transformations, and is shown to be accurate on 2D and 3D examples and robust in the presence of outliers and missing points.</p><p>5 0.094559774 <a title="111-tfidf-5" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>6 0.090265326 <a title="111-tfidf-6" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>7 0.08607927 <a title="111-tfidf-7" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>8 0.083289146 <a title="111-tfidf-8" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>9 0.081810154 <a title="111-tfidf-9" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>10 0.077681057 <a title="111-tfidf-10" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>11 0.07741116 <a title="111-tfidf-11" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>12 0.077341594 <a title="111-tfidf-12" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>13 0.072508223 <a title="111-tfidf-13" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>14 0.063220538 <a title="111-tfidf-14" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>15 0.054596022 <a title="111-tfidf-15" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>16 0.049436152 <a title="111-tfidf-16" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>17 0.048527591 <a title="111-tfidf-17" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>18 0.048404448 <a title="111-tfidf-18" href="./nips-2006-Implicit_Surfaces_with_Globally_Regularised_and_Compactly_Supported_Basis_Functions.html">95 nips-2006-Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions</a></p>
<p>19 0.046697803 <a title="111-tfidf-19" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>20 0.044591386 <a title="111-tfidf-20" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, 0.015), (2, 0.093), (3, -0.048), (4, 0.012), (5, -0.092), (6, -0.049), (7, -0.098), (8, -0.013), (9, 0.221), (10, -0.097), (11, -0.157), (12, -0.218), (13, 0.115), (14, -0.061), (15, -0.061), (16, -0.385), (17, -0.095), (18, -0.215), (19, -0.047), (20, -0.067), (21, 0.09), (22, -0.038), (23, 0.183), (24, 0.027), (25, -0.051), (26, -0.062), (27, 0.01), (28, -0.001), (29, 0.008), (30, 0.031), (31, 0.019), (32, 0.059), (33, -0.007), (34, 0.095), (35, 0.133), (36, -0.106), (37, 0.055), (38, -0.088), (39, -0.071), (40, 0.092), (41, 0.1), (42, 0.043), (43, -0.069), (44, 0.063), (45, -0.024), (46, 0.016), (47, 0.052), (48, 0.051), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97278684 <a title="111-lsi-1" href="./nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations.html">111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</a></p>
<p>Author: Lorenzo Torresani, Peggy Hackney, Christoph Bregler</p><p>Abstract: This paper presents an algorithm for synthesis of human motion in speciﬁed styles. We use a theory of movement observation (Laban Movement Analysis) to describe movement styles as points in a multi-dimensional perceptual space. We cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. We demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data. 1</p><p>2 0.85815269 <a title="111-lsi-2" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>Author: Ce Liu, William T. Freeman, Edward H. Adelson</p><p>Abstract: A reliable motion estimation algorithm must function under a wide range of conditions. One regime, which we consider here, is the case of moving objects with contours but no visible texture. Tracking distinctive features such as corners can disambiguate the motion of contours, but spurious features such as T-junctions can be badly misleading. It is difﬁcult to determine the reliability of motion from local measurements, since a full rank covariance matrix can result from both real and spurious features. We propose a novel approach that avoids these points altogether, and derives global motion estimates by utilizing information from three levels of contour analysis: edgelets, boundary fragments and contours. Boundary fragment are chains of orientated edgelets, for which we derive motion estimates from local evidence. The uncertainties of the local estimates are disambiguated after the boundary fragments are properly grouped into contours. The grouping is done by constructing a graphical model and marginalizing it using importance sampling. We propose two equivalent representations in this graphical model, reversible switch variables attached to the ends of fragments and fragment chains, to capture both local and global statistics of boundaries. Our system is successfully applied to both synthetic and real video sequences containing high-contrast boundaries and textureless regions. The system produces good motion estimates along with properly grouped and completed contours.</p><p>3 0.72710508 <a title="111-lsi-3" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. After training, the model ﬁnds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line ﬁlling in of data lost during motion capture. Website: http://www.cs.toronto.edu/∼gwtaylor/publications/nips2006mhmublv/</p><p>4 0.4594734 <a title="111-lsi-4" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>Author: René Vidal</p><p>Abstract: We propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes. Starting from a given or random initial condition, we use normalized gradient descent to update the coefﬁcients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory. As time proceeds, the estimates of the hyperplane normals are shown to track their true values in a stable fashion. The segmentation of the trajectories is then obtained by clustering their associated normal vectors. The ﬁnal result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes. We test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures, e.g., a bird ﬂoating on water. Our method not only segments the bird motion from the surrounding water motion, but also determines patterns of motion in the scene (e.g., periodic motion) directly from the temporal evolution of the estimated polynomial coefﬁcients. Our experiments also show that our method can deal with appearing and disappearing motions in the scene.</p><p>5 0.36111325 <a title="111-lsi-5" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>Author: David B. Grimes, Daniel R. Rashid, Rajesh P. Rao</p><p>Abstract: Learning by imitation represents an important mechanism for rapid acquisition of new behaviors in humans and robots. A critical requirement for learning by imitation is the ability to handle uncertainty arising from the observation process as well as the imitator’s own dynamics and interactions with the environment. In this paper, we present a new probabilistic method for inferring imitative actions that takes into account both the observations of the teacher as well as the imitator’s dynamics. Our key contribution is a nonparametric learning method which generalizes to systems with very different dynamics. Rather than relying on a known forward model of the dynamics, our approach learns a nonparametric forward model via exploration. Leveraging advances in approximate inference in graphical models, we show how the learned forward model can be directly used to plan an imitating sequence. We provide experimental results for two systems: a biomechanical model of the human arm and a 25-degrees-of-freedom humanoid robot. We demonstrate that the proposed method can be used to learn appropriate motor inputs to the model arm which imitates the desired movements. A second set of results demonstrates dynamically stable full-body imitation of a human teacher by the humanoid robot. 1</p><p>6 0.34629777 <a title="111-lsi-6" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>7 0.32453209 <a title="111-lsi-7" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>8 0.28059146 <a title="111-lsi-8" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>9 0.2804141 <a title="111-lsi-9" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>10 0.27107713 <a title="111-lsi-10" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>11 0.2379079 <a title="111-lsi-11" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>12 0.2374672 <a title="111-lsi-12" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>13 0.23283099 <a title="111-lsi-13" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>14 0.21618368 <a title="111-lsi-14" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>15 0.21470536 <a title="111-lsi-15" href="./nips-2006-Implicit_Surfaces_with_Globally_Regularised_and_Compactly_Supported_Basis_Functions.html">95 nips-2006-Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions</a></p>
<p>16 0.20476308 <a title="111-lsi-16" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>17 0.18538214 <a title="111-lsi-17" href="./nips-2006-Isotonic_Conditional_Random_Fields_and_Local_Sentiment_Flow.html">101 nips-2006-Isotonic Conditional Random Fields and Local Sentiment Flow</a></p>
<p>18 0.18354955 <a title="111-lsi-18" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>19 0.18342167 <a title="111-lsi-19" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>20 0.17876643 <a title="111-lsi-20" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.146), (3, 0.048), (7, 0.041), (8, 0.043), (9, 0.036), (12, 0.014), (13, 0.304), (20, 0.011), (22, 0.051), (44, 0.05), (57, 0.069), (65, 0.035), (69, 0.034), (71, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80259657 <a title="111-lda-1" href="./nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations.html">111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</a></p>
<p>Author: Lorenzo Torresani, Peggy Hackney, Christoph Bregler</p><p>Abstract: This paper presents an algorithm for synthesis of human motion in speciﬁed styles. We use a theory of movement observation (Laban Movement Analysis) to describe movement styles as points in a multi-dimensional perceptual space. We cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. We demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data. 1</p><p>2 0.7310794 <a title="111-lda-2" href="./nips-2006-Logistic_Regression_for_Single_Trial_EEG_Classification.html">126 nips-2006-Logistic Regression for Single Trial EEG Classification</a></p>
<p>Author: Ryota Tomioka, Kazuyuki Aihara, Klaus-Robert Müller</p><p>Abstract: We propose a novel framework for the classiﬁcation of single trial ElectroEncephaloGraphy (EEG), based on regularized logistic regression. Framed in this robust statistical framework no prior feature extraction or outlier removal is required. We present two variations of parameterizing the regression function: (a) with a full rank symmetric matrix coeﬃcient and (b) as a diﬀerence of two rank=1 matrices. In the ﬁrst case, the problem is convex and the logistic regression is optimal under a generative model. The latter case is shown to be related to the Common Spatial Pattern (CSP) algorithm, which is a popular technique in Brain Computer Interfacing. The regression coeﬃcients can also be topographically mapped onto the scalp similarly to CSP projections, which allows neuro-physiological interpretation. Simulations on 162 BCI datasets demonstrate that classiﬁcation accuracy and robustness compares favorably against conventional CSP based classiﬁers. 1</p><p>3 0.71066499 <a title="111-lda-3" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>Author: Nicol N. Schraudolph, Simon Günter, S.v.n. Vishwanathan</p><p>Abstract: We introduce two methods to improve convergence of the Kernel Hebbian Algorithm (KHA) for iterative kernel PCA. KHA has a scalar gain parameter which is either held constant or decreased as 1/t, leading to slow convergence. Our KHA/et algorithm accelerates KHA by incorporating the reciprocal of the current estimated eigenvalues as a gain vector. We then derive and apply Stochastic MetaDescent (SMD) to KHA/et; this further speeds convergence by performing gain adaptation in RKHS. Experimental results for kernel PCA and spectral clustering of USPS digits as well as motion capture and image de-noising problems conﬁrm that our methods converge substantially faster than conventional KHA. 1</p><p>4 0.68945479 <a title="111-lda-4" href="./nips-2006-Reducing_Calibration_Time_For_Brain-Computer_Interfaces%3A_A_Clustering_Approach.html">168 nips-2006-Reducing Calibration Time For Brain-Computer Interfaces: A Clustering Approach</a></p>
<p>Author: Matthias Krauledat, Michael Schröder, Benjamin Blankertz, Klaus-Robert Müller</p><p>Abstract: Up to now even subjects that are experts in the use of machine learning based BCI systems still have to undergo a calibration session of about 20-30 min. From this data their (movement) intentions are so far infered. We now propose a new paradigm that allows to completely omit such calibration and instead transfer knowledge from prior sessions. To achieve this goal we ﬁrst deﬁne normalized CSP features and distances in-between. Second, we derive prototypical features across sessions: (a) by clustering or (b) by feature concatenation methods. Finally, we construct a classiﬁer based on these individualized prototypes and show that, indeed, classiﬁers can be successfully transferred to a new session for a number of subjects.</p><p>5 0.51947826 <a title="111-lda-5" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>Author: Dong S. Cheng, Vittorio Murino, Mário Figueiredo</p><p>Abstract: This paper proposes a new approach to model-based clustering under prior knowledge. The proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as ﬁnite mixture learning under a grouping prior. To estimate the parameters of the proposed model, we derive a (generalized) EM algorithm with a closed-form E-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require Gibbs sampling or suboptimal shortcuts. We show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature Markov random ﬁeld priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efﬁcient way. Finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes. 1</p><p>6 0.51644415 <a title="111-lda-6" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>7 0.51489657 <a title="111-lda-7" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>8 0.51433998 <a title="111-lda-8" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>9 0.51342201 <a title="111-lda-9" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>10 0.51197898 <a title="111-lda-10" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>11 0.51173824 <a title="111-lda-11" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>12 0.51101375 <a title="111-lda-12" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>13 0.51069248 <a title="111-lda-13" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>14 0.50849879 <a title="111-lda-14" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>15 0.50820935 <a title="111-lda-15" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>16 0.50667417 <a title="111-lda-16" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>17 0.50644767 <a title="111-lda-17" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>18 0.50568503 <a title="111-lda-18" href="./nips-2006-MLLE%3A_Modified_Locally_Linear_Embedding_Using_Multiple_Weights.html">127 nips-2006-MLLE: Modified Locally Linear Embedding Using Multiple Weights</a></p>
<p>19 0.50467497 <a title="111-lda-19" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>20 0.50437135 <a title="111-lda-20" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
