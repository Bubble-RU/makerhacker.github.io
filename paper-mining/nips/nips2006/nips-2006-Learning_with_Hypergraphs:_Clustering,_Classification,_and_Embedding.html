<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-123" href="#">nips2006-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</h1>
<br/><p>Source: <a title="nips-2006-123-pdf" href="http://papers.nips.cc/paper/3128-learning-with-hypergraphs-clustering-classification-and-embedding.pdf">pdf</a></p><p>Author: Dengyong Zhou, Jiayuan Huang, Bernhard Schölkopf</p><p>Abstract: We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classiﬁcation on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. 1</p><p>Reference: <a title="nips-2006-123-reference" href="../nips2006_reference/nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. [sent-11, score-0.564]
</p><p>2 Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. [sent-13, score-0.234]
</p><p>3 1  Introduction  In machine learning problem settings, we generally assume pairwise relationships among the objects of our interest. [sent-14, score-0.125]
</p><p>4 An object set endowed with pairwise relationships can be naturally illustrated as a graph, in which the vertices represent the objects, and any two vertices that have some kind of relationship are joined together by an edge. [sent-15, score-0.376]
</p><p>5 It depends on whether the pairwise relationships among objects are symmetric or not. [sent-17, score-0.125]
</p><p>6 A hyperlink can be thought of as a directed edge because given an arbitrary hyperlink we cannot expect that there certainly exists an inverse one, that is, the hyperlink based relationships are asymmetric [20]. [sent-20, score-0.283]
</p><p>7 However, in many real-world problems, representing a set of complex relational objects as undirected or directed graphs is not complete. [sent-21, score-0.218]
</p><p>8 For illustrating this point of view, let us consider a problem of grouping a collection of articles into diﬀerent topics. [sent-22, score-0.112]
</p><p>9 One may construct an undirected graph in which two vertices are joined together by an edge if there is at least one common author of their corresponding articles (Figure 1), and then an undirected graph based clustering approach is applied, e. [sent-24, score-0.737]
</p><p>10 The undirected graph may be further embellished by assigning to each edge a weight equal to the  Figure 1: Hypergraph vs. [sent-27, score-0.192]
</p><p>11 The entry (vi , ej ) is set to 1 if ej is an author of article vi , and 0 otherwise. [sent-30, score-0.11]
</p><p>12 Middle: an undirected graph in which two articles are joined together by an edge if there is at least one author in common. [sent-31, score-0.398]
</p><p>13 This graph cannot tell us whether the same person is the author of three or more articles or not. [sent-32, score-0.27]
</p><p>14 Right: a hypergraph which completely illustrates the complex relationships among authors and articles. [sent-33, score-0.758]
</p><p>15 The above method may sound natural, but within its graph representation we obviously miss the information on whether the same person joined writing three or more articles or not. [sent-35, score-0.294]
</p><p>16 Such information loss is unexpected because the articles by the same person likely belong to the same topic and hence the information is useful for our grouping task. [sent-36, score-0.143]
</p><p>17 A natural way of remedying the information loss issue occurring in the above methodology is to represent the data as a hypergraph instead. [sent-37, score-0.727]
</p><p>18 A hypergraph is a graph in which an edge can connect more than two vertices [2]. [sent-38, score-0.943]
</p><p>19 In what follows, we shall uniﬁedly refer to the usual undirected or directed graphs as simple graphs. [sent-40, score-0.18]
</p><p>20 It is obvious that a simple graph is a special kind of hypergraph with each edge containing two vertices only. [sent-42, score-0.943]
</p><p>21 In the problem of clustering articles stated before, it is quite straightforward to construct a hypergraph with the vertices representing the articles, and the edges the authors (Figure 1). [sent-43, score-0.995]
</p><p>22 Each edge contains all articles by its corresponding author. [sent-44, score-0.148]
</p><p>23 A powerful technique for partitioning simple graphs is spectral clustering. [sent-51, score-0.217]
</p><p>24 Therefore, we generalize spectral clustering techniques to hypergraphs, more speciﬁcally, the normalized cut approach of [16]. [sent-52, score-0.366]
</p><p>25 Moreover, as in the case of simple graphs, a real-valued relaxation of the hypergraph normalized cut criterion leads to the eigendecomposition of a positive semideﬁnite matrix, which can be regarded as an analogue of the so-called Laplacian for simple graphs (cf. [sent-53, score-0.99]
</p><p>26 [5]), and hence we suggestively call it the hypergraph Laplacian. [sent-54, score-0.729]
</p><p>27 Consequently, we develop algorithms for hypergraph embedding and transductive inference based on the hypergraph Laplacian. [sent-55, score-1.524]
</p><p>28 There have actually existed a large amount of literature on hypergraph partitioning, which arises from a variety of practical problems, such as partitioning circuit netlists [11], clustering categorial data [9], and image segmentation [1]. [sent-56, score-0.812]
</p><p>29 Unlike the present work however, they generally transformed hypergraphs to simple ones by using the heuristics we discussed in the beginning or other domain-speciﬁc heuristics, and then applied simple graph based spectral clustering techniques. [sent-57, score-0.508]
</p><p>30 We ﬁrst introduce some basic notions on hypergraphs in Section 2. [sent-62, score-0.234]
</p><p>31 In Section 3, we generalize the simple graph normalized cut to  hypergraphs. [sent-63, score-0.276]
</p><p>32 As shown in Section 4, the hypergraph normalized cut has an elegant probabilistic interpretation based on a random walk naturally associated with a hypergraph. [sent-64, score-0.952]
</p><p>33 In Section 5, we introduce the real-valued relaxation to approximately obtain hypergraph normalized cuts, and also the hypergraph Laplacian derived from this relaxation. [sent-65, score-1.495]
</p><p>34 In section 6, we develop a spectral hypergraph embedding technique based on the hypergraph Laplacian. [sent-66, score-1.555]
</p><p>35 In Section 7, we address transductive inference on hypergraphs, this is, classifying the vertices of a hypergraph provided that some of its vertices have been labeled. [sent-67, score-1.013]
</p><p>36 Then we call G = (V, E) a hypergraph with the vertex set V and the hyperedge set E. [sent-70, score-0.946]
</p><p>37 A hyperedge containing just two vertices is a simple graph edge. [sent-71, score-0.385]
</p><p>38 A weighted hypergraph is a hypergraph that has a positive number w(e) associated with each hyperedge e, called the weight of hyperedge e. [sent-72, score-1.756]
</p><p>39 A hyperedge e is said to be incident with a vertex v when v ∈ e. [sent-74, score-0.284]
</p><p>40 For a hyperedge e ∈ E, its degree is deﬁned to be δ(e) = |e|. [sent-77, score-0.178]
</p><p>41 We say that there is a hyperpath between vertices v1 and vk when there is an alternative sequence of distinct vertices and hyperedges v1 , e1 , v2 , e2 , . [sent-78, score-0.369]
</p><p>42 A hypergraph is connected if there is a path for every pair of vertices. [sent-82, score-0.7]
</p><p>43 In what follows, the hypergraphs we mention are always assumed to be connected. [sent-83, score-0.234]
</p><p>44 A hypergraph G can be represented by a |V | × |E| matrix H with entries h(v, e) = 1 if v ∈ e and 0 otherwise, called the incidence matrix of G. [sent-84, score-0.7]
</p><p>45 Let Dv and De denote the diagonal matrices containing the vertex and hyperedge degrees respectively, and let W denote the diagonal matrix containing the weights of hyperedges. [sent-86, score-0.246]
</p><p>46 Then the adjacency matrix A of hypergraph G is deﬁned as A = HW H T − Dv , where H T is the transpose of H. [sent-87, score-0.7]
</p><p>47 3  Normalized hypergraph cut  For a vertex subset S ⊂ V, let S c denote the compliment of S. [sent-88, score-0.904]
</p><p>48 A cut of a hypergraph G = (V, E, w) is a partition of V into two parts S and S c . [sent-89, score-0.866]
</p><p>49 We say that a hyperedge e is cut if it is incident with the vertices in S and S c simultaneously. [sent-90, score-0.467]
</p><p>50 Given a vertex subset S ⊂ V, deﬁne the hyperedge boundary ∂S of S to be a hyperedge set which consists of hyperedges which are cut, i. [sent-91, score-0.483]
</p><p>51 ∂S := {e ∈ E|e ∩ S = ∅, e ∩ S c = ∅}, and deﬁne the volume vol S of S to be the sum of the degrees of the vertices in S, that is,vol S := v∈S d(v). [sent-93, score-0.51]
</p><p>52 Moreover, deﬁne the volume of ∂S by vol ∂S :=  w(e) e∈∂S  |e ∩ S| |e ∩ S c | . [sent-94, score-0.395]
</p><p>53 Then, when a hyperedge e is cut, there are |e ∩ S| |e ∩ S c | subedges are cut, and hence a single sum term in Equation (1) is the sum of the weights over the subedges which are cut. [sent-102, score-0.236]
</p><p>54 Naturally, we try to obtain a partition in which the connection among the vertices in the same cluster is dense while the connection between two clusters is sparse. [sent-103, score-0.145]
</p><p>55 (2) argmin c(S) := vol ∂S vol S vol S c ∅=S⊂V For a simple graph, |e ∩ S| = |e ∩ S c | = 1, and δ(e) = 2. [sent-105, score-1.185]
</p><p>56 Thus the right-hand side of Equation (2) reduces to the simple graph normalized cut [16] up to a factor 1/2. [sent-106, score-0.276]
</p><p>57 In what follows, we explain the hypergraph normalized cut in terms of random walks. [sent-107, score-0.884]
</p><p>58 4  Random walk explanation  We associate each hypergraph with a natural random walk which has the transition rule as follows. [sent-108, score-0.836]
</p><p>59 Given the current position u ∈ V, ﬁrst choose a hyperedge e over all hyperedges incident with u with the probability proportional to w(e), and then choose a vertex v ∈ e uniformly at random. [sent-109, score-0.343]
</p><p>60 Let P denote the transition probability matrix of this hypergraph random walk. [sent-111, score-0.7]
</p><p>61 From Equation (4), we have π(v),  (5)  v∈V  that is, the ratio vol S/ vol V is the probability with which the random walk occupies some vertex in S. [sent-116, score-0.926]
</p><p>62 It is worth pointing out that the random walk view is consistent with that for the simple graph normalized cut [13]. [sent-119, score-0.344]
</p><p>63 The consistency means that our generalization of the normalized cut approach from simple graphs to hypergraphs is reasonable. [sent-120, score-0.477]
</p><p>64 As in [20] where the spectral clustering methodology is generalized from undirected to directed simple graphs, we may consider generalizing the present approach to directed hypergraphs [8]. [sent-131, score-0.621]
</p><p>65 A directed hypergraph is a hypergraph in which each hyperedge e is an ordered pair (X, Y ) where X ⊆ V is the tail of e and Y ⊆ V \ X is the head. [sent-132, score-1.635]
</p><p>66 Directed hypergraphs have been used to model various practical problems from biochemical networks [15] to natural language parsing [12]. [sent-133, score-0.234]
</p><p>67 6  Spectral hypergraph embedding  As in the simple graph case [4, 10], it is straightforward to extend the spectral hypergraph clustering approach to k-way partitioning. [sent-134, score-1.715]
</p><p>68 We may obtain a k-way vol ∂Vi k partition by minimizing c(V1 , · · · , Vk ) = i=1 over all k-way partitions. [sent-136, score-0.425]
</p><p>69 Similarly, vol Vi the combinatorial optimization problem can be relaxed into a real-valued one, of which the solution can be any orthogonal basis of the linear space spanned by the eigenvectors of ∆ associated with the k smallest eigenvalues. [sent-137, score-0.462]
</p><p>70 Then k T −1 ri (Dv − HW De H T )ri c(V1 , · · · , Vk ) = TD r ri v i i=1 −1/2  Deﬁne si = Dv  ri , and fi = si / si , where  ·  denotes the usual Euclidean norm. [sent-145, score-0.132]
</p><p>71 And then the row vectors of X are regarded as the representations of the graph vertices in k-dimensional Euclidian space. [sent-156, score-0.207]
</p><p>72 Those vectors corresponding to the vertices are generally expected to be well separated, and consequently we can obtain a good partition simply by running k-means on them once. [sent-157, score-0.145]
</p><p>73 [18] has resorted to a semideﬁnite relaxation model for the k-way normalized cut instead of the relatively loose spectral relaxation, and then obtained a more accurate solution. [sent-158, score-0.345]
</p><p>74 7  Transductive inference  We have established algorithms for spectral hypergraph clustering and embedding. [sent-161, score-0.882]
</p><p>75 Speciﬁcally, given a hypergraph G = (V, E, w), the vertices in a subset S ⊂ V have labels in L = {1, −1}, our task is to predict the labels of the remaining unlabeled vertices. [sent-163, score-0.815]
</p><p>76 Basically, we should try to assign the same label to all vertices contained in the same hyperedge. [sent-164, score-0.115]
</p><p>77 It is actually straightforward to derive a transductive inference approach from a clustering scheme. [sent-165, score-0.151]
</p><p>78 Since in general normalized cuts are thought to be superior to mincuts, the transductive inference approach that we used in the later experiments is built on the above spectral hypergraph clustering method. [sent-168, score-1.013]
</p><p>79 In our experiments, we constructed a hypergraph for each dataset, where attribute values were regarded as hyperedges. [sent-176, score-0.728]
</p><p>80 We also constructed a simple graph for each dataset, and the simple graph spectral clustering based approach [19] was then used as the baseline. [sent-179, score-0.366]
</p><p>81 Those simple graphs were constructed in the way discussed in the beginning of Section 1, which is essentially to deﬁne pairwise relationships among the objects by the adjacency matrices of hypergraphs. [sent-180, score-0.184]
</p><p>82 We embedded those animals into Euclidean space by using the eigenvectors of the hypergraph Laplacian associated with the smallest eigenvalues (Figure 2). [sent-185, score-0.845]
</p><p>83 15  seasnake carp dolphin  stingray dogfish seahorse  −0. [sent-197, score-0.196]
</p><p>84 1  bear  gorilla girl toad frog seal pussycat pony lion pitviper deer seasnake tortoise platypus mink stingray tuatara newt vampire squirrel slowworm dolphin seahorse sealion carp dogfish ostrich kiwi flamingo hawk penguin dove gull swan  −0. [sent-204, score-0.947]
</p><p>85 15  flea  gnat  octopus pitviper crab clam newt slowworm starfish mink frog tuatara lion scorpion lobster toad bear pussycat platypus worm deer girl penguin pony cavy gorilla tortoise squirrel kiwi flea ladybird gull vampire ostrich hawk swan gnat housefly honeybee wasp flamingo dove  −0. [sent-210, score-1.115]
</p><p>86 Left panel: the eigenvectors with the 2nd and 3rd smallest eigenvalues; right panel: the eigenvectors with the 3rd and 4th smallest eigenvalues. [sent-222, score-0.134]
</p><p>87 Note that dolphin is between class 1 (denoted by ◦) containing the animals having milk and living on land, and class 4 (denoted by ) containing the animals living in sea. [sent-223, score-0.325]
</p><p>88 (a)-(c) Results from both the hypergraph based approach and the simple graph based approach. [sent-259, score-0.792]
</p><p>89 mapped to the positions between class 1 consisting of the animals having milk and living on land, and class 4 consisting of the animals living in sea. [sent-261, score-0.274]
</p><p>90 The third task is text categorization on a modiﬁed 20-newsgroup dataset with binary occurrence values for 100 words across 16242 articles (see http://www. [sent-267, score-0.112]
</p><p>91 The articles belong to 4 diﬀerent topics corresponding to the highest level of the original 20 newsgroups, with the sizes being 4605, 3519, 2657 and 5461 respectively. [sent-271, score-0.112]
</p><p>92 The ﬁnal task is to guess the letter categories with the letter dataset, in which each instance is described by 16 primitive numerical attributes (statistical moments and edge counts). [sent-272, score-0.178]
</p><p>93 The results show that the hypergraph based method is consistently better than the baseline. [sent-278, score-0.7]
</p><p>94 It is interesting that the α inﬂuences the baseline much more than the hypergraph based approach. [sent-280, score-0.7]
</p><p>95 9  Conclusion  We generalized spectral clustering techniques to hypergraphs, and developed algorithms for hypergraph embedding and transductive inference. [sent-281, score-1.006]
</p><p>96 It might be more sensible to model them as hypergraphs instead such that complex interactions will be completely taken into account. [sent-286, score-0.234]
</p><p>97 Spectral relaxation models and structure analysis for k-way graph clustering and bi-clustering. [sent-355, score-0.207]
</p><p>98 New spectral methods for ratio cut partitioning and clustering. [sent-361, score-0.294]
</p><p>99 Propagating distributions on a hypergraph by dual information regularization. [sent-408, score-0.7]
</p><p>100 On semideﬁnite relaxation for normalized k-cut and connections to spectral clustering. [sent-418, score-0.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypergraph', 0.7), ('vol', 0.395), ('hypergraphs', 0.234), ('hyperedge', 0.178), ('cut', 0.136), ('dv', 0.126), ('vertices', 0.115), ('spectral', 0.114), ('articles', 0.112), ('graph', 0.092), ('transductive', 0.083), ('vk', 0.08), ('animals', 0.078), ('clustering', 0.068), ('vertex', 0.068), ('walk', 0.068), ('undirected', 0.064), ('graphs', 0.059), ('hyperedges', 0.059), ('joined', 0.059), ('letter', 0.058), ('relationships', 0.058), ('directed', 0.057), ('hw', 0.054), ('dolphin', 0.051), ('normalized', 0.048), ('relaxation', 0.047), ('vi', 0.046), ('ri', 0.044), ('hyperlink', 0.044), ('milk', 0.044), ('partitioning', 0.044), ('embedding', 0.041), ('incident', 0.038), ('seal', 0.038), ('objects', 0.038), ('living', 0.037), ('edge', 0.036), ('author', 0.035), ('smallest', 0.034), ('eigenvectors', 0.033), ('erent', 0.032), ('person', 0.031), ('laplacian', 0.031), ('partition', 0.03), ('pairwise', 0.029), ('carp', 0.029), ('cavy', 0.029), ('clam', 0.029), ('dogfish', 0.029), ('dove', 0.029), ('flamingo', 0.029), ('flea', 0.029), ('girl', 0.029), ('gnat', 0.029), ('gorilla', 0.029), ('gull', 0.029), ('hawk', 0.029), ('honeybee', 0.029), ('housefly', 0.029), ('kiwi', 0.029), ('ladybird', 0.029), ('lobster', 0.029), ('mink', 0.029), ('newt', 0.029), ('ostrich', 0.029), ('penguin', 0.029), ('pitviper', 0.029), ('platypus', 0.029), ('pony', 0.029), ('pussycat', 0.029), ('scorpion', 0.029), ('seahorse', 0.029), ('sealion', 0.029), ('seasnake', 0.029), ('seawasp', 0.029), ('slowworm', 0.029), ('starfish', 0.029), ('stingray', 0.029), ('subedges', 0.029), ('suggestively', 0.029), ('swan', 0.029), ('toad', 0.029), ('tortoise', 0.029), ('tuatara', 0.029), ('vampire', 0.029), ('wasp', 0.029), ('worm', 0.029), ('zoo', 0.029), ('article', 0.029), ('attribute', 0.028), ('methodology', 0.027), ('euclidean', 0.027), ('categorical', 0.027), ('attributes', 0.026), ('frog', 0.025), ('remp', 0.025), ('crab', 0.025), ('deer', 0.025), ('lion', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="123-tfidf-1" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>Author: Dengyong Zhou, Jiayuan Huang, Bernhard Schölkopf</p><p>Abstract: We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classiﬁcation on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. 1</p><p>2 0.15517956 <a title="123-tfidf-2" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>3 0.12131169 <a title="123-tfidf-3" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>4 0.097780123 <a title="123-tfidf-4" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>5 0.096414149 <a title="123-tfidf-5" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>6 0.093781255 <a title="123-tfidf-6" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>7 0.084687829 <a title="123-tfidf-7" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>8 0.078257069 <a title="123-tfidf-8" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>9 0.06933409 <a title="123-tfidf-9" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>10 0.069147885 <a title="123-tfidf-10" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>11 0.064849377 <a title="123-tfidf-11" href="./nips-2006-Balanced_Graph_Matching.html">39 nips-2006-Balanced Graph Matching</a></p>
<p>12 0.064198107 <a title="123-tfidf-12" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>13 0.059023857 <a title="123-tfidf-13" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>14 0.057624262 <a title="123-tfidf-14" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>15 0.051240742 <a title="123-tfidf-15" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>16 0.045247987 <a title="123-tfidf-16" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>17 0.043187138 <a title="123-tfidf-17" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>18 0.040864706 <a title="123-tfidf-18" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>19 0.040256336 <a title="123-tfidf-19" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>20 0.037070412 <a title="123-tfidf-20" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.076), (2, -0.008), (3, 0.163), (4, 0.081), (5, -0.055), (6, 0.011), (7, 0.121), (8, 0.053), (9, -0.111), (10, 0.094), (11, -0.074), (12, -0.055), (13, 0.034), (14, -0.051), (15, -0.011), (16, -0.029), (17, 0.048), (18, 0.001), (19, -0.124), (20, -0.076), (21, -0.046), (22, 0.009), (23, -0.025), (24, 0.091), (25, 0.013), (26, -0.076), (27, 0.076), (28, 0.106), (29, -0.034), (30, -0.01), (31, 0.024), (32, 0.015), (33, 0.005), (34, -0.007), (35, -0.038), (36, -0.055), (37, -0.01), (38, -0.007), (39, -0.091), (40, -0.012), (41, 0.013), (42, -0.07), (43, 0.066), (44, 0.008), (45, -0.009), (46, -0.119), (47, -0.041), (48, 0.055), (49, -0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94937754 <a title="123-lsi-1" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>Author: Dengyong Zhou, Jiayuan Huang, Bernhard Schölkopf</p><p>Abstract: We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classiﬁcation on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. 1</p><p>2 0.69052142 <a title="123-lsi-2" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>3 0.6227833 <a title="123-lsi-3" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>4 0.60436082 <a title="123-lsi-4" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>5 0.57969099 <a title="123-lsi-5" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>Author: Karsten M. Borgwardt, Nicol N. Schraudolph, S.v.n. Vishwanathan</p><p>Abstract: Using extensions of linear algebra concepts to Reproducing Kernel Hilbert Spaces (RKHS), we deﬁne a unifying framework for random walk kernels on graphs. Reduction to a Sylvester equation allows us to compute many of these kernels in O(n3 ) worst-case time. This includes kernels whose previous worst-case time complexity was O(n6 ), such as the geometric kernels of G¨ rtner et al. [1] and a the marginal graph kernels of Kashima et al. [2]. Our algebra in RKHS allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods, yielding sub-cubic computational complexity when combined with conjugate gradient solvers or ﬁxed-point iterations. Experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches. 1</p><p>6 0.47346318 <a title="123-lsi-6" href="./nips-2006-Balanced_Graph_Matching.html">39 nips-2006-Balanced Graph Matching</a></p>
<p>7 0.47090876 <a title="123-lsi-7" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>8 0.45376033 <a title="123-lsi-8" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>9 0.44877437 <a title="123-lsi-9" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>10 0.43617383 <a title="123-lsi-10" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>11 0.43436453 <a title="123-lsi-11" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>12 0.41924793 <a title="123-lsi-12" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>13 0.41635221 <a title="123-lsi-13" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>14 0.41479981 <a title="123-lsi-14" href="./nips-2006-High-Dimensional_Graphical_Model_Selection_Using_%24%5Cell_1%24-Regularized_Logistic_Regression.html">92 nips-2006-High-Dimensional Graphical Model Selection Using $\ell 1$-Regularized Logistic Regression</a></p>
<p>15 0.41368064 <a title="123-lsi-15" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>16 0.40303084 <a title="123-lsi-16" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>17 0.39567515 <a title="123-lsi-17" href="./nips-2006-Hyperparameter_Learning_for_Graph_Based_Semi-supervised_Learning_Algorithms.html">93 nips-2006-Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</a></p>
<p>18 0.29113212 <a title="123-lsi-18" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>19 0.27720121 <a title="123-lsi-19" href="./nips-2006-Information_Bottleneck_for_Non_Co-Occurrence_Data.html">100 nips-2006-Information Bottleneck for Non Co-Occurrence Data</a></p>
<p>20 0.2722097 <a title="123-lsi-20" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.067), (3, 0.015), (7, 0.093), (9, 0.042), (12, 0.024), (20, 0.012), (22, 0.055), (44, 0.048), (57, 0.046), (64, 0.023), (65, 0.051), (69, 0.027), (83, 0.029), (87, 0.37)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76204765 <a title="123-lda-1" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>Author: Dengyong Zhou, Jiayuan Huang, Bernhard Schölkopf</p><p>Abstract: We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classiﬁcation on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs. 1</p><p>2 0.65639722 <a title="123-lda-2" href="./nips-2006-Near-Uniform_Sampling_of_Combinatorial_Spaces_Using_XOR_Constraints.html">144 nips-2006-Near-Uniform Sampling of Combinatorial Spaces Using XOR Constraints</a></p>
<p>Author: Carla P. Gomes, Ashish Sabharwal, Bart Selman</p><p>Abstract: We propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner. We focus on problems speciﬁed as a Boolean formula, i.e., on SAT instances. Sampling for SAT problems has been shown to have interesting connections with probabilistic reasoning, making practical sampling algorithms for SAT highly desirable. The best current approaches are based on Markov Chain Monte Carlo methods, which have some practical limitations. Our approach exploits combinatorial properties of random parity (XOR) constraints to prune away solutions near-uniformly. The ﬁnal sample is identiﬁed amongst the remaining ones using a state-of-the-art SAT solver. The resulting sampling distribution is provably arbitrarily close to uniform. Our experiments show that our technique achieves a signiﬁcantly better sampling quality than the best alternative. 1</p><p>3 0.52397579 <a title="123-lda-3" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>4 0.39005142 <a title="123-lda-4" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>5 0.38146332 <a title="123-lda-5" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>6 0.3812685 <a title="123-lda-6" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>7 0.38000131 <a title="123-lda-7" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>8 0.37876552 <a title="123-lda-8" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>9 0.37754634 <a title="123-lda-9" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>10 0.37654582 <a title="123-lda-10" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>11 0.37500352 <a title="123-lda-11" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>12 0.37444526 <a title="123-lda-12" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>13 0.37376365 <a title="123-lda-13" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>14 0.37336743 <a title="123-lda-14" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>15 0.3730346 <a title="123-lda-15" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>16 0.3702496 <a title="123-lda-16" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>17 0.36975658 <a title="123-lda-17" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>18 0.36974236 <a title="123-lda-18" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>19 0.36851302 <a title="123-lda-19" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>20 0.36850747 <a title="123-lda-20" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
