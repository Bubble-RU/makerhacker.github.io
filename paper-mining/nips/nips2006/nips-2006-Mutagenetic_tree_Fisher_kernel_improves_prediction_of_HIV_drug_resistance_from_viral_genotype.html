<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-142" href="#">nips2006-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</h1>
<br/><p>Source: <a title="nips-2006-142-pdf" href="http://papers.nips.cc/paper/2978-mutagenetic-tree-fisher-kernel-improves-prediction-of-hiv-drug-resistance-from-viral-genotype.pdf">pdf</a></p><p>Author: Tobias Sing, Niko Beerenwinkel</p><p>Abstract: Starting with the work of Jaakkola and Haussler, a variety of approaches have been proposed for coupling domain-speciﬁc generative models with statistical learning methods. The link is established by a kernel function which provides a similarity measure based inherently on the underlying model. In computational biology, the full promise of this framework has rarely ever been exploited, as most kernels are derived from very generic models, such as sequence proﬁles or hidden Markov models. Here, we introduce the MTreeMix kernel, which is based on a generative model tailored to the underlying biological mechanism. Speciﬁcally, the kernel quantiﬁes the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples. We compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of HIV drug resistance from genotype, using support vector regression. The results show signiﬁcant improvements in predictive performance across 17 anti-HIV drugs. Thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making. 1</p><p>Reference: <a title="nips-2006-142-reference" href="../nips2006_reference/nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype  Tobias Sing Department of Computational Biology Max Planck Institute for Informatics Saarbr¨ cken, Germany u tobias. [sent-1, score-1.133]
</p><p>2 Speciﬁcally, the kernel quantiﬁes the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples. [sent-8, score-0.793]
</p><p>3 We compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of HIV drug resistance from genotype, using support vector regression. [sent-9, score-0.902]
</p><p>4 Thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making. [sent-11, score-0.187]
</p><p>5 Unfortunately, despite some basic results on the derivation of novel kernels from existing kernels or from more general similarity measures (e. [sent-14, score-0.102]
</p><p>6 One of the most promising developments in the recent search for a systematic kernel design methodology is the generative-discriminative paradigm [2], also known under the more general term of model-dependent feature extraction (MDFE) [3]. [sent-18, score-0.114]
</p><p>7 The central idea of MDFE is to derive kernels from generative probabilistic models of a given process or phenomenon. [sent-19, score-0.1]
</p><p>8 To date, a variety of generic MDFE procedures have been proposed, including the Fisher kernel [2] and, more generally, marginalized kernels [5], as well as the TOP [3], heat [6], and probability product kernels [7], along with a number of variations. [sent-26, score-0.194]
</p><p>9 Furthermore, most approaches are based on standard graphical models, such as amino acid sequence proﬁles or hidden Markov models, that are not adapted in any speciﬁc way to the process at hand. [sent-28, score-0.128]
</p><p>10 This paper is motivated by a regression problem from clinical bioinformatics that has recently attracted substantial attention due to its pivotal role in anti-HIV therapy: the prediction of phenotypic drug resistance from viral genotype (reviewed in [8]). [sent-31, score-1.089]
</p><p>11 Drug resistant viruses present a major cause of treatment failure and their occurrence renders many of the available drugs ineffective. [sent-32, score-0.168]
</p><p>12 Therefore, knowing the precise patterns of drug resistance is an important prerequisite for the choice of optimal drug combinations [9, 10]. [sent-33, score-1.015]
</p><p>13 Drug resistance arises as a virus population evolves under partially suppressive antiviral therapy. [sent-34, score-0.4]
</p><p>14 The extreme evolutionary dynamics of HIV quickly generate viral genetic variants that are selected for their ability to replicate in the presence of the applied drug cocktail. [sent-35, score-0.749]
</p><p>15 These advantageous mutants eventually outgrow the wild type population and lead to therapy failure. [sent-36, score-0.13]
</p><p>16 Thus, the resistance phenotype is determined by the viral genotype. [sent-37, score-0.543]
</p><p>17 The genotype-phenotype prediction problem is of considerable clinical relevance, because genotyping is much faster and cheaper, while treatment decisions are ultimately based on the viral phenotype (i. [sent-38, score-0.344]
</p><p>18 From the perspective of MDFE, the interesting feature of HIV drug resistance lies in the structure of the underlying generative process. [sent-41, score-0.694]
</p><p>19 The development of resistance involves the stochastic accumulation of mutations in the viral genome along certain mutational pathways. [sent-42, score-0.914]
</p><p>20 Here, we demonstrate how to exploit this evolutionary structure in genotype-phenotype prediction by deriving a Fisher kernel for mixtures of mutagenetic trees, a family of graphical models designed to represent such genetic accumulation processes. [sent-43, score-0.85]
</p><p>21 In the next section, we brieﬂy summarize the mutagenetic trees mixture (MTreeMix) model, originally introduced in [11]. [sent-45, score-0.578]
</p><p>22 In Section 4, the kernel is applied to the genotype-phenotype prediction problem introduced above. [sent-47, score-0.124]
</p><p>23 2  Mixture models of mutagenetic trees  Consider n genetic events {1, . [sent-49, score-0.671]
</p><p>24 , n} will denote the mutations conferring resistance to a speciﬁc anti-HIV drug. [sent-57, score-0.537]
</p><p>25 Syntactically, a mutagenetic tree for n genetic events is a connected branching T = (V, E) on the vertices V = {0, 1, . [sent-58, score-0.685]
</p><p>26 Semantically, the mutagenetic tree model induced by T and the parameter vector θ = (θ1 , . [sent-62, score-0.554]
</p><p>27 Thus, a mutagenetic tree model is the family of distributions of X = (X1 , . [sent-69, score-0.554]
</p><p>28 v=1  Here, x0 := 1 (indicating the wild type state without any resistance mutations), and pa(v) denotes the parent of vertex v in T . [sent-73, score-0.36]
</p><p>29 Figure 1 shows a mutagenetic tree for the development of resistance to the protease inhibitor nelﬁnavir. [sent-74, score-1.01]
</p><p>30 10  10FI Figure 1: Mutagenetic tree for the development of resistance to the HIV protease inhibitor nelﬁnavir (NFV). [sent-84, score-0.572]
</p><p>31 Vertices of the tree are labeled with amino acid changes in the protease enzyme. [sent-85, score-0.357]
</p><p>32 The tree represents one component of the 6-trees mixture model estimated for this evolutionary process. [sent-87, score-0.282]
</p><p>33 This restriction sets mutagenetic trees apart from standard Bayesian networks in that it allows for an evolutionary interpretation of the tree topology. [sent-89, score-0.774]
</p><p>34 In particular, the model implies the existence of certain mutational pathways with distinct probabilities. [sent-90, score-0.194]
</p><p>35 The state space then becomes the following subset of {0, 1}n , C = {x ∈ {0, 1}n | (xpa(v) , xv ) = (0, 1), for all v ∈ V }, and the factorization of the joint distribution simpliﬁes to x θv v (1 − θv )1−xv . [sent-94, score-0.197]
</p><p>36 Pr(X = x | θ) = {v|xpa(v) =1}  The mutational pathway metaphor, originating in the virological literature, is generally considered to be a reasonable approximation to HIV evolution under drug pressure. [sent-95, score-0.562]
</p><p>37 However, sets of mutational patterns that support different tree topologies are commonly seen in clinical HIV databases. [sent-96, score-0.361]
</p><p>38 Thus, in order to allow for increased ﬂexibility in modeling evolutionary pathways and to account for noise in the observed data, we consider the larger model class of mixtures of mutagenetic trees. [sent-97, score-0.617]
</p><p>39 Intuitively, these mixture models correspond to the assumption that a variety of evolutionary forces contribute additively in shaping HIV genetic variability in vivo. [sent-98, score-0.275]
</p><p>40 The mutagenetic trees mixture model is the family of distributions of X of the form K  Pr(X = x | λ, θ) =  λk Pr(X = x | θk ). [sent-113, score-0.578]
</p><p>41 k=1  The state space C of this model is the union of the state spaces of the single tree models induced by T1 , . [sent-114, score-0.142]
</p><p>42 In our applications, we will always ﬁx the ﬁrst tree to be a star, such that C = {0, 1} n (i. [sent-118, score-0.116]
</p><p>43 The star accounts for the spontaneous and independent occurrence of genetic events. [sent-121, score-0.148]
</p><p>44 3  The MTreeMix Fisher kernel  We now derive a Fisher kernel for the mutagenetic trees mixture models introduced in the previous section. [sent-122, score-0.788]
</p><p>45 The value of the kernel K(x, x ) is displayed for all possible pairs of mutational patterns (x, x ). [sent-124, score-0.307]
</p><p>46 Empty cells are indexed with genotypes that are not compatible with the tree. [sent-125, score-0.094]
</p><p>47 We ﬁrst derive the Fisher kernel for the single mutagenetic tree model. [sent-133, score-0.646]
</p><p>48 The log-likelihood of observing a mutational pattern x ∈ {0, 1}n under this model is x (θ)  =  xv log(θv ) + (1 − xv ) log(1 − θv ). [sent-134, score-0.556]
</p><p>49 {v|xpa(v) =1}  Hence, the feature mapping of binary mutational patterns into Euclidean n-space, ∂ x (θ) ∂ x (θ) φ : C → Rn , x → ,. [sent-135, score-0.188]
</p><p>50 , , x (θ) = ∂θ1 ∂θn is given by the Fisher score consisting of the partial derivatives  −1 if (xpa(w) , xw ) = (1, 1) θw , ∂ x (θ) −xw xw −1 1−xpa(w) −1 = θw (θw − 1) 0 = (θw − 1) , if (xpa(w) , xw ) = (1, 0)  ∂θw 0, if (xpa(w) , xw ) = (0, 0). [sent-138, score-0.324]
</p><p>51 Thus, we can deﬁne the mutagenetic tree Fisher kernel as n  K(x, x ) =  x (θ),  x  θ−(xv +xv ) (θv − 1)(xv +xv )−2 02−(xpa(v) +xpa(v) ) . [sent-139, score-0.646]
</p><p>52 (θ) = v=1  For example, the Fisher kernels for the three mutagenetic trees on n = 2 genetic events are displayed in Table 1. [sent-140, score-0.723]
</p><p>53 0  t  Figure 2: Non-zero entries of the matrix κ(t) that deﬁnes the mutagenetic tree Fisher kernel. [sent-147, score-0.554]
</p><p>54 The three graphs are indexed in the same way as the matrix, namely by pairs ((x pa(v) , xv ), (xpa(v) , xv )) denoting the value of two genotypes x and x at an edge (pa(v), v) of the mutagenetic tree. [sent-148, score-0.926]
</p><p>55 The graphs illustrate that the largest contributions stem from shared, unlikely mutations (positive effect, solid and dashed line) and from differing, likely or unlikely mutations (negative effect, dash-dot line). [sent-149, score-0.522]
</p><p>56 with κ deﬁned as (0, 0) κ(t) = (1, 0) (1, 1)  (0, 0) 0 0 0  (1, 0) 0 (t − 1)−2 t−1 (t − 1)−1  (1, 1) 0 t−1 (t − 1)−1 t−2  The matrix κ(t) is indexed by pairs of pairs ((xpa(v) , xv ), (xpa(v) , xv )). [sent-150, score-0.418]
</p><p>57 An edge contributes strongly to the kernel value, if the two genotypes agree on it, but the common event (occurrence or non-occurrence of the mutation) was unlikely (Figure 2, solid and dashed line). [sent-152, score-0.224]
</p><p>58 If the two genotypes disagree, the edge contributes negatively, especially for extreme parameters θv close to zero or one (Figure 2, dash-dot line), which make one of the events very likely and the other very unlikely. [sent-153, score-0.124]
</p><p>59 Thus, the application of the Fisher kernel idea to mutagenetic trees leads to a kernel that measures similarity of evolutionary escape in a way that corresponds well to virological intuition. [sent-154, score-0.898]
</p><p>60 Due to the linear mixing process, extending the Fisher kernel from a single mutagenetic tree to a mixture model is straightforward. [sent-155, score-0.689]
</p><p>61 Let x (λ, θ) = log Pr(x | λ, θ)) be the log-likelihood function, and denote by λl Pr(x | θl ) γl (x | λ, θ) = Pr(x | λ, θ) the responsibility of tree component Tl for the observation x. [sent-156, score-0.116]
</p><p>62 Then the partial derivatives with respect to θ can be expressed in terms of the partials obtained for the single tree models, weighted by the responsibilities of the trees, ∂ x (λ, θ) ∂ x (θl ) = γl (x | λ, θ) . [sent-157, score-0.116]
</p><p>63 ∂λl Pr(x | λ, θ)  We obtain the mutagenetic trees mixture (MTreeMix) Fisher kernel K(x, x ) =  x (λ, θ), K−1  = l=1  x  (λ, θ)  [Pr(x | θl ) − Pr(x | θK )] [Pr(x | θl ) − Pr(x | θK )] Pr(x | λ, θ) Pr(x | λ, θ) K  n  +  γl (x | λ, θ)γl (x | λ, θ)κ(θl,w )(xpa(w) ,xw ),(xpa(w) ,xw ) . [sent-159, score-0.67]
</p><p>64 l=1 w=1  4  Experimental results  In this section, we use the Fisher kernel derived from mutagenetic tree mixtures for predicting HIV drug resistance from viral genotype. [sent-160, score-1.503]
</p><p>65 Brieﬂy, resistance is the ability of a virus to replicate in the presence of drug. [sent-161, score-0.364]
</p><p>66 The degree of resistance is usually communicated as a non-negative number. [sent-162, score-0.311]
</p><p>67 This number indicates the fold-change increase in drug concentration that is necessary to inhibit viral replication by 50%, as compared to a fully susceptible reference virus. [sent-163, score-0.522]
</p><p>68 Information on phenotypic resistance strongly affects treatment decisions, but the experimental procedures are too expensive and time-consuming for routine clinical diagnostics. [sent-166, score-0.456]
</p><p>69 Instead, at the time of therapy failure, the genotypic makeup of the viral population is determined using standard sequencing methods, leaving the challenge of inferring the phenotypic implications from the observed genotypic alterations. [sent-167, score-0.474]
</p><p>70 It is also desirable to minimize the number of sequence positions required for reliable determination of drug resistance. [sent-168, score-0.339]
</p><p>71 With a small number of positions, sequencing could be replaced by the much cheaper line-probe assay (LiPA) technology [12], which focuses on the determination of mutations at a limited number of pre-selected sites. [sent-169, score-0.373]
</p><p>72 This method could bring resistance testing to resource-poor settings in which DNA sequencing is not affordable. [sent-170, score-0.392]
</p><p>73 Application of the Fisher kernel to this task is motivated by the hypothesis that the traces of evolution present in the data and modelled by mutagenetic trees mixture models can provide additional information, leading to improved predictive performance. [sent-172, score-0.725]
</p><p>74 Based on a list of resistance mutations maintained by the International AIDS Society [15], we extract the residues listed in column 2. [sent-177, score-0.558]
</p><p>75 The number indicates the position in the viral enzyme (reverse transcriptase for the ﬁrst two groups of drugs, and protease for the third group), and the amino acids following the number denote the mutations at the respective site that are considered resistance-associated. [sent-178, score-0.687]
</p><p>76 For example, the feature vector for the drug zidovudine (ZDV) consists of six variables representing the reverse transcriptase mutations 41L, 67N, 70R, 210W, 215F or Y, and 219E or Q. [sent-179, score-0.714]
</p><p>77 In the naive indicator representation, a mutational pattern within these six mutations is transformed to a binary vector of length six, each entry encoding the presence or absence of the respective mutation. [sent-180, score-0.414]
</p><p>78 The Fisher kernel requires a mutagenetic trees mixture model for each of the evaluated drugs. [sent-181, score-0.67]
</p><p>79 Using the MTreeMix software package1 , these models were estimated from an independent set of sequences derived from patients failing a therapy that contained the speciﬁc drug of interest. [sent-182, score-0.421]
</p><p>80 In 100 replicates of ten-fold cross-validation for each drug model, we then recorded the squared correlation coefﬁcient (r 2 ) of indicator variable-based versus Fisher kernel-based support vector regression. [sent-183, score-0.42]
</p><p>81 For the indicator representation, the linear kernel performed best, whereas the Fisher scores performed best when combined with a Gaussian RBF kernel. [sent-187, score-0.118]
</p><p>82 de  The results displayed in columns 5 and 6 of Table 2 show the improvements attained via the Fisher kernel method as estimated by the squared correlation coefﬁcient, r 2 . [sent-193, score-0.174]
</p><p>83 The most drastic improvements were obtained for the drugs 3TC, NVP and NFV. [sent-197, score-0.107]
</p><p>84 Table 2: Comparison of support vector regression performance for the MTreeMix Fisher kernel (F ) versus a naive amino acid indicator (I) representation. [sent-201, score-0.246]
</p><p>85 The drugs (ﬁrst column) are grouped into the three classes of nucleoside/nucleotide reverse transcriptase inhibitors (rows 1–7), nonnucleoside reverse transcriptase inhibitors (rows 8–10), and protease inhibitors (rows 11–17). [sent-202, score-0.639]
</p><p>86 MTreeMix models were estimated based on the mutations listed in the second column. [sent-203, score-0.252]
</p><p>87 The third column indicates the number N of available genotype-phenotype pairs, and the number K of trees in the mixture model is shown in column 4. [sent-204, score-0.182]
</p><p>88 4  Conclusions  The Fisher kernel derived in this paper allows for leveraging stochastic models of HIV evolution in many kernel-based scenarios. [sent-258, score-0.147]
</p><p>89 To our knowledge, this is the ﬁrst study in which a probabilistic model tailored to a speciﬁc biological mechanism (namely, the evolution of drug resistance) is exploited in a discriminative context. [sent-259, score-0.389]
</p><p>90 Using the example of inferring drug resistance from viral genotype, we showed that signiﬁcant improvements in predictive performance can be obtained for almost all currently available antiretroviral drugs. [sent-260, score-0.891]
</p><p>91 These results provide strong incentive for further exploitation of evolutionary models in clinical decision making. [sent-261, score-0.206]
</p><p>92 The high correlation that can be observed with a relatively small number of mutations was unexpected and suggests that reliable resistance predictions can also be obtained on the basis of LiPA assays which are much cheaper than standard sequencing technologies. [sent-263, score-0.709]
</p><p>93 While our choice of mutations was based on a selection from the literature, an interesting problem would be to design dedicated LiPA assays containing a set of mutations that allow for optimal prediction performance in this generative-discriminative setting. [sent-264, score-0.512]
</p><p>94 Finally, mixtures of mutagenetic trees have already been applied  in other contexts, for example to model progressive chromosomal alterations in cancer [16], and we expect kernel methods to play an important role in this context, too. [sent-265, score-0.651]
</p><p>95 Computational methods for the design of effective therapies against drug a resistant HIV strains. [sent-352, score-0.362]
</p><p>96 Comparison of the LiPA HIV-1 RT test, selective PCR and direct solid phase sequencing for the detection of HIV-1 drug resistance mutations. [sent-391, score-0.731]
</p><p>97 Accurate prediction of HIV-1 drug response from the reverse transcriptase and protease amino acid sequences using sparse models created by convex optimization. [sent-402, score-0.787]
</p><p>98 Rapid, phenotypic HIV-1 drug sensitivity assay for protease and reverse transcriptase inhibitors. [sent-412, score-0.698]
</p><p>99 Update of the drug resistance mutations in HIV-1: Fall 2005. [sent-431, score-0.876]
</p><p>100 Estimating cancer survival and clinical outcome based on genetic tumor progression scores. [sent-442, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mutagenetic', 0.438), ('drug', 0.339), ('resistance', 0.311), ('xpa', 0.276), ('mutations', 0.226), ('hiv', 0.211), ('fisher', 0.207), ('xv', 0.197), ('viral', 0.183), ('mutational', 0.162), ('evolutionary', 0.123), ('tree', 0.116), ('mtreemix', 0.113), ('protease', 0.113), ('pr', 0.108), ('mdfe', 0.097), ('transcriptase', 0.097), ('trees', 0.097), ('kernel', 0.092), ('genetic', 0.083), ('drugs', 0.081), ('sequencing', 0.081), ('xw', 0.081), ('genotypes', 0.07), ('amino', 0.068), ('beerenwinkel', 0.065), ('lipa', 0.065), ('phenotypic', 0.065), ('acid', 0.06), ('genotype', 0.06), ('clinical', 0.057), ('therapy', 0.056), ('reverse', 0.052), ('kernels', 0.051), ('inhibitors', 0.049), ('phenotype', 0.049), ('wild', 0.049), ('jaakkola', 0.046), ('mixture', 0.043), ('bioinformatics', 0.042), ('occurrence', 0.041), ('haussler', 0.039), ('mutation', 0.039), ('unlikely', 0.035), ('cheaper', 0.034), ('tsuda', 0.034), ('antiretroviral', 0.032), ('antiviral', 0.032), ('assay', 0.032), ('clotet', 0.032), ('ddc', 0.032), ('genotypic', 0.032), ('inhibitor', 0.032), ('korn', 0.032), ('nel', 0.032), ('nfv', 0.032), ('nvp', 0.032), ('rahnenfuhrer', 0.032), ('selbig', 0.032), ('virological', 0.032), ('virus', 0.032), ('zdv', 0.032), ('accumulation', 0.032), ('pathways', 0.032), ('prediction', 0.032), ('evolution', 0.029), ('correlation', 0.029), ('lengauer', 0.028), ('assays', 0.028), ('hoffmann', 0.028), ('mechanistic', 0.028), ('sing', 0.028), ('walter', 0.028), ('tk', 0.028), ('events', 0.027), ('displayed', 0.027), ('contributes', 0.027), ('patterns', 0.026), ('improvements', 0.026), ('models', 0.026), ('replicates', 0.026), ('wilcoxon', 0.026), ('kaiser', 0.026), ('indicator', 0.026), ('population', 0.025), ('indexed', 0.024), ('star', 0.024), ('escape', 0.024), ('mixtures', 0.024), ('generative', 0.023), ('treatment', 0.023), ('resistant', 0.023), ('paradigm', 0.022), ('replicate', 0.021), ('tailored', 0.021), ('amari', 0.021), ('mar', 0.021), ('column', 0.021), ('perspective', 0.021), ('vertices', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="142-tfidf-1" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>Author: Tobias Sing, Niko Beerenwinkel</p><p>Abstract: Starting with the work of Jaakkola and Haussler, a variety of approaches have been proposed for coupling domain-speciﬁc generative models with statistical learning methods. The link is established by a kernel function which provides a similarity measure based inherently on the underlying model. In computational biology, the full promise of this framework has rarely ever been exploited, as most kernels are derived from very generic models, such as sequence proﬁles or hidden Markov models. Here, we introduce the MTreeMix kernel, which is based on a generative model tailored to the underlying biological mechanism. Speciﬁcally, the kernel quantiﬁes the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples. We compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of HIV drug resistance from genotype, using support vector regression. The results show signiﬁcant improvements in predictive performance across 17 anti-HIV drugs. Thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making. 1</p><p>2 0.099834763 <a title="142-tfidf-2" href="./nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</a></p>
<p>Author: Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to ﬁrst recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.</p><p>3 0.097213008 <a title="142-tfidf-3" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>4 0.081261784 <a title="142-tfidf-4" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>Author: Frank Moosmann, Bill Triggs, Frederic Jurie</p><p>Abstract: Some of the most effective recent methods for content-based image classiﬁcation work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting “visual word” codes over the image, and classifying these with a conventional classiﬁer such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests – ensembles of randomly created clustering trees – and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classiﬁcation tasks. 1</p><p>5 0.071910925 <a title="142-tfidf-5" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>Author: Konrad Rieck, Pavel Laskov, Sören Sonnenburg</p><p>Abstract: We propose a generic algorithm for computation of similarity measures for sequential data. The algorithm uses generalized sufﬁx trees for efﬁcient calculation of various kernel, distance and non-metric similarity functions. Its worst-case run-time is linear in the length of sequences and independent of the underlying embedding language, which can cover words, k-grams or all contained subsequences. Experiments with network intrusion detection, DNA analysis and text processing applications demonstrate the utility of distances and similarity coefﬁcients for sequences as alternatives to classical kernel functions.</p><p>6 0.063419752 <a title="142-tfidf-6" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>7 0.058626991 <a title="142-tfidf-7" href="./nips-2006-Scalable_Discriminative_Learning_for_Natural_Language_Parsing_and_Translation.html">172 nips-2006-Scalable Discriminative Learning for Natural Language Parsing and Translation</a></p>
<p>8 0.058619693 <a title="142-tfidf-8" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>9 0.050907623 <a title="142-tfidf-9" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>10 0.049815562 <a title="142-tfidf-10" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>11 0.048641335 <a title="142-tfidf-11" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<p>12 0.047442444 <a title="142-tfidf-12" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>13 0.044536863 <a title="142-tfidf-13" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>14 0.043402616 <a title="142-tfidf-14" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>15 0.04257641 <a title="142-tfidf-15" href="./nips-2006-Comparative_Gene_Prediction_using_Conditional_Random_Fields.html">54 nips-2006-Comparative Gene Prediction using Conditional Random Fields</a></p>
<p>16 0.041882705 <a title="142-tfidf-16" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>17 0.039378133 <a title="142-tfidf-17" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>18 0.038132429 <a title="142-tfidf-18" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>19 0.03742585 <a title="142-tfidf-19" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>20 0.035411287 <a title="142-tfidf-20" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, 0.014), (2, -0.001), (3, -0.024), (4, 0.042), (5, 0.025), (6, 0.01), (7, 0.011), (8, 0.065), (9, -0.087), (10, -0.089), (11, -0.014), (12, 0.039), (13, -0.096), (14, -0.027), (15, -0.055), (16, -0.027), (17, 0.053), (18, -0.066), (19, 0.05), (20, -0.117), (21, -0.082), (22, -0.138), (23, -0.104), (24, 0.08), (25, -0.088), (26, 0.056), (27, 0.038), (28, -0.033), (29, 0.076), (30, 0.029), (31, -0.014), (32, 0.074), (33, 0.014), (34, 0.009), (35, -0.024), (36, 0.006), (37, 0.108), (38, 0.043), (39, -0.047), (40, -0.082), (41, -0.111), (42, -0.09), (43, 0.076), (44, 0.031), (45, -0.066), (46, -0.062), (47, -0.057), (48, 0.183), (49, 0.165)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92336857 <a title="142-lsi-1" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>Author: Tobias Sing, Niko Beerenwinkel</p><p>Abstract: Starting with the work of Jaakkola and Haussler, a variety of approaches have been proposed for coupling domain-speciﬁc generative models with statistical learning methods. The link is established by a kernel function which provides a similarity measure based inherently on the underlying model. In computational biology, the full promise of this framework has rarely ever been exploited, as most kernels are derived from very generic models, such as sequence proﬁles or hidden Markov models. Here, we introduce the MTreeMix kernel, which is based on a generative model tailored to the underlying biological mechanism. Speciﬁcally, the kernel quantiﬁes the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples. We compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of HIV drug resistance from genotype, using support vector regression. The results show signiﬁcant improvements in predictive performance across 17 anti-HIV drugs. Thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making. 1</p><p>2 0.53075182 <a title="142-lsi-2" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>Author: Karsten M. Borgwardt, Nicol N. Schraudolph, S.v.n. Vishwanathan</p><p>Abstract: Using extensions of linear algebra concepts to Reproducing Kernel Hilbert Spaces (RKHS), we deﬁne a unifying framework for random walk kernels on graphs. Reduction to a Sylvester equation allows us to compute many of these kernels in O(n3 ) worst-case time. This includes kernels whose previous worst-case time complexity was O(n6 ), such as the geometric kernels of G¨ rtner et al. [1] and a the marginal graph kernels of Kashima et al. [2]. Our algebra in RKHS allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods, yielding sub-cubic computational complexity when combined with conjugate gradient solvers or ﬁxed-point iterations. Experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches. 1</p><p>3 0.49348724 <a title="142-lsi-3" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>Author: Konrad Rieck, Pavel Laskov, Sören Sonnenburg</p><p>Abstract: We propose a generic algorithm for computation of similarity measures for sequential data. The algorithm uses generalized sufﬁx trees for efﬁcient calculation of various kernel, distance and non-metric similarity functions. Its worst-case run-time is linear in the length of sequences and independent of the underlying embedding language, which can cover words, k-grams or all contained subsequences. Experiments with network intrusion detection, DNA analysis and text processing applications demonstrate the utility of distances and similarity coefﬁcients for sequences as alternatives to classical kernel functions.</p><p>4 0.48354912 <a title="142-lsi-4" href="./nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</a></p>
<p>Author: Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to ﬁrst recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.</p><p>5 0.43961495 <a title="142-lsi-5" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<p>Author: Hugh A. Chipman, Edward I. George, Robert E. Mcculloch</p><p>Abstract: We develop a Bayesian “sum-of-trees” model, named BART, where each tree is constrained by a prior to be a weak learner. Fitting and inference are accomplished via an iterative backﬁtting MCMC algorithm. This model is motivated by ensemble methods in general, and boosting algorithms in particular. Like boosting, each weak learner (i.e., each weak tree) contributes a small amount to the overall model. However, our procedure is deﬁned by a statistical model: a prior and a likelihood, while boosting is deﬁned by an algorithm. This model-based approach enables a full and accurate assessment of uncertainty in model predictions, while remaining highly competitive in terms of predictive accuracy. 1</p><p>6 0.38356832 <a title="142-lsi-6" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>7 0.38269258 <a title="142-lsi-7" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>8 0.3820225 <a title="142-lsi-8" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>9 0.38185409 <a title="142-lsi-9" href="./nips-2006-Hidden_Markov_Dirichlet_Process%3A_Modeling_Genetic_Recombination_in_Open_Ancestral_Space.html">90 nips-2006-Hidden Markov Dirichlet Process: Modeling Genetic Recombination in Open Ancestral Space</a></p>
<p>10 0.3761777 <a title="142-lsi-10" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>11 0.36757219 <a title="142-lsi-11" href="./nips-2006-Scalable_Discriminative_Learning_for_Natural_Language_Parsing_and_Translation.html">172 nips-2006-Scalable Discriminative Learning for Natural Language Parsing and Translation</a></p>
<p>12 0.36347437 <a title="142-lsi-12" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>13 0.33271867 <a title="142-lsi-13" href="./nips-2006-Adaptor_Grammars%3A_A_Framework_for_Specifying_Compositional_Nonparametric_Bayesian_Models.html">23 nips-2006-Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models</a></p>
<p>14 0.32816175 <a title="142-lsi-14" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>15 0.32159829 <a title="142-lsi-15" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>16 0.31734622 <a title="142-lsi-16" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>17 0.31724635 <a title="142-lsi-17" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>18 0.30259073 <a title="142-lsi-18" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>19 0.29132274 <a title="142-lsi-19" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>20 0.28201613 <a title="142-lsi-20" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.596), (3, 0.014), (7, 0.039), (9, 0.031), (20, 0.012), (22, 0.053), (44, 0.031), (57, 0.049), (65, 0.035), (69, 0.012), (71, 0.013), (90, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99330837 <a title="142-lda-1" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: The standard Support Vector Machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to deﬁne the generated classiﬁer. We present a modiﬁed version of SVM that allows the user to set a budget parameter B and focuses on minimizing the loss attained by the B worst-classiﬁed examples while ignoring the remaining examples. This idea can be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we obtain these new SVM variants by replacing the 1-norm in the standard SVM formulation with various interpolation-norms. We also adapt the SMO optimization algorithm to our setting and report on some preliminary experimental results. 1</p><p>2 0.98771596 <a title="142-lda-2" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>3 0.97777236 <a title="142-lda-3" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose a highly efﬁcient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work. 1</p><p>same-paper 4 0.9740563 <a title="142-lda-4" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>Author: Tobias Sing, Niko Beerenwinkel</p><p>Abstract: Starting with the work of Jaakkola and Haussler, a variety of approaches have been proposed for coupling domain-speciﬁc generative models with statistical learning methods. The link is established by a kernel function which provides a similarity measure based inherently on the underlying model. In computational biology, the full promise of this framework has rarely ever been exploited, as most kernels are derived from very generic models, such as sequence proﬁles or hidden Markov models. Here, we introduce the MTreeMix kernel, which is based on a generative model tailored to the underlying biological mechanism. Speciﬁcally, the kernel quantiﬁes the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples. We compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of HIV drug resistance from genotype, using support vector regression. The results show signiﬁcant improvements in predictive performance across 17 anti-HIV drugs. Thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making. 1</p><p>5 0.95439905 <a title="142-lda-5" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>Author: Shai Avidan, Moshe Butman</p><p>Abstract: Bob offers a face-detection web service where clients can submit their images for analysis. Alice would very much like to use the service, but is reluctant to reveal the content of her images to Bob. Bob, for his part, is reluctant to release his face detector, as he spent a lot of time, energy and money constructing it. Secure MultiParty computations use cryptographic tools to solve this problem without leaking any information. Unfortunately, these methods are slow to compute and we introduce a couple of machine learning techniques that allow the parties to solve the problem while leaking a controlled amount of information. The ﬁrst method is an information-bottleneck variant of AdaBoost that lets Bob ﬁnd a subset of features that are enough for classifying an image patch, but not enough to actually reconstruct it. The second machine learning technique is active learning that allows Alice to construct an online classiﬁer, based on a small number of calls to Bob’s face detector. She can then use her online classiﬁer as a fast rejector before using a cryptographically secure classiﬁer on the remaining image patches. 1</p><p>6 0.82976669 <a title="142-lda-6" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>7 0.82965773 <a title="142-lda-7" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>8 0.82196736 <a title="142-lda-8" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>9 0.81811589 <a title="142-lda-9" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>10 0.78934467 <a title="142-lda-10" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>11 0.77402717 <a title="142-lda-11" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>12 0.7736882 <a title="142-lda-12" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>13 0.77362543 <a title="142-lda-13" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>14 0.76762432 <a title="142-lda-14" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>15 0.75739294 <a title="142-lda-15" href="./nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations.html">111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</a></p>
<p>16 0.75544554 <a title="142-lda-16" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>17 0.7547437 <a title="142-lda-17" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>18 0.74865514 <a title="142-lda-18" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>19 0.74841726 <a title="142-lda-19" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>20 0.74376416 <a title="142-lda-20" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
