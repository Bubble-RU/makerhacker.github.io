<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-94" href="#">nips2006-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</h1>
<br/><p>Source: <a title="nips-2006-94-pdf" href="http://papers.nips.cc/paper/3126-image-retrieval-and-classification-using-local-distance-functions.pdf">pdf</a></p><p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>Reference: <a title="nips-2006-94-reference" href="../nips2006_reference/nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. [sent-8, score-0.796]
</p><p>2 We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. [sent-9, score-0.415]
</p><p>3 3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. [sent-11, score-0.571]
</p><p>4 1  Introduction  Visual categorization is a difﬁcult task in large part due to the large variation seen between images belonging to the same class. [sent-12, score-0.248]
</p><p>5 One of the more successful tools used in visual classiﬁcation is a class of patch-based shape and texture features that are invariant or robust to changes in scale, translation, and afﬁne deformations. [sent-19, score-0.365]
</p><p>6 These include the Gaussian-derivative jet descriptors of [2], SIFT descriptors [3], shape contexts [4], and geometric blur [5]. [sent-20, score-0.599]
</p><p>7 Then, (5) use distances between pairs of images as input to a learning algorithm, for example an SVM or nearest neighbor classiﬁer. [sent-24, score-0.372]
</p><p>8 When given a test image, patches and features are extracted, distances between the test image and training images are computed, and a classiﬁcation is made. [sent-25, score-0.828]
</p><p>9 The image on the left is a clear, color image of a cougar face. [sent-27, score-0.588]
</p><p>10 As with most cougar face exemplars, the locations and appearances of the eyes and ears are a strong signal for class membership, as well as the color pattern of the face. [sent-28, score-0.298]
</p><p>11 The image on the right shows the ears, eyes, and mouth, but due to articulation, the appearance of all have changed again, perhaps representing a common visual subcategory. [sent-31, score-0.293]
</p><p>12 In most approaches, machine learning only comes to play in step (5), after the distances or similarities between training images are computed. [sent-33, score-0.405]
</p><p>13 First, they would require representing each image as a ﬁxed-length feature vector. [sent-37, score-0.247]
</p><p>14 The goal of this paper is to demonstrate that in the setting of visual categorization, it can be useful to determine the relative importance of visual features on a ﬁner scale. [sent-41, score-0.284]
</p><p>15 In this work, we attack the problem from the other extreme, choosing to learn a distance function for each exemplar, where each function gives a distance value between its training image, or focal image, and any other image. [sent-42, score-0.911]
</p><p>16 These functions can be learned from either multi-way class labels or relative similarity information in the training data. [sent-43, score-0.266]
</p><p>17 The distance functions are built on top of elementary distance measures between patch-based features, and our problem is formulated such that we are learning a weighting over the features in each of our training images. [sent-44, score-0.668]
</p><p>18 Using these local distance functions, we address applications in image browsing, retrieval and classiﬁcation. [sent-47, score-0.389]
</p><p>19 In order to perform retrieval and classiﬁcation, we use an additional learning step that allows us to compare focal images to one another, and an inference procedure based on error-correcting output codes to make a class choice. [sent-48, score-0.853]
</p><p>20 3% using only ﬁfteen exemplar images per category, which is an improvement over the best previously published recognition rate in [11]. [sent-51, score-0.503]
</p><p>21 2  Distance Functions and Learning Procedure  In this section we will describe the distance functions and the learning procedure in terms of abstract patch-based image features. [sent-52, score-0.352]
</p><p>22 Any patch-based features could be used with the framework we present, and we will wait to address our choice of features in Section 3. [sent-53, score-0.295]
</p><p>23 The training image for which a given learning problem is being solved will be referred to as its focal image. [sent-55, score-0.836]
</p><p>24 In the rest of this section we will discuss one such learning problem and focal image, but keep in mind that in the full framework there are N of these. [sent-57, score-0.535]
</p><p>25 We deﬁne the distance function we are learning to be a combination of elementary patch-based distances, each of which are computed between a single patch-based feature in the focal image F and a set of features in a candidate image I, essentially giving us a patch-to-image distance. [sent-58, score-1.435]
</p><p>26 Any function between a patch feature and a set of features could be used to compute these elementary distances; we will discuss our choice in Section 3. [sent-59, score-0.47]
</p><p>27 If there are M patches in the focal image, we have M patch-to-image distances to compute between F and I, and we notate each distance in that set as dF (I), where j ∈ [1, M ], and refer to the vector of these as dF (I). [sent-60, score-0.781]
</p><p>28 The image-to-image j distance function D that we learn is a linear combination of these elementary distances. [sent-61, score-0.348]
</p><p>29 Where wF is a vector of weights with a weight corresponding to each patch feature: M F wj dF (I) = wF · dF (I) j  D(F, I) =  (1)  j=1  Our goal is to learn this weighting over the features in the focal image. [sent-62, score-0.809]
</p><p>30 We set up our algorithm to learn from “triplets” of images, each composed of (1) the focal image F, (2) an image labeled “less similar” to F, and (3) an image labeled “more similar” to F. [sent-63, score-1.182]
</p><p>31 This formulation has been used in other work for its ﬂexibility [7]; it makes it possible to use a relative ranking over images as training input, but also works naturally with multi-class labels by considering exemplars of the same class as F to be “more similar” than those of another class. [sent-64, score-0.56]
</p><p>32 If we could use our learned distance function for F to rank these two images relative to one another, we ideally would want I d to have a larger value than I s , i. [sent-66, score-0.417]
</p><p>33 Let xi = dF (I d ) − dF (I s ), the difference of the two elementary distance vectors for this triplet, now indexed by i. [sent-70, score-0.287]
</p><p>34 For a given focal image, we will construct T of these triplets from our training data (we will discuss how we choose triplets in Section 5. [sent-72, score-1.086]
</p><p>35 First, their triplets do not share the same focal image as they apply their method to learning one metric for all classes and instances. [sent-84, score-1.038]
</p><p>36 This would 2 appear to preclude our use of patch features and more interesting distance measures, but as we show, this is an unnecessary restriction for the optimization. [sent-86, score-0.361]
</p><p>37 3  Visual Features and Elementary Distances  The framework described above allows us to naturally combine different kinds of patch-based features, and we will make use of shape features at two different scales and a rudimentary color feature. [sent-95, score-0.38]
</p><p>38 Many papers have shown the beneﬁts of using ﬁlter-based patch features such as SIFT [3] and geometric blur [13] for shape- or texture-based object matching and recognition [14][15][13]. [sent-96, score-0.786]
</p><p>39 We chose to use geometric blur descriptors, which were used by Zhang et al. [sent-97, score-0.323]
</p><p>40 in [11] in combination with their KNN-SVM method to give the best previously published results on the Caltech 101 image recognition benchmark. [sent-98, score-0.415]
</p><p>41 Like SIFT, geometric blur features summarize oriented edges within a patch of the image, but are designed to be more robust to afﬁne transformation and differences in the periphery of the patch. [sent-99, score-0.562]
</p><p>42 In previous work using geometric blur descriptors on the Caltech 101 dataset [13][11], the patches used are centered at 400 or fewer edge points sampled from the image, and features are computed on patches of a ﬁxed scale and orientation. [sent-100, score-0.639]
</p><p>43 We use two different scales of geometric blur features, the same used in separate experiments in [11]. [sent-102, score-0.323]
</p><p>44 The larger has a patch radius of 70 pixels, and the smaller a patch radius of 42 pixels. [sent-103, score-0.262]
</p><p>45 Our color features are histograms of eight-pixel radius patches also centered at edge pixels in the image. [sent-106, score-0.362]
</p><p>46 Any “pixels” in a patch off the edge of the image are counted in a “undeﬁned” bin, and we convert the HSV coordinates of the remaining points to a Cartesian space where the z direction is value and (x, y) is the Cartesian projection of the hue/saturation dimensions. [sent-107, score-0.332]
</p><p>47 These were the only parameters that we tested with the color features, choosing not to tune the features to the Caltech 101 dataset. [sent-109, score-0.245]
</p><p>48 If we are computing the distance between the jth patch in the focal image to a candidate image I, we ﬁnd the closest feature of the same type in I using the L2 distance, and use that L2 distance as the jth elementary patch-toimage distance. [sent-112, score-1.566]
</p><p>49 We only compare features of the same type, so large geometric blur features are not compared to small geometric blur features. [sent-113, score-0.918]
</p><p>50 4  Image Browsing, Retrieval, and Classiﬁcation  The learned distance functions induce rankings that could naturally be the basis for a browsing application over a closed set of images. [sent-115, score-0.333]
</p><p>51 Consider a ranking of images with respect to one focal image, as in Figure 2. [sent-116, score-0.797]
</p><p>52 Clicking on the sixth image shown would then take them to the ranking with that sunﬂower image as the focal image, which contains more sunﬂower results. [sent-118, score-0.984]
</p><p>53 We also can make use of these distance functions to perform image retrieval: given a new image Q, return a listing of the N training images (or the top K) in order of similarity to Q. [sent-124, score-0.907]
</p><p>54 If given class labels, we would want images ranked high to be in the same class as Q. [sent-125, score-0.289]
</p><p>55 While we can use the N distance functions to compute the distance from each of the focal images Fi to Q, these distances are not directly comparable. [sent-126, score-1.113]
</p><p>56 To address this in cases where we have multi-class labels, we do a second round of training for each focal image where we ﬁt a logistic classiﬁer to the binary (in-class versus out-of-class) training labels and learned distances. [sent-128, score-1.012]
</p><p>57 Now, given a query image Q, we can compute a probability that the query is in the same class as each of the focal (training) images, and we can use these probabilities to rank the training images relative to one another. [sent-129, score-1.244]
</p><p>58 The probabilities are on the same scale, and the logistic also helps to penalize poor focal rankings. [sent-130, score-0.589]
</p><p>59 For each class, we sum the probabilities for all training images from that class, and the query is assigned to the class with the largest total. [sent-132, score-0.426]
</p><p>60 Formally, if pj is the probability for the jth training image Ij , and C is the set of classes, the chosen class is arg maxC j:Ij ∈C pj . [sent-133, score-0.369]
</p><p>61 This can be shown to be a relaxation of the Hamming decoding scheme for the error-correcting output codes in [17] in which the number of focal images is the same for each class. [sent-134, score-0.756]
</p><p>62 This dataset has artifacts that make a few classes easy, but many are quite difﬁcult, and due to the important challenges it poses for scalable object recognition, it has up to this point been one of the de facto standard benchmarks for multi-class image categorization/object recognition. [sent-136, score-0.364]
</p><p>63 The dataset contains images from 101 different categories, with the number of images per category ranging from 31 to 800, with a median of about 50 images. [sent-137, score-0.59]
</p><p>64 We ignore the background class and work in a forced-choice scenario with the 101 object categories, where a query image must be assigned to one of the 101 categories. [sent-138, score-0.362]
</p><p>65 [15]: we use varying numbers of training set sizes (given in number of examples per class), and in each training scenario, test with all other images in the Caltech101 dataset, except the BACKGROUND Google class. [sent-140, score-0.482]
</p><p>66 This normalizes the overall recognition rate so that the performance for categories with a larger number of test images does not skew the mean recognition rate. [sent-142, score-0.576]
</p><p>67 1  Training data  The images are ﬁrst resized to speed feature computation. [sent-144, score-0.264]
</p><p>68 We computed features for each of these images as described in Section 3. [sent-146, score-0.357]
</p><p>69 We used up to 400 of each type of feature (two sizes of geometric blur and one color), for a maximum total of 1,200 features per image. [sent-147, score-0.546]
</p><p>70 For images with few edge points, we computed fewer features so that the features were not overly redundant. [sent-148, score-0.518]
</p><p>71 After computing elementary distances, we rescale the distances for each focal image and feature to have a standard deviation of 0. [sent-149, score-1.034]
</p><p>72 Note that the training algorithm allows for a more nuanced training set where an image could be more similar with respect to one image and less similar with respect to another, but 3  You can also see retrieval rankings with probabilities at the web page. [sent-152, score-0.764]
</p><p>73 We experimented with abandoning the max-margin optimization and just training a logistic for each focal image; the results were far worse, perhaps because the logistic was ﬁtting noise in the tails. [sent-153, score-0.694]
</p><p>74 28  Figure 2: The ﬁrst 15 images from a ranking induced for the focal image in the upper-left corner, trained with 15 images/category. [sent-173, score-1.001]
</p><p>75 Each image is shown with its raw distance distance, and only those marked with (pos) or (neg) were in the learning set for this focal image. [sent-174, score-0.861]
</p><p>76 Instead of using the full pairwise combination of all in- and out-of-class images, we select triplets using elementary feature distances. [sent-180, score-0.461]
</p><p>77 Thus, we refer to all the images available for training as the training set and the set of images used to train with respect to a given focal image as its learning set. [sent-181, score-1.375]
</p><p>78 We want in our learning set those images that are similar to the focal image according to at least one elementary distance measure. [sent-182, score-1.247]
</p><p>79 For each of the M elementary patch distance measures, we ﬁnd the top K closest images. [sent-183, score-0.39]
</p><p>80 If all K images are in-class, then we ﬁnd the closest out-of-class image according to that distance measure and make K triplets with one out-of-class image and the K similar images. [sent-185, score-0.978]
</p><p>81 The ﬁnal set of triplets for F is the union of the triplets chosen by the M measures. [sent-188, score-0.454]
</p><p>82 On average, we used 2,210 triplets per focal image, and mean training time was 1-2 seconds (not including the time to compute the features, elementary distances, or choose the triplets). [sent-189, score-1.068]
</p><p>83 2  Results  We ran a series of experiments using all features, each with a different number of training images per category (either 5, 15, or 30), where we generated 10 independent random splits of the 8,677 images from the 101 categories into training and test sets. [sent-192, score-0.872]
</p><p>84 We determined the C parameter of the training algorithm using leave-one-out cross-validation on a small random subset of 15 images per category, and our ﬁnal results are reported using the best value of C found (0. [sent-194, score-0.362]
</p><p>85 In the 15 training images per category setting, we also performed recognition experiments on each of our features separately, the combination of the two shape features, and the combination of two shape features with the color features, for a total of ﬁve different feature combinations. [sent-198, score-1.255]
</p><p>86 8% standard deviation)7 The next best performance was from the bigger geometric blur features with 49. [sent-201, score-0.459]
</p><p>87 9%), followed by the smaller geometric blur features with 52. [sent-203, score-0.459]
</p><p>88 Combining the two shape features together, we achieved 58. [sent-206, score-0.246]
</p><p>89 7%), which 6  For big geometric blur, small geometric blur, both together, and color alone, the values were C=5, 1, 0. [sent-210, score-0.301]
</p><p>90 Figure 3: Number of training exemplars versus average recognition rate across classes (based on the graph in [11]). [sent-214, score-0.33]
</p><p>91 is better than the best previously published performance for 15 training images on the Caltech 101 dataset [11]. [sent-218, score-0.41]
</p><p>92 Combining shape and color performed better than using the two shape features alone for 52 of the categories, while it degraded performance for 46 of the categories, and did not change performance in the remaining 3. [sent-219, score-0.465]
</p><p>93 In Figure 4 we show the confusion matrix for combined shape and color using 15 training images per category. [sent-220, score-0.605]
</p><p>94 Almost all the processing at test time is the computation of the elementary distances between the focal images and the test image. [sent-222, score-1.054]
</p><p>95 In practice the weight vectors that we learn for our focal images are fairly sparse, with a median of 69% of the elements set to zero after learning, which greatly reduces  the number of feature comparisons performed at test time. [sent-223, score-0.892]
</p><p>96 8 After comparisons are computed, we only need to compute linear combinations and compare scores across focal images, which amounts to negligible processing time. [sent-225, score-0.57]
</p><p>97 Acknowledgements We would like to thank Hao Zhang and Alex Berg for use of their precomputed geometric blur features, and Hao, Alex, Mike Maire, Adam Kirk, Mark Paskin, and Chuck Rosenberg for many helpful discussions. [sent-228, score-0.323]
</p><p>98 Puzicha, “Shape matching and object recognition using shape contexts,” PAMI, vol. [sent-243, score-0.334]
</p><p>99 Darrell, “Pyramic match kernels: Discriminative classﬁciation with sets of image features (version 2),” Tech. [sent-288, score-0.34]
</p><p>100 Poggio, “Object recognition with features inspired by visual cortex,” in CVPR, 2005. [sent-320, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('focal', 0.535), ('wf', 0.266), ('blur', 0.227), ('triplets', 0.227), ('images', 0.221), ('image', 0.204), ('lilly', 0.184), ('elementary', 0.165), ('features', 0.136), ('distance', 0.122), ('water', 0.122), ('caltech', 0.121), ('recognition', 0.12), ('shape', 0.11), ('color', 0.109), ('ower', 0.107), ('patch', 0.103), ('df', 0.1), ('training', 0.097), ('geometric', 0.096), ('categories', 0.092), ('exemplars', 0.089), ('distances', 0.087), ('malik', 0.078), ('category', 0.077), ('pos', 0.076), ('object', 0.073), ('cvpr', 0.073), ('sun', 0.072), ('cougar', 0.071), ('published', 0.065), ('retrieval', 0.063), ('browsing', 0.061), ('lotus', 0.061), ('neg', 0.061), ('sift', 0.06), ('visual', 0.06), ('berg', 0.058), ('descriptors', 0.054), ('rankings', 0.053), ('exemplar', 0.053), ('singer', 0.052), ('query', 0.051), ('ears', 0.049), ('triplet', 0.049), ('metric', 0.048), ('zhang', 0.047), ('per', 0.044), ('feature', 0.043), ('ranking', 0.041), ('crocodile', 0.041), ('maire', 0.041), ('classi', 0.038), ('neighbor', 0.038), ('patches', 0.037), ('joachims', 0.037), ('hao', 0.036), ('facto', 0.036), ('jet', 0.036), ('mouth', 0.036), ('comparisons', 0.035), ('eyes', 0.035), ('learn', 0.035), ('class', 0.034), ('jth', 0.034), ('similarity', 0.033), ('schultz', 0.032), ('lowe', 0.032), ('matching', 0.031), ('logistic', 0.031), ('grauman', 0.03), ('schmid', 0.03), ('margin', 0.029), ('multiclass', 0.029), ('appearance', 0.029), ('alex', 0.029), ('cartesian', 0.029), ('relative', 0.028), ('radius', 0.028), ('categorization', 0.027), ('dataset', 0.027), ('pixels', 0.027), ('functions', 0.026), ('nearest', 0.026), ('combination', 0.026), ('edge', 0.025), ('benchmark', 0.025), ('texture', 0.025), ('berkeley', 0.025), ('naturally', 0.025), ('labels', 0.025), ('classes', 0.024), ('confusion', 0.024), ('test', 0.023), ('perona', 0.023), ('learned', 0.023), ('could', 0.023), ('probabilities', 0.023), ('contexts', 0.022), ('car', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="94-tfidf-1" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>2 0.20125026 <a title="94-tfidf-2" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>3 0.16207875 <a title="94-tfidf-3" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>Author: Frank Moosmann, Bill Triggs, Frederic Jurie</p><p>Abstract: Some of the most effective recent methods for content-based image classiﬁcation work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting “visual word” codes over the image, and classifying these with a conventional classiﬁer such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests – ensembles of randomly created clustering trees – and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classiﬁcation tasks. 1</p><p>4 0.15577833 <a title="94-tfidf-4" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><p>5 0.135885 <a title="94-tfidf-5" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>Author: Aharon B. Hillel, Daphna Weinshall</p><p>Abstract: We address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. Our approach is motivated by observations from cognitive psychology, which identify parts as the deﬁning component of basic level categories (like motorcycles), while sub-ordinate categories are more often deﬁned by part properties (like ’jagged wheels’). Accordingly, we suggest a two-stage algorithm: First, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). The model is then used to build a class-speciﬁc vector representation for images, where each entry corresponds to a model’s part. In the second stage we train a standard discriminative classiﬁer to classify subclass instances (e.g., cross motorcycles) based on the class-speciﬁc vector representation. We describe extensive experimental results with several subclasses. The proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classiﬁcation is based on a model of the sub-ordinate class. 1</p><p>6 0.11483806 <a title="94-tfidf-6" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>7 0.11444949 <a title="94-tfidf-7" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>8 0.10892533 <a title="94-tfidf-8" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>9 0.10874978 <a title="94-tfidf-9" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>10 0.10498143 <a title="94-tfidf-10" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>11 0.10341387 <a title="94-tfidf-11" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>12 0.094474487 <a title="94-tfidf-12" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>13 0.093427479 <a title="94-tfidf-13" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>14 0.089827821 <a title="94-tfidf-14" href="./nips-2006-Context_Effects_in_Category_Learning%3A_An_Investigation_of_Four_Probabilistic_Models.html">58 nips-2006-Context Effects in Category Learning: An Investigation of Four Probabilistic Models</a></p>
<p>15 0.0857042 <a title="94-tfidf-15" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>16 0.085481636 <a title="94-tfidf-16" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>17 0.080796525 <a title="94-tfidf-17" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>18 0.078811936 <a title="94-tfidf-18" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>19 0.07876531 <a title="94-tfidf-19" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>20 0.075853154 <a title="94-tfidf-20" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.224), (1, 0.04), (2, 0.194), (3, -0.086), (4, 0.049), (5, -0.042), (6, -0.264), (7, -0.139), (8, 0.009), (9, -0.084), (10, 0.135), (11, 0.086), (12, 0.019), (13, -0.049), (14, 0.161), (15, 0.092), (16, 0.048), (17, -0.004), (18, 0.022), (19, 0.01), (20, -0.062), (21, 0.035), (22, 0.032), (23, -0.083), (24, 0.025), (25, 0.089), (26, 0.026), (27, 0.025), (28, 0.095), (29, -0.044), (30, 0.023), (31, 0.062), (32, 0.061), (33, -0.044), (34, 0.103), (35, -0.102), (36, -0.033), (37, -0.014), (38, 0.035), (39, 0.024), (40, -0.057), (41, 0.015), (42, 0.082), (43, -0.035), (44, 0.089), (45, 0.052), (46, 0.052), (47, -0.151), (48, 0.08), (49, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94897044 <a title="94-lsi-1" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>2 0.76376146 <a title="94-lsi-2" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>Author: Anitha Kannan, John Winn, Carsten Rother</p><p>Abstract: Patch-based appearance models are used in a wide range of computer vision applications. To learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand. In the jigsaw model presented here, the shape, size and appearance of patches are learned automatically from the repeated structures in a set of training images. By learning such irregularly shaped ‘jigsaw pieces’, we are able to discover both the shape and the appearance of object parts without supervision. When applied to face images, for example, the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes, noses, eyebrows and cheeks, to name a few. We conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection. This enables parts of similar appearance but different shapes to be distinguished; for example, while foreheads and cheeks are both skin colored, they have markedly different shapes. 1</p><p>3 0.74661171 <a title="94-lsi-3" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>4 0.70953715 <a title="94-lsi-4" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>Author: Aharon B. Hillel, Daphna Weinshall</p><p>Abstract: We address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. Our approach is motivated by observations from cognitive psychology, which identify parts as the deﬁning component of basic level categories (like motorcycles), while sub-ordinate categories are more often deﬁned by part properties (like ’jagged wheels’). Accordingly, we suggest a two-stage algorithm: First, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). The model is then used to build a class-speciﬁc vector representation for images, where each entry corresponds to a model’s part. In the second stage we train a standard discriminative classiﬁer to classify subclass instances (e.g., cross motorcycles) based on the class-speciﬁc vector representation. We describe extensive experimental results with several subclasses. The proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classiﬁcation is based on a model of the sub-ordinate class. 1</p><p>5 0.68629968 <a title="94-lsi-5" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>Author: Frank Moosmann, Bill Triggs, Frederic Jurie</p><p>Abstract: Some of the most effective recent methods for content-based image classiﬁcation work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting “visual word” codes over the image, and classifying these with a conventional classiﬁer such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests – ensembles of randomly created clustering trees – and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classiﬁcation tasks. 1</p><p>6 0.62567556 <a title="94-lsi-6" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>7 0.59530419 <a title="94-lsi-7" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>8 0.58996868 <a title="94-lsi-8" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>9 0.58210045 <a title="94-lsi-9" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>10 0.55672699 <a title="94-lsi-10" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>11 0.55415279 <a title="94-lsi-11" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>12 0.50191396 <a title="94-lsi-12" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>13 0.50035155 <a title="94-lsi-13" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>14 0.49745578 <a title="94-lsi-14" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>15 0.49245417 <a title="94-lsi-15" href="./nips-2006-Statistical_Modeling_of_Images_with_Fields_of_Gaussian_Scale_Mixtures.html">182 nips-2006-Statistical Modeling of Images with Fields of Gaussian Scale Mixtures</a></p>
<p>16 0.46904916 <a title="94-lsi-16" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>17 0.45466661 <a title="94-lsi-17" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>18 0.44754291 <a title="94-lsi-18" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>19 0.44116881 <a title="94-lsi-19" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>20 0.40968528 <a title="94-lsi-20" href="./nips-2006-A_Humanlike_Predictor_of_Facial_Attractiveness.html">4 nips-2006-A Humanlike Predictor of Facial Attractiveness</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.077), (3, 0.02), (7, 0.106), (9, 0.035), (12, 0.012), (20, 0.015), (21, 0.294), (22, 0.062), (44, 0.05), (57, 0.138), (65, 0.059), (69, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81552941 <a title="94-lda-1" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>2 0.80097818 <a title="94-lda-2" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>Author: Frank Moosmann, Bill Triggs, Frederic Jurie</p><p>Abstract: Some of the most effective recent methods for content-based image classiﬁcation work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting “visual word” codes over the image, and classifying these with a conventional classiﬁer such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests – ensembles of randomly created clustering trees – and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classiﬁcation tasks. 1</p><p>3 0.78546876 <a title="94-lda-3" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We introduce a class of MPDs which greatly simplify Reinforcement Learning. They have discrete state spaces and continuous control spaces. The controls have the effect of rescaling the transition probabilities of an underlying Markov chain. A control cost penalizing KL divergence between controlled and uncontrolled transition probabilities makes the minimization problem convex, and allows analytical computation of the optimal controls given the optimal value function. An exponential transformation of the optimal value function makes the minimized Bellman equation linear. Apart from their theoretical signi cance, the new MDPs enable ef cient approximations to traditional MDPs. Shortest path problems are approximated to arbitrary precision with largest eigenvalue problems, yielding an O (n) algorithm. Accurate approximations to generic MDPs are obtained via continuous embedding reminiscent of LP relaxation in integer programming. Offpolicy learning of the optimal value function is possible without need for stateaction values; the new algorithm (Z-learning) outperforms Q-learning. This work was supported by NSF grant ECS–0524761. 1</p><p>4 0.60481775 <a title="94-lda-4" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>5 0.59758013 <a title="94-lda-5" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>Author: Aharon B. Hillel, Daphna Weinshall</p><p>Abstract: We address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. Our approach is motivated by observations from cognitive psychology, which identify parts as the deﬁning component of basic level categories (like motorcycles), while sub-ordinate categories are more often deﬁned by part properties (like ’jagged wheels’). Accordingly, we suggest a two-stage algorithm: First, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). The model is then used to build a class-speciﬁc vector representation for images, where each entry corresponds to a model’s part. In the second stage we train a standard discriminative classiﬁer to classify subclass instances (e.g., cross motorcycles) based on the class-speciﬁc vector representation. We describe extensive experimental results with several subclasses. The proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classiﬁcation is based on a model of the sub-ordinate class. 1</p><p>6 0.57720262 <a title="94-lda-6" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>7 0.56974518 <a title="94-lda-7" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>8 0.56827384 <a title="94-lda-8" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>9 0.55891281 <a title="94-lda-9" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>10 0.55854112 <a title="94-lda-10" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>11 0.55819762 <a title="94-lda-11" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>12 0.55663031 <a title="94-lda-12" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>13 0.55393612 <a title="94-lda-13" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>14 0.55160874 <a title="94-lda-14" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>15 0.5508185 <a title="94-lda-15" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>16 0.55046296 <a title="94-lda-16" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>17 0.54929215 <a title="94-lda-17" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>18 0.54907012 <a title="94-lda-18" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>19 0.54786128 <a title="94-lda-19" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>20 0.54762667 <a title="94-lda-20" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
