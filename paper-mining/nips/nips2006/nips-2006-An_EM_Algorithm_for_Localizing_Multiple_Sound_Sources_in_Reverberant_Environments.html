<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-27" href="#">nips2006-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</h1>
<br/><p>Source: <a title="nips-2006-27-pdf" href="http://papers.nips.cc/paper/3159-an-em-algorithm-for-localizing-multiple-sound-sources-in-reverberant-environments.pdf">pdf</a></p><p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>Reference: <a title="nips-2006-27-reference" href="../nips2006_reference/nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. [sent-10, score-0.723]
</p><p>2 The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. [sent-11, score-0.242]
</p><p>3 Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. [sent-14, score-0.296]
</p><p>4 1  Introduction  Determining the direction from which a sound originated using only two microphones is a difﬁcult problem. [sent-15, score-0.217]
</p><p>5 It is exacerbated by the presence of sounds from other sources and by realistic reverberations, as would be found in a classroom. [sent-16, score-0.239]
</p><p>6 A related and equally difﬁcult problem is determining in which regions of a spectrogram a sound is observable, the so-called time-frequency mask, useful for source separation [1]. [sent-17, score-0.57]
</p><p>7 Either they assume sound sources are statistically stationary, they assume anechoic conditions, or they require an array with at least as many microphones as there are sources to be localized. [sent-19, score-0.911]
</p><p>8 The method proposed in this paper takes a probabilistic approach to localization, using the psychoacoustic cue of interaural phase difference (IPD). [sent-20, score-0.367]
</p><p>9 The basic assumptions that make this possible are that a single source dominates each time-frequency point and that a single delay and ampliﬁcation cause the difference in the ears’ signals at a particular point. [sent-22, score-0.329]
</p><p>10 It is able to localize more sources than it has observations, even in reverberant environments. [sent-24, score-0.498]
</p><p>11 It makes no assumptions about the statistics of the source signal, making it well suited to localizing speech, a highly non-Gaussian and non-stationary signal. [sent-25, score-0.275]
</p><p>12 Its probabilistic nature also facilitates the incorporation of other probabilistic cues for source separation such as those obtained from single-microphone computational auditory scene analysis. [sent-26, score-0.389]
</p><p>13 Many comparable methods are also based on IPD, but they ﬁrst convert it into interaural time difference. [sent-27, score-0.269]
</p><p>14 Our system, however, is able to use observations across the entire frequency  range, because even though the same phase difference can correspond to multiple delays, a particular delay corresponds unambiguously to a speciﬁc phase difference at every frequency. [sent-29, score-0.36]
</p><p>15 We evaluate our system on the localization and separation of two and three simultaneous speakers in simulated anechoic and reverberant environments. [sent-30, score-0.935]
</p><p>16 The speech comes from the TIMIT acousticphonetic continuous speech corpus [2], the anechoic simulations use the head related transfer functions described in [3], and the reverberant simulations use the binaural classroom impulse responses described in [4]. [sent-31, score-1.444]
</p><p>17 We use four metrics to evaluate our system, the root mean square localization error, the mutual information between the estimated mask and a ground truth mask, the signal to noise ratio of separated speech from [5], and the W-disjoint orthogonality metric of [1]. [sent-32, score-1.041]
</p><p>18 Our EM approach outperformed Yilmaz and Rickard’s DUET algorithm [1] and Aarabi’s PHAT-histogram [6] in anechoic situations, and performed comparably to PHAT-histogram in reverberation. [sent-33, score-0.296]
</p><p>19 1  Previous work  Many systems exist for localizing sounds using a microphone array, e. [sent-35, score-0.227]
</p><p>20 [5] make empirical models of the timing and level differences for combinations of two sources in known positions synthesized with anechoic head-related transfer functions (HRTFs). [sent-42, score-0.527]
</p><p>21 They then classify each time-frequency cell in their auditory-based representation, creating a binary time-frequency mask which contains the cells that appear to be dominated by the target source. [sent-43, score-0.331]
</p><p>22 Yilmaz and Rickard [1] studied the interaction of speech signals in the time-frequency plane, concluding that multiple speech signals generally do not overlap much in both time and frequency. [sent-44, score-0.402]
</p><p>23 They also conclude, as in [5], that the best ground truth mask includes only points in which the signal to noise ratio is 0dB or greater. [sent-45, score-0.624]
</p><p>24 It is designed for anechoic and noise-free situations and subsequently its accuracy suffers in more realistic settings. [sent-47, score-0.321]
</p><p>25 Aarabi’s method, while quite simple, is still one of the most accurate methods for localizing many simultaneous sound sources, even in reverberation. [sent-49, score-0.297]
</p><p>26 A limitation of both algorithms, however, is the assumption that a single source dominates each analysis window, as compared to time-frequency masking algorithms which allow different sources to dominate different frequencies in the same analysis window. [sent-51, score-0.48]
</p><p>27 2  Framework  For the purposes of deriving this model we will examine the situation where one sound source arrives at two spatially distinct microphones or ears. [sent-52, score-0.395]
</p><p>28 Denote the sound source as s(t), and the signals received at the left and right ears as (t) and r(t), respectively. [sent-54, score-0.406]
</p><p>29 (1) The ratio of the short-time Fourier transforms, F{·}, of both equations is the interaural spectrogram, L(ω, t) XIS (ω, t) ≡ = α(ω, t)eφ(ω,t) = ea−jωτ N (ω, t), (2) R(ω, t) N F {n a where τ = τ − τr , N (ω, t) = Nr (ω,t) = F {nr (t)} , and a = log ar . [sent-58, score-0.295]
</p><p>30 As observed in [9], N (ω, t), the noise in the interaural spectrogram of a single source, is unimodal and approximately identically distributed for all frequencies and times. [sent-62, score-0.499]
</p><p>31 In this work, we ignore the magnitude noise and approximate the phase noise with a mixture of Gaussians, all with the same mean. [sent-65, score-0.238]
</p><p>32 Our model of the interaural phase difference is a mixture over sources, delays, and Gaussians. [sent-71, score-0.397]
</p><p>33 This discretization gives the most ﬂexible possible distribution over τ for each source, but since we expect sources to be compact in τ , a unimodal parametric distribution could work. [sent-74, score-0.224]
</p><p>34 Experiments approximating Laplacian and Gaussian distributions over τ , however, did not perform as well at localizing sources or creating masks as the more ﬂexible multinomial. [sent-75, score-0.461]
</p><p>35 For a particular source, then, the probability of an observed delay is p(φ(ω, t) | i, τ ) = p  ˆ  N (φ(ω, t; τ ))  (3)  where p N (·) is the probability density function of the phase noise, N (ω, t), described above. [sent-76, score-0.21]
</p><p>36 5  2  time / sec  0 -10  0 -8  -6  -4  -2  0  2  τ  4  -3  -2  -1  0  1  2  3 angle / rad  Figure 1: Example parameters estimated for two speakers located at 0◦ and 45◦ in a reverberant classroom. [sent-106, score-0.398]
</p><p>37 Speciﬁcally, we are interested in the marginal probability of a point’s coming from source i, p(i), the distributions over delays and Gaussians conditioned on the source, p(τ | i) and p(j | i), and the probability of each time-frequency point’s coming from each source, Mi (ω, t). [sent-113, score-0.233]
</p><p>38 (10)  jτ  See Figure 1(d)-(f) for an example of the parameters estimated for two speakers located at 0◦ and 45◦ in a reverberant classroom. [sent-115, score-0.356]
</p><p>39 4  Experiments  In order to evaluate our system, we simulated speech in anechoic and reverberant noise situations by convolving anechoic speech samples with binaural impulse responses. [sent-116, score-1.527]
</p><p>40 We used speech from the TIMIT acoustic-phonetic continuous speech corpus [2], a dataset of utterances spoken by 630 native American English speakers. [sent-117, score-0.331]
</p><p>41 To allow the speakers to be equally represented in each mixture, we normalized all of the signals by their average energies before convolving them with the binaural impulse responses. [sent-119, score-0.528]
</p><p>42 The anechoic binaural impulse responses came from Algazi et al. [sent-120, score-0.659]
</p><p>43 The measurements we used were for the KEMAR dummy head with small ears, although the dataset contains impulse responses for around 50 individuals. [sent-123, score-0.321]
</p><p>44 The reverberant binaural impulse responses we used were recorded by Shinn-Cunningham et al. [sent-124, score-0.614]
</p><p>45 We used the measurements taken in the middle of the classroom with the sources at a distance of 1 m from the subject. [sent-128, score-0.327]
</p><p>46 1  Comparison algorithms  We compare the performance of the time-frequency masks and the localization accuracy of our algorithm with those of two other algorithms. [sent-140, score-0.219]
</p><p>47 In order to estimate the interaural time and level differences of the signals in a mixture, DUET creates a two-dimensional histogram of them at every point in the interaural spectrogram. [sent-142, score-0.605]
</p><p>48 The interaural parameter calculation of DUET requires that the interaural phase of a measurement c unambiguously translates to a delay. [sent-144, score-0.662]
</p><p>49 Even though the frequencies used to estimate the interaural parameters are limited, a time-frequency mask can still be computed for all frequencies. [sent-149, score-0.678]
</p><p>50 See Figure 1(b) for an example of such a mask estimated by DUET. [sent-150, score-0.361]
</p><p>51 The algorithm localizes multiple simultaneous sources by cross-correlating the left and right channels using the Phase Transform (PHAT) for each frame of the interaural spectrogram. [sent-152, score-0.508]
</p><p>52 The I largest peaks in this histogram are assumed to be the interaural delays of the I sources. [sent-154, score-0.391]
</p><p>53 While not designed to create time-frequency masks, one can be constructed that simply assigns an entire frame to the source from which its delay originates. [sent-155, score-0.262]
</p><p>54 See Figure 1(c) for an example mask estimated by PHAT-histogram. [sent-156, score-0.361]
</p><p>55 For power-based metrics, we include ground truth and random masks in the comparison as baselines. [sent-158, score-0.296]
</p><p>56 The ground truth, or 0 dB, mask is the collection of all time-frequency points in which a particular source is louder than the mixture of all other sources, it is included to measure the maximum improvement achievable by an algorithmically created mask. [sent-159, score-0.584]
</p><p>57 The random mask is created by assigning each time-frequency point to one of the I sources at random, it is included to measure the performance of the simplest possible masking algorithm. [sent-160, score-0.583]
</p><p>58 The second is to measure the performance in terms of the amount of energy allowed through the mask or blocked by it, e. [sent-168, score-0.378]
</p><p>59 To measure performance valuing all points equally, we compute the mutual information between the ground truth mask and the predicted mask. [sent-171, score-0.551]
</p><p>60 Each mask point is treated as a binary random variable, so the mutual information can easily be calculated from their individual and joint entropies. [sent-172, score-0.392]
</p><p>61 One potential drawback to using the mutual information as a performance metric is that it has no ﬁxed maximum, it is bounded below by 0, but above by the entropy of the ground truth mask, which varies with each particular mixture. [sent-174, score-0.244]
</p><p>62 Fortunately, the entropy of the ground truth mask was close to 1 for almost all of the mixtures in this evaluation. [sent-175, score-0.49]
</p><p>63 To measure the signal to noise ratio (SNR), we follow [5] and take the ratio of the amount of energy in the original signal that is passed through the mask to the amount of energy in the mixed signal minus the original signal that is passed through the mask. [sent-176, score-0.744]
</p><p>64 This metric penalizes masks that eliminate signal as well as masks that pass noise. [sent-178, score-0.351]
</p><p>65 This is the signal to noise ratio in the mixture the mask passes through, multiplied by a (possibly negative) penalty term for eliminating signal energy. [sent-180, score-0.548]
</p><p>66 When evaluated on speech, energy based metrics tend to favor systems with better performance at frequencies below 500 Hz, where the energy is concentrated. [sent-181, score-0.222]
</p><p>67 In order to more evenly distribute the energy across frequencies and thus include the higher frequencies more equally in the energy-based metrics, we apply a mild high pass pre-emphasis ﬁlter to all of the speech segments. [sent-183, score-0.362]
</p><p>68 3  Results  We evaluated the performance of these algorithms in four different conditions, using two and three simultaneous speakers in reverberant and anechoic conditions. [sent-186, score-0.662]
</p><p>69 In the two source experiments, the target source was held at 0◦ , while the distracter was moved from 5◦ to 90◦ . [sent-187, score-0.3]
</p><p>70 In the three source experiments, the target source was held at 0◦ and distracters were located symmetrically on either side of the target from 5◦ to 90◦ . [sent-188, score-0.3]
</p><p>71 Its root mean square error is particularly low for two-speaker and anechoic tests, and only slightly higher for three speakers in reverberation. [sent-192, score-0.397]
</p><p>72 It does not localize well when the sources are very close together, i. [sent-193, score-0.247]
</p><p>73 Head shadowing causes interaural intensity differences at high frequencies which change the distribution of IPDs, and violate our model’s assumption that phase noise is identically distributed across frequencies. [sent-198, score-0.536]
</p><p>74 It also performs well at time-frequency masking, more so for anechoic simulations than reverberant. [sent-199, score-0.323]
</p><p>75 See Figure 1(d) for an example time-frequency mask in reverberation. [sent-200, score-0.331]
</p><p>76 4 kHz corresponding to the frequencies at which the sources have the same IPD, modulo 2π. [sent-204, score-0.277]
</p><p>77 1 0 -80  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  info / bits  0. [sent-209, score-0.934]
</p><p>78 6  -10 -80  -2 -60  -40 -20 0 separation / deg  -80  -60  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  1 10 0  5 0  0. [sent-211, score-2.09]
</p><p>79 1  -60  0 -80  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  info / bits  1  0. [sent-212, score-0.934]
</p><p>80 2  error / ms  gnd truth EM aarabi yilmaz random  -1  0. [sent-216, score-0.389]
</p><p>81 4  -2 -60  -40 -20 0 separation / deg  -80 1  10 0  5  0. [sent-218, score-0.418]
</p><p>82 1  -60  0 -80  -40 -20 0 separation / deg  -60  -40 -20 0 separation / deg  info / bits  1  0. [sent-221, score-0.934]
</p><p>83 8  -2 -60  -40 -20 0 separation / deg  -80 1  10 0  5  0. [sent-226, score-0.418]
</p><p>84 DUET suffers even in anechoic, two-source situations, possibly because it was designed for free-standing microphones as opposed to dummy head recordings. [sent-239, score-0.227]
</p><p>85 The advantage of our method for masking, however, is particularly clear in anechoic conditions, where it has the highest mutual information at all angles and the highest SNR and WDO at lower angles. [sent-241, score-0.357]
</p><p>86 In reverberant conditions, the mutual information between estimated masks and ground truth masks becomes quite low, but PHAT-histogram comes out slightly ahead. [sent-242, score-0.801]
</p><p>87 5  Conclusions and Future Work  We have derived and demonstrated an expectation-maximization algorithm for probabilistic source separation and time-frequency masking. [sent-245, score-0.341]
</p><p>88 Using the interaural phase delay, it is able to localize more sources than microphones, even in the reverberation found in a typical classroom. [sent-246, score-0.686]
</p><p>89 It does not depend on any assumptions about sound source statistics, making it well suited for such non-stationary signals as speech and music. [sent-247, score-0.483]
</p><p>90 Because it is probabilistic, it is straightforward to augment the feature representation with other monaural or binaural cues. [sent-248, score-0.233]
</p><p>91 Perhaps the largest gain in signal separation accuracy could come from the combination of this method with other computational auditory scene analysis techniques [11, 12]. [sent-250, score-0.292]
</p><p>92 A system using both monaural and binaural cues should surpass the performance of either approach alone. [sent-251, score-0.233]
</p><p>93 Another binaural cue that would be easy to add is IID caused by head shadowing and pinna ﬁltering, allowing localization in both azimuth and elevation. [sent-252, score-0.379]
</p><p>94 In addition, a parametric, heavy tailed model could be used instead of the current discrete model to ensure unimodality of the distributions and enforce the separation of different sources. [sent-255, score-0.227]
</p><p>95 Since sources tend to dominate regions of adjacent points in both time and frequency, the information at its neighbors could help a particular point localize itself. [sent-258, score-0.247]
</p><p>96 Acknowledgments The authors would like to thank Barbara Shinn-Cunningham for sharing her lab’s binaural room impulse response data with us and Richard Duda for making his lab’s head-related transfer functions available on the web. [sent-259, score-0.369]
</p><p>97 Localizing nearby sound sources in a classroom: Binaural room impulse responses. [sent-292, score-0.471]
</p><p>98 A practical methodology for speech source localization with microphone arrays. [sent-304, score-0.428]
</p><p>99 Estimating single-channel source separation masks: relevance vector machine classiﬁers vs pitch-based masking. [sent-319, score-0.341]
</p><p>100 The auditory organization of speech and other sources in listeners and computational models. [sent-324, score-0.381]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mask', 0.331), ('anechoic', 0.296), ('interaural', 0.269), ('reverberant', 0.251), ('deg', 0.227), ('sources', 0.199), ('binaural', 0.197), ('separation', 0.191), ('zij', 0.171), ('source', 0.15), ('ij', 0.142), ('impulse', 0.14), ('masks', 0.137), ('speech', 0.134), ('sound', 0.132), ('aarabi', 0.125), ('duet', 0.125), ('yilmaz', 0.125), ('localizing', 0.125), ('delay', 0.112), ('phase', 0.098), ('em', 0.098), ('classroom', 0.09), ('ipd', 0.09), ('truth', 0.086), ('snr', 0.086), ('microphones', 0.085), ('delays', 0.083), ('localization', 0.082), ('frequencies', 0.078), ('speakers', 0.075), ('ground', 0.073), ('freq', 0.072), ('reverberation', 0.072), ('spectrogram', 0.072), ('wdo', 0.072), ('signals', 0.067), ('khz', 0.066), ('head', 0.064), ('info', 0.062), ('microphone', 0.062), ('mutual', 0.061), ('ears', 0.057), ('noise', 0.055), ('kemar', 0.054), ('signal', 0.053), ('dummy', 0.053), ('masking', 0.053), ('ms', 0.053), ('db', 0.053), ('metrics', 0.05), ('auditory', 0.048), ('localize', 0.048), ('rickard', 0.047), ('timit', 0.047), ('energy', 0.047), ('recordings', 0.045), ('sec', 0.042), ('simultaneous', 0.04), ('sounds', 0.04), ('peaks', 0.039), ('daniel', 0.038), ('measurements', 0.038), ('utterances', 0.037), ('bits', 0.036), ('algazi', 0.036), ('mandel', 0.036), ('monaural', 0.036), ('sapa', 0.036), ('shadowing', 0.036), ('tailed', 0.036), ('orthogonality', 0.036), ('nr', 0.036), ('gaussians', 0.035), ('audio', 0.033), ('speaker', 0.033), ('transfer', 0.032), ('cocktail', 0.031), ('party', 0.031), ('roman', 0.031), ('estimated', 0.03), ('mixture', 0.03), ('fourier', 0.029), ('arrives', 0.028), ('window', 0.028), ('simulations', 0.027), ('barbara', 0.026), ('unambiguously', 0.026), ('corpus', 0.026), ('responses', 0.026), ('frequency', 0.026), ('slightly', 0.026), ('ratio', 0.026), ('suffers', 0.025), ('jebara', 0.025), ('matthew', 0.025), ('unimodal', 0.025), ('equally', 0.025), ('metric', 0.024), ('convolving', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="27-tfidf-1" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>2 0.28411543 <a title="27-tfidf-2" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>3 0.14520375 <a title="27-tfidf-3" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>4 0.10804654 <a title="27-tfidf-4" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>5 0.098797396 <a title="27-tfidf-5" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><p>6 0.092744239 <a title="27-tfidf-6" href="./nips-2006-Single_Channel_Speech_Separation_Using_Factorial_Dynamics.html">176 nips-2006-Single Channel Speech Separation Using Factorial Dynamics</a></p>
<p>7 0.061664637 <a title="27-tfidf-7" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>8 0.057132561 <a title="27-tfidf-8" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>9 0.05639331 <a title="27-tfidf-9" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>10 0.055878334 <a title="27-tfidf-10" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>11 0.053367235 <a title="27-tfidf-11" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>12 0.051346075 <a title="27-tfidf-12" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>13 0.050550256 <a title="27-tfidf-13" href="./nips-2006-Speakers_optimize_information_density_through_syntactic_reduction.html">180 nips-2006-Speakers optimize information density through syntactic reduction</a></p>
<p>14 0.05032498 <a title="27-tfidf-14" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>15 0.050186541 <a title="27-tfidf-15" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>16 0.049949639 <a title="27-tfidf-16" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>17 0.049355686 <a title="27-tfidf-17" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>18 0.048806224 <a title="27-tfidf-18" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>19 0.048142537 <a title="27-tfidf-19" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<p>20 0.044166844 <a title="27-tfidf-20" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.15), (1, -0.026), (2, 0.069), (3, -0.056), (4, -0.057), (5, -0.059), (6, 0.192), (7, 0.009), (8, -0.093), (9, 0.06), (10, 0.029), (11, -0.051), (12, -0.129), (13, -0.042), (14, 0.222), (15, -0.205), (16, 0.141), (17, -0.14), (18, 0.175), (19, -0.028), (20, -0.025), (21, -0.019), (22, -0.146), (23, 0.046), (24, 0.215), (25, 0.074), (26, -0.067), (27, -0.02), (28, -0.123), (29, 0.024), (30, 0.136), (31, 0.001), (32, -0.033), (33, 0.018), (34, 0.023), (35, -0.007), (36, -0.031), (37, -0.019), (38, -0.028), (39, -0.018), (40, 0.1), (41, -0.052), (42, 0.001), (43, 0.044), (44, 0.121), (45, 0.02), (46, -0.022), (47, -0.057), (48, -0.003), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96756411 <a title="27-lsi-1" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>2 0.86493796 <a title="27-lsi-2" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>Author: Lars Omlor, Martin Giese</p><p>Abstract: Blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. A special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. Since in this case the signals outnumber the sources the problem is over-determined. Most popular approaches for addressing this problem are based on purely linear mixing models. However, many applications like the modeling of acoustic signals, EMG signals, or movement trajectories, require temporal shift-invariance of the extracted components. This case has only rarely been treated in the computational literature, and speciﬁcally for the case of dimension reduction almost no algorithms have been proposed. We present a new algorithm for the solution of this problem, which is based on a timefrequency transformation (Wigner-Ville distribution) of the generative model. We show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. In addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms. 1</p><p>3 0.62866408 <a title="27-lsi-3" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>4 0.50054693 <a title="27-lsi-4" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>5 0.49392602 <a title="27-lsi-5" href="./nips-2006-Single_Channel_Speech_Separation_Using_Factorial_Dynamics.html">176 nips-2006-Single Channel Speech Separation Using Factorial Dynamics</a></p>
<p>Author: John R. Hershey, Trausti Kristjansson, Steven Rennie, Peder A. Olsen</p><p>Abstract: Human listeners have the extraordinary ability to hear and recognize speech even when more than one person is talking. Their machine counterparts have historically been unable to compete with this ability, until now. We present a modelbased system that performs on par with humans in the task of separating speech of two talkers from a single-channel recording. Remarkably, the system surpasses human recognition performance in many conditions. The models of speech use temporal dynamics to help infer the source speech signals, given mixed speech signals. The estimated source signals are then recognized using a conventional speech recognition system. We demonstrate that the system achieves its best performance when the model of temporal dynamics closely captures the grammatical constraints of the task. One of the hallmarks of human perception is our ability to solve the auditory cocktail party problem: we can direct our attention to a given speaker in the presence of interfering speech, and understand what was said remarkably well. Until now the same could not be said for automatic speech recognition systems. However, we have recently introduced a system which in many conditions performs this task better than humans [1][2]. The model addresses the Pascal Speech Separation Challenge task [3], and outperforms all other published results by more than 10% word error rate (WER). In this model, dynamics are modeled using a layered combination of one or two Markov chains: one for long-term dependencies and another for short-term dependencies. The combination of the two speakers was handled via an iterative Laplace approximation method known as Algonquin [4]. Here we describe experiments that show better performance on the same task with a simpler version of the model. The task we address is provided by the PASCAL Speech Separation Challenge [3], which provides standard training, development, and test data sets of single-channel speech mixtures following an arbitrary but simple grammar. In addition, the challenge organizers have conducted human-listening experiments to provide an interesting baseline for comparison of computational techniques. The overall system we developed is composed of the three components: a speaker identiﬁcation and gain estimation component, a signal separation component, and a speech recognition system. In this paper we focus on the signal separation component, which is composed of the acoustic and grammatical models. The details of the other components are discussed in [2]. Single-channel speech separation has previously been attempted using Gaussian mixture models (GMMs) on individual frames of acoustic features. However such models tend to perform well only when speakers are of different gender or have rather different voices [4]. When speakers have similar voices, speaker-dependent mixture models cannot unambiguously identify the component speakers. In such cases it is helpful to model the temporal dynamics of the speech. Several models in the literature have attempted to do so either for recognition [5, 6] or enhancement [7, 8] of speech. Such models have typically been based on a discrete-state hidden Markov model (HMM) operating on a frame-based acoustic feature vector. Modeling the dynamics of the log spectrum of speech is challenging in that different speech components evolve at different time-scales. For example the excitation, which carries mainly pitch, versus the ﬁlter, which consists of the formant structure, are somewhat independent of each other. The formant structure closely follows the sequences of phonemes in each word, which are pronounced at a rate of several per second. In non-tonal languages such as English, the pitch ﬂuctuates with prosody over the course of a sentence, and is not directly coupled with the words being spoken. Nevertheless, it seems to be important in separating speech, because the pitch harmonics carry predictable structure that stands out against the background. We address the various dynamic components of speech by testing different levels of dynamic constraints in our models. We explore four different levels of dynamics: no dynamics, low-level acoustic dynamics, high-level grammar dynamics, and a layered combination, dual dynamics, of the acoustic and grammar dynamics. The grammar dynamics and dual dynamics models perform the best in our experiments. The acoustic models are combined to model mixtures of speech using two methods: a nonlinear model known as Algonquin, which models the combination of log-spectrum models as a sum in the power spectrum, and a simpler max model that combines two log spectra using the max function. It turns out that whereas Algonquin works well, our formulation of the max model does better overall. With the combination of the max model and grammar-level dynamics, the model produces remarkable results: it is often able to extract two utterances from a mixture even when they are from the same speaker 1 . Overall results are given in Table 1, which shows that our closest competitors are human listeners. Table 1: Overall word error rates across all conditions on the challenge task. Human: average human error rate, IBM: our best result, Next Best: the best of the eight other published results on this task, and Chance: the theoretical error rate for random guessing. System: Word Error Rate: 1 Human 22.3% IBM 22.6% Next Best 34.2% Chance 93.0% Speech Models The model consists of an acoustic model and temporal dynamics model for each source, and a mixing model, which models how the source models are combined to describe the mixture. The acoustic features were short-time log spectrum frames computed every 15 ms. Each frame was of length 40 ms and a 640-point mixed-radix FFT was used. The DC component was discarded, producing a 319-dimensional log-power-spectrum feature vector yt . The acoustic model consists of a set of diagonal-covariance Gaussians in the features. For a given speaker, a, we model the conditional probability of the log-power spectrum of each source signal xa given a discrete acoustic state sa as Gaussian, p(xa |sa ) = N (xa ; µsa , Σsa ), with mean µsa , and covariance matrix Σsa . We used 256 Gaussians, one per acoustic state, to model the acoustic space of each speaker. For efﬁciency and tractability we restrict the covariance to be diagonal. A model with no dynamics can be formulated by producing state probabilities p(sa ), and is depicted in 1(a). Acoustic Dynamics: To capture the low-level dynamics of the acoustic signal, we modeled the acoustic dynamics of a given speaker, a, via state transitions p(sa |sa ) as shown in Figure 1(b). t t−1 There are 256 acoustic states, hence for each speaker a, we estimated a 256 × 256 element transition matrix Aa . Grammar Dynamics: The grammar dynamics are modeled by grammar state transitions, a a p(vt |vt−1 ), which consist of left-to-right phone models. The legal word sequences are given by the Speech Separation Challenge grammar [3] and are modeled using a set of pronunciations that 1 Demos and information can be found at: http : //www.research.ibm.com/speechseparation sa t−1 sa t sa t−1 sa t xt−1 xt xt−1 xt (a) No Dynamics (b) Acoustic Dynamics a vt−1 a vt a vt−1 a vt sa t−1 sa t sa t−1 sa t xt−1 xt xt−1 xt (c) Grammar Dynamics (d) Dual Dynamics Figure 1: Graph of models for a given source. In (a), there are no dynamics, so the model is a simple mixture model. In (b), only acoustic dynamics are modeled. In (c), grammar dynamics are modeled with a shared set of acoustic Gaussians, in (d) dual – grammar and acoustic – dynamics have been combined. Note that (a) (b) and (c) are special cases of (d), where different nodes are assumed independent. map from words to three-state context-dependent phone models. The state transition probabilities derived from these phone models are sparse in the sense that most transition probabilities are zero. We model speaker dependent distributions p(sa |v a ) that associate the grammar states, v a to the speaker-dependent acoustic states. These are learned from training data where the grammar state sequences and acoustic state sequences are known for each utterance. The grammar of our system has 506 states, so we estimate a 506 × 256 element conditional probability matrix B a for each speaker. Dual Dynamics: The dual-dynamics model combines the acoustic dynamics with the grammar dynamics. It is useful in this case to avoid modeling the full combination of s and v states in the joint transitions p(sa |sa , vt ). Instead we make a naive-Bayes assumption to approximate this as t t−1 1 p(sa |sa )α p(sa |vt )β , where α and β adjust the relative inﬂuence of the two probabilities, and z t t−1 t z is the normalizing constant. Here we simply use the probability matrices Aa and B a , deﬁned above. 2 Mixed Speech Models The speech separation challenge involves recognizing speech in mixtures of signals from two speakers, a and b. We consider only mixing models that operate independently on each frequency for analytical and computational tractability. The short-time log spectrum of the mixture yt , in a given frequency band, is related to that of the two sources xa and xb via the mixing model given by the t t conditional probability distribution, p(y|xa , xb ). The joint distribution of the observation and source in one feature dimension, given the source states is thus: p(yt , xa , xb |sa , sb ) = p(yt |xa , xb )p(xa |sa )p(xb |sb ). t t t t t t t t t t (1) In general, to infer and reconstruct speech we need to compute the likelihood of the observed mixture p(yt |sa , sb ) = t t p(yt , xa , xb |sa , sb )dxa dxb , t t t t t t (2) and the posterior expected values of the sources given the states, E(xa |yt , sa , sb ) = t t t xa p(xa , xb |yt , sa , sb )dxa dxb , t t t t t t t (3) and similarly for xb . These quantities, combined with a prior model for the joint state set quences {sa , sb }, allow us to compute the minimum mean squared error (MMSE) estima1..T 1..T ˆ ˆ tors E(xa |y1..T ) or the maximum a posteriori (MAP) estimate E(xa |y1..T , sa 1..T , sb 1..T ), 1..T 1..T ˆ ˆ where sa 1..T , sb 1..T = arg maxsa ,sb p(sa , sb |y1..T ), where the subscript, 1..T , refers to 1..T 1..T 1..T 1..T all frames in the signal. The mixing model can be deﬁned in a number of ways. We explore two popular candidates, for which the above integrals can be readily computed: Algonquin, and the max model. s a s xa b xb y (a) Mixing Model (v a v b )t−1 (v a v b )t (sa sb )t−1 (sa sb )t yt yt (b) Dual Dynamics Factorial Model Figure 2: Model combination for two talkers. In (a) all dependencies are shown. In (b) the full dual-dynamics model is graphed with the xa and xb integrated out, and corresponding states from each speaker combined into product states. The other models are special cases of this graph with different edges removed, as in Figure 1. Algonquin: The relationship between the sources and mixture in the log power spectral domain is approximated as p(yt |xa , xb ) = N (yt ; log(exp(xa ) + exp(xb )), Ψ) (4) t t t t where Ψ is introduced to model the error due to the omission of phase [4]. An iterative NewtonLaplace method accurately approximates the conditional posterior p(xa , xb |yt , sa , sb ) from (1) as t t t t Gaussian. This Gaussian allows us to analytically compute the observation likelihood p(yt |sa , sb ) t t and expected value E(xa |yt , sa , sb ), as in [4]. t t t Max model: The mixing model is simpliﬁed using the fact that log of a sum is approximately the log of the maximum: p(y|xa , xb ) = δ y − max(xa , xb ) (5) In this model the likelihood is p(yt |sa , sb ) = pxa (yt |sa )Φxb (yt |sb ) + pxb (yt |sb )Φxa (yt |sa ), (6) t t t t t t t t t y t where Φxa (yt |sa ) = −∞ N (xa ; µsa , Σsa )dxa is a Gaussian cumulative distribution function [5]. t t t t t t In [5], such a model was used to compute state likelihoods and ﬁnd the optimal state sequence. In [8], a simpliﬁed model was used to infer binary masking values for reﬁltering. We take the max model a step further and derive source posteriors, so that we can compute the MMSE estimators for the log power spectrum. Note that the source posteriors in xa and xb are each t t a mixture of a delta function and a truncated Gaussian. Thus we analytically derive the necessary expected value: E(xa |yt , sa , sb ) t t t p(xa = yt |yt , sa , sb )yt + p(xa < yt |yt , sa , sb )E(xa |xa < yt , sa ) t t t t t t t t t pxa (yt |sa ) t a b , = πt yt + πt µsa − Σsa t t t Φxa (yt |sa ) t t = (7) (8) a b a with weights πt = p(xa=yt |yt , sa , sb ) = pxa (yt |sa )Φxb (yt |sb )/p(yt |sa , sb ), and πt = 1 − πt . For t t t t t t t t a ≫ µ b in a given frequency many pairs of states one model is signiﬁcantly louder than another µs s band, relative to their variances. In such cases it is reasonable to approximate the likelihood as p(yt |sa , sb ) ≈ pxa (yt |sa ), and the posterior expected values according to E(xa |yt , sa , sb ) ≈ yt and t t t t t t t E(xb |yt , sa , sb ) ≈ min(yt , µsb ), and similarly for µsa ≪ µsb . t t t t 3 Likelihood Estimation Because of the large number of state combinations, the model would not be practical without techniques to reduce computation time. To speed up the evaluation of the joint state likelihood, we employed both band quantization of the acoustic Gaussians and joint-state pruning. Band Quantization: One source of computational savings stems from the fact that some of the Gaussians in our model may differ only in a few features. Band quantization addresses this by approximating each of the D Gaussians of each model with a shared set of d Gaussians, where d ≪ D, in each of the F frequency bands of the feature vector. A similar idea is described in [9]. It relies on the use of a diagonal covariance matrix, so that p(xa |sa ) = f N (xa ; µf,sa , Σf,sa ), where Σf,sa f are the diagonal elements of covariance matrix Σsa . The mapping Mf (si ) associates each of the D Gaussians with one of the d Gaussians in band f . Now p(xa |sa ) = f N (xa ; µf,Mf (sa ) , Σf,Mf (sa ) ) ˆ f is used as a surrogate for p(xa |sa ). Figure 3 illustrates the idea. Figure 3: In band quantization, many multi-dimensional Gaussians are mapped to a few unidimensional Gaussians. Under this model the d Gaussians are optimized by minimizing the KL-divergence D( sa p(sa )p(xa |sa )|| sa p(sa )ˆ(xa |sa )), and likewise for sb . Then in each frequency band, p only d×d, instead of D ×D combinations of Gaussians have to be evaluated to compute p(y|sa , sb ). Despite the relatively small number of components d in each band, taken across bands, band quantization is capable of expressing dF distinct patterns, in an F -dimensional feature space, although in practice only a subset of these will be used to approximate the Gaussians in a given model. We used d = 8 and D = 256, which reduced the likelihood computation time by three orders of magnitude. Joint State Pruning: Another source of computational savings comes from the sparseness of the model. Only a handful of sa , sb combinations have likelihoods that are signiﬁcantly larger than the rest for a given observation. Only these states are required to adequately explain the observation. By pruning the total number of combinations down to a smaller number we can speed up the likelihood calculation, estimation of the components signals, as well as the temporal inference. However, we must estimate the likelihoods in order to determine which states to retain. We therefore used band-quantization to estimate likelihoods for all states, perform state pruning, and then the full model on the pruned states using the exact parameters. In the experiments reported here, we pruned down to 256 state combinations. The effect of these speedup methods on accuracy will be reported in a future publication. 4 Inference In our experiments we performed inference in four different conditions: no dynamics, with acoustic dynamics only, with grammar dynamics only, and with dual dynamics (acoustic and grammar). With no dynamics the source models reduce to GMMs and we infer MMSE estimates of the sources based on p(xa , xb |y) as computed from (1), using Algonquin or the max model. Once the log spectrum of each source is estimated, we estimate the corresponding time-domain signal as shown in [4]. In the acoustic dynamics condition the exact inference algorithm uses a 2-Dimensional Viterbi search, described below, with acoustic temporal constraints p(st |st−1 ) and likelihoods from Eqn. (1), to ﬁnd the most likely joint state sequence s1..T . Similarly in the grammar dynamics condition, 2-D Viterbi search is used to infer the grammar state sequences, v1..T . Instead of single Gaussians as the likelihood models, however, we have mixture models in this case. So we can perform an MMSE estimate of the sources by averaging over the posterior probability of the mixture components given the grammar Viterbi sequence, and the observations. It is critical to use the 2-D Viterbi algorithm in both cases, rather than the forward-backward algorithm, because in the same-speaker condition at 0dB, the acoustic models and dynamics are symmetric. This symmetry means that the posterior is essentially bimodal and averaging over these modes would yield identical estimates for both speakers. By ﬁnding the best path through the joint state space, the 2-D Viterbi algorithm breaks this symmetry and allows the model to make different estimates for each speaker. In the dual-dynamics condition we use the model of section 2(b). With two speakers, exact inference is computationally complex because the full joint distribution of the grammar and acoustic states, (v a × sa ) × (v b × sb ) is required and is very large in number. Instead we perform approximate inference by alternating the 2-D Viterbi search between two factors: the Cartesian product sa × sb of the acoustic state sequences and the Cartesian product v a × v b of the grammar state sequences. When evaluating each state sequence we hold the other chain constant, which decouples its dynamics and allows for efﬁcient inference. This is a useful factorization because the states sa and sb interact strongly with each other and similarly for v a and v b . Again, in the same-talker condition, the 2-D Viterbi search breaks the symmetry in each factor. 2-D Viterbi search: The Viterbi algorithm estimates the maximum-likelihood state sequence s1..T given the observations x1..T . The complexity of the Viterbi search is O(T D2 ) where D is the number of states and T is the number of frames. For producing MAP estimates of the 2 sources, we require a 2 dimensional Viterbi search which ﬁnds the most likely joint state sequences sa and 1..T sb given the mixed signal y1..T as was proposed in [5]. 1..T On the surface, the 2-D Viterbi search appears to be of complexity O(T D4 ). Surprisingly, it can be computed in O(T D3 ) operations. This stems from the fact that the dynamics for each chain are independent. The forward-backward algorithm for a factorial HMM with N state variables requires only O(T N DN +1 ) rather than the O(T D2N ) required for a naive implementation [10]. The same is true for the Viterbi algorithm. In the Viterbi algorithm, we wish to ﬁnd the most probable paths leading to each state by ﬁnding the two arguments sa and sb of the following maximization: t−1 t−1 {ˆa , sb } = st−1 ˆt−1 = arg max p(sa |sa )p(sb |sb )p(sa , sb |y1..t−1 ) t t−1 t t−1 t−1 t−1 sa sb t−1 t−1 arg max p(sa |sa ) max p(sb |sb )p(sa , sb |y1..t−1 ). t t−1 t t−1 t−1 t−1 a st−1 sb t−1 (9) The two maximizations can be done in sequence, requiring O(D3 ) operations with O(D2 ) storage for each step. In general, as with the forward-backward algorithm, the N -dimensional Viterbi search requires O(T N DN +1 ) operations. We can also exploit the sparsity of the transition matrices and observation likelihoods, by pruning unlikely values. Using both of these methods our implementation of 2-D Viterbi search is faster than the acoustic likelihood computation that serves as its input, for the model sizes and grammars chosen in the speech separation task. Speaker and Gain Estimation: In the challenge task, the gains and identities of the two speakers were unknown at test time and were selected from a set of 34 speakers which were mixed at SNRs ranging from 6dB to -9dB. We used speaker-dependent acoustic models because of their advantages when separating different speakers. These models were trained on gain-normalized data, so the models are not well matched to the different gains of the signals at test time. This means that we have to estimate both the speaker identities and the gain in order to adapt our models to the source signals for each test utterance. The number of speakers and range of SNRs in the test set makes it too expensive to consider every possible combination of models and gains. Instead, we developed an efﬁcient model-based method for identifying the speakers and gains, described in [2]. The algorithm is based upon a very simple idea: identify and utilize frames that are dominated by a single source – based on their likelihoods under each speaker-dependent acoustic model – to determine what sources are present in the mixture. Using this criteria we can eliminate most of the unlikely speakers, and explore all combinations of the remaining speakers. An approximate EM procedure is then used to select a single pair of speakers and estimate their gains. Recognition: Although inference in the system may involve recognition of the words– for models that contain a grammar –we still found that a separately trained recognizer performed better. After reconstruction, each of the two signals is therefore decoded with a speech recognition system that incorporates Speaker Dependent Labeling (SDL) [2]. This method uses speaker dependent models for each of the 34 speakers. Instead of using the speaker identities provided by the speaker ID and gain module, we followed the approach for gender dependent labeling (GDL) described in [11]. This technique provides better results than if the true speaker ID is speciﬁed. 5 Results The Speech Separation Challenge [3] involves separating the mixed speech of two speakers drawn from of a set of 34 speakers. An example utterance is place white by R 4 now. In each recording, one of the speakers says white while the other says blue, red or green. The task is to recognize the letter and the digit of the speaker that said white. Using the SDL recognizer, we decoded the two estimated signals under the assumption that one signal contains white and the other does not, and vice versa. We then used the association that yielded the highest combined likelihood. 80 WER (%) 60 40 20 0 Same Talker No Separation No dynamics Same Gender Acoustic Dyn. Different Gender Grammar Dyn All Dual Dyn Human Figure 4: Average word error rate (WER) as a function of model dynamics, in different talker conditions, compared to Human error rates, using Algonquin. Human listener performance [3] is compared in Figure 4 to results using the SDL recognizer without speech separation, and for each the proposed models. Performance is poor without separation in all conditions. With no dynamics the models do surprisingly well in the different talker conditions, but poorly when the signals come from the same talker. Acoustic dynamics gives some improvement, mainly in the same-talker condition. The grammar dynamics seems to give the most beneﬁt, bringing the error rate in the same-gender condition below that of humans. The dual-dynamics model performed about the same as the grammar dynamics model, despite our intuitions. Replacing Algonquin with the max model reduced the error rate in the dual dynamics model (from 24.3% to 23.5%) and grammar dynamics model (from 24.6% to 22.6%), which brings the latter closer than any other model to the human recognition rate of 22.3%. Figure 5 shows the relative word error rate of the best system compared to human subjects. When both speakers are around the same loudness, the system exceeds human performance, and in the same-gender condition makes less than half the errors of the humans. Human listeners do better when the two signals are at different levels, even if the target is below the masker (i.e., in -9dB), suggesting that they are better able to make use of differences in amplitude as a cue for separation. Relative Word Error Rate (WER) 200 Same Talker Same Gender Different Gender Human 150 100 50 0 −50 −100 6 dB 3 dB 0 dB −3 dB Signal to Noise Ratio (SNR) −6 dB −9 dB Figure 5: Word error rate of best system relative to human performance. Shaded area is where the system outperforms human listeners. An interesting question is to what extent different grammar constraints affect the results. To test this, we limited the grammar to just the two test utterances, and the error rate on the estimated sources dropped to around 10%. This may be a useful paradigm for separating speech from background noise when the text is known, such as in closed-captioned recordings. At the other extreme, in realistic speech recognition scenarios, there is little knowledge of the background speaker’s grammar. In such cases the beneﬁts of models of low-level acoustic continuity over purely grammar-based systems may be more apparent. It is our hope that further experiments with both human and machine listeners will provide us with a better understanding of the differences in their performance characteristics, and provide insights into how the human auditory system functions, as well as how automatic speech perception in general can be brought to human levels of performance. References [1] T. Kristjansson, J. R. Hershey, P. A. Olsen, S. Rennie, and R. Gopinath, “Super-human multi-talker speech recognition: The IBM 2006 speech separation challenge system,” in ICSLP, 2006. [2] Steven Rennie, Pedera A. Olsen, John R. Hershey, and Trausti Kristjansson, “Separating multiple speakers using temporal constraints,” in ISCA Workshop on Statistical And Perceptual Audition, 2006. [3] Martin Cooke and Tee-Won Lee, “Interspeech speech separation http : //www.dcs.shef.ac.uk/ ∼ martin/SpeechSeparationChallenge.htm, 2006. challenge,” [4] T. Kristjansson, J. Hershey, and H. Attias, “Single microphone source separation using high resolution signal reconstruction,” ICASSP, 2004. [5] P. Varga and R.K. Moore, “Hidden Markov model decomposition of speech and noise,” ICASSP, pp. 845–848, 1990. [6] M. Gales and S. Young, “Robust continuous speech recognition using parallel model combination,” IEEE Transactions on Speech and Audio Processing, vol. 4, no. 5, pp. 352–359, September 1996. [7] Y. Ephraim, “A Bayesian estimation approach for speech enhancement using hidden Markov models.,” vol. 40, no. 4, pp. 725–735, 1992. [8] S. Roweis, “Factorial models and reﬁltering for speech separation and denoising,” Eurospeech, pp. 1009–1012, 2003. [9] E. Bocchieri, “Vector quantization for the efﬁcient computation of continuous density likelihoods. proceedings of the international conference on acoustics,” in ICASSP, 1993, vol. II, pp. 692–695. [10] Zoubin Ghahramani and Michael I. Jordan, “Factorial hidden Markov models,” in Advances in Neural Information Processing Systems, vol. 8. [11] Peder Olsen and Satya Dharanipragada, “An efﬁcient integrated gender detection scheme and time mediated averaging of gender dependent acoustic models,” in Eurospeech 2003, 2003, vol. 4, pp. 2509–2512.</p><p>6 0.49294248 <a title="27-lsi-6" href="./nips-2006-Towards_a_general_independent_subspace_analysis.html">194 nips-2006-Towards a general independent subspace analysis</a></p>
<p>7 0.35050955 <a title="27-lsi-7" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>8 0.34339872 <a title="27-lsi-8" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>9 0.32130811 <a title="27-lsi-9" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>10 0.30955294 <a title="27-lsi-10" href="./nips-2006-Speakers_optimize_information_density_through_syntactic_reduction.html">180 nips-2006-Speakers optimize information density through syntactic reduction</a></p>
<p>11 0.30119374 <a title="27-lsi-11" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>12 0.2895624 <a title="27-lsi-12" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>13 0.27917615 <a title="27-lsi-13" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>14 0.27457741 <a title="27-lsi-14" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>15 0.24966273 <a title="27-lsi-15" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>16 0.23715459 <a title="27-lsi-16" href="./nips-2006-Unified_Inference_for_Variational_Bayesian_Linear_Gaussian_State-Space_Models.html">198 nips-2006-Unified Inference for Variational Bayesian Linear Gaussian State-Space Models</a></p>
<p>17 0.23391667 <a title="27-lsi-17" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>18 0.23335065 <a title="27-lsi-18" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<p>19 0.23205352 <a title="27-lsi-19" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>20 0.2317522 <a title="27-lsi-20" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.143), (2, 0.364), (3, 0.032), (7, 0.045), (9, 0.037), (12, 0.014), (20, 0.01), (22, 0.03), (44, 0.056), (57, 0.06), (65, 0.037), (69, 0.061), (71, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79135567 <a title="27-lda-1" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>Author: Michael I. Mandel, Daniel P. Ellis, Tony Jebara</p><p>Abstract: We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. The method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for ﬁnding the maximum likelihood parameters of that model. These parameters include distributions over delays and assignments of time-frequency regions to sources. We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation. 1</p><p>2 0.73640227 <a title="27-lda-2" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><p>3 0.67408651 <a title="27-lda-3" href="./nips-2006-Geometric_entropy_minimization_%28GEM%29_for_anomaly_detection_and_localization.html">85 nips-2006-Geometric entropy minimization (GEM) for anomaly detection and localization</a></p>
<p>Author: Alfred O. Hero</p><p>Abstract: We introduce a novel adaptive non-parametric anomaly detection approach, called GEM, that is based on the minimal covering properties of K-point entropic graphs when constructed on N training samples from a nominal probability distribution. Such graphs have the property that as N → ∞ their span recovers the entropy minimizing set that supports at least ρ = K/N (100)% of the mass of the Lebesgue part of the distribution. When a test sample falls outside of the entropy minimizing set an anomaly can be declared at a statistical level of signiﬁcance α = 1 − ρ. A method for implementing this non-parametric anomaly detector is proposed that approximates this minimum entropy set by the inﬂuence region of a K-point entropic graph built on the training data. By implementing an incremental leave-one-out k-nearest neighbor graph on resampled subsets of the training data GEM can efﬁciently detect outliers at a given level of signiﬁcance and compute their empirical p-values. We illustrate GEM for several simulated and real data sets in high dimensional feature spaces. 1</p><p>4 0.65147203 <a title="27-lda-4" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>Author: Oren Boiman, Michal Irani</p><p>Abstract: We propose a new approach for measuring similarity between two signals, which is applicable to many machine learning tasks, and to many signal types. We say that a signal S1 is “similar” to a signal S2 if it is “easy” to compose S1 from few large contiguous chunks of S2 . Obviously, if we use small enough pieces, then any signal can be composed of any other. Therefore, the larger those pieces are, the more similar S1 is to S2 . This induces a local similarity score at every point in the signal, based on the size of its supported surrounding region. These local scores can in turn be accumulated in a principled information-theoretic way into a global similarity score of the entire S1 to S2 . “Similarity by Composition” can be applied between pairs of signals, between groups of signals, and also between different portions of the same signal. It can therefore be employed in a wide variety of machine learning problems (clustering, classiﬁcation, retrieval, segmentation, attention, saliency, labelling, etc.), and can be applied to a wide range of signal types (images, video, audio, biological data, etc.) We show a few such examples. 1</p><p>5 0.50523555 <a title="27-lda-5" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>6 0.47639 <a title="27-lda-6" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>7 0.45202112 <a title="27-lda-7" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>8 0.45118254 <a title="27-lda-8" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>9 0.44514573 <a title="27-lda-9" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>10 0.44334695 <a title="27-lda-10" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>11 0.44004685 <a title="27-lda-11" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>12 0.43978256 <a title="27-lda-12" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>13 0.43918437 <a title="27-lda-13" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>14 0.43881589 <a title="27-lda-14" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>15 0.43803516 <a title="27-lda-15" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>16 0.43776095 <a title="27-lda-16" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>17 0.43758371 <a title="27-lda-17" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>18 0.43668532 <a title="27-lda-18" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>19 0.43612429 <a title="27-lda-19" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>20 0.43543875 <a title="27-lda-20" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
