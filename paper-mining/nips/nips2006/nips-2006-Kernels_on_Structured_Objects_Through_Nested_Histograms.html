<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2006-Kernels on Structured Objects Through Nested Histograms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-103" href="#">nips2006-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2006-Kernels on Structured Objects Through Nested Histograms</h1>
<br/><p>Source: <a title="nips-2006-103-pdf" href="http://papers.nips.cc/paper/3056-kernels-on-structured-objects-through-nested-histograms.pdf">pdf</a></p><p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>Reference: <a title="nips-2006-103-reference" href="../nips2006_reference/nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. [sent-3, score-0.483]
</p><p>2 However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. [sent-4, score-0.44]
</p><p>3 We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. [sent-5, score-0.986]
</p><p>4 We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. [sent-6, score-0.237]
</p><p>5 We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms. [sent-7, score-0.352]
</p><p>6 , in bioinformatics, image and text analysis or signal processing) requires more arbitrary choices, both to represent the objects in a malleable form, and to choose suitable kernels on these representations. [sent-12, score-0.424]
</p><p>7 The challenge of using kernel methods on real-world data has thus recently fostered many proposals for kernels on complex objects, notably strings, trees, images or graphs to cite a few. [sent-13, score-0.559]
</p><p>8 In common practice, most of these objects can be regarded as structured aggregates of smaller components, and the coarsest approach to study such aggregates is to consider them directly as bags of components. [sent-14, score-0.434]
</p><p>9 In the ﬁeld of kernel methods, such a representation has not only been widely adopted (Haussler, 1999; Joachims, 2002; Sch¨ lkopf et al. [sent-15, score-0.218]
</p><p>10 , 2004), but it has also spurred the proo posal of kernels better suited to the geometry of the underlying histograms (Kondor & Jebara, 2003; Lafferty & Lebanon, 2005; Hein & Bousquet, 2005; Cuturi et al. [sent-16, score-0.653]
</p><p>11 While this viewpoint may translate into adequate properties for some learning tasks, such as translation or rotation invariance when using histograms of colors to manipulate images (Chapelle et al. [sent-19, score-0.589]
</p><p>12 As one would expect, these histograms are usually a sparse and need to be regularized using ad-hoc rules and prior knowledge (Leslie et al. [sent-22, score-0.394]
</p><p>13 , 2003) before being directly compared using kernels on histograms. [sent-23, score-0.259]
</p><p>14 ) kernels if particular care is taken to adapt them (Vert et al. [sent-27, score-0.305]
</p><p>15 2  t2  Figure 1: From the bag of components representation to a set of nested bags, using a set of labels. [sent-34, score-0.098]
</p><p>16 in this paper new families of kernels which can be easily tuned to detect both coarse and ﬁne similarities between the objects, in a range spanned from kernels which only consider coarse histograms to kernels which only detect strict local matches. [sent-35, score-1.545]
</p><p>17 To size such types of similarities between two objects, we elaborate on the elementary bag-of-components perspective to consider instead families of nested histograms (indexed by a set of hierarchical labels to be deﬁned) to describe each object. [sent-36, score-0.688]
</p><p>18 In this framework, the root label corresponds to the global representation introduced before, while longer labels represent a speciﬁc condition under which the components have been sampled. [sent-37, score-0.092]
</p><p>19 We then deﬁne kernels that take into account mixtures of similarities, spanning from detailed resolutions which only compare the smallest bags to the coarsest one. [sent-38, score-0.556]
</p><p>20 This trade-off between ﬁne and coarse perspectives sets an averaging framework to deﬁne kernels, which we introduce formally in Section 2. [sent-39, score-0.142]
</p><p>21 This theoretical framework would not be tractable without an efﬁcient factorization detailed in Section 3 which yields computations which grow linearly in time and space with respect to the number of labels to evaluate the value of the kernel. [sent-40, score-0.091]
</p><p>22 We then provide experimental results in Section 4 on an image retrieval task which shows that the methodology improves the performance of kernel based state-of-the art techniques in this ﬁeld with a low extra computational cost. [sent-41, score-0.265]
</p><p>23 2 Kernels Deﬁned through Hierarchies of Histograms In the kernel literature, structured objects are usually represented as histograms of components, e. [sent-42, score-0.67]
</p><p>24 , images as histograms of colors and/or features, texts as bags of words and sequences as histograms of letters or n-grams. [sent-44, score-1.017]
</p><p>25 One may instead create families of histograms, indexed by speciﬁc sampling conditions: • In image analysis, create color or feature histograms following a prior partition of the image into predeﬁned patches, as in (Grauman & Darrell, 2005). [sent-46, score-0.806]
</p><p>26 Another possibility would be to deﬁne families of histograms, all for the same image, which would consider increasingly granular discretizations of the color space. [sent-47, score-0.21]
</p><p>27 • In sequence analysis, extract local histograms which may correspond to predeﬁned regions of the original sequence, as in (Matsuda et al. [sent-48, score-0.394]
</p><p>28 by considering the 26 histogram of letters sampled just after the letters {A, B, · · · , Z}, or the 26 × 26 histograms of letters after contexts {AA, AB, · · · , ZZ}. [sent-52, score-0.62]
</p><p>29 • In text analysis, use histograms of words found after grammatical categories of increasing complexity, such as verbs, nouns, articles or adverbs. [sent-53, score-0.417]
</p><p>30 an index or a speciﬁc gene) and decompose each of the subsequent series into histograms of values conditioned to the value of the reference series. [sent-58, score-0.388]
</p><p>31 Structured objects are thus def b represented as a family µ of ML (X ) = (M+ (X ))L , that is µ = {µt }t∈L where for each t ∈ L, µt b is a bounded measure of M+ (X ). [sent-60, score-0.349]
</p><p>32 1 Local Similarities Between Measures To compare two objects under the light of any sampling condition t, that is comparing their respecb tive decompositions as measures µt and µt , we make use of an arbitrary p. [sent-63, score-0.144]
</p><p>33 kernel k on M+ (X ) to which we will refer as the base kernel throughout the paper. [sent-65, score-0.407]
</p><p>34 For interpretation purposes only, we will assume in the following sections that k is an inﬁnitely divisible kernel which can be written 1 b as k = e− λ ψ , λ > 0, where ψ is a negative deﬁnite (Berg et al. [sent-66, score-0.218]
</p><p>35 , 1984) kernel on M+ (X ), or equivalently −ψ is a conditionally p. [sent-67, score-0.172]
</p><p>36 For two elements µ, µ of ML (X ) and a given element t ∈ L, the kernel def kt (µ, µ ) = k(µt , µt ) quantiﬁes the similarity of µ and µ by measuring how similarly their components were observed with respect to label t. [sent-73, score-0.623]
</p><p>37 For two different labels s and t of L, ks and kt can be associated through polynomial combinations with positive coefﬁcients to result in new kernels, notably their sum ks +kt or their product ks kt . [sent-74, score-0.668]
</p><p>38 This is particularly adequate if some complementarity is assumed between s and t, so that their combination can provide new insights for a given learning task. [sent-75, score-0.092]
</p><p>39 If on the contrary these labels are assumed to be similar, then they can be regarded as a grouped label {s} ∪ {t} and result in the kernel def k{s}∪{t} (µ, µ ) = k(µs + µt , µs + µt ), which will measure the similarity of m and m under both s or t labels. [sent-76, score-0.507]
</p><p>40 Let us give an intuition for this deﬁnition by considering two texts A, B built up with words from a dictionary D. [sent-77, score-0.155]
</p><p>41 As an b alternative to the general histograms of words θA and θB of M+ (D), one may consider for instance A A B B θcan , θmay and θcan , θmay , the respective histograms of words that follow the words can and may in texts A and B respectively. [sent-78, score-0.883]
</p><p>42 If one considers that can and may are different words, then the following kernel quantiﬁes the similarity of A and B taking advantage of this difference: B A B A k{can},{may} (A, B) = k(θcan , θcan ) × k(θmay , θmay ). [sent-79, score-0.212]
</p><p>43 If on the contrary one decides that can and may are equivalent, an adequate kernel would ﬁrst merge the histograms, and then compare them: A A B B k{can,may} (A, B) = k(θcan + θmay , θcan + θmay ). [sent-80, score-0.264]
</p><p>44 The previous formula can be naturally extended to deﬁne kernels indexed on a set T ⊂ L of grouped labels, through def  def  def  kT (µ, µ ) = k (µT , µT ) , where µT =  µt and µT = t∈ T  µt . [sent-81, score-0.838]
</p><p>45 Let P be a ﬁnite partition of L, that is a ﬁnite family P = (T1 , . [sent-84, score-0.212]
</p><p>46 We write P(L) for the set of all partitions of L. [sent-88, score-0.163]
</p><p>47 Consider now the kernel deﬁned by a partition P as n  def  kP (µ, µ ) =  kTi (µ, µ ). [sent-89, score-0.475]
</p><p>48 i=1  (1)  The kernel kP quantiﬁes the similarity between two objects by detecting their joint similarity under all possible labels of L, assuming a priori that certain labels can be grouped together, following the subsets Ti enumerated in the partition P . [sent-90, score-0.699]
</p><p>49 Note that there is some arbitrary in this deﬁnition since a simple multiplication of base kernels kTi is used to deﬁne kP , rather than any other polynomial combination. [sent-91, score-0.322]
</p><p>50 We follow in that sense the convolution kernels (Haussler, 1999) approach, and indeed, for each partition P , kP can be regarded as a convolution kernel. [sent-92, score-0.541]
</p><p>51 More precisely, the multiplicative structure of Equation (1) quantiﬁes how similar two objects are given a partition P , in a way that imposes for the objects to be similar according to all subsets Ti . [sent-93, score-0.358]
</p><p>52 Figure 2: A useful set of labels L for images which would focus on pixel localization can be represented by a grid, such as the 8 × 8 one represented above. [sent-95, score-0.115]
</p><p>53 Any partition 3 P of the image which complies with the hierarchy P0 in the example above, can in turn be used to represent an image as a family of sub-probability measures, which reduces in the case of two-color images to binary histograms as illustrated in the right-most image. [sent-97, score-0.871]
</p><p>54 For two images, these respective histograms can be directly compared through the kernel kP . [sent-98, score-0.52]
</p><p>55 As illustrated in Figure 2, where images are summarized through histograms indexed by patches, a partition of L reﬂects a given belief on how patches may or may not be associated or split to focus on local dissimilarities. [sent-99, score-0.601]
</p><p>56 Hence, all partitions contained in the set P(L) of all possible partitions1 are not likely to be equally meaningful given that some labels may a natural form of grouping. [sent-100, score-0.254]
</p><p>57 If one uses a Markovian analysis, that is consider histograms of components conditioned by contexts, a natural way to group contexts would be to group them according to their semantic or grammatical content for text analysis or according to their sufﬁx for sequence analysis. [sent-102, score-0.464]
</p><p>58 Such meaningful partitions can be intuitively obtained when a hierarchical structure which groups elements of L together is known a priori. [sent-103, score-0.204]
</p><p>59 A hierarchy on L, such as the triadic hierarchy shown in Figure 3, is a family (Pd )D = {P0 = {L}, . [sent-104, score-0.396]
</p><p>60 To provide a hierarchical information, the family (Pd )D is such that any subset d=0 present in a partition Pd is strictly included in a (unique by deﬁnition of a partition) subset from the coarser partition Pd−1 . [sent-107, score-0.409]
</p><p>61 This is equivalent to stating that each subset T in a partition Pd is divided in Pd+1 as a partition of T which is not T itself. [sent-108, score-0.276]
</p><p>62 Consider now the subset PD ⊂ P(L) of all partitions of L obtained by using only sets contained in the collection def  D  def  D D P0 = d=0 Pd , namely PD = {P ∈ P(L) s. [sent-112, score-0.493]
</p><p>63 The set PD contains both the coarsest and the ﬁnest resolutions, respectively P0 and PD , but also all variable resolutions for sets D enumerated in P0 , as can be seen for instance in the third image of Figure 2. [sent-115, score-0.357]
</p><p>64 1  È  P(L) is quite a big space, since if L is a ﬁnite set of cardinal r, the cardinal of the set of partitions is known r as the Bell Number of order r with Br = 1 ∞ u ∼ er ln r . [sent-116, score-0.249]
</p><p>65 e r→∞  11  1  2  3  4  5  6  7  8  9  19 61  0  73  P0  P1  99  P2  Figure 3: A hierarchy generated by two successive triadic partitions. [sent-118, score-0.186]
</p><p>66 3 Averaging Resolution Speciﬁc Kernels Each partition P contained in PD provides a resolution to compare two objects, which generates a large family of kernels kP when P spans PD . [sent-120, score-0.51]
</p><p>67 Some partitions are likely to be better suited for certain tasks, which may call for an efﬁcient estimation scheme to select an optimal partition for a given task. [sent-121, score-0.301]
</p><p>68 We take in this section a different direction which has a more Bayesian ﬂavor by considering an averaging of such kernels based on a prior on the set of partitions. [sent-123, score-0.364]
</p><p>69 In practice, this averaging favours objects which share similarities under a large collection of resolutions, and may also be interpreted as a Bayesian averaging of convolution kernels (Haussler, 1999). [sent-124, score-0.641]
</p><p>70 Deﬁnition 1 Let L be an index set endowed with a hierarchy (Pd )D , π be a prior measure on the d=0 b b corresponding set of partitions PD and k a base kernel on M+ (X ) × M+ (X ). [sent-125, score-0.609]
</p><p>71 The averaged kernel kπ on ML (X ) × ML (X ) is deﬁned as kπ (µ, µ ) =  π(P ) kP (µ, µ ). [sent-126, score-0.172]
</p><p>72 (2)  P ∈ PD  As can be observed in Equation (2), the kernel automatically detects in the range of all partitions the ones which provide a good match between the compared objects, to increase subsequently the resulting similarity score. [sent-127, score-0.375]
</p><p>73 Also note that in an image-analysis context, the pyramid-matching kernel proposed in (Grauman & Darrell, 2005) only considers the original partitions of the hierarchy (Pd )D , while Equation (2) considers all possible partitions of PD . [sent-128, score-0.634]
</p><p>74 This can be carried out with d=0 little cost if an adequate set of priors π is selected as seen below. [sent-129, score-0.092]
</p><p>75 1 Partitions Generated by Branching Processes All partitions P of PD can be generated through the following rule, starting from the initial root partition P := P0 = {L}. [sent-132, score-0.301]
</p><p>76 either replace it by its siblings in s(T ) with probability εT , and reapply this rule to each sibling unless they belong to the ﬁnest partition PD . [sent-135, score-0.231]
</p><p>77 For a partition P ∈ PD , ◦  D T } gathers π(P ) = T ∈ P (1 − εT ) ◦ (εT ), where the set P = {T ∈ P0 s. [sent-137, score-0.138]
</p><p>78 ∃V ∈ P, V T∈P all coarser sets belonging to coarser resolutions than P , and can be regarded as the set of all ancestors D in P0 of sets enumerated in P . [sent-139, score-0.358]
</p><p>79 KT = (1 − εT )kT (µ, µ ) + εT Proof The proof follows from a factorization which uses the branching process prior used for the tree generation, and can be derived from the proof of (Catoni, 2004, Proposition 5. [sent-145, score-0.101]
</p><p>80 The opposite ﬁgure underlines the importance of incorporating to each node KT a weighted product of the sibling kernel evaluations KU . [sent-147, score-0.215]
</p><p>81 The update rule for the computation of kπ takes into account the branching process prior by weighting the kernel kT with all values kti obtained for ﬁner resolutions ti in s(T ). [sent-148, score-0.54]
</p><p>82 This complexity is also upperbounded by the total amount of components considered in the compared objects, as in (Cuturi & Vert, 2005) for instance. [sent-151, score-0.107]
</p><p>83 3 Choosing the Base Kernel b Any kernel on M+ (X ) can be used to comply with the terms of Deﬁnition 1 and apply an average scheme on families of measures. [sent-153, score-0.275]
</p><p>84 We also note that an even more general formulation can be obtained by using a different kernel kt for each label t of L, without altering the overall applicability of the factorization above. [sent-154, score-0.417]
</p><p>85 First, one can note that kernels such as the information diffusion kernel (Lafferty & Lebanon, 2005) and variance based kernels (Kondor & Jebara, 2003; Cuturi et al. [sent-156, score-0.736]
</p><p>86 The most adequate b geometry of M+ (X ), following the denormalization scheme proposed in (Amari & Nagaoka, 2001, √ p. [sent-160, score-0.092]
</p><p>87 More generally, one can consider the whole family of kernels for bounded measures described in (Hein & Bousquet, 1 2005) to choose the base kernel k, namely the family of Hilbertian metrics ψ such that k = e− λ ψ . [sent-162, score-0.676]
</p><p>88 4 Experiments in Image Retrieval We present in this section experiments inspired by the image retrieval task ﬁrst considered in (Chapelle et al. [sent-164, score-0.139]
</p><p>89 Our dataset was also extracted from the Corel Stock database and includes 12 families of labeled images, each class containing  2  1 log2 ( λ )  0  -6  -12 0  1/2 ε  1  0. [sent-166, score-0.103]
</p><p>90 13  Figure 4: Misclassiﬁcation rate on the corel experiment, using the Hellinger H1 distance between 1 histograms coupled with one-vs-all SVM classiﬁcation (C = 100) as a function of λ and ε. [sent-177, score-0.387]
</p><p>91 ε controls the granularity of the averaging kernel, ranging from the coarsest perspective (ε = 0) when only the global histogram is used, to the ﬁnest one (ε = 1) when only the ﬁnest histograms are considered. [sent-181, score-0.583]
</p><p>92 3% respectively 100 color images of 256 × 384 pixels. [sent-186, score-0.122]
</p><p>93 The families depict images of bears, African specialty animals, monkeys, cougars, ﬁreworks, mountains, ofﬁce interiors, bonsais, sunsets, clouds, apes and rocks and gems. [sent-187, score-0.168]
</p><p>94 We used 9 bits for the color of each pixel to reduce the size of the RGB color space to 83 = 512 from the original set of 2563 = 16, 777, 216 colors, and we deﬁned centered grids of 4, 42 = 16 and 43 = 64 local patches. [sent-193, score-0.114]
</p><p>95 We provide results for each of the 5 considered kernels and for each considered depth D ranging from 1 to 3. [sent-194, score-0.259]
</p><p>96 By considering values of ε ranging from 0 to 1, we aim at giving a sketch of the robustness of the averaging approach, since the SVM’s seem to perform better when 0 < ε < 1 for a large span of λ values. [sent-197, score-0.105]
</p><p>97 Finally, the Gaussian kernel was also tested but its very poor performance (with error rate above 22% for all parameters) illustrates once more that the Gaussian kernel is usually a poor choice to compare histograms directly. [sent-199, score-0.692]
</p><p>98 , 2004), although we do not aim here at learning linear combinations of the kernels kT , but rather start from an hierarchical belief on them to propose an algebraic combination. [sent-205, score-0.259]
</p><p>99 H1  H2  TV  Xi2  JD  D=1  D=2  D=3  Figure 5: Error-rate results for different kernels and depths are displayed in the same way that in Figure 4, using the same colorscale across experiments. [sent-207, score-0.259]
</p><p>100 Hilbertian metrics and positive deﬁnite kernels on probability measures. [sent-265, score-0.259]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pd', 0.428), ('histograms', 0.348), ('kernels', 0.259), ('kt', 0.204), ('kernel', 0.172), ('def', 0.165), ('partitions', 0.163), ('cuturi', 0.149), ('kp', 0.147), ('partition', 0.138), ('hierarchy', 0.136), ('resolutions', 0.129), ('vert', 0.121), ('objects', 0.11), ('coarsest', 0.108), ('ti', 0.104), ('families', 0.103), ('similarities', 0.093), ('adequate', 0.092), ('nest', 0.086), ('texts', 0.079), ('coarse', 0.077), ('kti', 0.075), ('matsuda', 0.075), ('family', 0.074), ('ku', 0.073), ('darrell', 0.073), ('grauman', 0.073), ('ml', 0.065), ('lebanon', 0.065), ('enumerated', 0.065), ('upperbounded', 0.065), ('images', 0.065), ('averaging', 0.065), ('notably', 0.063), ('base', 0.063), ('histogram', 0.062), ('hein', 0.06), ('branching', 0.06), ('bags', 0.06), ('haussler', 0.06), ('coarser', 0.059), ('color', 0.057), ('nested', 0.056), ('kondor', 0.055), ('image', 0.055), ('bousquet', 0.053), ('jebara', 0.052), ('labels', 0.05), ('indexed', 0.05), ('akutsu', 0.05), ('catoni', 0.05), ('granular', 0.05), ('minato', 0.05), ('nagaoka', 0.05), ('saigo', 0.05), ('siblings', 0.05), ('triadic', 0.05), ('ks', 0.049), ('convolution', 0.049), ('chapelle', 0.049), ('regarded', 0.046), ('et', 0.046), ('protein', 0.044), ('letters', 0.043), ('hilbertian', 0.043), ('fukumizu', 0.043), ('cardinal', 0.043), ('sibling', 0.043), ('hellinger', 0.043), ('jd', 0.043), ('quanti', 0.042), ('components', 0.042), ('contexts', 0.041), ('factorization', 0.041), ('meaningful', 0.041), ('similarity', 0.04), ('structured', 0.04), ('considering', 0.04), ('index', 0.04), ('spans', 0.039), ('corel', 0.039), ('hierarchies', 0.039), ('retrieval', 0.038), ('elementary', 0.038), ('colors', 0.038), ('contextual', 0.037), ('sonnenburg', 0.037), ('lafferty', 0.037), ('words', 0.036), ('aggregates', 0.035), ('leslie', 0.035), ('tsuda', 0.035), ('endowed', 0.035), ('detect', 0.035), ('measures', 0.034), ('grouped', 0.034), ('alignment', 0.033), ('tokyo', 0.033), ('grammatical', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="103-tfidf-1" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>2 0.15085962 <a title="103-tfidf-2" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>3 0.14323731 <a title="103-tfidf-3" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>Author: Daniel M. Roy, Charles Kemp, Vikash K. Mansinghka, Joshua B. Tenenbaum</p><p>Abstract: The objects in many real-world domains can be organized into hierarchies, where each internal node picks out a category of objects. Given a collection of features and relations deﬁned over a set of objects, an annotated hierarchy includes a speciﬁcation of the categories that are most useful for describing each individual feature and relation. We deﬁne a generative model for annotated hierarchies and the features and relations that they describe, and develop a Markov chain Monte Carlo scheme for learning annotated hierarchies. We show that our model discovers interpretable structure in several real-world data sets.</p><p>4 0.13121384 <a title="103-tfidf-4" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>5 0.11918677 <a title="103-tfidf-5" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>Author: Yonatan Amit, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: We describe and analyze an algorithmic framework for online classiﬁcation where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online hypothesis by deﬁning a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution for the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with online multiclass text categorization. Our experiments indicate that a combination of class-dependent features with the simultaneous projection method outperforms previously studied algorithms. 1</p><p>6 0.11020688 <a title="103-tfidf-6" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>7 0.095257983 <a title="103-tfidf-7" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>8 0.085481636 <a title="103-tfidf-8" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>9 0.083739288 <a title="103-tfidf-9" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>10 0.082733646 <a title="103-tfidf-10" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>11 0.082443103 <a title="103-tfidf-11" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>12 0.080413289 <a title="103-tfidf-12" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>13 0.078039549 <a title="103-tfidf-13" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>14 0.077796146 <a title="103-tfidf-14" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>15 0.075410895 <a title="103-tfidf-15" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>16 0.074847654 <a title="103-tfidf-16" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>17 0.073064677 <a title="103-tfidf-17" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>18 0.071065664 <a title="103-tfidf-18" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>19 0.070130154 <a title="103-tfidf-19" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>20 0.067863241 <a title="103-tfidf-20" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.222), (1, 0.078), (2, 0.048), (3, 0.053), (4, 0.051), (5, 0.046), (6, -0.09), (7, -0.062), (8, 0.039), (9, -0.099), (10, -0.034), (11, 0.063), (12, 0.037), (13, -0.093), (14, 0.035), (15, -0.04), (16, -0.099), (17, 0.023), (18, -0.116), (19, 0.004), (20, -0.202), (21, 0.018), (22, -0.18), (23, -0.091), (24, 0.126), (25, 0.064), (26, 0.01), (27, -0.02), (28, 0.019), (29, 0.075), (30, 0.006), (31, -0.011), (32, -0.099), (33, -0.058), (34, 0.107), (35, -0.092), (36, 0.262), (37, -0.056), (38, -0.079), (39, -0.029), (40, -0.025), (41, -0.084), (42, 0.101), (43, -0.012), (44, -0.134), (45, 0.074), (46, 0.097), (47, 0.037), (48, -0.027), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94767797 <a title="103-lsi-1" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>2 0.55409276 <a title="103-lsi-2" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>3 0.5505439 <a title="103-lsi-3" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>Author: Risi Kondor, Tony Jebara</p><p>Abstract: We propose a new method for constructing hyperkenels and deﬁne two promising special cases that can be computed in closed form. These we call the Gaussian and Wishart hyperkernels. The former is especially attractive in that it has an interpretable regularization scheme reminiscent of that of the Gaussian RBF kernel. We discuss how kernel learning can be used not just for improving the performance of classiﬁcation and regression methods, but also as a stand-alone algorithm for dimensionality reduction and relational or metric learning. 1</p><p>4 0.48333645 <a title="103-lsi-4" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>Author: Daniel M. Roy, Charles Kemp, Vikash K. Mansinghka, Joshua B. Tenenbaum</p><p>Abstract: The objects in many real-world domains can be organized into hierarchies, where each internal node picks out a category of objects. Given a collection of features and relations deﬁned over a set of objects, an annotated hierarchy includes a speciﬁcation of the categories that are most useful for describing each individual feature and relation. We deﬁne a generative model for annotated hierarchies and the features and relations that they describe, and develop a Markov chain Monte Carlo scheme for learning annotated hierarchies. We show that our model discovers interpretable structure in several real-world data sets.</p><p>5 0.46112201 <a title="103-lsi-5" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>Author: Konrad Rieck, Pavel Laskov, Sören Sonnenburg</p><p>Abstract: We propose a generic algorithm for computation of similarity measures for sequential data. The algorithm uses generalized sufﬁx trees for efﬁcient calculation of various kernel, distance and non-metric similarity functions. Its worst-case run-time is linear in the length of sequences and independent of the underlying embedding language, which can cover words, k-grams or all contained subsequences. Experiments with network intrusion detection, DNA analysis and text processing applications demonstrate the utility of distances and similarity coefﬁcients for sequences as alternatives to classical kernel functions.</p><p>6 0.45780241 <a title="103-lsi-6" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>7 0.44922245 <a title="103-lsi-7" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>8 0.44023791 <a title="103-lsi-8" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>9 0.41818139 <a title="103-lsi-9" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>10 0.41763821 <a title="103-lsi-10" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>11 0.38507736 <a title="103-lsi-11" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>12 0.38417482 <a title="103-lsi-12" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>13 0.38321385 <a title="103-lsi-13" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>14 0.36960819 <a title="103-lsi-14" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>15 0.3580977 <a title="103-lsi-15" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>16 0.35762027 <a title="103-lsi-16" href="./nips-2006-Modelling_transcriptional_regulation_using_Gaussian_Processes.html">135 nips-2006-Modelling transcriptional regulation using Gaussian Processes</a></p>
<p>17 0.3444235 <a title="103-lsi-17" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>18 0.3433117 <a title="103-lsi-18" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>19 0.33669791 <a title="103-lsi-19" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>20 0.33341765 <a title="103-lsi-20" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.078), (3, 0.014), (7, 0.061), (9, 0.026), (20, 0.016), (22, 0.038), (44, 0.047), (57, 0.062), (65, 0.543), (69, 0.014), (90, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94517004 <a title="103-lda-1" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>2 0.89511287 <a title="103-lda-2" href="./nips-2006-No-regret_Algorithms_for_Online_Convex_Programs.html">146 nips-2006-No-regret Algorithms for Online Convex Programs</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Online convex programming has recently emerged as a powerful primitive for designing machine learning algorithms. For example, OCP can be used for learning a linear classiﬁer, dynamically rebalancing a binary search tree, ﬁnding the shortest path in a graph with unknown edge lengths, solving a structured classiﬁcation problem, or ﬁnding a good strategy in an extensive-form game. Several researchers have designed no-regret algorithms for OCP. But, compared to algorithms for special cases of OCP such as learning from expert advice, these algorithms are not very numerous or ﬂexible. In learning from expert advice, one tool which has proved particularly valuable is the correspondence between no-regret algorithms and convex potential functions: by reasoning about these potential functions, researchers have designed algorithms with a wide variety of useful guarantees such as good performance when the target hypothesis is sparse. Until now, there has been no such recipe for the more general OCP problem, and therefore no ability to tune OCP algorithms to take advantage of properties of the problem or data. In this paper we derive a new class of no-regret learning algorithms for OCP. These Lagrangian Hedging algorithms are based on a general class of potential functions, and are a direct generalization of known learning rules like weighted majority and external-regret matching. In addition to proving regret bounds, we demonstrate our algorithms learning to play one-card poker. 1</p><p>3 0.88755387 <a title="103-lda-3" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>Author: Ling Li, Hsuan-tien Lin</p><p>Abstract: We present a reduction framework from ordinal regression to binary classiﬁcation based on extended examples. The framework consists of three steps: extracting extended examples from the original examples, learning a binary classiﬁer on the extended examples with any binary classiﬁcation algorithm, and constructing a ranking rule from the binary classiﬁer. A weighted 0/1 loss of the binary classiﬁer would then bound the mislabeling cost of the ranking rule. Our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classiﬁcation approaches, but also to derive new generalization bounds for ordinal regression from known bounds for binary classiﬁcation. In addition, our framework uniﬁes many existing ordinal regression algorithms, such as perceptron ranking and support vector ordinal regression. When compared empirically on benchmark data sets, some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms, which demonstrates the usefulness of our framework. 1</p><p>4 0.87893826 <a title="103-lda-4" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>Author: Ashutosh Saxena, Justin Driemeyer, Justin Kearns, Andrew Y. Ng</p><p>Abstract: We consider the problem of grasping novel objects, speciﬁcally ones that are being seen for the ﬁrst time through vision. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Instead it predicts, directly as a function of the images, a point at which to grasp the object. Our algorithm is trained via supervised learning, using synthetic images for the training set. We demonstrate on a robotic manipulation platform that this approach successfully grasps a wide variety of objects, such as wine glasses, duct tape, markers, a translucent box, jugs, knife-cutters, cellphones, keys, screwdrivers, staplers, toothbrushes, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. 1</p><p>5 0.87047642 <a title="103-lda-5" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>6 0.56875336 <a title="103-lda-6" href="./nips-2006-implicit_Online_Learning_with_Kernels.html">203 nips-2006-implicit Online Learning with Kernels</a></p>
<p>7 0.54191458 <a title="103-lda-7" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>8 0.53461206 <a title="103-lda-8" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>9 0.53381771 <a title="103-lda-9" href="./nips-2006-An_Approach_to_Bounded_Rationality.html">26 nips-2006-An Approach to Bounded Rationality</a></p>
<p>10 0.52502203 <a title="103-lda-10" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>11 0.5047431 <a title="103-lda-11" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>12 0.50137889 <a title="103-lda-12" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>13 0.48943546 <a title="103-lda-13" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>14 0.48739859 <a title="103-lda-14" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>15 0.48661989 <a title="103-lda-15" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>16 0.48214722 <a title="103-lda-16" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>17 0.479224 <a title="103-lda-17" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>18 0.47833061 <a title="103-lda-18" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>19 0.47788283 <a title="103-lda-19" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>20 0.47775263 <a title="103-lda-20" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
