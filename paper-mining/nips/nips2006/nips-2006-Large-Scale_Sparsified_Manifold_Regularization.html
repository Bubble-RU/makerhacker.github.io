<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2006-Large-Scale Sparsified Manifold Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-104" href="#">nips2006-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2006-Large-Scale Sparsified Manifold Regularization</h1>
<br/><p>Source: <a title="nips-2006-104-pdf" href="http://papers.nips.cc/paper/3005-large-scale-sparsified-manifold-regularization.pdf">pdf</a></p><p>Author: Ivor W. Tsang, James T. Kwok</p><p>Abstract: Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. By using a sparsiﬁed manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns. 1</p><p>Reference: <a title="nips-2006-104-reference" href="../nips2006_reference/nips-2006-Large-Scale_Sparsified_Manifold_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. [sent-5, score-0.524]
</p><p>2 In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. [sent-6, score-0.313]
</p><p>3 However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. [sent-7, score-0.545]
</p><p>4 Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. [sent-8, score-0.308]
</p><p>5 In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. [sent-9, score-0.342]
</p><p>6 By using a sparsiﬁed manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. [sent-10, score-0.34]
</p><p>7 Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns. [sent-11, score-0.462]
</p><p>8 1  Introduction  In many real-world applications, collection of labeled data is both time-consuming and expensive. [sent-12, score-0.134]
</p><p>9 On the other hand, a large amount of unlabeled data are often readily available. [sent-13, score-0.288]
</p><p>10 While traditional supervised learning methods can only learn from the limited amount of labeled data, semi-supervised learning [2] aims at improving the generalization performance by utilizing both the labeled and unlabeled data. [sent-14, score-0.593]
</p><p>11 When the data lie on a manifold, it is common to approximate this manifold by a weighted graph, leading to graph-based semi-supervised learning methods. [sent-17, score-0.192]
</p><p>12 In this paper, we focus on the manifold regularization frameo work proposed in [1]. [sent-20, score-0.249]
</p><p>13 By deﬁning a data-dependent reproducing kernel Hilbert space (RKHS), manifold regularization incorporates an additional regularizer to ensure that the learned function is smooth on the manifold. [sent-21, score-0.391]
</p><p>14 Moreover, while the original motivation of semi-supervised learning is to utilize the large amount of unlabeled data available, existing algorithms are only capable of handling a small to moderate amount of unlabeled data. [sent-25, score-0.576]
</p><p>15 [9] speeded up manifold regularization by restraining to linear models, which, however, may  not be ﬂexible enough for complicated target functions. [sent-28, score-0.249]
</p><p>16 As reported in a recent survey [14], most semisupervised learning methods can only handle 100 – 10,000 unlabeled examples. [sent-31, score-0.332]
</p><p>17 The largest graph a they worked with involve 75,888 labeled and unlabeled examples. [sent-34, score-0.425]
</p><p>18 Thus, no one has ever been experimented on massive data sets with, say, one million unlabeled examples. [sent-35, score-0.359]
</p><p>19 On the other hand, the Core Vector Machine (CVM) is recently proposed for scaling up kernel methods in both supervised (including classiﬁcation [12] and regression [13]) and unsupervised learning (e. [sent-36, score-0.101]
</p><p>20 Experimental results on real world data sets with millions of patterns demonstrated that the CVM is much faster than existing SVM implementations and can handle much larger data sets. [sent-41, score-0.119]
</p><p>21 To restore sparsity of the LapSVM solution, we ﬁrst introduce a sparsiﬁed manifold regularizer based on the -insensitive loss. [sent-43, score-0.313]
</p><p>22 In Section 2, we ﬁrst give a brief review on manifold regularization. [sent-47, score-0.192]
</p><p>23 2  Manifold Regularization  Given a training set {(xi , yi )}m with input xi ∈ X and output yi ∈ R. [sent-50, score-0.238]
</p><p>24 The regularized risk i=1 functional is the sum of the empirical risk (corresponding to a loss function ) and a regularizer Ω. [sent-51, score-0.163]
</p><p>25 Given a kernel k and its RKHS Hk , we minimize the regularized risk over function f in Hk : min  f ∈Hk  1 m  m  (xi , yi , f (xi )) + λΩ( f  (1)  Hk ). [sent-52, score-0.156]
</p><p>26 In semi-supervised learning, we have both labeled examples {(xi , yi )}m and unlabeled examples i=1 {xi }m+n . [sent-56, score-0.539]
</p><p>27 Manifold regularization uses an additional regularizer f 2 to ensure that the function I i=m+1 f is smooth on the intrinsic structure of the input. [sent-57, score-0.135]
</p><p>28 It is common to approximate I this manifold by a weighted graph deﬁned on all the labeled and unlabeled data, as G = (V, E) with V and E being the sets of vertices and edges respectively. [sent-62, score-0.617]
</p><p>29 Then, f 2 is approximated as1 I f  2 I  w(ue , ve )  = e∈E  1  f (xue ) f (xve ) − s(ue ) s(ve )  2  ,  (3)  When the set of labeled and unlabeled data is small, a function that is smooth on this small set may not be interesting. [sent-65, score-0.451]
</p><p>30 where ue and ve are vertices of the edge e, and s(u) = d(u) when the normalized graph Laplacian is used, and s(u) = 1 with the unnormalized one. [sent-67, score-0.272]
</p><p>31 As shown in [1], the minimizer of (2) becomes m+n f (x) = i=1 αi k(xi , x), which depends on both labeled and unlabeled examples. [sent-68, score-0.43]
</p><p>32 1  Laplacian SVM  Using the hinge loss (xi , yi , f (xi )) = max(0, 1 − yi f (xi )) in (2), we obtain the Laplacian SVM (LapSVM) [1]. [sent-70, score-0.189]
</p><p>33 , 1] , Qm×m = YJK(2λI + 2λI LK) J Y, Ym×m is the diagonal matrix with Yii = yi , K(m+n)×(m+n) is the kernel matrix over both the labeled and unlabeled data, L(m+n)×(m+n) is the graph Laplacian, and Jm×(m+n) with Jij = 1 if i = j and xi is a labeled example, and Jij = 0 otherwise. [sent-79, score-0.767]
</p><p>34 Substituting (4) into (2), we have: m  min  f ∈Hk  1 (xi , yi , f (xi ))+λI m i=1  w(ue , ve ) e∈E  f (xue ) f (xve ) − s(ue ) s(ve )  2  +λΩ( f  Hk ). [sent-91, score-0.122]
</p><p>35 ε ¯  By treating the terms inside the braces as the “loss function”, this can be regarded as regularized risk minimization and, using the standard representer theorem, the minimizer f then admits the form m+n f (x) = i=1 αi k(xi , x), same as that of the original manifold regularization. [sent-92, score-0.262]
</p><p>36 The primal  2 I  =  2 ∗ (ζe + ζe 2 )  where of the  (5)  e∈E  yi (w ϕ(xi ) + b) ≥ 1 − ε − ξi , i = 1, . [sent-96, score-0.107]
</p><p>37 Here, −U |E|µ U + Cθ I  K is the kernel matrix deﬁned using kernel k on the m labeled examples, U|E|×|E| = [ψ e ψ f + τe τf ], and Vm×|E| = [yi ϕ(xi ) ψ e + τe ]. [sent-116, score-0.262]
</p><p>38 1) requires O((m + n)2 ) kernel k(xi , xj ) evaluations, each entry in K here takes only O(1) kernel evaluations. [sent-118, score-0.128]
</p><p>39 Moreover, the primal vari¯ ables can be easily recovered from the dual variables by the KKT conditions. [sent-122, score-0.11]
</p><p>40 In particular, m m ∗ ∗ w = C i=1 βi yi ϕ(xi ) + e∈E (γe − γe )ψ e and b = C i=1 βi yi + e∈E (γe − γe )τe . [sent-123, score-0.144]
</p><p>41 Subsequently, the decision function f (x) = w ϕ(x) + b is a linear combination of k(xi , x)’s deﬁned on both the labeled and unlabeled examples, as in standard manifold regularization. [sent-124, score-0.593]
</p><p>42 ˜ ˜ ˜˜ ˜ ˜ The above formulation can be easily extended to the regression case, with the pattern output changed from ±1 to yi ∈ R, and the hinge loss replaced by the -insensitive loss. [sent-132, score-0.146]
</p><p>43 2 that the -insensitive loss achieves a similar effect as the 1 penalty in LASSO [11], which is known to produce sparse approximation. [sent-140, score-0.115]
</p><p>44 Similarly, manifold regularization ﬁnds a f that is locally smooth. [sent-148, score-0.249]
</p><p>45 While sparse approximation techniques such as basis pursuit typically use the L2 norm for the error, Girosi argued that the norm of the RKHS Hk is a better measure of smoothness. [sent-163, score-0.09]
</p><p>46 K  First, consider the simpler case where the manifold regularizer is replaced by a simple regularizer α 2 . [sent-172, score-0.348]
</p><p>47 On the other hand, consider the following variant of SVR  m 2 ∗2 ∗ (ξi + ξi ) + 2C ε : yi − w ϕ(xi ) ≤ ε + ξi , w ϕ(xi ) − yi ≤ ε + ξi . [sent-177, score-0.144]
</p><p>48 (12) ¯ ¯ ¯ i=1  It can be shown that its dual is identical to (11), with β, β ∗ as dual variables. [sent-178, score-0.15]
</p><p>49 As above, the key steps are on replacing the 2 norm by the kernel PCA map, and adding a 1 penalty on the variables. [sent-185, score-0.113]
</p><p>50 It can then be shown that sparsiﬁed manifold regularizer (based on the -insensitive loss) can again be recovered by using the LASSO penalty. [sent-186, score-0.27]
</p><p>51 (Here, we ignore O(m+|E|) space required for storing the m training patterns and 2|E| edge constraints, as these may be stored outside the core memory. [sent-191, score-0.109]
</p><p>52 ) They are thus independent of the numbers of labeled and unlabeled examples for a ﬁxed . [sent-192, score-0.434]
</p><p>53 This “reduced LapSVM” solves a smaller optimization problem that involves a random r×(m+n) rectangular subset of the kernel matrix, where the r patterns are chosen from both the labeled and unlabeled data. [sent-197, score-0.496]
</p><p>54 Moreover, both can be regarded as primal methods that maintain primal4 feasibility and work towards dual feasibility. [sent-202, score-0.11]
</p><p>55 Also, as is typical in column generation, the dual variable whose KKT condition is most violated is added at each iteration. [sent-203, score-0.111]
</p><p>56 Note that each dual variable then corresponds to a training pattern. [sent-208, score-0.097]
</p><p>57 Instead of requiring the dual solution to be strictly feasible, CVM only requires it to be feasible within a factor of (1 + ). [sent-213, score-0.107]
</p><p>58 This, together with the fact that its dual is a MEB problem, allows its number of iterations for convergence to be bounded and thus the total time and space complexities guaranteed. [sent-214, score-0.105]
</p><p>59 By regarding the CVM as the approximation algorithm counterpart of column generation, this suggests that the CVM can also be used in the same way as column generation in speeding up other optimization problems. [sent-216, score-0.11]
</p><p>60 The graph (for the manifold) is constructed by using the 6 nearest neighbors of each pattern, and the weight w(u e , ve ) 1 in (3) is deﬁned as exp(− xue − xve 2 /βg ), where βg = |E| e∈E xeu − xev 2 . [sent-223, score-0.226]
</p><p>61 Unless otherwise speciﬁed, m 1 ¯ we use the Gaussian kernel exp(− x − z 2 /β), with β = m i=1 xi − x 2 . [sent-226, score-0.136]
</p><p>62 To better illustrate the scaling behavior, we vary the number of unlabeled patterns used for training (from 1, 000 up to a maximum of 1 million). [sent-234, score-0.32]
</p><p>63 4  3  SLapCVM LapSVM Reduced LapSVM  2  10  number of kernel expansions  CPU time (in seconds)  10  1  10  0  10  −1  10  (a) Data distribution. [sent-238, score-0.112]
</p><p>64 3  10  4  5  10 10 number of unlabeled points  (c) CPU time. [sent-240, score-0.267]
</p><p>65 6  10  10  SLapCVM core−set Size LapSVM Reduced LapSVM  3  10  2  10 3 10  4  5  10 10 number of unlabeled points  (d) #kernel sions. [sent-241, score-0.267]
</p><p>66 6  10  expan-  Figure 1: Results on the two-moons data set (some abscissas and ordinates are in log scale). [sent-242, score-0.098]
</p><p>67 The two labeled examples are labeled in red in Figure 1(a). [sent-243, score-0.301]
</p><p>68 Both the LapSVM and SLapCVM always attain 100% accuracy on the test set, even with only two labeled examples (Figure 1(b)). [sent-245, score-0.189]
</p><p>69 and all the labeled and unlabeled examples are involved in the solution (Figure 1(d))). [sent-257, score-0.466]
</p><p>70 As can be seen from Figures 1(c) and 1(d), both the time and space required by the SLapCVM are almost constant, even when the unlabeled data set gets very large. [sent-259, score-0.267]
</p><p>71 The reduced LapSVM, though also fast, is slightly inferior to both the SLapCVM and LapSVM. [sent-260, score-0.109]
</p><p>72 One labeled example is randomly sampled from each class for training. [sent-264, score-0.134]
</p><p>73 To achieve comparable accuracy, we use r = 2, 000 for the reduced LapSVM. [sent-265, score-0.108]
</p><p>74 For comparison, we also train a standard SVM with the two labeled examples. [sent-266, score-0.134]
</p><p>75 For the SLapCVM, both the time required and number of kernel expansions involved grow only sublinearly with the number of unlabeled examples. [sent-269, score-0.379]
</p><p>76 Figure 2(c) demonstrates that semi-supervised learning (using either the LapSVMs or SLapCVM) can have much better generalization performance than supervised learning using the labeled examples only. [sent-270, score-0.226]
</p><p>77 On the other hand, the reduced LapSVM has comparable speed with the SLapCVM, but its performance is inferior and cannot handle large data sets. [sent-272, score-0.172]
</p><p>78 10 3 10  4  5  10 10 number of unlabeled points  (b) #kernel expansions. [sent-274, score-0.267]
</p><p>79 6  10  0 3 10  4  5  10 10 number of unlabeled points  6  10  (c) Test error. [sent-275, score-0.267]
</p><p>80 Figure 2: Results on the extended USPS data set (some abscissas and ordinates are in log scale). [sent-276, score-0.127]
</p><p>81 3  Extended MIT Face Data Set  In this section, we perform face detection using the extended MIT face database in [12]. [sent-278, score-0.091]
</p><p>82 Five labeled example are randomly sampled from each class and used in training. [sent-279, score-0.134]
</p><p>83 For comparison, we also train two SVMs: one uses the 10 labeled examples only while the other uses all the labeled examples (a total of 889,986) in the original training set of [12]. [sent-284, score-0.356]
</p><p>84 Note that the SLapCVM, using only 10 labeled examples, can attain comparable AUC and even better balanced loss than the SVM trained on the original, massive training set (Figures 3(c) and 3(d)). [sent-287, score-0.332]
</p><p>85 This clearly demonstrates the usefulness of semi-supervised learning when a large amount of unlabeled data can be utilized. [sent-288, score-0.31]
</p><p>86 On the other hand, note that the LapSVM again cannot be run with more than 3,000 unlabeled examples on our PC because of its high space requirement. [sent-289, score-0.3]
</p><p>87 2) How to handle data sets with millions of unlabeled examples? [sent-292, score-0.329]
</p><p>88 For the  number of kernel expansions  CPU time (in seconds)  10  50  3  10  2  10  SLapCVM core−set Size LapSVM Reduced LapSVM  45  4  10  3  10  40 35  1. [sent-293, score-0.112]
</p><p>89 85  0  10 3 10  2  4  10 number of unlabeled points  (a) CPU time. [sent-298, score-0.267]
</p><p>90 5  10  10 3 10  4  10 number of unlabeled points  (b) #kernel expansions. [sent-299, score-0.267]
</p><p>91 75  10 3 10  4  10 number of unlabeled points  (c) Balanced loss. [sent-302, score-0.267]
</p><p>92 7 3 10  4  10 number of unlabeled points  5  10  (d) AUC. [sent-304, score-0.267]
</p><p>93 Figure 3: Results on the extended MIT face data (some abscissas and ordinates are in log scale). [sent-305, score-0.158]
</p><p>94 ﬁrst issue, we introduce a sparsiﬁed manifold regularizer based on the -insensitive loss. [sent-306, score-0.27]
</p><p>95 For the second issue, we integrate manifold regularization with the CVM. [sent-307, score-0.249]
</p><p>96 Moreover, by avoiding the underlying matrix inversion in the original LapSVM, a sparse solution can also be recovered. [sent-309, score-0.101]
</p><p>97 Moreover, while the LapSVM can only handle several thousand unlabeled examples, the SLapCVM can handle one million unlabeled examples on the same machine. [sent-311, score-0.703]
</p><p>98 On one data set, this produces comparable or even better performance than the (supervised) CVM trained on 900K labeled examples. [sent-312, score-0.156]
</p><p>99 This clearly demonstrates the usefulness of semi-supervised learning when a large amount of unlabeled data can be utilized. [sent-313, score-0.31]
</p><p>100 Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. [sent-318, score-0.401]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lapsvm', 0.606), ('slapcvm', 0.407), ('cvm', 0.309), ('unlabeled', 0.267), ('manifold', 0.192), ('ue', 0.179), ('labeled', 0.134), ('meb', 0.114), ('reduced', 0.086), ('hk', 0.086), ('qp', 0.083), ('xve', 0.081), ('regularizer', 0.078), ('svm', 0.077), ('dual', 0.075), ('kkt', 0.072), ('yi', 0.072), ('xi', 0.072), ('xue', 0.071), ('sparsi', 0.068), ('kernel', 0.064), ('lasso', 0.062), ('regularization', 0.057), ('cpu', 0.057), ('massive', 0.057), ('core', 0.056), ('laplacian', 0.054), ('ve', 0.05), ('usps', 0.05), ('abscissas', 0.049), ('ordinates', 0.049), ('tsang', 0.049), ('expansions', 0.048), ('bonn', 0.045), ('loss', 0.045), ('sparse', 0.044), ('resultant', 0.043), ('kwok', 0.042), ('svr', 0.042), ('handle', 0.041), ('moreover', 0.039), ('generation', 0.038), ('august', 0.038), ('rkhs', 0.038), ('supervised', 0.037), ('column', 0.036), ('million', 0.035), ('primal', 0.035), ('sindhwani', 0.034), ('examples', 0.033), ('garcke', 0.033), ('jij', 0.033), ('lapsvms', 0.033), ('patns', 0.033), ('auc', 0.032), ('solution', 0.032), ('face', 0.031), ('pc', 0.031), ('transductive', 0.031), ('lk', 0.031), ('patterns', 0.031), ('complexities', 0.03), ('balanced', 0.03), ('minimizer', 0.029), ('px', 0.029), ('extended', 0.029), ('rtner', 0.028), ('faster', 0.026), ('sparser', 0.026), ('hong', 0.026), ('enclosing', 0.026), ('penalty', 0.026), ('inversion', 0.025), ('tp', 0.024), ('semisupervised', 0.024), ('kong', 0.024), ('graph', 0.024), ('norm', 0.023), ('restore', 0.023), ('inferior', 0.023), ('girosi', 0.023), ('training', 0.022), ('germany', 0.022), ('attain', 0.022), ('comparable', 0.022), ('demonstrates', 0.022), ('amount', 0.021), ('representer', 0.021), ('millions', 0.021), ('niyogi', 0.021), ('sparsity', 0.02), ('risk', 0.02), ('ij', 0.02), ('speedup', 0.02), ('figures', 0.02), ('thousand', 0.019), ('unnormalized', 0.019), ('belkin', 0.018), ('diag', 0.018), ('hand', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="104-tfidf-1" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok</p><p>Abstract: Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. By using a sparsiﬁed manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns. 1</p><p>2 0.20450212 <a title="104-tfidf-2" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Vikas Sindhwani, S. S. Keerthi</p><p>Abstract: Semi-supervised SVMs (S3 VM) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples. The associated optimization problem is non-convex. To examine the full potential of S3 VMs modulo local minima problems in current implementations, we apply branch and bound techniques for obtaining exact, globally optimal solutions. Empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely. While our current implementation is only applicable to small datasets, we discuss variants that can potentially lead to practically useful algorithms. 1</p><p>3 0.15397747 <a title="104-tfidf-3" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri</p><p>Abstract: In many modern large-scale learning applications, the amount of unlabeled data far exceeds that of labeled data. A common instance of this problem is the transductive setting where the unlabeled test points are known to the learning algorithm. This paper presents a study of regression problems in that setting. It presents explicit VC-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classiﬁcation bounds of Vapnik when applied to classiﬁcation. It also presents a new transductive regression algorithm inspired by our bound that admits a primal and kernelized closedform solution and deals efﬁciently with large amounts of unlabeled data. The algorithm exploits the position of unlabeled points to locally estimate their labels and then uses a global optimization to ensure robust predictions. Our study also includes the results of experiments with several publicly available regression data sets with up to 20,000 unlabeled examples. The comparison with other transductive regression algorithms shows that it performs well and that it can scale to large data sets.</p><p>4 0.11096213 <a title="104-tfidf-4" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>5 0.10383414 <a title="104-tfidf-5" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>Author: Wei Chu, Vikas Sindhwani, Zoubin Ghahramani, S. S. Keerthi</p><p>Abstract: Correlation between instances is often modelled via a kernel function using input attributes of the instances. Relational knowledge can further reveal additional pairwise correlations between variables of interest. In this paper, we develop a class of models which incorporates both reciprocal relational information and input attributes using Gaussian process techniques. This approach provides a novel non-parametric Bayesian framework with a data-dependent covariance function for supervised learning tasks. We also apply this framework to semi-supervised learning. Experimental results on several real world data sets verify the usefulness of this algorithm. 1</p><p>6 0.10168236 <a title="104-tfidf-6" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>7 0.095114157 <a title="104-tfidf-7" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>8 0.092381567 <a title="104-tfidf-8" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>9 0.082267962 <a title="104-tfidf-9" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>10 0.069281347 <a title="104-tfidf-10" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>11 0.068127126 <a title="104-tfidf-11" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>12 0.066722445 <a title="104-tfidf-12" href="./nips-2006-Hyperparameter_Learning_for_Graph_Based_Semi-supervised_Learning_Algorithms.html">93 nips-2006-Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</a></p>
<p>13 0.064798415 <a title="104-tfidf-13" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>14 0.064121999 <a title="104-tfidf-14" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>15 0.062914491 <a title="104-tfidf-15" href="./nips-2006-Dirichlet-Enhanced_Spam_Filtering_based_on_Biased_Samples.html">68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</a></p>
<p>16 0.059477933 <a title="104-tfidf-16" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>17 0.058280911 <a title="104-tfidf-17" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>18 0.055785555 <a title="104-tfidf-18" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>19 0.055384517 <a title="104-tfidf-19" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>20 0.052361537 <a title="104-tfidf-20" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.088), (2, -0.015), (3, 0.097), (4, -0.023), (5, 0.123), (6, -0.086), (7, 0.028), (8, 0.0), (9, 0.007), (10, -0.09), (11, -0.235), (12, -0.042), (13, -0.049), (14, 0.047), (15, 0.106), (16, 0.103), (17, -0.097), (18, -0.139), (19, 0.035), (20, -0.059), (21, -0.228), (22, 0.065), (23, 0.108), (24, -0.038), (25, 0.109), (26, -0.002), (27, -0.044), (28, -0.172), (29, -0.081), (30, -0.062), (31, 0.021), (32, 0.057), (33, -0.043), (34, -0.007), (35, 0.05), (36, 0.0), (37, -0.061), (38, -0.056), (39, 0.086), (40, -0.034), (41, -0.014), (42, 0.024), (43, -0.022), (44, -0.014), (45, -0.03), (46, 0.157), (47, 0.013), (48, -0.031), (49, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93825167 <a title="104-lsi-1" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok</p><p>Abstract: Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. By using a sparsiﬁed manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns. 1</p><p>2 0.79338658 <a title="104-lsi-2" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Vikas Sindhwani, S. S. Keerthi</p><p>Abstract: Semi-supervised SVMs (S3 VM) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples. The associated optimization problem is non-convex. To examine the full potential of S3 VMs modulo local minima problems in current implementations, we apply branch and bound techniques for obtaining exact, globally optimal solutions. Empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely. While our current implementation is only applicable to small datasets, we discuss variants that can potentially lead to practically useful algorithms. 1</p><p>3 0.72893906 <a title="104-lsi-3" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri</p><p>Abstract: In many modern large-scale learning applications, the amount of unlabeled data far exceeds that of labeled data. A common instance of this problem is the transductive setting where the unlabeled test points are known to the learning algorithm. This paper presents a study of regression problems in that setting. It presents explicit VC-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classiﬁcation bounds of Vapnik when applied to classiﬁcation. It also presents a new transductive regression algorithm inspired by our bound that admits a primal and kernelized closedform solution and deals efﬁciently with large amounts of unlabeled data. The algorithm exploits the position of unlabeled points to locally estimate their labels and then uses a global optimization to ensure robust predictions. Our study also includes the results of experiments with several publicly available regression data sets with up to 20,000 unlabeled examples. The comparison with other transductive regression algorithms shows that it performs well and that it can scale to large data sets.</p><p>4 0.57499576 <a title="104-lsi-4" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>Author: Chi-hoon Lee, Shaojun Wang, Feng Jiao, Dale Schuurmans, Russell Greiner</p><p>Abstract: We present a novel, semi-supervised approach to training discriminative random ﬁelds (DRFs) that efﬁciently exploits labeled and unlabeled training data to achieve improved accuracy in a variety of image processing tasks. We formulate DRF training as a form of MAP estimation that combines conditional loglikelihood on labeled data, given a data-dependent prior, with a conditional entropy regularizer deﬁned on unlabeled data. Although the training objective is no longer concave, we develop an efﬁcient local optimization procedure that produces classiﬁers that are more accurate than ones based on standard supervised DRF training. We then apply our semi-supervised approach to train DRFs to segment both synthetic and real data sets, and demonstrate signiﬁcant improvements over supervised DRFs in each case.</p><p>5 0.5488168 <a title="104-lsi-5" href="./nips-2006-Hyperparameter_Learning_for_Graph_Based_Semi-supervised_Learning_Algorithms.html">93 nips-2006-Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</a></p>
<p>Author: Xinhua Zhang, Wee S. Lee</p><p>Abstract: Semi-supervised learning algorithms have been successfully applied in many applications with scarce labeled data, by utilizing the unlabeled data. One important category is graph based semi-supervised learning algorithms, for which the performance depends considerably on the quality of the graph, or its hyperparameters. In this paper, we deal with the less explored problem of learning the graphs. We propose a graph learning method for the harmonic energy minimization method; this is done by minimizing the leave-one-out prediction error on labeled data points. We use a gradient based method and designed an efﬁcient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre-computation. Experimental results show that the graph learning method is effective in improving the performance of the classiﬁcation algorithm. 1</p><p>6 0.44536859 <a title="104-lsi-6" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>7 0.44417667 <a title="104-lsi-7" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>8 0.43934229 <a title="104-lsi-8" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>9 0.4321107 <a title="104-lsi-9" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>10 0.42157772 <a title="104-lsi-10" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>11 0.40251249 <a title="104-lsi-11" href="./nips-2006-Isotonic_Conditional_Random_Fields_and_Local_Sentiment_Flow.html">101 nips-2006-Isotonic Conditional Random Fields and Local Sentiment Flow</a></p>
<p>12 0.39063567 <a title="104-lsi-12" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>13 0.3837668 <a title="104-lsi-13" href="./nips-2006-Multi-Instance_Multi-Label_Learning_with_Application_to_Scene_Classification.html">136 nips-2006-Multi-Instance Multi-Label Learning with Application to Scene Classification</a></p>
<p>14 0.37837824 <a title="104-lsi-14" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>15 0.37554279 <a title="104-lsi-15" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>16 0.34465599 <a title="104-lsi-16" href="./nips-2006-Dirichlet-Enhanced_Spam_Filtering_based_on_Biased_Samples.html">68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</a></p>
<p>17 0.32699016 <a title="104-lsi-17" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>18 0.29973602 <a title="104-lsi-18" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>19 0.2726225 <a title="104-lsi-19" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>20 0.26992604 <a title="104-lsi-20" href="./nips-2006-Large_Margin_Component_Analysis.html">105 nips-2006-Large Margin Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.126), (3, 0.417), (7, 0.09), (9, 0.033), (22, 0.056), (44, 0.038), (57, 0.05), (64, 0.013), (65, 0.034), (69, 0.027), (72, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94983029 <a title="104-lda-1" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>Author: Ali Rahimi, Ben Recht</p><p>Abstract: We derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples. Limiting our search to invertible observation functions confers numerous beneﬁts, including a compact representation and no suboptimal local minima. Our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution. Our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates. The beneﬁts of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identiﬁcation are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an RFID tracking experiment. 1</p><p>2 0.9353804 <a title="104-lda-2" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>Author: Piotr DollÄ&sbquo;Ä&bdquo;r, Vincent Rabaud, Serge J. Belongie</p><p>Abstract: We present a new algorithm, Locally Smooth Manifold Learning (LSML), that learns a warping function from a point on an manifold to its neighbors. Important characteristics of LSML include the ability to recover the structure of the manifold in sparsely populated regions and beyond the support of the provided data. Applications of our proposed technique include embedding with a natural out-of-sample extension and tasks such as tangent distance estimation, frame rate up-conversion, video compression and motion transfer. 1</p><p>3 0.90123582 <a title="104-lda-3" href="./nips-2006-A_recipe_for_optimizing_a_time-histogram.html">17 nips-2006-A recipe for optimizing a time-histogram</a></p>
<p>Author: Hideaki Shimazaki, Shigeru Shinomoto</p><p>Abstract: The time-histogram method is a handy tool for capturing the instantaneous rate of spike occurrence. In most of the neurophysiological literature, the bin size that critically determines the goodness of the ﬁt of the time-histogram to the underlying rate has been selected by individual researchers in an unsystematic manner. We propose an objective method for selecting the bin size of a time-histogram from the spike data, so that the time-histogram best approximates the unknown underlying rate. The resolution of the histogram increases, or the optimal bin size decreases, with the number of spike sequences sampled. It is notable that the optimal bin size diverges if only a small number of experimental trials are available from a moderately ﬂuctuating rate process. In this case, any attempt to characterize the underlying spike rate will lead to spurious results. Given a paucity of data, our method can also suggest how many more trials are needed until the set of data can be analyzed with the required resolution. 1</p><p>same-paper 4 0.85899556 <a title="104-lda-4" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok</p><p>Abstract: Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. By using a sparsiﬁed manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns. 1</p><p>5 0.59051287 <a title="104-lda-5" href="./nips-2006-MLLE%3A_Modified_Locally_Linear_Embedding_Using_Multiple_Weights.html">127 nips-2006-MLLE: Modified Locally Linear Embedding Using Multiple Weights</a></p>
<p>Author: Zhenyue Zhang, Jing Wang</p><p>Abstract: The locally linear embedding (LLE) is improved by introducing multiple linearly independent local weight vectors for each neighborhood. We characterize the reconstruction weights and show the existence of the linearly independent weight vectors at each neighborhood. The modiﬁed locally linear embedding (MLLE) proposed in this paper is much stable. It can retrieve the ideal embedding if MLLE is applied on data points sampled from an isometric manifold. MLLE is also compared with the local tangent space alignment (LTSA). Numerical examples are given that show the improvement and efﬁciency of MLLE. 1</p><p>6 0.58255571 <a title="104-lda-6" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>7 0.5702619 <a title="104-lda-7" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>8 0.56371218 <a title="104-lda-8" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>9 0.56254989 <a title="104-lda-9" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>10 0.559228 <a title="104-lda-10" href="./nips-2006-Learning_to_be_Bayesian_without_Supervision.html">121 nips-2006-Learning to be Bayesian without Supervision</a></p>
<p>11 0.54479259 <a title="104-lda-11" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>12 0.54173809 <a title="104-lda-12" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>13 0.53346407 <a title="104-lda-13" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>14 0.52915943 <a title="104-lda-14" href="./nips-2006-Implicit_Surfaces_with_Globally_Regularised_and_Compactly_Supported_Basis_Functions.html">95 nips-2006-Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions</a></p>
<p>15 0.52619416 <a title="104-lda-15" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>16 0.52182633 <a title="104-lda-16" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>17 0.5143159 <a title="104-lda-17" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>18 0.51125389 <a title="104-lda-18" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>19 0.50906402 <a title="104-lda-19" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>20 0.50707847 <a title="104-lda-20" href="./nips-2006-Large_Margin_Component_Analysis.html">105 nips-2006-Large Margin Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
