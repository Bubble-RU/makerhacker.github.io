<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-177" href="#">nips2006-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</h1>
<br/><p>Source: <a title="nips-2006-177-pdf" href="http://papers.nips.cc/paper/2970-sparse-kernel-orthonormalized-pls-for-feature-extraction-in-large-data-sets.pdf">pdf</a></p><p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>Reference: <a title="nips-2006-177-reference" href="../nips2006_reference/nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sparse Kernel Orthonormalized PLS for feature extraction in large data sets  Jer´ nimo Arenas-Garc´a, Kaare Brandt Petersen and Lars Kai Hansen o ı Informatics and Mathematical Modelling Technical University of Denmark DK-2800 Kongens Lyngby, Denmark {jag,kbp,lkh}@imm. [sent-1, score-0.105]
</p><p>2 dk  Abstract In this paper we are presenting a novel multivariate analysis method. [sent-3, score-0.05]
</p><p>3 Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. [sent-4, score-0.529]
</p><p>4 The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. [sent-5, score-0.314]
</p><p>5 The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. [sent-6, score-0.263]
</p><p>6 1  Introduction  Partial Least Squares (PLS) is, in its general form, a family of techniques for analyzing relations between data sets by latent variables. [sent-7, score-0.093]
</p><p>7 It is a basic assumption that the information is overrepresented in the data sets, and that these therefore can be reduced in dimensionality by the latent variables. [sent-8, score-0.106]
</p><p>8 Exactly how these are found and how the data is projected varies within the approach, but they are often maximizing the covariance of two projected expressions. [sent-9, score-0.128]
</p><p>9 One of the appealing properties of PLS, which has made it popular, is that it can handle data sets with more dimensions than samples and massive collinearity between the variables. [sent-10, score-0.078]
</p><p>10 The basic PLS algorithm considers two data sets X and Y, where samples are arranged in rows, and consists on ﬁnding latent variables which account for the covariance XT Y between the data sets. [sent-11, score-0.113]
</p><p>11 Given the latent variables, the data sets X and Y are then transformed in a process which subtracts the information contained in the latent variables. [sent-13, score-0.147]
</p><p>12 This process, which is often referred to as deﬂation, can be done in a number of ways and these different approaches are deﬁning the many variants of PLS. [sent-14, score-0.05]
</p><p>13 The algorithm described in these, will in this paper be referred to as PLS2, and is based on the following two assumptions: First, that the latent variables of X are good predictors of Y and, second, that there is a linear relation between the latent variables of X and of Y. [sent-16, score-0.154]
</p><p>14 This linear relation is implying a certain deﬂation scheme, where the latent variable of X is used to deﬂate also the Y data set. [sent-17, score-0.1]
</p><p>15 In the second approach, the input data is mapped by a non-linear function into a high-dimensional space in which ordinary linear PLS is performed on the transformed data. [sent-22, score-0.077]
</p><p>16 A central property of this kernel approach is, as always, the exploitation of the kernel trick, i. [sent-23, score-0.238]
</p><p>17 It was Rosipal and Trejo who ﬁrst presented a non-linear kernel variant of PLS in [7]. [sent-26, score-0.162]
</p><p>18 In that paper, the kernel matrix and the Y matrix are deﬂated in the same way, and the PLS variant is thus more in line with the PLS2 variant than with the traditional algorithm from 1975 (PLS Mode A). [sent-27, score-0.247]
</p><p>19 The non-linear kernel PLS by Rosipal and Trejo is in this paper referred to as simply KPLS2, although many details could advocate more detailed nomenclator. [sent-28, score-0.139]
</p><p>20 The appealing property of kernel algorithms in general is that one can obtain the ﬂexibility of nonlinear expressions while still solving only linear equations. [sent-29, score-0.138]
</p><p>21 The downside is that for a data set of l samples, the kernel matrices to be handled are l × l, which, even for a moderate number of samples, quickly becomes a problem with respect to both memory and computing time. [sent-30, score-0.169]
</p><p>22 This problem is present not only in the training phase, but also when predicting the output given some large training data set: evaluating thousands of kernels for every new input vector is, in most applications, not acceptable. [sent-31, score-0.093]
</p><p>23 Furthermore, there is, for these so-called dense solutions in multivariate analysis, also the problem of overﬁtting. [sent-32, score-0.071]
</p><p>24 To counter the impractical dense solutions in kernel PLS, a few solutions have been proposed: In [2], the feature mapping directly is approximated following the Nystrom method, and in [6] the underlying cost function is modiﬁed to impose sparsity. [sent-33, score-0.226]
</p><p>25 In this paper, we introduce a novel kernel PLS variant called Reduced Kernel Orthonormalized Partial Least Squares (rKOPLS) for large scale feature extraction. [sent-34, score-0.213]
</p><p>26 It consists of two parts: A novel orthonormalized variant of kernel PLS called KOPLS, and a sparse approximation for large scale data sets. [sent-35, score-0.373]
</p><p>27 Compared to related approaches like [8], the KOPLS is transforming only the input data, and is keeping them orthonormal at two stages: the images in feature space and the projections in feature space. [sent-36, score-0.245]
</p><p>28 The sparse approximation is along the lines of [4], that is, we are representing the reduced kernel matrix as an outer product of a reduced and a full feature mapping, and thus keeping more information than changing the cost function or doing simple subsampling. [sent-37, score-0.282]
</p><p>29 Since rKOPLS is specially designed to handle large data sets, our experimental work will focus on such data sets, paying extra attention to the prediction of music genre, an application that typically involves large amount of high dimensional data. [sent-38, score-0.189]
</p><p>30 The abilities of our algorithm to discover non-linear relations between input and output data will be illustrated, as will be the relevance of the derived features compared to those provided by an existing kernel PLS method. [sent-39, score-0.194]
</p><p>31 The paper is structured as follows: In Section 2, the novel kernel orthonormalized PLS variant is introduced, and in Section 3 the sparse approximation is presented. [sent-40, score-0.353]
</p><p>32 Section 4 shows numerical results on UCI benchmark data sets, and on the above mentioned music application. [sent-41, score-0.18]
</p><p>33 , yl ]T , and denote by Φ = ΦU and Y = YV two matrices, each one containing np projections of the original input and output data, U and V being the projection matrices of sizes dim(F) × np and M × np , respectively. [sent-50, score-0.396]
</p><p>34 The objective of (kernel) Multivariate Analysis (MVA) algorithms is to search for projection matrices such that the projected input and output data are maximally aligned. [sent-51, score-0.15]
</p><p>35 In this paper, we propose a kernel extension of a different MVA method, namely, the Orthonormalized Partial Least Squares [18]. [sent-53, score-0.119]
</p><p>36 Our proposed kernel variant, called KOPLS, can be stated in the kernel framework as ˜T ˜ ˜ ˜ maximize: Tr{UT Φ YYT ΦU} (2) ˜T ˜ subject to: UT Φ ΦU = I Note that, unlike KCCA or KPLS, KOPLS only extracts projections of the input data. [sent-54, score-0.386]
</p><p>37 It is known that Orthonormalized PLS is optimal for performing linear regression on the input data when a bottleneck is imposed for data dimensionality reduction [10]. [sent-55, score-0.057]
</p><p>38 Similarly, KOPLS provides optimal projections for linear multi-regression in feature space. [sent-56, score-0.163]
</p><p>39 In other words, the solution to (2) also minimizes the sum of squares of the residuals of the approximation of the label matrix: ˜ ˜ ˆ ˆ ˜ ˜ ˜ ˜ Y−ΦB 2, B = (Φ T Φ )−1 Φ T Y (3) KOPLS :  F  ˆ where · F denotes the Frobenius norm of a matrix and B is the optimal regression matrix. [sent-57, score-0.081]
</p><p>40 Similarly to other MVA methods, KOPLS is not only useful for multi-regression problems, but it can also be used as a very powerful kernel feature extractor in supervised problems, including also the multi-label case, when Y is used to encode class membership information. [sent-58, score-0.151]
</p><p>41 Coming back to the KOPLS optimization problem, when projecting data into an inﬁnite dimensional space, we need to use the Representer Theorem that states that each of the projection vectors in U ˜ can be expressed as a linear combination of the training data. [sent-60, score-0.077]
</p><p>42 Applying ordinary linear algebra to (4), it can be shown that the columns of A are given by the solutions to the following generalized eigenvalue problem: Kx Ky Kx α = λKx Kx α (5) There are a number of ways to solve the above problem. [sent-65, score-0.08]
</p><p>43 Deﬂate the l × l matrix Kx Ky Kx according to: Kx Ky Kx ← Kx Ky Kx − λi Kx Kx αi αT Kx Kx i The motivation for this deﬂation strategy can be found in [13], in the discussion of generalized eigenvalue problems. [sent-71, score-0.045]
</p><p>44 Some intuition can be obtained if we observe its equivalence with Ky ← Ky − λi Kx αi αT Kx i which accounts for removing from the label matrix Y the best approximation based on the projections computed at step i, i. [sent-72, score-0.152]
</p><p>45 Since the rank of the original matrix Ky is at most rank(Y), this is the maximum number of projections that can be derived when using KOPLS. [sent-76, score-0.182]
</p><p>46 1  Centering of data in feature space can easily be done from the original kernel matrix. [sent-78, score-0.171]
</p><p>47 Details on this process are given in most text books describing kernel methods, e. [sent-79, score-0.119]
</p><p>48 3  Compact approximation of the KOPLS solution  The kernel formulation of the OPLS algorithm we have just presented suffers some drawbacks. [sent-82, score-0.119]
</p><p>49 In particular, as most other kernel methods, KOPLS requires the computation and storage of a kernel matrix of size l × l, which limits the maximum size of the datasets where the algorithm can be applied. [sent-83, score-0.28]
</p><p>50 In addition to this, algebraic procedures to solve the generalized eigenvalue problem (5) normally require the inversion of matrix Kx Kx which is usually rank deﬁcient. [sent-84, score-0.094]
</p><p>51 Finally, the matrix A will in general be dense rather than sparse, a fact which implies that when new data needs to be projected, it will be necessary to compute the kernels between the new data and all the samples in the training data set. [sent-85, score-0.133]
</p><p>52 Although it is possible to think of different solutions for each of the above issues, our proposal here is to impose sparsity in the projection vectors representation, i. [sent-86, score-0.063]
</p><p>53 , we will use the approximation U = ΦT B, where ΦR is a subset of the training data containing only R patterns (R < l) and R B = [β 1 , · · · , β np ] contains the parameters of the compact model. [sent-88, score-0.128]
</p><p>54 This is a very desirable property, specially when dealing with large data sets. [sent-94, score-0.05]
</p><p>55 • Training rKOPLS projections only requires the computation of a reduced kernel matrix KR of size R × l. [sent-95, score-0.303]
</p><p>56 Nevertheless, note that the approach we have followed is very different to subsampling, since rKOPLS is still using all training data in the MVA objective function. [sent-96, score-0.048]
</p><p>57 Note that KPLS2 does not admit a compact formulation as the one we have used for the new method, since the full kernel matrix is still needed for the deﬂation step. [sent-102, score-0.157]
</p><p>58 The main inconvenience of rKOPLS in relation to KPLS2 it that it requires the inversion of a matrix of size R×R. [sent-103, score-0.047]
</p><p>59 # Train/Test vehicle segmentation optdigits satellite pendigits letter  # Clases  dim  500 / 346 1310 / 1000 3823 / 1797 4435 / 2000 7494 / 3498 10000 / 10000  4 7 10 6 10 26  18 18 64 36 16 16  ν-SVM (%) (linear) 66. [sent-107, score-0.243]
</p><p>60 In addition to this, our extensive simulation work shows that the projections provided by rKOPLS are generally more relevant than those of KPLS2. [sent-116, score-0.168]
</p><p>61 4  Experiments  In this section, we will illustrate the ability of rKOPLS to discover relevant projections of the data. [sent-117, score-0.151]
</p><p>62 In particular, we include experiments on a benchmark of problems taken from the repository at the University of California Irvine (UCI) 2 , and on a musical genre classiﬁcation problem. [sent-119, score-0.154]
</p><p>63 This latter task is a good example of an application where rKOPLS can be specially useful, given the fact that the extraction of features from the raw audio data normally results in very large data sets of high dimensional data. [sent-120, score-0.215]
</p><p>64 The last four problems can be considered large problems for MVA algorithms, which are in general not sparse and require the computation of the kernels between any two points in the training set. [sent-124, score-0.058]
</p><p>65 For classiﬁcation, we use one of the simplest possible models: ˆ we compute the pseudoinverse of the projected training data to calculate B (see Eq. [sent-126, score-0.144]
</p><p>66 For the kernel ˆ classify according to Φ MVA algorithms we used a Gaussian kernel k(xi , xj ) = exp − xi − xj  2 2 2 /2σ  using 10-fold cross-validation (10-CV) on the training set to estimate σ. [sent-131, score-0.266]
</p><p>67 To obtain some reference accuracy rates, we also trained a ν-SVM with Gaussian kernel, using the LIBSVM implementation3 and 10-CV was carried out for both the kernel width and ν. [sent-132, score-0.146]
</p><p>68 Accuracy error rates for rKOPLS and different values of R are displayed in the ﬁrst rows and ﬁrst columns of Table 3. [sent-133, score-0.044]
</p><p>69 a √ √ l = 500 l l = 1000 l  vehicle segmentation optdigits satellite pendigits letter  80. [sent-147, score-0.225]
</p><p>70 4  vehicle segmentation optdigits satellite pendigits letter  81. [sent-203, score-0.225]
</p><p>71 Accuracy rates (%) and standard deviation of the estimation are given for 10 different runs of rKOPLS and KPLS2, both when using the pseudoinverse of the projected data together with the “winner-takes-all” activation function (ﬁrst rows), and when using a ν-SVM linear classiﬁer (last rows). [sent-273, score-0.142]
</p><p>72 For letter, we can see that, even for R = 1000, accuracy rates are far from those of SVM. [sent-277, score-0.053]
</p><p>73 The reason for this is that SVM is using 6226 support vectors, so that a very dense architecture seems to be necessary for this particular problem. [sent-278, score-0.043]
</p><p>74 In this way, both rKOPLS and KPLS2 need the same number of kernel evaluations. [sent-280, score-0.119]
</p><p>75 Note that, even in this case, KPLS2 results in an architecture with l nodes (l > R), so that projections of data are more expensive than for the respective rKOPLS. [sent-281, score-0.17]
</p><p>76 In any case, we must point out that subsampling was only considered for training the projections, but all training data was used to compute the pseudoinverse of the projected training data. [sent-282, score-0.242]
</p><p>77 Results without subsampling are also provided in Table 3 under the l = l column except for the letter data set which we were unable to process due to massive memory problems. [sent-283, score-0.156]
</p><p>78 In contrast to this, the maximum number of projections that the rKOPLS can provide equals the rank of the label matrix, i. [sent-285, score-0.161]
</p><p>79 In spite of using a much smaller number of projections, our algorithm performed signiﬁcantly better than KPLS2 with subsampling in four out of the ﬁve largest problems. [sent-288, score-0.042]
</p><p>80 However, we can see that the linear SVM is able to better exploit the projections provided by the MVA methods in vehicle and letter, precisely the two problems where previous results were less satisfactory. [sent-291, score-0.198]
</p><p>81 In addition to this, these projections are more “informative”, in the sense that we can obtain a better recognition accuracy using a smaller number of projections. [sent-293, score-0.158]
</p><p>82 2  Feature Extraction for Music Genre Classiﬁcation  In this subsection we consider the problem of predicting the genre of a song using the audio data only, a task which since the seminal paper [14] has been subject of much interest. [sent-296, score-0.269]
</p><p>83 analyze has been previously investigated in [5], and consists of 1317 snippets each of 30 seconds distributed evenly among 11 music genres: alternative, country, easy listening, electronica, jazz, latin, pop&dance;, rap&hip-hop;, r&b;, reggae and rock. [sent-298, score-0.144]
</p><p>84 The music snippets are MP3 (MPEG1layer3) encoded music with a bitrate of 128 kbps or higher, down sampled to 22050 Hz, and they are processed following the method in [5]: MFCC features are extracted from overlapping frames of the song, using a window size of 20 ms. [sent-299, score-0.284]
</p><p>85 For training and testing the system we have split the data set into two subsets with 817 and 500 songs, respectively. [sent-302, score-0.048]
</p><p>86 After processing the audio data, we have 57388 and 36556 135-dimensional vectors in the training and test partitions, an amount which for most kernel MVA methods is prohibitively large. [sent-303, score-0.199]
</p><p>87 For the rKOPLS, however, the compact representation is enabling usage of the entire training data. [sent-304, score-0.045]
</p><p>88 Note that, in this case, comparisons between rKOPLS and KPLS2 are for a ﬁxed architecture complexity (R = l ), since the most signiﬁcant computational burden for the training of the system is in the projection of the data. [sent-306, score-0.096]
</p><p>89 Since every song consists of about seventy AR vectors, we can measure the classiﬁcation accuracy in two different ways: 1) On the level of individual AR vectors or 2) by majority voting among the AR vectors of a given song. [sent-307, score-0.111]
</p><p>90 The strong results are very pronounced in Figure 1(b) where, for R = 750, rKOPLS is outperforming ordinary KPLS, and is doing so with only ten projections compared to ﬁfty projections of the KPLS2. [sent-309, score-0.321]
</p><p>91 This demonstrates that the features extracted by rKOPLS holds much more information relevant to the genre classiﬁcation task than KPLS2. [sent-310, score-0.154]
</p><p>92 5  Conclusions  In this paper we have presented a novel kernel PLS algorithm, that we call reduced kernel orthonormalized PLS (rKOPLS). [sent-311, score-0.431]
</p><p>93 Compared to similar approaches, rKOPLS is making the data in feature space orthonormal, and imposing sparsity on the solution to ensure competitive performance on large data sets. [sent-312, score-0.107]
</p><p>94 Our method has been tested on a benchmark of UCI data sets, and we have found that the results were competitive in comparison to those of rbf-SVM, and superior to those of the ordinary KPLS2 method. [sent-313, score-0.101]
</p><p>95 Furthermore, when applied to a music genre classiﬁcation task, rKOPLS performed very well even with only a few features, keeping also the complexity of the algorithm under control. [sent-314, score-0.248]
</p><p>96 Notes on the history and nature of partial least squares (PLS) modelling. [sent-321, score-0.139]
</p><p>97 Primal space sparse kernel partial least squares regression for large problems. [sent-330, score-0.288]
</p><p>98 Kernel partial least squares regression in reproducing kernel hilbert space. [sent-351, score-0.258]
</p><p>99 A survey of partial least squares (PLS) methods, with emphasis on the two-block case. [sent-383, score-0.139]
</p><p>100 Characterizing the response of pet and fMRI data using multivariate linear models (MLM). [sent-408, score-0.051]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rkopls', 0.611), ('pls', 0.426), ('kopls', 0.284), ('kx', 0.277), ('ky', 0.169), ('kr', 0.148), ('mva', 0.142), ('orthonormalized', 0.142), ('projections', 0.131), ('music', 0.119), ('kernel', 0.119), ('genre', 0.113), ('song', 0.084), ('kt', 0.075), ('ar', 0.068), ('np', 0.063), ('rosipal', 0.062), ('squares', 0.06), ('kpls', 0.057), ('letter', 0.057), ('partial', 0.056), ('uci', 0.054), ('latent', 0.054), ('projected', 0.054), ('ation', 0.053), ('audio', 0.052), ('vehicle', 0.05), ('variant', 0.043), ('optdigits', 0.043), ('pendigits', 0.043), ('subsampling', 0.042), ('pseudoinverse', 0.042), ('benchmark', 0.041), ('ordinary', 0.04), ('chemometrics', 0.037), ('trejo', 0.037), ('extraction', 0.034), ('reduced', 0.032), ('ut', 0.032), ('feature', 0.032), ('satellite', 0.032), ('multivariate', 0.031), ('rank', 0.03), ('matrices', 0.03), ('specially', 0.03), ('variants', 0.03), ('sparse', 0.03), ('projection', 0.029), ('training', 0.028), ('ate', 0.028), ('denmark', 0.028), ('kcca', 0.028), ('yyt', 0.028), ('accuracy', 0.027), ('classi', 0.027), ('relation', 0.026), ('svm', 0.026), ('rates', 0.026), ('roman', 0.025), ('snippets', 0.025), ('yv', 0.025), ('leonard', 0.025), ('chemistry', 0.025), ('dense', 0.024), ('eigenvalue', 0.024), ('least', 0.023), ('lars', 0.023), ('tr', 0.022), ('storage', 0.021), ('matrix', 0.021), ('kai', 0.021), ('features', 0.021), ('referred', 0.02), ('relevant', 0.02), ('data', 0.02), ('reformulated', 0.02), ('burden', 0.02), ('pseudo', 0.02), ('massive', 0.02), ('normally', 0.019), ('novel', 0.019), ('appealing', 0.019), ('sets', 0.019), ('counter', 0.019), ('outperforming', 0.019), ('maximize', 0.019), ('architecture', 0.019), ('table', 0.018), ('bt', 0.018), ('dim', 0.018), ('centering', 0.018), ('sparsity', 0.018), ('rows', 0.018), ('provided', 0.017), ('input', 0.017), ('compact', 0.017), ('orthonormal', 0.017), ('imposing', 0.017), ('solutions', 0.016), ('keeping', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="177-tfidf-1" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>2 0.15536426 <a title="177-tfidf-2" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>3 0.11707184 <a title="177-tfidf-3" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>4 0.068209693 <a title="177-tfidf-4" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>Author: Yonatan Amit, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: We describe and analyze an algorithmic framework for online classiﬁcation where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online hypothesis by deﬁning a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution for the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with online multiclass text categorization. Our experiments indicate that a combination of class-dependent features with the simultaneous projection method outperforms previously studied algorithms. 1</p><p>5 0.067387305 <a title="177-tfidf-5" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>6 0.05767782 <a title="177-tfidf-6" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>7 0.0545405 <a title="177-tfidf-7" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>8 0.052291278 <a title="177-tfidf-8" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>9 0.050822731 <a title="177-tfidf-9" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>10 0.049503587 <a title="177-tfidf-10" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>11 0.048987396 <a title="177-tfidf-11" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>12 0.047122698 <a title="177-tfidf-12" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>13 0.04491581 <a title="177-tfidf-13" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>14 0.04381955 <a title="177-tfidf-14" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>15 0.039285786 <a title="177-tfidf-15" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>16 0.037521709 <a title="177-tfidf-16" href="./nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</a></p>
<p>17 0.037245817 <a title="177-tfidf-17" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>18 0.034575354 <a title="177-tfidf-18" href="./nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</a></p>
<p>19 0.033288248 <a title="177-tfidf-19" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>20 0.032970682 <a title="177-tfidf-20" href="./nips-2006-Modeling_Dyadic_Data_with_Binary_Latent_Factors.html">132 nips-2006-Modeling Dyadic Data with Binary Latent Factors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, 0.044), (2, 0.016), (3, 0.079), (4, -0.042), (5, 0.023), (6, -0.014), (7, -0.009), (8, 0.024), (9, 0.015), (10, -0.132), (11, 0.051), (12, 0.01), (13, -0.203), (14, 0.027), (15, -0.021), (16, -0.056), (17, 0.001), (18, -0.058), (19, 0.047), (20, -0.022), (21, 0.046), (22, -0.046), (23, -0.009), (24, -0.002), (25, 0.008), (26, -0.017), (27, 0.04), (28, 0.001), (29, 0.032), (30, -0.029), (31, -0.047), (32, -0.152), (33, -0.041), (34, -0.099), (35, 0.013), (36, 0.193), (37, 0.052), (38, -0.057), (39, -0.052), (40, -0.023), (41, 0.021), (42, 0.036), (43, 0.057), (44, 0.022), (45, -0.027), (46, 0.119), (47, 0.038), (48, 0.013), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90859169 <a title="177-lsi-1" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>2 0.72871739 <a title="177-lsi-2" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>3 0.71966094 <a title="177-lsi-3" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>Author: Risi Kondor, Tony Jebara</p><p>Abstract: We propose a new method for constructing hyperkenels and deﬁne two promising special cases that can be computed in closed form. These we call the Gaussian and Wishart hyperkernels. The former is especially attractive in that it has an interpretable regularization scheme reminiscent of that of the Gaussian RBF kernel. We discuss how kernel learning can be used not just for improving the performance of classiﬁcation and regression methods, but also as a stand-alone algorithm for dimensionality reduction and relational or metric learning. 1</p><p>4 0.6628468 <a title="177-lsi-4" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>5 0.64283371 <a title="177-lsi-5" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>6 0.58963203 <a title="177-lsi-6" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>7 0.47366899 <a title="177-lsi-7" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>8 0.46773979 <a title="177-lsi-8" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>9 0.45107219 <a title="177-lsi-9" href="./nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</a></p>
<p>10 0.45037851 <a title="177-lsi-10" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>11 0.44122589 <a title="177-lsi-11" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>12 0.42556635 <a title="177-lsi-12" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>13 0.39507133 <a title="177-lsi-13" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>14 0.38957822 <a title="177-lsi-14" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>15 0.38727415 <a title="177-lsi-15" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<p>16 0.38126174 <a title="177-lsi-16" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>17 0.36234882 <a title="177-lsi-17" href="./nips-2006-Nonnegative_Sparse_PCA.html">149 nips-2006-Nonnegative Sparse PCA</a></p>
<p>18 0.36021337 <a title="177-lsi-18" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>19 0.35964173 <a title="177-lsi-19" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>20 0.34703556 <a title="177-lsi-20" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.116), (3, 0.024), (7, 0.078), (9, 0.048), (20, 0.018), (22, 0.072), (30, 0.011), (44, 0.031), (57, 0.05), (64, 0.015), (65, 0.035), (67, 0.353), (69, 0.022), (71, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69741148 <a title="177-lda-1" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>2 0.61833888 <a title="177-lda-2" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>Author: Matthias Hein, Markus Maier</p><p>Abstract: We consider the problem of denoising a noisily sampled submanifold M in Rd , where the submanifold M is a priori unknown and we are only given a noisy point sample. The presented denoising algorithm is based on a graph-based diffusion process of the point sample. We analyze this diffusion process using recent results about the convergence of graph Laplacians. In the experiments we show that our method is capable of dealing with non-trivial high-dimensional noise. Moreover using the denoising algorithm as pre-processing method we can improve the results of a semi-supervised learning algorithm. 1</p><p>3 0.4516336 <a title="177-lda-3" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>4 0.44447693 <a title="177-lda-4" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>Author: Dong S. Cheng, Vittorio Murino, Mário Figueiredo</p><p>Abstract: This paper proposes a new approach to model-based clustering under prior knowledge. The proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as ﬁnite mixture learning under a grouping prior. To estimate the parameters of the proposed model, we derive a (generalized) EM algorithm with a closed-form E-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require Gibbs sampling or suboptimal shortcuts. We show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature Markov random ﬁeld priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efﬁcient way. Finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes. 1</p><p>5 0.44106936 <a title="177-lda-5" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>6 0.43938258 <a title="177-lda-6" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>7 0.43805048 <a title="177-lda-7" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>8 0.43704721 <a title="177-lda-8" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>9 0.43674183 <a title="177-lda-9" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>10 0.43582138 <a title="177-lda-10" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>11 0.43563363 <a title="177-lda-11" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>12 0.4355675 <a title="177-lda-12" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>13 0.43510726 <a title="177-lda-13" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>14 0.43391538 <a title="177-lda-14" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>15 0.43290025 <a title="177-lda-15" href="./nips-2006-MLLE%3A_Modified_Locally_Linear_Embedding_Using_Multiple_Weights.html">127 nips-2006-MLLE: Modified Locally Linear Embedding Using Multiple Weights</a></p>
<p>16 0.43279716 <a title="177-lda-16" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>17 0.43242025 <a title="177-lda-17" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>18 0.4316594 <a title="177-lda-18" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>19 0.43093574 <a title="177-lda-19" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>20 0.43091834 <a title="177-lda-20" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
