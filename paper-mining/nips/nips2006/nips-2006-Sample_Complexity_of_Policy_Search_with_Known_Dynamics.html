<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-171" href="#">nips2006-171</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</h1>
<br/><p>Source: <a title="nips-2006-171-pdf" href="http://papers.nips.cc/paper/2990-sample-complexity-of-policy-search-with-known-dynamics.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>Reference: <a title="nips-2006-171-reference" href="../nips2006_reference/nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sample complexity of policy search with known dynamics  Peter L. [sent-1, score-0.582]
</p><p>2 edu  Abstract We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. [sent-6, score-0.509]
</p><p>3 The policy is chosen based on its empirical performance in simulations. [sent-7, score-0.551]
</p><p>4 We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. [sent-8, score-1.144]
</p><p>5 We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. [sent-9, score-0.242]
</p><p>6 Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. [sent-10, score-0.261]
</p><p>7 These assumptions and ours are both stronger than the usual combinatorial complexity measures. [sent-11, score-0.16]
</p><p>8 We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient. [sent-12, score-0.328]
</p><p>9 the future is independent of the past given the current state of the environment. [sent-15, score-0.073]
</p><p>10 Except for toy problems with a few states, computing an optimal policy for an MDP is usually out of the question. [sent-16, score-0.509]
</p><p>11 One possibility is to avoid considering all possible policies by restricting oneself to a smaller class Π of policies. [sent-18, score-0.197]
</p><p>12 Given a simulator for the environment, we try to pick the best policy from Π. [sent-19, score-0.538]
</p><p>13 The hope is that if the policy class is appropriately chosen, the best policy in Π would not be too much worse than the true optimal policy. [sent-20, score-1.081]
</p><p>14 Use of simulators introduces an additional issue: how is one to be sure that performance of policies in the class Π on a few simulations is indicative of their true performance? [sent-21, score-0.17]
</p><p>15 There the aim is to learn a concept and one restricts attention to a hypotheses class which may or may not contain the “true” concept. [sent-23, score-0.063]
</p><p>16 The answer turns out to depend on “complexity” of the hypothesis class as measured by combinatorial quantities associated with the class such as the VC dimension, the pseudodimension and the fat-shattering dimension. [sent-25, score-0.422]
</p><p>17 Some progress [6,7] has already been made to obtain uniform bounds on the difference between value functions and their empirical estimates, where the value function of a policy is the expected long term reward starting from a certain state and following the policy thereafter. [sent-26, score-1.333]
</p><p>18 We continue this line of work by further investigating what properties of the policy class determine the rate of uniform convergence of value function estimates. [sent-27, score-0.62]
</p><p>19 The key difference between the usual statistical learning setting and ours is that we not only have to consider the complexity of the class Π but also of the  classes derived from Π by composing the functions in Π with themselves and with the state evolution process implied by the simulator. [sent-28, score-0.274]
</p><p>20 Ng and Jordan [7] used a ﬁnite pseudodimension condition along with Lipschitz continuity to derive uniform bounds. [sent-29, score-0.253]
</p><p>21 The number of samples required grows linearly with the dimension of the parameter space but is independent of the dimension of the state space. [sent-32, score-0.137]
</p><p>22 Ng and Jordan’s and our assumptions are both stronger than just assuming ﬁniteness of some combinatorial dimension. [sent-33, score-0.13]
</p><p>23 We show that this is unavoidable by constructing two examples where the fat-shattering dimension and the pseudodimension respectively are bounded, yet no simulation based method succeeds in estimating the true values of policies well. [sent-34, score-0.377]
</p><p>24 This happens because iteratively composing a function class with itself can quickly destroy ﬁniteness of combinatorial dimensions. [sent-35, score-0.187]
</p><p>25 Additional assumptions are therefore needed to ensure that these iterates continue to have bounded combinatorial dimensions. [sent-36, score-0.175]
</p><p>26 Although we restrict ourselves to MDPs for ease of exposition, the analysis in this paper carries over easily to the case of partially obervable MDPs (POMDPs), provided the simulator also simulates the conditional distribution of observations given state using a bounded amount of computation. [sent-37, score-0.186]
</p><p>27 Section 3 proves Theorem 1, which gives a sample complexity bound for achieving a desired level of performance within the policy class. [sent-41, score-0.569]
</p><p>28 In Section 4, we give two examples of policy classes whose combinatorial dimensions are bounded. [sent-42, score-0.647]
</p><p>29 Nevertheless, we can prove strong minimax lower bounds implying that no method of choosing a policy based on empirical estimates can do well for these examples. [sent-43, score-0.709]
</p><p>30 In this paper, we assume that the state space S and the action space A are ﬁnite dimensional Euclidean spaces of dimensionality dS and dA respectively. [sent-45, score-0.102]
</p><p>31 A (randomized) policy π is a mapping from S to distributions over A. [sent-46, score-0.509]
</p><p>32 Each policy π induces a natural Markov chain on the state space of the MDP, namely the one obtained by starting in a start state s0 sampled from D and st+1 sampled according to P (·|st , at ) with at drawn from π(st ) for t ≥ 0. [sent-47, score-0.655]
</p><p>33 Let rt (π) be the expected reward at time step t in this Markov chain, i. [sent-48, score-0.131]
</p><p>34 Note that the expectation is over the randomness in the choice of the initial state, the state transitions, and the randomized policy and reward outcomes. [sent-51, score-0.672]
</p><p>35 Deﬁne the value VM (π) of the policy by ∞  γ t rt (π) . [sent-52, score-0.55]
</p><p>36 For a class Π of policies, deﬁne opt(M, Π) = sup VM (π) . [sent-54, score-0.199]
</p><p>37 π∈Π  The regret of a policy π relative to an MDP M and a policy class Π is deﬁned as RegM,Π (π ) = opt(M, Π) − VM (π ) . [sent-55, score-1.17]
</p><p>38 We use a degree bounded version of the Blum-Shub-Smale [3] model of computation over reals. [sent-56, score-0.084]
</p><p>39 At each time step, we can perform one of the four arithmetic operations +, −, ×, / or can branch based on a comparison (say <). [sent-57, score-0.138]
</p><p>40 The function f is (Ξ, τ )-computable if there exists a degree bounded ﬁnite dimensional machine M over R with input space Rk+m and output space Rl such that the following hold. [sent-62, score-0.124]
</p><p>41 Informally, the deﬁnition states that given access to an oracle which generates samples from Ξ, we can generate samples from f (x) by doing a bounded amount of computation. [sent-67, score-0.116]
</p><p>42 In Section 3, we assume that the policy class Π is parameterized by a ﬁnite dimensional parameter θ ∈ Rd . [sent-70, score-0.572]
</p><p>43 This assumption will be satisﬁed if we have three “programs” that make a call to a random number generator for distribution Ξ, do a ﬁxed number of ﬂoating-point operations and simulate the policies in our class, the state-transition dynamics and the rewards respectively. [sent-76, score-0.347]
</p><p>44 • Linear Dynamical System with Additive Noise 1 Suppose P and Q are dS × dS and dS × dA matrices and the system dynamics is given by st+1 = P st + Qat + ξt , (1) where ξt are i. [sent-78, score-0.123]
</p><p>45 Thus, if ξ has uniform distribution on (0, 1], then f (ξ) = k with probability pk . [sent-96, score-0.108]
</p><p>46 ignoring rewards beyond time H does not affect the value of any policy by more than . [sent-103, score-0.643]
</p><p>47 To obtain sample rewards, given initial state s0 and policy πθ = π(·; θ), we ﬁrst compute the trajectory s0 , . [sent-104, score-0.582]
</p><p>48 A further H + 1 calls to Mr are then required to generate the rewards ρ0 through ρH . [sent-109, score-0.179]
</p><p>49 Deﬁne the empirical estimate of the value of the policy π by n H 1 (i) ˆH VM (πθ ) = γ t ρt (s0 , θ, ξi ) . [sent-116, score-0.551]
</p><p>50 Deﬁne an -approximate maximizer ˆ to be a policy π such that of V ˆH ˆH VM (π ) ≥ sup VM (π) − . [sent-118, score-0.738]
</p><p>51 π∈Π  1 In this case, the realizable dynamics (mapping from state to next state for a given policy class) is not uniformly Lipschitz if policies allow unbounded actions. [sent-119, score-0.805]
</p><p>52 Finally, we mention the deﬁnitions of three standard combinatorial dimensions. [sent-121, score-0.091]
</p><p>53 Let X be some space and consider classes G and F of {−1, +1} and real valued functions on X , respectively. [sent-122, score-0.075]
</p><p>54 We say that G shatters X if for all bit vectors b ∈ {0, 1}n there exists g ∈ G such that for all i, bi = 0 ⇒ g(xi ) = −1, bi = 1 ⇒ g(xi ) = +1. [sent-127, score-0.331]
</p><p>55 We say that F shatters X if there exists r ∈ Rn such that, for all bit vectors b ∈ {0, 1}n , there exists f ∈ F such that for all i, bi = 0 ⇒ f (xi ) < ri , bi = 1 ⇒ f (xi ) ≥ ri . [sent-128, score-0.443]
</p><p>56 We say that F -shatters X if these exists r ∈ Rn such that, for all bit vectors b ∈ {0, 1}n, there exists f ∈ F such that for all i, bi = 0 ⇒ f (xi ) ≤ ri − , bi = 1 ⇒ f (xi ) ≥ ri + . [sent-129, score-0.361]
</p><p>57 We then have the following deﬁnitions, VCdim(G) = max{|X| : G shatters X} , Pdim(F ) = max{|X| : F shatters X} , fatF ( ) = max{|X| : F -shatters X} . [sent-130, score-0.164]
</p><p>58 Fix an MDP M, a policy class Π = {s → π(s; θ) : θ ∈ Rd }, and an > 0. [sent-132, score-0.572]
</p><p>59 Then R2 Hdτ R n>O log (1 − γ)2 2 (1 − γ) ˆ ensures that E RegM,Π (πn ) ≤ 3 + , where πn is an -approximate maximizer of V and H = log1/γ (2R/( (1 − γ))) is the /2 horizon time. [sent-134, score-0.122]
</p><p>60 Given initial state s0 , parameter θ and random numbers ξ1 through ξ3H+1 , we ﬁrst compute the trajectory as follows. [sent-138, score-0.073]
</p><p>61 st = ΦMP (st−1 , ΦMπ (θ, s, ξ2t−1 ), ξ2t ), 1 ≤ t ≤ H . [sent-140, score-0.08]
</p><p>62 (2)  The rewards are then computed by ρt = ΦMr (st , ξ2H+t+1 ), 0 ≤ t ≤ H . [sent-141, score-0.134]
</p><p>63 (3)  The H-step discounted reward sum is computed as H  γ t ρt = ρ0 + γ(ρ1 + γ(ρ2 + . [sent-142, score-0.116]
</p><p>64 (4)  t=0 H  Deﬁne the function class R = {(s0 , ξ) → t=0 γ t ρt (s0 , θ, ξ) : θ ∈ Rd }, where we have explicitly shown the dependence of ρt on s0 , θ and ξ. [sent-149, score-0.063]
</p><p>65 Let us count the number of arithmetic operations needed to compute a function in this class. [sent-150, score-0.138]
</p><p>66 Using Assumption A, we see that steps (2) and (3) require no more than 2τ H and τ (H + 1) operations respectively. [sent-151, score-0.063]
</p><p>67 Goldberg and Jerrum [4] showed that the VC dimension of a function class can be bounded in terms of an upper bound on the number of arithmetic operations it takes to compute the functions in the class. [sent-154, score-0.375]
</p><p>68 Since the pseudodimension of R can be written as Pdim(R) = VCdim{(s0 , ξ, c) → sign(f (s0 , ξ) − c) : f ∈ R, c ∈ R} , we get the following bound by [2, Thm. [sent-155, score-0.266]
</p><p>69 (6)  Functions in R are positive and bounded above by R = R/(1 − γ). [sent-162, score-0.084]
</p><p>70 There are well-known bounds for deviations of empirical estimates from true expectations for bounded function classes in terms of the pseudodimension of the class (see, for example, Theorems 3 and 5 in [5]; also see Pollard’s book [8]). [sent-163, score-0.522]
</p><p>71 ˆ In order to ensure that P n (∃π ∈ Π : |V H (π) − V H (π)| > /2) < δ, we need 64eR  8  2 Pdim(R)  e−  2  n/256R  2  <δ,  Using the bound (5) on Pdim(R), we get that ˆ P n sup V H (π) − V (π) >  <δ,  (7)  π∈Π  provided n>  256R2 (1 − γ)2  2  log  8 δ  + 8d(6τ H + 3) log  64eR (1 − γ)  . [sent-165, score-0.197]
</p><p>72 Since πn is an -approximate maximizer of V ˆ Thus, for all π ∈ Π, V (π) ≤ V H (πn ) + + . [sent-172, score-0.093]
</p><p>73 Taking the supremum over π ∈ Π and using the ˆ H (πn ) ≤ V (πn ) + , we get sup fact that V π∈Π V (π) ≤ V (πn ) + 2 + , which is equivalent to RegM,Π (πn ) ≤ 2 + . [sent-173, score-0.167]
</p><p>74 where we used the fact that regret is bounded above by R/(1 − γ). [sent-176, score-0.173]
</p><p>75 4 Two Policy Classes Having Bounded Combinatorial Dimensions We will describe two policy classes for which we can prove that there are strong limitations on the performance of any method (of choosing a policy out of a policy class) that has access only to empirically observed rewards. [sent-177, score-1.616]
</p><p>76 Somewhat surprisingly, one can show this for policy classes which are “simple” in the sense that standard combinatorial dimensions of these classes are bounded. [sent-178, score-0.694]
</p><p>77 This shows that sufﬁcient conditions for the success of simulation based policy search (such as the assumptions in [7] and in our Theorem 1) have to be necessarily stronger than boundedness of standard combinatorial dimensions. [sent-179, score-0.728]
</p><p>78 The ﬁrst example is a policy class F1 for which fatF1 ( ) < ∞ for all > 0. [sent-180, score-0.572]
</p><p>79 The second example is a class F2 for which Pdim(F2 ) = 1. [sent-181, score-0.063]
</p><p>80 Since ﬁniteness of pseudodimension is a stronger condition, the second example makes our point more forcefully than the ﬁrst one. [sent-182, score-0.244]
</p><p>81 r = deterministic reward that maps s to s, and γ = some ﬁxed discount factor in (0, 1). [sent-198, score-0.09]
</p><p>82 For a function f : [−1, +1] → [−1, +1], let πf denote the (deterministic) policy which takes action f (s) − s in state s. [sent-199, score-0.642]
</p><p>83 Given a class F of functions, we deﬁne an associated policy class ΠF = {πf : f ∈ F}. [sent-200, score-0.635]
</p><p>84 By construction, functions in F1 have bounded total variation and so, fatF1 ( ) is O(1/ ) (see, for example, [2, Chap. [sent-209, score-0.112]
</p><p>85 , sn to a policy in ΠF1 , there is an initial state distribution such that the expected regret of the selected policy is at least γ 2 /(1 − γ) − 2 1 . [sent-226, score-1.263]
</p><p>86 This is in sharp contrast to Theorem 1 where we could reduce, by using sufﬁciently many samples, the expected regret down to any positive number given the ability to maximize the ˆ empirical estimates V . [sent-227, score-0.178]
</p><p>87 Let us see how maximization of empirical estimates behaves in this case. [sent-228, score-0.115]
</p><p>88 The transitions, policies and rewards here are all deterministic. [sent-232, score-0.241]
</p><p>89 This means that the 1-step reward function family is just F1 . [sent-234, score-0.09]
</p><p>90 So the estimates of 1-step rewards are  still uniformly concentrated around their expected values. [sent-235, score-0.181]
</p><p>91 Since the contribution of rewards from time step 2 onwards can be no more than γ 2 + γ 3 + . [sent-236, score-0.134]
</p><p>92 = γ 2 /(1 − γ), we can claim that the expected ˆ regret of the V maximizer πn behaves like γ2 E RegM,ΠF1 (πn ) ≤ + en 1−γ where en → 0. [sent-239, score-0.264]
</p><p>93 ,sn ) ) = sup Es∼D s + γf (s) + f ∈F1  γ2 2 f (s) 1−γ  − E(s1 ,. [sent-261, score-0.136]
</p><p>94 , sn )(s) + = sup Es∼D γf (s) + f ∈F1  γ2 gn (s1 , . [sent-267, score-0.505]
</p><p>95 , sn )2 (s)] 1−γ For all f1 , f2 , |Ef1 − Ef2 | ≤ E|f1 − f2 | ≤ 1 . [sent-273, score-0.083]
</p><p>96 , sn )(s) +  − 2γ  1  2  =  γ 1−γ − 2γ  sup Es∼D f 2 (s) + 1 − E(s1 ,. [sent-287, score-0.219]
</p><p>97 , sn )2 (s) + 1]  f ∈F1 1  2 From (8), we know that fT (x) + 1 restricted to x ∈ (0, 1) is the same as 1(x∈T ) . [sent-293, score-0.083]
</p><p>98 Therefore, restricting D to probability measures on (0, 1) and applying Lemma 1, we get γ2 inf sup opt(MD , ΠF1 ) − E(s1 ,. [sent-294, score-0.238]
</p><p>99 gn D 1−γ To ﬁnish the proof, we note that γ < 1 and, by deﬁnition, RegMD ,ΠF1 (πgn (s1 ,. [sent-301, score-0.286]
</p><p>100 Example 2 We use the MDP of the previous section with a different policy class which we now describe. [sent-308, score-0.572]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.509), ('gn', 0.286), ('pdim', 0.282), ('ft', 0.247), ('regm', 0.235), ('pseudodimension', 0.205), ('stretch', 0.164), ('sup', 0.136), ('rewards', 0.134), ('mix', 0.125), ('mdp', 0.116), ('dn', 0.113), ('lipschitz', 0.109), ('policies', 0.107), ('vm', 0.104), ('opt', 0.102), ('ds', 0.095), ('maximizer', 0.093), ('combinatorial', 0.091), ('reward', 0.09), ('regret', 0.089), ('bounded', 0.084), ('sn', 0.083), ('shatters', 0.082), ('st', 0.08), ('arithmetic', 0.075), ('state', 0.073), ('bi', 0.073), ('regmd', 0.071), ('tep', 0.071), ('vmd', 0.071), ('class', 0.063), ('operations', 0.063), ('md', 0.062), ('pk', 0.06), ('fix', 0.06), ('boundedness', 0.056), ('ns', 0.055), ('theorem', 0.053), ('odd', 0.052), ('mp', 0.049), ('niteness', 0.049), ('da', 0.049), ('uniform', 0.048), ('ambuj', 0.047), ('vcdim', 0.047), ('estimates', 0.047), ('classes', 0.047), ('rk', 0.045), ('calls', 0.045), ('inf', 0.044), ('dynamics', 0.043), ('prove', 0.042), ('empirical', 0.042), ('mr', 0.041), ('rt', 0.041), ('jerrum', 0.041), ('exists', 0.04), ('mdps', 0.04), ('stronger', 0.039), ('berkeley', 0.039), ('es', 0.038), ('goldberg', 0.037), ('halting', 0.037), ('ri', 0.036), ('pollard', 0.035), ('minimax', 0.035), ('bit', 0.034), ('bounds', 0.034), ('simulation', 0.033), ('composing', 0.033), ('blum', 0.033), ('states', 0.032), ('na', 0.032), ('rm', 0.032), ('dimension', 0.032), ('let', 0.031), ('get', 0.031), ('complexity', 0.03), ('suppose', 0.03), ('bartlett', 0.03), ('nitions', 0.03), ('subscript', 0.03), ('vc', 0.03), ('bound', 0.03), ('action', 0.029), ('nite', 0.029), ('say', 0.029), ('horizon', 0.029), ('simulator', 0.029), ('haussler', 0.029), ('functions', 0.028), ('en', 0.028), ('computable', 0.028), ('nition', 0.027), ('restricting', 0.027), ('everywhere', 0.027), ('rd', 0.026), ('behaves', 0.026), ('discounted', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="171-tfidf-1" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>2 0.36612207 <a title="171-tfidf-2" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><p>3 0.25606176 <a title="171-tfidf-3" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>Author: Mohammad Ghavamzadeh, Yaakov Engel</p><p>Abstract: Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. Since Monte Carlo methods tend to have high variance, a large number of samples is required, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost. 1</p><p>4 0.21175699 <a title="171-tfidf-4" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>Author: Angela J. Yu</p><p>Abstract: Survival in a non-stationary, potentially adversarial environment requires animals to detect sensory changes rapidly yet accurately, two oft competing desiderata. Neurons subserving such detections are faced with the corresponding challenge to discern “real” changes in inputs as quickly as possible, while ignoring noisy ﬂuctuations. Mathematically, this is an example of a change-detection problem that is actively researched in the controlled stochastic processes community. In this paper, we utilize sophisticated tools developed in that community to formalize an instantiation of the problem faced by the nervous system, and characterize the Bayes-optimal decision policy under certain assumptions. We will derive from this optimal strategy an information accumulation and decision process that remarkably resembles the dynamics of a leaky integrate-and-ﬁre neuron. This correspondence suggests that neurons are optimized for tracking input changes, and sheds new light on the computational import of intracellular properties such as resting membrane potential, voltage-dependent conductance, and post-spike reset voltage. We also explore the inﬂuence that factors such as timing, uncertainty, neuromodulation, and reward should and do have on neuronal dynamics and sensitivity, as the optimal decision strategy depends critically on these factors. 1</p><p>5 0.1858681 <a title="171-tfidf-5" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>6 0.14687039 <a title="171-tfidf-6" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>7 0.14282867 <a title="171-tfidf-7" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>8 0.13456988 <a title="171-tfidf-8" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>9 0.12707295 <a title="171-tfidf-9" href="./nips-2006-Multi-Robot_Negotiation%3A_Approximating_the_Set_of_Subgame_Perfect_Equilibria_in_General-Sum_Stochastic_Games.html">137 nips-2006-Multi-Robot Negotiation: Approximating the Set of Subgame Perfect Equilibria in General-Sum Stochastic Games</a></p>
<p>10 0.12492105 <a title="171-tfidf-10" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>11 0.10984364 <a title="171-tfidf-11" href="./nips-2006-implicit_Online_Learning_with_Kernels.html">203 nips-2006-implicit Online Learning with Kernels</a></p>
<p>12 0.105534 <a title="171-tfidf-12" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>13 0.09468393 <a title="171-tfidf-13" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>14 0.082559109 <a title="171-tfidf-14" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>15 0.082050554 <a title="171-tfidf-15" href="./nips-2006-No-regret_Algorithms_for_Online_Convex_Programs.html">146 nips-2006-No-regret Algorithms for Online Convex Programs</a></p>
<p>16 0.076881818 <a title="171-tfidf-16" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>17 0.074564956 <a title="171-tfidf-17" href="./nips-2006-A_Novel_Gaussian_Sum_Smoother_for_Approximate_Inference_in_Switching_Linear_Dynamical_Systems.html">10 nips-2006-A Novel Gaussian Sum Smoother for Approximate Inference in Switching Linear Dynamical Systems</a></p>
<p>18 0.066301934 <a title="171-tfidf-18" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>19 0.066113561 <a title="171-tfidf-19" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>20 0.062181771 <a title="171-tfidf-20" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, -0.034), (2, -0.396), (3, -0.205), (4, 0.039), (5, -0.049), (6, 0.035), (7, -0.06), (8, 0.36), (9, 0.02), (10, 0.093), (11, -0.072), (12, 0.028), (13, -0.059), (14, 0.047), (15, 0.081), (16, 0.033), (17, -0.031), (18, 0.047), (19, -0.109), (20, -0.054), (21, 0.165), (22, 0.041), (23, 0.049), (24, -0.016), (25, 0.059), (26, -0.102), (27, -0.058), (28, -0.047), (29, 0.056), (30, 0.069), (31, 0.054), (32, -0.131), (33, 0.021), (34, 0.04), (35, -0.062), (36, -0.037), (37, -0.079), (38, -0.035), (39, 0.024), (40, -0.058), (41, -0.046), (42, -0.082), (43, -0.106), (44, 0.025), (45, 0.018), (46, 0.023), (47, 0.013), (48, 0.007), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96390426 <a title="171-lsi-1" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>2 0.89720231 <a title="171-lsi-2" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><p>3 0.71050477 <a title="171-lsi-3" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>Author: Mohammad Ghavamzadeh, Yaakov Engel</p><p>Abstract: Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. Since Monte Carlo methods tend to have high variance, a large number of samples is required, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost. 1</p><p>4 0.63440579 <a title="171-lsi-4" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>Author: Sandeep Pandey, Christopher Olston</p><p>Abstract: We consider how a search engine should select advertisements to display with search results, in order to maximize its revenue. Under the standard “pay-per-click” arrangement, revenue depends on how well the displayed advertisements appeal to users. The main diﬃculty stems from new advertisements whose degree of appeal has yet to be determined. Often the only reliable way of determining appeal is exploration via display to users, which detracts from exploitation of other advertisements known to have high appeal. Budget constraints and ﬁnite advertisement lifetimes make it necessary to explore as well as exploit. In this paper we study the tradeoﬀ between exploration and exploitation, modeling advertisement placement as a multi-armed bandit problem. We extend traditional bandit formulations to account for budget constraints that occur in search engine advertising markets, and derive theoretical bounds on the performance of a family of algorithms. We measure empirical performance via extensive experiments over real-world data. 1</p><p>5 0.55356795 <a title="171-lsi-5" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>Author: Alborz Geramifard, Michael Bowling, Martin Zinkevich, Richard S. Sutton</p><p>Abstract: We present new theoretical and empirical results with the iLSTD algorithm for policy evaluation in reinforcement learning with linear function approximation. iLSTD is an incremental method for achieving results similar to LSTD, the dataefﬁcient, least-squares version of temporal difference learning, without incurring the full cost of the LSTD computation. LSTD is O(n2 ), where n is the number of parameters in the linear function approximator, while iLSTD is O(n). In this paper, we generalize the previous iLSTD algorithm and present three new results: (1) the ﬁrst convergence proof for an iLSTD algorithm; (2) an extension to incorporate eligibility traces without changing the asymptotic computational complexity; and (3) the ﬁrst empirical results with an iLSTD algorithm for a problem (mountain car) with feature vectors large enough (n = 10, 000) to show substantial computational advantages over LSTD. 1</p><p>6 0.5451169 <a title="171-lsi-6" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>7 0.51451534 <a title="171-lsi-7" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>8 0.47900799 <a title="171-lsi-8" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>9 0.46209663 <a title="171-lsi-9" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>10 0.43627143 <a title="171-lsi-10" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>11 0.4077659 <a title="171-lsi-11" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>12 0.39071572 <a title="171-lsi-12" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>13 0.36144203 <a title="171-lsi-13" href="./nips-2006-Multi-Robot_Negotiation%3A_Approximating_the_Set_of_Subgame_Perfect_Equilibria_in_General-Sum_Stochastic_Games.html">137 nips-2006-Multi-Robot Negotiation: Approximating the Set of Subgame Perfect Equilibria in General-Sum Stochastic Games</a></p>
<p>14 0.31790242 <a title="171-lsi-14" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>15 0.30394459 <a title="171-lsi-15" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>16 0.29773071 <a title="171-lsi-16" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>17 0.29487756 <a title="171-lsi-17" href="./nips-2006-No-regret_Algorithms_for_Online_Convex_Programs.html">146 nips-2006-No-regret Algorithms for Online Convex Programs</a></p>
<p>18 0.28121346 <a title="171-lsi-18" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>19 0.26289403 <a title="171-lsi-19" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>20 0.22613256 <a title="171-lsi-20" href="./nips-2006-Near-Uniform_Sampling_of_Combinatorial_Spaces_Using_XOR_Constraints.html">144 nips-2006-Near-Uniform Sampling of Combinatorial Spaces Using XOR Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.088), (2, 0.047), (3, 0.019), (7, 0.072), (9, 0.069), (17, 0.296), (22, 0.062), (44, 0.083), (57, 0.05), (65, 0.049), (69, 0.038), (71, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74221331 <a title="171-lda-1" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>2 0.51565498 <a title="171-lda-2" href="./nips-2006-Geometric_entropy_minimization_%28GEM%29_for_anomaly_detection_and_localization.html">85 nips-2006-Geometric entropy minimization (GEM) for anomaly detection and localization</a></p>
<p>Author: Alfred O. Hero</p><p>Abstract: We introduce a novel adaptive non-parametric anomaly detection approach, called GEM, that is based on the minimal covering properties of K-point entropic graphs when constructed on N training samples from a nominal probability distribution. Such graphs have the property that as N → ∞ their span recovers the entropy minimizing set that supports at least ρ = K/N (100)% of the mass of the Lebesgue part of the distribution. When a test sample falls outside of the entropy minimizing set an anomaly can be declared at a statistical level of signiﬁcance α = 1 − ρ. A method for implementing this non-parametric anomaly detector is proposed that approximates this minimum entropy set by the inﬂuence region of a K-point entropic graph built on the training data. By implementing an incremental leave-one-out k-nearest neighbor graph on resampled subsets of the training data GEM can efﬁciently detect outliers at a given level of signiﬁcance and compute their empirical p-values. We illustrate GEM for several simulated and real data sets in high dimensional feature spaces. 1</p><p>3 0.51447845 <a title="171-lda-3" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><p>4 0.50948364 <a title="171-lda-4" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>5 0.50480914 <a title="171-lda-5" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>Author: Honghao Shan, Lingyun Zhang, Garrison W. Cottrell</p><p>Abstract: Independent Component Analysis (ICA) is a popular method for extracting independent features from visual data. However, as a fundamentally linear technique, there is always nonlinear residual redundancy that is not captured by ICA. Hence there have been many attempts to try to create a hierarchical version of ICA, but so far none of the approaches have a natural way to apply them more than once. Here we show that there is a relatively simple technique that transforms the absolute values of the outputs of a previous application of ICA into a normal distribution, to which ICA maybe applied again. This results in a recursive ICA algorithm that may be applied any number of times in order to extract higher order structure from previous layers. 1</p><p>6 0.50382876 <a title="171-lda-6" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>7 0.50212485 <a title="171-lda-7" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>8 0.50085181 <a title="171-lda-8" href="./nips-2006-An_EM_Algorithm_for_Localizing_Multiple_Sound_Sources_in_Reverberant_Environments.html">27 nips-2006-An EM Algorithm for Localizing Multiple Sound Sources in Reverberant Environments</a></p>
<p>9 0.49973825 <a title="171-lda-9" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>10 0.4995763 <a title="171-lda-10" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>11 0.4989543 <a title="171-lda-11" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>12 0.49830538 <a title="171-lda-12" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>13 0.49801034 <a title="171-lda-13" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>14 0.49770004 <a title="171-lda-14" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>15 0.49704722 <a title="171-lda-15" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>16 0.49586278 <a title="171-lda-16" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>17 0.4949064 <a title="171-lda-17" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>18 0.49435955 <a title="171-lda-18" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>19 0.4943102 <a title="171-lda-19" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>20 0.49272603 <a title="171-lda-20" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
