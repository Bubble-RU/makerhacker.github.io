<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-25" href="#">nips2006-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</h1>
<br/><p>Source: <a title="nips-2006-25-pdf" href="http://papers.nips.cc/paper/3151-an-application-of-reinforcement-learning-to-aerobatic-helicopter-flight.pdf">pdf</a></p><p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><p>Reference: <a title="nips-2006-25-reference" href="../nips2006_reference/nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Stanford University Stanford, CA 94305  Abstract Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. [sent-3, score-0.738]
</p><p>2 This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. [sent-4, score-1.162]
</p><p>3 Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. [sent-5, score-0.952]
</p><p>4 We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. [sent-6, score-1.565]
</p><p>5 Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. [sent-7, score-0.305]
</p><p>6 1  Introduction  Autonomous helicopter ﬂight represents a challenging control problem with high-dimensional, asymmetric, noisy, nonlinear, non-minimum phase dynamics. [sent-9, score-0.761]
</p><p>7 The control of autonomous helicopters thus provides a challenging and important testbed for learning and control algorithms. [sent-15, score-0.403]
</p><p>8 In the “upright ﬂight regime” there has recently been considerable progress in autonomous helicopter ﬂight. [sent-16, score-0.903]
</p><p>9 For example, Bagnell and Schneider [6] achieved sustained autonomous hover. [sent-17, score-0.262]
</p><p>10 [17] achieved sustained autonomous hover and accurate ﬂight in regimes where the helicopter’s orientation is fairly close to upright. [sent-20, score-0.372]
</p><p>11 In contrast, autonomous ﬂight achievements in other ﬂight regimes have been very limited. [sent-24, score-0.257]
</p><p>12 In particular, we present the ﬁrst successful autonomous completion of the following four maneuvers: forward ﬂip and axial roll at low speed, tail-in funnel, and nose-in funnel. [sent-30, score-0.391]
</p><p>13 Not only are we ﬁrst to autonomously complete such a single ﬂip and roll, our controllers are also able to continuously repeat the ﬂips and rolls without any pauses in between. [sent-31, score-0.194]
</p><p>14 The number of ﬂips and rolls and the duration of the funnel trajectories were chosen to be sufﬁciently large to demonstrate that the helicopter could continue the maneuvers indeﬁnitely (assuming unlimited fuel and battery endurance). [sent-33, score-1.142]
</p><p>15 In the (forward) ﬂip, the helicopter rotates 360 degrees forward around its lateral axis (the axis going from the right to the left of the helicopter). [sent-35, score-0.883]
</p><p>16 To prevent altitude loss during the maneuver, the helicopter pushes itself back up by using the (inverted) main rotor thrust halfway through the ﬂip. [sent-36, score-0.951]
</p><p>17 In the (right) axial roll the helicopter rotates 360 degrees around its longitudinal axis (the axis going from the back to the front of the helicopter). [sent-37, score-0.982]
</p><p>18 Similarly to the ﬂip, the helicopter prevents altitude  loss by pushing itself back up by using the (inverted) main rotor thrust halfway through the roll. [sent-38, score-0.951]
</p><p>19 In the tail-in funnel, the helicopter repeatedly ﬂies a circle sideways with the tail pointing to the center of the circle. [sent-39, score-0.804]
</p><p>20 For the trajectory to be a funnel maneuver, the helicopter speed and the circle radius are chosen such that the helicopter must pitch up steeply to stay in the circle. [sent-40, score-1.798]
</p><p>21 The nose-in funnel is similar to the tail-in funnel, the difference being that the nose points to the center of the circle throughout the maneuver. [sent-41, score-0.338]
</p><p>22 We discuss our apprenticeship learning approach to choosing the reward function, as well as other design decisions and lessons learned. [sent-46, score-0.196]
</p><p>23 Section 4 describes our helicopter platform and our experimental results. [sent-47, score-0.67]
</p><p>24 Movies of our autonomous helicopter ﬂights are available at the following webpage: http://www. [sent-49, score-0.903]
</p><p>25 1 Data Collection The E 3 -family of algorithms [12] and its extensions [11, 7, 10] are the state of the art RL algorithms for autonomous data collection. [sent-54, score-0.282]
</p><p>26 Unfortunately, such exploration policies do not even try to ﬂy the helicopter well, and thus would invariably lead to crashes. [sent-56, score-0.721]
</p><p>27 Collect data from a human pilot ﬂying the desired maneuvers with the helicopter. [sent-58, score-0.232]
</p><p>28 This procedure has similarities with model-based RL and with the common approach in control to ﬁrst perform system identiﬁcation and then ﬁnd a controller using the resulting model. [sent-66, score-0.21]
</p><p>29 2 Model Learning The helicopter state s comprises its position (x, y, z), orientation (expressed as a unit quaternion), velocity (x, y, z) and angular velocity (ωx , ωy , ωz ). [sent-72, score-0.89]
</p><p>30 The helicopter is controlled by a 4-dimensional ˙ ˙ ˙ action space (u1 , u2 , u3 , u4 ). [sent-73, score-0.67]
</p><p>31 By using the cyclic pitch (u1 , u2 ) and tail rotor (u3 ) controls, the pilot can rotate the helicopter around each of its main axes and bring the helicopter to any orientation. [sent-74, score-1.706]
</p><p>32 This allows the pilot to direct the thrust of the main rotor in any particular direction (and thus ﬂy in any particular direction). [sent-75, score-0.344]
</p><p>33 By adjusting the collective pitch angle (control input u4 ), the pilot can adjust the thrust generated by the main rotor. [sent-76, score-0.382]
</p><p>34 For a positive collective pitch angle the main rotor will blow air downward relative to the helicopter. [sent-77, score-0.4]
</p><p>35 For a negative collective pitch angle the main rotor will blow air upward relative to the helicopter. [sent-78, score-0.375]
</p><p>36 Accelerations are then integrated to obtain the helicopter states over time. [sent-81, score-0.67]
</p><p>37 The key idea from [1] is that, after subtracting out the effects of gravity, the forces and moments acting on the helicopter are independent of position and orientation of the helicopter, when expressed in a “body coordinate frame”, a coordinate frame attached to the body of the helicopter. [sent-82, score-0.769]
</p><p>38 We estimate the coefﬁcients A· , B· , C· , D· and E· from helicopter ﬂight data. [sent-88, score-0.67]
</p><p>39 The coefﬁcient D0 captures sideways acceleration of the helicopter due to thrust generated by the tail rotor. [sent-92, score-0.882]
</p><p>40 The term E0 (xb , y b , z b ) 2 models translational lift: the additional lift the helicopter gets ˙ ˙ ˙ when ﬂying at higher speed. [sent-93, score-0.719]
</p><p>41 Speciﬁcally, during hover, the helicopter’s rotor imparts a downward velocity on the air above and below it. [sent-94, score-0.264]
</p><p>42 This downward velocity reduces the effective pitch (angle of attack) of the rotor blades, causing less lift to be produced [14, 20]. [sent-95, score-0.357]
</p><p>43 As the helicopter transitions into faster ﬂight, this region of altered airﬂow is left behind and the blades enter “clean” air. [sent-96, score-0.726]
</p><p>44 Thus, the angle of attack is higher and more lift is produced for a given choice of the collective control (u4 ). [sent-97, score-0.212]
</p><p>45 The translational lift term was important for modeling the helicopter dynamics during the funnels. [sent-98, score-0.75]
</p><p>46 The coefﬁcient C24 captures the pitch acceleration due to main rotor thrust. [sent-99, score-0.285]
</p><p>47 This coefﬁcient is nonzero since (after equipping our helicopter with our sensor packages) the center of gravity is further backward than the center of main rotor thrust. [sent-100, score-0.872]
</p><p>48 (2) Our model’s state does not include the blade-ﬂapping angles, which are the angles the rotor blades make with the helicopter body while sweeping through the air. [sent-104, score-0.945]
</p><p>49 Both inertial coupling and blade ﬂapping have previously been shown to improve accuracy of helicopter models for other RC helicopters. [sent-105, score-0.722]
</p><p>50 1s time scale used for control—the blade ﬂapping angles’ effects are sufﬁciently well captured by using a ﬁrst order model from cyclic inputs to roll and pitch rates. [sent-109, score-0.246]
</p><p>51 Such a ﬁrst order model maps cyclic inputs to angular accelerations (rather than the steady state angular rate), effectively capturing the delay introduced by the blades reacting (moving) ﬁrst before the helicopter body follows. [sent-110, score-0.97]
</p><p>52 It is well-known that the optimal policy for the LQR control problem is a linear feedback controller which can be efﬁciently computed using dynamic programming. [sent-125, score-0.284]
</p><p>53 The standard extension (which we 0 H use) expresses the dynamics and reward function as a function of the error state e(t) = s(t) − s∗ (t) rather than the actual state s(t). [sent-130, score-0.215]
</p><p>54 Compute a linear approximation to the dynamics and a quadratic approximation to the reward function around the trajectory obtained when using the current policy. [sent-135, score-0.187]
</p><p>55 Compute the optimal policy for the LQR problem obtained in Step 1 and set the current policy equal to the optimal policy for the LQR problem. [sent-137, score-0.222]
</p><p>56 This axis angle representation results in the linearizations being more accurate approximations of the non-linear model since the axis angle representation maps more directly to the angular rates than naively differencing the quaternions or Euler angles. [sent-144, score-0.243]
</p><p>57 Using DDP as thus far explained resulted in unstable controllers on the real helicopter: The controllers tended to rapidly switch between low and high values, which resulted in poor ﬂight performance. [sent-146, score-0.232]
</p><p>58 Our funnel controllers performed signiﬁcantly better with integral control. [sent-173, score-0.415]
</p><p>59 3 Trade-offs in the reward function Our reward function contained 24 features, consisting of the squared error state variables, the squared inputs, the squared change in inputs between consecutive timesteps, and the squared integral of the error state variables. [sent-180, score-0.343]
</p><p>60 For the reinforcement learning algorithm to ﬁnd a controller that ﬂies “well,” it is critical that the correct trade-off between these features is speciﬁed. [sent-181, score-0.219]
</p><p>61 1 Experimental Platform The helicopter used is an XCell Tempest, a competition-class aerobatic helicopter (length 54”, height 19”, weight 13 lbs), powered by a 0. [sent-190, score-1.419]
</p><p>62 We instrumented the helicopter with a Microstrain 3DM-GX1 orientation sensor, and a Novatel RT2 GPS receiver. [sent-193, score-0.711]
</p><p>63 Later, we used three PointGrey DragonFly2 cameras that track the helicopter from the ground. [sent-199, score-0.67]
</p><p>64 The model used to design the ﬂip and roll controllers is estimated from 5 minutes of ﬂight data during which the pilot performs frequency sweeps on each of the four control inputs (which covers as similar a ﬂight regime as possible without having to invert the helicopter). [sent-207, score-0.483]
</p><p>65 For the funnel controllers, we learn a model from the same frequency sweeps and from our pilot ﬂying the funnels. [sent-208, score-0.403]
</p><p>66 For the funnels, our initial controllers did not perform as well, and we performed two iterations of the apprenticeship learning algorithm described in Section 2. [sent-210, score-0.194]
</p><p>67 1 Flip In the ideal forward ﬂip, the helicopter rotates 360 degrees forward around its lateral axis (the axis going from the right to the left of the helicopter) while staying in place. [sent-218, score-0.924]
</p><p>68 The top row of Figure 1 (a) shows a series of snapshots of our helicopter during an autonomous ﬂip. [sent-219, score-0.993]
</p><p>69 In the ﬁrst frame, the helicopter is hovering upright autonomously. [sent-220, score-0.763]
</p><p>70 At this point, the helicopter does not have the ability to counter its descent since it can only produce thrust in the direction of the main rotor. [sent-222, score-0.76]
</p><p>71 The ﬂip continues until the helicopter is completely inverted. [sent-223, score-0.67]
</p><p>72 At this moment, the controller must apply negative collective to regain altitude lost during the half-ﬂip, while continuing the ﬂip and returning to the upright position. [sent-224, score-0.323]
</p><p>73 2 Roll In the ideal axial roll, the helicopter rotates 360 degrees around its longitudinal axis (the axis going from the back to the front of the helicopter) while staying in place. [sent-230, score-0.892]
</p><p>74 The bottom row of Figure 1 (b) shows a series of snapshots of our helicopter during an autonomous roll. [sent-231, score-0.993]
</p><p>75 In the ﬁrst frame, the helicopter is hovering upright autonomously. [sent-232, score-0.763]
</p><p>76 When inverted, the helicopter applies negative collective to regain altitude lost during the ﬁrst half of the roll, while continuing the roll and returning to the upright position. [sent-234, score-0.941]
</p><p>77 3 Tail-In Funnel The tail-in funnel maneuver is essentially a medium to high speed circle ﬂown sideways, with the tail of the helicopter pointed towards the center of the circle. [sent-238, score-1.071]
</p><p>78 Throughout, the helicopter is pitched backwards such that the main rotor thrust not only compensates for gravity, but also provides the centripetal acceleration to stay in the circle. [sent-239, score-0.985]
</p><p>79 For a funnel of radius r at velocity v the centripetal acceleration is v 2 /r, so—assuming the main rotor thrust only provides the centripetal acceleration and compensation for gravity—we obtain a pitch angle θ = atan(v 2 /(rg)). [sent-240, score-0.842]
</p><p>80 4 For the funnel reported in this paper, we had H = 80 s, r = 5 m, and v = 5. [sent-242, score-0.27]
</p><p>81 Figure 1 (c) shows an overlay of snapshots of the helicopter throughout a tail-in funnel. [sent-244, score-0.838]
</p><p>82 The deﬁning characteristic of the funnel is repeatability—the ability to pass consistently through the same points in space after multiple circuits. [sent-245, score-0.27]
</p><p>83 Our autonomous funnels are signiﬁcantly more accurate than funnels ﬂown by expert human pilots. [sent-246, score-0.345]
</p><p>84 In ﬁgure 2 (b) we superimposed the heading of the helicopter on a partial trajectory (showing the entire trajectory with heading superimposed gives a cluttered plot). [sent-248, score-0.946]
</p><p>85 Our autonomous funnels have an RMS position error of 1. [sent-249, score-0.272]
</p><p>86 4 Nose-In Funnel The nose-in funnel maneuver is very similar to the tail-in funnel maneuver, except that the nose points to the center of the circle, rather than the tail. [sent-254, score-0.619]
</p><p>87 Our autonomous nose-in funnel controller results in highly repeatable trajectories (similar to the tail-in funnel), and it achieves a level of performance that is difﬁcult for a human pilot to match. [sent-255, score-0.753]
</p><p>88 5  Conclusion  To summarize, we presented our successful DDP-based control design for four new aerobatic maneuvers: forward ﬂip, sideways roll (at low speed), tail-in funnel, and nose-in funnel. [sent-257, score-0.359]
</p><p>89 The key design decisions for the DDP-based controller to ﬂy our helicopter successfully are the following: 4 The maneuver is actually broken into three parts: an accelerating leg, the funnel leg, and a decelerating leg. [sent-258, score-1.216]
</p><p>90 During the accelerating and decelerating legs, the helicopter accelerates at amax (= 0. [sent-259, score-0.693]
</p><p>91 5 Without the integral of heading error in the cost function we observed signiﬁcantly larger heading errors of 20-40 degrees, which resulted in the linearization being so inaccurate that controllers often failed entirely. [sent-261, score-0.32]
</p><p>92 (c) Overlay of snapshots of the helicopter throughout a tail-in funnel. [sent-265, score-0.799]
</p><p>93 (d) Overlay of snapshots of the helicopter throughout a nose-in funnel. [sent-266, score-0.799]
</p><p>94 )  6  4  4  North (m)  8  6  North (m)  8  2 0 −2  2 0 −2  −4  −4  −6  −6  −8  −8 −8 −6 −4 −2  0  2  4  6  8  East (m)  (a)  −8 −6 −4 −2  0  2  4  6  8  East (m)  (b)  (c)  Figure 2: (a) Trajectory followed by the helicopter during tail-in funnel. [sent-268, score-0.67]
</p><p>95 We used apprenticeship learning algorithms, which take advantage of an expert demonstration, to determine the reward function and to learn the model. [sent-273, score-0.198]
</p><p>96 To the best of our knowledge, these are the most challenging autonomous ﬂight maneuvers achieved to date. [sent-276, score-0.357]
</p><p>97 Acknowledgments We thank Ben Tse for piloting our helicopter and working on the electronics of our helicopter. [sent-277, score-0.67]
</p><p>98 Autonomous helicopter control using reinforcement learning policy search methods. [sent-312, score-0.889]
</p><p>99 Flight test and simulation results for an autonomous aerobatic helicopter. [sent-324, score-0.312]
</p><p>100 Low-cost ﬂight control system for a small autonomous helicopter. [sent-388, score-0.301]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('helicopter', 0.67), ('funnel', 0.27), ('ight', 0.259), ('autonomous', 0.233), ('ddp', 0.146), ('rotor', 0.146), ('controller', 0.142), ('maneuvers', 0.124), ('controllers', 0.116), ('pilot', 0.108), ('ip', 0.093), ('snapshots', 0.09), ('thrust', 0.09), ('roll', 0.09), ('pitch', 0.089), ('reward', 0.086), ('aerobatic', 0.079), ('maneuver', 0.079), ('apprenticeship', 0.078), ('ips', 0.078), ('rolls', 0.078), ('reinforcement', 0.077), ('policy', 0.074), ('trajectory', 0.07), ('lqr', 0.069), ('control', 0.068), ('heading', 0.068), ('accelerations', 0.059), ('upright', 0.059), ('inverted', 0.058), ('blades', 0.056), ('gravity', 0.056), ('collective', 0.054), ('xb', 0.054), ('axis', 0.052), ('acceleration', 0.05), ('lift', 0.049), ('sideways', 0.049), ('state', 0.049), ('velocity', 0.048), ('air', 0.045), ('altitude', 0.045), ('hover', 0.045), ('rotates', 0.045), ('ying', 0.045), ('inputs', 0.044), ('gps', 0.042), ('orientation', 0.041), ('forward', 0.041), ('angle', 0.041), ('funnels', 0.039), ('linearization', 0.039), ('overlay', 0.039), ('throughout', 0.039), ('abbeel', 0.037), ('rl', 0.035), ('frame', 0.034), ('angular', 0.034), ('expert', 0.034), ('apping', 0.034), ('flight', 0.034), ('gavrilets', 0.034), ('helicopters', 0.034), ('hovering', 0.034), ('linearized', 0.034), ('mettler', 0.034), ('novatel', 0.034), ('pointing', 0.033), ('design', 0.032), ('dynamics', 0.031), ('centripetal', 0.029), ('inertial', 0.029), ('sustained', 0.029), ('circle', 0.029), ('integral', 0.029), ('exploration', 0.028), ('axial', 0.027), ('robotics', 0.026), ('sweeps', 0.025), ('downward', 0.025), ('kalman', 0.025), ('regimes', 0.024), ('differential', 0.024), ('body', 0.024), ('degrees', 0.023), ('tail', 0.023), ('phase', 0.023), ('policies', 0.023), ('blade', 0.023), ('coates', 0.023), ('decelerating', 0.023), ('ies', 0.023), ('imu', 0.023), ('linearizations', 0.023), ('longitudinal', 0.023), ('maneuvering', 0.023), ('microstrain', 0.023), ('psu', 0.023), ('regain', 0.023), ('saripalli', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="25-tfidf-1" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><p>2 0.11553057 <a title="25-tfidf-2" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>3 0.079295583 <a title="25-tfidf-3" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>Author: Silvia Richter, Douglas Aberdeen, Jin Yu</p><p>Abstract: Current road-trafﬁc optimisation practice around the world is a combination of hand tuned policies with a small degree of automatic adaption. Even state-ofthe-art research controllers need good models of the road trafﬁc, which cannot be obtained directly from existing sensors. We use a policy-gradient reinforcement learning approach to directly optimise the trafﬁc signals, mapping currently deployed sensor observations to control signals. Our trained controllers are (theoretically) compatible with the trafﬁc system used in Sydney and many other cities around the world. We apply two policy-gradient methods: (1) the recent natural actor-critic algorithm, and (2) a vanilla policy-gradient algorithm for comparison. Along the way we extend natural-actor critic approaches to work for distributed and online inﬁnite-horizon problems. 1</p><p>4 0.075307116 <a title="25-tfidf-4" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><p>5 0.066113561 <a title="25-tfidf-5" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>6 0.059707239 <a title="25-tfidf-6" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>7 0.05674291 <a title="25-tfidf-7" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>8 0.056619752 <a title="25-tfidf-8" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>9 0.055859424 <a title="25-tfidf-9" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>10 0.049546335 <a title="25-tfidf-10" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>11 0.044948652 <a title="25-tfidf-11" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>12 0.044587534 <a title="25-tfidf-12" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>13 0.041003253 <a title="25-tfidf-13" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>14 0.037663665 <a title="25-tfidf-14" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>15 0.030038144 <a title="25-tfidf-15" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>16 0.029603759 <a title="25-tfidf-16" href="./nips-2006-Multi-Robot_Negotiation%3A_Approximating_the_Set_of_Subgame_Perfect_Equilibria_in_General-Sum_Stochastic_Games.html">137 nips-2006-Multi-Robot Negotiation: Approximating the Set of Subgame Perfect Equilibria in General-Sum Stochastic Games</a></p>
<p>17 0.028565655 <a title="25-tfidf-17" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>18 0.024642445 <a title="25-tfidf-18" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>19 0.024584051 <a title="25-tfidf-19" href="./nips-2006-Single_Channel_Speech_Separation_Using_Factorial_Dynamics.html">176 nips-2006-Single Channel Speech Separation Using Factorial Dynamics</a></p>
<p>20 0.022730216 <a title="25-tfidf-20" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.086), (1, -0.032), (2, -0.08), (3, -0.082), (4, 0.011), (5, -0.075), (6, 0.027), (7, -0.04), (8, 0.165), (9, 0.021), (10, -0.015), (11, -0.049), (12, -0.038), (13, -0.016), (14, -0.003), (15, -0.005), (16, -0.013), (17, -0.031), (18, 0.022), (19, 0.01), (20, 0.012), (21, -0.012), (22, 0.019), (23, 0.014), (24, -0.024), (25, 0.024), (26, -0.029), (27, -0.002), (28, -0.009), (29, -0.034), (30, 0.018), (31, -0.034), (32, 0.062), (33, -0.025), (34, -0.04), (35, -0.092), (36, -0.004), (37, -0.013), (38, 0.069), (39, 0.003), (40, 0.125), (41, 0.016), (42, 0.036), (43, 0.049), (44, -0.131), (45, -0.063), (46, 0.086), (47, -0.007), (48, -0.039), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93475199 <a title="25-lsi-1" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><p>2 0.6602751 <a title="25-lsi-2" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>Author: Silvia Richter, Douglas Aberdeen, Jin Yu</p><p>Abstract: Current road-trafﬁc optimisation practice around the world is a combination of hand tuned policies with a small degree of automatic adaption. Even state-ofthe-art research controllers need good models of the road trafﬁc, which cannot be obtained directly from existing sensors. We use a policy-gradient reinforcement learning approach to directly optimise the trafﬁc signals, mapping currently deployed sensor observations to control signals. Our trained controllers are (theoretically) compatible with the trafﬁc system used in Sydney and many other cities around the world. We apply two policy-gradient methods: (1) the recent natural actor-critic algorithm, and (2) a vanilla policy-gradient algorithm for comparison. Along the way we extend natural-actor critic approaches to work for distributed and online inﬁnite-horizon problems. 1</p><p>3 0.59767157 <a title="25-lsi-3" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>4 0.49326551 <a title="25-lsi-4" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>Author: Huan Xu, Shie Mannor</p><p>Abstract: Computation of a satisfactory control policy for a Markov decision process when the parameters of the model are not exactly known is a problem encountered in many practical applications. The traditional robust approach is based on a worstcase analysis and may lead to an overly conservative policy. In this paper we consider the tradeoff between nominal performance and the worst case performance over all possible models. Based on parametric linear programming, we propose a method that computes the whole set of Pareto efﬁcient policies in the performancerobustness plane when only the reward parameters are subject to uncertainty. In the more general case when the transition probabilities are also subject to error, we show that the strategy with the “optimal” tradeoff might be non-Markovian and hence is in general not tractable. 1</p><p>5 0.48209533 <a title="25-lsi-5" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>Author: Gediminas Lukšys, Jérémie Knüsel, Denis Sheynikhovich, Carmen Sandi, Wulfram Gerstner</p><p>Abstract: Stress and genetic background regulate different aspects of behavioral learning through the action of stress hormones and neuromodulators. In reinforcement learning (RL) models, meta-parameters such as learning rate, future reward discount factor, and exploitation-exploration factor, control learning dynamics and performance. They are hypothesized to be related to neuromodulatory levels in the brain. We found that many aspects of animal learning and performance can be described by simple RL models using dynamic control of the meta-parameters. To study the effects of stress and genotype, we carried out 5-hole-box light conditioning and Morris water maze experiments with C57BL/6 and DBA/2 mouse strains. The animals were exposed to different kinds of stress to evaluate its effects on immediate performance as well as on long-term memory. Then, we used RL models to simulate their behavior. For each experimental session, we estimated a set of model meta-parameters that produced the best ﬁt between the model and the animal performance. The dynamics of several estimated meta-parameters were qualitatively similar for the two simulated experiments, and with statistically signiﬁcant differences between different genetic strains and stress conditions. 1</p><p>6 0.45707417 <a title="25-lsi-6" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>7 0.44042337 <a title="25-lsi-7" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>8 0.43547139 <a title="25-lsi-8" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>9 0.42046526 <a title="25-lsi-9" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>10 0.38394329 <a title="25-lsi-10" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>11 0.34400868 <a title="25-lsi-11" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>12 0.33085772 <a title="25-lsi-12" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>13 0.32276368 <a title="25-lsi-13" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>14 0.31950364 <a title="25-lsi-14" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>15 0.31712389 <a title="25-lsi-15" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>16 0.31581095 <a title="25-lsi-16" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>17 0.31468099 <a title="25-lsi-17" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>18 0.29471233 <a title="25-lsi-18" href="./nips-2006-Single_Channel_Speech_Separation_Using_Factorial_Dynamics.html">176 nips-2006-Single Channel Speech Separation Using Factorial Dynamics</a></p>
<p>19 0.28729492 <a title="25-lsi-19" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>20 0.27732047 <a title="25-lsi-20" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.048), (2, 0.019), (3, 0.052), (7, 0.048), (9, 0.023), (12, 0.014), (22, 0.033), (24, 0.379), (25, 0.014), (44, 0.048), (47, 0.012), (57, 0.055), (59, 0.026), (65, 0.033), (69, 0.034), (71, 0.031), (82, 0.013), (91, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80688107 <a title="25-lda-1" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><p>2 0.63447422 <a title="25-lda-2" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>Author: Amit Gore, Shantanu Chakrabartty</p><p>Abstract: A key challenge in designing analog-to-digital converters for cortically implanted prosthesis is to sense and process high-dimensional neural signals recorded by the micro-electrode arrays. In this paper, we describe a novel architecture for analog-to-digital (A/D) conversion that combines Σ∆ conversion with spatial de-correlation within a single module. The architecture called multiple-input multiple-output (MIMO) Σ∆ is based on a min-max gradient descent optimization of a regularized linear cost function that naturally lends to an A/D formulation. Using an online formulation, the architecture can adapt to slow variations in cross-channel correlations, observed due to relative motion of the microelectrodes with respect to the signal sources. Experimental results with real recorded multi-channel neural data demonstrate the effectiveness of the proposed algorithm in alleviating cross-channel redundancy across electrodes and performing data-compression directly at the A/D converter. 1</p><p>3 0.46442229 <a title="25-lda-3" href="./nips-2006-Sparse_Representation_for_Signal_Classification.html">179 nips-2006-Sparse Representation for Signal Classification</a></p>
<p>Author: Ke Huang, Selin Aviyente</p><p>Abstract: In this paper, application of sparse representation (factorization) of signals over an overcomplete basis (dictionary) for signal classiﬁcation is discussed. Searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two terms: one that measures the signal reconstruction error and another that measures the sparsity. This objective function works well in applications where signals need to be reconstructed, like coding and denoising. On the other hand, discriminative methods, such as linear discriminative analysis (LDA), are better suited for classiﬁcation tasks. However, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction. In this paper, we present a theoretical framework for signal classiﬁcation with sparse representation. The approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions: noise, missing data and outliers. The proposed approach is therefore capable of robust classiﬁcation with a sparse representation of signals. The theoretical results are demonstrated with signal classiﬁcation tasks, showing that the proposed approach outperforms the standard discriminative methods and the standard sparse representation in the case of corrupted signals. 1</p><p>4 0.30226791 <a title="25-lda-4" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>5 0.29741779 <a title="25-lda-5" href="./nips-2006-A_recipe_for_optimizing_a_time-histogram.html">17 nips-2006-A recipe for optimizing a time-histogram</a></p>
<p>Author: Hideaki Shimazaki, Shigeru Shinomoto</p><p>Abstract: The time-histogram method is a handy tool for capturing the instantaneous rate of spike occurrence. In most of the neurophysiological literature, the bin size that critically determines the goodness of the ﬁt of the time-histogram to the underlying rate has been selected by individual researchers in an unsystematic manner. We propose an objective method for selecting the bin size of a time-histogram from the spike data, so that the time-histogram best approximates the unknown underlying rate. The resolution of the histogram increases, or the optimal bin size decreases, with the number of spike sequences sampled. It is notable that the optimal bin size diverges if only a small number of experimental trials are available from a moderately ﬂuctuating rate process. In this case, any attempt to characterize the underlying spike rate will lead to spurious results. Given a paucity of data, our method can also suggest how many more trials are needed until the set of data can be analyzed with the required resolution. 1</p><p>6 0.29649711 <a title="25-lda-6" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>7 0.29597941 <a title="25-lda-7" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>8 0.29580259 <a title="25-lda-8" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>9 0.29530832 <a title="25-lda-9" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>10 0.29441947 <a title="25-lda-10" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>11 0.29437613 <a title="25-lda-11" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>12 0.29221839 <a title="25-lda-12" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>13 0.29192549 <a title="25-lda-13" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>14 0.2914196 <a title="25-lda-14" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>15 0.29137456 <a title="25-lda-15" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>16 0.29096437 <a title="25-lda-16" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>17 0.29093501 <a title="25-lda-17" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>18 0.2906912 <a title="25-lda-18" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>19 0.28972208 <a title="25-lda-19" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>20 0.28963697 <a title="25-lda-20" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
