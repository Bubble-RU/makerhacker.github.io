<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2006-Bayesian Image Super-resolution, Continued</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-42" href="#">nips2006-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 nips-2006-Bayesian Image Super-resolution, Continued</h1>
<br/><p>Source: <a title="nips-2006-42-pdf" href="http://papers.nips.cc/paper/3037-bayesian-image-super-resolution-continued.pdf">pdf</a></p><p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>Reference: <a title="nips-2006-42-reference" href="../nips2006_reference/nips-2006-Bayesian_Image_Super-resolution%2C_Continued_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. [sent-11, score-1.061]
</p><p>2 In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. [sent-12, score-0.723]
</p><p>3 By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. [sent-13, score-0.855]
</p><p>4 In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. [sent-14, score-0.216]
</p><p>5 We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach. [sent-15, score-0.081]
</p><p>6 1 Introduction Multi-frame image super-resolution refers to the process by which a group of images of the same scene are fused to produce an image or images with a higher spatial resolution, or with more visible detail in the high spatial frequency features [7]. [sent-16, score-0.912]
</p><p>7 Limits on the resolution of the original imaging device can be improved by exploiting the relative sub-pixel motion between the scene and the imaging plane. [sent-18, score-0.252]
</p><p>8 No matter how accurate the registration estimate, there will be some residual uncertainty associated with the parameters [13]. [sent-19, score-0.676]
</p><p>9 We propose a scheme to deal with this uncertainty by integrating over the registration parameters, and demonstrate improved results on synthetic and real digital image data. [sent-20, score-1.125]
</p><p>10 Image registration and super-resolution are often treated as distinct processes, to be considered sequentially [1, 3, 7]. [sent-21, score-0.619]
</p><p>11 demonstrated that the low-resolution image registration can be updated using the super-resolution image estimate, and that this improves a Maximum a Posteriori (MAP) super-resolution image estimate [5]. [sent-23, score-1.55]
</p><p>12 used a similar joint MAP approach to learn more general geometric and photometric registrations, the super-resolution image, and values for the prior’s parameters simultaneously [12]. [sent-25, score-0.229]
</p><p>13 This gives an improvement in the accuracy of the recovered registration (measured against known truth on synthetic data) compared to the MAP approach. [sent-27, score-0.758]
</p><p>14 It is generally more desirable to integrate over the registration parameters rather than the superresolution image, because it is the registration that constitutes the “nuisance parameters”, and the super-resolution image that we wish to estimate. [sent-30, score-1.657]
</p><p>15 We derive a new view of Bayesian image superresolution in which a MAP high-resolution image estimate is found by marginalizing over the uncertain registration parameters. [sent-31, score-1.433]
</p><p>16 Memory requirements are considerably lower than the imageintegrating case; while the algorithm is more costly than a simple MAP super-resolution estimate, it is not infeasible to run on images of several hundred pixels in size. [sent-32, score-0.172]
</p><p>17 Section 4 evaluates results on synthetically-generated sequences (with ground truth for comparison), and on a real data example. [sent-34, score-0.174]
</p><p>18 2 Generative model The generative model for multi-frame super-resolution assumes a known scene x (vectorized, size N × 1), and a given registration vector θ(k) . [sent-36, score-0.666]
</p><p>19 These are used to generate a vectorized low-resolution image y(k) with M pixels through a system matrix W(k) . [sent-37, score-0.389]
</p><p>20 noise with precision β is then added to y(k) , y(k) (k)  (k)  = λ(k) W θ(k) x + λβ + α  (k)  (1)  ∼ N 0, β −1 I . [sent-41, score-0.03]
</p><p>21 (2)  Photometric parameters λα and λβ provide a global afﬁne correction for the scene illumination, and λβ is simply an M × 1 vector ﬁlled out with the value of λβ . [sent-42, score-0.075]
</p><p>22 Each row of W(k) constructs a single pixel in y(k) , and the row’s entries are the vectorized and point-spread function (PSF) response for each low-resolution pixel, in the frame of the super-resolution image [2, 3, 16]. [sent-43, score-0.487]
</p><p>23 The PSF is usually assumed to be an isotropic Gaussian on the imaging plane, though for some motion models (e. [sent-44, score-0.14]
</p><p>24 planar projective) this does not necessarily lead to a Gaussian distribution on the frame of x. [sent-46, score-0.023]
</p><p>25 For an individual low-resolution image, given registrations and x, the data likelihood is p y(k) x, θ(k) , λ(k)  =  β 2π  M 2  exp −  β (k) y(k) − λ(k) W θ (k) x − λβ α 2  2 2  . [sent-47, score-0.151]
</p><p>26 This belongs to a family of functions often favored over Gaussians for super-resolution image priors [2, 3, 14] because the Huber distribution’s heavy tails mean image edges are penalized less severely. [sent-50, score-0.643]
</p><p>27 The difﬁculty in computing the partition function Zx is a consideration when marginalizing over x as in [16], though for the MAP image estimate, a value for this scale factor is not required. [sent-51, score-0.418]
</p><p>28 Tipping and Bishop’s approach takes an ML estimate of the registration by marginalizing over x, then calculates the super-resolution estimate as in (9). [sent-54, score-0.781]
</p><p>29 While Tipping and Bishop did not include a photometric model, the equivalent expression to be maximized with respect to θ and λ is K  y(y)  p  θ (k) , λ(k)  =  p y(y) x, θ(k) , λ(k) dx. [sent-55, score-0.193]
</p><p>30 p (x)  (10)  k=1  Note that Tipping and Bishop’s work does employ the same data likelihood expression as in (3), which forced them to select a Gaussian form for p (x), rather than a more suitable image prior, in order to keep the integral tractable. [sent-56, score-0.373]
</p><p>31 Finally, in this paper we ﬁnd x through marginalizing over θ and λ, so that a MAP estimate of x can be obtained by maximizing p x y(k) directly with respect to x. [sent-57, score-0.153]
</p><p>32 Note that the integral does not involve the prior, p (x). [sent-59, score-0.049]
</p><p>33 3 Marginalizing over registration parameters In order to obtain an expression for p x| y(k)  from expressions (3), (6) and (7) above, the ¯ (k) ¯ parameter variations δ must be integrated out of the problem. [sent-60, score-0.673]
</p><p>34 Registration estimates θ , λα ¯β can be obtained using classical registration methods, either intensity-based [8] or estimation and λ from image points [6], and the diagonal matrix C is constructed to reﬂect the conﬁdence in each parameter estimate. [sent-61, score-0.944]
</p><p>35 This might mean a standard deviation of a tenth of a low-resolution pixel on image translation parameters, or a few gray levels’ shift on the illumination model, for instance. [sent-62, score-0.624]
</p><p>36 (k)  The integral performed is p x| y(k)  = K  ×  exp − k=1  p β 2  1 y(k)  β 2π  KM 2  b 2π  Kn 2  1 ν exp − ρ (Dx, α) Zx 2 (k)  y(k) − λ(k) W θ (k) x − λβ α  2 2  1 + δ (k) C(k)−1 δ (k) 2  dδ, (12)  where δ T = δ (1)T , δ (2)T , . [sent-63, score-0.137]
</p><p>37 , δ (K)T and all the λ and θ parameters are functions of δ as in (4). [sent-66, score-0.028]
</p><p>38 Finally, letting S =  β 2H  + V−1 ,  exp {f } dδ  = =  β exp − F 2 β exp − F 2  1 β exp − GT δ − δ T Sδ dδ 2 2 nK 1 β 2 T −1 − (2π) 2 |S| 2 exp G S G . [sent-69, score-0.22]
</p><p>39 8  (17) (18)  The objective function, L, to be minimized with respect to x is obtained by taking the negative log of (12), using the result from (18), and neglecting the constant terms: L =  ν β 1 β 2 T −1 ρ (Dx, α) + F + log |S| − G S G. [sent-70, score-0.028]
</p><p>40 1 Implementation notes Notice that the value F from (16) is simply the reprojection error of the current estimate of x at the mean registration parameter values, and that gradients of this expression with respect to the λ parameters, and with respect to x can both be found analytically. [sent-73, score-0.773]
</p><p>41 To ﬁnd the gradient with respect to (k) a geometric registration parameter θi , and elements of the Hessian involving it, a central difference scheme involving only the k th image is used. [sent-74, score-1.007]
</p><p>42 Mean values for the registration are computed by standard registration techniques, and x is initialized using around 10 iterations of SCG to ﬁnd the maximum likelihood solution evaluated at these mean 1 parameters. [sent-75, score-1.261]
</p><p>43 Additionally, pixel values are scaled to lie between − 2 and 1 , and the ML solution is 2 bounded to lie within these values in order to curb the severe overﬁtting usually observed in ML super-resolution results. [sent-76, score-0.1]
</p><p>44 In our implementation, the parameters representing the λ values are scaled so that they share the same standard deviations as the θ parameters, which represent the sub-pixel geometric registration shifts, which makes the matrix V a multiple of the identity. [sent-77, score-0.761]
</p><p>45 The scale factors are chosen so that one standard deviation in λβ gives a 10-gray-level shift, and one standard deviation in λα varies pixel values by around 10 gray levels at mean image intensity. [sent-78, score-0.589]
</p><p>46 4 Results The ﬁrst experiment takes a sixteen-image synthetic dataset created from an eyechart image. [sent-79, score-0.088]
</p><p>47 Data is generated at a zoom factor of 4, using a 2D translation-only motion model, and the two-parameter global afﬁne illumination model described above, giving a total of four registration parameters per low-resolution image. [sent-80, score-0.866]
</p><p>48 Gaussian noise with standard deviation equivalent to 5 gray levels is added to each low-resolution pixel independently. [sent-81, score-0.217]
</p><p>49 The sub-pixel perturbations are evenly spaced over a grid up to plus or minus one half of a low-resolution pixel, giving a similar setup to that described in [10], but with additional lighting variation. [sent-82, score-0.078]
</p><p>50 The ground truth image and two of the low-resolution images appear in the ﬁrst row of Figure 1. [sent-83, score-0.59]
</p><p>51 Geometric and photometric registration parameters were initialized to the identity, and the images were registered using an iterative intensity-based scheme. [sent-84, score-0.893]
</p><p>52 The resulting parameter values were used to recover two sets of super-resolution images: one using the standard Huber MAP algorithm, and the second using our extension integrating over the registration uncertainty. [sent-85, score-0.74]
</p><p>53 01 for all runs, and ν was varied over a range of possible values representing ratios between ν and the image noise precision β. [sent-87, score-0.328]
</p><p>54 The images giving lowest RMS error from each set are displayed in the second row of Figure 1. [sent-88, score-0.175]
</p><p>55 Visually, the differences between the images are subtle, though the bottom row of letters is better deﬁned in the output from the new algorithm. [sent-89, score-0.213]
</p><p>56 Plotting the RMSE as a function of ν in Figure 2, we see that the proposed registration-integrating approach achieves a lower error, compared to the ground truth high-resolution image, than the standard Huber MAP algorithm for any choice of prior strength, ν in the optimal region. [sent-90, score-0.23]
</p><p>57 (a) ground truth high−res  (d) best Huber (err = 15. [sent-91, score-0.146]
</p><p>58 The variation in intensity is clearly visible, and the sub-pixel displacements necessary for multi-frame image super-resolution are most apparent on the “D” characters to the right of each image; (d) The best (ı. [sent-95, score-0.321]
</p><p>59 minimum MSE – see Figure 2) image from the regular Huber MAP algorithm, having super-resolved the dataset multiple times with different prior strength settings; (e) The best result using out approach of integrating over θ and λ. [sent-97, score-0.57]
</p><p>60 As well as having a lower RMSE, note the improvement in black-white edge detail on some of the letters on the bottom line. [sent-98, score-0.064]
</p><p>61 The second experiment uses real data with a 2D translation motion model and an afﬁne lighting model exactly as above. [sent-99, score-0.171]
</p><p>62 The ﬁrst and last images appear on the top row of Figure 3. [sent-100, score-0.146]
</p><p>63 Image registration was carried out in the same manner as before, and the geometric parameters agree with the provided homographies to within a few hundredths of a pixel. [sent-101, score-0.709]
</p><p>64 Super-resolution images were created  RMSE comparison 23 Standard Huber MAP Integrating over registrations and illumination  22  RMSE in gray levels  21 20 19 18 17 16 15 14  0  0. [sent-102, score-0.392]
</p><p>65 08 ratio of prior strength parameter, ν, and noise precision, β  0. [sent-110, score-0.108]
</p><p>66 1  Figure 2: Plot showing the variation of RMSE with prior strength for the standard Huber-prior MAP super-resolution method and our approach integrating over θ and λ. [sent-112, score-0.229]
</p><p>67 The images corresponding to the minima of the two curves are shown in Figure 1 for a number of ν values, the equivalent values to those quoted in [3] were found subjectively to be the most suitable. [sent-113, score-0.149]
</p><p>68 The covariance of the registration values was chosen to be similar to that used in the synthetic experiments. [sent-114, score-0.672]
</p><p>69 Finally, Tipping and Bishop’s method was extended to cover the illumination model and used to register and super-resolve the dataset, using the same PSF standard deviation (0. [sent-115, score-0.203]
</p><p>70 The three sets of results on the real data sequence are shown in the middle and bottom rows of Figure 3. [sent-117, score-0.028]
</p><p>71 To facilitate a better comparison, a sub-region of each is expanded to make the letter details clearer. [sent-118, score-0.035]
</p><p>72 The Huber prior tends to make the edges unnaturally sharp, though it is very successful at regularizing the solution elsewhere. [sent-119, score-0.093]
</p><p>73 Between the Tipping and Bishop image and the registration-integrating approach, the text appears more clear in our method, and the regularization in the constant background regions is slightly more successful. [sent-120, score-0.298]
</p><p>74 5 Discussion It is possible to interpret the extra terms introduced into the objective function in the derivation of this method as an extra regularizer term or image prior. [sent-121, score-0.356]
</p><p>75 Considering (19), the ﬁrst two terms are identical to the standard MAP super-resolution problem using a Huber image prior. [sent-122, score-0.321]
</p><p>76 The intuition behind the method’s success is that this extra prior resulting from the ﬁnal two terms of (19) will favor image solutions which are not acutely sensitive to minor adjustments in the image registration. [sent-124, score-0.686]
</p><p>77 The images of ﬁgure 4 illustrate the type of solution which would score poorly. [sent-125, score-0.107]
</p><p>78 To create the ﬁgure, one dataset was used to produce two super-resolved images, using two independent sets of registration parameters which were randomly perturbed by an i. [sent-126, score-0.716]
</p><p>79 Gaussian vector with a standard deviation of only 0. [sent-129, score-0.074]
</p><p>80 The checker-board pattern typical of ML super-resolution images can be observed, and the difference image on the right shows the drastic contrast between the two image estimates. [sent-131, score-0.703]
</p><p>81 (d) Detailed region of the central letters, again with our algorithm. [sent-134, score-0.037]
</p><p>82 (e) Detailed region of the regular Huber MAP super-resolution image, using parameter values suggested in [3], which are also found to be subjectively good choices. [sent-135, score-0.11]
</p><p>83 (f) Close-up of letter detail for comparison with Tipping and Bishop’s method of marginalization. [sent-137, score-0.064]
</p><p>84 The Gaussian form of their prior leads to a more blurred output, or one that over-ﬁts to the image noise on the input data if the prior’s inﬂuence is decreased. [sent-138, score-0.359]
</p><p>85 1 Conclusion This work has developed an alternative approach for Bayesian image super-resolution with several advantages over Tipping and Bishop’s original algorithm. [sent-140, score-0.298]
</p><p>86 These are namely a formal treatment of registration uncertainty, the use of a much more realistic image prior, and the computational speed and memory efﬁciency relating to the smaller dimension of the space over which we integrate. [sent-141, score-0.945]
</p><p>87 The results on real and synthetic images with this method show an advantage over the popular MAP approach, and over the result from Tipping and Bishop’s method, largely owing to our more favorable prior over the super-resolution image. [sent-142, score-0.284]
</p><p>88 Finally, the best way of learning the appropriate covariance values for the distribution over θ given the observed data, and how to assess the trade-off between its “prior-like” effects and the need for a standard Huber-style image prior, are still open questions. [sent-144, score-0.321]
</p><p>89 Acknowledgements The real dataset used in the results section is due to Tomas Pajdla and Daniel Martinec, CMP, Prague, and is available at http://www. [sent-145, score-0.063]
</p><p>90 (a) truth  (b) ML image 1  (c) ML image 2  (d) difference  Figure 4: An example of the effect of tiny changes in the registration parameters. [sent-151, score-1.301]
</p><p>91 (a) Ground truth image from which a 16-image low-resolution dataset was generated. [sent-152, score-0.419]
</p><p>92 In both cases, the same dataset was used, but the registration parameters were perturbed by an i. [sent-154, score-0.716]
</p><p>93 In all these images, values outside the valid image intensity range have been rounded to white or black values. [sent-160, score-0.298]
</p><p>94 Joint map registration and high-resolution image estimation using a sequence of undersampled images. [sent-189, score-1.001]
</p><p>95 Efﬁcient generalized cross-validation with applications to parametric image restoration and resolution enhancement. [sent-215, score-0.348]
</p><p>96 A bayesian approach to image expansion for improved deﬁnition. [sent-244, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('registration', 0.619), ('image', 0.298), ('huber', 0.275), ('tipping', 0.247), ('bishop', 0.22), ('photometric', 0.139), ('dx', 0.109), ('images', 0.107), ('pickup', 0.107), ('registrations', 0.107), ('illumination', 0.106), ('rmse', 0.099), ('integrating', 0.098), ('ml', 0.093), ('psf', 0.093), ('superresolution', 0.093), ('marginalizing', 0.088), ('truth', 0.086), ('map', 0.084), ('zx', 0.08), ('pixel', 0.071), ('geometric', 0.062), ('prior', 0.061), ('motion', 0.061), ('ground', 0.06), ('gt', 0.057), ('roberts', 0.056), ('vectorized', 0.056), ('dame', 0.053), ('hardie', 0.053), ('notre', 0.053), ('scg', 0.053), ('synthetic', 0.053), ('deviation', 0.051), ('resolution', 0.05), ('integral', 0.049), ('lighting', 0.049), ('strength', 0.047), ('scene', 0.047), ('imaging', 0.047), ('irani', 0.046), ('exp', 0.044), ('bayesian', 0.043), ('nk', 0.043), ('subjectively', 0.042), ('isbn', 0.042), ('gray', 0.042), ('detailed', 0.041), ('row', 0.039), ('region', 0.037), ('estimate', 0.037), ('dataset', 0.035), ('letter', 0.035), ('favorable', 0.035), ('pixels', 0.035), ('af', 0.035), ('letters', 0.035), ('gradients', 0.035), ('limits', 0.035), ('marginalization', 0.034), ('perturbed', 0.034), ('translation', 0.033), ('exponent', 0.032), ('err', 0.032), ('though', 0.032), ('regular', 0.031), ('levels', 0.03), ('costly', 0.03), ('september', 0.03), ('precision', 0.03), ('uncertainty', 0.029), ('transactions', 0.029), ('giving', 0.029), ('scaled', 0.029), ('extra', 0.029), ('detail', 0.029), ('camera', 0.029), ('real', 0.028), ('parameters', 0.028), ('respect', 0.028), ('relating', 0.028), ('diagonal', 0.027), ('expression', 0.026), ('numerically', 0.026), ('visible', 0.026), ('priors', 0.024), ('standard', 0.023), ('frame', 0.023), ('favored', 0.023), ('hartley', 0.023), ('icpr', 0.023), ('zoom', 0.023), ('aesthetic', 0.023), ('baker', 0.023), ('crisp', 0.023), ('displacements', 0.023), ('elad', 0.023), ('holiday', 0.023), ('nguyen', 0.023), ('register', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="42-tfidf-1" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>2 0.29711699 <a title="42-tfidf-2" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>Author: Andriy Myronenko, Xubo Song, Miguel Á. Carreira-Perpiñán</p><p>Abstract: We introduce Coherent Point Drift (CPD), a novel probabilistic method for nonrigid registration of point sets. The registration is treated as a Maximum Likelihood (ML) estimation problem with motion coherence constraint over the velocity ﬁeld such that one point set moves coherently to align with the second set. We formulate the motion coherence constraint and derive a solution of regularized ML estimation through the variational approach, which leads to an elegant kernel form. We also derive the EM algorithm for the penalized ML optimization with deterministic annealing. The CPD method simultaneously ﬁnds both the non-rigid transformation and the correspondence between two point sets without making any prior assumption of the transformation model except that of motion coherence. This method can estimate complex non-linear non-rigid transformations, and is shown to be accurate on 2D and 3D examples and robust in the presence of outliers and missing points.</p><p>3 0.13671906 <a title="42-tfidf-3" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>Author: Ali Rahimi, Ben Recht</p><p>Abstract: We derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples. Limiting our search to invertible observation functions confers numerous beneﬁts, including a compact representation and no suboptimal local minima. Our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution. Our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates. The beneﬁts of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identiﬁcation are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an RFID tracking experiment. 1</p><p>4 0.11444949 <a title="42-tfidf-4" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>5 0.11016071 <a title="42-tfidf-5" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>6 0.095324211 <a title="42-tfidf-6" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>7 0.093917407 <a title="42-tfidf-7" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>8 0.078748748 <a title="42-tfidf-8" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>9 0.069772646 <a title="42-tfidf-9" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>10 0.068700269 <a title="42-tfidf-10" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>11 0.064698704 <a title="42-tfidf-11" href="./nips-2006-Parameter_Expanded_Variational_Bayesian_Methods.html">159 nips-2006-Parameter Expanded Variational Bayesian Methods</a></p>
<p>12 0.064284727 <a title="42-tfidf-12" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>13 0.061743896 <a title="42-tfidf-13" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>14 0.061500747 <a title="42-tfidf-14" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>15 0.060978148 <a title="42-tfidf-15" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>16 0.058310121 <a title="42-tfidf-16" href="./nips-2006-Using_Combinatorial_Optimization_within_Max-Product_Belief_Propagation.html">201 nips-2006-Using Combinatorial Optimization within Max-Product Belief Propagation</a></p>
<p>17 0.057677381 <a title="42-tfidf-17" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>18 0.057508577 <a title="42-tfidf-18" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>19 0.05677481 <a title="42-tfidf-19" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>20 0.056748625 <a title="42-tfidf-20" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.189), (1, 0.009), (2, 0.144), (3, -0.057), (4, 0.044), (5, -0.082), (6, -0.055), (7, -0.131), (8, -0.041), (9, 0.105), (10, 0.165), (11, -0.014), (12, -0.021), (13, 0.044), (14, 0.104), (15, 0.11), (16, -0.051), (17, -0.196), (18, -0.075), (19, 0.167), (20, -0.078), (21, 0.057), (22, -0.018), (23, -0.046), (24, -0.174), (25, 0.188), (26, 0.007), (27, 0.163), (28, 0.136), (29, 0.104), (30, 0.013), (31, -0.034), (32, -0.075), (33, 0.013), (34, 0.067), (35, 0.017), (36, -0.058), (37, 0.09), (38, 0.061), (39, 0.113), (40, -0.18), (41, -0.099), (42, -0.072), (43, 0.194), (44, 0.023), (45, -0.016), (46, -0.114), (47, 0.01), (48, -0.126), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93234408 <a title="42-lsi-1" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>2 0.84354633 <a title="42-lsi-2" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>Author: Andriy Myronenko, Xubo Song, Miguel Á. Carreira-Perpiñán</p><p>Abstract: We introduce Coherent Point Drift (CPD), a novel probabilistic method for nonrigid registration of point sets. The registration is treated as a Maximum Likelihood (ML) estimation problem with motion coherence constraint over the velocity ﬁeld such that one point set moves coherently to align with the second set. We formulate the motion coherence constraint and derive a solution of regularized ML estimation through the variational approach, which leads to an elegant kernel form. We also derive the EM algorithm for the penalized ML optimization with deterministic annealing. The CPD method simultaneously ﬁnds both the non-rigid transformation and the correspondence between two point sets without making any prior assumption of the transformation model except that of motion coherence. This method can estimate complex non-linear non-rigid transformations, and is shown to be accurate on 2D and 3D examples and robust in the presence of outliers and missing points.</p><p>3 0.56877869 <a title="42-lsi-3" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>4 0.49670303 <a title="42-lsi-4" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>Author: Anitha Kannan, John Winn, Carsten Rother</p><p>Abstract: Patch-based appearance models are used in a wide range of computer vision applications. To learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand. In the jigsaw model presented here, the shape, size and appearance of patches are learned automatically from the repeated structures in a set of training images. By learning such irregularly shaped ‘jigsaw pieces’, we are able to discover both the shape and the appearance of object parts without supervision. When applied to face images, for example, the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes, noses, eyebrows and cheeks, to name a few. We conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection. This enables parts of similar appearance but different shapes to be distinguished; for example, while foreheads and cheeks are both skin colored, they have markedly different shapes. 1</p><p>5 0.42768419 <a title="42-lsi-5" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>Author: Ali Rahimi, Ben Recht</p><p>Abstract: We derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples. Limiting our search to invertible observation functions confers numerous beneﬁts, including a compact representation and no suboptimal local minima. Our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution. Our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates. The beneﬁts of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identiﬁcation are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an RFID tracking experiment. 1</p><p>6 0.42136487 <a title="42-lsi-6" href="./nips-2006-Statistical_Modeling_of_Images_with_Fields_of_Gaussian_Scale_Mixtures.html">182 nips-2006-Statistical Modeling of Images with Fields of Gaussian Scale Mixtures</a></p>
<p>7 0.42031008 <a title="42-lsi-7" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>8 0.41914657 <a title="42-lsi-8" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>9 0.3877314 <a title="42-lsi-9" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>10 0.32767403 <a title="42-lsi-10" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>11 0.31928718 <a title="42-lsi-11" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>12 0.31419486 <a title="42-lsi-12" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>13 0.30681285 <a title="42-lsi-13" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>14 0.28461593 <a title="42-lsi-14" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>15 0.2804966 <a title="42-lsi-15" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>16 0.27949607 <a title="42-lsi-16" href="./nips-2006-Learning_to_Traverse_Image_Manifolds.html">120 nips-2006-Learning to Traverse Image Manifolds</a></p>
<p>17 0.27825555 <a title="42-lsi-17" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>18 0.27531976 <a title="42-lsi-18" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>19 0.26664859 <a title="42-lsi-19" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>20 0.26582932 <a title="42-lsi-20" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.09), (3, 0.051), (7, 0.083), (9, 0.041), (20, 0.04), (22, 0.042), (44, 0.065), (57, 0.11), (61, 0.278), (65, 0.052), (69, 0.034), (71, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78461623 <a title="42-lda-1" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>2 0.77703619 <a title="42-lda-2" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<p>Author: Ryota Kobayashi, Shigeru Shinomoto</p><p>Abstract: It has been established that a neuron reproduces highly precise spike response to identical ﬂuctuating input currents. We wish to accurately predict the ﬁring times of a given neuron for any input current. For this purpose we adopt a model that mimics the dynamics of the membrane potential, and then take a cue from its dynamics for predicting the spike occurrence for a novel input current. It is found that the prediction is signiﬁcantly improved by observing the state space of the membrane potential and its time derivative(s) in advance of a possible spike, in comparison to simply thresholding an instantaneous value of the estimated potential. 1</p><p>3 0.77553374 <a title="42-lda-3" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>4 0.57572484 <a title="42-lda-4" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>Author: Wolf Kienzle, Felix A. Wichmann, Matthias O. Franz, Bernhard Schölkopf</p><p>Abstract: This paper addresses the bottom-up inﬂuence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear ﬁlters, e.g., Gabor or Difference-of-Gaussians ﬁlters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end ﬁlters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justiﬁed. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the ﬁeld that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble ﬁndings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive ﬁelds in the early human visual system. 1</p><p>5 0.57491124 <a title="42-lda-5" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>6 0.564771 <a title="42-lda-6" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>7 0.56447601 <a title="42-lda-7" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>8 0.56395864 <a title="42-lda-8" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>9 0.56091464 <a title="42-lda-9" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>10 0.5606364 <a title="42-lda-10" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>11 0.55984455 <a title="42-lda-11" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>12 0.55795127 <a title="42-lda-12" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>13 0.55744779 <a title="42-lda-13" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>14 0.55740517 <a title="42-lda-14" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>15 0.55673665 <a title="42-lda-15" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>16 0.55554551 <a title="42-lda-16" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>17 0.554892 <a title="42-lda-17" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>18 0.55463338 <a title="42-lda-18" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>19 0.55343038 <a title="42-lda-19" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>20 0.5519852 <a title="42-lda-20" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
