<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2006-Theory and Dynamics of Perceptual Bistability</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-192" href="#">nips2006-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 nips-2006-Theory and Dynamics of Perceptual Bistability</h1>
<br/><p>Source: <a title="nips-2006-192-pdf" href="http://papers.nips.cc/paper/3055-theory-and-dynamics-of-perceptual-bistability.pdf">pdf</a></p><p>Author: Paul R. Schrater, Rashmi Sundareswara</p><p>Abstract: Perceptual Bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing. Although switching behavior is increasingly well characterized, the origins remain elusive. We propose that perceptual switching naturally arises from the brain’s search for best interpretations while performing Bayesian inference. In particular, we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation. We formalize the theory, explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates. Finally, predictions of the theory are shown to be consistent with measured changes in human switching dynamics to Necker cube stimuli induced by context.</p><p>Reference: <a title="nips-2006-192-reference" href="../nips2006_reference/nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Perceptual Bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing. [sent-9, score-0.289]
</p><p>2 Although switching behavior is increasingly well characterized, the origins remain elusive. [sent-10, score-0.222]
</p><p>3 We propose that perceptual switching naturally arises from the brain’s search for best interpretations while performing Bayesian inference. [sent-11, score-0.517]
</p><p>4 In particular, we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation. [sent-12, score-0.611]
</p><p>5 We formalize the theory, explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates. [sent-13, score-0.738]
</p><p>6 Finally, predictions of the theory are shown to be consistent with measured changes in human switching dynamics to Necker cube stimuli induced by context. [sent-14, score-0.688]
</p><p>7 1 Introduction Our visual system is remarkably good at producing consistent, crisp percepts of the world around us, in the process hiding interpretation uncertainty. [sent-15, score-0.331]
</p><p>8 Perceptual bistability is one of the few circumstances where ambiguity in the visual processing is exposed to conscious awareness. [sent-16, score-0.409]
</p><p>9 Spontaneous switching of perceptual states frequently occurs during continuously viewing an ambiguous image, and when a new interpretation of a previously stable stimuli is revealed (as in the sax/girl in ﬁgure ? [sent-17, score-0.722]
</p><p>10 Moreover, although perceptual switching can be modulated by conscious effort[? [sent-21, score-0.481]
</p><p>11 (b) can be interpreted as a cube viewed from two different viewpoints. [sent-25, score-0.217]
</p><p>12 Stimuli that produce bistability are characterized by having several distinct interpretations that are in some sense equally plausible. [sent-26, score-0.44]
</p><p>13 Given the successes of Bayesian inference as a model of perception ∗  http://www. [sent-27, score-0.179]
</p><p>14 ]), these observations suggest that bistability is intimately connected with making perceptual decisions in the presence of a multi-modal posterior distribution, as previously noted by several authors[? [sent-32, score-0.673]
</p><p>15 In fact, most explanations of bistability have been historically rooted in proposals about the nature of neural processing of visual stimuli, involving low-level visual processes like retinal adaptation and neural fatigue[? [sent-36, score-0.413]
</p><p>16 However, the abundance of behavioral and brain imaging data that show high level inﬂuences on switching (like intentional control which can produce 3-fold changes in alternation rate[? [sent-40, score-0.441]
</p><p>17 The goal of this paper is to provide a simple explanation for the origins of bistability based on general principles that can potentially handle both top-down and bottom-up effects. [sent-43, score-0.294]
</p><p>18 2 Basic theory The basic ideas that constitute our theory are simple and partly form standard assumptions about perceptual processing. [sent-44, score-0.369]
</p><p>19 Perception performs Bayesian inference by exploring and updating the posterior distribution across time by a kind of sampling process (e. [sent-46, score-0.337]
</p><p>20 Conscious percepts result from a decision process that picks the interpretations by ﬁnding sample interpretations with the highest posterior probability (possibly weighted by the cost of making errors). [sent-51, score-0.635]
</p><p>21 The results of these decisions and their associated posterior probabilities are stored in memory until a better interpretation is sampled. [sent-53, score-0.433]
</p><p>22 The posterior probability associated with the interpretation in memory decays with time. [sent-55, score-0.473]
</p><p>23 The intuition behind the model is that most percepts of objects in a scene are built up across a series of ﬁxations. [sent-56, score-0.106]
</p><p>24 When an object previously ﬁxated is eccentrically viewed or occluded, the brain should store the previous interpretation in memory until better data comes along or the memory becomes too old to be trusted. [sent-57, score-0.524]
</p><p>25 Finally, the interpretation space required for direct Bayesian inference is too large for even simple images, but sampling schemes may provide a simple way to perform approximate inference. [sent-58, score-0.211]
</p><p>26 The theory provides a natural interface to interpret both high-level and low-level effects on bistability, because any event that has an impact on the relative heights or positions of the modes in the posterior can potentially inﬂuence durations. [sent-59, score-0.398]
</p><p>27 For example, patterns of eye ﬁxations have long been known to inﬂuence the dominant percept[? [sent-60, score-0.094]
</p><p>28 Because eye movement events create sudden changes in image information, it is natural that they should be associated with changes in the dominant mode. [sent-62, score-0.343]
</p><p>29 Similarly, control of information via selective attention and changes in decision thresholds offer concrete loci for intentional effects on bistability. [sent-63, score-0.199]
</p><p>30 3 Analysis To analyze the proposed theory, we need to develop temporal distributions for the maxima of a multimodal posterior based on a sampling process and describe circumstances under which a current sample will produce an interpretation better than the one in memory. [sent-64, score-0.655]
</p><p>31 First we develop a general approximation to multi-modal posterior distributions that can vary over time, and analyze the probability that a sample from the posterior are close to maximal. [sent-66, score-0.386]
</p><p>32 We then describe how the samples close to the max interact with a sample in memory with decay. [sent-67, score-0.178]
</p><p>33 shape for Necker Cube) at time t, θt,i is the location of the maxima of the ith mode, Dt is the most recent data, D0:t−∆t is the data history, ∗ and Pi (θ|D0:t−∆t ; θt,i ) is the predictive distribution (prior) for the current data based on recent experience1. [sent-71, score-0.11]
</p><p>34 Thus, samples from a posterior mode will be approximately χ2 distributed near the maxi−1 mum with effective degrees of freedom n given by the number of signiﬁcant eigenvalues of Ii . [sent-73, score-0.471]
</p><p>35 2 Essentially n encodes the effective degrees of freedom in interpretation space. [sent-74, score-0.268]
</p><p>36 Given these assumptions, we can approximate the probability distribution for update events. [sent-77, score-0.113]
</p><p>37 Because variances add, the average effect on the distance ωt is a linear increase: ωt = ω0 + ρσt, where ρ is the sampling rate. [sent-85, score-0.146]
</p><p>38 These disturbances could represent changes in the local of the maxima of the posterior due to the incorporation of new data, neural noise, or even discounting (note that linearly increasing ωt corresponds to exponential or multiplicative discounting in probability). [sent-86, score-0.322]
</p><p>39 1  Because time is critical to our arguments, we assume that the posterior is updated across time (and hence new data) using a process that resembles Bayesian updating. [sent-87, score-0.23]
</p><p>40 2 For the Necker cube, the interpretation space can be thought of as the depths of the vertices. [sent-88, score-0.104]
</p><p>41 A strong prior assumption that world angles between vertices are close to 90deg produces two dominate modes in the posterior that correspond to the typical interpretations. [sent-89, score-0.203]
</p><p>42 To understand the behavior of this memory process, notice that every mθ (0) must be within distance ξ of the maximum of the posterior for an update to occur. [sent-92, score-0.444]
</p><p>43 Due to the properties of extrema of distributions of χ2 random variables, an mθ (0) will be (in expectation) a characteristic distance µm (ξ) below ξ and for t > 0 drifts with linear dynamics3. [sent-93, score-0.098]
</p><p>44 This suggests the approximation, p(ωt < ξ) ≈ δ(µm + ρσt − ωt ), which can be formally justiﬁed because p(ωt < ξ) will be i highly peaked with respect to the distribution of the sampling process p(δτ ). [sent-94, score-0.276]
</p><p>45 Finally assuming slow 4 drift means (1 − P (ωt < ξ)) ≈ 0 on the time scale that transitions occur . [sent-95, score-0.093]
</p><p>46 reduces to: t  P (Mti < min{ξ, ωt }) =  0  i p(δτ < ωt )δ(µm + ρσt − ωt )P (i|τ )dτ  ≈ P (Mti < µm + ρσt)P (i) where P (i) is the average frequency of sampling from the i  th  (5) (6)  mode. [sent-98, score-0.107]
</p><p>47 Extrema of the posterior sampling process If the sampling process has no long-range temporal dependence, then under mild assumptions the distribution of extrema converge in distribution5 to one of three characteristic forms that depend only on the domain of the random variable[? [sent-99, score-0.672]
</p><p>48 Set N = ρt and let ρ = 1 for 2 convenience, where ρ is the effective sampling rate, and equation ? [sent-102, score-0.142]
</p><p>49 In particular, for n > 4 and µm (ξ) relatively small, the distribution has a gamma-like behavior, where new memory update transitions are suppressed near recent transitions. [sent-106, score-0.344]
</p><p>50 This behavior shows the effect of the decision threshold, as without a decision threshold the asymptotic behavior of simple sampling schemes will generate approximately exponentially distributed update event times, as a consequence of extreme value theory. [sent-108, score-0.477]
</p><p>51 Note that the time scale of events can be arbitrarily controlled by appropriately selecting ρ (controls the time scale of the sampling process) and σ (controls the time scale of the memory decay process). [sent-113, score-0.428]
</p><p>52 Effects of posterior parameters on update events The memory update distributions are effected primarily by two factors, the log posterior heights and their difference ∆kij = ki − kj , and the effective number of degrees of freedom per mode n. [sent-114, score-1.256]
</p><p>53 Effect of ki , ∆kij The variable ∆kij has possible effects both on the probability that a mode is sampled, and the temporal distributions. [sent-115, score-0.354]
</p><p>54 When the modes are strongly peaked (and the sampling procedure is unbiased) log P (i) ≈ ∆kij . [sent-116, score-0.249]
</p><p>55 Finally, if the posterior becomes more peaked while the threshold remains ﬁxed, the update rates should increase and the temporal distributions will move toward exponential. [sent-118, score-0.56]
</p><p>56 If we assume increased viewing time makes the posterior more peaked, then our model predicts the common ﬁnding of increased transition rates with viewing duration. [sent-119, score-0.46]
</p><p>57 3  In the simulations, µm is chosen as the expected value of the set of events below the threshold xi Conversely fast drift in the limit means P (ωt < ξ) ≈ 0, which results in transitions entirely determined by the minima of the sampling process and ξ. [sent-120, score-0.476]
</p><p>58 Time to next update > t (P(T>t))  1 Effective degrees of freedom n=8 n=4 n=2  0. [sent-122, score-0.202]
</p><p>59 1 0 0  500  1000  1500  2000  2500  3000  3500  4000  Time(t) (in number of samples)  Figure 2: Examples of cumulative distribution functions of memory update times. [sent-131, score-0.251]
</p><p>60 Solid curves are generated by simulating the sampling with decision process described in the text. [sent-132, score-0.274]
</p><p>61 Effect of n One of the surprising aspects of the theory above is the strong dependence on the effective number of degrees of freedom. [sent-136, score-0.147]
</p><p>62 The theory makes a strong prediction that stimuli that have more interpretation degrees of freedom will have longer durations between transitions, which appears to be qualitatively true across both rivalry and bistability experiments[? [sent-137, score-0.786]
</p><p>63 Relating theory to behavioral data via induced Semi-Markov Renewal Process Assuming that memory update events involving transitions to the same mode are not perceptually accessible, only update events that switch modes are potentially measurable. [sent-140, score-1.034]
</p><p>64 However, the process described above ﬁts the description of a generator for a semi-Markov renewal process. [sent-141, score-0.283]
</p><p>65 A semi-Markov renewal process involves Markov transitions between discrete states i and j determined by a matrix with entries Pij , coupled with random durations spent in that state sampled from time distributions Fij (t). [sent-142, score-0.58]
</p><p>66 The product of these distributions Qij (t) = Pij Fij (t) is the generator of the process, that describes the conditional probability of ﬁrst transition between states i → j in time less than t, given ﬁrst entry into state i occurs at time t = 0. [sent-143, score-0.31]
</p><p>67 In the theory above, Fij (t) = Fii (t) = P (Ti < t), while Pij = Pjj = P (j)6 The main reason for introducing the notion of a renewal process is that they can be used to express the relationship between the theoretical distributions and observable quantities. [sent-144, score-0.345]
</p><p>68 The most commonly collected data are times between transitions and (possibly contingent) percept frequencies. [sent-145, score-0.213]
</p><p>69 Let the state s(t) = i refer to when the memory process is in the support of mode i: mθ (t) ∈ Si at time t. [sent-148, score-0.455]
</p><p>70 Let P (0), denote the probability of sampling from mode 0. [sent-151, score-0.303]
</p><p>71 Moreover, 6 The independence relations are a consequence of an assumption of independence in the sampling procedure, and relaxing that assumption can produce state contingencies in Qij (t). [sent-153, score-0.235]
</p><p>72 MCMC-like sampling with large steps) can create contingencies in the frequency of sampling from the ith mode that will produce a non-independent transition matrix Pij = P (θt ∈ Si |θt−ρ∆t ∈ Sj ). [sent-157, score-0.606]
</p><p>73 for gamma-like distributions, the convolution integral tends to increase the shape parameter, which means that gamma parameter estimates produced by ﬁtting transition durations will overestimate the amount of ’memory’ in the process7 . [sent-158, score-0.369]
</p><p>74 Finally note the limiting behavior as P (0) → 0, G01 (t) = P (T0 < t), so that direct measurement of the temporal distributions is possible but only for the (almost) supressed perceptual state. [sent-159, score-0.372]
</p><p>75 To bias perception of a bistable stimuli, we had observers view a Necker cube ﬂanked with ’ﬁelds of cubes’ that are perceptually unambiguous and match one of the two percepts (see ﬁgure ? [sent-161, score-0.645]
</p><p>76 Subjects are typically biased toward seeing the Necker cube in the “looking down” state (65-70% response rates), and the context stimuli shown in ﬁgure ? [sent-164, score-0.45]
</p><p>77 We found that the looking up context, boosts “looking up” response rates from 30% to 55%. [sent-167, score-0.219]
</p><p>78 1 Methods Subject’s perceptual state while viewing the stimuli in ﬁg. [sent-169, score-0.42]
</p><p>79 Each observer viewed 100 randomly generated context stimuli and each stimulus was viewed long enough to acquire 10 responses (taking 10-12 sec on average). [sent-177, score-0.174]
</p><p>80 (a) An instance of the “Looking down” con- (b) An instance of the “Looking up” context text with the Necker cube in the middle with the Necker cube in the middle  Figure 3: The two ﬁgures are examples of the “Looking down” and “Looking up” context conditions. [sent-179, score-0.534]
</p><p>81 2 Results We measured the effect of context on estimates of perceptual switching rates, Ri = P (s(t) = i), ﬁrst transition durations Gij , and survival probabilities Pii = P (s(t) = i|s(0) = i) by counting the number of events of each type. [sent-181, score-1.026]
</p><p>82 Additionally, we ﬁt a semi-Markov renewal process Qij (t) = Pij Fij (t) to the data using a sampling based procedure. [sent-182, score-0.356]
</p><p>83 For ease of sampling, Fij (t) were gamma with separate parameters for each of the four conditionals {00, 01, 10, 11}, resulting in 10 parameters 7  gamma shape parameters are frequently interpreted as the number of events in some abstract Poisson process that must occur before transition  overall. [sent-184, score-0.541]
</p><p>84 The process was ﬁt by iteratively choosing parameter values for Qij (t), simulating response data and measuring the mismatch between the simulated and human Gij and Pii distributions. [sent-185, score-0.17]
</p><p>85 The effect of context on Gij and Pii is shown in Fig. [sent-186, score-0.089]
</p><p>86 of ﬁrst transition and the survival probability of the “Looking down” percept. [sent-218, score-0.309]
</p><p>87 of ﬁrst transition and conditional survival probability of the “Looking Up” percept. [sent-220, score-0.309]
</p><p>88 A semi-Markov renewal process with transition paramters Pij , gamma means mij and gamma variances vij was ﬁt to all the data via max. [sent-221, score-0.533]
</p><p>89 ] also presents a theory for average transitions times in bistability based on random processes and a multi-modal posterior distribution, their theory is fundamentally different as it derives  switching events from tunneling probabilities that arise from input noise. [sent-248, score-1.0]
</p><p>90 Moreover, their theory predicts increasing transition times as the posterior becomes increasingly peaked, exactly opposite our predictions. [sent-249, score-0.388]
</p><p>91 In conclusion, we have presented a novel theory of perceptual bistability based on simple assumptions about how the brain makes perceptual decisions. [sent-250, score-0.903]
</p><p>92 In addition, results from a simple experiment show that manipulations which change the dominance of a percept produce coupled changes in the probability of transition events as predicted by theory. [sent-251, score-0.543]
</p><p>93 We believe the strength of the theory is that it can make a large set of qualitative predictions about the distribution of transition events by coupling transition times to simple properties of the posterior distribution. [sent-253, score-0.72]
</p><p>94 Our theory suggests that the basic descriptive model sufﬁcient to capture perceptual bistability is a semi-Markov renewal process, which we showed could successfully simulate the temporal dynamics of human data for the Necker cube. [sent-254, score-0.898]
</p><p>95 Distributions of alternation rates in various forms of bistable perception. [sent-272, score-0.155]
</p><p>96 Are switches in perception of the Necker cube related to eye position? [sent-280, score-0.45]
</p><p>97 T (1994) The generic viewpoint assumption in a framework for visual perception Nature vol. [sent-283, score-0.221]
</p><p>98 (1992) Prime Time: Fatigue and set effects in the perception of reversible ﬁgures. [sent-313, score-0.256]
</p><p>99 (2006) Noise characteristics and prior expectations in human visual speed perception Nature Neuroscience vol. [sent-333, score-0.265]
</p><p>100 (2005) Dynamics of perceptual bi-stability for stereoscopic slant rivalry. [sent-365, score-0.277]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bistability', 0.294), ('necker', 0.294), ('perceptual', 0.228), ('cube', 0.217), ('mti', 0.196), ('looking', 0.18), ('switching', 0.18), ('perception', 0.179), ('memory', 0.178), ('renewal', 0.17), ('mode', 0.156), ('posterior', 0.151), ('transition', 0.15), ('events', 0.143), ('survival', 0.119), ('durations', 0.117), ('interpretations', 0.109), ('sampling', 0.107), ('percepts', 0.106), ('interpretation', 0.104), ('qij', 0.098), ('fij', 0.097), ('transitions', 0.093), ('peaked', 0.09), ('stimuli', 0.09), ('pij', 0.086), ('percept', 0.085), ('process', 0.079), ('maxima', 0.075), ('kij', 0.075), ('bistable', 0.073), ('conscious', 0.073), ('mamassian', 0.073), ('toppino', 0.073), ('update', 0.073), ('freedom', 0.069), ('gij', 0.068), ('fit', 0.068), ('gamma', 0.067), ('dt', 0.066), ('psychophysics', 0.065), ('brain', 0.064), ('intentional', 0.064), ('heights', 0.064), ('viewing', 0.06), ('ambiguous', 0.06), ('degrees', 0.06), ('ki', 0.059), ('pii', 0.058), ('temporal', 0.058), ('gures', 0.056), ('extrema', 0.054), ('threshold', 0.054), ('eye', 0.054), ('changes', 0.053), ('dynamics', 0.052), ('modes', 0.052), ('theory', 0.052), ('ee', 0.051), ('toward', 0.051), ('context', 0.05), ('contingencies', 0.049), ('schrater', 0.049), ('slant', 0.049), ('van', 0.048), ('simulating', 0.047), ('ti', 0.046), ('distributions', 0.044), ('human', 0.044), ('alternation', 0.043), ('disturbances', 0.043), ('minnesota', 0.043), ('behavior', 0.042), ('state', 0.042), ('visual', 0.042), ('effects', 0.041), ('decision', 0.041), ('dominant', 0.04), ('probability', 0.04), ('bayesian', 0.039), ('qualitative', 0.039), ('rates', 0.039), ('effect', 0.039), ('event', 0.038), ('assumptions', 0.037), ('spontaneous', 0.037), ('produce', 0.037), ('perceptually', 0.036), ('reversible', 0.036), ('fatigue', 0.036), ('gure', 0.036), ('shape', 0.035), ('coupled', 0.035), ('effective', 0.035), ('involving', 0.035), ('times', 0.035), ('observers', 0.034), ('observer', 0.034), ('generator', 0.034), ('ross', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="192-tfidf-1" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>Author: Paul R. Schrater, Rashmi Sundareswara</p><p>Abstract: Perceptual Bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing. Although switching behavior is increasingly well characterized, the origins remain elusive. We propose that perceptual switching naturally arises from the brain’s search for best interpretations while performing Bayesian inference. In particular, we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation. We formalize the theory, explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates. Finally, predictions of the theory are shown to be consistent with measured changes in human switching dynamics to Necker cube stimuli induced by context.</p><p>2 0.11139192 <a title="192-tfidf-2" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We introduce a class of MPDs which greatly simplify Reinforcement Learning. They have discrete state spaces and continuous control spaces. The controls have the effect of rescaling the transition probabilities of an underlying Markov chain. A control cost penalizing KL divergence between controlled and uncontrolled transition probabilities makes the minimization problem convex, and allows analytical computation of the optimal controls given the optimal value function. An exponential transformation of the optimal value function makes the minimized Bellman equation linear. Apart from their theoretical signi cance, the new MDPs enable ef cient approximations to traditional MDPs. Shortest path problems are approximated to arbitrary precision with largest eigenvalue problems, yielding an O (n) algorithm. Accurate approximations to generic MDPs are obtained via continuous embedding reminiscent of LP relaxation in integer programming. Offpolicy learning of the optimal value function is possible without need for stateaction values; the new algorithm (Z-learning) outperforms Q-learning. This work was supported by NSF grant ECS–0524761. 1</p><p>3 0.095776744 <a title="192-tfidf-3" href="./nips-2006-A_Bayesian_Approach_to_Diffusion_Models_of_Decision-Making_and_Response_Time.html">1 nips-2006-A Bayesian Approach to Diffusion Models of Decision-Making and Response Time</a></p>
<p>Author: Michael D. Lee, Ian G. Fuss, Daniel J. Navarro</p><p>Abstract: We present a computational Bayesian approach for Wiener diffusion models, which are prominent accounts of response time distributions in decision-making. We ﬁrst develop a general closed-form analytic approximation to the response time distributions for one-dimensional diffusion processes, and derive the required Wiener diffusion as a special case. We use this result to undertake Bayesian modeling of benchmark data, using posterior sampling to draw inferences about the interesting psychological parameters. With the aid of the benchmark data, we show the Bayesian account has several advantages, including dealing naturally with the parameter variation needed to account for some key features of the data, and providing quantitative measures to guide decisions about model construction. 1</p><p>4 0.08762458 <a title="192-tfidf-4" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Many neural areas, notably, the hippocampus, show structured, dynamical, population behavior such as coordinated oscillations. It has long been observed that such oscillations provide a substrate for representing analog information in the ﬁring phases of neurons relative to the underlying population rhythm. However, it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture, and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems. Here, we observe that, since neurons in an oscillatory network need not only ﬁre once in each cycle (or even at all), uncertainty about the analog quantities each neuron represents by its ﬁring phase might naturally be reported through the degree of concentration of the spikes that it ﬁres. We apply this theory to memory in a model of oscillatory associative recall in hippocampal area CA3. Although it is not well treated in the literature, representing and manipulating uncertainty is fundamental to competent memory; our theory enables us to view CA3 as an effective uncertainty-aware, retrieval system. 1</p><p>5 0.082584068 <a title="192-tfidf-5" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>Author: Wolf Kienzle, Felix A. Wichmann, Matthias O. Franz, Bernhard Schölkopf</p><p>Abstract: This paper addresses the bottom-up inﬂuence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear ﬁlters, e.g., Gabor or Difference-of-Gaussians ﬁlters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end ﬁlters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justiﬁed. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the ﬁeld that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble ﬁndings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive ﬁelds in the early human visual system. 1</p><p>6 0.078658663 <a title="192-tfidf-6" href="./nips-2006-A_Nonparametric_Bayesian_Method_for_Inferring_Features_From_Similarity_Judgments.html">9 nips-2006-A Nonparametric Bayesian Method for Inferring Features From Similarity Judgments</a></p>
<p>7 0.077804968 <a title="192-tfidf-7" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>8 0.073088437 <a title="192-tfidf-8" href="./nips-2006-Learning_Time-Intensity_Profiles_of_Human_Activity_using_Non-Parametric_Bayesian_Models.html">114 nips-2006-Learning Time-Intensity Profiles of Human Activity using Non-Parametric Bayesian Models</a></p>
<p>9 0.066524148 <a title="192-tfidf-9" href="./nips-2006-Inducing_Metric_Violations_in_Human_Similarity_Judgements.html">97 nips-2006-Inducing Metric Violations in Human Similarity Judgements</a></p>
<p>10 0.065870546 <a title="192-tfidf-10" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>11 0.064961791 <a title="192-tfidf-11" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>12 0.063039295 <a title="192-tfidf-12" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>13 0.056836009 <a title="192-tfidf-13" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>14 0.055677671 <a title="192-tfidf-14" href="./nips-2006-Modeling_Dyadic_Data_with_Binary_Latent_Factors.html">132 nips-2006-Modeling Dyadic Data with Binary Latent Factors</a></p>
<p>15 0.055143904 <a title="192-tfidf-15" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>16 0.053776756 <a title="192-tfidf-16" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>17 0.053650927 <a title="192-tfidf-17" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>18 0.053543113 <a title="192-tfidf-18" href="./nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation.html">141 nips-2006-Multiple timescales and uncertainty in motor adaptation</a></p>
<p>19 0.050662275 <a title="192-tfidf-19" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>20 0.050048109 <a title="192-tfidf-20" href="./nips-2006-Multi-dynamic_Bayesian_Networks.html">139 nips-2006-Multi-dynamic Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.185), (1, -0.065), (2, 0.026), (3, -0.093), (4, 0.016), (5, -0.05), (6, 0.078), (7, -0.045), (8, -0.002), (9, 0.014), (10, 0.069), (11, 0.069), (12, 0.026), (13, -0.029), (14, -0.106), (15, -0.061), (16, 0.059), (17, -0.002), (18, -0.115), (19, -0.093), (20, 0.044), (21, 0.052), (22, 0.018), (23, -0.046), (24, 0.045), (25, 0.064), (26, -0.044), (27, -0.038), (28, -0.038), (29, -0.013), (30, -0.091), (31, -0.065), (32, 0.09), (33, 0.019), (34, -0.094), (35, 0.097), (36, -0.057), (37, 0.027), (38, 0.007), (39, 0.079), (40, -0.081), (41, 0.192), (42, -0.005), (43, 0.094), (44, -0.008), (45, -0.095), (46, 0.151), (47, -0.068), (48, 0.067), (49, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95699406 <a title="192-lsi-1" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>Author: Paul R. Schrater, Rashmi Sundareswara</p><p>Abstract: Perceptual Bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing. Although switching behavior is increasingly well characterized, the origins remain elusive. We propose that perceptual switching naturally arises from the brain’s search for best interpretations while performing Bayesian inference. In particular, we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation. We formalize the theory, explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates. Finally, predictions of the theory are shown to be consistent with measured changes in human switching dynamics to Necker cube stimuli induced by context.</p><p>2 0.67523342 <a title="192-lsi-2" href="./nips-2006-A_Bayesian_Approach_to_Diffusion_Models_of_Decision-Making_and_Response_Time.html">1 nips-2006-A Bayesian Approach to Diffusion Models of Decision-Making and Response Time</a></p>
<p>Author: Michael D. Lee, Ian G. Fuss, Daniel J. Navarro</p><p>Abstract: We present a computational Bayesian approach for Wiener diffusion models, which are prominent accounts of response time distributions in decision-making. We ﬁrst develop a general closed-form analytic approximation to the response time distributions for one-dimensional diffusion processes, and derive the required Wiener diffusion as a special case. We use this result to undertake Bayesian modeling of benchmark data, using posterior sampling to draw inferences about the interesting psychological parameters. With the aid of the benchmark data, we show the Bayesian account has several advantages, including dealing naturally with the parameter variation needed to account for some key features of the data, and providing quantitative measures to guide decisions about model construction. 1</p><p>3 0.61921346 <a title="192-lsi-3" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We introduce a class of MPDs which greatly simplify Reinforcement Learning. They have discrete state spaces and continuous control spaces. The controls have the effect of rescaling the transition probabilities of an underlying Markov chain. A control cost penalizing KL divergence between controlled and uncontrolled transition probabilities makes the minimization problem convex, and allows analytical computation of the optimal controls given the optimal value function. An exponential transformation of the optimal value function makes the minimized Bellman equation linear. Apart from their theoretical signi cance, the new MDPs enable ef cient approximations to traditional MDPs. Shortest path problems are approximated to arbitrary precision with largest eigenvalue problems, yielding an O (n) algorithm. Accurate approximations to generic MDPs are obtained via continuous embedding reminiscent of LP relaxation in integer programming. Offpolicy learning of the optimal value function is possible without need for stateaction values; the new algorithm (Z-learning) outperforms Q-learning. This work was supported by NSF grant ECS–0524761. 1</p><p>4 0.53222978 <a title="192-lsi-4" href="./nips-2006-Hidden_Markov_Dirichlet_Process%3A_Modeling_Genetic_Recombination_in_Open_Ancestral_Space.html">90 nips-2006-Hidden Markov Dirichlet Process: Modeling Genetic Recombination in Open Ancestral Space</a></p>
<p>Author: Kyung-ah Sohn, Eric P. Xing</p><p>Abstract: We present a new statistical framework called hidden Markov Dirichlet process (HMDP) to jointly model the genetic recombinations among possibly inﬁnite number of founders and the coalescence-with-mutation events in the resulting genealogies. The HMDP posits that a haplotype of genetic markers is generated by a sequence of recombination events that select an ancestor for each locus from an unbounded set of founders according to a 1st-order Markov transition process. Conjoining this process with a mutation model, our method accommodates both between-lineage recombination and within-lineage sequence variations, and leads to a compact and natural interpretation of the population structure and inheritance process underlying haplotype data. We have developed an efﬁcient sampling algorithm for HMDP based on a two-level nested P´ lya urn scheme. On both simulated o and real SNP haplotype data, our method performs competitively or signiﬁcantly better than extant methods in uncovering the recombination hotspots along chromosomal loci; and in addition it also infers the ancestral genetic patterns and offers a highly accurate map of ancestral compositions of modern populations. 1</p><p>5 0.5291627 <a title="192-lsi-5" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>Author: Danko Nikolić, Stefan Haeusler, Wolf Singer, Wolfgang Maass</p><p>Abstract: We use multi-electrode recordings from cat primary visual cortex and investigate whether a simple linear classifier can extract information about the presented stimuli. We find that information is extractable and that it even lasts for several hundred milliseconds after the stimulus has been removed. In a fast sequence of stimulus presentation, information about both new and old stimuli is present simultaneously and nonlinear relations between these stimuli can be extracted. These results suggest nonlinear properties of cortical representations. The important implications of these properties for the nonlinear brain theory are discussed.</p><p>6 0.51260042 <a title="192-lsi-6" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<p>7 0.47641212 <a title="192-lsi-7" href="./nips-2006-Learning_Time-Intensity_Profiles_of_Human_Activity_using_Non-Parametric_Bayesian_Models.html">114 nips-2006-Learning Time-Intensity Profiles of Human Activity using Non-Parametric Bayesian Models</a></p>
<p>8 0.47174963 <a title="192-lsi-8" href="./nips-2006-Adaptor_Grammars%3A_A_Framework_for_Specifying_Compositional_Nonparametric_Bayesian_Models.html">23 nips-2006-Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models</a></p>
<p>9 0.43790847 <a title="192-lsi-9" href="./nips-2006-Effects_of_Stress_and_Genotype_on_Meta-parameter_Dynamics_in_Reinforcement_Learning.html">71 nips-2006-Effects of Stress and Genotype on Meta-parameter Dynamics in Reinforcement Learning</a></p>
<p>10 0.43777072 <a title="192-lsi-10" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<p>11 0.43315014 <a title="192-lsi-11" href="./nips-2006-Context_Effects_in_Category_Learning%3A_An_Investigation_of_Four_Probabilistic_Models.html">58 nips-2006-Context Effects in Category Learning: An Investigation of Four Probabilistic Models</a></p>
<p>12 0.43006742 <a title="192-lsi-12" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>13 0.42185852 <a title="192-lsi-13" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>14 0.41193134 <a title="192-lsi-14" href="./nips-2006-Multiple_timescales_and_uncertainty_in_motor_adaptation.html">141 nips-2006-Multiple timescales and uncertainty in motor adaptation</a></p>
<p>15 0.40927008 <a title="192-lsi-15" href="./nips-2006-Modelling_transcriptional_regulation_using_Gaussian_Processes.html">135 nips-2006-Modelling transcriptional regulation using Gaussian Processes</a></p>
<p>16 0.4001779 <a title="192-lsi-16" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>17 0.39962503 <a title="192-lsi-17" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>18 0.39280227 <a title="192-lsi-18" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<p>19 0.38056171 <a title="192-lsi-19" href="./nips-2006-Single_Channel_Speech_Separation_Using_Factorial_Dynamics.html">176 nips-2006-Single Channel Speech Separation Using Factorial Dynamics</a></p>
<p>20 0.37103194 <a title="192-lsi-20" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.088), (3, 0.038), (7, 0.065), (9, 0.052), (12, 0.013), (14, 0.296), (20, 0.058), (22, 0.055), (34, 0.011), (44, 0.077), (57, 0.065), (65, 0.031), (69, 0.016), (71, 0.036), (90, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79414272 <a title="192-lda-1" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>Author: Paul R. Schrater, Rashmi Sundareswara</p><p>Abstract: Perceptual Bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing. Although switching behavior is increasingly well characterized, the origins remain elusive. We propose that perceptual switching naturally arises from the brain’s search for best interpretations while performing Bayesian inference. In particular, we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation. We formalize the theory, explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates. Finally, predictions of the theory are shown to be consistent with measured changes in human switching dynamics to Necker cube stimuli induced by context.</p><p>2 0.77063745 <a title="192-lda-2" href="./nips-2006-Dirichlet-Enhanced_Spam_Filtering_based_on_Biased_Samples.html">68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</a></p>
<p>Author: Steffen Bickel, Tobias Scheffer</p><p>Abstract: We study a setting that is motivated by the problem of ﬁltering spam messages for many users. Each user receives messages according to an individual, unknown distribution, reﬂected only in the unlabeled inbox. The spam ﬁlter for a user is required to perform well with respect to this distribution. Labeled messages from publicly available sources can be utilized, but they are governed by a distinct distribution, not adequately representing most inboxes. We devise a method that minimizes a loss function with respect to a user’s personal distribution based on the available biased sample. A nonparametric hierarchical Bayesian model furthermore generalizes across users by learning a common prior which is imposed on new email accounts. Empirically, we observe that bias-corrected learning outperforms naive reliance on the assumption of independent and identically distributed data; Dirichlet-enhanced generalization across users outperforms a single (“one size ﬁts all”) ﬁlter as well as independent ﬁlters for all users. 1</p><p>3 0.50262344 <a title="192-lda-3" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>4 0.49672243 <a title="192-lda-4" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>Author: Frank Wood, Thomas L. Griffiths</p><p>Abstract: Many unsupervised learning problems can be expressed as a form of matrix factorization, reconstructing an observed data matrix as the product of two matrices of latent variables. A standard challenge in solving these problems is determining the dimensionality of the latent matrices. Nonparametric Bayesian matrix factorization is one way of dealing with this challenge, yielding a posterior distribution over possible factorizations of unbounded dimensionality. A drawback to this approach is that posterior estimation is typically done using Gibbs sampling, which can be slow for large problems and when conjugate priors cannot be used. As an alternative, we present a particle ﬁlter for posterior estimation in nonparametric Bayesian matrix factorization models. We illustrate this approach with two matrix factorization models and show favorable performance relative to Gibbs sampling.</p><p>5 0.49311662 <a title="192-lda-5" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>6 0.49243885 <a title="192-lda-6" href="./nips-2006-Learning_to_be_Bayesian_without_Supervision.html">121 nips-2006-Learning to be Bayesian without Supervision</a></p>
<p>7 0.49147081 <a title="192-lda-7" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>8 0.48996657 <a title="192-lda-8" href="./nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</a></p>
<p>9 0.48747405 <a title="192-lda-9" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>10 0.48734885 <a title="192-lda-10" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>11 0.48716068 <a title="192-lda-11" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>12 0.48680708 <a title="192-lda-12" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>13 0.48644716 <a title="192-lda-13" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>14 0.48627993 <a title="192-lda-14" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>15 0.48601094 <a title="192-lda-15" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>16 0.48504189 <a title="192-lda-16" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>17 0.48465142 <a title="192-lda-17" href="./nips-2006-Information_Bottleneck_for_Non_Co-Occurrence_Data.html">100 nips-2006-Information Bottleneck for Non Co-Occurrence Data</a></p>
<p>18 0.48394111 <a title="192-lda-18" href="./nips-2006-MLLE%3A_Modified_Locally_Linear_Embedding_Using_Multiple_Weights.html">127 nips-2006-MLLE: Modified Locally Linear Embedding Using Multiple Weights</a></p>
<p>19 0.48379236 <a title="192-lda-19" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>20 0.48364231 <a title="192-lda-20" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
