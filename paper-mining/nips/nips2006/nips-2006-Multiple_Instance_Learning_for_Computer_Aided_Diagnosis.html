<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-140" href="#">nips2006-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</h1>
<br/><p>Source: <a title="nips-2006-140-pdf" href="http://papers.nips.cc/paper/3124-multiple-instance-learning-for-computer-aided-diagnosis.pdf">pdf</a></p><p>Author: Murat Dundar, Balaji Krishnapuram, R. B. Rao, Glenn M. Fung</p><p>Abstract: Many computer aided diagnosis (CAD) problems can be best modelled as a multiple-instance learning (MIL) problem with unbalanced data: i.e. , the training data typically consists of a few positive bags, and a very large number of negative instances. Existing MIL algorithms are much too computationally expensive for these datasets. We describe CH, a framework for learning a Convex Hull representation of multiple instances that is signiﬁcantly faster than existing MIL algorithms. Our CH framework applies to any standard hyperplane-based learning algorithm, and for some algorithms, is guaranteed to ﬁnd the global optimal solution. Experimental studies on two different CAD applications further demonstrate that the proposed algorithm signiﬁcantly improves diagnostic accuracy when compared to both MIL and traditional classiﬁers. Although not designed for standard MIL problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other MIL methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art.</p><p>Reference: <a title="nips-2006-140-reference" href="../nips2006_reference/nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Many computer aided diagnosis (CAD) problems can be best modelled as a multiple-instance learning (MIL) problem with unbalanced data: i. [sent-7, score-0.114]
</p><p>2 , the training data typically consists of a few positive bags, and a very large number of negative instances. [sent-9, score-0.143]
</p><p>3 We describe CH, a framework for learning a Convex Hull representation of multiple instances that is signiﬁcantly faster than existing MIL algorithms. [sent-11, score-0.046]
</p><p>4 Experimental studies on two different CAD applications further demonstrate that the proposed algorithm signiﬁcantly improves diagnostic accuracy when compared to both MIL and traditional classiﬁers. [sent-13, score-0.057]
</p><p>5 Although not designed for standard MIL problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other MIL methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art. [sent-14, score-0.291]
</p><p>6 1 Introduction In many computer aided diagnosis applications, the goal is to detect potentially malignant tumors and lesions in medical images (CT scans, X-ray, MRI etc). [sent-15, score-0.217]
</p><p>7 In an almost universal paradigm for CAD algorithms, this problem is addressed by a 3 stage system: identiﬁcation of potentially unhealthy regions of interest (ROI) by a candidate generator, computation of descriptive features for each candidate, and labeling of each candidate (e. [sent-16, score-0.205]
</p><p>8 The training dataset for the classiﬁer is generated as follows: Expert radiologists examine a set of images to mark out tumors. [sent-19, score-0.157]
</p><p>9 Then, candidate ROIs (with associated computed features) are marked positive if they are sufﬁciently close to a radiologist mark, and negative otherwise. [sent-20, score-0.363]
</p><p>10 Many CAD datasets have fewer than 1-10% positive candidates. [sent-21, score-0.128]
</p><p>11 In Section 2 we show that CAD data is better modeled in the multiple instance learning (MIL) framework, and subsequently present a novel convex-hull-based MIL algorithm. [sent-23, score-0.045]
</p><p>12 In Section 3 we provide experimental evidence from two different CAD problems to show that the proposed algorithm is signiﬁcantly faster than other MIL algorithms, and more accurate when compared to other MIL algorithms and to traditional classiﬁers. [sent-24, score-0.034]
</p><p>13 Further—although this is not the main focus of our paper—on traditional benchmarks for MIL, our algorithm is again shown to be competitive with the current state-of-the-art. [sent-25, score-0.034]
</p><p>14 This property is clearly violated in a CAD dataset, due to spatial adjacency of the regions identiﬁed by a candidate generator, both the features and the class labels of several adjacent candidates (training instances) are highly correlated. [sent-30, score-0.232]
</p><p>15 Second, because candidates are labelled positive if they are within some pre-determined distance from a radiologist mark, multiple positive candidates could correspond with the same (positive) radiologist mark on the image. [sent-32, score-0.72]
</p><p>16 Note that some of the positively labelled candidates may actually refer to healthy structures that just happen to be near a mark, thereby introducing an asymmetric labeling error in the training data. [sent-33, score-0.169]
</p><p>17 In MIL terminology from previous literature, a “bag” may contain many observation instances of the same underlying entity, and every training bag is provided a class label (e. [sent-34, score-0.299]
</p><p>18 The objective in MIL is to learn a classiﬁer that correctly classiﬁes at least one instance from every bag. [sent-37, score-0.087]
</p><p>19 In particular, even if one of the candidates that refers to the underlying malignant structure (radiologist mark) is correctly highlighted to the radiologist, the malignant structure is detected; i. [sent-39, score-0.29]
</p><p>20 , the correct classiﬁcation of every candidate instance is not as important as the ability to detect at least one candidate that points to a malignant region. [sent-41, score-0.283]
</p><p>21 Furthermore, we would like to classify every sample that is distant from radiologist mark as negative, this is easily accomplished by considering each negative candidate as a bag. [sent-42, score-0.372]
</p><p>22 Therefore, it would appear that MIL algorithms should outperform traditional classiﬁers on CAD datasets. [sent-43, score-0.034]
</p><p>23 In CAD we typically have several thousand mostly negative candidates (instances), and a few hundred positive bags; existing MIL algorithms are simply unable to handle such large datasets due to time or memory requirements. [sent-45, score-0.329]
</p><p>24 i  i Notation: Let the i-th bag of class j be represented by the matrix Bj ∈ ℜmj ×n , i = 1, . [sent-46, score-0.224]
</p><p>25 , rj , i il j ∈ {±1}, n is the number of features. [sent-49, score-0.106]
</p><p>26 The row l of Bj , denoted by Bj represents the datapoint l of the bag i in class j with l = 1, . [sent-50, score-0.224]
</p><p>27 1 Key idea: Relaxation of MIL via Convex-Hulls The original MIL problem requires at least one of the samples in a bag to be correctly labeled by the classiﬁer: this corresponds to a set of discrete constraints on the classiﬁer. [sent-57, score-0.266]
</p><p>28 By contrast, we shall relax this and require that at least one point in the convex hull of a bag of samples (including, possibly one of the original samples) has to be correctly classiﬁed. [sent-58, score-0.409]
</p><p>29 As mentioned above, we will i i consider that a bag Bj is correctly classiﬁed if any point inside the convex hull of the bag Bj (i. [sent-61, score-0.633]
</p><p>30 i i ′ i any convex combination of points of Bj ) is correctly classiﬁed. [sent-63, score-0.102]
</p><p>31 0 ≤ λj , e λj = 1 be the vector containing the coefﬁcients of the convex combination that deﬁnes the representative point of bag i in class j. [sent-66, score-0.318]
</p><p>32 Let r be the total number of representative points, i. [sent-67, score-0.034]
</p><p>33 Let γ be the total number of convex hull coefﬁcients corresponding to the representative points in class j, i. [sent-70, score-0.177]
</p><p>34 E : ℜr ⇒ ℜ represents the loss j function, Φ : ℜ(n+1) ⇒ ℜ is a regularization function on the hyperplane coefﬁcients [2] and Ψ is a regularization function on the convex combination coefﬁcients λi . [sent-82, score-0.089]
</p><p>35 Positive and negative classes are represented by blue circles and red diamonds respectively. [sent-98, score-0.061]
</p><p>36 Cyan polyhedrons represent the convex hulls for the three positives bags, the points chosen by our algorithm to represent each bag is shown by blue stars. [sent-99, score-0.284]
</p><p>37 The magenta line represents the linear hyperplane obtained by our algorithm and the black line represents the hyperplane for the SVM. [sent-100, score-0.058]
</p><p>38 2 2  and Ω = ℜr+ , leads to MIL versions of the  and Ω = ℜr , leads to MIL versions of the Least-  2  3. [sent-105, score-0.062]
</p><p>39 ν = 1, E(ξ) = ξ 2 , Ω = {ξ : e′ ξj = 0, j ∈ {±}} leads to MIL versions of the QP formulation for Fisher’s linear discriminant (FD) [4]. [sent-106, score-0.088]
</p><p>40 min  (ξ,w,η,λ)∈Rr+n+1+γ  ξ  2 2  +  Φ(w, η) + Ψ(λ)  i  i ξ = di − (λi Bj w − eη) j (2) e ξj = 0 ′ i e λj = 1 0 ≤ λi j The number of variables to be optimized in (2) is r+n+1+γ: this is computationally infeasible when i the number of bags is large (r > 104 ). [sent-110, score-0.135]
</p><p>41 Effectively, this results in the MIL version of the traditional FD algorithm. [sent-113, score-0.034]
</p><p>42 As discussed later in the paper, in addition to the obvious computational gains, this manipulation results in some algorithmic advantages as well (For more information on the equivalence between the single instance learning versions of (2) and (3) see [4]). [sent-114, score-0.076]
</p><p>43 wT (µ+ − µ− ) = e′ λi = j 0 ≤ T  b 1 λi j  (3)  1 1 where SW = j∈{±} rj (Xj − µj e′ ) (Xj − µj e′ ) is the within class scatter matrix, µj = rj Xj e is the mean for class j. [sent-119, score-0.212]
</p><p>44 Xj ∈ ℜrj ×n is a matrix containing the rj representative points on ni dimensional space such that the row of Xj denoted by bi = Bj λi is the representative point of bag j j i in class j where i = {1, . [sent-120, score-0.398]
</p><p>45 When Φ(w) and Ψ(λ) are strongly convex functions, both the original objective function and the two subproblems (for optimizing λ and w) in (3) are strongly convex, meaning that the algorithm converges to a global minimizer [6]. [sent-127, score-0.087]
</p><p>46 For computational efﬁciency, in the remainder of the paper we will use the 2 2 regularizers Φ(w) = ǫ w 2 and Ψ(λ) = ǫ λ 2 , where ǫ is a positive regularization parameter. [sent-128, score-0.053]
</p><p>47 Since SW is the sum of two covariance matrices, it is guaranteed to be at least positive semideﬁnite and thus the problem in (4) is convex. [sent-133, score-0.053]
</p><p>48 the number of bags is much greater than the number of dimensionality, SW is positive deﬁnite and thus the problem in (4) is strictly convex. [sent-136, score-0.188]
</p><p>49 This changes the order of complexity from O(nr2 ) to O(n2 r) and brings some computational advantages when dealing with datasets with r >> n. [sent-138, score-0.1]
</p><p>50 For the positive class elements k=1 m+ + 1 through r+ i k k ¯i k=1 m+ of bj are nonzero, for the negative class nonzero elements are located at k=1 m+ + r+ i−1 i k k k ¯ k=1 m− + 1 through k=1 m+ + k=1 m− . [sent-142, score-0.242]
</p><p>51 Note that SW is also a sum of two covariance matrices, it is positive semideﬁnite and thus the problem in (5) is convex. [sent-143, score-0.053]
</p><p>52 Unlike sub problem 1 ¯ the positive deﬁniteness of SW does not depend on the data, since it always true that r ≤ γ. [sent-144, score-0.093]
</p><p>53 As it was mentioned before, in CAD applications, a bag is deﬁned as a set of candidates that are spatially close to the radiologist marked ground-truth. [sent-146, score-0.545]
</p><p>54 Any candidate that is spatially far from this location is considered negative in the training data, therefore the concept of bag for negative examples does not make any practical sense in this scenario. [sent-147, score-0.491]
</p><p>55 Moreover, since ground truth is only available on the training set, there is no concept of a bag on the test set for both positive and negative examples. [sent-148, score-0.367]
</p><p>56 The learned classiﬁer labels (ie classiﬁes) individual instances - the bag information for positive examples is only used to help learn a better classiﬁer from the training data. [sent-149, score-0.352]
</p><p>57 The nonlinear version of the proposed algorithm can be obtained by ﬁrst transforming the original ¯ datapoints to a kernel space spanned by all datapoints through a kernel operator, i. [sent-170, score-0.126]
</p><p>58 However when γ is ¯ large, for computational reasons we can use the technique presented in [7] to limit the number of datapoints spanning this new space. [sent-174, score-0.063]
</p><p>59 All algorithms are trained on the training data and then tested on the sequestered test data. [sent-181, score-0.08]
</p><p>60 Pulmonary embolism (PE), a potentially life-threatening condition, is a result of underlying venous thromboembolic disease. [sent-185, score-0.089]
</p><p>61 An early and accurate diagnosis is the key to survival. [sent-186, score-0.047]
</p><p>62 Computed tomography angiography (CTA) has emerged as an accurate diagnostic tool for PE, and However, there are hundreds of CT slices in each CTA study and manual reading is laborious, time consuming and complicated by various PE lookalikes. [sent-187, score-0.047]
</p><p>63 At four different hospitals (two North American sites and two European sites), we collected 72 cases with 242 PE bags comprised of 1069 positive candidates marked by expert chest radiologists. [sent-189, score-0.406]
</p><p>64 The cases were randomly divided into two sets: training (48 cases with 173 PE bags and 3655 candidates) and testing (24 cases with 69 PE bags and 1857 candidates). [sent-190, score-0.299]
</p><p>65 The test group was sequestered and only used to evaluate the performance of the ﬁnal system. [sent-191, score-0.051]
</p><p>66 Colorectal cancer is the third most common cancer in both men and women. [sent-193, score-0.112]
</p><p>67 It is estimated that in 2004, nearly 147,000 cases of colon and rectal cancer will be diagnosed in the US, and more than 56,730 people would die from colon cancer [13]. [sent-194, score-0.554]
</p><p>68 CT colonography is emerging as a new procedure to help in early detection of colon polyps. [sent-195, score-0.276]
</p><p>69 However, reading through a large CT dataset, which typically consists of two CT series of the patient in prone and supine positions, each with several hundred slices, is time-consuming. [sent-196, score-0.072]
</p><p>70 Colon CAD [14] can play a critical role to help the radiologist avoid the missing of colon polyps. [sent-197, score-0.357]
</p><p>71 Moreover, for large polyps, a typical candidate generation algorithm generates several candidates across the polyp surface. [sent-199, score-0.266]
</p><p>72 The test group was sequestered and only used to evaluate the performance of the ﬁnal system. [sent-202, score-0.051]
</p><p>73 Note that CAD performance is only valid in the clinically acceptable range, < 10fp/patient for PE, < 5fp/volume for Colon (generally there are 2 volumes per patient). [sent-206, score-0.036]
</p><p>74 Among MIL algorithms, for the PE data, CH-FD was roughly 2-times and 9-times as fast than IAPR and EMDD respectively, and for the much larger colon dataset was roughly 85-times and 2000-times faster, respectively(see Table 1). [sent-234, score-0.253]
</p><p>75 1  0  0  10  20  30 FP/Patient  40  50  60  0  0  10  20  30 False positive per volume  40  50  60  Figure 2: ROC curves obtained for (left) PE Testing data and (right) COLON testing Data 3. [sent-253, score-0.053]
</p><p>76 2 Experiments on Benchmark Datasets We compare CH-FD with several state-of-the-art MIL algorithms on 5 benchmark MIL datasets: 2 Musk datasets [9] and 3 Image Annotation datasets [15]. [sent-254, score-0.192]
</p><p>77 Each of these datasets contain both positive and negative bags. [sent-255, score-0.189]
</p><p>78 CH-FD (and MICA) use just the positive bag information and ignore the negative bag information, in effect, treating each negative instance as a separate bag. [sent-256, score-0.668]
</p><p>79 All the other MIL algorithms use both the positive and negative bag information. [sent-257, score-0.338]
</p><p>80 The Musk datasets contains feature vectors describing the surfaces of low-energy shapes from molecules. [sent-258, score-0.075]
</p><p>81 The goal is to differentiate molecules that smell ”musky” from the rest of the molecules. [sent-260, score-0.097]
</p><p>82 Approximately half of the molecules are known to smell musky. [sent-261, score-0.097]
</p><p>83 MUSK1 contains 92 molecules with a total of 476 instances. [sent-263, score-0.063]
</p><p>84 MUSK2 contains 102 molecules with a total of 6598 instances. [sent-264, score-0.063]
</p><p>85 72 of the molecules are shared between two datasets but MUSK2 dataset contain more instances for the shared molecules. [sent-265, score-0.216]
</p><p>86 Each dataset has 100 positive bags and 100 negative bags. [sent-267, score-0.281]
</p><p>87 For the musk datasets our results are based on a Radial Basis Function 2 (RBF) kernel K(xi , xj ) = exp(−σ x − y ). [sent-269, score-0.181]
</p><p>88 The kernel space is assumed to be spanned by all the datapoints in MUSK1 dataset and a subset of the datapoints in MUSK2 dataset (one tenth of the original training set is randomly selected for this purpose). [sent-270, score-0.219]
</p><p>89 We follow the benchmark experiment design and report average accuracy of 10 runs of 10-fold Cross Validation  Table 2: Average accuracy on Benchmark Datasets. [sent-273, score-0.042]
</p><p>90 Table 2 shows that CH-FD is comparable to other techniques on all datasets, even though it ignores the negative bag information. [sent-313, score-0.285]
</p><p>91 4 Discussions Relationship to previous literature on MIL: The Multiple Instance Learning problem described in this paper has been studied widely in the literature [9, 15, 16, 17, 8]. [sent-316, score-0.048]
</p><p>92 The convex-hull idea presented in this paper to represent each bag is similar in nature to the one presented in [1]. [sent-317, score-0.224]
</p><p>93 However in contrast with [1] and many other approaches in the literature [9, 15, 17] our formulation leads to a strongly convex minimization problem that converges to a unique minimizer. [sent-318, score-0.084]
</p><p>94 Since our algorithm considers each negative instance as an individual bag, it is complexity is square proportional to the number of positive instances only which makes it scalable to large datasets with large number of negative examples. [sent-319, score-0.341]
</p><p>95 In our benchmark experiments, the resulting algorithm achieves accuracy that is comparable to the current state of the art, but at a signiﬁcantly lower run time (typically several orders of magnitude speed ups were observed). [sent-326, score-0.042]
</p><p>96 Related work: Note that as the distance between candidate ROI increases, the correlations between their features and labels decreases. [sent-327, score-0.092]
</p><p>97 Thus we jointly classify entire batches of correlated samples both during training and testing. [sent-329, score-0.05]
</p><p>98 Instead of classifying each sample independently, we use this spatial information along with the features of each candidate to simultaneously classify all the candidate ROIs for a single patient/volume in a joint operation [18]. [sent-330, score-0.205]
</p><p>99 Computer aided detection of pulmonary embolism on multi-detector ct, 2004. [sent-408, score-0.224]
</p><p>100 Computerized detection of pulmonary embolism in 3D computed tomographic (CT) images: vessel tracking and segmentation techniques. [sent-421, score-0.157]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mil', 0.646), ('cad', 0.34), ('bag', 0.224), ('colon', 0.221), ('pe', 0.166), ('fd', 0.148), ('candidates', 0.14), ('radiologist', 0.136), ('bags', 0.135), ('rj', 0.106), ('bj', 0.106), ('iapr', 0.102), ('sw', 0.093), ('candidate', 0.092), ('emdd', 0.085), ('hull', 0.083), ('datasets', 0.075), ('fisher', 0.075), ('embolism', 0.068), ('idapr', 0.068), ('musk', 0.068), ('pulmonary', 0.068), ('aided', 0.067), ('molecules', 0.063), ('datapoints', 0.063), ('ct', 0.062), ('mark', 0.062), ('negative', 0.061), ('convex', 0.06), ('discriminant', 0.057), ('auc', 0.056), ('cancer', 0.056), ('malignant', 0.054), ('classi', 0.054), ('positive', 0.053), ('ao', 0.051), ('polyps', 0.051), ('sequestered', 0.051), ('ch', 0.05), ('diagnosis', 0.047), ('instances', 0.046), ('instance', 0.045), ('benchmark', 0.042), ('correctly', 0.042), ('sub', 0.04), ('xj', 0.038), ('patient', 0.038), ('volumes', 0.036), ('representative', 0.034), ('colonography', 0.034), ('cta', 0.034), ('dundar', 0.034), ('elephant', 0.034), ('mica', 0.034), ('polyp', 0.034), ('radiologists', 0.034), ('smell', 0.034), ('supine', 0.034), ('annotation', 0.034), ('traditional', 0.034), ('wisconsin', 0.032), ('dataset', 0.032), ('versions', 0.031), ('ic', 0.031), ('comprised', 0.03), ('ftp', 0.03), ('roi', 0.03), ('rois', 0.03), ('tiger', 0.03), ('hyperplane', 0.029), ('er', 0.029), ('training', 0.029), ('medical', 0.028), ('sites', 0.027), ('receiver', 0.027), ('fung', 0.027), ('subproblems', 0.027), ('krishnapuram', 0.027), ('wt', 0.025), ('brings', 0.025), ('rr', 0.025), ('wc', 0.025), ('spatially', 0.024), ('literature', 0.024), ('generator', 0.024), ('dd', 0.024), ('operating', 0.024), ('slices', 0.024), ('roc', 0.023), ('diagnostic', 0.023), ('madison', 0.023), ('patients', 0.023), ('nonzero', 0.022), ('table', 0.022), ('multi', 0.022), ('fix', 0.022), ('potentially', 0.021), ('classify', 0.021), ('detection', 0.021), ('marked', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="140-tfidf-1" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>Author: Murat Dundar, Balaji Krishnapuram, R. B. Rao, Glenn M. Fung</p><p>Abstract: Many computer aided diagnosis (CAD) problems can be best modelled as a multiple-instance learning (MIL) problem with unbalanced data: i.e. , the training data typically consists of a few positive bags, and a very large number of negative instances. Existing MIL algorithms are much too computationally expensive for these datasets. We describe CH, a framework for learning a Convex Hull representation of multiple instances that is signiﬁcantly faster than existing MIL algorithms. Our CH framework applies to any standard hyperplane-based learning algorithm, and for some algorithms, is guaranteed to ﬁnd the global optimal solution. Experimental studies on two different CAD applications further demonstrate that the proposed algorithm signiﬁcantly improves diagnostic accuracy when compared to both MIL and traditional classiﬁers. Although not designed for standard MIL problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other MIL methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art.</p><p>2 0.062591121 <a title="140-tfidf-2" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers for structurally incomplete data, where some objects have a subset of features inherently absent due to complex relationships between the features. The common approach for handling missing features is to begin with a preprocessing phase that completes the missing features, and then use a standard classiﬁcation procedure. In this paper we show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate this task using a geometrically-inspired objective function, and discuss two optimization approaches: The linearly separable case is written as a set of convex feasibility problems, and the non-separable case has a non-convex objective that we optimize iteratively. By avoiding the pre-processing phase in which the data is completed, these approaches oﬀer considerable computational savings. More importantly, we show that by elegantly handling complex patterns of missing values, our approach is both competitive with other methods when the values are missing at random and outperforms them when the missing values have non-trivial structure. We demonstrate our results on two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. 1</p><p>3 0.055725537 <a title="140-tfidf-3" href="./nips-2006-Multi-Instance_Multi-Label_Learning_with_Application_to_Scene_Classification.html">136 nips-2006-Multi-Instance Multi-Label Learning with Application to Scene Classification</a></p>
<p>Author: Zhi-hua Zhou, Min-ling Zhang</p><p>Abstract: In this paper, we formalize multi-instance multi-label learning, where each training example is associated with not only multiple instances but also multiple class labels. Such a problem can occur in many real-world tasks, e.g. an image usually contains multiple patches each of which can be described by a feature vector, and the image can belong to multiple categories since its semantics can be recognized in different ways. We analyze the relationship between multi-instance multi-label learning and the learning frameworks of traditional supervised learning, multiinstance learning and multi-label learning. Then, we propose the M IML B OOST and M IML S VM algorithms which achieve good performance in an application to scene classiﬁcation. 1</p><p>4 0.047979813 <a title="140-tfidf-4" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>Author: Frank Moosmann, Bill Triggs, Frederic Jurie</p><p>Abstract: Some of the most effective recent methods for content-based image classiﬁcation work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting “visual word” codes over the image, and classifying these with a conventional classiﬁer such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests – ensembles of randomly created clustering trees – and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classiﬁcation tasks. 1</p><p>5 0.047514506 <a title="140-tfidf-5" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>Author: Mark Girolami, Mingjun Zhong</p><p>Abstract: By adopting Gaussian process priors a fully Bayesian solution to the problem of integrating possibly heterogeneous data sets within a classiﬁcation setting is presented. Approximate inference schemes employing Variational & Expectation Propagation based methods are developed and rigorously assessed. We demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classiﬁer combination. 1</p><p>6 0.046075221 <a title="140-tfidf-6" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>7 0.044310264 <a title="140-tfidf-7" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>8 0.041761972 <a title="140-tfidf-8" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>9 0.041147828 <a title="140-tfidf-9" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>10 0.040943954 <a title="140-tfidf-10" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>11 0.040898178 <a title="140-tfidf-11" href="./nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</a></p>
<p>12 0.04087083 <a title="140-tfidf-12" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>13 0.039979808 <a title="140-tfidf-13" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>14 0.038718801 <a title="140-tfidf-14" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>15 0.038185813 <a title="140-tfidf-15" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>16 0.036602661 <a title="140-tfidf-16" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>17 0.036349349 <a title="140-tfidf-17" href="./nips-2006-Randomized_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">164 nips-2006-Randomized PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>18 0.036083896 <a title="140-tfidf-18" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>19 0.036047783 <a title="140-tfidf-19" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>20 0.035731059 <a title="140-tfidf-20" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.031), (2, 0.013), (3, -0.004), (4, -0.025), (5, 0.032), (6, -0.048), (7, 0.004), (8, -0.001), (9, -0.007), (10, -0.074), (11, 0.03), (12, -0.014), (13, -0.023), (14, 0.023), (15, 0.048), (16, 0.014), (17, -0.03), (18, 0.003), (19, 0.029), (20, 0.023), (21, -0.002), (22, 0.03), (23, 0.024), (24, 0.034), (25, -0.036), (26, -0.027), (27, -0.04), (28, 0.053), (29, -0.016), (30, -0.001), (31, -0.044), (32, 0.06), (33, 0.032), (34, 0.039), (35, 0.028), (36, 0.072), (37, 0.12), (38, -0.033), (39, -0.05), (40, 0.002), (41, -0.014), (42, 0.086), (43, 0.003), (44, 0.049), (45, 0.016), (46, -0.098), (47, 0.016), (48, 0.141), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87170702 <a title="140-lsi-1" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>Author: Murat Dundar, Balaji Krishnapuram, R. B. Rao, Glenn M. Fung</p><p>Abstract: Many computer aided diagnosis (CAD) problems can be best modelled as a multiple-instance learning (MIL) problem with unbalanced data: i.e. , the training data typically consists of a few positive bags, and a very large number of negative instances. Existing MIL algorithms are much too computationally expensive for these datasets. We describe CH, a framework for learning a Convex Hull representation of multiple instances that is signiﬁcantly faster than existing MIL algorithms. Our CH framework applies to any standard hyperplane-based learning algorithm, and for some algorithms, is guaranteed to ﬁnd the global optimal solution. Experimental studies on two different CAD applications further demonstrate that the proposed algorithm signiﬁcantly improves diagnostic accuracy when compared to both MIL and traditional classiﬁers. Although not designed for standard MIL problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other MIL methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art.</p><p>2 0.52283782 <a title="140-lsi-2" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers for structurally incomplete data, where some objects have a subset of features inherently absent due to complex relationships between the features. The common approach for handling missing features is to begin with a preprocessing phase that completes the missing features, and then use a standard classiﬁcation procedure. In this paper we show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate this task using a geometrically-inspired objective function, and discuss two optimization approaches: The linearly separable case is written as a set of convex feasibility problems, and the non-separable case has a non-convex objective that we optimize iteratively. By avoiding the pre-processing phase in which the data is completed, these approaches oﬀer considerable computational savings. More importantly, we show that by elegantly handling complex patterns of missing values, our approach is both competitive with other methods when the values are missing at random and outperforms them when the missing values have non-trivial structure. We demonstrate our results on two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. 1</p><p>3 0.49701625 <a title="140-lsi-3" href="./nips-2006-Large_Margin_Component_Analysis.html">105 nips-2006-Large Margin Component Analysis</a></p>
<p>Author: Lorenzo Torresani, Kuang-chih Lee</p><p>Abstract: Metric learning has been shown to signiﬁcantly improve the accuracy of k-nearest neighbor (kNN) classiﬁcation. In problems involving thousands of features, distance learning algorithms cannot be used due to overﬁtting and high computational complexity. In such cases, previous work has relied on a two-step solution: ﬁrst apply dimensionality reduction methods to the data, and then learn a metric in the resulting low-dimensional subspace. In this paper we show that better classiﬁcation performance can be achieved by unifying the objectives of dimensionality reduction and metric learning. We propose a method that solves for the low-dimensional projection of the inputs, which minimizes a metric objective aimed at separating points in different classes by a large margin. This projection is deﬁned by a signiﬁcantly smaller number of parameters than metrics learned in input space, and thus our optimization reduces the risks of overﬁtting. Theory and results are presented for both a linear as well as a kernelized version of the algorithm. Overall, we achieve classiﬁcation rates similar, and in several cases superior, to those of support vector machines. 1</p><p>4 0.47050831 <a title="140-lsi-4" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: The standard Support Vector Machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to deﬁne the generated classiﬁer. We present a modiﬁed version of SVM that allows the user to set a budget parameter B and focuses on minimizing the loss attained by the B worst-classiﬁed examples while ignoring the remaining examples. This idea can be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we obtain these new SVM variants by replacing the 1-norm in the standard SVM formulation with various interpolation-norms. We also adapt the SMO optimization algorithm to our setting and report on some preliminary experimental results. 1</p><p>5 0.46537641 <a title="140-lsi-5" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>6 0.4617022 <a title="140-lsi-6" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>7 0.46114722 <a title="140-lsi-7" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>8 0.43753028 <a title="140-lsi-8" href="./nips-2006-Multi-Instance_Multi-Label_Learning_with_Application_to_Scene_Classification.html">136 nips-2006-Multi-Instance Multi-Label Learning with Application to Scene Classification</a></p>
<p>9 0.42038399 <a title="140-lsi-9" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>10 0.41944987 <a title="140-lsi-10" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>11 0.41379377 <a title="140-lsi-11" href="./nips-2006-Aggregating_Classification_Accuracy_across_Time%3A_Application_to_Single_Trial_EEG.html">24 nips-2006-Aggregating Classification Accuracy across Time: Application to Single Trial EEG</a></p>
<p>12 0.41254669 <a title="140-lsi-12" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>13 0.40689585 <a title="140-lsi-13" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<p>14 0.40612069 <a title="140-lsi-14" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>15 0.40605798 <a title="140-lsi-15" href="./nips-2006-Efficient_sparse_coding_algorithms.html">75 nips-2006-Efficient sparse coding algorithms</a></p>
<p>16 0.39787111 <a title="140-lsi-16" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>17 0.39515626 <a title="140-lsi-17" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>18 0.39145061 <a title="140-lsi-18" href="./nips-2006-Recursive_Attribute_Factoring.html">166 nips-2006-Recursive Attribute Factoring</a></p>
<p>19 0.38349172 <a title="140-lsi-19" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>20 0.38253739 <a title="140-lsi-20" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.111), (3, 0.027), (7, 0.063), (9, 0.04), (20, 0.013), (22, 0.07), (44, 0.04), (57, 0.075), (65, 0.057), (69, 0.019), (71, 0.024), (90, 0.022), (99, 0.334)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91626775 <a title="140-lda-1" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>Author: Sandeep Pandey, Christopher Olston</p><p>Abstract: We consider how a search engine should select advertisements to display with search results, in order to maximize its revenue. Under the standard “pay-per-click” arrangement, revenue depends on how well the displayed advertisements appeal to users. The main diﬃculty stems from new advertisements whose degree of appeal has yet to be determined. Often the only reliable way of determining appeal is exploration via display to users, which detracts from exploitation of other advertisements known to have high appeal. Budget constraints and ﬁnite advertisement lifetimes make it necessary to explore as well as exploit. In this paper we study the tradeoﬀ between exploration and exploitation, modeling advertisement placement as a multi-armed bandit problem. We extend traditional bandit formulations to account for budget constraints that occur in search engine advertising markets, and derive theoretical bounds on the performance of a family of algorithms. We measure empirical performance via extensive experiments over real-world data. 1</p><p>2 0.80703318 <a title="140-lda-2" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>Author: Konrad P. Körding, Joshua B. Tenenbaum</p><p>Abstract: Many recent studies analyze how data from different modalities can be combined. Often this is modeled as a system that optimally combines several sources of information about the same variable. However, it has long been realized that this information combining depends on the interpretation of the data. Two cues that are perceived by different modalities can have different causal relationships: (1) They can both have the same cause, in this case we should fully integrate both cues into a joint estimate. (2) They can have distinct causes, in which case information should be processed independently. In many cases we will not know if there is one joint cause or two independent causes that are responsible for the cues. Here we model this situation as a Bayesian estimation problem. We are thus able to explain some experiments on visual auditory cue combination as well as some experiments on visual proprioceptive cue integration. Our analysis shows that the problem solved by people when they combine cues to produce a movement is much more complicated than is usually assumed, because they need to infer the causal structure that is underlying their sensory experience.</p><p>same-paper 3 0.73221445 <a title="140-lda-3" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>Author: Murat Dundar, Balaji Krishnapuram, R. B. Rao, Glenn M. Fung</p><p>Abstract: Many computer aided diagnosis (CAD) problems can be best modelled as a multiple-instance learning (MIL) problem with unbalanced data: i.e. , the training data typically consists of a few positive bags, and a very large number of negative instances. Existing MIL algorithms are much too computationally expensive for these datasets. We describe CH, a framework for learning a Convex Hull representation of multiple instances that is signiﬁcantly faster than existing MIL algorithms. Our CH framework applies to any standard hyperplane-based learning algorithm, and for some algorithms, is guaranteed to ﬁnd the global optimal solution. Experimental studies on two different CAD applications further demonstrate that the proposed algorithm signiﬁcantly improves diagnostic accuracy when compared to both MIL and traditional classiﬁers. Although not designed for standard MIL problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other MIL methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art.</p><p>4 0.47579467 <a title="140-lda-4" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>5 0.47482112 <a title="140-lda-5" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>Author: Samuel S. Gross, Olga Russakovsky, Chuong B. Do, Serafim Batzoglou</p><p>Abstract: We consider the problem of training a conditional random ﬁeld (CRF) to maximize per-label predictive accuracy on a training set, an approach motivated by the principle of empirical risk minimization. We give a gradient-based procedure for minimizing an arbitrarily accurate approximation of the empirical risk under a Hamming loss function. In experiments with both simulated and real data, our optimization procedure gives signiﬁcantly better testing performance than several current approaches for CRF training, especially in situations of high label noise. 1</p><p>6 0.47459111 <a title="140-lda-6" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>7 0.47133112 <a title="140-lda-7" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>8 0.46918917 <a title="140-lda-8" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>9 0.46895269 <a title="140-lda-9" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>10 0.46866122 <a title="140-lda-10" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>11 0.46848851 <a title="140-lda-11" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>12 0.4683153 <a title="140-lda-12" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>13 0.46771577 <a title="140-lda-13" href="./nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">20 nips-2006-Active learning for misspecified generalized linear models</a></p>
<p>14 0.46662518 <a title="140-lda-14" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>15 0.46569142 <a title="140-lda-15" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>16 0.46474531 <a title="140-lda-16" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>17 0.46414247 <a title="140-lda-17" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>18 0.46406776 <a title="140-lda-18" href="./nips-2006-Multi-Task_Feature_Learning.html">138 nips-2006-Multi-Task Feature Learning</a></p>
<p>19 0.46394292 <a title="140-lda-19" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>20 0.46345964 <a title="140-lda-20" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
