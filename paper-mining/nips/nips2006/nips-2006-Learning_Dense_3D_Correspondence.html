<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2006-Learning Dense 3D Correspondence</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-110" href="#">nips2006-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 nips-2006-Learning Dense 3D Correspondence</h1>
<br/><p>Source: <a title="nips-2006-110-pdf" href="http://papers.nips.cc/paper/2957-learning-dense-3d-correspondence.pdf">pdf</a></p><p>Author: Florian Steinke, Volker Blanz, Bernhard Schölkopf</p><p>Abstract: Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. 1</p><p>Reference: <a title="nips-2006-110-reference" href="../nips2006_reference/nips-2006-Learning_Dense_3D_Correspondence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. [sent-5, score-0.963]
</p><p>2 By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. [sent-7, score-0.472]
</p><p>3 1  Introduction  Establishing 3D correspondence between surfaces such as human faces is a crucial element of classspeciﬁc representations of objects in computer vision and graphics. [sent-8, score-0.761]
</p><p>4 Dense correspondence is a mapping or ”warp” from all points of a surface onto another surface (in some cases, including the present work, extending from the surface to the embedding space). [sent-10, score-1.877]
</p><p>5 The practical relevance of surface correspondence has been increasing over the last years. [sent-13, score-0.866]
</p><p>6 In computer vision, an increasing number of algorithms for face and object recognition based on 2D images or 3D scans, as well as shape retrieval in databases and 3D surface reconstruction from images, rely on shape representations that are built upon dense surface correspondence. [sent-15, score-1.266]
</p><p>7 Unlike existing algorithms that deﬁne some ad-hoc criteria for identifying corresponding points on two objects, we treat correspondence as a machine learning problem and propose a data-driven approach that learns the relevant criteria from a dataset of given object correspondences. [sent-16, score-0.75]
</p><p>8 In stereo vision and optical ﬂow [2, 3], a correspondence is correct if and only if it maps a point in one scene to a point in another scene which stems from the same physical point. [sent-17, score-0.538]
</p><p>9 In contrast, correspondence between different objects is not a well-deﬁned problem. [sent-18, score-0.497]
</p><p>10 On a more fundamental level, however, even the problem of matching the eyes is difﬁcult to cast in a formal way, and in fact this matching involves many of the basic problems of computer vision and feature detection. [sent-20, score-0.355]
</p><p>11 In a given application, the desired correspondence can be dependent on anatomical facts, measures of shape similarity, or the overall layout of features on the surface. [sent-21, score-0.524]
</p><p>12 However, it may also depend on the properties of human perception, on functional or semantic issues, on the context within a given object class or even on social  convention. [sent-22, score-0.244]
</p><p>13 Given two objects O1 and O2 , we are seeking a correspondence mapping τ such that certain properties of x (relative to O1 ) are preserved in τ (x) (relative to O2 ) — they are invariant. [sent-25, score-0.608]
</p><p>14 We shall do this by providing a dictionary of potential properties (such as geometric features, or texture properties) and approximating a “true” property characterizing correspondence as an expansion in that dictionary. [sent-28, score-0.7]
</p><p>15 The remainder of the paper is structured as follows: in Section 2 we review some related work, whereas in Section 3 we set up our general framework for computing correspondence ﬁelds. [sent-30, score-0.408]
</p><p>16 Following this, we explain in Section 4 how to learn the characteristic properties for correspondence and continue to explain two new feature functions in Section 5. [sent-31, score-0.749]
</p><p>17 2  Related Work  The problem of establishing dense correspondence has been addressed in the domain of 2D images, on surfaces embedded in 3D space, and on volumetric data. [sent-33, score-0.598]
</p><p>18 In the image domain, correspondence from optical ﬂow [2, 3] has been used to describe the transformations of faces with pose changes and facial expressions [4], and to describe the differences in the shapes of individual faces [5]. [sent-34, score-0.717]
</p><p>19 An algorithm for computing correspondence on parameterized 3D surfaces has been introduced for creating a class-speciﬁc representation of human faces [1] and bodies [6]. [sent-35, score-0.644]
</p><p>20 See the review [9] for an overview of a wide range of additional correspondence algorithms. [sent-39, score-0.408]
</p><p>21 Algorithms that are applied to 3D faces typically rely on surface parameterizations, such as cylindrical coordinates, and then compute optical ﬂow on the texture map as well as the depth image [1]. [sent-40, score-0.707]
</p><p>22 One such algorithm is presented in [10]: here, the surfaces are embedded into the surrounding space and a 3D volume deformation is computed. [sent-44, score-0.288]
</p><p>23 The use of the signed distance function as a guiding feature ensures correct surface to surface mappings. [sent-45, score-1.584]
</p><p>24 Though implicit surface representations allow the extraction of such features [11], these differential geometric properties are inherently instable with respect to noise. [sent-48, score-0.612]
</p><p>25 [12] propose a related 3D geometric feature based on integrals and thus more stable to compute. [sent-49, score-0.261]
</p><p>26 We present a slightly modiﬁed version thereof which allows for a much easier computation of this feature from a signed distance function represented as a kernel expansion in comparison to a complete space voxelisation step required in [12]. [sent-50, score-0.76]
</p><p>27 Given a reference object Or and a target Ot the goal of computing a correspondence can then be expressed as determining the deformation function τ : X → X which maps each point x ∈ X on Or to its corresponding point τ (x) on Ot . [sent-52, score-0.892]
</p><p>28 We further assume that we can construct a dictionary of so-called feature functions fi : X → R, i = 1, . [sent-53, score-0.304]
</p><p>29 [10] propose to use the signed distance function, which maps to each point x ∈ X the distance to the objects surface — with positive sign outside the shape and negative sign inside. [sent-56, score-1.141]
</p><p>30 They also use the ﬁrst derivative of the signed distance function, which can be interpreted as the surface normal. [sent-57, score-0.881]
</p><p>31 In section Section 5 we will propose two additional features which are characteristic for 3D shapes, namely a curvature related feature and surface texture. [sent-58, score-0.861]
</p><p>32 We assume that the warp-invariant feature can be represented or at least approximated by an expansion in this dictionary. [sent-59, score-0.252]
</p><p>33 The second term measures the local similarity of the warp-invariant feature function extracted on the reference object f r and on the target object f t and integrates it over the volume of interest. [sent-66, score-0.729]
</p><p>34 This formulation is a modiﬁcation of [10] where two feature functions were chosen a priori (the signed distance and its derivative) and used instead of fγ . [sent-67, score-0.68]
</p><p>35 In contrast, the present approach starts from the notion of invariance and estimates a location-dependent linear combination of feature functions with a maximal degree of invariance for correct correspondences (cf. [sent-69, score-0.453]
</p><p>36 We consider location-dependent linear combinations since one cannot expect that all the feature functions that deﬁne correspondence are equally important for all points of an object. [sent-71, score-0.699]
</p><p>37 As discussed above, it is unclear how to characterize and evaluate correspondence in a principled way. [sent-74, score-0.439]
</p><p>38 The authors of [10] propose a strategy based on a two-way morph: they ﬁrst compute a deformation from the reference object to the target, and afterwards vice versa. [sent-75, score-0.465]
</p><p>39 4  Learning the optimal feature function  We assume that a database of D objects that are already in correspondence is available. [sent-81, score-0.758]
</p><p>40 This could for example be achieved by manually picking many corresponding point pairs and training a regression to map all the points onto each other, or by (semi-)automatic methods optimized for the given object class (e. [sent-82, score-0.26]
</p><p>41 feature function (as deﬁned in the introduction) that characterizes correspondence using the basic features in our dictionary. [sent-86, score-0.684]
</p><p>42 Thus for each point x ∈ X , we maximize r d Ed,zd fγ (x) − fγ (zd )  2  r d Ed fγ (x) − fγ (τd (x))  2  (2)  r d Here, fγ , fγ are the warp-invariant feature functions evaluated on the reference object and the d-th database object respectively. [sent-89, score-0.754]
</p><p>43 τd (x) is the point matching x on the d-th database object and zd is a random point sampled from it. [sent-90, score-0.297]
</p><p>44 If not, the smoothness term τ H gets relatively more weight implying that the local correspondence is mostly determined through more global contributions. [sent-97, score-0.437]
</p><p>45 Note, moreover that while we have described the above for the leading eigenvector only, nothing prevents us from computing several eigenvectors and stacking up the resulting warp-invariant feature 2 m 1 functions fγ , fγ , . [sent-98, score-0.314]
</p><p>46 5  Basic Feature Functions  In our dictionary of basic feature functions we included the signed distance function and its derivative. [sent-102, score-0.766]
</p><p>47 We added a curvature related feature, the ”signed balls”, and surface texture intensity. [sent-103, score-0.6]
</p><p>48 Take a ball BR (x) with radius R centered at that point and compute the average of the signed distance function s : X → R over the ball’s volume: Is (x)  =  1 VBR (x)  s(x )dx − s(x)  (6)  BR (x)  If the surface around x is ﬂat on the scale of the ball, we obtain zero. [sent-106, score-0.968]
</p><p>49 At points where the surface is bent outwards this value is positive, at concave points it is negative. [sent-107, score-0.58]
</p><p>50 The normalization to the value  B 4mm  B 28mm  C 0mm  C 5mm  C 15mm  Figure 1: The two ﬁgures on the left show the color-coded values of the ”signed balls” feature at different radii R. [sent-108, score-0.263]
</p><p>51 Convex parts of the surface are assigned positive values (blue), concave parts negative (red). [sent-110, score-0.458]
</p><p>52 The three ﬁgures on the right show how the surface feature function that was trained with texture intensity extends off the surface (for clarity visualized in false colors) and becomes smoother. [sent-111, score-1.279]
</p><p>53 of the signed distance function at the center of the ball allows us to compute this feature function also for off-surface points, where the interpretation with respect to the other iso-surfaces does not change. [sent-113, score-0.662]
</p><p>54 Due to the integration, this feature is stable with respect to surface noise, while mean curvature in differential geometry may be affected signiﬁcantly. [sent-114, score-0.742]
</p><p>55 We propose to represent the implicit surface function as in [10] where a compactly supported kernel expansion is trained to approximate the signed distance. [sent-116, score-1.071]
</p><p>56 In this case the integral and the kernel summation can be interchanged, so we only need to evaluate terms of the form BR (x) k(xi , x )dx and then add them in the same way as the signed distance function is computed. [sent-117, score-0.535]
</p><p>57 For the case where the surface looks locally like a sphere it is easy to show that in the limit of small balls the value of the ”signed balls” feature function is related to the differential geometric mean curvature H by Is (x) = 3π H 2 R2 + O(R3 ). [sent-123, score-0.965]
</p><p>58 2  Surface properties — Texture  The volume deformation approach presented in Section 3 requires the use of feature functions deﬁned on the whole domain X . [sent-125, score-0.438]
</p><p>59 In order to include information f |∂Ω which is just given on a surface ∂Ω of the object whose interior volume is Ω, e. [sent-126, score-0.658]
</p><p>60 the texture intensity, we propose to extended the surface feature f |∂Ω into a differentiable feature function f : X → R such that f → f |∂Ω as we get closer to the surface. [sent-128, score-0.964]
</p><p>61 We propose to use a multi-scale compactly supported kernel regression to determine f : at each scale, from coarse to ﬁne, we select approximately equally spaced points on the surface at a distance related to the kernel width of that scale. [sent-132, score-0.867]
</p><p>62 Then we compute the feature value at these points averaged over a sphere of radius of the corresponding kernel support. [sent-133, score-0.337]
</p><p>63 Due to the sub-sampling the kernel regressions do not contain too many kernel centers and the compact support of the kernel ensures sparse kernel matrices. [sent-135, score-0.401]
</p><p>64 In order to optimize (1) we followed the approach of [10]: we represent the deformation τ as a multi-scale compactly supported kernel expansion, i. [sent-139, score-0.308]
</p><p>65 Figure 2: Locations that are marked yellow show an above threshold, relative contribution (see text) of a given feature in the warp-invariant feature function. [sent-144, score-0.465]
</p><p>66 C is the surface intensity feature, B the signed balls feature (R = 6mm), N the surface normals in different directions. [sent-145, score-1.687]
</p><p>67 Note that points where color has a large contribution (yellow points in C) are clustered around regions with characteristic color information, such as the eyes or the mouth. [sent-146, score-0.412]
</p><p>68 As a test object class we used 3D heads with known correspondence [1]. [sent-153, score-0.705]
</p><p>69 100 heads were used for the training object database and 10 to test our correspondence algorithm. [sent-154, score-0.775]
</p><p>70 As a reference head we used the mean head of the database. [sent-155, score-0.334]
</p><p>71 However, the correspondence of the objects in the database is only deﬁned on the surface. [sent-157, score-0.567]
</p><p>72 In order to extend it to the off-surface points xi,s , we generated these locations by ﬁrst sampling points from the surface and then displacing them along their surface normals. [sent-158, score-1.084]
</p><p>73 In one run through the database we computed for each head the values of all proposed basic feature functions for all locations, corresponding to kernel centers on the reference head, as well as for 100 randomly sampled points z. [sent-161, score-0.771]
</p><p>74 Thus, we sampled points up to distances to the surface proportional to the kernel widths used for the deformation τ . [sent-163, score-0.75]
</p><p>75 The parameters Creg — one for each scale — were determined by optimizing computed deformation ﬁelds from the reference head to some of the training database heads. [sent-165, score-0.48]
</p><p>76 We minimized the mismatch to the correspondence given in the database. [sent-166, score-0.408]
</p><p>77 In Figure 1, our new feature functions are visualized on an example head. [sent-168, score-0.273]
</p><p>78 Each feature extracts speciﬁc plausible information, and the surface color can be extended off the surface. [sent-169, score-0.735]
</p><p>79 In Figure 2, we have marked those points on the surface where a given feature has a high relative contribution in the warp-invariant feature function. [sent-171, score-0.95]
</p><p>80 As a measure of contribution we took the component of the weight vector γ(xi,s ) that corresponds to the feature of interest and multiplied it with the standard deviation of this feature over all heads and all positions. [sent-172, score-0.559]
</p><p>81 Note that the weight vector is not invariant to rescaling the basic feature functions, unlike the proposed measure. [sent-173, score-0.259]
</p><p>82 333  Here and below, S is signed distance, N surface normals, C the proposed surface feature function trained with the intensity values on the faces, and B is the ”signed balls” feature with radii given by the percentage numbers scaled to the diameter of the head. [sent-195, score-1.765]
</p><p>83 The signed distance function is the best preserved feature (e. [sent-196, score-0.66]
</p><p>84 all surface points take the value zero up to small approximation errors). [sent-198, score-0.519]
</p><p>85 The resulting large weight of this feature is plausible as a surface-to-surface mapping is a necessary condition for a morph. [sent-199, score-0.265]
</p><p>86 However, combined with Figure 2  Reference  Deformed  Target  Deformed  Target  Figure 3: The average head of the database – the reference – is deformed to match four of the target heads of the test set. [sent-200, score-0.534]
</p><p>87 Correct correspondence deforms the shape of the reference head to the target face with the texture of the mean face well aligned to the shape details. [sent-201, score-0.916]
</p><p>88 We applied our correspondence algorithm to compute the correspondence to the test set of 10 heads. [sent-204, score-0.816]
</p><p>89 We compare our method with the results of the correspondence algorithm of [1] on points that are uniformly drawn form the surface (ﬁrst column) and for 24 selected marker points (second column). [sent-207, score-0.988]
</p><p>90 These markers were placed at locations around the eyes or the mouth where correspondence can be assumed to be better deﬁned than for example on the forehead. [sent-208, score-0.551]
</p><p>91 A careful weighting of each feature separately, but independent of location (b) — as could potentially be achieved by [10] — improves the quality of the correspondence. [sent-233, score-0.251]
</p><p>92 Experiment (g) which is identical to (c) but with the color and signed balls feature omitted demonstrates the usefulness of these additional basic feature functions. [sent-238, score-0.969]
</p><p>93 For large radii R the signed balls feature becomes quite expensive to compute, since many summands of the signed distance function expansion have to be accumulated. [sent-240, score-1.249]
</p><p>94 before the optimization is  Reference  25%  50%  75%  Target  Figure 4: A morph between a human head and the head of the character Gollum (available from www. [sent-243, score-0.347]
</p><p>95 As Gollum’s head falls out of our object class (human heads), we assisted the training procedure with 28 manually placed markers. [sent-246, score-0.278]
</p><p>96 7  Conclusion  We have proposed a new approach to the challenging problem of deﬁning criteria that characterize a valid correspondence between 3D objects of a given class. [sent-248, score-0.567]
</p><p>97 The learning technique has been implemented efﬁciently in a correspondence algorithm for textured surfaces. [sent-251, score-0.408]
</p><p>98 Even though we have concentrated in our experiments on 3D surface data, the method may be applicable also in other ﬁelds such as to align CT or MR scans in medical imaging. [sent-253, score-0.567]
</p><p>99 It would also be intriguing to explore the question whether our paradigm of learning the features characterizing correspondences might reﬂect some of the cognitive processes that are involved when humans learn about similarities within object classes. [sent-254, score-0.312]
</p><p>100 The correlated correspondence algorithm for unsupervised registration of nonrigid surfaces. [sent-310, score-0.476]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('surface', 0.458), ('correspondence', 0.408), ('signed', 0.356), ('feature', 0.191), ('object', 0.169), ('deformation', 0.146), ('balls', 0.146), ('heads', 0.128), ('reference', 0.116), ('faces', 0.111), ('head', 0.109), ('correspondences', 0.097), ('texture', 0.09), ('objects', 0.089), ('morph', 0.085), ('kernel', 0.085), ('surfaces', 0.081), ('characteristic', 0.08), ('cz', 0.078), ('compactly', 0.077), ('dictionary', 0.074), ('steinke', 0.073), ('radii', 0.072), ('database', 0.07), ('shape', 0.07), ('eyes', 0.069), ('distance', 0.067), ('morphs', 0.064), ('br', 0.064), ('centers', 0.061), ('points', 0.061), ('expansion', 0.061), ('weighting', 0.06), ('deformed', 0.058), ('blanz', 0.058), ('zd', 0.058), ('correct', 0.054), ('target', 0.053), ('curvature', 0.052), ('fj', 0.049), ('contribution', 0.049), ('creg', 0.049), ('fid', 0.049), ('gollum', 0.049), ('siegen', 0.049), ('eigenvector', 0.048), ('ball', 0.048), ('optical', 0.048), ('preserved', 0.046), ('color', 0.046), ('locations', 0.046), ('features', 0.046), ('human', 0.044), ('graphics', 0.043), ('fir', 0.043), ('visualized', 0.043), ('criterion', 0.042), ('locally', 0.041), ('differential', 0.041), ('dense', 0.041), ('scans', 0.04), ('medical', 0.04), ('plausible', 0.04), ('criteria', 0.039), ('intensity', 0.039), ('scale', 0.039), ('shapes', 0.039), ('basic', 0.039), ('mesh', 0.039), ('normals', 0.039), ('functions', 0.039), ('establishing', 0.038), ('eigenvectors', 0.036), ('nonrigid', 0.036), ('invariance', 0.036), ('geometric', 0.036), ('propose', 0.034), ('mapping', 0.034), ('ow', 0.034), ('ot', 0.034), ('yellow', 0.034), ('deformations', 0.032), ('registration', 0.032), ('concatenation', 0.032), ('characterize', 0.031), ('properties', 0.031), ('synthesis', 0.031), ('volume', 0.031), ('embedded', 0.03), ('picking', 0.03), ('invariant', 0.029), ('smoothness', 0.029), ('ns', 0.029), ('align', 0.029), ('siggraph', 0.029), ('markers', 0.028), ('cast', 0.028), ('vision', 0.028), ('priori', 0.027), ('integral', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="110-tfidf-1" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>Author: Florian Steinke, Volker Blanz, Bernhard Schölkopf</p><p>Abstract: Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. 1</p><p>2 0.16516204 <a title="110-tfidf-2" href="./nips-2006-Implicit_Surfaces_with_Globally_Regularised_and_Compactly_Supported_Basis_Functions.html">95 nips-2006-Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions</a></p>
<p>Author: Christian Walder, Olivier Chapelle, Bernhard Schölkopf</p><p>Abstract: We consider the problem of constructing a function whose zero set is to represent a surface, given sample points with surface normal vectors. The contributions include a novel means of regularising multi-scale compactly supported basis functions that leads to the desirable properties previously only associated with fully supported bases, and show equivalence to a Gaussian process with modiﬁed covariance function. We also provide a regularisation framework for simpler and more direct treatment of surface normals, along with a corresponding generalisation of the representer theorem. We demonstrate the techniques on 3D problems of up to 14 million data points, as well as 4D time series data. 1</p><p>3 0.10498143 <a title="110-tfidf-3" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>4 0.09931957 <a title="110-tfidf-4" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>Author: Aharon B. Hillel, Daphna Weinshall</p><p>Abstract: We address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. Our approach is motivated by observations from cognitive psychology, which identify parts as the deﬁning component of basic level categories (like motorcycles), while sub-ordinate categories are more often deﬁned by part properties (like ’jagged wheels’). Accordingly, we suggest a two-stage algorithm: First, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). The model is then used to build a class-speciﬁc vector representation for images, where each entry corresponds to a model’s part. In the second stage we train a standard discriminative classiﬁer to classify subclass instances (e.g., cross motorcycles) based on the class-speciﬁc vector representation. We describe extensive experimental results with several subclasses. The proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classiﬁcation is based on a model of the sub-ordinate class. 1</p><p>5 0.098940931 <a title="110-tfidf-5" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>6 0.097109228 <a title="110-tfidf-6" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>7 0.094266444 <a title="110-tfidf-7" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>8 0.094029598 <a title="110-tfidf-8" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>9 0.087950848 <a title="110-tfidf-9" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>10 0.086171679 <a title="110-tfidf-10" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>11 0.083440632 <a title="110-tfidf-11" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>12 0.076899931 <a title="110-tfidf-12" href="./nips-2006-Balanced_Graph_Matching.html">39 nips-2006-Balanced Graph Matching</a></p>
<p>13 0.0750321 <a title="110-tfidf-13" href="./nips-2006-Inducing_Metric_Violations_in_Human_Similarity_Judgements.html">97 nips-2006-Inducing Metric Violations in Human Similarity Judgements</a></p>
<p>14 0.070347168 <a title="110-tfidf-14" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>15 0.069904864 <a title="110-tfidf-15" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>16 0.069664612 <a title="110-tfidf-16" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>17 0.06648694 <a title="110-tfidf-17" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>18 0.065548375 <a title="110-tfidf-18" href="./nips-2006-Using_Combinatorial_Optimization_within_Max-Product_Belief_Propagation.html">201 nips-2006-Using Combinatorial Optimization within Max-Product Belief Propagation</a></p>
<p>19 0.06517294 <a title="110-tfidf-19" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>20 0.06398309 <a title="110-tfidf-20" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.221), (1, 0.046), (2, 0.117), (3, 0.001), (4, 0.039), (5, -0.046), (6, -0.111), (7, -0.066), (8, 0.033), (9, -0.056), (10, 0.028), (11, 0.017), (12, -0.003), (13, -0.082), (14, 0.133), (15, -0.057), (16, -0.056), (17, -0.102), (18, -0.099), (19, 0.034), (20, 0.026), (21, 0.162), (22, 0.038), (23, -0.033), (24, -0.065), (25, -0.038), (26, 0.18), (27, -0.008), (28, 0.059), (29, -0.115), (30, 0.093), (31, -0.11), (32, -0.085), (33, 0.041), (34, -0.063), (35, -0.032), (36, -0.025), (37, -0.044), (38, 0.005), (39, 0.166), (40, 0.086), (41, 0.011), (42, -0.013), (43, -0.048), (44, -0.118), (45, -0.046), (46, -0.045), (47, -0.195), (48, 0.015), (49, -0.146)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95637417 <a title="110-lsi-1" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>Author: Florian Steinke, Volker Blanz, Bernhard Schölkopf</p><p>Abstract: Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. 1</p><p>2 0.71045989 <a title="110-lsi-2" href="./nips-2006-Implicit_Surfaces_with_Globally_Regularised_and_Compactly_Supported_Basis_Functions.html">95 nips-2006-Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions</a></p>
<p>Author: Christian Walder, Olivier Chapelle, Bernhard Schölkopf</p><p>Abstract: We consider the problem of constructing a function whose zero set is to represent a surface, given sample points with surface normal vectors. The contributions include a novel means of regularising multi-scale compactly supported basis functions that leads to the desirable properties previously only associated with fully supported bases, and show equivalence to a Gaussian process with modiﬁed covariance function. We also provide a regularisation framework for simpler and more direct treatment of surface normals, along with a corresponding generalisation of the representer theorem. We demonstrate the techniques on 3D problems of up to 14 million data points, as well as 4D time series data. 1</p><p>3 0.60697019 <a title="110-lsi-3" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>Author: Graham Mcneill, Sethu Vijayakumar</p><p>Abstract: Correspondence algorithms typically struggle with shapes that display part-based variation. We present a probabilistic approach that matches shapes using independent part transformations, where the parts themselves are learnt during matching. Ideas from semi-supervised learning are used to bias the algorithm towards ﬁnding ‘perceptually valid’ part structures. Shapes are represented by unlabeled point sets of arbitrary size and a background component is used to handle occlusion, local dissimilarity and clutter. Thus, unlike many shape matching techniques, our approach can be applied to shapes extracted from real images. Model parameters are estimated using an EM algorithm that alternates between ﬁnding a soft correspondence and computing the optimal part transformations using Procrustes analysis.</p><p>4 0.49419954 <a title="110-lsi-4" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>Author: Aharon B. Hillel, Daphna Weinshall</p><p>Abstract: We address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. Our approach is motivated by observations from cognitive psychology, which identify parts as the deﬁning component of basic level categories (like motorcycles), while sub-ordinate categories are more often deﬁned by part properties (like ’jagged wheels’). Accordingly, we suggest a two-stage algorithm: First, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). The model is then used to build a class-speciﬁc vector representation for images, where each entry corresponds to a model’s part. In the second stage we train a standard discriminative classiﬁer to classify subclass instances (e.g., cross motorcycles) based on the class-speciﬁc vector representation. We describe extensive experimental results with several subclasses. The proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classiﬁcation is based on a model of the sub-ordinate class. 1</p><p>5 0.47269553 <a title="110-lsi-5" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>6 0.47110143 <a title="110-lsi-6" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>7 0.46653596 <a title="110-lsi-7" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>8 0.45438588 <a title="110-lsi-8" href="./nips-2006-Inducing_Metric_Violations_in_Human_Similarity_Judgements.html">97 nips-2006-Inducing Metric Violations in Human Similarity Judgements</a></p>
<p>9 0.44911161 <a title="110-lsi-9" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>10 0.44018412 <a title="110-lsi-10" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>11 0.42991775 <a title="110-lsi-11" href="./nips-2006-A_Humanlike_Predictor_of_Facial_Attractiveness.html">4 nips-2006-A Humanlike Predictor of Facial Attractiveness</a></p>
<p>12 0.42656803 <a title="110-lsi-12" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>13 0.41105032 <a title="110-lsi-13" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>14 0.39551368 <a title="110-lsi-14" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>15 0.38801813 <a title="110-lsi-15" href="./nips-2006-An_Information_Theoretic_Framework_for_Eukaryotic_Gradient_Sensing.html">29 nips-2006-An Information Theoretic Framework for Eukaryotic Gradient Sensing</a></p>
<p>16 0.3755407 <a title="110-lsi-16" href="./nips-2006-Speakers_optimize_information_density_through_syntactic_reduction.html">180 nips-2006-Speakers optimize information density through syntactic reduction</a></p>
<p>17 0.36579853 <a title="110-lsi-17" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>18 0.36482561 <a title="110-lsi-18" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>19 0.36204848 <a title="110-lsi-19" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>20 0.35640121 <a title="110-lsi-20" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.083), (3, 0.038), (5, 0.21), (7, 0.104), (8, 0.012), (9, 0.057), (12, 0.038), (22, 0.059), (44, 0.04), (57, 0.131), (65, 0.066), (69, 0.05), (90, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84447312 <a title="110-lda-1" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>Author: Florian Steinke, Volker Blanz, Bernhard Schölkopf</p><p>Abstract: Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. 1</p><p>2 0.69442976 <a title="110-lda-2" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>3 0.68673825 <a title="110-lda-3" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>Author: Wolf Kienzle, Felix A. Wichmann, Matthias O. Franz, Bernhard Schölkopf</p><p>Abstract: This paper addresses the bottom-up inﬂuence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear ﬁlters, e.g., Gabor or Difference-of-Gaussians ﬁlters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end ﬁlters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justiﬁed. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the ﬁeld that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble ﬁndings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive ﬁelds in the early human visual system. 1</p><p>4 0.68567669 <a title="110-lda-4" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>Author: Christopher J. Burges, Robert Ragno, Quoc V. Le</p><p>Abstract: The quality measures used in information retrieval are particularly difﬁcult to optimize directly, since they depend on the model scores only through the sorted order of the documents returned for a given query. Thus, the derivatives of the cost with respect to the model parameters are either zero, or are undeﬁned. In this paper, we propose a class of simple, ﬂexible algorithms, called LambdaRank, which avoids these difﬁculties by working with implicit cost functions. We describe LambdaRank using neural network models, although the idea applies to any differentiable function class. We give necessary and sufﬁcient conditions for the resulting implicit cost function to be convex, and we show that the general method has a simple mechanical interpretation. We demonstrate signiﬁcantly improved accuracy, over a state-of-the-art ranking algorithm, on several datasets. We also show that LambdaRank provides a method for signiﬁcantly speeding up the training phase of that ranking algorithm. Although this paper is directed towards ranking, the proposed method can be extended to any non-smooth and multivariate cost functions. 1</p><p>5 0.68408805 <a title="110-lda-5" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>Author: Boaz Nadler, Meirav Galun</p><p>Abstract: Spectral clustering methods are common graph-based approaches to clustering of data. Spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. One contribution of this paper is to present fundamental limitations of this general local to global approach. We show that based only on local information, the normalized cut functional is not a suitable measure for the quality of clustering. Further, even with a suitable similarity measure, we show that the ﬁrst few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. Based on these ﬁndings, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. Our measure can be used in conjunction with any bottom-up graph-based clustering method, it is scale-free and can determine coherent clusters at all scales. We present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail. In contrast, using this coherence measure ﬁnds the expected clusters at all scales. Keywords: Clustering, kernels, learning theory. 1</p><p>6 0.67688298 <a title="110-lda-6" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>7 0.67269546 <a title="110-lda-7" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>8 0.67268747 <a title="110-lda-8" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>9 0.67226547 <a title="110-lda-9" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>10 0.67011577 <a title="110-lda-10" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>11 0.66944277 <a title="110-lda-11" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>12 0.66928852 <a title="110-lda-12" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>13 0.66880065 <a title="110-lda-13" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>14 0.66847992 <a title="110-lda-14" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>15 0.66787636 <a title="110-lda-15" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>16 0.66686219 <a title="110-lda-16" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>17 0.6666379 <a title="110-lda-17" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>18 0.66647166 <a title="110-lda-18" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>19 0.66184223 <a title="110-lda-19" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>20 0.66182071 <a title="110-lda-20" href="./nips-2006-Balanced_Graph_Matching.html">39 nips-2006-Balanced Graph Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
