<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2006-Learning Structural Equation Models for fMRI</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-113" href="#">nips2006-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2006-Learning Structural Equation Models for fMRI</h1>
<br/><p>Source: <a title="nips-2006-113-pdf" href="http://papers.nips.cc/paper/3022-learning-structural-equation-models-for-fmri.pdf">pdf</a></p><p>Author: Enrico Simonotto, Heather Whalley, Stephen Lawrie, Lawrence Murray, David Mcgonigle, Amos J. Storkey</p><p>Abstract: Structural equation models can be seen as an extension of Gaussian belief networks to cyclic graphs, and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of Gaussian dynamic belief networks. Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. First we demonstrate structure learning on a toy problem. We then show that for particular fMRI data the simple models usually assumed are not supported. We show that is is possible to learn sensible structural equation models that can provide modelling beneﬁts, but that are not necessarily going to be the same as a true causal model, and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more beneﬁts than learning structural equations alone. 1</p><p>Reference: <a title="nips-2006-113-reference" href="../nips2006_reference/nips-2006-Learning_Structural_Equation_Models_for_fMRI_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. [sent-3, score-0.687]
</p><p>2 In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. [sent-4, score-0.68]
</p><p>3 1  Introduction  Structural equation modelling (SEM) is a technique widely used in the behavioural sciences. [sent-8, score-0.213]
</p><p>4 It has also appeared as a standard approach for analysis of what has become known as effective connectivity in the functional magnetic resonance imaging (fMRI) literature and is still in common use despite the increasing interest in dynamical methods such as dynamic causal models [6]. [sent-9, score-0.999]
</p><p>5 Simply put, effective connectivity analysis involves looking at the possible causal inﬂuences between brain regions given measurements of the activity of those regions. [sent-10, score-0.904]
</p><p>6 Structural equation models are a Gaussian modelling tool, and are similar to Gaussian belief networks. [sent-11, score-0.367]
</p><p>7 In fact Gaussian belief networks can be seen as a subset of valid structural equation models. [sent-12, score-0.569]
</p><p>8 However structural equation models do not have the same acyclicity constraints as belief networks. [sent-13, score-0.692]
</p><p>9 It should be noted that the graphical form used in this paper is at odds with traditional SEM representations, and consistent with that used for belief networks, as those will be more familiar to the expected audience. [sent-14, score-0.142]
</p><p>10 Within the fMRI context, the use of structural equation generally takes the following form. [sent-15, score-0.465]
</p><p>11 First certain regions of interests (commonly called seeds) are chosen according to some understanding of what brain regions might be of interest or of importance. [sent-16, score-0.285]
</p><p>12 Then neurobiological knowledge is used to propose a connectivity model. [sent-17, score-0.466]
</p><p>13 This connectivity model states what regions are connected to what other regions, and the direction of the connectivity. [sent-18, score-0.593]
</p><p>14 This connectivity model is used to deﬁne a structural equation model. [sent-19, score-0.931]
</p><p>15 In this paper we consider what can be done when it is hard to specify connectivity a priori, and ask how much we can achieve by learning network structures from the fMRI data itself. [sent-21, score-0.582]
</p><p>16 The novel developments of this paper include the examination of various generative representations for structural equation models which allow straightforward comparisons with belief networks and other models such as dynamic causal models. [sent-22, score-1.045]
</p><p>17 We implement Bayesian Information Criterion approximations to the evidence and use this in a Metropolis-Hastings sampling scheme for learning structural equation models. [sent-23, score-0.465]
</p><p>18 These models are then applied to toy data, and to fMRI data, which allows the examination of the types of assumptions typically made. [sent-24, score-0.206]
</p><p>19 1 Related Work: Structural Equation Models Structural equation models and path analysis have a long history. [sent-26, score-0.269]
</p><p>20 Linear Gaussian structural equation models can be split into the case of path analysis [20], where the all the variables are directly measurable and structural equation models with latent variables [1], where latent variable models are allowed. [sent-29, score-1.475]
</p><p>21 Furthermore structural equation models can also be characterised by the inclusion of exogenous inﬂuences. [sent-31, score-0.55]
</p><p>22 Structural equation models have been analysed and understood in Bayesian terms before. [sent-32, score-0.215]
</p><p>23 They form a part of the causal modelling framework of Pearl [11], and have been discussed within that context, as well as a number of others [11, 4, 13, 10]. [sent-33, score-0.225]
</p><p>24 Approaches to learning structural equation models have not played a signiﬁcant part in fMRI methods. [sent-34, score-0.55]
</p><p>25 For dynamic causal models (rather than structural equation models) the issue of model comparison was dealt with in [12], but large scale structure learning was not considered. [sent-37, score-0.825]
</p><p>26 In fMRI literature, SEMs have generally been used to model ‘effective connectivity’, or rather modelling the causal relationships between different brain regions. [sent-38, score-0.334]
</p><p>27 In fact it seems SEMs have been the most widely used model for connectivity analyses in neuroimaging. [sent-41, score-0.466]
</p><p>28 In all of the studies cited above the underlying structure was presumed known or presumed to be one of a small number of possibilities. [sent-42, score-0.207]
</p><p>29 The presumption in much fMRI connectivity analysis is that we can obtain models for activity dependence from neuro-anatomical sources. [sent-45, score-0.606]
</p><p>30 The problem with this is that it fails to account for the fact that connectivity analysis is usually done with a limited number of regions. [sent-46, score-0.466]
</p><p>31 Furthermore, just because regions are physically connected does not mean there is any actual functional inﬂuence in a particular context. [sent-49, score-0.206]
</p><p>32 Hence it has to be accepted that neuro-anatomically derived connectivity is a ﬁrst guess at best. [sent-50, score-0.466]
</p><p>33 It is not the purpose of this paper to propose that anatomical connectivity be ignored, but instead it asks what happens if we go to the other extreme: can we say something about connectivity from the data? [sent-51, score-1.04]
</p><p>34 In reality anatomical connectivity models are needed, and can be used to provide good priors for the connections and even for the relative connection strengths. [sent-52, score-0.93]
</p><p>35 Statistically there are huge equivalences in structural equation models that will not be determined by the data alone. [sent-53, score-0.592]
</p><p>36 3  Understanding Structural Equation Models  In this section two generative views of structural equation modelling are presented. [sent-54, score-0.582]
</p><p>37 The idea behind structural equation modelling is that it represents causal dependence between different variables. [sent-55, score-0.69]
</p><p>38 The fact that cyclic structures are allowed in structural equation models could be seen as an implicit assumption of some underlying dynamic which the structural equation model is an equilibrium rep-  resentation of. [sent-56, score-1.284]
</p><p>39 Indeed that is commonly how effective connectivity models are interpreted in an fMRI context. [sent-57, score-0.595]
</p><p>40 Two linear models, both of which produce a structural equation model prior, are presented here. [sent-58, score-0.465]
</p><p>41 1 The Traditional Model The standard SEM view is that the core SEM structure is a covariance produced by the solution to a set of linear equations x = Ax + ω with Gaussian term ω. [sent-61, score-0.132]
</p><p>42 This does not have any direct generative elucidation, but can instead be thought of as relating to a deterministic dynamical system subject to uncertain ﬁxed input. [sent-62, score-0.14]
</p><p>43 Suppose we have a dynamical system xt+1 = Axt + ω, subject to some input ω, where we presume the system input is unknown and Gaussian distributed. [sent-63, score-0.169]
</p><p>44 To generate from the model, we sample ω, run the dynamical system to its ﬁxed point, and use that ﬁxed point as a sample of x. [sent-64, score-0.145]
</p><p>45 This ﬁxed point is given by x = (I − A)−1 ω which produces the standard SEM covariance structure for x. [sent-65, score-0.132]
</p><p>46 2 Average Activity Of A Gaussian Dynamic Bayesian Network An alternative and potentially appealing view is that the the SEM represents the distribution of the long term activity of the nodes in a Gaussian dynamic Bayesian network (Kalman ﬁlter). [sent-69, score-0.174]
</p><p>47 This deﬁnes a Markov chain, and is the evolution equation of a Gaussian dynamic Bayesian network. [sent-74, score-0.209]
</p><p>48 This interpretation says that if we have some latent system running as a Gaussian dynamic Bayesian network, but our measuring equipment is only capable of capturing longer term averages of the network activity then our measurements are distributed according to an SEM. [sent-80, score-0.258]
</p><p>49 By formulating the generative framework we see it is important to restrict the form of connectivity model in this way. [sent-83, score-0.5]
</p><p>50 (2)  where Kω is the covariance of ω, Kσ the covariance of σ etc. [sent-87, score-0.156]
</p><p>51 The next simplest case would involve presuming once again that there are no inputs but that in fact the observations are stochastic functions of the latent variables. [sent-92, score-0.147]
</p><p>52 In this section we provide prior distributions for the parameters of the structural equation model. [sent-106, score-0.503]
</p><p>53 For the purposes of this paper we take Aij = 0, presume we have no particular a priori bias towards positive or negative connections and a uniform prior over structures. [sent-108, score-0.274]
</p><p>54 An independent prior over connections seems reasonable as two separate connections between different brain regions would have no a priori reason to be related. [sent-109, score-0.573]
</p><p>55 We can calculate all the relevant derivatives for the SEM straightforwardly, and adapt the parameters to maximize the posterior of the structural equation model. [sent-117, score-0.567]
</p><p>56 This will enable us to sample from an approximate posterior distribution of structures to ﬁnd a sample which best represents the data. [sent-121, score-0.272]
</p><p>57 6  Sampling From SEMs  In order to represent the posterior distribution over network structures, we resort to a sampling approach. [sent-122, score-0.142]
</p><p>58 Because there are no acyclicity constraints, MCMC proposals are simpler than the comparable situation for belief networks in that no acyclicity checking needs to be done for the proposals. [sent-123, score-0.25]
</p><p>59 We choose highly sparse swap matrices, but to reduce the possibility of transitioning randomly about the larger graphs, without ever considering smaller networks we introduce a bias towards removing connections rather than adding connections in generating the swap matrix. [sent-125, score-0.401]
</p><p>60 22 0       This connectivity matrix is represented graphically in Figure 11. [sent-151, score-0.466]
</p><p>61 15 of the cases, (c) the highest posterior structure from the sample (d) a random sample. [sent-165, score-0.203]
</p><p>62 The graphs for the maximum posterior sample and a random sample are shown in Figure 1. [sent-166, score-0.196]
</p><p>63 We can conclude from this that we can gain some information from learning SEM structures, but as with learning any graphical models there are many symmetries and equivalences, so it is vital not to infer too much from the learnt structures. [sent-169, score-0.168]
</p><p>64 We used the auditory paced ﬁnger-tapping task; brieﬂy, a single subject tapped his right index ﬁnger, paced by an auditory tone (1. [sent-172, score-0.288]
</p><p>65 5mm resolution) was acquired to facilitate anatomical localisation of the functional data. [sent-183, score-0.242]
</p><p>66 After removal of the ﬁrst two volumes to account for T1 saturation effects, cerebral volumes were realigned to correct for both within- and between-session subject motion). [sent-190, score-0.24]
</p><p>67 For comparison with previous extant work, the most signiﬁcant voxel in each cluster was chosen as a seed, giving 13 seeds representing 13 separate anatomical regions. [sent-195, score-0.286]
</p><p>68 When it was obvious that a given cluster encompassed more than one distinct anatomical region, seeds were also selected for other regions covered by the cluster. [sent-196, score-0.323]
</p><p>69 A high resolution structural scan was acquired using a 3D T1-weighted sequence (TI = 600 ms). [sent-201, score-0.39]
</p><p>70 For an effective connectivity analysis, a number of brain regions (seeds) were chosen on the basis of the results of a functional connectivity study [19] and taking regard of areas which may be of particular clinical interest. [sent-212, score-1.252]
</p><p>71 Hence we are interested in learning a 28 by 28 connectivity matrix. [sent-214, score-0.466]
</p><p>72 The stability of the log posterior along with estimations of cross-correlation against lag were used as heuristics to determine convergence prior to obtaining 10000 sample points. [sent-217, score-0.187]
</p><p>73 [15] for a Schizophrenia studies), we found that samples from the posterior of this model were in fact so highly connected that displaying them would infeasible. [sent-220, score-0.179]
</p><p>74 For D2 a connectivity of 350 of the 752 total possible connections was typical. [sent-221, score-0.595]
</p><p>75 We can generalise the path analysis model by making the region activities latent variables, and allow the measurement variables to be noisy versions of those regions. [sent-224, score-0.205]
</p><p>76 For dataset D1, we sample posterior structures given the training data with T = 100. [sent-228, score-0.225]
</p><p>77 In addition an a priori connectivity  (a)  (b)  Figure 2: Structure for (a) the hand speciﬁed model (b) the highest posterior sample. [sent-230, score-0.612]
</p><p>78 (a)  (b)  (c)  Figure 3: Graphical structure for (a) the highest posterior structure from the sample (b) random sample. [sent-231, score-0.257]
</p><p>79 structure is proposed for the regions in the study, taking into account the task involved. [sent-235, score-0.142]
</p><p>80 This was obtained by using knowledge of neuroanatomical connectivity drawn from studies using tract-tracing in non-human primates. [sent-236, score-0.503]
</p><p>81 It was produced independent of the connectivity analysis and without knowledge of its results, but taking into account the seed locations and their corresponding activities. [sent-237, score-0.524]
</p><p>82 Both these models perform better than other random models with equivalent numbers of connections. [sent-249, score-0.17]
</p><p>83 In reality learnt models are going to be used in new situations and situations with less data. [sent-250, score-0.209]
</p><p>84 By estimating the model parameters on 100 data points, instead of 2000, we ﬁnd that the learnt model performs very slightly better than the hand speciﬁed model (log odds ratio of 63 on a 574 point test set), and both perform better than the full covariance (log odds of 292). [sent-252, score-0.307]
</p><p>85 Maximum posterior samples and a random sample are illustrated in Figure 3. [sent-255, score-0.187]
</p><p>86 Even so this is signiﬁcantly greater than the idealised connectivity structures typically used in most studies. [sent-257, score-0.542]
</p><p>87 One further approach is to assume a fully connected structure, but where the connectivity is in two categories. [sent-258, score-0.505]
</p><p>88 We have priors on connectivity with the same values of Tij as before for the strong connections and much larger values for the weaker connections. [sent-259, score-0.661]
</p><p>89 Following this procedure we ﬁnd that models of the form of 3c are typical samples from the posterior where only the larger  connections are shown. [sent-261, score-0.354]
</p><p>90 Again connections such as those between the Cuneus/Precuneus and the Superior Frontal Gyrus, the Thalamic connections, and some of the cross-hemispheric connections are amongst those that would be expected. [sent-262, score-0.258]
</p><p>91 This approach is related to recent work on the use of sparse priors for effective connectivity [18]. [sent-263, score-0.576]
</p><p>92 9  Future Directions  This work demonstrates that if we learn structural equation models from data, we ﬁnd there is little evidence for the simple forms of path analysis model which is in common use in the fMRI literature. [sent-264, score-0.604]
</p><p>93 We suggest that learning connectivity can be a reasonable complement to current procedures where prior speciﬁcation is hard. [sent-265, score-0.54]
</p><p>94 It should be expected that combining learnt structures with prior anatomical models will help in the speciﬁcation of more accurate connectivity assumptions, as it will reduce the number of equivalence and focus on more reasonable structural forms. [sent-268, score-1.227]
</p><p>95 Furthermore future comparisons can be made using a sample of reasonable models instead of a single a priori chosen model. [sent-269, score-0.212]
</p><p>96 We would also expect that the major gains in learning models with come from the focus on dynamical networks which do not suffer from speciﬁcity problems. [sent-270, score-0.171]
</p><p>97 The preedictive value of changes in effective connectivity for human learning. [sent-283, score-0.51]
</p><p>98 Attentional modulation of effective connectivity from V2 to V5/MT in humans. [sent-303, score-0.51]
</p><p>99 Structural equation modelling and its application to network analysis in functional brain imaging. [sent-331, score-0.441]
</p><p>100 Altered effective connectivity during working memory performance in schizophrenia: a study with fMRI and structural equation modeling. [sent-375, score-0.975]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('connectivity', 0.466), ('structural', 0.335), ('fmri', 0.299), ('sem', 0.233), ('gyrus', 0.148), ('causal', 0.142), ('equation', 0.13), ('connections', 0.129), ('seeds', 0.127), ('edinburgh', 0.113), ('brain', 0.109), ('anatomical', 0.108), ('posterior', 0.102), ('frontal', 0.102), ('neuroimage', 0.089), ('regions', 0.088), ('models', 0.085), ('sems', 0.084), ('latent', 0.084), ('modelling', 0.083), ('learnt', 0.083), ('functional', 0.079), ('dynamic', 0.079), ('covariance', 0.078), ('structures', 0.076), ('acyclicity', 0.073), ('odds', 0.073), ('psychiatry', 0.073), ('toy', 0.07), ('belief', 0.069), ('friston', 0.068), ('volumes', 0.068), ('priors', 0.066), ('schizophrenia', 0.063), ('presume', 0.063), ('presuming', 0.063), ('cyclic', 0.062), ('contraction', 0.058), ('seed', 0.058), ('presumed', 0.058), ('activity', 0.055), ('acquired', 0.055), ('subject', 0.055), ('path', 0.054), ('swap', 0.054), ('structure', 0.054), ('imaging', 0.053), ('aij', 0.052), ('equilibrium', 0.052), ('examination', 0.051), ('voxel', 0.051), ('dynamical', 0.051), ('epi', 0.049), ('frackowiak', 0.049), ('intersession', 0.049), ('mcgonigle', 0.049), ('paced', 0.049), ('realigned', 0.049), ('simonotto', 0.049), ('whalley', 0.049), ('auditory', 0.048), ('sample', 0.047), ('priori', 0.044), ('effective', 0.044), ('parietal', 0.042), ('equivalences', 0.042), ('axt', 0.042), ('stephan', 0.042), ('movement', 0.041), ('reality', 0.041), ('effects', 0.041), ('subjects', 0.041), ('network', 0.04), ('xt', 0.04), ('connected', 0.039), ('bayesian', 0.039), ('epoch', 0.039), ('nger', 0.039), ('tone', 0.039), ('regularisation', 0.039), ('prior', 0.038), ('samples', 0.038), ('studies', 0.037), ('scotland', 0.036), ('mh', 0.036), ('siemens', 0.036), ('reasonable', 0.036), ('included', 0.035), ('networks', 0.035), ('connection', 0.035), ('division', 0.035), ('gaussian', 0.035), ('generative', 0.034), ('convolved', 0.034), ('inferior', 0.034), ('centre', 0.034), ('variables', 0.034), ('region', 0.033), ('bs', 0.032), ('temporal', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="113-tfidf-1" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>Author: Enrico Simonotto, Heather Whalley, Stephen Lawrie, Lawrence Murray, David Mcgonigle, Amos J. Storkey</p><p>Abstract: Structural equation models can be seen as an extension of Gaussian belief networks to cyclic graphs, and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of Gaussian dynamic belief networks. Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. First we demonstrate structure learning on a toy problem. We then show that for particular fMRI data the simple models usually assumed are not supported. We show that is is possible to learn sensible structural equation models that can provide modelling beneﬁts, but that are not necessarily going to be the same as a true causal model, and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more beneﬁts than learning structural equations alone. 1</p><p>2 0.15928473 <a title="113-tfidf-2" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>3 0.11758434 <a title="113-tfidf-3" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>Author: Seyoung Kim, Padhraic Smyth</p><p>Abstract: Data sets involving multiple groups with shared characteristics frequently arise in practice. In this paper we extend hierarchical Dirichlet processes to model such data. Each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportions and the component parameters. Variabilities in mixing proportions across groups are handled using hierarchical Dirichlet processes, also allowing for automatic determination of the number of components. In addition, each group is allowed to have its own component parameters coming from a prior described by a template mixture model. This group-level variability in the component parameters is handled using a random effects model. We present a Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate model parameters and demonstrate the method by applying it to the problem of modeling spatial brain activation patterns across multiple images collected via functional magnetic resonance imaging (fMRI). 1</p><p>4 0.10321876 <a title="113-tfidf-4" href="./nips-2006-Neurophysiological_Evidence_of_Cooperative_Mechanisms_for_Stereo_Computation.html">145 nips-2006-Neurophysiological Evidence of Cooperative Mechanisms for Stereo Computation</a></p>
<p>Author: Jason M. Samonds, Brian R. Potetz, Tai S. Lee</p><p>Abstract: Although there has been substantial progress in understanding the neurophysiological mechanisms of stereopsis, how neurons interact in a network during stereo computation remains unclear. Computational models on stereopsis suggest local competition and long-range cooperation are important for resolving ambiguity during stereo matching. To test these predictions, we simultaneously recorded from multiple neurons in V1 of awake, behaving macaques while presenting surfaces of different depths rendered in dynamic random dot stereograms. We found that the interaction between pairs of neurons was a function of similarity in receptive fields, as well as of the input stimulus. Neurons coding the same depth experienced common inhibition early in their responses for stimuli presented at their nonpreferred disparities. They experienced mutual facilitation later in their responses for stimulation at their preferred disparity. These findings are consistent with a local competition mechanism that first removes gross mismatches, and a global cooperative mechanism that further refines depth estimates. 1 In trod u ction The human visual system is able to extract three-dimensional (3D) structures in random noise stereograms even when such images evoke no perceptible patterns when viewed monocularly [1]. Bela Julesz proposed that this is accomplished by a stereopsis mechanism that detects correlated shifts in 2D noise patterns between the two eyes. He also suggested that this mechanism likely involves cooperative neural processing early in the visual system. Marr and Poggio formalized the computational constraints for solving stereo matching (Fig. 1a) and devised an algorithm that can discover the underlying 3D structures in a variety of random dot stereogram patterns [2]. Their algorithm was based on two rules: (1) each element or feature is unique (i.e., can be assigned only one disparity) and (2) surfaces of objects are cohesive (i.e., depth changes gradually across space). To describe their algorithm in neurophysiological terms, we can consider neurons in primary visual cortex as simple element or feature detectors. The first rule is implemented by introducing competitive interactions (mutual inhibition) among neurons of different disparity tuning at each location (Fig. 1b, blue solid horizontal or vertical lines), allowing only one disparity to be detected at each location. The second rule is implemented by introducing cooperative interactions (mutual facilitation) among neurons tuned to the same depth (image disparity) across different spatial locations (Fig. 1b, along the red dashed diagonal lines). In other words, a disparity estimate at one location is more likely to be correct if neighboring locations have similar disparity estimates. A dynamic system under such constraints can relax to a stable global disparity map. Here, we present neurophysiological evidence of interactions between disparity-tuned neurons in the primary visual cortex that is consistent with this general approach. We sampled from a variety of spatially distributed disparity tuned neurons (see electrodes Fig. 1b) while displaying DRDS stimuli defined at various disparities (see stimulus Fig.1b). We then measured the dynamics of interactions by assessing the temporal evolution of correlation in neural responses. a Left Image b Right Image Electrodes Disparity Left Image ? Stimulus Right Image Figure 1: (a) Left and right images of random dot stereogram (right image has been shifted to the right). (b) 1D graphical depiction of competition (blue solid lines) and cooperation (red dashed lines) among disparity-tuned neurons with respect to space as defined by Marr and Poggio’s stereo algorithm [2]. 2 2.1 Methods Recording and stimulation a Posterior - Anterior Recordings were made in V1 of two awake, behaving macaques. We simultaneously recorded from 4-8 electrodes providing data from up to 10 neurons in a single recording session (some electrodes recorded from as many as 3 neurons). We collected data from 112 neurons that provided 224 pairs for cross-correlation analysis. For stimuli, we used 12 Hz dynamic random dot stereograms (DRDS; 25% density black and white pixels on a mean luminance background) presented in a 3.5-degree aperture. Liquid crystal shutter goggles were used to present random dot patterns to each eye separately. Eleven horizontal disparities between the two eyes, ranging from ±0.9 degrees, were tested. Seventy-four neurons (66%) had significant disparity tuning and 99 pairs (44%) were comprised of neurons that both had significant disparity tuning (1-way ANOVA, p<0.05). b 5mm Medial - Lateral 100µV 0.2ms 1° Figure 2: (a) Example recording session from five electrodes in V1. (b) Receptive field (white box—arrow represents direction preference) and random dot stereogram locations for same recording session (small red square is the fixation spot). 2.2 Data analysis Interaction between neurons was described as</p><p>5 0.10305782 <a title="113-tfidf-5" href="./nips-2006-Combining_causal_and_similarity-based_reasoning.html">53 nips-2006-Combining causal and similarity-based reasoning</a></p>
<p>Author: Charles Kemp, Patrick Shafto, Allison Berke, Joshua B. Tenenbaum</p><p>Abstract: Everyday inductive reasoning draws on many kinds of knowledge, including knowledge about relationships between properties and knowledge about relationships between objects. Previous accounts of inductive reasoning generally focus on just one kind of knowledge: models of causal reasoning often focus on relationships between properties, and models of similarity-based reasoning often focus on similarity relationships between objects. We present a Bayesian model of inductive reasoning that incorporates both kinds of knowledge, and show that it accounts well for human inferences about the properties of biological species. 1</p><p>6 0.099842168 <a title="113-tfidf-6" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<p>7 0.095456287 <a title="113-tfidf-7" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>8 0.085337773 <a title="113-tfidf-8" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>9 0.080304556 <a title="113-tfidf-9" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>10 0.076179236 <a title="113-tfidf-10" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>11 0.075959757 <a title="113-tfidf-11" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>12 0.074583456 <a title="113-tfidf-12" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>13 0.070497133 <a title="113-tfidf-13" href="./nips-2006-Inferring_Network_Structure_from_Co-Occurrences.html">98 nips-2006-Inferring Network Structure from Co-Occurrences</a></p>
<p>14 0.070439242 <a title="113-tfidf-14" href="./nips-2006-Modeling_Dyadic_Data_with_Binary_Latent_Factors.html">132 nips-2006-Modeling Dyadic Data with Binary Latent Factors</a></p>
<p>15 0.070305459 <a title="113-tfidf-15" href="./nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</a></p>
<p>16 0.065588534 <a title="113-tfidf-16" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>17 0.064961791 <a title="113-tfidf-17" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>18 0.064836442 <a title="113-tfidf-18" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>19 0.064371526 <a title="113-tfidf-19" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>20 0.064130239 <a title="113-tfidf-20" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, -0.063), (2, 0.075), (3, -0.088), (4, -0.022), (5, -0.053), (6, 0.145), (7, 0.019), (8, -0.065), (9, -0.04), (10, 0.029), (11, 0.022), (12, -0.024), (13, 0.01), (14, -0.18), (15, -0.086), (16, 0.086), (17, -0.105), (18, -0.108), (19, 0.079), (20, -0.042), (21, 0.037), (22, 0.079), (23, 0.075), (24, 0.035), (25, -0.102), (26, 0.089), (27, -0.081), (28, 0.074), (29, -0.094), (30, -0.033), (31, 0.034), (32, -0.095), (33, -0.052), (34, -0.071), (35, 0.013), (36, 0.107), (37, 0.042), (38, -0.029), (39, -0.065), (40, -0.132), (41, 0.016), (42, -0.102), (43, 0.057), (44, -0.063), (45, 0.111), (46, -0.029), (47, -0.014), (48, 0.073), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94268435 <a title="113-lsi-1" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>Author: Enrico Simonotto, Heather Whalley, Stephen Lawrie, Lawrence Murray, David Mcgonigle, Amos J. Storkey</p><p>Abstract: Structural equation models can be seen as an extension of Gaussian belief networks to cyclic graphs, and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of Gaussian dynamic belief networks. Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. First we demonstrate structure learning on a toy problem. We then show that for particular fMRI data the simple models usually assumed are not supported. We show that is is possible to learn sensible structural equation models that can provide modelling beneﬁts, but that are not necessarily going to be the same as a true causal model, and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more beneﬁts than learning structural equations alone. 1</p><p>2 0.72557962 <a title="113-lsi-2" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>3 0.58689034 <a title="113-lsi-3" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>Author: Seyoung Kim, Padhraic Smyth</p><p>Abstract: Data sets involving multiple groups with shared characteristics frequently arise in practice. In this paper we extend hierarchical Dirichlet processes to model such data. Each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportions and the component parameters. Variabilities in mixing proportions across groups are handled using hierarchical Dirichlet processes, also allowing for automatic determination of the number of components. In addition, each group is allowed to have its own component parameters coming from a prior described by a template mixture model. This group-level variability in the component parameters is handled using a random effects model. We present a Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate model parameters and demonstrate the method by applying it to the problem of modeling spatial brain activation patterns across multiple images collected via functional magnetic resonance imaging (fMRI). 1</p><p>4 0.53856164 <a title="113-lsi-4" href="./nips-2006-A_Probabilistic_Algorithm_Integrating_Source_Localization_and_Noise_Suppression_of_MEG_and_EEG_data.html">12 nips-2006-A Probabilistic Algorithm Integrating Source Localization and Noise Suppression of MEG and EEG data</a></p>
<p>Author: Johanna M. Zumer, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan</p><p>Abstract: We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efﬁcient than traditional multidipole ﬁtting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or ﬁxed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated. 1</p><p>5 0.50985354 <a title="113-lsi-5" href="./nips-2006-Combining_causal_and_similarity-based_reasoning.html">53 nips-2006-Combining causal and similarity-based reasoning</a></p>
<p>Author: Charles Kemp, Patrick Shafto, Allison Berke, Joshua B. Tenenbaum</p><p>Abstract: Everyday inductive reasoning draws on many kinds of knowledge, including knowledge about relationships between properties and knowledge about relationships between objects. Previous accounts of inductive reasoning generally focus on just one kind of knowledge: models of causal reasoning often focus on relationships between properties, and models of similarity-based reasoning often focus on similarity relationships between objects. We present a Bayesian model of inductive reasoning that incorporates both kinds of knowledge, and show that it accounts well for human inferences about the properties of biological species. 1</p><p>6 0.48023808 <a title="113-lsi-6" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>7 0.43493462 <a title="113-lsi-7" href="./nips-2006-A_Bayesian_Approach_to_Diffusion_Models_of_Decision-Making_and_Response_Time.html">1 nips-2006-A Bayesian Approach to Diffusion Models of Decision-Making and Response Time</a></p>
<p>8 0.43482387 <a title="113-lsi-8" href="./nips-2006-Hidden_Markov_Dirichlet_Process%3A_Modeling_Genetic_Recombination_in_Open_Ancestral_Space.html">90 nips-2006-Hidden Markov Dirichlet Process: Modeling Genetic Recombination in Open Ancestral Space</a></p>
<p>9 0.41976151 <a title="113-lsi-9" href="./nips-2006-Learning_Time-Intensity_Profiles_of_Human_Activity_using_Non-Parametric_Bayesian_Models.html">114 nips-2006-Learning Time-Intensity Profiles of Human Activity using Non-Parametric Bayesian Models</a></p>
<p>10 0.41464013 <a title="113-lsi-10" href="./nips-2006-Theory_and_Dynamics_of_Perceptual_Bistability.html">192 nips-2006-Theory and Dynamics of Perceptual Bistability</a></p>
<p>11 0.41423786 <a title="113-lsi-11" href="./nips-2006-Inferring_Network_Structure_from_Co-Occurrences.html">98 nips-2006-Inferring Network Structure from Co-Occurrences</a></p>
<p>12 0.41279247 <a title="113-lsi-12" href="./nips-2006-Temporal_dynamics_of_information_content_carried_by_neurons_in_the_primary_visual_cortex.html">189 nips-2006-Temporal dynamics of information content carried by neurons in the primary visual cortex</a></p>
<p>13 0.40738702 <a title="113-lsi-13" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<p>14 0.40598777 <a title="113-lsi-14" href="./nips-2006-Neurophysiological_Evidence_of_Cooperative_Mechanisms_for_Stereo_Computation.html">145 nips-2006-Neurophysiological Evidence of Cooperative Mechanisms for Stereo Computation</a></p>
<p>15 0.40415737 <a title="113-lsi-15" href="./nips-2006-Mixture_Regression_for_Covariate_Shift.html">131 nips-2006-Mixture Regression for Covariate Shift</a></p>
<p>16 0.36779791 <a title="113-lsi-16" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>17 0.34151527 <a title="113-lsi-17" href="./nips-2006-Relational_Learning_with_Gaussian_Processes.html">169 nips-2006-Relational Learning with Gaussian Processes</a></p>
<p>18 0.34020033 <a title="113-lsi-18" href="./nips-2006-Modeling_Dyadic_Data_with_Binary_Latent_Factors.html">132 nips-2006-Modeling Dyadic Data with Binary Latent Factors</a></p>
<p>19 0.33812642 <a title="113-lsi-19" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>20 0.33260271 <a title="113-lsi-20" href="./nips-2006-Context_dependent_amplification_of_both_rate_and_event-correlation_in_a_VLSI_network_of_spiking_neurons.html">59 nips-2006-Context dependent amplification of both rate and event-correlation in a VLSI network of spiking neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.078), (3, 0.024), (7, 0.077), (9, 0.046), (20, 0.043), (22, 0.066), (34, 0.026), (44, 0.057), (55, 0.31), (57, 0.076), (65, 0.032), (69, 0.031), (71, 0.033), (90, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92744297 <a title="113-lda-1" href="./nips-2006-TrueSkill%E2%84%A2%3A_A_Bayesian_Skill_Rating_System.html">196 nips-2006-TrueSkill™: A Bayesian Skill Rating System</a></p>
<p>Author: Ralf Herbrich, Tom Minka, Thore Graepel</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.77016771 <a title="113-lda-2" href="./nips-2006-Learning_Structural_Equation_Models_for_fMRI.html">113 nips-2006-Learning Structural Equation Models for fMRI</a></p>
<p>Author: Enrico Simonotto, Heather Whalley, Stephen Lawrie, Lawrence Murray, David Mcgonigle, Amos J. Storkey</p><p>Abstract: Structural equation models can be seen as an extension of Gaussian belief networks to cyclic graphs, and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of Gaussian dynamic belief networks. Most use of structural equation models in fMRI involves postulating a particular structure and comparing learnt parameters across different groups. In this paper it is argued that there are situations where priors about structure are not ﬁrm or exhaustive, and given sufﬁcient data, it is worth investigating learning network structure as part of the approach to connectivity analysis. First we demonstrate structure learning on a toy problem. We then show that for particular fMRI data the simple models usually assumed are not supported. We show that is is possible to learn sensible structural equation models that can provide modelling beneﬁts, but that are not necessarily going to be the same as a true causal model, and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more beneﬁts than learning structural equations alone. 1</p><p>3 0.67960095 <a title="113-lda-3" href="./nips-2006-Parameter_Expanded_Variational_Bayesian_Methods.html">159 nips-2006-Parameter Expanded Variational Bayesian Methods</a></p>
<p>Author: Tommi S. Jaakkola, Yuan Qi</p><p>Abstract: Bayesian inference has become increasingly important in statistical machine learning. Exact Bayesian calculations are often not feasible in practice, however. A number of approximate Bayesian methods have been proposed to make such calculations practical, among them the variational Bayesian (VB) approach. The VB approach, while useful, can nevertheless suffer from slow convergence to the approximate solution. To address this problem, we propose Parameter-eXpanded Variational Bayesian (PX-VB) methods to speed up VB. The new algorithm is inspired by parameter-expanded expectation maximization (PX-EM) and parameterexpanded data augmentation (PX-DA). Similar to PX-EM and -DA, PX-VB expands a model with auxiliary variables to reduce the coupling between variables in the original model. We analyze the convergence rates of VB and PX-VB and demonstrate the superior convergence rates of PX-VB in variational probit regression and automatic relevance determination. 1</p><p>4 0.5102303 <a title="113-lda-4" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>Author: Alexis Battle, Gal Chechik, Daphne Koller</p><p>Abstract: We present a probabilistic model applied to the fMRI video rating prediction task of the Pittsburgh Brain Activity Interpretation Competition (PBAIC) [2]. Our goal is to predict a time series of subjective, semantic ratings of a movie given functional MRI data acquired during viewing by three subjects. Our method uses conditionally trained Gaussian Markov random ﬁelds, which model both the relationships between the subjects’ fMRI voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. We also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. The model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 PBAIC. 1</p><p>5 0.47728702 <a title="113-lda-5" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>6 0.47721514 <a title="113-lda-6" href="./nips-2006-Hierarchical_Dirichlet_Processes_with_Random_Effects.html">91 nips-2006-Hierarchical Dirichlet Processes with Random Effects</a></p>
<p>7 0.47271398 <a title="113-lda-7" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>8 0.47247633 <a title="113-lda-8" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>9 0.47231838 <a title="113-lda-9" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>10 0.47128493 <a title="113-lda-10" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>11 0.46953303 <a title="113-lda-11" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>12 0.46822292 <a title="113-lda-12" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>13 0.46789962 <a title="113-lda-13" href="./nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</a></p>
<p>14 0.46780196 <a title="113-lda-14" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>15 0.46772507 <a title="113-lda-15" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>16 0.46769738 <a title="113-lda-16" href="./nips-2006-Real-time_adaptive_information-theoretic_optimization_of_neurophysiology_experiments.html">165 nips-2006-Real-time adaptive information-theoretic optimization of neurophysiology experiments</a></p>
<p>17 0.46765241 <a title="113-lda-17" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>18 0.46719402 <a title="113-lda-18" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>19 0.46663487 <a title="113-lda-19" href="./nips-2006-Bayesian_Ensemble_Learning.html">41 nips-2006-Bayesian Ensemble Learning</a></p>
<p>20 0.4665305 <a title="113-lda-20" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
