<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-28" href="#">nips2006-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</h1>
<br/><p>Source: <a title="nips-2006-28-pdf" href="http://papers.nips.cc/paper/3059-an-efficient-method-for-gradient-based-adaptation-of-hyperparameters-in-svm-models.pdf">pdf</a></p><p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>Reference: <a title="nips-2006-28-reference" href="../nips2006_reference/nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e. [sent-9, score-0.7]
</p><p>2 The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. [sent-12, score-0.543]
</p><p>3 We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. [sent-13, score-0.512]
</p><p>4 Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. [sent-14, score-0.346]
</p><p>5 The n machine’s output o for any example x is given as o = w · φ(x) − b = j=1 αj yj k(x, xi ) − b where the αi are the dual variables, b is the threshold parameter and, as usual, computations involving φ are handled using the kernel function: k(x, z) = φ(x) · φ(z). [sent-17, score-0.252]
</p><p>6 For example, the Gaussian kernel is given by k(x, z) = exp(−γ x − z 2 ) (2) The regularization parameter C and kernel parameters such as γ comprise the vector h of hyperparameters in the model. [sent-18, score-0.354]
</p><p>7 h is usually chosen by optimizing a validation measure (such as the k-fold cross validation error) on a grid of values (e. [sent-19, score-1.044]
</p><p>8 Particularly, when n is large, this search is so time-consuming that one usually resorts to either default hyperparameter values or crude search strategies. [sent-23, score-0.372]
</p><p>9 Such an approach requires the computation of the gradient of the validation function with respect to h. [sent-31, score-0.543]
</p><p>10 A major disadvantage of this method is that it requires the expensive computation and storage of the inverse of a kernel sub-matrix corresponding to the support vectors. [sent-35, score-0.139]
</p><p>11 (1) We consider differentiable versions of validation-set-based objective functions for model selection (such as k-fold error) and give an efﬁcient method for computing the gradient of this function with respect to h. [sent-38, score-0.363]
</p><p>12 Our method does not require the computation of the inverse of a large kernel sub-matrix. [sent-39, score-0.139]
</p><p>13 In essence, the cost of computing the gradient with respect to h is about the same, and usually much lesser than the cost of solving (1) for a given h. [sent-41, score-0.291]
</p><p>14 (2) Our method is applicable to a wide range of validation objective functions and SVM models that may involve many hyperparameters. [sent-42, score-0.452]
</p><p>15 (3) Large-scale empirical results show that with BFGS optimization, just trying about 10-20 hyperparameter points leads to the determination of optimal hyperparameters. [sent-44, score-0.224]
</p><p>16 Moreover, even as compared to a ﬁne grid search, the gradient procedure provides a more precise placement of hyperparameters leading to better generalization performance. [sent-45, score-0.522]
</p><p>17 The beneﬁt in terms of efﬁciency over the grid approach is evident even with just two hyperparameters. [sent-46, score-0.202]
</p><p>18 We also show the usefulness of our method for tuning more than two hyperparameters when optimizing validation functions such as the F measure and weighted error rate. [sent-47, score-0.831]
</p><p>19 In section 3, we describe our framework and provide the details of the gradient computation for general validation functions. [sent-50, score-0.511]
</p><p>20 In section 4, we discuss how to develop differentiable versions of several common performance validation functions. [sent-51, score-0.553]
</p><p>21 We assume that the kernel function k is a continuously differentiable function of h. [sent-58, score-0.226]
</p><p>22 Three commonly used SVM loss functions are: (1) hinge loss; (2) squared hinge loss; and (3) squared loss. [sent-59, score-0.526]
</p><p>23 The solution usually leads to a linear system relating α and b: P  α b  =q  (4)  where P and q are, in general, functions of h. [sent-61, score-0.144]
</p><p>24 We make the following assumption: Locally around h (at which we are interested in calculating the gradient of the validation function to be deﬁned soon) P and q are continuously differentiable functions of h. [sent-62, score-0.669]
</p><p>25 We write down P and q for the hinge loss function and discuss the validity of the above assumption. [sent-63, score-0.327]
</p><p>26 Let α0 , αc , αu , yc , yu , ec , eu , Ωuc , Ωuu etc be appropriately deﬁned vectors and matrices. [sent-68, score-0.156]
</p><p>27 The modiﬁed Huber loss function can also be used, though the derivation of (4) for it is more complex than for the three loss functions mentioned above. [sent-71, score-0.211]
</p><p>28 Recently, weighted hinge loss with asymmetric margins (Grandvalet et al. [sent-72, score-0.376]
</p><p>29 where Ci = C+ , mi = m+ if yi = 1 and Ci = C− , mi = m− if yi = −1. [sent-76, score-0.184]
</p><p>30 Because C+ and C− are present, the hyperparameter C in (1) can be omitted. [sent-77, score-0.224]
</p><p>31 The SVM model with weighted hinge loss has four extra hyperparameters, C + , C− , m+ and m− , apart from the kernel hyperparameters. [sent-78, score-0.482]
</p><p>32 Our methods in this paper allow the possibility of efﬁciently tuning all these parameters together with kernel parameters. [sent-79, score-0.174]
</p><p>33 It extends to a wide class of kernel methods for which the optimality conditions for minimizing a training objective function can be expressed as a linear system (4) in a continuously differentiable manner 1 . [sent-81, score-0.316]
</p><p>34 Note that throughout the hyperparameter optimization process, the training-validation splits are ﬁxed. [sent-87, score-0.29]
</p><p>35 Let Kli = k(˜l , xi ) involving a kernel calculation between x ˜ l=1 x an element of a validation set with an element of the training set. [sent-89, score-0.565]
</p><p>36 The output on the l th validation ˜ example is ol = i αi yi Kli − b which, for convenience, we will rewrite as ˜ T ol = ψ l β ˜  (6)  ˜ where β is a vector containing α and b, and ψl is a vector containing yi Kli , i = 1, . [sent-90, score-1.036]
</p><p>37 , on ) o ˜˜  (7)  h  where f is a differentiable validation function of the outputs ol which implicitly depend on h. [sent-97, score-0.782]
</p><p>38 ⇒  ˙ ˙ β = P −1 (q − P β) ˙  (8)  n ˜  f˙ =  ˙ (∂f /∂ ol )ol ˜ ˜  (9)  l=1 1  Infact, the main ideas easily extend when the optimality conditions form a non-linear system in (α, b) (e. [sent-103, score-0.281]
</p><p>39 ˙ where ol is obtained by differentiating (6): ˜ T ˙ ˙T ˙ o l = ψ l β + ψl β ˜  (10)  ˙ The computation of β in (8) is the most expensive step, mainly because it requires P −1 . [sent-106, score-0.329]
</p><p>40 Note that, −1 for hinge loss, P can be computed in a somewhat cheaper way: only a matrix of the dimension of Iu needs to be inverted. [sent-107, score-0.199]
</p><p>41 We now give a simple trick that shows that if the gradient calculations are re-organized, then obtaining the solution of just a single linear system sufﬁces for computing the full gradient of f with ˙ respect to all elements of h. [sent-114, score-0.364]
</p><p>42 Let us denote the coefﬁcient of ol in the expression for f˙ in (9) by δl , ˜ i. [sent-115, score-0.249]
</p><p>43 , for hinge loss when computing ˙ ˙ P β the parts of P corresponding to α0 (see (5)) can be ignored. [sent-126, score-0.315]
</p><p>44 The sequence of steps for the computation of the full gradient of f with respect to h is as follows. [sent-128, score-0.162]
</p><p>45 For various choices of validation function, we outline this computation in the next section. [sent-130, score-0.411]
</p><p>46 The computation of P β has to be performed for each hyperparameter separately. [sent-133, score-0.254]
</p><p>47 For θ = γ or γt , when using (2,3), one can cache pairwise distance computations while computing the kernel matrix. [sent-138, score-0.158]
</p><p>48 We have found (see section 5) that the cost of computing the gradient of f with respect to h to be usually much less than the cost of solving (1) and then obtaining f . [sent-139, score-0.291]
</p><p>49 We can also employ the above ideas in a validation scheme where one uses k training-validation splits (e. [sent-140, score-0.413]
</p><p>50 In this case, for each partition one obtains the linear system (4), corresponding validation outputs (6) and the linear system in (14). [sent-142, score-0.496]
</p><p>51 , Quasi-Newton methods such as BFGS which only require function value and gradient at a hyperparameter setting. [sent-148, score-0.324]
</p><p>52 4  Smooth validation functions  We consider validation functions that are general functions of the confusion matrix, of the form f (tp, f p) where tp is the number of true positives and f p is the number of false positives. [sent-155, score-1.057]
</p><p>53 Denote u l = u(˜l ol ), which ˜ y˜ evaluates to 1 if the lth example is correctly classiﬁed and 0 otherwise. [sent-157, score-0.249]
</p><p>54 Then, tp and f p can be ˜ ˜ ˜ ˜ written as tp = l:˜l =+1 ul , f p = l:˜l =−1 (1 − ul ). [sent-158, score-0.522]
</p><p>55 Let n+ and n− be the number of validation y y examples in the positive and negative classes. [sent-159, score-0.381]
</p><p>56 The most commonly used validation function is error rate. [sent-160, score-0.416]
</p><p>57 n n  For classiﬁcation problems with imbalanced classes it is usual to consider either weighted error rate or a function of precision and recall such as the F measure. [sent-164, score-0.285]
</p><p>58 Weighted Error rate (wer) is given by wer = (˜ + − tp + ηf p)/(˜ + + η˜ − ), where η is the ratio n n n of the cost of misclassiﬁcations of the negative class to that of the positive class. [sent-165, score-0.3]
</p><p>59 F measure (F ) is the harmonic mean of precision and recall: F = 2tp/(˜ + + tp + f p) n Alternatively, one may want to maximize precision under a recall constraint, or maximize the area under the ROC Curve or maximize the precision-recall breakeven point. [sent-166, score-0.307]
</p><p>60 , at any given threshold σ0 , tp and f p can be redeﬁned in terms of the following, (15)  ul = u (˜l (˜l − σ0 )) ˜ y o  For imbalanced problems one may wish to maximize a score such as the F measure over all values of σ0 . [sent-171, score-0.485]
</p><p>61 In such cases, it is appropriate to incorporate σ0 as an additional hyperparameter that needs to be tuned. [sent-172, score-0.224]
</p><p>62 The validation functions discussed above are based on discrete counts. [sent-175, score-0.426]
</p><p>63 To develop smooth versions of validation functions, we deﬁne sl , which is a sigmoidal approximation to ul (15) of the following form: ˜ ˜ (16)  sl = 1/[1 + exp (−σ1 yl (˜l − σ0 ))] ˜ ˜ o  where σ1 > 0 is a sigmoidal scale factor. [sent-177, score-1.074]
</p><p>64 In general, σ0 , σ1 may be functions of the validation outputs. [sent-178, score-0.426]
</p><p>65 ) The scale factor σ1 inﬂuences how closely sl approximates the step function ul and hence controls ˜ ˜ the degree of smoothness in building the sigmoidal approximation. [sent-180, score-0.342]
</p><p>66 As the hyperparameter space is probed, the magnitude of the outputs can vary quite a bit. [sent-181, score-0.275]
</p><p>67 We build a differentiable version of such a function by simply replacing ul by sl . [sent-184, score-0.336]
</p><p>68 where the partial derivatives of sl with respect to ol , σ0 , σ1 can be easily derived from (16) and ˜ ˜ (∂f /∂˜l ) = (∂f /∂tp)(∂tp/∂˜l ) + (∂f /∂f p)(∂f p/∂˜l ). [sent-192, score-0.441]
</p><p>69 s s s We now discuss three methods to compute the sigmoidal parameters σ0 , σ1 . [sent-193, score-0.152]
</p><p>70 For each of these methods the partial derivatives of σ0 , σ1 with respect to ol can be obtained (Keerthi et al. [sent-194, score-0.336]
</p><p>71 Here, we treat σ0 as a hyperparameter and set σ1 as above. [sent-200, score-0.25]
</p><p>72 In this method, we obtain σ0 , σ1 by performing sigmoidal ﬁtting based on unconstrained minimization of some smooth criterion N , i. [sent-202, score-0.147]
</p><p>73 A natural choice of N is based on Platt’s method (Platt (1999)) where sl is interpreted as the posterior ˜ probability that the class of l th validation example is yl , and we take N to be the Negative-Log˜ Likelihood: N = Nnll = − l log(˜l ). [sent-205, score-0.612]
</p><p>74 The probabilistic error rate: per = l (1 − sl )/˜ and f = Nnll ˜ n are suitable validation functions which go well with the choice N = Nnll . [sent-208, score-0.595]
</p><p>75 Five fold cross validation was used to form the validation functions. [sent-212, score-0.802]
</p><p>76 The Gaussian kernel (2) and the ARD-Gaussian kernel (3) were used. [sent-224, score-0.166]
</p><p>77 For (C, γ) tuning with the Gaussian Kernel, we also tried the popular Grid over a 15 × 15 grid of values. [sent-225, score-0.293]
</p><p>78 For C, γ tuning with the gradient method, the starting point C = γ = 1 was used. [sent-226, score-0.191]
</p><p>79 Figure 1 shows the contours of the smoothed validation error rate and the actual test error rate for the IJCNN dataset with 2000 training examples on the (log C, log γ) plane. [sent-228, score-0.746]
</p><p>80 Grid and Grad respectively denote the grid and the gradient methods applied to the (C, γ) tuning task. [sent-229, score-0.393]
</p><p>81 We also generated corresponding contours (omitted) for f = per and f = Nnll (see end of section 4) and found that the validation er with the direct method better represents the test error rate. [sent-232, score-0.562]
</p><p>82 Although the efﬁciency of Grid can be improved in certain ways (say, by performing a crude search followed by a reﬁned search, by avoiding unnecessary exploration of difﬁcult regions in the hyperparameter space etc) Grad determines the optimal hyperparameters more precisely. [sent-237, score-0.483]
</p><p>83 ntrg  cpu  Grid  erate  nf  2000 4000 8000 16000 32000  10. [sent-245, score-0.556]
</p><p>84 ntrg 2000 4000 8000 16000  nf 9 16 10 6  Grad cpu 3. [sent-284, score-0.339]
</p><p>85 To study the effectiveness of our gradient-based approach when many hyperparameters are present, we use the ARD-Gaussian kernel in (3) and tune C together with all the γ t ’s. [sent-315, score-0.363]
</p><p>86 Results are reported in Table 1 as Grad-ARD where cpu denotes the extra time for this optimization. [sent-318, score-0.159]
</p><p>87 We see that Grad-ARD achieves signiﬁcant improvements in generalization performance over Grad without increasing the computational cost by much even though a large number of hyperparameters are being tuned. [sent-319, score-0.231]
</p><p>88 We implemented two methods: (1) we set σ0 = 0 and tuned only C and γ; (2) we tuned the three hyperparameters C, γ and σ 0 . [sent-325, score-0.262]
</p><p>89 Without σ0 , the mean (standard deviation) of F measure values on 5-fold cross validation and on the test set were: 0. [sent-327, score-0.421]
</p><p>90 The ability to easily include the threshold as an extra hyperparameter is a very useful advantage for our method. [sent-338, score-0.334]
</p><p>91 In imbalanced problems where the proportion of examples in the positive class is small, one usually minimizes weighted error rate wer (see section 4) with a small value of η. [sent-340, score-0.352]
</p><p>92 (4) Use the full Weighted Hinge loss model and tune  C+ , C− , m+ and m− . [sent-344, score-0.175]
</p><p>93 The top half of Table 3 reports weighted error rates associated with validation and test. [sent-349, score-0.481]
</p><p>94 Interestingly, for the weighted hinge loss method, tuning of threshold has little effect. [sent-387, score-0.522]
</p><p>95 In doing this, there are three steps that take up the bulk of the computational cost: (1) training using the SMO algorithm; (2) the solution of the linear system in (14); and ˙ (3) the remaining computations associated with the gradient, of which the computation of P β in (13) is the major part. [sent-392, score-0.189]
</p><p>96 We also found that the P β cost of Grad-ARD doesn’t become large in spite of the fact that 23 hyperparameters are tuned there. [sent-396, score-0.268]
</p><p>97 Even in models with just two hyperparameters our approach is faster and offers a more precise hyperparameter placement than the Grid approach. [sent-399, score-0.444]
</p><p>98 The ability to tune many hyperparameters easily should be used with care. [sent-401, score-0.28]
</p><p>99 So, for a given problem it is important to choose the set of hyperparameters carefully, in accordance with the richness of the training set. [sent-403, score-0.246]
</p><p>100 An efﬁcient method for gradient-based adaptation of hyperparameters in SVM models. [sent-409, score-0.246]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('validation', 0.381), ('grad', 0.293), ('ol', 0.249), ('hyperparameter', 0.224), ('erate', 0.217), ('ijcnn', 0.209), ('grid', 0.202), ('hinge', 0.199), ('hyperparameters', 0.188), ('tp', 0.16), ('nf', 0.134), ('sl', 0.134), ('cpu', 0.133), ('nnll', 0.12), ('oi', 0.115), ('adult', 0.115), ('imbalanced', 0.115), ('svm', 0.114), ('sigmoidal', 0.107), ('differentiable', 0.101), ('ul', 0.101), ('gradient', 0.1), ('keerthi', 0.096), ('tune', 0.092), ('tuning', 0.091), ('vehicle', 0.084), ('threshold', 0.084), ('loss', 0.083), ('kernel', 0.083), ('iu', 0.072), ('kli', 0.072), ('ntrg', 0.072), ('contours', 0.067), ('yi', 0.065), ('weighted', 0.065), ('wer', 0.063), ('training', 0.058), ('grandvalet', 0.057), ('hk', 0.056), ('er', 0.053), ('outputs', 0.051), ('differentiating', 0.05), ('smo', 0.05), ('smoothed', 0.048), ('yc', 0.048), ('splice', 0.048), ('chapelle', 0.047), ('discuss', 0.045), ('functions', 0.045), ('yl', 0.044), ('etc', 0.044), ('cost', 0.043), ('involving', 0.043), ('continuously', 0.042), ('classi', 0.042), ('computations', 0.042), ('uu', 0.042), ('cross', 0.04), ('smooth', 0.04), ('usually', 0.04), ('calculations', 0.04), ('bfgs', 0.038), ('tuned', 0.037), ('search', 0.037), ('precision', 0.036), ('error', 0.035), ('table', 0.035), ('optimization', 0.034), ('tting', 0.034), ('rate', 0.034), ('crude', 0.034), ('sindhwani', 0.034), ('computing', 0.033), ('respect', 0.032), ('system', 0.032), ('splits', 0.032), ('mpi', 0.032), ('eu', 0.032), ('placement', 0.032), ('adaptation', 0.032), ('yu', 0.032), ('partitions', 0.032), ('computation', 0.03), ('et', 0.029), ('ic', 0.029), ('storing', 0.028), ('dataset', 0.027), ('th', 0.027), ('omitted', 0.027), ('mi', 0.027), ('log', 0.027), ('solution', 0.027), ('treat', 0.026), ('extra', 0.026), ('versions', 0.026), ('method', 0.026), ('apart', 0.026), ('derivatives', 0.026), ('ef', 0.026), ('maximize', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="28-tfidf-1" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>2 0.12792408 <a title="28-tfidf-2" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose a highly efﬁcient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work. 1</p><p>3 0.12674852 <a title="28-tfidf-3" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf</p><p>Abstract: We present a local learning approach for clustering. The basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. An optimization problem is formulated such that its solution has the above property. Relaxation and eigen-decomposition are applied to solve this optimization problem. We also brieﬂy investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. Experimental results are provided to validate the effectiveness of the proposed approach. 1</p><p>4 0.11519749 <a title="28-tfidf-4" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: The standard Support Vector Machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to deﬁne the generated classiﬁer. We present a modiﬁed version of SVM that allows the user to set a budget parameter B and focuses on minimizing the loss attained by the B worst-classiﬁed examples while ignoring the remaining examples. This idea can be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we obtain these new SVM variants by replacing the 1-norm in the standard SVM formulation with various interpolation-norms. We also adapt the SMO optimization algorithm to our setting and report on some preliminary experimental results. 1</p><p>5 0.10835701 <a title="28-tfidf-5" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>6 0.090858258 <a title="28-tfidf-6" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>7 0.086866893 <a title="28-tfidf-7" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>8 0.074455358 <a title="28-tfidf-8" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>9 0.073935024 <a title="28-tfidf-9" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>10 0.073349729 <a title="28-tfidf-10" href="./nips-2006-Tighter_PAC-Bayes_Bounds.html">193 nips-2006-Tighter PAC-Bayes Bounds</a></p>
<p>11 0.073225841 <a title="28-tfidf-11" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>12 0.069650427 <a title="28-tfidf-12" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>13 0.069281347 <a title="28-tfidf-13" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>14 0.066526175 <a title="28-tfidf-14" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>15 0.06468413 <a title="28-tfidf-15" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>16 0.062262639 <a title="28-tfidf-16" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>17 0.062001649 <a title="28-tfidf-17" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>18 0.061007824 <a title="28-tfidf-18" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>19 0.060558543 <a title="28-tfidf-19" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>20 0.059714772 <a title="28-tfidf-20" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.213), (1, 0.062), (2, -0.015), (3, 0.019), (4, -0.024), (5, 0.112), (6, -0.078), (7, 0.035), (8, 0.012), (9, -0.002), (10, -0.139), (11, -0.038), (12, -0.072), (13, -0.081), (14, -0.001), (15, 0.05), (16, 0.072), (17, -0.033), (18, -0.02), (19, -0.013), (20, 0.041), (21, 0.019), (22, 0.009), (23, 0.054), (24, -0.116), (25, 0.028), (26, 0.036), (27, -0.08), (28, -0.021), (29, 0.003), (30, 0.042), (31, 0.011), (32, 0.068), (33, -0.042), (34, 0.056), (35, 0.039), (36, 0.194), (37, -0.098), (38, -0.001), (39, -0.031), (40, -0.114), (41, 0.183), (42, 0.054), (43, 0.017), (44, 0.054), (45, -0.052), (46, -0.021), (47, -0.086), (48, -0.02), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93888944 <a title="28-lsi-1" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>2 0.65644979 <a title="28-lsi-2" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><p>3 0.63453764 <a title="28-lsi-3" href="./nips-2006-Support_Vector_Machines_on_a_Budget.html">186 nips-2006-Support Vector Machines on a Budget</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: The standard Support Vector Machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to deﬁne the generated classiﬁer. We present a modiﬁed version of SVM that allows the user to set a budget parameter B and focuses on minimizing the loss attained by the B worst-classiﬁed examples while ignoring the remaining examples. This idea can be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we obtain these new SVM variants by replacing the 1-norm in the standard SVM formulation with various interpolation-norms. We also adapt the SMO optimization algorithm to our setting and report on some preliminary experimental results. 1</p><p>4 0.60560685 <a title="28-lsi-4" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose a highly efﬁcient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work. 1</p><p>5 0.55455303 <a title="28-lsi-5" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Author: Cheng-tao Chu, Sang K. Kim, Yi-an Lin, Yuanyuan Yu, Gary Bradski, Kunle Olukotun, Andrew Y. Ng</p><p>Abstract: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and uniﬁed way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Speciﬁcally, we show that algorithms that ﬁt the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors. 1</p><p>6 0.55069941 <a title="28-lsi-6" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>7 0.5466888 <a title="28-lsi-7" href="./nips-2006-Large-Scale_Sparsified_Manifold_Regularization.html">104 nips-2006-Large-Scale Sparsified Manifold Regularization</a></p>
<p>8 0.53042585 <a title="28-lsi-8" href="./nips-2006-Multiple_Instance_Learning_for_Computer_Aided_Diagnosis.html">140 nips-2006-Multiple Instance Learning for Computer Aided Diagnosis</a></p>
<p>9 0.51173937 <a title="28-lsi-9" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>10 0.50791609 <a title="28-lsi-10" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>11 0.48962387 <a title="28-lsi-11" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>12 0.46650216 <a title="28-lsi-12" href="./nips-2006-Branch_and_Bound_for_Semi-Supervised_Support_Vector_Machines.html">48 nips-2006-Branch and Bound for Semi-Supervised Support Vector Machines</a></p>
<p>13 0.46556979 <a title="28-lsi-13" href="./nips-2006-On_Transductive_Regression.html">150 nips-2006-On Transductive Regression</a></p>
<p>14 0.46408075 <a title="28-lsi-14" href="./nips-2006-Tighter_PAC-Bayes_Bounds.html">193 nips-2006-Tighter PAC-Bayes Bounds</a></p>
<p>15 0.45937407 <a title="28-lsi-15" href="./nips-2006-Isotonic_Conditional_Random_Fields_and_Local_Sentiment_Flow.html">101 nips-2006-Isotonic Conditional Random Fields and Local Sentiment Flow</a></p>
<p>16 0.45524079 <a title="28-lsi-16" href="./nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</a></p>
<p>17 0.45276317 <a title="28-lsi-17" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>18 0.45232305 <a title="28-lsi-18" href="./nips-2006-Large_Scale_Hidden_Semi-Markov_SVMs.html">108 nips-2006-Large Scale Hidden Semi-Markov SVMs</a></p>
<p>19 0.42225575 <a title="28-lsi-19" href="./nips-2006-Large_Margin_Hidden_Markov_Models_for_Automatic_Speech_Recognition.html">106 nips-2006-Large Margin Hidden Markov Models for Automatic Speech Recognition</a></p>
<p>20 0.41146058 <a title="28-lsi-20" href="./nips-2006-Hyperparameter_Learning_for_Graph_Based_Semi-supervised_Learning_Algorithms.html">93 nips-2006-Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.126), (3, 0.015), (7, 0.548), (9, 0.03), (22, 0.055), (44, 0.037), (57, 0.051), (65, 0.038), (69, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99176741 <a title="28-lda-1" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><p>2 0.96027613 <a title="28-lda-2" href="./nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">129 nips-2006-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Author: Cheng-tao Chu, Sang K. Kim, Yi-an Lin, Yuanyuan Yu, Gary Bradski, Kunle Olukotun, Andrew Y. Ng</p><p>Abstract: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and uniﬁed way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Speciﬁcally, we show that algorithms that ﬁt the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors. 1</p><p>3 0.95835602 <a title="28-lda-3" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><p>same-paper 4 0.95056903 <a title="28-lda-4" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>Author: S. S. Keerthi, Vikas Sindhwani, Olivier Chapelle</p><p>Abstract: We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very efﬁciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of hyperparameters can be identiﬁed by our approach with very few training rounds and gradient computations. . 1</p><p>5 0.87627631 <a title="28-lda-5" href="./nips-2006-The_Neurodynamics_of_Belief_Propagation_on_Binary_Markov_Random_Fields.html">190 nips-2006-The Neurodynamics of Belief Propagation on Binary Markov Random Fields</a></p>
<p>Author: Thomas Ott, Ruedi Stoop</p><p>Abstract: We rigorously establish a close relationship between message passing algorithms and models of neurodynamics by showing that the equations of a continuous Hopﬁeld network can be derived from the equations of belief propagation on a binary Markov random ﬁeld. As Hopﬁeld networks are equipped with a Lyapunov function, convergence is guaranteed. As a consequence, in the limit of many weak connections per neuron, Hopﬁeld networks exactly implement a continuous-time variant of belief propagation starting from message initialisations that prevent from running into convergence problems. Our results lead to a better understanding of the role of message passing algorithms in real biological neural networks.</p><p>6 0.85041106 <a title="28-lda-6" href="./nips-2006-Convergence_of_Laplacian_Eigenmaps.html">60 nips-2006-Convergence of Laplacian Eigenmaps</a></p>
<p>7 0.79464918 <a title="28-lda-7" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>8 0.73476177 <a title="28-lda-8" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>9 0.69658965 <a title="28-lda-9" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>10 0.68749869 <a title="28-lda-10" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<p>11 0.68628335 <a title="28-lda-11" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>12 0.68625885 <a title="28-lda-12" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>13 0.67668992 <a title="28-lda-13" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>14 0.67499518 <a title="28-lda-14" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<p>15 0.67053896 <a title="28-lda-15" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>16 0.66836828 <a title="28-lda-16" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>17 0.66595608 <a title="28-lda-17" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>18 0.6626668 <a title="28-lda-18" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>19 0.66159034 <a title="28-lda-19" href="./nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</a></p>
<p>20 0.66035229 <a title="28-lda-20" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
