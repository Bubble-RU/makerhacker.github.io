<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-102" href="#">nips2006-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</h1>
<br/><p>Source: <a title="nips-2006-102-pdf" href="http://papers.nips.cc/paper/3121-kernel-maximum-entropy-data-transformation-and-an-enhanced-spectral-clustering-algorithm.pdf">pdf</a></p><p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>Reference: <a title="nips-2006-102-reference" href="../nips2006_reference/nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. [sent-2, score-0.592]
</p><p>2 We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. [sent-4, score-0.887]
</p><p>3 An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. [sent-5, score-0.996]
</p><p>4 Some of the most well-known approaches to data transformation are based on eigenvectors of certain matrices. [sent-8, score-0.228]
</p><p>5 Of special interest to this paper is kernel PCA [3], a member of the kernel-based methods [4]. [sent-12, score-0.385]
</p><p>6 Recently, it has been shown that there is a close connection between the kernel methods and information theoretic learning [5, 6, 7, 8]. [sent-13, score-0.412]
</p><p>7 We propose a new kernel-based data transformation technique based on the idea of maximum entropy preservation. [sent-14, score-0.216]
</p><p>8 The new method, named kernel MaxEnt, is based on Renyi’s quadratic entropy estimated via Parzen windowing. [sent-15, score-0.519]
</p><p>9 The data transformation is obtained using eigenvectors of the data afﬁnity matrix. [sent-16, score-0.259]
</p><p>10 These eigenvectors are in general not the same as those used in kernel PCA. [sent-17, score-0.515]
</p><p>11 We show that kernel MaxEnt may produce strikingly different transformed data sets than kernel PCA. [sent-18, score-0.887]
</p><p>12 We propose an enhanced spectral clustering algorithm, by replacing kernel PCA by kernel MaxEnt as an intermediate step. [sent-19, score-0.996]
</p><p>13 Section 3 is devoted to the kernel MaxEnt method. [sent-23, score-0.385]
</p><p>14 The enhanced spectral clustering is discussed in section 5. [sent-25, score-0.193]
</p><p>15 2 Kernel PCA PCA is a linear data transformation technique based on the eigenvalues and eigenvectors of the (d × d) data correlation matrix, where d is the data dimensionality. [sent-27, score-0.404]
</p><p>16 A dimensionality reduction from d to l < d is obtained by projecting a data point onto a subspace spanned by the eigenvectors (principal axes) corresponding to the l largest eigenvalues. [sent-28, score-0.337]
</p><p>17 [3] proposed a non-linear extension, by pero forming PCA implicitly in a kernel feature space which is non-linearly related to the input space via the mapping xi → Φ(xi ), i = 1, . [sent-36, score-0.597]
</p><p>18 This matrix can be eigendecomposed as Kx = EDET , where D is a diagonal matrix storing the eigenvalues in descending order, and E is a matrix with the eigenvectors as columns. [sent-44, score-0.357]
</p><p>19 , N , onto the subspace spanned by the 1  l largest kernel space principal axes. [sent-48, score-0.57]
</p><p>20 Then, Φpca = Dl2 ET , where the (l × l) matrix Dl stores l the l largest eigenvalues, and the (N × l) matrix El stores the corresponding eigenvectors. [sent-49, score-0.206]
</p><p>21 Kernel PCA thus preserves variance in terms of the kernel induced feature space. [sent-51, score-0.462]
</p><p>22 However, kernel PCA is not easily interpreted in terms of the input space data set. [sent-52, score-0.49]
</p><p>23 How does variance preservation in the kernel feature space correspond to an operation on the input space data set? [sent-53, score-0.584]
</p><p>24 To the best of our knowledge, there are no such intuitive interpretations of kernel PCA. [sent-54, score-0.385]
</p><p>25 In the next section, we introduce kernel MaxEnt, which we show is related to kernel PCA. [sent-55, score-0.77]
</p><p>26 However, kernel MaxEnt may be interpreted in terms of the input space, and will in general perform a different projection in the kernel space. [sent-56, score-0.808]
</p><p>27 A non-parametric estimator for H2 (x) is obtained by replacing the actual pdf by its Parzen window estimator, given by [10] 1 ˆ f (x) = N  N  Wσ (x, xi ),  Wσ (x, xi ) =  i=1  1 (2πσ 2 )  d 2  exp −  ||x − xi ||2 2σ 2  . [sent-62, score-0.297]
</p><p>28 In the following, we construct the kernel matrix Kx , such that element (i, j) of Kx equals k(xi , xj ), i, j = 1, . [sent-69, score-0.483]
</p><p>29 It is easily shown that the Renyi quadratic entropy may be expressed compactly in terms of the 1 ˆ kernel matrix as H2 (x) = − log N 2 1T Kx 1, where 1 is a (N × 1) ones-vector. [sent-73, score-0.544]
</p><p>30 It is thus clear that all the information regarding the Renyi entropy resides in the kernel matrix Kx . [sent-75, score-0.529]
</p><p>31 Hence, the kernel matrix is the input space quantity of interest in this paper. [sent-76, score-0.515]
</p><p>32 A well-known input space data transformation principle is founded on the idea of maximum entropy (MaxEnt), usually deﬁned in terms of minimum model complexity. [sent-77, score-0.32]
</p><p>33 In this paper, we deﬁne MaxEnt differently, as a mapping X → Y, such that the entropy associated with Y is maximally similar 1  In [3] the kernel feature space data was assumed centered, obtained by a centering operation of the kernel matrix. [sent-78, score-1.046]
</p><p>34 This means that the kernel matrix Ky must be maximally similar to Kx in some sense. [sent-82, score-0.473]
</p><p>35 Since our input space quantity of concern is the kernel matrix, we are only implicitly concerned with the Y data set (we do not actually want to obtain Y, nor is the dimensionality of interest). [sent-83, score-0.546]
</p><p>36 The kernel matrix can be decomposed as Kx = EDET . [sent-84, score-0.428]
</p><p>37 The kernel matrix is at the same time an inner-product matrix in the Mercer kernel induced feature space. [sent-85, score-0.891]
</p><p>38 Let Φ x be a matrix such that each column represents an approximation to the corresponding kernel feature space data point in the set 1 Φ(x1 ), . [sent-86, score-0.53]
</p><p>39 Note that Φx = D 2 E is the projection onto all the principal axes in the Mercer kernel feature space, hence deﬁning a N -dimensional data set. [sent-91, score-0.539]
</p><p>40 We now describe a dimensionality reduction in the Mercer kernel space, obtaining the k-dimensional Φy from Φx , yielding Ky = ΦT Φy such that V (y) ≈ V (x). [sent-92, score-0.433]
</p><p>41 Let us deﬁne the data set Φy = Dk ET , using the k eigenvalues k 2 and eigenvectors of Kx corresponding to the k largest products λi γi . [sent-101, score-0.319]
</p><p>42 Hence, Ky = ΦT Φy = y 1  1  2 2 Ek Dk Dk ET = Ek Dk ET , and k k  1 V (y) = 2 N  k 2 λi γ i = i=1  1 T 1 Ky 1, N2  (4)  the best approximation to the entropy estimate V (x) using k eigenvalues and eigenvectors. [sent-102, score-0.199]
</p><p>43 We 1  2 thus refer to the mapping Φy = Dk ET as a maximum entropy data transformation in a Mercer k kernel feature space. [sent-103, score-0.664]
</p><p>44 Note that this is not the same as the PCA dimensionality reduction in Mercer 1 kernel feature space, which is deﬁned as Φpca = Dl2 ET , using the eigenvalues and eigenvectors l corresponding to the l largest eigenvalues of Kx . [sent-104, score-0.854]
</p><p>45 The kernel MaxEnt procedure, as described above, is summarized in Table 1. [sent-106, score-0.385]
</p><p>46 It is important to realize that kernel MaxEnt outputs two quantities, which may be used for further data analysis. [sent-107, score-0.416]
</p><p>47 The 1 2 kernel space output quantity is the transformed data set Φy = Dk ET . [sent-108, score-0.531]
</p><p>48 The input space output k T quantity is the kernel matrix Ky = Ek Dk Ek , which is an approximation to the original kernel matrix Kx . [sent-109, score-0.943]
</p><p>49 There are two possible outputs; the input space kernel matrix Ky , and the kernel space data set Φy . [sent-112, score-0.939]
</p><p>50 1 Interpretation in Terms of Cost Function Minimization Kernel MaxEnt produces a new kernel matrix Ky = Ek Dk ET , such that the sum of the elements of k Ky is maximally equal to the sum of the elements of Kx . [sent-114, score-0.543]
</p><p>51 Hence, kernel MaxEnt picks eigenvectors and eigenvalues in order to minimize the cost function 1T (Kx − Ky )1. [sent-115, score-0.613]
</p><p>52 On the other hand, it is well known that the kernel PCA matrix Kpca = El Dl ET , based on the l largest eigenvalues, minimizes l the Frobenius norm of (Kx − Kpca ), that is 1T (Kx − Kpca ). [sent-116, score-0.488]
</p><p>53 2 Kernel MaxEnt Eigenvectors Reveal Cluster Structure Under “ideal” circumstances, kernel MaxEnt and kernel PCA yield the same result, as shown in the following. [sent-120, score-0.77]
</p><p>54 Assume that the data consists of C = 2 different maximally compact subsets, such that k(xi , xj ) = 1 for xi and xj in the same subset, and k(xi , xj ) = 0 for xi and xj in different subsets (point clusters). [sent-121, score-0.404]
</p><p>55 Hence, a data point xi in subgroup one will be represented by xi → [1 0]T and a data point xj in subgroup two will be represented by xj → [0 1]T both using Φy and Φpca . [sent-125, score-0.364]
</p><p>56 Thus, kernel MaxEnt and kernel PCA yield the same data mapping, where each subgroup is mapped into mutually orthogonal points in the kernel space (the clusters were points also in the input space, but not necessarily orthogonal). [sent-127, score-1.383]
</p><p>57 Also, the eigenvectors carry all necessary information about the cluster structure (cluster memberships can be assigned by a proper thresholding). [sent-129, score-0.186]
</p><p>58 This kind of “ideal” analysis has been used as a justiﬁcation for the kernel PCA mapping, where the mapping is based on the C largest eigenvalues/eigenvectors. [sent-130, score-0.473]
</p><p>59 Such a situation will correspond to maximally concentrated eigenvalues of the kernel matrix. [sent-131, score-0.56]
</p><p>60 Shawe-Taylor and Cristianini [4] note that kernel PCA can only detect stable patterns if the eigenvalues are concentrated. [sent-133, score-0.483]
</p><p>61 In practice, the ﬁrst C eigenvectors may not necessarily be those which carry most information about the clustering structure of the data set. [sent-134, score-0.282]
</p><p>62 However, kernel MaxEnt will seek to pick those eigenvectors with the blockwise structure corresponding to cluster groupings, because this will make the sum of the elements in Ky as close as possible to the sum of the elements of Kx . [sent-135, score-0.703]
</p><p>63 Note that the entropy terms corresponding to the ﬁrst, fourth and seventh eigenvalues are signiﬁcantly larger than the rest. [sent-150, score-0.257]
</p><p>64 This means that kernel MaxEnt is based on the ﬁrst, fourth and seventh eigenvalue/eigenvector pair (yielding a 3-dimensional transformed data set). [sent-151, score-0.51]
</p><p>65 In contrast, kernel PCA is based on the eigenvalue/eigenvector pair corresponding to the three largest eigenvalues. [sent-152, score-0.445]
</p><p>66 In (c) the kernel MaxEnt data transformation is shown. [sent-153, score-0.483]
</p><p>67 The kernel PCA data transformation is shown in (d). [sent-156, score-0.483]
</p><p>68 In fact, the mean vectors of the clusters in the kernel PCA representation are not spread angularly. [sent-158, score-0.445]
</p><p>69 Observe how eigenvectors one, four and seven are those which carry information about the cluster structure, with their blockwise appearance. [sent-161, score-0.291]
</p><p>70 In (g), the kernel MaxEnt approximation Ky to the original kernel matrix is shown, obtained from eigenvectors one, four and seven. [sent-165, score-0.943]
</p><p>71 In (b) and (c) the kernel MaxEnt (eigenvalues/eigenvectors one and ﬁve) and kernel PCA transformations are shown, respectively. [sent-171, score-0.807]
</p><p>72 Again, kernel MaxEnt produces a data set where the clusters are located along almost orthogonal lines, in contrast to kernel PCA. [sent-172, score-0.896]
</p><p>73 The same phenomenon is observed for the data set shown in (d), with the kernel MaxEnt (eigenvalues/eigenvectors one and four) and kernel PCA transformations shown in (e) and (f), respectively. [sent-173, score-0.838]
</p><p>74 In addition, (g) and (h) shows the kernel MaxEnt (eigenvalues/eigenvectors one, two and ﬁve) and kernel PCA transformations of the 16-dimensional penbased handwritten digit recognition data set (three clusters, digits 0, 1 and 2), extracted from the UCI repository. [sent-174, score-0.838]
</p><p>75 These illustrations show that kernel MaxEnt produces a different transformed data set than kernel PCA. [sent-176, score-0.929]
</p><p>76 Also, it produces a kernel matrix K y having a blockwise appearance. [sent-177, score-0.555]
</p><p>77 Both the transformed data Φ y and the new kernel matrix can be utilized for further data analysis. [sent-178, score-0.541]
</p><p>78 Then, an estimator for the CS measure can be expressed as [6] ˆ ˆ f1 (x)f2 (x)dx ˆ D(f1 , f2 ) = = cos (m1 , m2 ), (6) ˆ2 ˆ2 f1 (x)dx f2 (x)dx where m1 and m2 are the kernel feature space mean vectors of the data points corresponding to ˆ ˆ the two clusters. [sent-182, score-0.527]
</p><p>79 Kernel PCA was used for representing the data in the kernel feature space. [sent-190, score-0.451]
</p><p>80 As an illustration of the utility of kernel MaxEnt, we here replace kernel PCA by kernel MaxEnt. [sent-191, score-1.155]
</p><p>81 3) Obtain a C-dimensional kernel feature space representation using kernel MaxEnt. [sent-195, score-0.841]
</p><p>82 There are three curves: Our spectral clustering algorithm using kernel MaxEnt (marked by the circle-symbol), and kernel PCA (star-symbol), and in addition we compare  30  1  20  0. [sent-205, score-0.91]
</p><p>83 6  (d)  (f)  (e)  (g)  (h)  Figure 1: Examples of data transformations using kernel MaxEnt and kernel PCA. [sent-227, score-0.838]
</p><p>84 The clustering is performed over a range of kernel sizes. [sent-230, score-0.462]
</p><p>85 The vertical line indicates the “optimal” kernel size using Silverman’s rule. [sent-231, score-0.385]
</p><p>86 Over the whole range, kernel MaxEnt performs equally good as NG, and better than kernel PCA for small kernel sizes. [sent-232, score-1.155]
</p><p>87 Kernel MaxEnt has the best performance over the most part of the kernel range. [sent-236, score-0.385]
</p><p>88 On this data set, kernel MaxEnt performs considerably better than the two other methods, over a wide range of kernel sizes. [sent-239, score-0.801]
</p><p>89 These preliminary experiments show the potential beneﬁts of kernel MaxEnt in data analysis, especially when the kernel space cost function is based on an angular measure. [sent-240, score-0.837]
</p><p>90 Using kernel MaxEnt makes the algorithm competitive to spectral clustering using the Laplacian matrix. [sent-241, score-0.525]
</p><p>91 We note that kernel MaxEnt in theory requires the full eigendecomposition, thus making it more computationally complex than clustering based on only the C largest eigenvectors. [sent-242, score-0.522]
</p><p>92 2 0  (h)  Figure 2: Examples of data transformations using kernel MaxEnt and kernel PCA. [sent-307, score-0.838]
</p><p>93 6 Conclusions In this paper, we have introduced a new data transformation technique, named kernel MaxEnt, which has a clear theoretical foundation based on the concept of maximum entropy preservation. [sent-308, score-0.634]
</p><p>94 The new method is similar in structure to kernel PCA, but may produce totally different transformed data sets. [sent-309, score-0.467]
</p><p>95 We have shown that kernel MaxEnt signiﬁcantly enhances a recent spectral clustering algorithm. [sent-310, score-0.525]
</p><p>96 Kernel MaxEnt also produces a new kernel matrix, which may be useful for further data analysis. [sent-311, score-0.438]
</p><p>97 Kernel MaxEnt requires the kernel to be a valid Parzen window (i. [sent-312, score-0.447]
</p><p>98 Kernel PCA requires the kernel to be a Mercer kernel (positive semideﬁnite), hence not necessarily a density. [sent-315, score-0.819]
</p><p>99 In that sense, kernel PCA may use a broader class of kernels. [sent-316, score-0.385]
</p><p>100 On the other hand, kernel MaxEnt may use Parzen windows which are not Mercer kernels (indeﬁnite), such as the Epanechnikov kernel. [sent-317, score-0.405]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maxent', 0.683), ('kernel', 0.385), ('kx', 0.244), ('pca', 0.206), ('renyi', 0.175), ('ky', 0.166), ('parzen', 0.146), ('eigenvectors', 0.13), ('dk', 0.11), ('blockwise', 0.105), ('entropy', 0.101), ('eigenvalues', 0.098), ('kpca', 0.097), ('mercer', 0.079), ('clustering', 0.077), ('transformation', 0.067), ('spectral', 0.063), ('window', 0.062), ('largest', 0.06), ('illustrations', 0.055), ('xj', 0.055), ('xi', 0.054), ('enhanced', 0.053), ('ek', 0.053), ('edet', 0.052), ('eltoft', 0.052), ('jenssen', 0.052), ('transformed', 0.051), ('erdogmus', 0.046), ('principe', 0.046), ('silverman', 0.046), ('maximally', 0.045), ('dx', 0.043), ('matrix', 0.043), ('ng', 0.042), ('subgroup', 0.042), ('clusters', 0.04), ('et', 0.037), ('transformations', 0.037), ('space', 0.036), ('strikingly', 0.035), ('pdf', 0.035), ('feature', 0.035), ('cluster', 0.035), ('ideal', 0.034), ('named', 0.033), ('orthogonal', 0.033), ('data', 0.031), ('cs', 0.031), ('stores', 0.03), ('founded', 0.03), ('eigenvalue', 0.029), ('quantity', 0.028), ('mapping', 0.028), ('preserves', 0.027), ('theoretic', 0.027), ('hence', 0.026), ('xii', 0.026), ('dl', 0.026), ('seventh', 0.026), ('spanned', 0.026), ('ei', 0.024), ('density', 0.024), ('reduction', 0.024), ('dimensionality', 0.024), ('elements', 0.024), ('impact', 0.023), ('inde', 0.023), ('adjustment', 0.023), ('lines', 0.023), ('input', 0.023), ('necessarily', 0.023), ('produces', 0.022), ('preservation', 0.022), ('laplacian', 0.022), ('onto', 0.022), ('estimator', 0.021), ('principal', 0.021), ('carry', 0.021), ('spread', 0.02), ('windows', 0.02), ('subspace', 0.02), ('cristianini', 0.019), ('concerned', 0.019), ('axes', 0.019), ('ller', 0.019), ('cos', 0.019), ('el', 0.018), ('fourth', 0.017), ('maximum', 0.017), ('replacing', 0.017), ('ring', 0.017), ('correspond', 0.016), ('intermediate', 0.016), ('concentrated', 0.016), ('correlation', 0.016), ('terms', 0.015), ('outermost', 0.015), ('onoda', 0.015), ('thyroid', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="102-tfidf-1" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>2 0.30513659 <a title="102-tfidf-2" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>3 0.17895012 <a title="102-tfidf-3" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>Author: Hamed Valizadegan, Rong Jin</p><p>Abstract: Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its eﬃciency for real-world applications. First, it is computationally expensive and diﬃcult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose “generalized maximum margin clustering” framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It signiﬁcantly improves the computational eﬃciency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the eﬃciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository. 1</p><p>4 0.15536426 <a title="102-tfidf-4" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>5 0.13127518 <a title="102-tfidf-5" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf</p><p>Abstract: We present a local learning approach for clustering. The basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. An optimization problem is formulated such that its solution has the above property. Relaxation and eigen-decomposition are applied to solve this optimization problem. We also brieﬂy investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. Experimental results are provided to validate the effectiveness of the proposed approach. 1</p><p>6 0.11735474 <a title="102-tfidf-6" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>7 0.11409794 <a title="102-tfidf-7" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>8 0.094315477 <a title="102-tfidf-8" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>9 0.091773674 <a title="102-tfidf-9" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>10 0.09073928 <a title="102-tfidf-10" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>11 0.086194195 <a title="102-tfidf-11" href="./nips-2006-Nonnegative_Sparse_PCA.html">149 nips-2006-Nonnegative Sparse PCA</a></p>
<p>12 0.083327971 <a title="102-tfidf-12" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>13 0.082808547 <a title="102-tfidf-13" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>14 0.082207836 <a title="102-tfidf-14" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>15 0.080413289 <a title="102-tfidf-15" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>16 0.073533572 <a title="102-tfidf-16" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>17 0.069163717 <a title="102-tfidf-17" href="./nips-2006-Randomized_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">164 nips-2006-Randomized PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>18 0.061125152 <a title="102-tfidf-18" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>19 0.058008932 <a title="102-tfidf-19" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>20 0.057787701 <a title="102-tfidf-20" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, 0.096), (2, 0.025), (3, 0.29), (4, -0.01), (5, -0.05), (6, -0.003), (7, 0.03), (8, 0.121), (9, 0.097), (10, -0.176), (11, 0.101), (12, 0.068), (13, -0.279), (14, 0.07), (15, -0.07), (16, -0.1), (17, 0.079), (18, -0.095), (19, 0.129), (20, 0.017), (21, 0.069), (22, -0.098), (23, 0.022), (24, 0.005), (25, 0.043), (26, -0.004), (27, -0.051), (28, -0.033), (29, 0.059), (30, -0.129), (31, 0.031), (32, -0.127), (33, -0.014), (34, -0.095), (35, -0.019), (36, 0.067), (37, -0.003), (38, -0.023), (39, -0.0), (40, -0.061), (41, 0.021), (42, -0.053), (43, 0.026), (44, 0.072), (45, -0.113), (46, 0.053), (47, 0.014), (48, -0.097), (49, -0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97839534 <a title="102-lsi-1" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>2 0.86041117 <a title="102-lsi-2" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>3 0.76459241 <a title="102-lsi-3" href="./nips-2006-Sparse_Kernel_Orthonormalized_PLS_for_feature_extraction_in_large_data_sets.html">177 nips-2006-Sparse Kernel Orthonormalized PLS for feature extraction in large data sets</a></p>
<p>Author: Jerónimo Arenas-garcía, Kaare B. Petersen, Lars K. Hansen</p><p>Abstract: In this paper we are presenting a novel multivariate analysis method. Our scheme is based on a novel kernel orthonormalized partial least squares (PLS) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. The algorithm is tested on a benchmark of UCI data sets, and on the analysis of integrated short-time music features for genre prediction. The upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel PLS, and therefore is an appealing method for feature extraction of labelled data. 1</p><p>4 0.75292623 <a title="102-lsi-4" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>Author: Risi Kondor, Tony Jebara</p><p>Abstract: We propose a new method for constructing hyperkenels and deﬁne two promising special cases that can be computed in closed form. These we call the Gaussian and Wishart hyperkernels. The former is especially attractive in that it has an interpretable regularization scheme reminiscent of that of the Gaussian RBF kernel. We discuss how kernel learning can be used not just for improving the performance of classiﬁcation and regression methods, but also as a stand-alone algorithm for dimensionality reduction and relational or metric learning. 1</p><p>5 0.68011343 <a title="102-lsi-5" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>Author: Nicol N. Schraudolph, Simon Günter, S.v.n. Vishwanathan</p><p>Abstract: We introduce two methods to improve convergence of the Kernel Hebbian Algorithm (KHA) for iterative kernel PCA. KHA has a scalar gain parameter which is either held constant or decreased as 1/t, leading to slow convergence. Our KHA/et algorithm accelerates KHA by incorporating the reciprocal of the current estimated eigenvalues as a gain vector. We then derive and apply Stochastic MetaDescent (SMD) to KHA/et; this further speeds convergence by performing gain adaptation in RKHS. Experimental results for kernel PCA and spectral clustering of USPS digits as well as motion capture and image de-noising problems conﬁrm that our methods converge substantially faster than conventional KHA. 1</p><p>6 0.56529707 <a title="102-lsi-6" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>7 0.52882499 <a title="102-lsi-7" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>8 0.51852626 <a title="102-lsi-8" href="./nips-2006-Nonnegative_Sparse_PCA.html">149 nips-2006-Nonnegative Sparse PCA</a></p>
<p>9 0.49189267 <a title="102-lsi-9" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>10 0.49098861 <a title="102-lsi-10" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>11 0.46312046 <a title="102-lsi-11" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>12 0.45726779 <a title="102-lsi-12" href="./nips-2006-A_Kernel_Subspace_Method_by_Stochastic_Realization_for_Learning_Nonlinear_Dynamical_Systems.html">6 nips-2006-A Kernel Subspace Method by Stochastic Realization for Learning Nonlinear Dynamical Systems</a></p>
<p>13 0.44271672 <a title="102-lsi-13" href="./nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</a></p>
<p>14 0.42274618 <a title="102-lsi-14" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>15 0.3791419 <a title="102-lsi-15" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>16 0.36901939 <a title="102-lsi-16" href="./nips-2006-Computation_of_Similarity_Measures_for_Sequential_Data_using_Generalized_Suffix_Trees.html">55 nips-2006-Computation of Similarity Measures for Sequential Data using Generalized Suffix Trees</a></p>
<p>17 0.36179844 <a title="102-lsi-17" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>18 0.355077 <a title="102-lsi-18" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>19 0.34201226 <a title="102-lsi-19" href="./nips-2006-An_Efficient_Method_for_Gradient-Based_Adaptation_of_Hyperparameters_in_SVM_Models.html">28 nips-2006-An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models</a></p>
<p>20 0.33784163 <a title="102-lsi-20" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.074), (3, 0.038), (7, 0.06), (9, 0.089), (20, 0.013), (22, 0.061), (44, 0.055), (57, 0.034), (65, 0.424), (69, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9482736 <a title="102-lda-1" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu</p><p>Abstract: We propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. However, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. We use this hierarchy of histograms to deﬁne elementary kernels which can detect coarse and ﬁne similarities between the objects. We compute through an efﬁcient averaging trick a mixture of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently local and global matches. We propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms.</p><p>same-paper 2 0.91921222 <a title="102-lda-2" href="./nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</a></p>
<p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><p>3 0.91771042 <a title="102-lda-3" href="./nips-2006-No-regret_Algorithms_for_Online_Convex_Programs.html">146 nips-2006-No-regret Algorithms for Online Convex Programs</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Online convex programming has recently emerged as a powerful primitive for designing machine learning algorithms. For example, OCP can be used for learning a linear classiﬁer, dynamically rebalancing a binary search tree, ﬁnding the shortest path in a graph with unknown edge lengths, solving a structured classiﬁcation problem, or ﬁnding a good strategy in an extensive-form game. Several researchers have designed no-regret algorithms for OCP. But, compared to algorithms for special cases of OCP such as learning from expert advice, these algorithms are not very numerous or ﬂexible. In learning from expert advice, one tool which has proved particularly valuable is the correspondence between no-regret algorithms and convex potential functions: by reasoning about these potential functions, researchers have designed algorithms with a wide variety of useful guarantees such as good performance when the target hypothesis is sparse. Until now, there has been no such recipe for the more general OCP problem, and therefore no ability to tune OCP algorithms to take advantage of properties of the problem or data. In this paper we derive a new class of no-regret learning algorithms for OCP. These Lagrangian Hedging algorithms are based on a general class of potential functions, and are a direct generalization of known learning rules like weighted majority and external-regret matching. In addition to proving regret bounds, we demonstrate our algorithms learning to play one-card poker. 1</p><p>4 0.90080529 <a title="102-lda-4" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>Author: Ling Li, Hsuan-tien Lin</p><p>Abstract: We present a reduction framework from ordinal regression to binary classiﬁcation based on extended examples. The framework consists of three steps: extracting extended examples from the original examples, learning a binary classiﬁer on the extended examples with any binary classiﬁcation algorithm, and constructing a ranking rule from the binary classiﬁer. A weighted 0/1 loss of the binary classiﬁer would then bound the mislabeling cost of the ranking rule. Our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classiﬁcation approaches, but also to derive new generalization bounds for ordinal regression from known bounds for binary classiﬁcation. In addition, our framework uniﬁes many existing ordinal regression algorithms, such as perceptron ranking and support vector ordinal regression. When compared empirically on benchmark data sets, some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms, which demonstrates the usefulness of our framework. 1</p><p>5 0.88566422 <a title="102-lda-5" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>Author: Ashutosh Saxena, Justin Driemeyer, Justin Kearns, Andrew Y. Ng</p><p>Abstract: We consider the problem of grasping novel objects, speciﬁcally ones that are being seen for the ﬁrst time through vision. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Instead it predicts, directly as a function of the images, a point at which to grasp the object. Our algorithm is trained via supervised learning, using synthetic images for the training set. We demonstrate on a robotic manipulation platform that this approach successfully grasps a wide variety of objects, such as wine glasses, duct tape, markers, a translucent box, jugs, knife-cutters, cellphones, keys, screwdrivers, staplers, toothbrushes, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. 1</p><p>6 0.64024359 <a title="102-lda-6" href="./nips-2006-implicit_Online_Learning_with_Kernels.html">203 nips-2006-implicit Online Learning with Kernels</a></p>
<p>7 0.60625213 <a title="102-lda-7" href="./nips-2006-Convex_Repeated_Games_and_Fenchel_Duality.html">61 nips-2006-Convex Repeated Games and Fenchel Duality</a></p>
<p>8 0.59952635 <a title="102-lda-8" href="./nips-2006-Fast_Iterative_Kernel_PCA.html">79 nips-2006-Fast Iterative Kernel PCA</a></p>
<p>9 0.58854198 <a title="102-lda-9" href="./nips-2006-An_Approach_to_Bounded_Rationality.html">26 nips-2006-An Approach to Bounded Rationality</a></p>
<p>10 0.57571578 <a title="102-lda-10" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>11 0.57285929 <a title="102-lda-11" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>12 0.56834459 <a title="102-lda-12" href="./nips-2006-Gaussian_and_Wishart_Hyperkernels.html">82 nips-2006-Gaussian and Wishart Hyperkernels</a></p>
<p>13 0.56371635 <a title="102-lda-13" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>14 0.5612542 <a title="102-lda-14" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>15 0.55770671 <a title="102-lda-15" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>16 0.55689567 <a title="102-lda-16" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>17 0.55643845 <a title="102-lda-17" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>18 0.55287659 <a title="102-lda-18" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>19 0.55114609 <a title="102-lda-19" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>20 0.54349756 <a title="102-lda-20" href="./nips-2006-Predicting_spike_times_from_subthreshold_dynamics_of_a_neuron.html">162 nips-2006-Predicting spike times from subthreshold dynamics of a neuron</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
