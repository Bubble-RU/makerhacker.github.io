<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2006-Blind Motion Deblurring Using Image Statistics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-45" href="#">nips2006-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 nips-2006-Blind Motion Deblurring Using Image Statistics</h1>
<br/><p>Source: <a title="nips-2006-45-pdf" href="http://papers.nips.cc/paper/3085-blind-motion-deblurring-using-image-statistics.pdf">pdf</a></p><p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>Reference: <a title="nips-2006-45-reference" href="../nips2006_reference/nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Blind Motion Deblurring Using Image Statistics Anat Levin∗ School of Computer Science and Engineering The Hebrew University of Jerusalem  Abstract We address the problem of blind motion deblurring from a single image, caused by a few moving objects. [sent-1, score-0.539]
</p><p>2 In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. [sent-2, score-0.603]
</p><p>3 Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. [sent-3, score-0.876]
</p><p>4 However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. [sent-4, score-0.913]
</p><p>5 Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. [sent-5, score-0.605]
</p><p>6 Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. [sent-7, score-0.666]
</p><p>7 This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. [sent-8, score-0.752]
</p><p>8 1 Introduction Motion blur is the result of the relative motion between the camera and the scene during image exposure time. [sent-11, score-0.972]
</p><p>9 As blurring can signiﬁcantly degrade the visual quality of images, photographers and camera manufactures are frequently searching for methods to limit the phenomenon. [sent-13, score-0.441]
</p><p>10 One solution that reduces the degree of blur is to capture images using shorter exposure intervals. [sent-14, score-0.653]
</p><p>11 An alternative approach is to try to remove the blur off-line. [sent-16, score-0.602]
</p><p>12 Blur is usually modeled as a linear convolution of an image with a blurring kernel, also known as the point spread function (or PSF). [sent-17, score-0.613]
</p><p>13 Image deconvolution is the process of recovering the unknown image from its blurred version, given a blurring kernel. [sent-18, score-1.053]
</p><p>14 In most situations, however, the blurring kernel is unknown as well, and the task also requires the estimation of the underlying blurring kernel. [sent-19, score-0.877]
</p><p>15 Most of the existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. [sent-21, score-0.876]
</p><p>16 While the uniform blur assumption is valid for a restricted set of camera motions, it’s usually far from being satisfying when the scene contains several objects moving independently. [sent-22, score-0.699]
</p><p>17 In this work, however, we would like to address blind multiple motions deblurring using a single frame. [sent-24, score-0.481]
</p><p>18 The ﬁrst assumption is that the image consists of a small number of blurring layers with the same blurring kernel within each layer. [sent-26, score-1.17]
</p><p>19 Most of the examples in this paper include a single blurred object and an unblurred background. [sent-27, score-0.446]
</p><p>20 As a result, within each blurred layer, the blurring kernel is a simple one dimensional box ﬁlter, so that the only unknown parameters are the blur direction and the width of the blur kernel. [sent-31, score-1.996]
</p><p>21 Deblurring different motions requires the segmentation of the image into layers with different blurs as well as the reconstruction of the blurring kernel in each layer. [sent-32, score-1.136]
</p><p>22 While image segmentation is an active and challenging research area which utilizes various low level and high level cues, the only segmentation cue used in this work is the degree of blur. [sent-33, score-0.485]
</p><p>23 In order to discriminate different degrees of blur we use the statistics of natural images. [sent-34, score-0.624]
</p><p>24 Our observation is that statistics of derivatives responses in images are signiﬁcantly changed as a result of blur, and that the expected statistics under different blurring kernels can be modeled. [sent-35, score-0.728]
</p><p>25 Given a model of the derivatives statistics under different blurring kernels our algorithm searches for a mixture model that will best describe the distribution observed in the input image. [sent-36, score-0.67]
</p><p>26 This results in a set of 2 (or some other small number) blurring kernels that were used in the image. [sent-37, score-0.44]
</p><p>27 In order to segment the image into blurring layers we measure the likelihood of the derivatives in small image windows, under each model. [sent-38, score-1.124]
</p><p>28 Research about blind deconvolution given a single image, usually concentrate at cases in which the image is uniformly blurred. [sent-42, score-0.503]
</p><p>29 Early deblurring methods treated blurs that can be characterized by a regular pattern of zeros in the frequency domain such as box ﬁlter blurs [26]. [sent-44, score-0.713]
</p><p>30 Even in the noise free case, box ﬁlter blurs can not be identiﬁed in the frequency domain if different blurs are present. [sent-46, score-0.439]
</p><p>31 In a creative recent research which inspired our approach, Fergus et al [12] use the statistics of natural images to estimate the blurring kernel (again, assuming a uniform blur). [sent-49, score-0.559]
</p><p>32 Their approach searches for the max-marginal blurring kernel and a deblurred image, using a prior on derivatives distribution in an unblurred image. [sent-50, score-0.862]
</p><p>33 They address more than box ﬁlters, and present impressing reconstructions of complex blurring kernels. [sent-51, score-0.52]
</p><p>34 As the edge’s scale provides some measure of blur this is used for segmenting an image into a focus and out of focus layers. [sent-55, score-0.799]
</p><p>35 In [4], blind restoration of spatially-varying blur was studied in the case of astronomical images, which have statistics quite different from the natural scenes addressed in this paper. [sent-57, score-0.794]
</p><p>36 As a scene point focus is a function of its depth, the relative blur is used to estimate depth information. [sent-62, score-0.665]
</p><p>37 2 Image statistics and blurring Figure 1(a) presents an image of an outdoor scene, with a passing bus. [sent-65, score-0.65]
</p><p>38 The bus is blurred horizontally as a result of the bus motion. [sent-66, score-0.484]
</p><p>39 In ﬁg 1(b) we plot the log histogram of the vertical derivatives of this image, and the horizontal derivatives within the blurred area (marked with a rectangle). [sent-67, score-0.831]
</p><p>40 As can be  0  0  0  vertical horizontal  −1  Input 5 taps blur 21 taps blur  −2  −2  horizontal blurred vertical  −1  −2 −3  −4  −3 −4 −4  −6  −5 −5 −6  −8  −6 −7 −10  −7  −8  −9 −0. [sent-68, score-1.769]
</p><p>41 (b) Horizontal derivatives within the blurred region versus vertical derivatives in the entire image. [sent-100, score-0.728]
</p><p>42 (d) Horizontal derivatives within the blurred region matched with blurred verticals (4 tap blur). [sent-102, score-0.728]
</p><p>43 seen, the blur changes the shape of the histogram signiﬁcantly. [sent-103, score-0.619]
</p><p>44 This suggests that the statistics of derivative ﬁlters responses can be used for detecting blurred image areas. [sent-104, score-0.509]
</p><p>45 How does the degree of blur affects the derivatives histogram? [sent-105, score-0.752]
</p><p>46 We convolve the image with the kernels f k (where k runs from 1 to 30) and compute the vertical derivatives distributions: T pk ∝ hist(dy ∗ fk ∗ I)  (1)  T  where dy = [1 − 1] . [sent-108, score-0.624]
</p><p>47 As the size of the blurring kernel changes the derivatives distribution, we would also like to use the histograms for determining the degree of blur. [sent-110, score-0.706]
</p><p>48 For example, as illustrated in ﬁg 1(d), we can match the distribution of vertical derivatives in the blurred area, and p 4 , the distribution of horizontal derivatives after blurring with a 4 tap kernel. [sent-111, score-1.191]
</p><p>49 1 Identifying blur using image statistics Given an image, the direction of motion blur can be selected as the direction with minimal derivatives variation, as in [28]. [sent-113, score-1.705]
</p><p>50 For the simplicity of the derivation we will assume here that the motion direction is horizontal, and that the image contains a single blurred object plus an unblurred background. [sent-114, score-0.762]
</p><p>51 Our goal is to determine the size of the blur kernel. [sent-115, score-0.574]
</p><p>52 That is, to recover the ﬁlter f k which is responsible for the blur observed in the image. [sent-116, score-0.574]
</p><p>53 Therefore, without segmenting the blurred areas there is no single blurring model p k that will describe the observed histogram. [sent-119, score-0.724]
</p><p>54 We deﬁne the log-likelihood of the derivatives in a window with respect to each of the blurring models as: k (i)  =  log pk (Ix (j))  (2)  j∈Wi  Where Ix (j) is the horizontal derivative in pixel j, and W i is a window around pixel i. [sent-121, score-0.811]
</p><p>55 On the other hand, uniform areas receive the highest likelihoods from wide blur kernels (since the derivatives distribution for wide kernels is more concentrated around zero, as can be observed in ﬁgure 1(c)). [sent-124, score-0.883]
</p><p>56 When the image consists of large uniform areas, this bias the likelihood toward wider blur kernels. [sent-125, score-0.816]
</p><p>57 In order to make our model consistent, when building the blurred distribution models p k (eq 1), we also take into account only pixels within a window around a vertical edge. [sent-127, score-0.441]
</p><p>58 2 Segmenting blur layers Once the blurring kernel f k has been found, we can use it to deconvolve the image, as in ﬁg 2(b). [sent-130, score-1.18]
</p><p>59 While this signiﬁcantly improves the image in the blurred areas, serious artifacts are observed in the background. [sent-131, score-0.49]
</p><p>60 Therefore, in addition to recovering the blurring kernel, we need to segment the image into blurred and unblurred layers. [sent-132, score-1.126]
</p><p>61 The ﬁnal restorated image is computed as: R(i) = x(i)I −fk (i) + (1 − x(i))I(i) (7)  3 Results To compute a deconvolved image I −fk given the blurring kernel, we follow [12] in using the matlab implementation (deconvlucy) of the Richardson-Lucy deconvolution algorithm [23, 18]. [sent-143, score-1.035]
</p><p>62 For the doll example the image was segmented into 3 blurring layers. [sent-145, score-0.679]
</p><p>63 To determine the blur direction in those images we select the direction with minimal derivatives variation, as in [28]. [sent-148, score-0.872]
</p><p>64 For each image we show what happens if the segmentation is ignored and the entire image is deconvolved with the selected kernel (for the doll case the wider kernel is shown). [sent-150, score-0.862]
</p><p>65 In comparison, the third row presents the restorated images computed from eq 7 using the blurring layers segmentation. [sent-152, score-0.641]
</p><p>66 (e)Segmentation contour  The recovered blur sizes for those examples were 12 pixels for the bicycles image and 4 pixels for the bus. [sent-165, score-0.978]
</p><p>67 For the doll image a 9 pixels blur was identiﬁed in the skirt segment and a 2 pixels blur in the doll head. [sent-166, score-1.611]
</p><p>68 We note that while recovering big degrees of blur as in the bicycles example is visually more impressing, discriminating small degrees of blur as in the bus example is more challenging from the statistical aspect. [sent-167, score-1.409]
</p><p>69 This is because the derivatives distributions in the case of small blurs are much more similar to the distributions of unblurred images. [sent-168, score-0.544]
</p><p>70 For the bus image the size of the blur kernel found by our algorithm was 4 pixels. [sent-169, score-0.945]
</p><p>71 Segmentation: As demonstrated in ﬁg 2(b) deconvolving the entire image with the same kernel damages the unblurred parts. [sent-175, score-0.522]
</p><p>72 One obvious solution is to divide the image into regions and match a separate blur kernel to each region. [sent-176, score-0.848]
</p><p>73 While likelihood measure based on a big window is more reliable, such a window might cover regions from different blurring layers. [sent-180, score-0.53]
</p><p>74 Another alternative is to brake the image into segments using an unsupervised segmentation algorithm, and match a kernel to each segment. [sent-181, score-0.432]
</p><p>75 The fact that blur changes the derivatives distributions also suggests that it might be captured as a kind of texture cue. [sent-182, score-0.814]
</p><p>76 However, as this is an unsupervised segmentation process which does not take into account the grouping goal, it’s hard to expect it to yield exactly the blurred layers. [sent-186, score-0.421]
</p><p>77 The output over-segments blur layers, while merging parts of blurred and unblurred objects. [sent-188, score-1.02]
</p><p>78 To do that independently of the segmentation, we manually segmented the bus and applied the matlab blind deconvolution function (deconvblind), initialized with a 1 × 7 box kernel. [sent-195, score-0.495]
</p><p>79 Yet, the histogram structure of different images is not identical, and we found that trying to deblur one image using the statistics of a different image doesn’t work that well. [sent-199, score-0.516]
</p><p>80 For example, ﬁgure 5 shows the result of deblurring the bus image using the bicycles image statistics. [sent-200, score-0.814]
</p><p>81 The selected blur in this case was a 6-tap kernel, but deblurring the image with this kernel introduces artifacts. [sent-201, score-1.122]
</p><p>82 Our solution was to work on each image using the vertical derivatives histograms from the same image. [sent-203, score-0.497]
</p><p>83 This isn’t an optimal solution as when the image is blurred horizontally some of the vertical derivatives are degraded as well. [sent-204, score-0.737]
</p><p>84 (a)  (b)  (c)  (d)  Figure 5: Deblurring the bus image using the bicycles image statistics. [sent-206, score-0.54]
</p><p>85 One failure source is blurs which can’t be described as a box ﬁlter, or failures in identifying the blur direction. [sent-213, score-0.848]
</p><p>86 Even when this isn’t the case, the algorithm may fail to identify the correct blur size or it may not infer the correct segmentation. [sent-214, score-0.574]
</p><p>87 The bushes area consists of many small derivatives which are explained better by a small blur model than by a no-blur model. [sent-217, score-0.816]
</p><p>88 As a result the algorithm selected a 6-pixels blur model. [sent-219, score-0.574]
</p><p>89 This model might increase the likelihood of the bushes texture and the noise on the road, but it doesn’t remove the blur of the car. [sent-220, score-0.687]
</p><p>90 4 Discussion This paper addresses the problem of blind motion deconvolution without assuming that the entire image undergone the same blur. [sent-227, score-0.606]
</p><p>91 Thus, in addition to recovering an unknown blur kernel, we segment the image into layers with different blurs. [sent-228, score-0.953]
</p><p>92 We treat this highly challenging task using a surprisingly simple approach, relying on the derivatives distribution in blurred images. [sent-229, score-0.441]
</p><p>93 We model the expected derivatives distributions under different degrees of blur, and those distributions are used for detecting different blurs in image windows. [sent-230, score-0.577]
</p><p>94 The box ﬁlters model used in this work is deﬁnitely limiting, and as pointed out by [12, 6], many blurring patterns observed in real images are more complex. [sent-231, score-0.524]
</p><p>95 Stronger models might enable us to identify a wider class of blurring kernels rather than just box ﬁlters. [sent-233, score-0.537]
</p><p>96 Particularly, they could provide  a better strategy for identifying the blur direction. [sent-234, score-0.592]
</p><p>97 In future work, it will also be interesting to try to detect different blurs without assuming a small number of blurring layers. [sent-236, score-0.61]
</p><p>98 This will require estimating the blurs in the image in a continues way, and might also provide a depth from focus algorithm that will work on a single image. [sent-237, score-0.422]
</p><p>99 Local scale control for edge detection and blur estimation. [sent-294, score-0.574]
</p><p>100 Simultaneous image formation and motion blur restoration via multiple capture. [sent-334, score-0.904]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blur', 0.574), ('blurring', 0.399), ('deblurring', 0.274), ('blurred', 0.263), ('image', 0.195), ('blurs', 0.183), ('unblurred', 0.183), ('derivatives', 0.178), ('deconvolution', 0.146), ('blind', 0.143), ('segmentation', 0.136), ('layers', 0.098), ('bus', 0.097), ('motion', 0.087), ('kernel', 0.079), ('horizontal', 0.075), ('vertical', 0.074), ('box', 0.073), ('doll', 0.066), ('fk', 0.058), ('window', 0.054), ('deconvolved', 0.053), ('bicycles', 0.053), ('images', 0.052), ('pixels', 0.05), ('lter', 0.05), ('recovering', 0.05), ('histograms', 0.05), ('restoration', 0.048), ('scene', 0.047), ('motions', 0.046), ('bushes', 0.046), ('histogram', 0.045), ('texture', 0.044), ('depth', 0.044), ('camera', 0.042), ('kernels', 0.041), ('levin', 0.04), ('segment', 0.036), ('entire', 0.035), ('eq', 0.035), ('direction', 0.034), ('areas', 0.032), ('artifacts', 0.032), ('recovered', 0.031), ('convolve', 0.03), ('deconvolve', 0.03), ('deconvolving', 0.03), ('elder', 0.03), ('impressing', 0.03), ('matting', 0.03), ('restorated', 0.03), ('taps', 0.03), ('segmenting', 0.03), ('pk', 0.029), ('statistics', 0.029), ('try', 0.028), ('presents', 0.027), ('windows', 0.027), ('horizontally', 0.027), ('optics', 0.027), ('exposure', 0.027), ('isn', 0.027), ('stronger', 0.025), ('contour', 0.025), ('eccv', 0.025), ('wider', 0.024), ('pami', 0.024), ('tap', 0.024), ('concentrates', 0.024), ('ections', 0.024), ('ix', 0.024), ('seam', 0.024), ('cvpr', 0.024), ('lters', 0.024), ('likelihood', 0.023), ('searches', 0.023), ('unsupervised', 0.022), ('derivative', 0.022), ('degrees', 0.021), ('scanning', 0.02), ('eij', 0.02), ('energy', 0.02), ('discriminating', 0.019), ('dy', 0.019), ('usually', 0.019), ('segmented', 0.019), ('velocity', 0.019), ('fergus', 0.019), ('identifying', 0.018), ('smoothness', 0.018), ('captured', 0.018), ('align', 0.018), ('doesn', 0.018), ('siggraph', 0.018), ('address', 0.018), ('area', 0.018), ('moving', 0.017), ('matlab', 0.017), ('likelihoods', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="45-tfidf-1" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>2 0.20125026 <a title="45-tfidf-2" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>3 0.1813284 <a title="45-tfidf-3" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>Author: Eizaburo Doi, Michael S. Lewicki</p><p>Abstract: Efﬁcient coding models predict that the optimal code for natural images is a population of oriented Gabor receptive ﬁelds. These results match response properties of neurons in primary visual cortex, but not those in the retina. Does the retina use an optimal code, and if so, what is it optimized for? Previous theories of retinal coding have assumed that the goal is to encode the maximal amount of information about the sensory signal. However, the image sampled by retinal photoreceptors is degraded both by the optics of the eye and by the photoreceptor noise. Therefore, de-blurring and de-noising of the retinal signal should be important aspects of retinal coding. Furthermore, the ideal retinal code should be robust to neural noise and make optimal use of all available neurons. Here we present a theoretical framework to derive codes that simultaneously satisfy all of these desiderata. When optimized for natural images, the model yields ﬁlters that show strong similarities to retinal ganglion cell (RGC) receptive ﬁelds. Importantly, the characteristics of receptive ﬁelds vary with retinal eccentricities where the optical blur and the number of RGCs are signiﬁcantly different. The proposed model provides a uniﬁed account of retinal coding, and more generally, it may be viewed as an extension of the Wiener ﬁlter with an arbitrary number of noisy units. 1</p><p>4 0.11461881 <a title="45-tfidf-4" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>Author: Dong S. Cheng, Vittorio Murino, Mário Figueiredo</p><p>Abstract: This paper proposes a new approach to model-based clustering under prior knowledge. The proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as ﬁnite mixture learning under a grouping prior. To estimate the parameters of the proposed model, we derive a (generalized) EM algorithm with a closed-form E-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require Gibbs sampling or suboptimal shortcuts. We show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature Markov random ﬁeld priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efﬁcient way. Finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes. 1</p><p>5 0.11016071 <a title="45-tfidf-5" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>6 0.10235995 <a title="45-tfidf-6" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>7 0.071065664 <a title="45-tfidf-7" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>8 0.068514608 <a title="45-tfidf-8" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>9 0.064277694 <a title="45-tfidf-9" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>10 0.063361056 <a title="45-tfidf-10" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>11 0.063220538 <a title="45-tfidf-11" href="./nips-2006-Learning_Motion_Style_Synthesis_from_Perceptual_Observations.html">111 nips-2006-Learning Motion Style Synthesis from Perceptual Observations</a></p>
<p>12 0.05839505 <a title="45-tfidf-12" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<p>13 0.055889603 <a title="45-tfidf-13" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>14 0.055325158 <a title="45-tfidf-14" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>15 0.053437091 <a title="45-tfidf-15" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>16 0.052747324 <a title="45-tfidf-16" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>17 0.052056193 <a title="45-tfidf-17" href="./nips-2006-Blind_source_separation_for_over-determined_delayed_mixtures.html">46 nips-2006-Blind source separation for over-determined delayed mixtures</a></p>
<p>18 0.046747185 <a title="45-tfidf-18" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>19 0.046736896 <a title="45-tfidf-19" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>20 0.046574254 <a title="45-tfidf-20" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.146), (1, 0.01), (2, 0.151), (3, -0.02), (4, 0.015), (5, -0.076), (6, -0.142), (7, -0.114), (8, 0.016), (9, 0.113), (10, 0.13), (11, 0.008), (12, -0.042), (13, -0.039), (14, 0.081), (15, 0.123), (16, -0.03), (17, -0.036), (18, 0.007), (19, 0.08), (20, -0.16), (21, 0.025), (22, -0.023), (23, -0.052), (24, 0.127), (25, 0.099), (26, 0.0), (27, 0.011), (28, 0.092), (29, 0.179), (30, -0.041), (31, -0.012), (32, 0.008), (33, -0.017), (34, 0.072), (35, -0.006), (36, -0.066), (37, 0.006), (38, -0.005), (39, -0.045), (40, -0.099), (41, -0.098), (42, 0.138), (43, -0.025), (44, 0.09), (45, 0.077), (46, 0.123), (47, -0.262), (48, -0.02), (49, 0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94240195 <a title="45-lsi-1" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>2 0.6457817 <a title="45-lsi-2" href="./nips-2006-A_Theory_of_Retinal_Population_Coding.html">16 nips-2006-A Theory of Retinal Population Coding</a></p>
<p>Author: Eizaburo Doi, Michael S. Lewicki</p><p>Abstract: Efﬁcient coding models predict that the optimal code for natural images is a population of oriented Gabor receptive ﬁelds. These results match response properties of neurons in primary visual cortex, but not those in the retina. Does the retina use an optimal code, and if so, what is it optimized for? Previous theories of retinal coding have assumed that the goal is to encode the maximal amount of information about the sensory signal. However, the image sampled by retinal photoreceptors is degraded both by the optics of the eye and by the photoreceptor noise. Therefore, de-blurring and de-noising of the retinal signal should be important aspects of retinal coding. Furthermore, the ideal retinal code should be robust to neural noise and make optimal use of all available neurons. Here we present a theoretical framework to derive codes that simultaneously satisfy all of these desiderata. When optimized for natural images, the model yields ﬁlters that show strong similarities to retinal ganglion cell (RGC) receptive ﬁelds. Importantly, the characteristics of receptive ﬁelds vary with retinal eccentricities where the optical blur and the number of RGCs are signiﬁcantly different. The proposed model provides a uniﬁed account of retinal coding, and more generally, it may be viewed as an extension of the Wiener ﬁlter with an arbitrary number of noisy units. 1</p><p>3 0.58996183 <a title="45-lsi-3" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>4 0.57600451 <a title="45-lsi-4" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>Author: Lyndsey C. Pickup, David P. Capel, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: This paper develops a multi-frame image super-resolution approach from a Bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. In Tipping and Bishop’s Bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. By integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. In addition to the motion model used by Tipping and Bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. We show results on real and synthetic datasets to illustrate the efﬁcacy of this approach.</p><p>5 0.57081842 <a title="45-lsi-5" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>Author: Anitha Kannan, John Winn, Carsten Rother</p><p>Abstract: Patch-based appearance models are used in a wide range of computer vision applications. To learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand. In the jigsaw model presented here, the shape, size and appearance of patches are learned automatically from the repeated structures in a set of training images. By learning such irregularly shaped ‘jigsaw pieces’, we are able to discover both the shape and the appearance of object parts without supervision. When applied to face images, for example, the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes, noses, eyebrows and cheeks, to name a few. We conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection. This enables parts of similar appearance but different shapes to be distinguished; for example, while foreheads and cheeks are both skin colored, they have markedly different shapes. 1</p><p>6 0.47941953 <a title="45-lsi-6" href="./nips-2006-Statistical_Modeling_of_Images_with_Fields_of_Gaussian_Scale_Mixtures.html">182 nips-2006-Statistical Modeling of Images with Fields of Gaussian Scale Mixtures</a></p>
<p>7 0.4732106 <a title="45-lsi-7" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>8 0.42249638 <a title="45-lsi-8" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>9 0.39200264 <a title="45-lsi-9" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>10 0.38399464 <a title="45-lsi-10" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>11 0.38226452 <a title="45-lsi-11" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>12 0.35897765 <a title="45-lsi-12" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>13 0.32560763 <a title="45-lsi-13" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>14 0.28973439 <a title="45-lsi-14" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>15 0.28973341 <a title="45-lsi-15" href="./nips-2006-Non-rigid_point_set_registration%3A_Coherent_Point_Drift.html">147 nips-2006-Non-rigid point set registration: Coherent Point Drift</a></p>
<p>16 0.27733487 <a title="45-lsi-16" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<p>17 0.27665037 <a title="45-lsi-17" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>18 0.27626741 <a title="45-lsi-18" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>19 0.26602891 <a title="45-lsi-19" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>20 0.25374356 <a title="45-lsi-20" href="./nips-2006-Analysis_of_Contour_Motions.html">31 nips-2006-Analysis of Contour Motions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.09), (3, 0.038), (7, 0.052), (8, 0.031), (9, 0.027), (15, 0.298), (20, 0.03), (21, 0.013), (22, 0.029), (44, 0.052), (57, 0.101), (65, 0.056), (69, 0.063), (71, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77292907 <a title="45-lda-1" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>Author: Anat Levin</p><p>Abstract: We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative ﬁlters in images are signiﬁcantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box ﬁlter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.</p><p>2 0.63552219 <a title="45-lda-2" href="./nips-2006-Information_Bottleneck_Optimization_and_Independent_Component_Extraction_with_Spiking_Neurons.html">99 nips-2006-Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons</a></p>
<p>Author: Stefan Klampfl, Wolfgang Maass, Robert A. Legenstein</p><p>Abstract: The extraction of statistically independent components from high-dimensional multi-sensory input streams is assumed to be an essential component of sensory processing in the brain. Such independent component analysis (or blind source separation) could provide a less redundant representation of information about the external world. Another powerful processing strategy is to extract preferentially those components from high-dimensional input streams that are related to other information sources, such as internal predictions or proprioceptive feedback. This strategy allows the optimization of internal representation according to the information bottleneck method. However, concrete learning rules that implement these general unsupervised learning principles for spiking neurons are still missing. We show how both information bottleneck optimization and the extraction of independent components can in principle be implemented with stochastically spiking neurons with refractoriness. The new learning rule that achieves this is derived from abstract information optimization principles. 1</p><p>3 0.5058974 <a title="45-lda-3" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: Pyramid intersection is an efﬁcient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. The matching similarity is computed in linear time and forms a Mercer kernel. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classiﬁer, our approach achieves improved object recognition results over a state-of-the-art set kernel. 1</p><p>4 0.50588238 <a title="45-lda-4" href="./nips-2006-Modeling_Human_Motion_Using_Binary_Latent_Variables.html">134 nips-2006-Modeling Human Motion Using Binary Latent Variables</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. After training, the model ﬁnds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line ﬁlling in of data lost during motion capture. Website: http://www.cs.toronto.edu/∼gwtaylor/publications/nips2006mhmublv/</p><p>5 0.50067896 <a title="45-lda-5" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>Author: Wolf Kienzle, Felix A. Wichmann, Matthias O. Franz, Bernhard Schölkopf</p><p>Abstract: This paper addresses the bottom-up inﬂuence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear ﬁlters, e.g., Gabor or Difference-of-Gaussians ﬁlters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end ﬁlters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justiﬁed. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the ﬁeld that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble ﬁndings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive ﬁelds in the early human visual system. 1</p><p>6 0.49720839 <a title="45-lda-6" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>7 0.49272421 <a title="45-lda-7" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>8 0.48932388 <a title="45-lda-8" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>9 0.48932093 <a title="45-lda-9" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>10 0.488848 <a title="45-lda-10" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>11 0.48615387 <a title="45-lda-11" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>12 0.48595279 <a title="45-lda-12" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>13 0.48522612 <a title="45-lda-13" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>14 0.48496726 <a title="45-lda-14" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>15 0.48366603 <a title="45-lda-15" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>16 0.48303258 <a title="45-lda-16" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>17 0.48239014 <a title="45-lda-17" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>18 0.47992966 <a title="45-lda-18" href="./nips-2006-Learning_to_Rank_with_Nonsmooth_Cost_Functions.html">119 nips-2006-Learning to Rank with Nonsmooth Cost Functions</a></p>
<p>19 0.47908813 <a title="45-lda-19" href="./nips-2006-Max-margin_classification_of_incomplete_data.html">130 nips-2006-Max-margin classification of incomplete data</a></p>
<p>20 0.47854683 <a title="45-lda-20" href="./nips-2006-Bayesian_Model_Scoring_in_Markov_Random_Fields.html">43 nips-2006-Bayesian Model Scoring in Markov Random Fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
