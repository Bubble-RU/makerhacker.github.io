<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-173" href="#">nips2006-173</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</h1>
<br/><p>Source: <a title="nips-2006-173-pdf" href="http://papers.nips.cc/paper/2982-shifting-one-inclusion-mistake-bounds-and-tight-multiclass-expected-risk-bounds.pdf">pdf</a></p><p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><p>Reference: <a title="nips-2006-173-reference" href="../nips2006_reference/nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Under the prediction model of learning, a prediction strategy is presented with an i. [sent-14, score-0.338]
</p><p>2 achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. [sent-19, score-0.247]
</p><p>3 The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. [sent-20, score-0.189]
</p><p>4 The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. [sent-22, score-0.318]
</p><p>5 Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. [sent-23, score-0.408]
</p><p>6 This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. [sent-24, score-0.183]
</p><p>7 The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. [sent-25, score-0.299]
</p><p>8 On the one hand the derived VC(F)/n upper-bound on the worst-case expected risk of the one-inclusion strategy learning from F ⊆ {0, 1}X improved on the PAC-based previous-best by an order of log n. [sent-28, score-0.23]
</p><p>9 At the same time Haussler [3] introduced the idea of shifting sub-  sets of the n-cube down around the origin—an idea previously developed in Combinatorics—as a powerful tool for learning-theoretic results. [sent-30, score-0.275]
</p><p>10 In particular, shifting admitted deeply insightful proofs of Sauer’s Lemma and a VC-dimension bound on the density of the one-inclusion graph—the key result needed for the one-inclusion strategy’s expected risk bound. [sent-31, score-0.57]
</p><p>11 Recently shifting has impacted on work towards the sample compressibility conjecture of [7] e. [sent-32, score-0.368]
</p><p>12 Here we continue to study the one-inclusion graph—the natural graph structure induced by a subset of the n-cube—and its related prediction strategy under the lens of shifting. [sent-35, score-0.381]
</p><p>13 After the necessary background, we develop the technique of shatter-invariant shifting in Section 3. [sent-36, score-0.275]
</p><p>14 While a subset’s VC-dimension cannot be increased by shifting, shatter-invariant shifting guarantees a ﬁnite sequence of shifts to a ﬁxed-point under which the shattering of a chosen set remains invariant, thus preserving VC-dimension throughout. [sent-37, score-0.406]
</p><p>15 In Section 4 we apply a group-theoretic symmetrization to tighten the mistake bound—the worst-case expected risk bound—of the deterministic (randomized) oned d d d inclusion strategy from d/n to Dn /n (Dn /n), where Dn < d for all n, d. [sent-38, score-0.517]
</p><p>16 The derived Dn density bound positively resolves a conjecture of Kuzmin & Warmuth which was suggested as a step towards a correctness proof of the Peeling compression scheme [5]. [sent-39, score-0.426]
</p><p>17 Finally we generalize the prediction model, the one-inclusion strategy and its bounds from binary to k-class learning in Section 5. [sent-40, score-0.273]
</p><p>18 Where ΨG -dim (F) and ΨP -dim (F) denote the Graph and Pollard dimensions of F, the best bound on expected risk for k ∈ N to-date is O(α log α) for α = ΨG -dim (F) /n, for consistent learners [8, 1, 2, 4]. [sent-41, score-0.204]
</p><p>19 For large n this is O(log nΨG -dim (F) /n); we derive an improved bound of ΨP -dim (F) /n which we show is at most a O(log k) factor from optimal. [sent-42, score-0.094]
</p><p>20 Thus, as in the binary case, exploiting class structure enables signiﬁcantly better bounds on expected risk for multiclass prediction. [sent-43, score-0.229]
</p><p>21 We write the density of graph G = (V, E) as dens (G) = |E|/|V |, the indicator of A as 1 [A], and ∃! [sent-51, score-0.515]
</p><p>22 1  The prediction model of learning  We begin with the basic setup of [4]. [sent-54, score-0.109]
</p><p>23 For notational convenience we write sam (x, f ) = ((x1 , f (x1 )) , . [sent-56, score-0.118]
</p><p>24 A prediction strategy is a mapping of the form Q : n>1 (X × {0, 1}) Deﬁnition 2. [sent-61, score-0.229]
</p><p>25 1 The prediction model of learning concerns the following scenario. [sent-62, score-0.109]
</p><p>26 Given full knowledge of strategy Q, an adversary picks a distribution P on X and concept f ∈ F so as to maximize i. [sent-63, score-0.163]
</p><p>27 Thus the measure of performance is the worst-case expected risk ˆ MQ,F (n) = sup sup EX∼P n [1 [Q (sam ((X1 , . [sent-70, score-0.199]
</p><p>28 f ∈F P  ˆ A mistake bound for Q with respect to F is an upper-bound on MQ,F . [sent-74, score-0.29]
</p><p>29 In contrast to Valiant’s PAC model, the prediction learning model is not interested in approximating f given an f -labeled sample, but instead in predicting f (Xn ) with small worst-case probability of erring. [sent-75, score-0.109]
</p><p>30 1 [4]) For any n > 1, concept class F and prediction strategy Q, 1 ˆ MQ,F (n) ≤ sup sup 1 Q sam xg(1) , . [sent-79, score-0.5]
</p><p>31 ˆ ˆ A permutation mistake bound for Q with respect to F is an upper-bound on MQ,F . [sent-84, score-0.311]
</p><p>32 3 The Vapnik-Chervonenkis dimension of concept class F is deﬁned as VC(F) = sup {n | ∃x ∈ X n , Πx (F) = {0, 1}n }. [sent-94, score-0.098]
</p><p>33 We next describe three important translation families used in this paper. [sent-120, score-0.075]
</p><p>34 3  The one-inclusion prediction strategy  A subset of the n-cube—the projection of some F—induces the one-inclusion graph, which underlies a natural prediction strategy. [sent-130, score-0.373]
</p><p>35 , k}n is the undirected graph with vertex-set V and hyperedge-set E of maximal (with respect to inclusion) sets of pairwise hamming-1 separated vertices. [sent-139, score-0.095]
</p><p>36 Algorithm 1 The deterministic multiclass one-inclusion prediction strategy Q G,F Given: F ⊆ {0, . [sent-140, score-0.363]
</p><p>37 , xn−1 ), f ) ∈ (X × {0, 1}) Returns: a prediction of f (xn )  n−1  , xn ∈ X  V ←− Πx (F) ; G ←− G (V ) ; → − G ←− orient G to minimize the maximum outdegree ; Vspace ←− {v ∈ V | v1 = f (x1 ), . [sent-146, score-0.258]
</p><p>38 , vn−1 = f (xn−1 )} ; if Vspace = {v} then return vn ; → − else return the nth component of the head of hyperedge Vspace in G ; The one-inclusion graph’s prediction strategy QG,F [4] immediately generalizes to the multiclass prediction strategy described by Algorithm 1. [sent-149, score-0.832]
</p><p>39 A lower bound in [6] showed that the one-inclusion strategy’s performance is optimal within a factor of 1 + o(1). [sent-154, score-0.094]
</p><p>40 Replacing orientation with a distribution over each edge induces a randomized strategy QGrand,F . [sent-155, score-0.211]
</p><p>41 4 [4]) For any n ∈ N and V ⊆ {0, 1}n , dens (G (V )) ≤ VC(V ). [sent-160, score-0.33]
</p><p>42 Consider any s ∈ [n], v ∈ V and let Ss (v; V ) be v shifted along s: if vs = 0, or if vs = 1 and there exists some u ∈ V differing to v only in the sth coordinate, then Ss (v; V ) = v; otherwise v shifts down—its sth coordinate is decreased from 1 to 0. [sent-162, score-0.569]
</p><p>43 The entire family V can be shifted to Ss (V ) = {Ss (v; V ) | v ∈ V } and this shifted vertex-set induces Ss (E) the edge-set of G (Ss (V )), where (V, E) = G (V ). [sent-163, score-0.309]
</p><p>44 A number of properties of shifting follow relatively easily: |Ss (V )| VC(Ss (V )) |E| |Ss (E)|  = ≤ ≤ ≥  ∃T ∈ N, s ∈ [n]T  |V | , by the injectivity of Ss ( · ; V ) VC(V ) , as Ss (V ) shatters I ⊆ [n] ⇒ V shatters I |V | · VC(V ) , as V closed-below ⇒ maxv∈V v l1 ≤ VC(V ) |E| , by cases  s. [sent-168, score-0.527]
</p><p>45 Shatter-invariant shifting  While [3] shifts to bound density, the number of edges can increase and the VC-dimension can decrease—both contributing to the observed gap between graph density and capacity. [sent-190, score-0.631]
</p><p>46 The next result demonstrates that shifting can in fact be controlled to preserve VC-dimension. [sent-191, score-0.296]
</p><p>47 1 Consider arbitrary n ∈ N, I ⊆ [n] and V ⊆ {0, 1}n that shatters I. [sent-193, score-0.126]
</p><p>48 Proof: ΠI (·) is invariant to shifting on I = [n]\I. [sent-205, score-0.3]
</p><p>49 So some ﬁnite number of shifts on I will produce a I-closed-below family W that shatters I. [sent-206, score-0.276]
</p><p>50 Thus the shattering of I is invariant to the shifting of W on I, so that a ﬁnite number of shifts on I produces an I-closed-below W that shatters I. [sent-208, score-0.557]
</p><p>51 Repeating the process a ﬁnite number of times until no non-trivial shifts are made produces a closed-below family that shatters I. [sent-209, score-0.276]
</p><p>52 4  Tightly bounding graph density by symmetrization  d Kuzmin and Warmuth [5] introduced Dn as a potential bound on the graph density of maximum d classes. [sent-211, score-0.552]
</p><p>53 We begin with properties of Dn , a technical lemma and then proceed to the main result. [sent-212, score-0.107]
</p><p>54 2 Dn d (i) equals the graph density of Vn for each n ∈ N and d ∈ [n]; (ii) is strictly upper-bounded by d, for all n; (iii) equals d for all n = d ∈ N; 2 (iv) is strictly monotonic increasing in d (with n ﬁxed); (v) is strictly monotonic increasing in n (with d ﬁxed); and (vi) limits to d as n → ∞. [sent-217, score-0.352]
</p><p>55 d d Proof: By counting, for each d ≤ n < ∞, the density of G Vn equals Dn : d E G Vn d |Vn |  =  d i=1 i d i=0  n i n i  =  n  d−1 i+1 n i=0 n i+1 n ≤d  =  n  d−1 n−1 i=0 i n ≤d  =  n  n−1 ≤d−1 n ≤d  A A+C A C proving (i). [sent-218, score-0.146]
</p><p>56 3 For arbitrary U, V ⊆ {0, 1}n with dens (G (V )) ≥ ρ > 0, |U | ≤ |V | and |E (G (U )) | ≥ |E (G (V )) |, if dens (G (U ∩ V )) < ρ then dens (G (U ∪ V )) > ρ. [sent-238, score-0.99]
</p><p>57 The density bounding D n is plotted (dotted solid) alongside the previous best d (dashed), for each d ∈ {1, 2, 10}. [sent-241, score-0.129]
</p><p>58 4 Every family V ⊆ {0, 1}n with d = VC(V ) has (V, E) = G (V ) with graph density |E| d ≤ Dn < d . [sent-243, score-0.258]
</p><p>59 Proof: Allow a permutation g ∈ Sn to act on vector v ∈ {0, 1}n and family V ⊆ {0, 1}n by g(v) = vg(1) , . [sent-246, score-0.094]
</p><p>60 Note  d that a closed-below VC-dimension d family V ⊆ {0, 1}n satisﬁes Sn (V ) = V iff V = Vn , as VC(V ) ≥ d implies V contains an embedded d-cube, invariance to Sn implies further that V d contains all n such cubes, and VC(V ) ≤ d implies that V ⊆ Vn . [sent-250, score-0.224]
</p><p>61 Consider now any d ∗ Vn,d  ∈ arg min |U |  U∈  arg max  dens (G (U ))  . [sent-251, score-0.33]
</p><p>62 Then if ∗ ∗ dens G Vn,d ∩ g(Vn,d )  ∗ ≥ dens G Vn,d  ∗ then Vn,d would not have been selected above  ∗ ∗ (i. [sent-253, score-0.66]
</p><p>63 But then again Vn,d would > dens G Vn,d Thus dens G Vn,d ∪ g(Vn,d ) ∗ ∗ not have been selected (i. [sent-258, score-0.66]
</p><p>64 (i) dens G Vn,d  d ∗ = Dn , for d = VC(Vn,d ) ≤ d. [sent-263, score-0.33]
</p><p>65 (iv) this implies that d = d and (6) is true for all closed-below families; Vn uniquely maximizes density amongst all closed-below VC-dimension d families in the n-cube. [sent-266, score-0.213]
</p><p>66 Noting that VC(W ) ≤ d and dens (G (V )) ≤ dens (G (W )) by (2) and (1) & (4) respectively, the bound (6) follows directly for V . [sent-269, score-0.754]
</p><p>67 Furthermore if we shift to preserve d VC-dimension then VC(W ) = d while still |V | = |W |. [sent-270, score-0.08]
</p><p>68 And since dens (G (W )) = Dn only if d W = Vn , it follows that V maximizes density amongst all VC-dimension d families in the n-cube, d with dens (G (V )) = Dn , only if it is maximum. [sent-271, score-0.851]
</p><p>69 4 improves on the VC-dimension density bound of Lemma 2. [sent-273, score-0.184]
</p><p>70 This new result immediately implies the following one-inclusion mistake bounds. [sent-275, score-0.218]
</p><p>71 d For small d, n∗ (d) = min n ≥ d | d = Dn —the ﬁrst n for which the new and old deterministic one-inclusion mistake bounds coincide—appears to remain very close to 2. [sent-279, score-0.278]
</p><p>72 5  Bounds for multiclass prediction  As in the k = 1 case, the key to developing the multiclass one-inclusion mistake bound is in bounding hypergraph density. [sent-283, score-0.702]
</p><p>73 We proceed by shifting a graph induced by the one-inclusion hypergraph. [sent-284, score-0.392]
</p><p>74 Proof: We begin by replacing the hyperedge structure E with a related edge structure E . [sent-290, score-0.077]
</p><p>75 Two vertices u, v ∈ V are connected in the graph (V, E ) iff there exists an i ∈ [n] such that u, v differ only at i and no w ∈ V exists such that ui < wi < vi and wj = uj = vj on [n]\{i}. [sent-291, score-0.266]
</p><p>76 |V | |V | |V |  (7)  Consider now shifting vertex v ∈ V at shift label t ∈ [k] along shift coordinate s ∈ [n] by Ss,t (v; V ) where vs(i) vs  = vs(vs ) = (v1 , . [sent-293, score-0.547]
</p><p>77 We shift V on s at t as usual; we shift V on s alone by bubbling vertices down to ﬁll gaps below: Ss,t (V ) = {Ss,t (v; V ) | v ∈ V } Ss (V ) = Ss,k (Ss,k−1 (. [sent-307, score-0.181]
</p><p>78 If i = s then no other vertex w ∈ V can come between u and v during shifting by construction of E , so {Ss (u; V ), Ss (v; V )} ∈ Ss (E ). [sent-314, score-0.275]
</p><p>79 If both vertices shift down by the same number of labels then they remain connected in Ss (E ). [sent-316, score-0.122]
</p><p>80 Otherwise assume WLOG that Ss (u; V )s < Ss (v; V )s then the shifted vertices will lose their edge, however since vs did not shift down to Ss (u; V )s there must have been some w ∈ V different to u on {i, s} such that ws < vs with Ss (w; V )s = Ss (u; V )s . [sent-317, score-0.493]
</p><p>81 (10)  In a ﬁnite number of shifts starting from (V, E ), a closed-below family W with induced edge-set F will be reached. [sent-325, score-0.172]
</p><p>82 Counting edges in F by upper-adjoining vertices we have proved that (V, E ) ﬁnitely shifts to closed-below graph (W, F ) Combining properties (7)–(11) we have that  |E| |V |  ≤  |E | |V |  ≤  s. [sent-334, score-0.235]
</p><p>83 The remaining arguments from the k = 1 case of [4, 3] now imply the multiclass mistake bound. [sent-338, score-0.292]
</p><p>84 The multiˆ class one-inclusion prediction strategy satisﬁes MQG,F ,F (n) ≤ ΨP -dim (F) /n. [sent-344, score-0.229]
</p><p>85 1  A lower bound  We now show that the preceding multiclass mistake bound is optimal to within a O(log k) factor, noting that ΨN is smaller than ΨP by at most such a factor [1, Theorem 10]. [sent-346, score-0.48]
</p><p>86 4 Consider any deterministic or randomized prediction strategy Q and any F ⊆ {0, . [sent-353, score-0.315]
</p><p>87 Proof: Following [2], we use the probabilistic method to prove the existence of a target in F for which prediction under a distribution P supported by a ΨN -shattered subset is hard. [sent-358, score-0.144]
</p><p>88 , zd } ΨN -shattered by F and then a subset FZ ⊆ F of 2d functions that ΨN -shatters Z. [sent-363, score-0.084]
</p><p>89 For any f ∈ FZ and x ∈ Z n with xn = xi for all i ∈ en [n − 1], exactly half of the functions in FZ consistent with sam ((x1 , . [sent-366, score-0.287]
</p><p>90 , k} on xn and the remaining half output some j ∈ {0, . [sent-372, score-0.169]
</p><p>91 6  Conclusions and open problems  In this paper we have developed new shifting machinery and tightened the binary one-inclusion d d mistake bound from d/n to Dn /n ( Dn /n for the deterministic strategy) representing a solid improvement for d ≈ n. [sent-388, score-0.603]
</p><p>92 We have described the multiclass generalization of the prediction learning model and derived a mistake bound for the multiclass one-inclusion prediction strategy that improves on previous PAC-based expected risk bounds by O(log n) and that is within O(log k) of optimal. [sent-389, score-0.953]
</p><p>93 Here shifting with invariance to the shattering of a single set was described, however we are aware of invariance to more complex shatterings. [sent-390, score-0.389]
</p><p>94 Another serious application of shatter-invariant shifting, to appear in a sequel to this paper, is to the study of the cubical structure of maximum and maximal classes with connections to the compressibility conjecture of [7]. [sent-391, score-0.093]
</p><p>95 4 resolves one conjecture of Kuzmin & Warmuth [5], the remainder of the conjectured correctness proof for the Peeling compression scheme is known to be false [8]. [sent-393, score-0.215]
</p><p>96 4 can be extended over subgroups G ⊂ S n to gain tighter d density bounds. [sent-395, score-0.09]
</p><p>97 Just as the Sn -invariant Vn is the maximizer of density among all closed-below d V ⊆ Vn , there exist G-invariant families that maximize the density over all of their sub-families. [sent-396, score-0.255]
</p><p>98 While a general ΨG -based bound would allow direct comparison with the PAC-based expected risk bound, it should also be noted that Ψ P and ΨG are in fact incomparable—neither ΨG ≤ ΨP nor ΨP ≤ ΨG singly holds for all classes [1, Theorem 1]. [sent-399, score-0.183]
</p><p>99 : A general lower bound on the number of examples needed for learning. [sent-419, score-0.094]
</p><p>100 : The one-inclusion graph algorithm is near optimal for the prediction model of learning. [sent-435, score-0.204]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ss', 0.436), ('vc', 0.34), ('dens', 0.33), ('shifting', 0.275), ('dn', 0.231), ('mistake', 0.196), ('vn', 0.187), ('xn', 0.149), ('vs', 0.132), ('shatters', 0.126), ('strategy', 0.12), ('sam', 0.118), ('prediction', 0.109), ('lemma', 0.107), ('shifted', 0.107), ('warmuth', 0.105), ('sn', 0.104), ('sst', 0.103), ('haussler', 0.1), ('multiclass', 0.096), ('graph', 0.095), ('bound', 0.094), ('density', 0.09), ('kuzmin', 0.09), ('xg', 0.083), ('fz', 0.082), ('shifts', 0.077), ('families', 0.075), ('family', 0.073), ('hypergraph', 0.072), ('sauer', 0.072), ('rubinstein', 0.065), ('risk', 0.064), ('vertices', 0.063), ('vspace', 0.062), ('shift', 0.059), ('theorem', 0.057), ('sup', 0.055), ('witnesses', 0.054), ('shattering', 0.054), ('conjecture', 0.052), ('compression', 0.052), ('symmetrization', 0.049), ('peeling', 0.049), ('zd', 0.049), ('randomized', 0.048), ('pollard', 0.046), ('littlestone', 0.046), ('bounds', 0.044), ('proof', 0.043), ('concept', 0.043), ('compressibility', 0.041), ('eunif', 0.041), ('resolves', 0.041), ('vt', 0.04), ('iv', 0.039), ('bounding', 0.039), ('deterministic', 0.038), ('nition', 0.038), ('hyperedge', 0.036), ('sth', 0.036), ('prp', 0.036), ('subset', 0.035), ('berkeley', 0.034), ('vg', 0.033), ('valiant', 0.031), ('invariance', 0.03), ('equals', 0.03), ('iff', 0.03), ('nth', 0.029), ('correctness', 0.027), ('positively', 0.027), ('exists', 0.027), ('bartlett', 0.026), ('proving', 0.026), ('amongst', 0.026), ('generalizes', 0.026), ('embedded', 0.025), ('inclusion', 0.025), ('invariant', 0.025), ('expected', 0.025), ('vi', 0.024), ('combinatorial', 0.024), ('implies', 0.022), ('induces', 0.022), ('induced', 0.022), ('monotonic', 0.022), ('meeting', 0.022), ('proofs', 0.022), ('coordinate', 0.022), ('permutation', 0.021), ('relating', 0.021), ('strictly', 0.021), ('preserve', 0.021), ('log', 0.021), ('edge', 0.021), ('counting', 0.02), ('replacing', 0.02), ('half', 0.02), ('division', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="173-tfidf-1" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><p>2 0.16171485 <a title="173-tfidf-2" href="./nips-2006-High-Dimensional_Graphical_Model_Selection_Using_%24%5Cell_1%24-Regularized_Logistic_Regression.html">92 nips-2006-High-Dimensional Graphical Model Selection Using $\ell 1$-Regularized Logistic Regression</a></p>
<p>Author: Martin J. Wainwright, John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We focus on the problem of estimating the graph structure associated with a discrete Markov random ﬁeld. We describe a method based on 1 regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an 1 -constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. Our main result is to establish suﬃcient conditions on the triple (n, p, d) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain mutual incoherence conditions analogous to those imposed in previous work on linear regression, we prove that consistent neighborhood selection can be obtained as long as the number of observations n grows more quickly than 6d6 log d + 2d5 log p, thereby establishing that logarithmic growth in the number of samples n relative to graph size p is suﬃcient to achieve neighborhood consistency. Keywords: Graphical models; Markov random ﬁelds; structure learning; 1 -regularization; model selection; convex risk minimization; high-dimensional asymptotics; concentration. 1</p><p>3 0.12591363 <a title="173-tfidf-3" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><p>4 0.11169393 <a title="173-tfidf-4" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>5 0.105534 <a title="173-tfidf-5" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>6 0.090283722 <a title="173-tfidf-6" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>7 0.084687829 <a title="173-tfidf-7" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>8 0.078499615 <a title="173-tfidf-8" href="./nips-2006-Conditional_mean_field.html">57 nips-2006-Conditional mean field</a></p>
<p>9 0.066148333 <a title="173-tfidf-9" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>10 0.063558914 <a title="173-tfidf-10" href="./nips-2006-Online_Classification_for_Complex_Problems_Using_Simultaneous_Projections.html">152 nips-2006-Online Classification for Complex Problems Using Simultaneous Projections</a></p>
<p>11 0.060111824 <a title="173-tfidf-11" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>12 0.059451155 <a title="173-tfidf-12" href="./nips-2006-Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors.html">64 nips-2006-Data Integration for Classification Problems Employing Gaussian Process Priors</a></p>
<p>13 0.059343934 <a title="173-tfidf-13" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>14 0.053041078 <a title="173-tfidf-14" href="./nips-2006-Stochastic_Relational_Models_for_Discriminative_Link_Prediction.html">183 nips-2006-Stochastic Relational Models for Discriminative Link Prediction</a></p>
<p>15 0.051893786 <a title="173-tfidf-15" href="./nips-2006-Geometric_entropy_minimization_%28GEM%29_for_anomaly_detection_and_localization.html">85 nips-2006-Geometric entropy minimization (GEM) for anomaly detection and localization</a></p>
<p>16 0.051455557 <a title="173-tfidf-16" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<p>17 0.050882641 <a title="173-tfidf-17" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>18 0.050643593 <a title="173-tfidf-18" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>19 0.05058828 <a title="173-tfidf-19" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>20 0.050023962 <a title="173-tfidf-20" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, 0.057), (2, -0.122), (3, 0.0), (4, 0.037), (5, 0.087), (6, 0.009), (7, 0.114), (8, -0.053), (9, -0.032), (10, 0.127), (11, -0.104), (12, 0.057), (13, -0.018), (14, -0.065), (15, -0.026), (16, 0.015), (17, 0.029), (18, -0.016), (19, -0.168), (20, -0.096), (21, 0.102), (22, 0.064), (23, -0.054), (24, 0.048), (25, -0.087), (26, 0.017), (27, 0.121), (28, 0.078), (29, 0.154), (30, 0.052), (31, 0.056), (32, 0.007), (33, -0.101), (34, 0.033), (35, -0.042), (36, -0.051), (37, -0.06), (38, 0.06), (39, -0.15), (40, 0.066), (41, -0.068), (42, -0.104), (43, -0.104), (44, 0.05), (45, -0.077), (46, 0.048), (47, -0.093), (48, -0.076), (49, -0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96702778 <a title="173-lsi-1" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><p>2 0.67559808 <a title="173-lsi-2" href="./nips-2006-High-Dimensional_Graphical_Model_Selection_Using_%24%5Cell_1%24-Regularized_Logistic_Regression.html">92 nips-2006-High-Dimensional Graphical Model Selection Using $\ell 1$-Regularized Logistic Regression</a></p>
<p>Author: Martin J. Wainwright, John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We focus on the problem of estimating the graph structure associated with a discrete Markov random ﬁeld. We describe a method based on 1 regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an 1 -constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. Our main result is to establish suﬃcient conditions on the triple (n, p, d) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain mutual incoherence conditions analogous to those imposed in previous work on linear regression, we prove that consistent neighborhood selection can be obtained as long as the number of observations n grows more quickly than 6d6 log d + 2d5 log p, thereby establishing that logarithmic growth in the number of samples n relative to graph size p is suﬃcient to achieve neighborhood consistency. Keywords: Graphical models; Markov random ﬁelds; structure learning; 1 -regularization; model selection; convex risk minimization; high-dimensional asymptotics; concentration. 1</p><p>3 0.59108853 <a title="173-lsi-3" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><p>4 0.5187856 <a title="173-lsi-4" href="./nips-2006-An_Oracle_Inequality_for_Clipped_Regularized_Risk_Minimizers.html">30 nips-2006-An Oracle Inequality for Clipped Regularized Risk Minimizers</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels for classiﬁcation this oracle inequality leads to learning rates that are faster than the ones established in [9]. Finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]. 1</p><p>5 0.51279509 <a title="173-lsi-5" href="./nips-2006-Learnability_and_the_doubling_dimension.html">109 nips-2006-Learnability and the doubling dimension</a></p>
<p>Author: Yi Li, Philip M. Long</p><p>Abstract: Given a set of classiﬁers and a probability distribution over their domain, one can deﬁne a metric by taking the distance between a pair of classiﬁers to be the probability that they classify a random item differently. We prove bounds on the sample complexity of PAC learning in terms of the doubling dimension of this metric. These bounds imply known bounds on the sample complexity of learning halfspaces with respect to the uniform distribution that are optimal up to a constant factor. We prove a bound that holds for any algorithm that outputs a classiﬁer with zero error whenever this is possible; this bound is in terms of the maximum of the doubling dimension and the VC-dimension of , and strengthens the best known bound in terms of the VC-dimension alone. We show that there is no bound on the doubling dimension in terms of the VC-dimension of (in contrast with the metric dimension).</p><p>6 0.49763775 <a title="173-lsi-6" href="./nips-2006-Learning_from_Multiple_Sources.html">116 nips-2006-Learning from Multiple Sources</a></p>
<p>7 0.43268961 <a title="173-lsi-7" href="./nips-2006-Learning_with_Hypergraphs%3A_Clustering%2C_Classification%2C_and_Embedding.html">123 nips-2006-Learning with Hypergraphs: Clustering, Classification, and Embedding</a></p>
<p>8 0.42975593 <a title="173-lsi-8" href="./nips-2006-A_Small_World_Threshold_for_Economic_Network_Formation.html">14 nips-2006-A Small World Threshold for Economic Network Formation</a></p>
<p>9 0.41576511 <a title="173-lsi-9" href="./nips-2006-AdaBoost_is_Consistent.html">21 nips-2006-AdaBoost is Consistent</a></p>
<p>10 0.3809565 <a title="173-lsi-10" href="./nips-2006-Fast_Computation_of_Graph_Kernels.html">77 nips-2006-Fast Computation of Graph Kernels</a></p>
<p>11 0.37978017 <a title="173-lsi-11" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>12 0.37722114 <a title="173-lsi-12" href="./nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">156 nips-2006-Ordinal Regression by Extended Binary Classification</a></p>
<p>13 0.33380598 <a title="173-lsi-13" href="./nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</a></p>
<p>14 0.32451406 <a title="173-lsi-14" href="./nips-2006-Tighter_PAC-Bayes_Bounds.html">193 nips-2006-Tighter PAC-Bayes Bounds</a></p>
<p>15 0.32246703 <a title="173-lsi-15" href="./nips-2006-Approximate_inference_using_planar_graph_decomposition.html">35 nips-2006-Approximate inference using planar graph decomposition</a></p>
<p>16 0.31955007 <a title="173-lsi-16" href="./nips-2006-No-regret_Algorithms_for_Online_Convex_Programs.html">146 nips-2006-No-regret Algorithms for Online Convex Programs</a></p>
<p>17 0.313214 <a title="173-lsi-17" href="./nips-2006-Manifold_Denoising.html">128 nips-2006-Manifold Denoising</a></p>
<p>18 0.29883093 <a title="173-lsi-18" href="./nips-2006-Optimal_Single-Class_Classification_Strategies.html">155 nips-2006-Optimal Single-Class Classification Strategies</a></p>
<p>19 0.29873472 <a title="173-lsi-19" href="./nips-2006-A_Kernel_Method_for_the_Two-Sample-Problem.html">5 nips-2006-A Kernel Method for the Two-Sample-Problem</a></p>
<p>20 0.29311505 <a title="173-lsi-20" href="./nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.053), (7, 0.055), (9, 0.546), (22, 0.04), (44, 0.082), (57, 0.034), (65, 0.04), (69, 0.018), (87, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93924087 <a title="173-lda-1" href="./nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</a></p>
<p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><p>2 0.9298166 <a title="173-lda-2" href="./nips-2006-Uncertainty%2C_phase_and_oscillatory_hippocampal_recall.html">197 nips-2006-Uncertainty, phase and oscillatory hippocampal recall</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Many neural areas, notably, the hippocampus, show structured, dynamical, population behavior such as coordinated oscillations. It has long been observed that such oscillations provide a substrate for representing analog information in the ﬁring phases of neurons relative to the underlying population rhythm. However, it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture, and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems. Here, we observe that, since neurons in an oscillatory network need not only ﬁre once in each cycle (or even at all), uncertainty about the analog quantities each neuron represents by its ﬁring phase might naturally be reported through the degree of concentration of the spikes that it ﬁres. We apply this theory to memory in a model of oscillatory associative recall in hippocampal area CA3. Although it is not well treated in the literature, representing and manipulating uncertainty is fundamental to competent memory; our theory enables us to view CA3 as an effective uncertainty-aware, retrieval system. 1</p><p>3 0.9219709 <a title="173-lda-3" href="./nips-2006-Nonnegative_Sparse_PCA.html">149 nips-2006-Nonnegative Sparse PCA</a></p>
<p>Author: Ron Zass, Amnon Shashua</p><p>Abstract: We describe a nonnegative variant of the ”Sparse PCA” problem. The goal is to create a low dimensional representation from a collection of points which on the one hand maximizes the variance of the projected points and on the other uses only parts of the original coordinates, and thereby creating a sparse representation. What distinguishes our problem from other Sparse PCA formulations is that the projection involves only nonnegative weights of the original coordinates — a desired quality in various ﬁelds, including economics, bioinformatics and computer vision. Adding nonnegativity contributes to sparseness, where it enforces a partitioning of the original coordinates among the new axes. We describe a simple yet efﬁcient iterative coordinate-descent type of scheme which converges to a local optimum of our optimization criteria, giving good results on large real world datasets. 1</p><p>4 0.81166881 <a title="173-lda-4" href="./nips-2006-A_Local_Learning_Approach_for_Clustering.html">7 nips-2006-A Local Learning Approach for Clustering</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf</p><p>Abstract: We present a local learning approach for clustering. The basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. An optimization problem is formulated such that its solution has the above property. Relaxation and eigen-decomposition are applied to solve this optimization problem. We also brieﬂy investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. Experimental results are provided to validate the effectiveness of the proposed approach. 1</p><p>5 0.5459199 <a title="173-lda-5" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>Author: Honghao Shan, Lingyun Zhang, Garrison W. Cottrell</p><p>Abstract: Independent Component Analysis (ICA) is a popular method for extracting independent features from visual data. However, as a fundamentally linear technique, there is always nonlinear residual redundancy that is not captured by ICA. Hence there have been many attempts to try to create a hierarchical version of ICA, but so far none of the approaches have a natural way to apply them more than once. Here we show that there is a relatively simple technique that transforms the absolute values of the outputs of a previous application of ICA into a normal distribution, to which ICA maybe applied again. This results in a recursive ICA algorithm that may be applied any number of times in order to extract higher order structure from previous layers. 1</p><p>6 0.54424363 <a title="173-lda-6" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>7 0.51681298 <a title="173-lda-7" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>8 0.50997543 <a title="173-lda-8" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>9 0.50582916 <a title="173-lda-9" href="./nips-2006-Doubly_Stochastic_Normalization_for_Spectral_Clustering.html">70 nips-2006-Doubly Stochastic Normalization for Spectral Clustering</a></p>
<p>10 0.49199998 <a title="173-lda-10" href="./nips-2006-Generalized_Maximum_Margin_Clustering_and_Unsupervised_Kernel_Learning.html">83 nips-2006-Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</a></p>
<p>11 0.48089412 <a title="173-lda-11" href="./nips-2006-Temporal_Coding_using_the_Response_Properties_of_Spiking_Neurons.html">187 nips-2006-Temporal Coding using the Response Properties of Spiking Neurons</a></p>
<p>12 0.45711982 <a title="173-lda-12" href="./nips-2006-Large_Margin_Multi-channel_Analog-to-Digital_Conversion_with_Applications_to_Neural_Prosthesis.html">107 nips-2006-Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis</a></p>
<p>13 0.45643002 <a title="173-lda-13" href="./nips-2006-Differential_Entropic_Clustering_of_Multivariate_Gaussians.html">67 nips-2006-Differential Entropic Clustering of Multivariate Gaussians</a></p>
<p>14 0.45150244 <a title="173-lda-14" href="./nips-2006-MLLE%3A_Modified_Locally_Linear_Embedding_Using_Multiple_Weights.html">127 nips-2006-MLLE: Modified Locally Linear Embedding Using Multiple Weights</a></p>
<p>15 0.45125693 <a title="173-lda-15" href="./nips-2006-Nonlinear_physically-based_models_for_decoding_motor-cortical_population_activity.html">148 nips-2006-Nonlinear physically-based models for decoding motor-cortical population activity</a></p>
<p>16 0.44594851 <a title="173-lda-16" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>17 0.4427487 <a title="173-lda-17" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>18 0.44228545 <a title="173-lda-18" href="./nips-2006-Stability_of_%24K%24-Means_Clustering.html">181 nips-2006-Stability of $K$-Means Clustering</a></p>
<p>19 0.42376474 <a title="173-lda-19" href="./nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">163 nips-2006-Prediction on a Graph with a Perceptron</a></p>
<p>20 0.42033356 <a title="173-lda-20" href="./nips-2006-Stratification_Learning%3A_Detecting_Mixed_Density_and_Dimensionality_in_High_Dimensional_Point_Clouds.html">184 nips-2006-Stratification Learning: Detecting Mixed Density and Dimensionality in High Dimensional Point Clouds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
