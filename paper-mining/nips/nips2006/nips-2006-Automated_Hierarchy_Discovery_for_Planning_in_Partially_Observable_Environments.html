<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-38" href="#">nips2006-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</h1>
<br/><p>Source: <a title="nips-2006-38-pdf" href="http://papers.nips.cc/paper/2968-automated-hierarchy-discovery-for-planning-in-partially-observable-environments.pdf">pdf</a></p><p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>Reference: <a title="nips-2006-38-reference" href="../nips2006_reference/nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. [sent-7, score-0.429]
</p><p>2 Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. [sent-8, score-0.521]
</p><p>3 More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. [sent-10, score-1.003]
</p><p>4 Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. [sent-12, score-0.323]
</p><p>5 It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. [sent-13, score-0.326]
</p><p>6 However, in many realworld scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. [sent-15, score-0.429]
</p><p>7 [22], and Hansen and Zhou [10] proposed various algorithms that speed up planning in partially observable domains by exploiting the decompositions induced by a hierarchy. [sent-22, score-0.32]
</p><p>8 However these approaches assume that a policy hierarchy is speciﬁed by the user, so an important question arises: how can we automate the discovery of a policy hierarchy? [sent-23, score-0.852]
</p><p>9 In fully observable domains, there exists a large body of work on hierarchical Markov decision processes and reinforcement learning [6, 21, 7, 15] and several hierarchy discovery techniques have been proposed [23, 13, 11, 20]. [sent-24, score-0.614]
</p><p>10 However those techniques rely on the assumption that states are fully observable to detect abstractions and subgoals, which prevents their use in partially observable domains. [sent-25, score-0.327]
</p><p>11 We propose to frame hierarchy and policy discovery as an optimization problem with variables corresponding to the hierarchy and policy parameters. [sent-26, score-1.147]
</p><p>12 We present an approach that searches in the space of hierarchical controllers [10] for a good hierarchical policy. [sent-27, score-0.661]
</p><p>13 The search leads to a difﬁcult non-convex optimization problem that we tackle using three approaches: generic non-linear solvers, a mixed-integer non-linear programming approximation or an alternating optimization technique that can be thought as a form of hierarchical bounded policy iteration. [sent-28, score-0.622]
</p><p>14 We also generalize Hansen and Zhou’s hierarchical controllers [10] to allow recursive controllers. [sent-29, score-0.675]
</p><p>15 These are controllers that  may recursively call themselves, with the ability of representing policies with a ﬁnite number of parameters that would otherwise require inﬁnitely many parameters. [sent-30, score-0.492]
</p><p>16 Recursive policies are likely to arise in language processing tasks such as dialogue management and text generation due to the recursive nature of language models. [sent-31, score-0.469]
</p><p>17 1), which is the framework used throughout the paper for planning in partially observable domains. [sent-34, score-0.282]
</p><p>18 Then we review how to represent POMDP policies as ﬁnite state controllers (Sect. [sent-35, score-0.515]
</p><p>19 2) as well as some algorithms to optimize controllers of a ﬁxed size (Sect. [sent-37, score-0.377]
</p><p>20 1  POMDPs  POMDPs have emerged as a popular framework for planning in partially observable domains [12]. [sent-41, score-0.32]
</p><p>21 Thus we deﬁne a policy π as a mapping from histories of past actions and observations to actions. [sent-47, score-0.409]
</p><p>22 The value V π of policy π when starting with belief b is measured by the expected sum of the future rewards: V π (b) = t R(bt , π(bt )), where R(b, a) = s b(s)R(s, a). [sent-54, score-0.317]
</p><p>23 An optimal policy π ∗ is a policy with the highest value V ∗ for all beliefs (i. [sent-55, score-0.568]
</p><p>24 2  Policy Representation  A convenient representation for an important class of policies is that of ﬁnite state controllers [9]. [sent-60, score-0.515]
</p><p>25 A ﬁnite state controller consists of a ﬁnite state automaton (N, E) with a set N of nodes and a set E of directed edges Each node n has one outgoing edge per observation. [sent-61, score-0.621]
</p><p>26 A controller encodes a policy π = (α, β) by mapping each node to an action (i. [sent-62, score-0.728]
</p><p>27 , α(n) = a) and each edge (referred by its observation label o and its parent node n) to a successor node (i. [sent-64, score-0.34]
</p><p>28 At runtime, the policy encoded by a controller is executed by doing the action at = α(nt ) associated with the node nt traversed at time step t and following the edge labelled with observation ot to reach the next node nt+1 = β(nt , ot ). [sent-67, score-0.937]
</p><p>29 Stochastic controllers [18] can also be used to represent stochastic policies by redeﬁning α and β as distributions over actions and successor nodes. [sent-68, score-0.666]
</p><p>30 More precisely, let Prα (a|n) be the distribution from which an action a is sampled in node n and let Prβ (n |n, a, o) be the distribution from which the successor node n is sampled after executing a and receiving o in node n. [sent-69, score-0.565]
</p><p>31 Given time and memory constraints, it is common practice to search for the best controller with a bounded number of nodes [18]. [sent-71, score-0.385]
</p><p>32 However, when the number of nodes is ﬁxed, the best controller is not necessarily deterministic. [sent-72, score-0.35]
</p><p>33 This explains why searching in the space of stochastic controllers may be advantageous. [sent-73, score-0.413]
</p><p>34 Table 1: Quadratically constrained optimization program for bounded stochastic controllers [1]. [sent-74, score-0.506]
</p><p>35 3  x  n  x  i  ∀n, s  y  ∀n, o  x  Optimization of Stochastic Controllers  The optimization of a stochastic controller with a ﬁxed number of nodes can be formulated as a quadratically constrained optimization problem (QCOP) [1]. [sent-78, score-0.532]
</p><p>36 Several techniques have been tried including gradient ascent [14], stochastic local search [3], bounded policy iteration (BPI) [18] and a general non-linear solver called SNOPT (based on sequential quadratic programming) [1, 8]. [sent-85, score-0.387]
</p><p>37 Given a policy with ﬁxed parameters Pr(a, n |n, o), policy evaluation solves the linear system in Eq 1 to ﬁnd Vn (s) for all n, s. [sent-90, score-0.568]
</p><p>38 1  3  Hierarchical controllers  Hansen and Zhou [10] recently proposed hierarchical ﬁnite-state controllers as a simple and intuitive way of encoding hierarchical policies. [sent-93, score-1.038]
</p><p>39 A hierarchical controller consists of a set of nodes and edges as in a ﬂat controller, however some nodes may be abstract, corresponding to sub-controllers themselves. [sent-94, score-0.65]
</p><p>40 As with ﬂat controllers, concrete nodes are parameterized with an action mapping α and edges outgoing concrete nodes are parameterized by a successor node mapping β. [sent-95, score-0.787]
</p><p>41 In contrast, abstract nodes are parameterized by a child node mapping indicating in which child node the subcontroller should start. [sent-96, score-0.772]
</p><p>42 In fully observable domains, it is customary to stop the subcontroller once a goal state (from a predeﬁned set of terminal states) is reached. [sent-99, score-0.448]
</p><p>43 This strategy cannot work in partially observable domains, so Hansen and Zhou propose to terminate a subcontroller when an end node (from a predeﬁned set of terminal nodes) is reached. [sent-100, score-0.632]
</p><p>44 Since the decision to reach a terminal node is made according to the successor node mapping β, the timing for returning control is implicitly optimized. [sent-101, score-0.474]
</p><p>45 Terminal nodes do not have any outgoing edges nor any action mapping since they already have an action assigned. [sent-104, score-0.349]
</p><p>46 The hierarchy of the controller is assumed to be ﬁnite and speciﬁed by the programmer. [sent-105, score-0.46]
</p><p>47 Subcontrollers at the bottom level are made up only of concrete nodes and therefore can be optimized as usual using any controller optimization technique. [sent-107, score-0.482]
</p><p>48 Hence, the immediate reward of an abstract node n corresponds to the value Vα(¯ ) (s) of its child node α(¯ ). [sent-110, score-0.384]
</p><p>49 Similarly, the probability of reach¯ n n ing state s after executing the subcontroller of an abstract node n corresponds to the probability ¯ Pr(send |s, α(¯ )) of terminating the subcontroller in send when starting in s at child node α(¯ ). [sent-111, score-1.032]
</p><p>50 Hansen’s hierarchical controllers have two limitations: the hierarchy must have a ﬁnite number of levels and it must be speciﬁed by hand. [sent-122, score-0.783]
</p><p>51 In the next section we describe recursive controllers which may have inﬁnitely many levels. [sent-123, score-0.533]
</p><p>52 We also describe an algorithm to discover a suitable hierarchy by simultaneously optimizing the controller parameters and hierarchy. [sent-124, score-0.488]
</p><p>53 1  Recursive Controllers  In some domains, policies are naturally recursive in the sense that they decompose into subpolicies that may call themselves. [sent-126, score-0.298]
</p><p>54 Assuming POMDP dialogue management eventually handles decisions at the sentence level, recursive policies will naturally arise. [sent-129, score-0.397]
</p><p>55 Similarly, language generation with POMDPs would naturally lead to recursive policies that reﬂect the recursive nature of language models. [sent-130, score-0.499]
</p><p>56 We now propose several modiﬁcations to Hansen and Zhou’s hierarchical controllers that simplify things while allowing recursive controllers. [sent-131, score-0.675]
</p><p>57 First, the subcontrollers of abstract nodes may be composed of any node (including the parent node itself) and transitions can be made to any node anywhere (whether concrete or abstract). [sent-132, score-0.752]
</p><p>58 This allows recursive controllers and smaller controllers since nodes may be shared across levels. [sent-133, score-1.037]
</p><p>59 Second, we use a single terminal node that has no action nor any outer edge. [sent-134, score-0.289]
</p><p>60 Hence, the actions that were associated with the terminal nodes in Hansen and Zhou’s proposal are associated with the abstract nodes in our proposal. [sent-138, score-0.424]
</p><p>61 This allows a uniform parameterization of actions for all nodes while reducing the number of terminal nodes to 1. [sent-139, score-0.424]
</p><p>62 Fourth, the outer edges of abstract nodes are labelled with regular observations since an observation will be made following the execution of the action of an abstract node. [sent-140, score-0.293]
</p><p>63 2  Hierarchy and Policy Optimization  We formulate the search for a good stochastic recursive controller, including the automated hierarchy discovery, as an optimization problem (see Table 2). [sent-147, score-0.51]
</p><p>64 The global maximum of this optimization problem corresponds to the optimal policy (and hierarchy) for a ﬁxed set N of concrete nodes n ¯ and a ﬁxed set N of abstract nodes n. [sent-148, score-0.67]
</p><p>65 The variables consist of the value function Vn (s), the policy ¯ parameters Pr(n , a|n, o), the (stochastic) child node mapping Pr(n |¯ ) for each abstract node n and n ¯ the occupancy frequency oc(n, s|n0 , s0 ) of each (n, s)-pair when starting in (n0 , s0 ). [sent-149, score-0.651]
</p><p>66 3) is the expected value s b0 (s)Vn0 (s) of starting the controller in node n0 with initial belief b0 . [sent-151, score-0.397]
</p><p>67 The expected value of an abstract node corresponds to the sum of three terms: the expected value Vnbeg (s) of its subcontroller given by its child node nbeg , the reward R(send , an ) ¯ immediately after the termination of the subcontroller and the future rewards Vn (s ). [sent-153, score-1.062]
</p><p>68 Note that the reward R(send , an ) depends on the state send in which the subcontroller terminates. [sent-158, score-0.464]
</p><p>69 ¯ Hence we need to compute the probability that the last state visited in the subcontroller is send . [sent-159, score-0.425]
</p><p>70 This probability is given by the occupancy frequency oc(send , nend |s, nbeg ), which is recursively deﬁned in Eq. [sent-160, score-0.296]
</p><p>71 Only the nodes labelled with numbers larger than the label of an abstract node can be children of that abstract node. [sent-167, score-0.324]
</p><p>72 This constraint ensures that chains of child node mappings have a ﬁnite length, eventually reaching a concrete node where an action is executed. [sent-168, score-0.498]
</p><p>73 Constraints, like the ones in Table 1, are also needed to guarantee that the policy parameters and the child node mappings are proper distributions. [sent-169, score-0.509]
</p><p>74 html  Table 2: Non-convex quarticly constrained optimization problem for hierarchy and policy discovery in bounded stochastic recursive controllers. [sent-190, score-0.853]
</p><p>75 Note that ﬁxing Vn (s ) and oc(s, n|s0 , n0 ) while varying the policy ¯ parameters is reminiscent of policy iteration, hence the name bounded hierarchical policy iteration. [sent-199, score-1.029]
</p><p>76 4  Discussion  Discovering a hierarchy offers many advantages over previous methods that assume the hierarchy is already known. [sent-201, score-0.474]
</p><p>77 Note however that discovering the hierarchy while optimizing the policy is a much more difﬁcult problem than simply optimizing the policy parameters. [sent-204, score-0.844]
</p><p>78 Our approach can also be used when the hierarchy and the policy are partly known. [sent-208, score-0.521]
</p><p>79 It is also interesting to note that hierarchical policies may be encoded with exponentially fewer nodes in a hierarchical controller than a ﬂat controller. [sent-211, score-0.749]
</p><p>80 Intuitively, when a subcontroller is called by k abstract nodes, this subcontroller is shared by all its abstract parents. [sent-212, score-0.43]
</p><p>81 If a hierarchical controller has l levels with subcontrollers shared by k parents in each level, then the equivalent ﬂat controller will need O(k l ) copies. [sent-214, score-0.723]
</p><p>82 By allowing recursive controllers, policies may be represented even more compactly. [sent-215, score-0.271]
</p><p>83 Recursive controllers allow abstract nodes to call subcontrollers that may contain themselves. [sent-216, score-0.612]
</p><p>84 73  equivalent non-hierarchical controller would have to unroll the recursion by creating a separate copy of the subcontroller each time it is called. [sent-240, score-0.438]
</p><p>85 Since recursive controllers essentially call themselves inﬁnitely many times, they can represent inﬁnitely large non-recursive controllers with ﬁnitely many nodes. [sent-241, score-0.91]
</p><p>86 As a comparison, recursive controllers are to non-recursive hierarchical controllers what context-free grammars are to regular expressions. [sent-242, score-1.052]
</p><p>87 Since the leading approaches for controller optimization ﬁx the number of nodes [18, 1], one may be able to ﬁnd a much better policy by considering hierarchical recursive controllers. [sent-243, score-0.99]
</p><p>88 In addition, hierarchical controllers may be easier to understand and interpret than ﬂat controllers given their natural decomposition into subcontrollers and their possibly smaller size. [sent-244, score-1.004]
</p><p>89 We used the SNOPT package to directly solve the non-convex optimization problem in Table 2 and bounded hierarchical policy iteration (BHPI) to solve it iteratively. [sent-246, score-0.519]
</p><p>90 We optimized hierarchical controllers of two levels with a ﬁxed number of nodes reported in the column labelled “Num. [sent-249, score-0.729]
</p><p>91 In particular, the hierarchy discovered for the paint problem matches the one hand coded by Pineau in her PhD thesis [16]. [sent-256, score-0.305]
</p><p>92 5  Conclusion & Future Work  This paper proposes the ﬁrst approach for hierarchy discovery in partially observable planning problems. [sent-259, score-0.566]
</p><p>93 We model the search for a good hierarchical policy as a non-convex optimization problem with variables corresponding to the hierarchy and policy parameters. [sent-260, score-1.005]
</p><p>94 We propose to tackle the optimization problem using non-linear solvers such as SNOPT or by reformulating the problem as an approximate MINLP or as a sequence of MILPs that can be thought of as a form of hierarchical bounded policy iteration. [sent-261, score-0.567]
</p><p>95 We also generalize Hansen and Zhou’s hierarchical controllers to recursive controllers. [sent-265, score-0.675]
</p><p>96 Recursive controllers can encode policies with ﬁnitely many nodes that would otherwise require inﬁnitely large 3  http://pomdp. [sent-266, score-0.619]
</p><p>97 Further details about recursive controllers and our other contributions can be found in [5]. [sent-271, score-0.533]
</p><p>98 We plan to further investigate the use of recursive controllers in dialogue management and text generation where recursive policies are expected to naturally capture the recursive nature of language models. [sent-272, score-1.122]
</p><p>99 Automated hierarchy discovery for planning in partially observable domains. [sent-296, score-0.566]
</p><p>100 An improved policy iteration algorithm for partially observable MDPs. [sent-318, score-0.47]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pr', 0.393), ('controllers', 0.377), ('policy', 0.284), ('hierarchy', 0.237), ('controller', 0.223), ('subcontroller', 0.215), ('nbeg', 0.188), ('send', 0.187), ('oc', 0.175), ('recursive', 0.156), ('vn', 0.143), ('hierarchical', 0.142), ('node', 0.141), ('nodes', 0.127), ('observable', 0.12), ('hansen', 0.119), ('policies', 0.115), ('nend', 0.108), ('snopt', 0.108), ('subcontrollers', 0.108), ('planning', 0.096), ('terminal', 0.09), ('pomdp', 0.085), ('dialogue', 0.081), ('actions', 0.08), ('pomdps', 0.079), ('concrete', 0.074), ('zhou', 0.072), ('bhpi', 0.067), ('minlp', 0.067), ('partially', 0.066), ('child', 0.063), ('successor', 0.058), ('action', 0.058), ('optimization', 0.058), ('labelled', 0.056), ('bpi', 0.054), ('ok', 0.054), ('outgoing', 0.053), ('nitely', 0.051), ('waterloo', 0.047), ('discovery', 0.047), ('reinforcement', 0.046), ('management', 0.045), ('absorb', 0.04), ('disjunctive', 0.04), ('lbx', 0.04), ('paint', 0.04), ('pineau', 0.04), ('ubx', 0.04), ('termination', 0.039), ('discovering', 0.039), ('transition', 0.039), ('reward', 0.039), ('domains', 0.038), ('stochastic', 0.036), ('language', 0.036), ('bounded', 0.035), ('nt', 0.034), ('belief', 0.033), ('ontario', 0.032), ('xh', 0.032), ('solver', 0.032), ('edges', 0.031), ('quadratically', 0.03), ('discount', 0.029), ('aaai', 0.028), ('discover', 0.028), ('thesis', 0.028), ('posmdps', 0.027), ('poupart', 0.027), ('subgoals', 0.027), ('subpolicies', 0.027), ('theocharous', 0.027), ('vnbeg', 0.027), ('discounting', 0.027), ('levels', 0.027), ('bb', 0.026), ('executing', 0.026), ('phd', 0.025), ('tackle', 0.025), ('table', 0.024), ('histories', 0.023), ('maze', 0.023), ('shuttle', 0.023), ('state', 0.023), ('automated', 0.023), ('solvers', 0.023), ('decision', 0.022), ('user', 0.022), ('mapping', 0.022), ('execution', 0.021), ('abstractions', 0.021), ('terminating', 0.021), ('rewards', 0.021), ('mappings', 0.021), ('constraints', 0.02), ('transitions', 0.02), ('programming', 0.02), ('notoriously', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="38-tfidf-1" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>2 0.29242697 <a title="38-tfidf-2" href="./nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</a></p>
<p>Author: Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to ﬁrst recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.</p><p>3 0.24096598 <a title="38-tfidf-3" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>Author: Silvia Richter, Douglas Aberdeen, Jin Yu</p><p>Abstract: Current road-trafﬁc optimisation practice around the world is a combination of hand tuned policies with a small degree of automatic adaption. Even state-ofthe-art research controllers need good models of the road trafﬁc, which cannot be obtained directly from existing sensors. We use a policy-gradient reinforcement learning approach to directly optimise the trafﬁc signals, mapping currently deployed sensor observations to control signals. Our trained controllers are (theoretically) compatible with the trafﬁc system used in Sydney and many other cities around the world. We apply two policy-gradient methods: (1) the recent natural actor-critic algorithm, and (2) a vanilla policy-gradient algorithm for comparison. Along the way we extend natural-actor critic approaches to work for distributed and online inﬁnite-horizon problems. 1</p><p>4 0.20319398 <a title="38-tfidf-4" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>Author: Mohammad Ghavamzadeh, Yaakov Engel</p><p>Abstract: Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. Since Monte Carlo methods tend to have high variance, a large number of samples is required, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost. 1</p><p>5 0.1858681 <a title="38-tfidf-5" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: We consider methods that try to ﬁnd a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.</p><p>6 0.1726782 <a title="38-tfidf-6" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>7 0.12628786 <a title="38-tfidf-7" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>8 0.11553057 <a title="38-tfidf-8" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>9 0.10592648 <a title="38-tfidf-9" href="./nips-2006-Multi-Robot_Negotiation%3A_Approximating_the_Set_of_Subgame_Perfect_Equilibria_in_General-Sum_Stochastic_Games.html">137 nips-2006-Multi-Robot Negotiation: Approximating the Set of Subgame Perfect Equilibria in General-Sum Stochastic Games</a></p>
<p>10 0.10028908 <a title="38-tfidf-10" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>11 0.089036748 <a title="38-tfidf-11" href="./nips-2006-Distributed_Inference_in_Dynamical_Systems.html">69 nips-2006-Distributed Inference in Dynamical Systems</a></p>
<p>12 0.077382669 <a title="38-tfidf-12" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>13 0.062263448 <a title="38-tfidf-13" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>14 0.062206235 <a title="38-tfidf-14" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>15 0.060394678 <a title="38-tfidf-15" href="./nips-2006-Cross-Validation_Optimization_for_Large_Scale_Hierarchical_Classification_Kernel_Methods.html">63 nips-2006-Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods</a></p>
<p>16 0.052785188 <a title="38-tfidf-16" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>17 0.052526191 <a title="38-tfidf-17" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>18 0.049815562 <a title="38-tfidf-18" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>19 0.048923619 <a title="38-tfidf-19" href="./nips-2006-Kernels_on_Structured_Objects_Through_Nested_Histograms.html">103 nips-2006-Kernels on Structured Objects Through Nested Histograms</a></p>
<p>20 0.046942592 <a title="38-tfidf-20" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, -0.015), (2, -0.219), (3, -0.236), (4, 0.104), (5, -0.055), (6, 0.052), (7, -0.022), (8, 0.398), (9, -0.023), (10, -0.03), (11, -0.086), (12, 0.019), (13, -0.114), (14, -0.082), (15, -0.014), (16, -0.027), (17, -0.034), (18, 0.08), (19, 0.103), (20, -0.087), (21, -0.168), (22, -0.182), (23, -0.017), (24, 0.023), (25, -0.132), (26, 0.158), (27, 0.016), (28, 0.044), (29, -0.056), (30, 0.07), (31, 0.029), (32, 0.105), (33, 0.076), (34, 0.007), (35, -0.046), (36, -0.023), (37, 0.019), (38, -0.004), (39, -0.037), (40, 0.075), (41, -0.004), (42, 0.047), (43, 0.14), (44, -0.054), (45, 0.062), (46, 0.065), (47, 0.034), (48, -0.081), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9824307 <a title="38-lsi-1" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>2 0.84797418 <a title="38-lsi-2" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>Author: Silvia Richter, Douglas Aberdeen, Jin Yu</p><p>Abstract: Current road-trafﬁc optimisation practice around the world is a combination of hand tuned policies with a small degree of automatic adaption. Even state-ofthe-art research controllers need good models of the road trafﬁc, which cannot be obtained directly from existing sensors. We use a policy-gradient reinforcement learning approach to directly optimise the trafﬁc signals, mapping currently deployed sensor observations to control signals. Our trained controllers are (theoretically) compatible with the trafﬁc system used in Sydney and many other cities around the world. We apply two policy-gradient methods: (1) the recent natural actor-critic algorithm, and (2) a vanilla policy-gradient algorithm for comparison. Along the way we extend natural-actor critic approaches to work for distributed and online inﬁnite-horizon problems. 1</p><p>3 0.6498735 <a title="38-lsi-3" href="./nips-2006-Bayesian_Policy_Gradient_Algorithms.html">44 nips-2006-Bayesian Policy Gradient Algorithms</a></p>
<p>Author: Mohammad Ghavamzadeh, Yaakov Engel</p><p>Abstract: Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. Since Monte Carlo methods tend to have high variance, a large number of samples is required, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost. 1</p><p>4 0.64436954 <a title="38-lsi-4" href="./nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</a></p>
<p>Author: Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to ﬁrst recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.</p><p>5 0.55672681 <a title="38-lsi-5" href="./nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</a></p>
<p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><p>6 0.41760692 <a title="38-lsi-6" href="./nips-2006-Sample_Complexity_of_Policy_Search_with_Known_Dynamics.html">171 nips-2006-Sample Complexity of Policy Search with Known Dynamics</a></p>
<p>7 0.39900419 <a title="38-lsi-7" href="./nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</a></p>
<p>8 0.36667284 <a title="38-lsi-8" href="./nips-2006-Mutagenetic_tree_Fisher_kernel_improves_prediction_of_HIV_drug_resistance_from_viral_genotype.html">142 nips-2006-Mutagenetic tree Fisher kernel improves prediction of HIV drug resistance from viral genotype</a></p>
<p>9 0.30951798 <a title="38-lsi-9" href="./nips-2006-Near-Uniform_Sampling_of_Combinatorial_Spaces_Using_XOR_Constraints.html">144 nips-2006-Near-Uniform Sampling of Combinatorial Spaces Using XOR Constraints</a></p>
<p>10 0.28328472 <a title="38-lsi-10" href="./nips-2006-Handling_Advertisements_of_Unknown_Quality_in_Search_Advertising.html">89 nips-2006-Handling Advertisements of Unknown Quality in Search Advertising</a></p>
<p>11 0.28155178 <a title="38-lsi-11" href="./nips-2006-Multi-Robot_Negotiation%3A_Approximating_the_Set_of_Subgame_Perfect_Equilibria_in_General-Sum_Stochastic_Games.html">137 nips-2006-Multi-Robot Negotiation: Approximating the Set of Subgame Perfect Equilibria in General-Sum Stochastic Games</a></p>
<p>12 0.26941016 <a title="38-lsi-12" href="./nips-2006-Distributed_Inference_in_Dynamical_Systems.html">69 nips-2006-Distributed Inference in Dynamical Systems</a></p>
<p>13 0.25912201 <a title="38-lsi-13" href="./nips-2006-Learning_annotated_hierarchies_from_relational_data.html">115 nips-2006-Learning annotated hierarchies from relational data</a></p>
<p>14 0.25146395 <a title="38-lsi-14" href="./nips-2006-iLSTD%3A_Eligibility_Traces_and_Convergence_Analysis.html">202 nips-2006-iLSTD: Eligibility Traces and Convergence Analysis</a></p>
<p>15 0.2339143 <a title="38-lsi-15" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>16 0.21725358 <a title="38-lsi-16" href="./nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</a></p>
<p>17 0.19709639 <a title="38-lsi-17" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>18 0.19423378 <a title="38-lsi-18" href="./nips-2006-Linearly-solvable_Markov_decision_problems.html">124 nips-2006-Linearly-solvable Markov decision problems</a></p>
<p>19 0.19037533 <a title="38-lsi-19" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<p>20 0.18052295 <a title="38-lsi-20" href="./nips-2006-Online_Clustering_of_Moving_Hyperplanes.html">153 nips-2006-Online Clustering of Moving Hyperplanes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.081), (3, 0.017), (7, 0.049), (9, 0.049), (20, 0.017), (22, 0.032), (44, 0.073), (57, 0.079), (59, 0.388), (65, 0.042), (69, 0.028), (71, 0.027), (90, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.72021621 <a title="38-lda-1" href="./nips-2006-Combining_causal_and_similarity-based_reasoning.html">53 nips-2006-Combining causal and similarity-based reasoning</a></p>
<p>Author: Charles Kemp, Patrick Shafto, Allison Berke, Joshua B. Tenenbaum</p><p>Abstract: Everyday inductive reasoning draws on many kinds of knowledge, including knowledge about relationships between properties and knowledge about relationships between objects. Previous accounts of inductive reasoning generally focus on just one kind of knowledge: models of causal reasoning often focus on relationships between properties, and models of similarity-based reasoning often focus on similarity relationships between objects. We present a Bayesian model of inductive reasoning that incorporates both kinds of knowledge, and show that it accounts well for human inferences about the properties of biological species. 1</p><p>same-paper 2 0.71038997 <a title="38-lda-2" href="./nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</a></p>
<p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><p>3 0.37650055 <a title="38-lda-3" href="./nips-2006-Recursive_ICA.html">167 nips-2006-Recursive ICA</a></p>
<p>Author: Honghao Shan, Lingyun Zhang, Garrison W. Cottrell</p><p>Abstract: Independent Component Analysis (ICA) is a popular method for extracting independent features from visual data. However, as a fundamentally linear technique, there is always nonlinear residual redundancy that is not captured by ICA. Hence there have been many attempts to try to create a hierarchical version of ICA, but so far none of the approaches have a natural way to apply them more than once. Here we show that there is a relatively simple technique that transforms the absolute values of the outputs of a previous application of ICA into a normal distribution, to which ICA maybe applied again. This results in a recursive ICA algorithm that may be applied any number of times in order to extract higher order structure from previous layers. 1</p><p>4 0.37403202 <a title="38-lda-4" href="./nips-2006-Denoising_and_Dimension_Reduction_in_Feature_Space.html">65 nips-2006-Denoising and Dimension Reduction in Feature Space</a></p>
<p>Author: Mikio L. Braun, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: We show that the relevant information about a classiﬁcation problem in feature space is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem. Thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data to perform classiﬁcation. Practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classiﬁcation results. 1</p><p>5 0.37360761 <a title="38-lda-5" href="./nips-2006-Analysis_of_Empirical_Bayesian_Methods_for_Neuroelectromagnetic_Source_Localization.html">32 nips-2006-Analysis of Empirical Bayesian Methods for Neuroelectromagnetic Source Localization</a></p>
<p>Author: Rey Ramírez, Jason Palmer, Scott Makeig, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: The ill-posed nature of the MEG/EEG source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an inﬁnite set of candidates. Bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantiﬁed. Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior. While seemingly quite different in many respects, we apply a unifying framework based on automatic relevance determination (ARD) that elucidates various attributes of these methods and suggests directions for improvement. We also derive theoretical properties of this methodology related to convergence, local minima, and localization bias and explore connections with established algorithms. 1</p><p>6 0.3733277 <a title="38-lda-6" href="./nips-2006-A_Complexity-Distortion_Approach_to_Joint_Pattern_Alignment.html">3 nips-2006-A Complexity-Distortion Approach to Joint Pattern Alignment</a></p>
<p>7 0.37283877 <a title="38-lda-7" href="./nips-2006-Simplifying_Mixture_Models_through_Function_Approximation.html">175 nips-2006-Simplifying Mixture Models through Function Approximation</a></p>
<p>8 0.37226596 <a title="38-lda-8" href="./nips-2006-Optimal_Change-Detection_and_Spiking_Neurons.html">154 nips-2006-Optimal Change-Detection and Spiking Neurons</a></p>
<p>9 0.37194809 <a title="38-lda-9" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>10 0.37063435 <a title="38-lda-10" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>11 0.37010702 <a title="38-lda-11" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>12 0.36940989 <a title="38-lda-12" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>13 0.36835572 <a title="38-lda-13" href="./nips-2006-Natural_Actor-Critic_for_Road_Traffic_Optimisation.html">143 nips-2006-Natural Actor-Critic for Road Traffic Optimisation</a></p>
<p>14 0.3675285 <a title="38-lda-14" href="./nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">158 nips-2006-PG-means: learning the number of clusters in data</a></p>
<p>15 0.36709338 <a title="38-lda-15" href="./nips-2006-Learning_on_Graph_with_Laplacian_Regularization.html">117 nips-2006-Learning on Graph with Laplacian Regularization</a></p>
<p>16 0.36488032 <a title="38-lda-16" href="./nips-2006-Emergence_of_conjunctive_visual_features_by_quadratic_independent_component_analysis.html">76 nips-2006-Emergence of conjunctive visual features by quadratic independent component analysis</a></p>
<p>17 0.36480838 <a title="38-lda-17" href="./nips-2006-Part-based_Probabilistic_Point_Matching_using_Equivalence_Constraints.html">160 nips-2006-Part-based Probabilistic Point Matching using Equivalence Constraints</a></p>
<p>18 0.36408526 <a title="38-lda-18" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>19 0.36401799 <a title="38-lda-19" href="./nips-2006-Efficient_Learning_of_Sparse_Representations_with_an_Energy-Based_Model.html">72 nips-2006-Efficient Learning of Sparse Representations with an Energy-Based Model</a></p>
<p>20 0.36327133 <a title="38-lda-20" href="./nips-2006-Bayesian_Detection_of_Infrequent_Differences_in_Sets_of_Time_Series_with_Shared_Structure.html">40 nips-2006-Bayesian Detection of Infrequent Differences in Sets of Time Series with Shared Structure</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
