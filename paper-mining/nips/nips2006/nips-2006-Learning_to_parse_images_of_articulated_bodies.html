<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2006-Learning to parse images of articulated bodies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-122" href="#">nips2006-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 nips-2006-Learning to parse images of articulated bodies</h1>
<br/><p>Source: <a title="nips-2006-122-pdf" href="http://papers.nips.cc/paper/2976-learning-to-parse-images-of-articulated-bodies.pdf">pdf</a></p><p>Author: Deva Ramanan</p><p>Abstract: We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database. 1</p><p>Reference: <a title="nips-2006-122-reference" href="../nips2006_reference/nips-2006-Learning_to_parse_images_of_articulated_bodies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning to parse images of articulated bodies  Deva Ramanan Toyota Technological Institute at Chicago Chicago, IL 60637 ramanan@tti-c. [sent-1, score-0.384]
</p><p>2 org  Abstract We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. [sent-2, score-0.412]
</p><p>3 Following a established line of research, pose estimation is framed as inference in a probabilistic model. [sent-4, score-0.382]
</p><p>4 Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. [sent-6, score-0.331]
</p><p>5 We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. [sent-7, score-0.451]
</p><p>6 Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database. [sent-8, score-0.393]
</p><p>7 1  Introduction  We consider the machine vision task of pose estimation from static images, speciﬁcally for the case of articulated objects. [sent-9, score-0.412]
</p><p>8 Following a established line of research, pose estimation is framed as inference in a probabilistic model. [sent-11, score-0.382]
</p><p>9 When reliable features can be extracted (through say, background subtraction or skin detection), approaches tend to do well. [sent-13, score-0.225]
</p><p>10 Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. [sent-16, score-0.331]
</p><p>11 Since our approach is fairly general (we do not use any skin or face detectors), we also apply it to estimate horse poses from the Weizmann dataset [1]. [sent-17, score-0.313]
</p><p>12 Another practical difﬁculty, speciﬁcally with pose estimation, is that of reporting results. [sent-18, score-0.27]
</p><p>13 This is because the posterior of body poses is often multimodal, a single MAP/mode estimate won’t summarize it. [sent-20, score-0.345]
</p><p>14 We calculate the probability of observing the actual pose under the distribution returned by our algorithm. [sent-22, score-0.27]
</p><p>15 Related Work: Human pose estimation from static images is a very active research area. [sent-24, score-0.46]
</p><p>16 Our work relies on the conditional random ﬁeld (CRF) notion of deformable matching in [9]. [sent-26, score-0.504]
</p><p>17 Our approach is related to those that simultaneously estimate pose and segment an image [7, 10, 2, 5], since we learn low-level segmentation cues to build part-speciﬁc region models. [sent-27, score-0.712]
</p><p>18 We describe an iterative algorithm for pose estimation that learns a region model for each body part and for the background. [sent-32, score-0.774]
</p><p>19 1  Overview  Assume we are given an image of a person, who happens to be a soccer player wearing a white shirt on a green playing ﬁeld (Fig. [sent-37, score-0.229]
</p><p>20 We match an edge-based deformable model to the image to obtain (soft) estimates of body part positions. [sent-42, score-0.897]
</p><p>21 The algorithm uses the estimated body part positions to build a rough region model for each body part and the background – it might learn that the torso is white-ish and the background is green-ish. [sent-47, score-1.092]
</p><p>22 The algorithm then builds a region-based deformable model that looks for white torsos. [sent-48, score-0.54]
</p><p>23 Soft estimates of body position from the new model are then used to build new region models, and the process is repeated. [sent-49, score-0.389]
</p><p>24 As one might suspect, such an iterative procedure is quite sensitive to its starting point – the edgebased deformable model used for initialization and the region-based deformable model used in the ﬁrst iteration prove crucial. [sent-50, score-1.25]
</p><p>25 3), most of this paper deals with smart ways of building the deformable models. [sent-52, score-0.504]
</p><p>26 2  Edge-based deformable model  Our edge-based deformable model is an extension of the one proposed in [9]. [sent-53, score-1.08]
</p><p>27 Let the location of each part li be paraminitial parse missing arm  torso  head  ru−arm  ll−leg  hallucinated leg  Figure 2: We build a deformable pose model based on edges. [sent-55, score-2.066]
</p><p>28 Given an image I, we use a edgebased deformable model (middle) to compute body part locations P(L|I). [sent-56, score-0.947]
</p><p>29 This deﬁnes an initial parse of the image into several body part regions right. [sent-57, score-0.616]
</p><p>30 It is easy to hallucinate extra arms or legs in the negatives spaces between actual body parts (the extra leg). [sent-58, score-0.449]
</p><p>31 When a body part is surrounded by clutter (the right arm), it is hard to localize. [sent-59, score-0.289]
</p><p>32 The green region in between the legs is a poor leg candidate because of ﬁgure/ground cues – it groups better with the background grass. [sent-61, score-0.479]
</p><p>33 Also, we can ﬁnd left/right limb pairs by appealing to symmetry – if one limb is visible, we can build a model of its appearance, and use it to ﬁnd the other one. [sent-62, score-0.326]
</p><p>34 We operationalize both these notions by our iterative parsing procedure in Fig. [sent-63, score-0.215]
</p><p>35 We deﬁne a parse to be a soft labeling of pixels into a region type (bg,torso,left lower arm, etc. [sent-66, score-0.375]
</p><p>36 To exploit symmetry in appearance, we learn a single color model for left/right limb pairs. [sent-71, score-0.344]
</p><p>37 We then use these masks as features for a deformable model that re-computes P(L|I). [sent-73, score-0.587]
</p><p>38 final parse  sample poses best pose  torso  head  ru−arm  ll−leg  input  Figure 4: The result of our procedure. [sent-75, score-0.863]
</p><p>39 Given P(L|I) from the ﬁnal iteration, we obtain a clean parse ˆ for the image. [sent-76, score-0.23]
</p><p>40 We can write the deformable model as a log-linear model     P(L|I) ∝ exp  i,j∈E  ψ(li − lj ) +  φ(li )  (1)  i  Ψ(li − lj ) corresponds to a spatial prior on the relative arrangement of part i and j. [sent-84, score-1.025]
</p><p>41 T ψ(li − li ) =αi bin(li − lj )  (2)  Doing so allows us to capture more intricate distributions, at the cost of having more parameters to ﬁt. [sent-88, score-0.417]
</p><p>42 Here αi is a model parameter that favors certain (relative) spatial and angular bins for part i with respect to its parent. [sent-90, score-0.235]
</p><p>43 Figure 5: We record the spatial conﬁguration of an arm given the torso by placing a grid on the torso, and noting which bin the arm falls into. [sent-91, score-0.596]
</p><p>44 We center the grid at the average location of arm in the training data. [sent-92, score-0.251]
</p><p>45 We likewise bin the angular orientations to deﬁne a spatial distribution of arms given torsos. [sent-93, score-0.262]
</p><p>46 Φ(li ) corresponds to the local image evidence for a part, which we deﬁne as T φ(li ) =βi fi (I(li ))  (3)  We write fi (I(li )) for feature vector extracted from the oriented image patch at location li . [sent-94, score-0.559]
</p><p>47 Since E is a tree, we ﬁrst pass “upstream” messages from part i to its parent j We compute the message from part i to j as mi (lj ) ∝  ψ(li − lj )ai (li )  (4)  lj  ai (li ) ∝ φ(li )  mk (li )  (5)  k∈kidsi  Message passing can be performed exhaustively and efﬁciently with convolutions. [sent-100, score-0.594]
</p><p>48 If we temporarily ignore orientation and think of li = (xi , yi ), we can represent messages as 2D images. [sent-101, score-0.323]
</p><p>49 The image ai is obtained by multiplying together response images from the children of part i and from the imaging model φ(li ). [sent-102, score-0.369]
</p><p>50 φ(li ) can be computed by convolving the edge image with the ﬁlter βi . [sent-103, score-0.232]
</p><p>51 mi (lj ) can be computed by convolving ai with a spatial ﬁlter extending over the bins from Fig. [sent-104, score-0.192]
</p><p>52 This means that in practice, computing φ(li ) is the computational bottleneck, since that requires convolving the edge image repeatedly with rotated versions of ﬁlter βi . [sent-109, score-0.293]
</p><p>53 Starting from the root, we can pass messages downstream from part j to part i (again with convolutions) P(li |I) ∝ ai (li )  ψ(li − lj )P(lj |I)  (6)  lj  For numerical stability, we normalize images to 1 as they are computed. [sent-110, score-0.695]
</p><p>54 We label training images with body part locations L, and ﬁnd the ﬁlters that maximize P(L|I) for the training set. [sent-114, score-0.432]
</p><p>55 3  Building a region model  One can use the marginals (for say, the head) to deﬁne a soft labeling for the image into head/nonhead pixels. [sent-117, score-0.277]
</p><p>56 One can do this by repeatedly sampling a head location (according to P(li |I)) and then  rendering a head at the given location and orientation. [sent-118, score-0.26]
</p><p>57 Let the rendered appearance for part i be an image patch si ; we use a simple rectangular mask. [sent-119, score-0.291]
</p><p>58 In the limit of inﬁnite samples, one will obtain an image P(xi , yi , θi |I)sθi (x − xi , y − yi ) i  pi (x, y) =  (7)  xi ,yi ,θi  We call such an image a parse for part i (the images on the right from Fig. [sent-120, score-0.71]
</p><p>59 Given the parse image pi , we learn a color histogram model for part i and “its” background. [sent-123, score-0.726]
</p><p>60 P(f gi (k)) ∝  pi (x, y)δ(im(x, y) = k)  (8)  (1 − pi (x, y))δ(im(x, y) = k)  (9)  x,y  P(bgi (k)) ∝ x,y  We use the part-speciﬁc histogram models to label each pixel as foreground or background with a likelihood ratio test (as shown in Fig. [sent-124, score-0.252]
</p><p>61 To enforce symmetry in appearance, we learn a single color model for left/right limb pairs. [sent-126, score-0.344]
</p><p>62 4  Region-based deformable model  After an initial parse, our algorithm has built an initial region model for each part (and its background). [sent-127, score-0.82]
</p><p>63 We write the oriented patch features extracted from these label images as fir (for “region”-based). [sent-129, score-0.291]
</p><p>64 We want to use these features to help re-estimate the pose in an image – we using training data to learn how to do so. [sent-130, score-0.528]
</p><p>65 We learn model parameters for a region-based deformable model Θr by CRF parameter estimation, as in Sec. [sent-131, score-0.656]
</p><p>66 When learning Θr from training data, deﬁning fir is tricky – should we use the ground-truth part locations to learn the color histogram models? [sent-133, score-0.422]
</p><p>67 Doing so might be unrealistic – it assumes at “runtime”, the edge-based deformable model will always correctly estimate part locations. [sent-134, score-0.622]
</p><p>68 Rather, we run the edge-based model on the training data, and use the resulting parses to learn the color histogram models. [sent-135, score-0.366]
</p><p>69 When applying the region-based deformable model, we have already computed the edge responses e φe (li ) = βi T f e (I(li )) (to train the region model). [sent-137, score-0.677]
</p><p>70 If this was the case, one would learn a zero weight for r the edge feature when learning βi from training data. [sent-140, score-0.184]
</p><p>71 We learn roughly equal weights for the edge and region features, indicating both cues are complementary rather than redundant. [sent-141, score-0.307]
</p><p>72 Given the parse from the region-based model, we can re-learn a color model for each part and the background (and re-parse given the new models, and iterate). [sent-142, score-0.514]
</p><p>73 We have amassed a dataset of 305 images of people in interesting poses (which will be available on the author’s webpage). [sent-147, score-0.376]
</p><p>74 To our knowledge, it is the largest labeled dataset available for human pose recognition. [sent-149, score-0.352]
</p><p>75 Evalutation: Given an image, our parsing procedure returns a distribution over poses P(L|I). [sent-151, score-0.321]
</p><p>76 Ideally, we want the true pose to have a high probability, and all other poses to have a low value. [sent-152, score-0.403]
</p><p>77 Given ˆ a set of T test images each with a labeled ground-truth pose Lt , we score performance by computing 1 ˆ t |It ). [sent-153, score-0.403]
</p><p>78 − T t log P(L  Figure 6: We visualize the part models for our deformable templates – light areas correspond to positive βi weights, and dark corresponds to negative. [sent-155, score-0.707]
</p><p>79 It is crucial to initialize our iterative procedure with a good edge-based deformable model. [sent-156, score-0.594]
</p><p>80 Given a collection of training images with labeled body parts, one could build an edge template for each part by averaging (left) – this is the standard maxie mum likelihood (ML) solution. [sent-157, score-0.536]
</p><p>81 3 is also r very crucial – we similarly learn region-based part templates βi with a CRF (right). [sent-161, score-0.25]
</p><p>82 These templates focus more on region cues rather than edges. [sent-162, score-0.246]
</p><p>83 These templates appear more sophisticated than rectangle-based limb detectors [8, 9] – for example, to ﬁnd upper arms and legs, it seems important to emphasize the edge facing away from the body. [sent-163, score-0.388]
</p><p>84 For each image, our parsing procedure returns a distribution of poses. [sent-174, score-0.188]
</p><p>85 We evaluate our algorithm by looking at a perplexity-based score [11] – the negative log probability of the ground truth pose given the estimated distribution, averaged over the test set. [sent-175, score-0.302]
</p><p>86 On the left, we look at the large datasets of people and horses (each with 300 images). [sent-176, score-0.231]
</p><p>87 Iter0 corresponds to the distribution computed by the edge-based model, while Iter1 and Iter2 show the results after our iterative parsing with a region-based model. [sent-177, score-0.187]
</p><p>88 We localize some difﬁcult poses quite well, and furthermore, the estimated posterior P(L|I) oftentimes reﬂects actual ambiguity in the data (ie, if multiple people are present). [sent-187, score-0.295]
</p><p>89 The posterior pose distribution often captures the non-rigid deformations in the body. [sent-192, score-0.336]
</p><p>90 This suggests we can use the uncertainty in our deformable matching algorithm to recover extra information about the object. [sent-193, score-0.541]
</p><p>91 Discussion: We have described an iterative parsing approach to pose estimation. [sent-196, score-0.457]
</p><p>92 Starting with an edge-based detector, we obtain an initial parse and iteratively build better features with which to subsequently parse. [sent-197, score-0.376]
</p><p>93 In many cases the posterior is ambiguous because the image is (ie, multiple people are present). [sent-207, score-0.228]
</p><p>94 In particular, it may be surprising that the pair in the bottom-right both are recognized by the region model – this suggests that the the iter-region dissimilarity learned by the color histograms is a much stronger than the foreground similarity. [sent-208, score-0.269]
</p><p>95 Posecut: simultaneous segmentation and 3d pose estimation of humans using dynamic graph-cuts. [sent-214, score-0.349]
</p><p>96 Learning to estimate human pose with data driven belief propagation. [sent-230, score-0.309]
</p><p>97 6 – the posterior can capture rich non-rigid deformations of body parts. [sent-241, score-0.245]
</p><p>98 The Weizmann set of horses seems to be easier than our people dataset - we quantify this with a perplexity score in Table 1. [sent-242, score-0.35]
</p><p>99 Proposal maps driven mcmc for estimating human body pose in static images. [sent-246, score-0.536]
</p><p>100 Recovering human body conﬁgurations using pairwise constraints between parts. [sent-271, score-0.218]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deformable', 0.504), ('pose', 0.27), ('li', 0.25), ('parse', 0.23), ('body', 0.179), ('arm', 0.176), ('lj', 0.167), ('torso', 0.14), ('leg', 0.134), ('poses', 0.133), ('horses', 0.132), ('parsing', 0.125), ('crf', 0.112), ('arms', 0.112), ('region', 0.104), ('images', 0.101), ('iter', 0.101), ('ramanan', 0.101), ('people', 0.099), ('image', 0.096), ('color', 0.096), ('head', 0.09), ('templates', 0.088), ('limb', 0.088), ('legs', 0.084), ('part', 0.082), ('learn', 0.08), ('weizmann', 0.08), ('bin', 0.071), ('appearance', 0.071), ('background', 0.07), ('skin', 0.07), ('build', 0.07), ('cvpr', 0.07), ('edge', 0.069), ('convolving', 0.067), ('fir', 0.066), ('histogram', 0.063), ('iterative', 0.062), ('im', 0.061), ('parses', 0.056), ('ai', 0.054), ('cues', 0.054), ('articulated', 0.053), ('bgi', 0.05), ('edgebased', 0.05), ('fie', 0.05), ('shirt', 0.05), ('wearing', 0.05), ('static', 0.048), ('features', 0.047), ('angular', 0.046), ('edges', 0.045), ('symmetry', 0.044), ('hallucinated', 0.044), ('perplexity', 0.044), ('dataset', 0.043), ('pi', 0.043), ('patch', 0.042), ('messages', 0.042), ('soft', 0.041), ('estimation', 0.041), ('ru', 0.04), ('location', 0.04), ('human', 0.039), ('bins', 0.038), ('tend', 0.038), ('segmentation', 0.038), ('framed', 0.037), ('convolutions', 0.037), ('horse', 0.037), ('extra', 0.037), ('model', 0.036), ('oriented', 0.035), ('casting', 0.035), ('training', 0.035), ('returns', 0.035), ('inference', 0.034), ('deformations', 0.033), ('foreground', 0.033), ('visualize', 0.033), ('rotated', 0.033), ('posterior', 0.033), ('spatial', 0.033), ('green', 0.033), ('score', 0.032), ('experience', 0.032), ('yi', 0.031), ('ren', 0.031), ('detectors', 0.031), ('ie', 0.031), ('fairly', 0.03), ('quite', 0.03), ('initial', 0.029), ('hard', 0.028), ('procedure', 0.028), ('versions', 0.028), ('recovering', 0.028), ('sequentially', 0.028), ('eccv', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="122-tfidf-1" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>Author: Deva Ramanan</p><p>Abstract: We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database. 1</p><p>2 0.27256098 <a title="122-tfidf-2" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>Author: Alessandro Bissacco, Ming-Hsuan Yang, Stefano Soatto</p><p>Abstract: We consider the problem of detecting humans and classifying their pose from a single image. Speciﬁcally, our goal is to devise a statistical model that simultaneously answers two questions: 1) is there a human in the image? and, if so, 2) what is a low-dimensional representation of her pose? We investigate models that can be learned in an unsupervised manner on unlabeled images of human poses, and provide information that can be used to match the pose of a new image to the ones present in the training set. Starting from a set of descriptors recently proposed for human detection, we apply the Latent Dirichlet Allocation framework to model the statistics of these features, and use the resulting model to answer the above questions. We show how our model can efﬁciently describe the space of images of humans with their pose, by providing an effective representation of poses for tasks such as classiﬁcation and matching, while performing remarkably well in human/non human decision problems, thus enabling its use for human detection. We validate the model with extensive quantitative experiments and comparisons with other approaches on human detection and pose matching. 1</p><p>3 0.11501876 <a title="122-tfidf-3" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><p>4 0.10874978 <a title="122-tfidf-4" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>Author: Andrea Frome, Yoram Singer, Jitendra Malik</p><p>Abstract: In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classiﬁcation of novel images. On the Caltech 101 object recognition benchmark, we achieve 60.3% mean recognition across classes using 15 training images per class, which is better than the best published performance by Zhang, et al. 1</p><p>5 0.094331272 <a title="122-tfidf-5" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>Author: Samuel S. Gross, Olga Russakovsky, Chuong B. Do, Serafim Batzoglou</p><p>Abstract: We consider the problem of training a conditional random ﬁeld (CRF) to maximize per-label predictive accuracy on a training set, an approach motivated by the principle of empirical risk minimization. We give a gradient-based procedure for minimizing an arbitrarily accurate approximation of the empirical risk under a Hamming loss function. In experiments with both simulated and real data, our optimization procedure gives signiﬁcantly better testing performance than several current approaches for CRF training, especially in situations of high label noise. 1</p><p>6 0.090969585 <a title="122-tfidf-6" href="./nips-2006-Scalable_Discriminative_Learning_for_Natural_Language_Parsing_and_Translation.html">172 nips-2006-Scalable Discriminative Learning for Natural Language Parsing and Translation</a></p>
<p>7 0.085212633 <a title="122-tfidf-7" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>8 0.084484823 <a title="122-tfidf-8" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>9 0.082118243 <a title="122-tfidf-9" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>10 0.07474561 <a title="122-tfidf-10" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>11 0.074037939 <a title="122-tfidf-11" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>12 0.073009081 <a title="122-tfidf-12" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>13 0.070916429 <a title="122-tfidf-13" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>14 0.069585957 <a title="122-tfidf-14" href="./nips-2006-A_Switched_Gaussian_Process_for_Estimating_Disparity_and_Segmentation_in_Binocular_Stereo.html">15 nips-2006-A Switched Gaussian Process for Estimating Disparity and Segmentation in Binocular Stereo</a></p>
<p>15 0.068700269 <a title="122-tfidf-15" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>16 0.061571393 <a title="122-tfidf-16" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>17 0.061170399 <a title="122-tfidf-17" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>18 0.060292009 <a title="122-tfidf-18" href="./nips-2006-Comparative_Gene_Prediction_using_Conditional_Random_Fields.html">54 nips-2006-Comparative Gene Prediction using Conditional Random Fields</a></p>
<p>19 0.05911788 <a title="122-tfidf-19" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>20 0.057956144 <a title="122-tfidf-20" href="./nips-2006-Causal_inference_in_sensorimotor_integration.html">49 nips-2006-Causal inference in sensorimotor integration</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.196), (1, 0.016), (2, 0.148), (3, -0.126), (4, 0.066), (5, -0.056), (6, -0.146), (7, -0.108), (8, -0.0), (9, -0.083), (10, 0.051), (11, 0.048), (12, 0.005), (13, 0.081), (14, 0.133), (15, 0.063), (16, -0.063), (17, -0.058), (18, 0.061), (19, -0.058), (20, 0.002), (21, -0.032), (22, -0.007), (23, 0.011), (24, 0.038), (25, -0.064), (26, 0.039), (27, -0.14), (28, -0.007), (29, -0.031), (30, -0.213), (31, -0.0), (32, -0.106), (33, -0.008), (34, -0.064), (35, -0.043), (36, -0.04), (37, -0.164), (38, -0.043), (39, -0.2), (40, -0.004), (41, 0.01), (42, -0.016), (43, 0.054), (44, -0.005), (45, -0.141), (46, -0.03), (47, -0.014), (48, 0.06), (49, -0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94595146 <a title="122-lsi-1" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>Author: Deva Ramanan</p><p>Abstract: We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database. 1</p><p>2 0.7906723 <a title="122-lsi-2" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>Author: Alessandro Bissacco, Ming-Hsuan Yang, Stefano Soatto</p><p>Abstract: We consider the problem of detecting humans and classifying their pose from a single image. Speciﬁcally, our goal is to devise a statistical model that simultaneously answers two questions: 1) is there a human in the image? and, if so, 2) what is a low-dimensional representation of her pose? We investigate models that can be learned in an unsupervised manner on unlabeled images of human poses, and provide information that can be used to match the pose of a new image to the ones present in the training set. Starting from a set of descriptors recently proposed for human detection, we apply the Latent Dirichlet Allocation framework to model the statistics of these features, and use the resulting model to answer the above questions. We show how our model can efﬁciently describe the space of images of humans with their pose, by providing an effective representation of poses for tasks such as classiﬁcation and matching, while performing remarkably well in human/non human decision problems, thus enabling its use for human detection. We validate the model with extensive quantitative experiments and comparisons with other approaches on human detection and pose matching. 1</p><p>3 0.64363503 <a title="122-lsi-3" href="./nips-2006-Robotic_Grasping_of_Novel_Objects.html">170 nips-2006-Robotic Grasping of Novel Objects</a></p>
<p>Author: Ashutosh Saxena, Justin Driemeyer, Justin Kearns, Andrew Y. Ng</p><p>Abstract: We consider the problem of grasping novel objects, speciﬁcally ones that are being seen for the ﬁrst time through vision. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Instead it predicts, directly as a function of the images, a point at which to grasp the object. Our algorithm is trained via supervised learning, using synthetic images for the training set. We demonstrate on a robotic manipulation platform that this approach successfully grasps a wide variety of objects, such as wine glasses, duct tape, markers, a translucent box, jugs, knife-cutters, cellphones, keys, screwdrivers, staplers, toothbrushes, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. 1</p><p>4 0.62649989 <a title="122-lsi-4" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>Author: Anitha Kannan, John Winn, Carsten Rother</p><p>Abstract: Patch-based appearance models are used in a wide range of computer vision applications. To learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand. In the jigsaw model presented here, the shape, size and appearance of patches are learned automatically from the repeated structures in a set of training images. By learning such irregularly shaped ‘jigsaw pieces’, we are able to discover both the shape and the appearance of object parts without supervision. When applied to face images, for example, the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes, noses, eyebrows and cheeks, to name a few. We conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection. This enables parts of similar appearance but different shapes to be distinguished; for example, while foreheads and cheeks are both skin colored, they have markedly different shapes. 1</p><p>5 0.59838653 <a title="122-lsi-5" href="./nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</a></p>
<p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><p>6 0.48464769 <a title="122-lsi-6" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<p>7 0.44828263 <a title="122-lsi-7" href="./nips-2006-Fast_Discriminative_Visual_Codebooks_using_Randomized_Clustering_Forests.html">78 nips-2006-Fast Discriminative Visual Codebooks using Randomized Clustering Forests</a></p>
<p>8 0.41514984 <a title="122-lsi-8" href="./nips-2006-Modeling_General_and_Specific_Aspects_of_Documents_with_a_Probabilistic_Topic_Model.html">133 nips-2006-Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model</a></p>
<p>9 0.40988317 <a title="122-lsi-9" href="./nips-2006-Isotonic_Conditional_Random_Fields_and_Local_Sentiment_Flow.html">101 nips-2006-Isotonic Conditional Random Fields and Local Sentiment Flow</a></p>
<p>10 0.39877313 <a title="122-lsi-10" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>11 0.39077556 <a title="122-lsi-11" href="./nips-2006-Subordinate_class_recognition_using_relational_object_models.html">185 nips-2006-Subordinate class recognition using relational object models</a></p>
<p>12 0.38919368 <a title="122-lsi-12" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>13 0.38203034 <a title="122-lsi-13" href="./nips-2006-Similarity_by_Composition.html">174 nips-2006-Similarity by Composition</a></p>
<p>14 0.37391576 <a title="122-lsi-14" href="./nips-2006-Efficient_Methods_for_Privacy_Preserving_Face_Detection.html">73 nips-2006-Efficient Methods for Privacy Preserving Face Detection</a></p>
<p>15 0.36195359 <a title="122-lsi-15" href="./nips-2006-Comparative_Gene_Prediction_using_Conditional_Random_Fields.html">54 nips-2006-Comparative Gene Prediction using Conditional Random Fields</a></p>
<p>16 0.3579922 <a title="122-lsi-16" href="./nips-2006-Blind_Motion_Deblurring_Using_Image_Statistics.html">45 nips-2006-Blind Motion Deblurring Using Image Statistics</a></p>
<p>17 0.34911165 <a title="122-lsi-17" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>18 0.34775665 <a title="122-lsi-18" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>19 0.34670144 <a title="122-lsi-19" href="./nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</a></p>
<p>20 0.34669483 <a title="122-lsi-20" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.055), (3, 0.025), (7, 0.079), (9, 0.026), (12, 0.375), (20, 0.013), (22, 0.042), (44, 0.061), (57, 0.146), (65, 0.048), (69, 0.035), (90, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83500421 <a title="122-lda-1" href="./nips-2006-Learning_to_parse_images_of_articulated_bodies.html">122 nips-2006-Learning to parse images of articulated bodies</a></p>
<p>Author: Deva Ramanan</p><p>Abstract: We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database. 1</p><p>2 0.80105644 <a title="122-lda-2" href="./nips-2006-Balanced_Graph_Matching.html">39 nips-2006-Balanced Graph Matching</a></p>
<p>Author: Timothee Cour, Praveen Srinivasan, Jianbo Shi</p><p>Abstract: Graph matching is a fundamental problem in Computer Vision and Machine Learning. We present two contributions. First, we give a new spectral relaxation technique for approximate solutions to matching problems, that naturally incorporates one-to-one or one-to-many constraints within the relaxation scheme. The second is a normalization procedure for existing graph matching scoring functions that can dramatically improve the matching accuracy. It is based on a reinterpretation of the graph matching compatibility matrix as a bipartite graph on edges for which we seek a bistochastic normalization. We evaluate our two contributions on a comprehensive test set of random graph matching problems, as well as on image correspondence problem. Our normalization procedure can be used to improve the performance of many existing graph matching algorithms, including spectral matching, graduated assignment and semideﬁnite programming. 1</p><p>3 0.66384327 <a title="122-lda-3" href="./nips-2006-Analysis_of_Representations_for_Domain_Adaptation.html">33 nips-2006-Analysis of Representations for Domain Adaptation</a></p>
<p>Author: Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira</p><p>Abstract: Discriminative learning methods for classiﬁcation perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classiﬁer which performs well on a target domain with a different distribution. Under what conditions can we adapt a classiﬁer trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justiﬁcation for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set. 1</p><p>4 0.49493214 <a title="122-lda-4" href="./nips-2006-Learning_Dense_3D_Correspondence.html">110 nips-2006-Learning Dense 3D Correspondence</a></p>
<p>Author: Florian Steinke, Volker Blanz, Bernhard Schölkopf</p><p>Abstract: Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difﬁcult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads. 1</p><p>5 0.4723002 <a title="122-lda-5" href="./nips-2006-Detecting_Humans_via_Their_Pose.html">66 nips-2006-Detecting Humans via Their Pose</a></p>
<p>Author: Alessandro Bissacco, Ming-Hsuan Yang, Stefano Soatto</p><p>Abstract: We consider the problem of detecting humans and classifying their pose from a single image. Speciﬁcally, our goal is to devise a statistical model that simultaneously answers two questions: 1) is there a human in the image? and, if so, 2) what is a low-dimensional representation of her pose? We investigate models that can be learned in an unsupervised manner on unlabeled images of human poses, and provide information that can be used to match the pose of a new image to the ones present in the training set. Starting from a set of descriptors recently proposed for human detection, we apply the Latent Dirichlet Allocation framework to model the statistics of these features, and use the resulting model to answer the above questions. We show how our model can efﬁciently describe the space of images of humans with their pose, by providing an effective representation of poses for tasks such as classiﬁcation and matching, while performing remarkably well in human/non human decision problems, thus enabling its use for human detection. We validate the model with extensive quantitative experiments and comparisons with other approaches on human detection and pose matching. 1</p><p>6 0.46351638 <a title="122-lda-6" href="./nips-2006-Fundamental_Limitations_of_Spectral_Clustering.html">80 nips-2006-Fundamental Limitations of Spectral Clustering</a></p>
<p>7 0.45643604 <a title="122-lda-7" href="./nips-2006-Temporal_and_Cross-Subject_Probabilistic_Models_for_fMRI_Prediction_Tasks.html">188 nips-2006-Temporal and Cross-Subject Probabilistic Models for fMRI Prediction Tasks</a></p>
<p>8 0.45195529 <a title="122-lda-8" href="./nips-2006-Approximate_Correspondences_in_High_Dimensions.html">34 nips-2006-Approximate Correspondences in High Dimensions</a></p>
<p>9 0.45084134 <a title="122-lda-9" href="./nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</a></p>
<p>10 0.44865638 <a title="122-lda-10" href="./nips-2006-Clustering_appearance_and_shape_by_learning_jigsaws.html">52 nips-2006-Clustering appearance and shape by learning jigsaws</a></p>
<p>11 0.44541463 <a title="122-lda-11" href="./nips-2006-A_Nonparametric_Approach_to_Bottom-Up_Visual_Saliency.html">8 nips-2006-A Nonparametric Approach to Bottom-Up Visual Saliency</a></p>
<p>12 0.44426629 <a title="122-lda-12" href="./nips-2006-Graph-Based_Visual_Saliency.html">86 nips-2006-Graph-Based Visual Saliency</a></p>
<p>13 0.43992162 <a title="122-lda-13" href="./nips-2006-Chained_Boosting.html">50 nips-2006-Chained Boosting</a></p>
<p>14 0.43952417 <a title="122-lda-14" href="./nips-2006-Bayesian_Image_Super-resolution%2C_Continued.html">42 nips-2006-Bayesian Image Super-resolution, Continued</a></p>
<p>15 0.4363687 <a title="122-lda-15" href="./nips-2006-Training_Conditional_Random_Fields_for_Maximum_Labelwise_Accuracy.html">195 nips-2006-Training Conditional Random Fields for Maximum Labelwise Accuracy</a></p>
<p>16 0.43424779 <a title="122-lda-16" href="./nips-2006-Multi-Instance_Multi-Label_Learning_with_Application_to_Scene_Classification.html">136 nips-2006-Multi-Instance Multi-Label Learning with Application to Scene Classification</a></p>
<p>17 0.43344286 <a title="122-lda-17" href="./nips-2006-Boosting_Structured_Prediction_for_Imitation_Learning.html">47 nips-2006-Boosting Structured Prediction for Imitation Learning</a></p>
<p>18 0.43305197 <a title="122-lda-18" href="./nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</a></p>
<p>19 0.43167424 <a title="122-lda-19" href="./nips-2006-Learning_Nonparametric_Models_for_Probabilistic_Imitation.html">112 nips-2006-Learning Nonparametric Models for Probabilistic Imitation</a></p>
<p>20 0.4307496 <a title="122-lda-20" href="./nips-2006-Image_Retrieval_and_Classification_Using_Local_Distance_Functions.html">94 nips-2006-Image Retrieval and Classification Using Local Distance Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
