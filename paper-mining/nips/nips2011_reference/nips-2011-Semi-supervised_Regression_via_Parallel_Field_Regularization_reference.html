<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>248 nips-2011-Semi-supervised Regression via Parallel Field Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-248" href="../nips2011/nips-2011-Semi-supervised_Regression_via_Parallel_Field_Regularization.html">nips2011-248</a> <a title="nips-2011-248-reference" href="#">nips2011-248-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>248 nips-2011-Semi-supervised Regression via Parallel Field Regularization</h1>
<br/><p>Source: <a title="nips-2011-248-pdf" href="http://papers.nips.cc/paper/4398-semi-supervised-regression-via-parallel-field-regularization.pdf">pdf</a></p><p>Author: Binbin Lin, Chiyuan Zhang, Xiaofei He</p><p>Abstract: This paper studies the problem of semi-supervised learning from the vector ﬁeld perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semisupervised regression problems. To achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient ﬁeld of a linear function has to be a parallel vector ﬁeld. Consequently, we propose to ﬁnd a function which minimizes the empirical error, and simultaneously requires its gradient ﬁeld to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efﬁciently. The experimental results have demonstrated the effectiveness of our proposed approach. 1</p><br/>
<h2>reference text</h2><p>[1] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.</p>
<p>[2] D. L. Donoho and C. E. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences of the United States of America, 100(10):5591–5596, 2003.</p>
<p>[3] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000.</p>
<p>[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems 14, pages 585–591. 2001.</p>
<p>[5] Fan R. K. Chung. Spectral Graph Theory, volume 92 of Regional Conference Series in Mathematics. AMS, 1997.</p>
<p>[6] X. Zhu and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proc. of the 20th Internation Conference on Machine Learning, 2003.</p>
<p>[7] D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and B. Sch¨lkopf. Learning with local and global o consistency. In Advances in Neural Information Processing Systems 16, 2003.</p>
<p>[8] Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Regularization and semi-supervised learning on large graphs. In Conference on Learning Theory, pages 624–638, 2004.</p>
<p>[9] John Lafferty and Larry Wasserman. Statistical analysis of semi-supervised regression. In Advances in Neural Information Processing Systems 20, pages 801–808, 2007.</p>
<p>[10] P. Petersen. Riemannian Geometry. Springer, New York, 1998.</p>
<p>[11] G. H. Golub and C. F. Van Loan. Matrix computations. Johns Hopkins University Press, 3rd edition, 1996.</p>
<p>[12] B. Chow, P. Lu, and L. Ni. Hamilton’s Ricci Flow. AMS, Providence, Rhode Island, 2006.</p>
<p>[13] Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based manifold methods. In Conference on Learning Theory, pages 486–500, 2005.</p>
<p>[14] Matthias Hein, Jean yves Audibert, and Ulrike Von Luxburg. From graphs to manifolds - weak and strong pointwise consistency of graph laplacians. In Conference on Learning Theory, pages 470–485, 2005.</p>
<p>[15] K. I. Kim, F. Steinke, and M. Hein. Semi-supervised regression using hessian energy with an application to semi-supervised dimensionality reduction. In Advances in Neural Information Processing Systems 22, pages 979–987. 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
