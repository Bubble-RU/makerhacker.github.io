<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>240 nips-2011-Robust Multi-Class Gaussian Process Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-240" href="../nips2011/nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">nips2011-240</a> <a title="nips-2011-240-reference" href="#">nips2011-240-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>240 nips-2011-Robust Multi-Class Gaussian Process Classification</h1>
<br/><p>Source: <a title="nips-2011-240-pdf" href="http://papers.nips.cc/paper/4241-robust-multi-class-gaussian-process-classification.pdf">pdf</a></p><p>Author: Daniel Hernández-lobato, Jose M. Hernández-lobato, Pierre Dupont</p><p>Abstract: Multi-class Gaussian Process Classiﬁers (MGPCs) are often affected by overﬁtting problems when labeling errors occur far from the decision boundaries. To prevent this, we investigate a robust MGPC (RMGPC) which considers labeling errors independently of their distance to the decision boundaries. Expectation propagation is used for approximate inference. Experiments with several datasets in which noise is injected in the labels illustrate the beneﬁts of RMGPC. This method performs better than other Gaussian process alternatives based on considering latent Gaussian noise or heavy-tailed processes. When no noise is injected in the labels, RMGPC still performs equal or better than the other methods. Finally, we show how RMGPC can be used for successfully identifying data instances which are difﬁcult to classify correctly in practice. 1</p><br/>
<h2>reference text</h2><p>[1] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2006.</p>
<p>[2] Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.</p>
<p>[3] Hyun-Chul Kim and Zoubin Ghahramani. Bayesian Gaussian process classiﬁcation with the EM-EP algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(12):1948–1959, 2006.</p>
<p>[4] R.M Neal. Regression and classiﬁcation using Gaussian process priors. Bayesian Statistics, 6:475–501, 1999.</p>
<p>[5] Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classiﬁcation with multiple classes. Technical report, University of California, Berkeley, 2004.</p>
<p>[6] M. Opper and O. Winther. Gaussian process classiﬁcation and SVM: Mean ﬁeld results. In P. Bartlett, B.Schoelkopf, D. Schuurmans, and A. Smola, editors, Advances in large margin classiﬁers, pages 43–65. MIT Press, 2000.</p>
<p>[7] Daniel Hern´ ndez-Lobato and Jos´ Miguel Hern´ ndez-Lobato. Bayes machines for binary a e a classiﬁcation. Pattern Recognition Letters, 29(10):1466–1473, 2008.</p>
<p>[8] Hyun-Chul Kim and Zoubin Ghahramani. Outlier robust Gaussian process classiﬁcation. In Structural, Syntactic, and Statistical Pattern Recognition, volume 5342 of Lecture Notes in Computer Science, pages 896–905. Springer Berlin / Heidelberg, 2008.</p>
<p>[9] Fabian L. Wauthier and Michael I. Jordan. Heavy-Tailed Process Priors for Selective Shrinkage. In J. Lafferty, C. K. I. Williams, R. Zemel, J. Shawe-Taylor, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2406–2414. 2010.</p>
<p>[10] Thomas Minka. A Family of Algorithms for approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, 2001.</p>
<p>[11] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.</p>
<p>[12] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines, 2001.</p>
<p>[13] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, August 2006.</p>
<p>[14] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Adnan Darwiche and Nir Friedman, editors, Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence, pages 352–359. Morgan Kaufmann, 2002.</p>
<p>[15] Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005.</p>
<p>[16] H Nickisch and CE Rasmussen. Approximations for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 9:2035–2078, 10 2008.</p>
<p>[17] Marcel Van Gerven, Botond Cseke, Robert Oostenveld, and Tom Heskes. Bayesian source localization with the multivariate Laplace prior. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1901–1909, 2009.</p>
<p>[18] Matthias Seeger. Expectation propagation for exponential families. Technical report, Department of EECS, University of California, Berkeley, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
