<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-189" href="../nips2011/nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">nips2011-189</a> <a title="nips-2011-189-reference" href="#">nips2011-189-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</h1>
<br/><p>Source: <a title="nips-2011-189-pdf" href="http://papers.nips.cc/paper/4265-non-parametric-group-orthogonal-matching-pursuit-for-sparse-learning-with-multiple-kernels.pdf">pdf</a></p><p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><br/>
<h2>reference text</h2><p>[1] N. Aronszajn. Theory of reproducing kernel hilbert spaces. Transactions of the American Mathematical Society, 68(3):337–404, 1950.</p>
<p>[2] F. Bach. Consistency of group lasso and multiple kernel learning. JMLR, 9:1179–1225, 2008.</p>
<p>[3] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. In Technical report, HAL 00413473, 2009.</p>
<p>[4] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. In Technical report, HAL 00413473, 2010. 4  see http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/protsubloc-wabi08-supp.pdf  8</p>
<p>[5] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In ICML, 2004.</p>
<p>[6] P. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 3:463–482, 2002.</p>
<p>[7] A. Ben-Hur and W. S. Noble. Kernel methods for predicting protein–protein interactions. Bioinformatics, 21, January 2005.</p>
<p>[8] C. Cortes, M. Mohri, and Afshin Rostamizadeh. Generalization bounds for learning kernels. In ICML, 2010.</p>
<p>[9] A. K. Fletcher and S. Rangan. Orthogonal matching pursuit from noisy measurements: A new analysis. In NIPS, 2009.</p>
<p>[10] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. lp -norm multiple kernel learning. JMLR, 12:953–997, 2011.</p>
<p>[11] M. Kloft, U. Ruckert, and P. Bartlett. A unifying view of multiple kernel learning. In European Conference on Machine Learning (ECML), 2010.</p>
<p>[12] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics, 38(6):3660– 3695, 2010.</p>
<p>[13] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. J. Mach. Learn. Res., 5:27–72, December 2004.</p>
<p>[14] G. R. G. Lanckriet, T. De Bie, N. Cristianini, M. I. Jordan, and W. S. Noble. A statistical framework for genomic data fusion. Bioinformatics, 20, November 2004.</p>
<p>[15] A. C. Lozano and V. Sindhwani. Block variable selection in multivariate regression and high-dimensional causal inference. In NIPS, 2010.</p>
<p>[16] A. C. Lozano, G. Swirszcz, and N. Abe. Group orthogonal matching pursuit for variable selection and prediction. In NIPS, 2009.</p>
<p>[17] C. Michelli and M. Pontil. Learning the kernel function via regularization. JMLR, 6:1099–1125, 2005.</p>
<p>[18] H. Liu P. Ravikumar, J. Lafferty and L. Wasserman. Sparse additive models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB), 71 (5):1009–1030, 2009.</p>
<p>[19] P. Pavlidis, J. Cai, J. Weston, and W.S. Noble. Learning gene functional classiﬁcations from multiple data types. Journal of Computational Biology, 9:401–411, 2002.</p>
<p>[20] A. Rakotomamonjy, F.Bach, S. Cano, and Y. Grandvalet. SimpleMKL. Journal of Machine Learning Research, 9:2491–2521, 2008.</p>
<p>[21] G. Raskutti, M. Wainwrigt, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. In Technical Report 795, Statistics Department, UC Berkeley., 2010.</p>
<p>[22] Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2001.</p>
<p>[23] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>[24] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. J. Mach. a a o Learn. Res., 7, December 2006.</p>
<p>[25] Zhang T. Sparse recovery with orthogonal matching pursuit under rip. Computing Research Repository, 2010.</p>
<p>[26] R. Tomioka and T. Suzuki. Sparsity-accuracy tradeoff in mkl. In NIPS Workshop: Understanding Multiple Kernel Learning Methods. Technical report, arXiv:1001.2615v1, 2010.</p>
<p>[27] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory,, 50(10):2231–2242, 2004.</p>
<p>[28] P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning, 48:165–188, 2002.</p>
<p>[29] Z. Xu, R. Jin, H. Yang, I. King, and M.R. Lyu. Simple and efﬁcient multiple kernel learning by group lasso. In ICML, 2010.</p>
<p>[30] Ming Yuan, Ali Ekici, Zhaosong Lu, and Renato Monteiro. Dimension reduction and coefﬁcient estimation in multivariate linear regression. Journal Of The Royal Statistical Society Series B, 69(3):329–346, 2007.</p>
<p>[31] Tong Zhang. On the consistency of feature selection using greedy least squares regression. J. Mach. Learn. Res., 10, June 2009.</p>
<p>[32] H. Zhou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, 67(2):301–320, 2005.</p>
<p>[33] A. Zien and Cheng S. Ong. Multiclass multiple kernel learning. ICML, 2007.  9</p>
<br/>
<br/><br/><br/></body>
</html>
