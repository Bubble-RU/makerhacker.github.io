<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-161" href="../nips2011/nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">nips2011-161</a> <a title="nips-2011-161-reference" href="#">nips2011-161-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</h1>
<br/><p>Source: <a title="nips-2011-161-pdf" href="http://papers.nips.cc/paper/4434-linearized-alternating-direction-method-with-adaptive-penalty-for-low-rank-representation.pdf">pdf</a></p><p>Author: Zhouchen Lin, Risheng Liu, Zhixun Su</p><p>Abstract: Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efﬁciently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve lowrank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3 ) of the original ADM based method to O(rn2 ), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. In Michael Jordan, editor, Foundations and Trends in Machine Learning, 2010.</p>
<p>[2] J. Cai, E. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix completion. e preprint, 2008.</p>
<p>[3] E. Cand` s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 2011. e</p>
<p>[4] E. Cand` s and B. Recht. Exact matrix completion via convex optimization. Foundations of e Computational Mathematics, 2009.</p>
<p>[5] E. J. Cand` s and M. Wakin. An introduction to compressive sampling. IEEE Signal Processing e Magazine, 2008.</p>
<p>[6] P. Favaro, R. Vidal, and A. Ravichandran. A closed form solution to robust subspace estimation and clustering. In CVPR, 2011.</p>
<p>[7] B. He, M. Tao, and X. Yuan. Alternating direction method with Gaussian back substitution for separable convex programming. SIAM Journal on Optimization, accepted.</p>
<p>[8] B. He, H. Yang, and S. Wang. Alternating direction method with self-adaptive penalty parameters for monotone variational inequality. J. Optimization Theory and Applications, 106:337– 356, 2000.</p>
<p>[9] R. Larsen. Lanczos bidiagonalization with partial reorthogonalization. Department of Computer Science, Aarhus University, Technical report, DAIMI PB-357, 1998.</p>
<p>[10] Z. Lin. Some software packages for partial SVD computation. arXiv:1108.1548.</p>
<p>[11] Z. Lin, M. Chen, and Y. Ma. The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215, 2009, arXiv:1009.5055.</p>
<p>[12] G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation by low-rank representation. In ICML, 2010.</p>
<p>[13] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via efﬁcient l2,1 norm minimization. In UAI, 2009.</p>
<p>[14] Y. Ni, J. Sun, X. Yuan, S. Yan, and L. Cheong. Robust low-rank subspace segmentation with semideﬁnite guarantees. In ICDM Workshop, 2010.</p>
<p>[15] M. Tao and X.M. Yuan. Recovering low-rank and sparse components of matrices from incomplete and noisy observations. SIAM Journal on Optimization, 21(1):57–81, 2011.</p>
<p>[16] K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least sequares problems. Paciﬁc J. Optimization, 6:615–640, 2010.</p>
<p>[17] R. Tron and R. Vidal. A benchmark for the comparison of 3D montion segmentation algorithms. In CVPR, 2007.</p>
<p>[18] J. Wright, A. Ganesh, S. Rao, and Y. Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In NIPS, 2009.</p>
<p>[19] J. Wright, Y. Ma, J. Mairal, G. Sapirao, T. Huang, and S. Yan. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE, 2010.</p>
<p>[20] J. Yang and X. Yuan. Linearized augmented Lagrangian and alternating direction methods for nuclear norm minimization. submitted, 2011.</p>
<p>[21] J. Yang and Y. Zhang. Alternating direction algorithms for l1 problems in compressive sensing. SIAM J. Scientiﬁc Computing, 2010.</p>
<p>[22] W. Yin. Analysis and generalizations of the linearized Bregman method. SIAM Journal on Imaging Sciences, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
