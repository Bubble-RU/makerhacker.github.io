<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2011-Demixed Principal Component Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-68" href="../nips2011/nips-2011-Demixed_Principal_Component_Analysis.html">nips2011-68</a> <a title="nips-2011-68-reference" href="#">nips2011-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 nips-2011-Demixed Principal Component Analysis</h1>
<br/><p>Source: <a title="nips-2011-68-pdf" href="http://papers.nips.cc/paper/4215-demixed-principal-component-analysis.pdf">pdf</a></p><p>Author: Wieland Brendel, Ranulfo Romo, Christian K. Machens</p><p>Abstract: In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difﬁcult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, University of California, Berkeley, 2005.</p>
<p>[2] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 2006.</p>
<p>[3] C. D. Brody, A. Hernández, A. Zainos, and R. Romo. Timing and neural encoding of somatosensory parametric working memory in macaque prefrontal cortex. Cerebral Cortex, 13(11):1196–1207, 2003.</p>
<p>[4] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001.</p>
<p>[5] A. Krishnan, L. J. Williams, A. R. McIntosh, and H. Abdi. Partial least squares (PLS) methods for neuroimaging: a tutorial and review. NeuroImage, 56:455–475, 2011.</p>
<p>[6] P.-O. Lowdin. On the non-orthogonality problem connected with the use of atomic wave functions in the theory of molecules and crystals. The Journal of Chemical Physics, 18(3):365, 1950.</p>
<p>[7] C. K. Machens. Demixing population activity in higher cortical areas. Frontiers in computational neuroscience, 4(October):8, 2010.</p>
<p>[8] C. K. Machens, R. Romo, and C. D. Brody. Functional, but not anatomical, separation of “what” and “when” in prefrontal cortex. Journal of Neuroscience, 30(1):350–360, 2010.</p>
<p>[9] R. Romo, C. D. Brody, A. Hernandez, and L. Lemus. Neuronal correlates of parametric working memory in the prefrontal cortex. Nature, 399(6735):470–473, 1999.</p>
<p>[10] S. Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, 10:626–632, 1998.</p>
<p>[11] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society - Series B: Statistical Methodology, 61(3):611–622, 1999.</p>
<p>[12] S. Yu, K. Yu, V. Tresp, H. P. Kriegel, and M. Wu. Supervised probabilistic principal component analysis. Proceedings of 12th ACM SIGKDD International Conf. on KDD, 10, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
