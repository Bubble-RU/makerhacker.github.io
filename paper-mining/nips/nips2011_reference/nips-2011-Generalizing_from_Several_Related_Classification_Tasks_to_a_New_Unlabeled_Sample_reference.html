<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-106" href="../nips2011/nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">nips2011-106</a> <a title="nips-2011-106-reference" href="#">nips2011-106-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</h1>
<br/><p>Source: <a title="nips-2011-106-pdf" href="http://papers.nips.cc/paper/4312-generalizing-from-several-related-classification-tasks-to-a-new-unlabeled-sample.pdf">pdf</a></p><p>Author: Gilles Blanchard, Gyemin Lee, Clayton Scott</p><p>Abstract: We consider the problem of assigning class labels to an unlabeled test data set, given several labeled training data sets drawn from similar distributions. This problem arises in several applications where data distributions ﬂuctuate because of biological, technical, or other sources of variation. We develop a distributionfree, kernel-based approach to the problem. This approach involves identifying an appropriate reproducing kernel Hilbert space and optimizing a regularized empirical risk over the space. We present generalization error analysis, describe universal kernels, and establish universal consistency of the proposed methodology. Experimental results on ﬂow cytometry data are presented. 1</p><br/>
<h2>reference text</h2><p>[1] S. Thrun, “Is learning the n-th thing any easier than learning the ﬁrst?,” Advances in Neural Information Processing Systems, pp. 640–646, 1996.</p>
<p>[2] R. Caruana, “Multitask learning,” Machine Learning, vol. 28, pp. 41–75, 1997.</p>
<p>[3] T. Evgeniou and M. Pontil, “Learning multiple tasks with kernel methods,” J. Machine Learning Research, pp. 615–637, 2005.</p>
<p>[4] S. Bickel, M. Br¨ ckner, and T. Scheffer, “Discriminative learning under covariate shift,” J. u Machine Learning Research, pp. 2137–2155, 2009.</p>
<p>[5] J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence, Dataset Shift in Machine Learning, The MIT Press, 2009.</p>
<p>[6] R. K. Ando and T. Zhang, “A high-performance semi-supervised learning method for text chunking,” Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 05), pp. 1–9, 2005.</p>
<p>[7] A. Rettinger, M. Zinkevich, and M. Bowling, “Boosting expert ensembles for rapid concept recall,” Proceedings of the 21st National Conference on Artiﬁcial Intelligence (AAAI 06), vol. 1, pp. 464–469, 2006.</p>
<p>[8] A. Arnold, R. Nallapati, and W.W. Cohen, “A comparative study of methods for transductive transfer learning,” Seventh IEEE International Conference on Data Mining Workshops, pp. 77–82, 2007.</p>
<p>[9] O. Kallenberg, Foundations of Modern Probability, Springer, 2002.</p>
<p>[10] P. Bartlett, M. Jordan, and J. McAuliffe, “Convexity, classiﬁcation, and risk bounds,” J. Amer. Stat. Assoc., vol. 101, no. 473, pp. 138–156, 2006.</p>
<p>[11] G. Blanchard, G. Lee, and C. Scott, “Supplemental material,” NIPS 2011.</p>
<p>[12] I. Steinwart and A. Christmann, Support Vector Machines, Springer, 2008.</p>
<p>[13] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. Smola, “A kernel approach to como paring distributions,” in Proceedings of the 22nd AAAI Conference on Artiﬁcial Intelligence, R. Holte and A. Howe, Eds., 2007, pp. 1637–1641.</p>
<p>[14] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. Smola, “A kernel method o for the two-sample-problem,” in Advances in Neural Information Processing Systems 19, B. Sch¨ lkopf, J. Platt, and T. Hoffman, Eds., 2007, pp. 513–520. o</p>
<p>[15] B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨ lkopf, and G. Lanckriet, “Hilbert space o embeddings and metrics on probability measures,” Journal of Machine Learning Research, vol. 11, pp. 1517–1561, 2010.</p>
<p>[16] A. Christmann and I. Steinwart, “Universal kernels on non-standard input spaces,” in Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 406–414.</p>
<p>[17] C. McDiarmid, “On the method of bounded differences,” Surveys in Combinatorics, vol. 141, pp. 148–188, 1989.</p>
<p>[18] V. Koltchinskii, “Rademacher penalties and structural risk minimization,” IEEE Transactions on Information Theory, vol. 47, no. 5, pp. 1902 – 1914, 2001.</p>
<p>[19] P. Bartlett and S. Mendelson, “Rademacher and Gaussian complexities: Risk bounds and structural results,” Journal of Machine Learning Research, vol. 3, pp. 463–482, 2002.</p>
<p>[20] T. Joachims, “Making large-scale SVM learning practical,” in Advances in Kernel Methods Support Vector Learning, B. Sch¨ lkopf, C. Burges, and A. Smola, Eds., chapter 11, pp. 169– o 184. MIT Press, Cambridge, MA, 1999.</p>
<p>[21] J. Toedling, P. Rhein, R. Ratei, L. Karawajew, and R. Spang, “Automated in-silico detection of cell populations in ﬂow cytometry readouts and its application to leukemia disease monitoring,” BMC Bioinformatics, vol. 7, pp. 282, 2006.</p>
<p>[22] J. Wiens, Machine Learning for Patient-Adaptive Ectopic Beat Classication, Masters Thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
