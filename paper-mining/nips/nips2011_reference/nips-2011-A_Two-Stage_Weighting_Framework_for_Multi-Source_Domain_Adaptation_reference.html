<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-12" href="../nips2011/nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">nips2011-12</a> <a title="nips-2011-12-reference" href="#">nips2011-12-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</h1>
<br/><p>Source: <a title="nips-2011-12-pdf" href="http://papers.nips.cc/paper/4195-a-two-stage-weighting-framework-for-multi-source-domain-adaptation.pdf">pdf</a></p><p>Author: Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, Jieping Ye</p><p>Abstract: Discriminative learning when training and test data belong to different distributions is a challenging and complex task. Often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions. The difference in distributions may be both in marginal and conditional probabilities. Most of the existing domain adaptation work focuses on the marginal probability distribution difference between the domains, assuming that the conditional probabilities are similar. However in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences. In this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (ﬁrst stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted Rademacher complexity measure. Empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach. 1</p><br/>
<h2>reference text</h2><p>[1] S.J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2009.</p>
<p>[2] H. Daum III. Frustratingly easy domain adaptation. In ACL, 2007.</p>
<p>[3] L. Duan, I.W. Tsang, D. Xu, and S.J. Maybank. Domain transfer svm for video concept detection. In CVPR, 2009.  8</p>
<p>[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In ACL, 2007.</p>
<p>[5] S.J. Pan, J.T. Kwok, and Q. Yang. Transfer learning via dimensionality reduction. In AAAI 08.</p>
<p>[6] J. Huang, A.J. Smola, A. Gretton, K.M. Borgwardt, and B. Scholkopf. Correcting sample selection bias by unlabeled data. In NIPS, volume 19, page 601, 2007.</p>
<p>[7] H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. In JSPI, 2000.</p>
<p>[8] S. Bickel, M. Br¨ ckner, and T. Scheffer. Discriminative learning under covariate shift. In JMLR, 2009. u</p>
<p>[9] C. Cortes, Y. Mansour, and M. Mohri. Learning bounds for importance weighing. In NIPS, 2010.</p>
<p>[10] M. Sugiyama, S. Nakajima, H. Kashima, P.V. Buenau, and M. Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In NIPS, 2008.</p>
<p>[11] S.J. Pan, I.W. Tsang, J.T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. In IJCAI, 2009.</p>
<p>[12] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In NIPS, 2009.</p>
<p>[13] L. Duan, I.W. Tsang, D. Xu, and T. Chua. Domain adaptation from multiple sources via auxiliary classiﬁers. In ICML, pages 289–296, 2009.</p>
<p>[14] J. Gao, W. Fan, J. Jiang, and J. Han. Knowledge transfer via multiple model local structure mapping. In KDD, pages 283–291, 2008.</p>
<p>[15] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.P. Kriegel, B. Scholkopf, and A.J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. In Bioinformatics, volume 22, pages 49–57, 2006.</p>
<p>[16] R. Chattopadhyay, J. Ye, S. Panchanathan, W. Fan, and I. Davidson. Multi-source domain adaptation and its application to early detection of fatigue. In KDD, 2011.</p>
<p>[17] P.L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 3:463–482, 2002.</p>
<p>[18] V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902–1914, 2001.</p>
<p>[19] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J.W. Vaughan. A theory of learning from different domains. Journal of Mach Learn, 79:151–175, 2010.</p>
<p>[20] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. Computing Research Repository, abs/0902.3430, 2009.</p>
<p>[21] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. In JMLR, volume 2, page 93, 2002.</p>
<p>[22] I.H. Witten and E. Frank. In Data Mining: Practical Machine Learning Tools with Java Implementations, San Francisco, CA, 2000. Morgan Kaufmann.</p>
<p>[23] E. Eaton and M. desJardins. Set-based boosting for instance-level transfer. In IEEE International Conference on Data Mining Workshops, 2009.</p>
<p>[24] E. Zhong, W. Fan, J. Peng, K. Zhang, J. Ren, D. Turaga, and O. Verscheure. Cross domain distribution adaptation via kernel mapping. In KDD, Paris, France, 2009. ACM.</p>
<p>[25] P. Contessa, A. Adam, and C.J. De Luca. Motor unit control and force ﬂuctuation during fatigue. Journal of Applied Physiology, April 2009.</p>
<p>[26] B. Gerdle, B. Larsson, and S. Karlsson. Criterion validation of surface EMG variables as fatigue indicators using peak torque: a study of repetitive maximum isokinetic knee extensions. Journal of Electromyography and Kinesiology, 10(4):225–232, August 2000.</p>
<p>[27] E. leon, G. Clarke, V. Callaghan, and F. Sepulveda. A user independent real time emotion recognition system for software agents in domestic environment. In Engineering Application of Artiﬁcial Intelligence, April 2007.</p>
<p>[28] J. Kim and E. Andre. Emotion recognition based on physiological changes in music listening. In Pattern Analysis and Machine Intelligence, December 2008.</p>
<p>[29] C. McDiarmid. On the method of bounded differences., volume 5. Cambridge University Press, Cambridge, 1989.</p>
<p>[30] S. Kakade and A. Tewari. Lecture notes of CMSC 35900: Learning theory, Toyota Technological Institute at Chicago. Spring 2008.</p>
<p>[31] P. Massart. Some applications of concentration inequalities to statistics. Annales de la Faculte des sciences de ToulouseSciences de Toulouse, IX(2):245–303, 2000.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
