<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-188" href="../nips2011/nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">nips2011-188</a> <a title="nips-2011-188-reference" href="#">nips2011-188-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</h1>
<br/><p>Source: <a title="nips-2011-188-pdf" href="http://papers.nips.cc/paper/4407-non-conjugate-variational-message-passing-for-multinomial-and-binary-regression.pdf">pdf</a></p><p>Author: David A. Knowles, Tom Minka</p><p>Abstract: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability. 1</p><br/>
<h2>reference text</h2><p>[1] H. Attias. A variational Bayesian framework for graphical models. Advances in neural information processing systems, 12(1-2):209215, 2000.</p>
<p>[2] M. Beal and Z. Ghahramani. Variational Bayesian learning of directed graphical models with hidden variables. Bayesian Analysis, 1(4):793832, 2006.</p>
<p>[3] D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics, 2007.</p>
<p>[4] D. Bohning. Multinomial logistic regression algorithm. Annals of the Institute of Statistical Mathematics, 44:197–200, 1992. 10.1007/BF00048682.</p>
<p>[5] G. Bouchard. Efﬁcient bounds for the softmax and applications to approximate inference in hybrid models. In NIPS workshop on approximate inference in hybrid models, 2007.</p>
<p>[6] M. Girolami and S. Rogers. Variational bayesian multinomial probit regression with gaussian process priors. Neural Computation, 18(8):1790–1817, 2006.</p>
<p>[7] A. Honkela, T. Raiko, M. Kuusela, M. Tornio, and J. Karhunen. Approximate riemannian conjugate gradient learning for ﬁxed-form variational bayes. Journal of Machine Learning Research, 11:3235–3268, 2010.</p>
<p>[8] A. Honkela, M. Tornio, T. Raiko, and J. Karhunen. Natural conjugate gradient in variational inference. In M. Ishikawa, K. Doya, H. Miyamoto, and T. Yamakawa, editors, ICONIP (2), volume 4985 of Lecture Notes in Computer Science, pages 305–314. Springer, 2007.</p>
<p>[9] T. S. Jaakkola and M. I. Jordan. A variational approach to bayesian logistic regression models and their extensions. In International Conference on Artiﬁcial Intelligence and Statistics, 1996.</p>
<p>[10] M. E. Khan, B. M. Marlin, G. Bouchard, and K. P. Murphy. Variational bounds for mixed-data factor analysis. In Advances in Neural Information Processing (NIPS) 23, 2010.</p>
<p>[11] B. M. Marlin, M. E. Khan, and K. P. Murphy. Piecewise bounds for estimating bernoullilogistic latent gaussian models. In Proceedings of the 28th Annual International Conference on Machine Learning, 2011.</p>
<p>[12] T. P. Minka. Expectation propagation for approximate bayesian inference. In Uncertainty in Artiﬁcial Intelligence, volume 17, 2001.</p>
<p>[13] T. P. Minka, J. M. Winn, J. P. Guiver, and D. A. Knowles. Infer.NET 2.4, 2010. Microsoft Research Cambridge. http://research.microsoft.com/infernet.</p>
<p>[14] H. Nickisch and C. E. Rasmussen. Approximations for binary gaussian process classiﬁcation. Journal of Machine Learning Research, 9:2035–2078, Oct. 2008.</p>
<p>[15] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation, 21(3):786–792, 2009.</p>
<p>[16] Y. A. Qi and T. Jaakkola. Parameter expanded variational bayesian methods. In B. Sch¨ lkopf, o J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing (NIPS) 19, pages 1097–1104. MIT Press, 2006.</p>
<p>[17] T. Raiko, H. Valpola, M. Harva, and J. Karhunen. Building blocks for variational bayesian learning of latent variable models. Journal of Machine Learning Research, 8:155–201, 2007.</p>
<p>[18] L. K. Saul and M. I. Jordan. A mean ﬁeld learning algorithm for unsupervised neural networks. Learning in graphical models, 1999.</p>
<p>[19] M. P. Wand, J. T. Ormerod, S. A. Padoan, and R. Fruhwirth. Variational bayes for elaborate distributions. In Workshop on Recent Advances in Bayesian Computation, 2010.</p>
<p>[20] J. Winn and C. M. Bishop. Variational message passing. Journal of Machine Learning Research, 6(1):661, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
