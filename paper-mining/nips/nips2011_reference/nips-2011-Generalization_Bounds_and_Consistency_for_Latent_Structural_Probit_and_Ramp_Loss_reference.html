<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-103" href="../nips2011/nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">nips2011-103</a> <a title="nips-2011-103-reference" href="#">nips2011-103-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</h1>
<br/><p>Source: <a title="nips-2011-103-pdf" href="http://papers.nips.cc/paper/4268-generalization-bounds-and-consistency-for-latent-structural-probit-and-ramp-loss.pdf">pdf</a></p><p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><br/>
<h2>reference text</h2><p>[1] Olivier Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learning. Institute of Mathematical Statistics LECTURE NOTES MONOGRAPH SERIES, 2007.</p>
<p>[2] D. Chiang, K. Knight, and W. Wang. 11,001 new features for statistical machine translation. In Proc. NAACL, 2009, 2009.</p>
<p>[3] Chuong B. Do, Quoc Le, Choon Hui Teo, Olivier Chapelle, and Alex Smola. Tighter bounds for structured estimation. In nips, 2008.</p>
<p>[4] Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. Pac-bayesian learning of linear classiﬁers. In ICML, 2009.</p>
<p>[5] Ross Girshick, Pedro Felzenszwalb, and David McAllester. Object detection with grammar models. In NIPS, 2011.</p>
<p>[6] Joseph Keshet, David McAllester, and Tamir Hazan. Pac-bayesian approach for minimization of phoneme error rate. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2011.</p>
<p>[7] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eightneenth International Conference on Machine Learning, pages 282–289, 2001.</p>
<p>[8] P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. An end-to-end discriminative approach to machine translation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL), 2006.</p>
<p>[9] David McAllester. Generalization bounds and consistency for structured labeling. In G. Bakir nd T. Hofmann, B. Scholkopf, A. Smola, B. Taskar, and S. V. N. Vishwanathan, editors, Predicting Structured Data. MIT Press, 2007.</p>
<p>[10] David A. McAllester, Tamir Hazan, and Joseph Keshet. Direct loss minimization for structured prediction. In Advances in Neural Information Processing Systems 24, 2010.</p>
<p>[11] A. Quattoni, S. Wang, L.P. Morency, M Collins, and T Darrell. Hidden conditional random ﬁelds. PAMI, 29, 2007.</p>
<p>[12] R.Collobert, F.H.Sinz, J.Weston, and L.Bottou. Trading convexity for scalability. In ICML, 2006.</p>
<p>[13] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Advances in Neural Information Processing Systems 17, 2003.</p>
<p>[14] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004.</p>
<p>[15] Chun-Nam John Yu and T. Joachims. Learning structural svms with latent variables. In International Conference on Machine Learning (ICML), 2009.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
