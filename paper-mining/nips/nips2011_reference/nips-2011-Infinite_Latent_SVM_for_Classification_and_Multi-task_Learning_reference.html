<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-134" href="../nips2011/nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">nips2011-134</a> <a title="nips-2011-134-reference" href="#">nips2011-134-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</h1>
<br/><p>Source: <a title="nips-2011-134-pdf" href="http://papers.nips.cc/paper/4365-infinite-latent-svm-for-classification-and-multi-task-learning.pdf">pdf</a></p><p>Author: Jun Zhu, Ning Chen, Eric P. Xing</p><p>Abstract: Unlike existing nonparametric Bayesian models, which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, we study nonparametric Bayesian inference with regularization on the desired posterior distributions. While priors can indirectly affect posterior distributions through Bayes’ theorem, imposing posterior regularization is arguably more direct and in some cases can be much easier. We particularly focus on developing inﬁnite latent support vector machines (iLSVM) and multi-task inﬁnite latent support vector machines (MT-iLSVM), which explore the largemargin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classiﬁcation and multi-task learning, respectively. We present efﬁcient inference methods and report empirical studies on several benchmark datasets. Our results appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics.</p><br/>
<h2>reference text</h2><p>[1] R. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, (6):1817–1853, 2005.</p>
<p>[2] C.E. Antoniak. Mixture of Dirichlet process with applications to Bayesian nonparametric problems. Annals of Stats, (273):1152–1174, 1974.</p>
<p>[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. In NIPS, 2007.</p>
<p>[4] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. JMLR, (4):83–99, 2003.</p>
<p>[5] M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. The inﬁnite hidden Markov model. In NIPS, 2002.</p>
<p>[6] K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning with expectation constraints. In UAI, 2009.</p>
<p>[7] E. Bonilla, K.M.A. Chai, and C. Williams. Multi-task Gaussian process prediction. In NIPS, 2008.</p>
<p>[8] N. Chen, J. Zhu, and E.P. Xing. Predictive subspace learning for multiview data: a large margin approach. In NIPS, 2010.</p>
<p>[9] F. Doshi-Velez, K. Miller, J. Van Gael, and Y.W. Teh. Variational inference for the Indian buffet process. In AISTATS, 2009.</p>
<p>[10] D. Dunson and S. Peddada. Bayesian nonparametric inferences on stochastic ordering. ISDS Discussion Paper, 2, 2007.</p>
<p>[11] K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. JMLR, (11):2001–2094, 2010.</p>
<p>[12] T.L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS, 2006.</p>
<p>[13] D. Hoff. Bayesian methods for partial stochastic orderings. Biometrika, 90:303–317, 2003.</p>
<p>[14] S. Huh and S. Fienberg. Discriminative topic modeling based on manifold learning. In KDD, 2010.</p>
<p>[15] T. Jaakkola, M. Meila, and T. Jebara. Maximum entropy discrimination. In NIPS, 1999.</p>
<p>[16] T. Jebara. Multitask sparsity via maximum entropy discrimination. JMLR, (12):75–110, 2011.</p>
<p>[17] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999.</p>
<p>[18] M. E. Khan, B. Marlin, G. Bouchard, and K. Murphy. Variational bounds for mixed-data factor analysis. In NIPS, 2010.</p>
<p>[19] P. Liang, M. Jordan, and D. Klein. Learning from measurements in exponential families. In ICML, 2009.</p>
<p>[20] S.N. MacEachern. Dependent nonparametric process. In the Section on Bayesian Statistical Science of ASA, 1999.</p>
<p>[21] G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning with weakly labeled data. JMLR, (11):955–984, 2010.</p>
<p>[22] K. Miller, T. Grifﬁths, and M. Jordan. Nonparametric latent feature models for link prediction. In NIPS, 2009.</p>
<p>[23] P. Rai and H. Daume III. Inﬁnite predictor subspace models for multitask learning. In AISTATS, 2010.</p>
<p>[24] C.E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In NIPS, 2002.</p>
<p>[25] Y.W. Teh, D. Gorur, and Z. Ghahramani. Stick-breaking construction of the Indian buffet process. In AISTATS, 2007.</p>
<p>[26] Y.W. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet process. JASA, 101(476):1566–1581, 2006.</p>
<p>[27] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an application to information retrieval. In NIPS, 2004.</p>
<p>[28] Y. Xue, D. Dunson, and L. Carin. The matrix stick-breaking process for ﬂexible multi-task learning. In ICML, 2007.</p>
<p>[29] A. Zellner. Optimal information processing and Bayes’ theorem. American Statistician, 42:278–280, 1988.</p>
<p>[30] Y. Zhang and D.Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In UAI, 2010.</p>
<p>[31] J. Zhu, A. Ahmed, and E.P. Xing. MedLDA: Maximum margin supervised topic models for regression and classiﬁcation. In ICML, 2009.</p>
<p>[32] J. Zhu, N. Chen, and E.P. Xing. Inﬁnite SVM: a Dirichlet process mixture of large-margin kernel machines. In ICML, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
