<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>299 nips-2011-Variance Penalizing AdaBoost</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-299" href="../nips2011/nips-2011-Variance_Penalizing_AdaBoost.html">nips2011-299</a> <a title="nips-2011-299-reference" href="#">nips2011-299-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>299 nips-2011-Variance Penalizing AdaBoost</h1>
<br/><p>Source: <a title="nips-2011-299-pdf" href="http://papers.nips.cc/paper/4207-variance-penalizing-adaboost.pdf">pdf</a></p><p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efﬁciently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results conﬁrm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Signiﬁcant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART). 1</p><br/>
<h2>reference text</h2><p>[1] J-Y. Audibert, R. Munos, and C. Szepesv´ ri. Tuning bandit algorithms in stochastic environa ments. In ALT, 2007.</p>
<p>[2] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.</p>
<p>[3] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classiﬁcation and Regression Trees. Chapman and Hall, New York, 1984.</p>
<p>[4] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[5] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In COLT, 2009.</p>
<p>[6] V. Mnih, C. Szepesv´ ri, and J-Y. Audibert. Empirical Bernstein stopping. In COLT, 2008. a</p>
<p>[7] G. Raetsch, T. Onoda, and K.-R. Muller. Soft margins for AdaBoost. Machine Learning, 43:287–320, 2001.</p>
<p>[8] L. Reyzin and R. Schapire. How boosting the margin can also boost classiﬁer complexity. In ICML, 2006.</p>
<p>[9] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee. Boosting the margin: a new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651–1686, 1998.</p>
<p>[10] P. K. Shivaswamy and T. Jebara. Empirical Bernstein boosting. In AISTATS, 2010.</p>
<p>[11] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, NY, 1995.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
