<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2011-Bayesian Bias Mitigation for Crowdsourcing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-42" href="../nips2011/nips-2011-Bayesian_Bias_Mitigation_for_Crowdsourcing.html">nips2011-42</a> <a title="nips-2011-42-reference" href="#">nips2011-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 nips-2011-Bayesian Bias Mitigation for Crowdsourcing</h1>
<br/><p>Source: <a title="nips-2011-42-pdf" href="http://papers.nips.cc/paper/4311-bayesian-bias-mitigation-for-crowdsourcing.pdf">pdf</a></p><p>Author: Fabian L. Wauthier, Michael I. Jordan</p><p>Abstract: Biased labelers are a systemic problem in crowdsourcing, and a comprehensive toolbox for handling their responses is still being developed. A typical crowdsourcing application can be divided into three steps: data collection, data curation, and learning. At present these steps are often treated separately. We present Bayesian Bias Mitigation for Crowdsourcing (BBMC), a Bayesian model to unify all three. Most data curation methods account for the effects of labeler bias by modeling all labels as coming from a single latent truth. Our model captures the sources of bias by describing labelers as inﬂuenced by shared random effects. This approach can account for more complex bias patterns that arise in ambiguous or hard labeling tasks and allows us to merge data curation and learning into a single computation. Active learning integrates data collection with learning, but is commonly considered infeasible with Gibbs sampling inference. We propose a general approximation strategy for Markov chains to efﬁciently quantify the effect of a perturbation on the stationary distribution and specialize this approach to active learning. Experiments show BBMC to outperform many common heuristics. 1</p><br/>
<h2>reference text</h2><p>[1] K. Chaloner and I. Verdinelli. Bayesian Experimental Design: A Review. Statistical Science, 10(3):273–304, 1995.</p>
<p>[2] O. Dekel and O. Shamir. Good Learners for Evil Teachers. In L. Bottou and M. Littman, editors, Proceedings of the 26th International Conference on Machine Learning (ICML). Omnipress, 2009.</p>
<p>[3] O. Dekel and O. Shamir. Vox Populi: Collecting High-Quality Labels from a Crowd. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), Montreal, Quebec, Canada, 2009.</p>
<p>[4] P. Donmez, J. G. Carbonell, and J. Schneider. Efﬁciently Learning the Accuracy of Labeling Sources for Selective Sampling. In Proceedings of the 15th ACM SIGKDD, KDD, Paris, France, 2009.</p>
<p>[5] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite Latent Feature Models and the Indian Buffet Process. Technical report, Gatsby Computational Neuroscience Unit, 2005.</p>
<p>[6] P. G. Ipeirotis, F. Provost, and J. Wang. Quality Management on Amazon Mechanical Turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP, pages 64–67, Washington DC, 2010.</p>
<p>[7] D. V. Lindley. On a Measure of the Information Provided by an Experiment. The Annals of Mathematical Statistics, 27(4):986–1005, 1956.</p>
<p>[8] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from Crowds. Journal of Machine Learning Research, 11:1297–1322, April 2010.</p>
<p>[9] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get Another Label? Improving Data Quality and Data Mining using Multiple, Noisy Labelers. In Proceeding of the 14th ACM SIGKDD, KDD, Las Vegas, Nevada, 2008.</p>
<p>[10] P. Smyth, U. M. Fayyad, M. C. Burl, P. Perona, and P. Baldi. Inferring Ground Truth from Subjective Labelling of Venus Images. In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, Advances in Neural Information Processing Systems 7 (NIPS). MIT Press, 1994.</p>
<p>[11] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and Fast—But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of EMNLP. Association for Computational Linguistics, 2008.</p>
<p>[12] A. Sorokin and D. Forsyth. Utility Data Annotation with Amazon Mechanical Turk. In CVPR Workshop on Internet Vision, Anchorage, Alaska, 2008.</p>
<p>[13] X. Su and T. M. Khoshgoftaar. A Survey of Collaborative Filtering Techniques. Advances in Artiﬁcial Intelligence, 2009:4:2–4:2, January 2009.</p>
<p>[14] P. Wais, S. Lingamnei, D. Cook, J. Fennell, B. Goldenberg, D. Lubarov, D. Marin, and H. Simons. Towards Building a High-Quality Workforce with Mechanical Turk. In NIPS Workshop on Computational Social Science and the Wisdom of Crowds, Whistler, BC, Canada, 2010.</p>
<p>[15] P. Welinder, S. Branson, S. Belongie, and P. Perona. The Multidimensional Wisdom of Crowds. In J. Lafferty, C. K. I. Williams, R. Zemel, J. Shawe-Taylor, and A. Culotta, editors, Advances in Neural Information Processing Systems 23 (NIPS). MIT Press, 2010.</p>
<p>[16] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22 (NIPS). MIT Press, 2009.</p>
<p>[17] Y. Yan, R. Rosales, G. Fung, and J. G. Dy. Active Learning from Crowds. In L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, Washington, 2011.</p>
<p>[18] Y. Yan, R. Rosales, G. Fung, M. Schmidt, G. Hermosillo, L. Bogoni, L. Moy, and J. G. Dy. Modeling Annotator Expertise: Learning When Everybody Knows a Bit of Something. In Proceedings of AISTATS, volume 9, Chia Laguna, Sardinia, Italy, 2010.</p>
<p>[19] K. Yu, A. Schwaighofer, V. Tresp, X. Xu, and H. Kriegel. Probabilistic Memory-based Collaborative Filtering. IEEE Transactions On Knowledge and Data Engineering, 16(1):56–69, January 2004.  9</p>
<br/>
<br/><br/><br/></body>
</html>
