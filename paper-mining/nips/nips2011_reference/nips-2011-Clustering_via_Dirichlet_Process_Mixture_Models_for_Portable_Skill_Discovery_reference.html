<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-52" href="../nips2011/nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">nips2011-52</a> <a title="nips-2011-52-reference" href="#">nips2011-52-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</h1>
<br/><p>Source: <a title="nips-2011-52-pdf" href="http://papers.nips.cc/paper/4238-clustering-via-dirichlet-process-mixture-models-for-portable-skill-discovery.pdf">pdf</a></p><p>Author: Scott Niekum, Andrew G. Barto</p><p>Abstract: Skill discovery algorithms in reinforcement learning typically identify single states or regions in state space that correspond to task-speciﬁc subgoals. However, such methods do not directly address the question of how many distinct skills are appropriate for solving the tasks that the agent faces. This can be highly inefﬁcient when many identiﬁed subgoals correspond to the same underlying skill, but are all used individually as skill goals. Furthermore, skills created in this manner are often only transferable to tasks that share identical state spaces, since corresponding subgoals across tasks are not merged into a single skill goal. We show that these problems can be overcome by clustering subgoal data deﬁned in an agent-space and using the resulting clusters as templates for skill termination conditions. Clustering via a Dirichlet process mixture model is used to discover a minimal, sufﬁcient collection of portable skills. 1</p><br/>
<h2>reference text</h2><p>[1] Bram Bakker and J¨ rgen Schmidhuber. Hierarchical reinforcement learning based on subgoal discovery u and subpolicy specialization. In Proc. of the 8th Conference on Intelligent Autonomous Systems, pages 438–445, 2004.</p>
<p>[2] A. G. Barto, S. Singh, and N. Chentanez. Intrinsically motivated learning of hierarchical collections of skills. In Proc. of the International Conference on Developmental Learning, pages 112–119, 2004.</p>
<p>[3] Bruce L. Digney. Learning hierarchical control structures for multiple tasks and changing environments. In Proc. of the 5th Conference on the Simulation of Adaptive Behavior. MIT Press, 1998.</p>
<p>[4] W. R. Gilks and P. Wild. Adaptive Rejection Sampling for Gibbs Sampling. Journal of the Royal Statistical Society, Series C, 41(2):337–348, 1992.</p>
<p>[5] M. Halkidi and M. Vazirgiannis. Npclu: An approach for clustering spatially extended objects. Intell. Data Anal., 12:587–606, December 2008.</p>
<p>[6] Engin Ipek, Onur Mutlu, Jose F. Martinez, and Rich Caruana. Self-optimizing memory controllers: A reinforcement learning approach. Computer Architecture, International Symposium on, 0:39–50, 2008.</p>
<p>[7] Anders Jonsson and Andrew Barto. Causal graph based decomposition of factored mdps. J. Mach. Learn. Res., 7:2259–2301, December 2006.</p>
<p>[8] G.D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforcement learning using the fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial Intelligence, 2011.</p>
<p>[9] George Konidaris and Andrew G. Barto. Building portable options: Skill transfer in reinforcement learning. In Proc. of the 20th International Joint Conference on Artiﬁcial Intelligence, pages 895–900, 2007.</p>
<p>[10] George Konidaris and Andrew G. Barto. Skill discovery in continuous reinforcement learning domains using skill chaining. In Advances in Neural Information Processing Systems 22, pages 1015–1023, 2009.</p>
<p>[11] Amy McGovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. In ICML, pages 361–368, 2001.</p>
<p>[12] Brett Moore, Periklis Panousis, Vivek Kulkarni, Larry Pyeatt, and Anthony Doufas. Reinforcement learning for closed-loop propofol anesthesia: A human volunteer study. In Innovative Applications of Artiﬁcial Intelligence, 2010.</p>
<p>[13] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of computational and graphical statistics, 9(2):249–265, 2000.</p>
<p>[14] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems, pages 849–856. MIT Press, 2001.</p>
<p>[15] Marc Pickett and Andrew G. Barto. Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning. In ICML, pages 506–513, 2002.</p>
<p>[16] Carl Edward Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information Processing Systems 12, pages 554–560. MIT Press, 2000. ¨ u ¸ ¸</p>
<p>[17] Ozg¨ r Simsek and Andrew G. Barto. Using relative novelty to identify useful temporal abstractions in reinforcement learning. In Proc. of the Twenty-First International Conference on Machine Learning, pages 751–758, 2004. ¨ u ¸ ¸</p>
<p>[18] Ozg¨ r Simsek and Andrew G. Barto. Skill characterization based on betweenness. In NIPS, pages 1497– 1504, 2008.</p>
<p>[19] Richard Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112:181–211, 1999.</p>
<p>[20] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.</p>
<p>[21] Sebastian Thrun and Anton Schwartz. Finding structure in reinforcement learning. In Advances in Neural Information Processing Systems 7, pages 385–392. MIT Press, 1995.</p>
<p>[22] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: A hierarchical bayesian approach. In In: ICML 07: Proceedings of the 24th international conference on Machine learning, page 1015. ACM Press, 2007.  9</p>
<br/>
<br/><br/><br/></body>
</html>
