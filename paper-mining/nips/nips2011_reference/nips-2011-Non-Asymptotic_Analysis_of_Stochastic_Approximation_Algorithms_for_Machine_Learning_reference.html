<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2011-Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-187" href="../nips2011/nips-2011-Non-Asymptotic_Analysis_of_Stochastic_Approximation_Algorithms_for_Machine_Learning.html">nips2011-187</a> <a title="nips-2011-187-reference" href="#">nips2011-187-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>187 nips-2011-Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning</h1>
<br/><p>Source: <a title="nips-2011-187-pdf" href="http://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">pdf</a></p><p>Author: Eric Moulines, Francis R. Bach</p><p>Abstract: We consider the minimization of a convex objective function deﬁned on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a. Robbins-Monro algorithm) as well as a simple modiﬁcation where iterates are averaged (a.k.a. Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.</p><br/>
<h2>reference text</h2><p>[1] M. N. Broadie, D. M. Cicek, and A. Zeevi. General bounds and ﬁnite-time improvement for stochastic approximation algorithms. Technical report, Columbia University, 2009.</p>
<p>[2] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications. Springer-Verlag, second edition, 2003. `</p>
<p>[3] O. Yu. Kul′ chitski˘ and A. E. Mozgovo˘. An estimate for the rate of convergence of recurrent ı ı robust identiﬁcation algorithms. Kibernet. i Vychisl. Tekhn., 89:36–39, 1991.</p>
<p>[4] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.</p>
<p>[5] D. Ruppert. Efﬁcient estimations from a slowly convergent Robbins-Monro process. Technical Report 781, Cornell University Operations Research and Industrial Engineering, 1988.</p>
<p>[6] V. Fabian. On asymptotic normality in stochastic approximation. The Annals of Mathematical Statistics, 39(4):1327–1332, 1968.</p>
<p>[7] Y. Nesterov and J. P. Vial. Conﬁdence level solutions for stochastic programming. Automatica, 44(6):1559–1568, 2008.</p>
<p>[8] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.</p>
<p>[9] L. Bottou and Y. Le Cun. On-line learning for very large data sets. Applied Stochastic Models in Business and Industry, 21(2):137–151, 2005.</p>
<p>[10] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems (NIPS), 20, 2008.</p>
<p>[11] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In Proc. ICML, 2008.</p>
<p>[12] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. ICML, 2007.</p>
<p>[13] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In Conference on Learning Theory (COLT), 2009.</p>
<p>[14] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 9:2543–2596, 2010.</p>
<p>[15] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899–2934, 2009.</p>
<p>[16] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and Examples. Springer, 2006.</p>
<p>[17] R. Durrett. Probability: theory and examples. Duxbury Press, third edition, 2004.</p>
<p>[18] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2001. o</p>
<p>[19] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>[20] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic Publishers, 2004.</p>
<p>[21] K. Sridharan, N. Srebro, and S. Shalev-Shwartz. Fast rates for regularized objectives. Advances in Neural Information Processing Systems, 22, 2008.</p>
<p>[22] N. N. Vakhania, V. I. Tarieladze, and S. A. Chobanyan. Probability distributions on Banach spaces. Reidel, 1987.</p>
<p>[23] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Technical Report 00608041, HAL, 2011.</p>
<p>[24] A.S. Nemirovsky and D.B. Yudin. Problem complexity and method efﬁciency in optimization. Wiley & Sons, 1983.</p>
<p>[25] A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization, 2010. Tech. report, Arxiv 1009.0571.</p>
<p>[26] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proc. COLT, 2001.</p>
<p>[27] G. Casella and R. L. Berger. Statistical Inference. Duxbury Press, 2001.</p>
<p>[28] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2002. 9</p>
<br/>
<br/><br/><br/></body>
</html>
