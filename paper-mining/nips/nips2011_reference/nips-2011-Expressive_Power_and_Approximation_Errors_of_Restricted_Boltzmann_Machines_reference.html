<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-92" href="../nips2011/nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">nips2011-92</a> <a title="nips-2011-92-reference" href="#">nips2011-92-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</h1>
<br/><p>Source: <a title="nips-2011-92-pdf" href="http://papers.nips.cc/paper/4380-expressive-power-and-approximation-errors-of-restricted-boltzmann-machines.pdf">pdf</a></p><p>Author: Guido F. Montufar, Johannes Rauh, Nihat Ay</p><p>Abstract: We present explicit classes of probability distributions that can be learned by Restricted Boltzmann Machines (RBMs) depending on the number of units that they contain, and which are representative for the expressive power of the model. We use this to show that the maximal Kullback-Leibler divergence to the RBM model with n visible and m hidden units is bounded from above by (n−1)−log(m+1). In this way we can specify the number of hidden units that guarantees a sufﬁciently rich model containing different classes of distributions and respecting a given error tolerance. 1</p><br/>
<h2>reference text</h2><p>[1] N. Ay and A. Knauf. Maximizing multi-information. Kybernetika, 42:517–538, 2006.</p>
<p>[2] N. Ay, G. Mont´ far, and J. Rauh. Selection criteria for neuromanifolds of stochastic dynamics. u International Conference on Cognitive Neurodynamics, 2011.</p>
<p>[3] N. Ay and T. Wennekers. Dynamical properties of strongly interacting Markov chains. Neural Networks, 16:1483–1497, 2003.</p>
<p>[4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. NIPS, 2007.</p>
<p>[5] L. Brown. Fundamentals of Statistical Exponential Families: With Applications in Statistical Decision Theory. Inst. Math. Statist., Hayworth, CA, USA, 1986.</p>
<p>[6] M. A. Carreira-Perpi˜ an and G. E. Hinton. On contrastive divergence learning. In Proceedings n of the 10-th International Workshop on Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[7] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, 2006.</p>
<p>[8] M. A. Cueto, J. Morton, and B. Sturmfels. Geometry of the Restricted Boltzmann Machine. In M. A. G. Viana and H. P. Wynn, editors, Algebraic methods in statistics and probability II, AMS Special Session. AMS, 2010.</p>
<p>[9] P. Diaconis and D. Freedman. Finite exchangeable sequences. Ann. Probab., 8:745–764, 1980.</p>
<p>[10] Y. Freund and D. Haussler. Unsupervised learning of distributions on binary vectors using 2-layer networks. NIPS, pages 912–919, 1992.</p>
<p>[11] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Comput., 14:1771–1800, 2002.</p>
<p>[12] G. E. Hinton. A practical guide to training Restricted Boltzmann Machines, version 1. Technical report, UTML2010-003, University of Toronto, 2010.</p>
<p>[13] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for Deep Belief Nets. Neural Comput., 18:1527–1554, 2006.</p>
<p>[14] S. Kullback and R. Leibler. On information and sufﬁciency. Ann. Math. Stat., 22:79–86, 1951.</p>
<p>[15] N. Le Roux and Y. Bengio. Representational power of Restricted Boltzmann Machines and Deep Belief Networks. Neural Comput., 20(6):1631–1649, 2008.</p>
<p>[16] N. Le Roux and Y. Bengio. Deep Belief Networks are compact universal approximators. Neural Comput., 22:2192–2207, 2010.</p>
<p>[17] B. Lindsay. Mixture models: theory, geometry, and applications. Inst. Math. Statist., 1995.</p>
<p>[18] P. M. Long and R. A. Servedio. Restricted Boltzmann Machines are hard to approximately evaluate or simulate. In Proceedings of the 27-th ICML, pages 703–710, 2010.</p>
<p>[19] G. Mont´ far. Mixture decompositions using a decomposition of the sample space. ArXiv u 1008.0204, 2010.</p>
<p>[20] G. Mont´ far. Mixture models and representational power of RBMs, DBNs and DBMs. NIPS u Deep Learning and Unsupervised Feature Learning Workshop, 2010.</p>
<p>[21] G. Mont´ far and N. Ay. Reﬁnements of universal approximation results for Deep Belief Netu works and Restricted Boltzmann Machines. Neural Comput., 23(5):1306–1319, 2011.</p>
<p>[22] J. Rauh. Finding the maximizers of the information divergence from an exponential family. PhD thesis, Universit¨ t Leipzig, 2011. a</p>
<p>[23] J. Rauh, T. Kahle, and N. Ay. Support sets of exponential families and oriented matroids. Int. J. Approx. Reason., 52(5):613–626, 2011.</p>
<p>[24] P. Smolensky. Information processing in dynamical systems: foundations of harmony theory. In Symposium on Parallel and Distributed Processing, 1986.</p>
<p>[25] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). MIT Press, March 1998.</p>
<p>[26] K. G. Zahedi, N. Ay, and R. Der. Higher coordination with less control – a result of infromation maximization in the sensori-motor loop. Adaptive Behavior, 18(3-4):338–355, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
