<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-273" href="../nips2011/nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">nips2011-273</a> <a title="nips-2011-273-reference" href="#">nips2011-273-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</h1>
<br/><p>Source: <a title="nips-2011-273-pdf" href="http://papers.nips.cc/paper/4426-structural-equations-and-divisive-normalization-for-energy-dependent-component-analysis.pdf">pdf</a></p><p>Author: Jun-ichiro Hirayama, Aapo Hyvärinen</p><p>Abstract: Components estimated by independent component analysis and related methods are typically not independent in real data. A very common form of nonlinear dependency between the components is correlations in their variances or energies. Here, we propose a principled probabilistic model to model the energycorrelations between the latent variables. Our two-stage model includes a linear mixing of latent signals into the observed ones like in ICA. The main new feature is a model of the energy-correlations based on the structural equation model (SEM), in particular, a Linear Non-Gaussian SEM. The SEM is closely related to divisive normalization which effectively reduces energy correlation. Our new twostage model enables estimation of both the linear mixing and the interactions related to energy-correlations, without resorting to approximations of the likelihood function or other non-principled approaches. We demonstrate the applicability of our method with synthetic dataset, natural images and brain signals. 1</p><br/>
<h2>reference text</h2><p>[1] S. Amari, A. Cichoki, and H. H. Yang. A new learning algorithm for blind signal separation. In Advances in Neural Information Processing Systems, volume 8, 1996.</p>
<p>[2] A. J. Bell and T. J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision Res., 37:3327–3338, 1997.</p>
<p>[3] K. A. Bollen. Structural Equations with Latent Variables. Wiley, New York, 1989.</p>
<p>[4] M. Carandini, D. J. Heeger, and J. A. Movshon. Linearity and normalization in simple cells of the macaque primary visual cortex. Journal of Neuroscience, 17:8621–8644, 1997.</p>
<p>[5] A. Cichocki and P. Georgiev. Blind source separation algorithms with matrix constraints. IEICE Trans. Fundamentals, E86-A(3):522–531, 2003.  8</p>
<p>[6] P. Garrigues and B. A. Olshausen. Learning horizontal connections in a sparse coding model of natural images. In Advances in Neural Information Processing Systems, volume 20, pages 505–512, 2008.</p>
<p>[7] D. J. Heeger. Normalization of cell responses in cat striate cortex. Visual Neuroscience, 9:181–197, 1992.</p>
<p>[8] P. O. Hoyer, D. Janzing, J. Mooij, J. Peters, and B. Sch¨ lkopf. Nonlinear causal discovery with additive o noise models. In Advances in Neural Information Processing Systems, volume 21, pages 689–696, 2009.</p>
<p>[9] A. Hyv¨ rinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Transa actions on Neural Networks, 10(3):626–634, 1999.</p>
<p>[10] A. Hyv¨ rinen and P.O. Hoyer. Emergence of phase and shift invariant features by decomposition of a natural images into independent feature subspaces. Neural Comput., 12(7):1705–1720, 2000.</p>
<p>[11] A. Hyv¨ rinen, P.O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Comput., a 13(7):1527–1558, 2001.</p>
<p>[12] A Hyv¨ rinen, J. Hurri, and P. O. Hoyer. Natural Image Statistics – A probabilistic approach to early a computational vision. Springer-Verlag, 2009.</p>
<p>[13] A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. John Wiley & Sons, 2001. a</p>
<p>[14] Y. Karklin and M. S. Lewicki. A hierarchical Bayesian model for learning nonlinear statistical regularities in nonstationary natural signals. Neural Comput., 17:397–423, 2005.</p>
<p>[15] Y. Karklin and M. S. Lewicki. Emergence of complex cell properties by learning to generalize in natural scenes. Nature, 457:83–86, January 2009.</p>
<p>[16] M. Kawanabe and K.-R. M¨ ller. Estimating functions for blind separation when sources have variance u dependencies. Journal of Machine Learning Research, 6:453–482, 2005.</p>
<p>[17] U. K¨ ster and A. Hyv¨ rinen. A two-layer model of natural stimuli estimated with score matching. Neural o a Comput., 22:2308–2333, 2010.</p>
<p>[18] G. Lacerda, P. Spirtes, J. Ramsey, and P. Hoyer. Discovering cyclic causal models by independent components analysis. In Proceedings of the Twenty-Fourth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI’08), pages 366–374, 2008.</p>
<p>[19] S. Lyu. Divisive normalization: Justiﬁcation and effectiveness as efﬁcient coding transform. In Advances in Neural Information Processing Systems 23, pages 1522–1530, 2010.</p>
<p>[20] J. Malo, I. Epifanio, R. Navarro, and E. P. Simoncelli. Nonlinear image representation for efﬁcient perceptual coding. IEEE Trans Image Process, 15(1):68–80, 2006.</p>
<p>[21] J. Malo and V. Laparra. Psychophysically tuned divisive normalization approximately factorizes the PDF of natural images. Neural Comput., 22(12):3179–3206, 2010.</p>
<p>[22] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996.</p>
<p>[23] S. Osindero, M. Welling, and G. E. Hinton. Topographic product models applied to natural scene statistics. Neural Comput., 18:381–414, 2006.</p>
<p>[24] J. Pearl. On the statistical interpretation of structural equations. Technical Report R-200, UCLA Cognitive Systems Laboratory, 1993.</p>
<p>[25] P. Ramkumar, L. Parkkonen, R. Hari, and A. Hyv¨ rinen. Characterization of neuromagnetic brain rhythms a over time scales of minutes using spatial independent component analysis. Human Brain Mapping, 2011. In press.</p>
<p>[26] O. Schwartz and E. P. Simoncelli. Natural signal statistics and sensory gain control. Nature Neuroscience, 4(8), 2001.</p>
<p>[27] S. Shimizu, P.O. Hoyer, A. Hyv¨ rinen, and A. Kerminen. A linear non-Gaussian acyclic model for causal a discovery. Journal of Machine Learning Research, 7:2003–2030, 2006.</p>
<p>[28] E. P. Simoncelli and B. A. Olshausen. Natural image statistics and neural representation. Annu. Rev. Neurosci., 24:1193–1216, 2001.</p>
<p>[29] R. Valerio and R. Navarro. Optimal coding through divisive normalization models of V1 neurons. Network: Computation in Neural Systems, 14:579–593, 2003.</p>
<p>[30] H. Valpola, M. Harva, and J. Karhunen. Hierarchical models of variance sources. Signal Processing, 84(2):267–282, 2004.</p>
<p>[31] J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proc. R. Soc. Lond. B, 265(359–366), 1998.</p>
<p>[32] M. J. Wainwright and E. P. Simoncelli. Scale mixtures of gaussians and the statistics of natural images. In Advances in Neural Information Processing Systems, volume 12, pages 855–861, 2000.</p>
<p>[33] K. Zhang and A. Hyv¨ rinen. Source separation and higher-order causal analysis of MEG and EEG. In a Proceedings of the Twenty-Sixth Conference (UAI 2010), pages 709–716, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
