<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-203" href="../nips2011/nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">nips2011-203</a> <a title="nips-2011-203-reference" href="#">nips2011-203-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</h1>
<br/><p>Source: <a title="nips-2011-203-pdf" href="http://papers.nips.cc/paper/4270-on-the-accuracy-of-l1-filtering-of-signals-with-block-sparse-structure.pdf">pdf</a></p><p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:1179–1225, 2008.</p>
<p>[2] Z. Ben-Haim and Y. C. Eldar. Near-oracle performance of greedy block-sparse estimation techniques from noisy measurements. Technical report, 2010. http://arxiv.org/abs/ 1009.0906.</p>
<p>[3] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Stat., 37(4):1705–1732, 2008.</p>
<p>[4] P. B¨ hlmann and S. van de Geer. On the conditions used to prove oracle results for the Lasso. u Electron. J. Statist., 3:1360–1392, 2009.</p>
<p>[5] E. J. Cand` s and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory, 51:4203– e 4215, 2006.</p>
<p>[6] L. Cavalier, G. K. Golubev, D. Picard, and A. B. Tsybakov. Oracle inequalities for inverse problems. Ann. Statist., 30(3):843–874, 2002.</p>
<p>[7] A. Chambolle. An algorithm for total variation minimization and applications. Journal of Mathematical Imaging and Vision, 20(1-2):89–97, 2004.</p>
<p>[8] C. Chesneau and M. Hebiri. Some theoretical results on the grouped variables Lasso. Mathematical Methods of Statistics, 27(4):317–326, 2008.</p>
<p>[9] Y. C. Eldar, P. Kuppinger, and H. B¨ lcskei. Block-sparse signals: Uncertainty relations and o efﬁcient recovery. IEEE Trans. on Signal Processing, 58(6):3042–3054, 2010.</p>
<p>[10] Y. C. Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces. IEEE Trans. Inf. Theory, 55(11):5302–5316, 2009.</p>
<p>[11] J. Huang and T. Zhang. The beneﬁt of group sparsity. Annals of Stat., 38(4):1978–2004, 2010.</p>
<p>[12] G. M. James, P. Radchenko, and J. Lv. Dasso: connections between the Dantzig selector and Lasso. J. Roy. Statist. Soc. Ser. B, 71(1):127–142, 2009.</p>
<p>[13] A. B. Juditsky, F. Kılınc-Karzan, and A. S. Nemirovski. Veriﬁable conditions of 1 recovery ¸ for sparse signals with sign restrictions. Math. Progr., 127(1):89–122, 2010. http://www. optimization-online.org/DB_HTML/2009/03/2272.html.</p>
<p>[14] A. B. Juditsky and A. S. Nemirovski. Accuracy guarantees for 1 -recovery. Technical report, 2010. http://www.optimization-online.org/DB_HTML/2010/10/ 2778.html.</p>
<p>[15] A. B. Juditsky and A. S. Nemirovski. On veriﬁable sufﬁcient conditions for sparse signal recovery via 1 minimization. Math. Progr., 127(1):57–88, 2010. Special issue on machine learning.</p>
<p>[16] H. Liu and J. Zhang. Estimation consistency of the group Lasso and its applications. Journal of Machine Learning Research - Proceedings Track, 5:376–383, 2009.</p>
<p>[17] H. Liu, J. Zhang, X. Jiang, and J. Liu. The group Dantzig selector. Journal of Machine Learning Research - Proceedings Track, 9:461–468, 2010.</p>
<p>[18] K. Lounici, M. Pontil, A. Tsybakov, and S. van de Geer. Oracle inequalities and optimal inference under group sparsity. Technical report, 2010. http://arxiv.org/pdf/1007. 1771.</p>
<p>[19] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group Lasso estimator for linear models. Electron. J. Statist., 2:605–633, 2008.</p>
<p>[20] G. Obozinski, M. J. Wainwright, and M. I. Jordan. Support union recovery in high-dimensional multivariate regression. Annals of Stat., 39(1):1–47, 2011.</p>
<p>[21] L. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D, 60(1-4):259–268, 1992.</p>
<p>[22] M. Stojnic, F. Parvaresh, and B. Hassibi. On the reconstruction of block-sparse signals with an optimal number of measurements. IEEE Trans. on Signal Processing, 57(8):3075–3085, 2009.</p>
<p>[23] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat. Soc. Ser. B, 68(1):49–67, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
