<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-117" href="../nips2011/nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">nips2011-117</a> <a title="nips-2011-117-reference" href="#">nips2011-117-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</h1>
<br/><p>Source: <a title="nips-2011-117-pdf" href="http://papers.nips.cc/paper/4370-high-dimensional-graphical-model-selection-tractable-graph-families-and-necessary-conditions.pdf">pdf</a></p><p>Author: Animashree Anandkumar, Vincent Tan, Alan S. Willsky</p><p>Abstract: We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efﬁcient threshold-based algorithm for structure estimation based on conditional mutual information thresholding. This simple local algorithm requires only loworder statistics of the data and decides whether two nodes are neighbors in the unknown graph. We identify graph families for which the proposed algorithm has low sample and computational complexities. Under some transparent assumptions, we establish that the proposed algorithm is −4 structurally consistent (or sparsistent) when the number of samples scales as n = Ω(Jmin log p), where p is the number of nodes and Jmin is the minimum edge potential. We also develop novel non-asymptotic techniques for obtaining necessary conditions for graphical model selection. Keywords: Graphical model selection, high-dimensional learning, local-separation property, necessary conditions, typical sets, Fano’s inequality.</p><br/>
<h2>reference text</h2><p>[1] S. Lauritzen, Graphical models: Clarendon Press.  Clarendon Press, 1996.</p>
<p>[2] D. Karger and N. Srebro, “Learning Markov Networks: Maximum Bounded Tree-width Graphs,” in Proc. of ACM-SIAM symposium on Discrete algorithms, 2001, pp. 392–401.</p>
<p>[3] C. Chow and C. Liu, “Approximating Discrete Probability Distributions with Dependence Trees,” IEEE Tran. on Information Theory, vol. 14, no. 3, pp. 462–467, 1968.</p>
<p>[4] A. d’Aspremont, O. Banerjee, and L. El Ghaoui, “First-order methods for sparse covariance selection,” SIAM. J. Matrix Anal. & Appl., vol. 30, no. 56, 2008.</p>
<p>[5] P. Ravikumar, M. Wainwright, G. Raskutti, and B. Yu, “High-dimensional covariance estimation by minimizing 1 -penalized log-determinant divergence,” Arxiv preprint arXiv:0811.3628, 2008.</p>
<p>[6] P. Ravikumar, M. Wainwright, and J. Lafferty, “High-dimensional Ising Model Selection Using l1-Regularized Logistic Regression,” Annals of Statistics, 2008. 8</p>
<p>[7] B. Bollob´ s, Random Graphs. Academic Press, 1985. a</p>
<p>[8] F. Chung and L. Lu, Complex graphs and network. Amer. Mathematical Society, 2006.</p>
<p>[9] D. Watts and S. Strogatz, “Collective dynamics of small-worldnetworks,” Nature, vol. 393, no. 6684, pp. 440– 442, 1998.</p>
<p>[10] M. Newman, D. Watts, and S. Strogatz, “Random graph models of social networks,” Proc. of the National Academy of Sciences of the United States of America, vol. 99, no. Suppl 1, 2002.</p>
<p>[11] R. Albert and A. Barab´ si, “Statistical mechanics of complex networks,” Reviews of modern physics, vol. 74, a no. 1, p. 47, 2002.</p>
<p>[12] H. Georgii, Gibbs Measures and Phase Transitions. Walter de Gruyter, 1988.</p>
<p>[13] J. Bento and A. Montanari, “Which Graphical Models are Difﬁcult to Learn?” in Proc. of Neural Information Processing Systems (NIPS), Vancouver, Canada, Dec. 2009.</p>
<p>[14] D. Malioutov, J. Johnson, and A. Willsky, “Walk-Sums and Belief Propagation in Gaussian Graphical Models,” J. of Machine Learning Research, vol. 7, pp. 2031–2064, 2006.</p>
<p>[15] G. Bresler, E. Mossel, and A. Sly, “Reconstruction of Markov Random Fields from Samples: Some Observations and Algorithms,” in Intl. workshop APPROX Approximation, Randomization and Combinatorial Optimization. Springer, 2008, pp. 343–356.</p>
<p>[16] P. Netrapalli, S. Banerjee, S. Sanghavi, and S. Shakkottai, “Greedy Learning of Markov Network Structure ,” in Proc. of Allerton Conf. on Communication, Control and Computing, Monticello, USA, Sept. 2010.</p>
<p>[17] F. Chung, Spectral graph theory. Amer Mathematical Society, 1997.</p>
<p>[18] A. Gamburd, S. Hoory, M. Shahshahani, A. Shalev, and B. Virag, “On the girth of random cayley graphs,” Random Structures & Algorithms, vol. 35, no. 1, pp. 100–117, 2009.</p>
<p>[19] S. Dommers, C. Giardin` , and R. van der Hofstad, “Ising models on power-law random graphs,” Journal of a Statistical Physics, pp. 1–23, 2010.</p>
<p>[20] B. McKay, N. Wormald, and B. Wysocka, “Short cycles in random regular graphs,” The Electronic Journal of Combinatorics, vol. 11, no. R66, p. 1, 2004.</p>
<p>[21] A. Anandkumar, V. Y. F. Tan, and A. S. Willsky, “High-Dimensional Structure Learning of Ising Models: Tractable Graph Families,” Preprint, Available on ArXiv 1107.1736, June 2011.</p>
<p>[22] ——, “High-Dimensional Gaussian Graphical Model Selection: Tractable Graph Families,” Preprint, ArXiv 1107.1270, June 2011.</p>
<p>[23] V. Tan, A. Anandkumar, and A. Willsky, “Learning Markov Forest Models: Analysis of Error Rates,” J. of Machine Learning Research, vol. 12, pp. 1617–1653, May 2011.</p>
<p>[24] N. Meinshausen and P. Buehlmann, “High Dimensional Graphs and Variable Selection With the Lasso,” Annals of Statistics, vol. 34, no. 3, pp. 1436–1462, 2006.</p>
<p>[25] D. Weitz, “Counting independent sets up to the tree threshold,” in Proc. of ACM symp. on Theory of computing, 2006, pp. 140–149.</p>
<p>[26] W. Wang, M. Wainwright, and K. Ramchandran, “Information-theoretic bounds on model selection for Gaussian Markov random ﬁelds,” in IEEE International Symposium on Information Theory Proceedings (ISIT), Austin, Tx, June 2010.</p>
<p>[27] T. Cover and J. Thomas, Elements of Information Theory. John Wiley & Sons, Inc., 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
