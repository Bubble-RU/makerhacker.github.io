<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 nips-2011-On the Universality of Online Mirror Descent</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-202" href="../nips2011/nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">nips2011-202</a> <a title="nips-2011-202-reference" href="#">nips2011-202-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>202 nips-2011-On the Universality of Online Mirror Descent</h1>
<br/><p>Source: <a title="nips-2011-202-pdf" href="http://papers.nips.cc/paper/4413-on-the-universality-of-online-mirror-descent.pdf">pdf</a></p><p>Author: Nati Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2008.</p>
<p>[2] Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. 8</p>
<p>[3] Keith Ball, Eric A. Carlen, and Elliott H. Lieb. Sharp uniform convexity and smoothness inequalities for trace norms. Invent. Math., 115:463–482, 1994.</p>
<p>[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.</p>
<p>[5] H. D. Block. The perceptron: A model for brain functioning. Reviews of Modern Physics, 34:123–135, 1962. Reprinted in ”Neurocomputing” by Anderson and Rosenfeld.</p>
<p>[6] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Sparse and low-rank matrix decompositions. In IFAC Symposium on System Identiﬁcation, 2009.</p>
<p>[7] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the ￿1 -ball for learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning, 2008.</p>
<p>[8] Ali Jalali, Pradeep Ravikumar, Sujay Sanghavi, and Chao Ruan. A Dirty Model for Multi-task Learning. In NIPS, December 2010.</p>
<p>[9] A. Juditsky, G. Lan, A. Nemirovski, and A. Shapiro. Stochastic approximation approach to stochastic programming. SIAM J. Optim, 19(4):1574–1609, 2009.</p>
<p>[10] Sham M. Kakade, Shai Shalev-shwartz, and Ambuj Tewari. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization, 2010.</p>
<p>[11] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–64, January 1997.</p>
<p>[12] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. In Advances in Neural Information Processing Systems 21, pages 905–912, 2009.</p>
<p>[13] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318, 1988.</p>
<p>[14] A. Nemirovski and D. Yudin. On cesaro’s convergence of the gradient descent method for ﬁnding saddle points of convex-concave functions. Doklady Akademii Nauk SSSR, 239(4), 1978.</p>
<p>[15] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. Nauka Publishers, Moscow, 1978.</p>
<p>[16] G. Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics, 20(3–4):326–350, 1975.</p>
<p>[17] G. Pisier. Martingales in banach spaces (in connection with type and cotype). Winter School/IHP Graduate Course, 2011.</p>
<p>[18] A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial parameters, and learnability. NIPS, 2010.</p>
<p>[19] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT, 2009.</p>
<p>[20] S. Shalev-Shwartz and Y. Singer. Convex repeated games and fenchel duality. Advances in Neural Information Processing Systems, 19:1265, 2007.</p>
<p>[21] Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakola. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329–1336. MIT Press, 2005.</p>
<p>[22] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th Annual Conference on Learning Theory, pages 545–560. Springer-Verlag, 2005.</p>
<p>[23] Nathan Srebro and Ambuj Tewari. Stochastic optimization for machine learning. In ICML 2010, tutorial, 2010.</p>
<p>[24] K. Sridharan and A. Tewari. Convex games in Banach spaces. In Proceedings of the 23nd Annual Conference on Learning Theory, 2010.</p>
<p>[25] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University of Jerusalem, 2007.</p>
<p>[26] Manfred K. Warmuth and Dima Kuzmin. Randomized online pca algorithms with regret bounds that are logarithmic in the dimension, 2007.</p>
<p>[27] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.</p>
<p>[28] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320, 2005. 9</p>
<br/>
<br/><br/><br/></body>
</html>
