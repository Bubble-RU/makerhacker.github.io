<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 nips-2011-Scalable Training of Mixture Models via Coresets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-241" href="../nips2011/nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">nips2011-241</a> <a title="nips-2011-241-reference" href="#">nips2011-241-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>241 nips-2011-Scalable Training of Mixture Models via Coresets</h1>
<br/><p>Source: <a title="nips-2011-241-pdf" href="http://papers.nips.cc/paper/4363-scalable-training-of-mixture-models-via-coresets.pdf">pdf</a></p><p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ﬁtting the coreset will also provide a good ﬁt for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /ε2 ) data points sufﬁces for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efﬁciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><br/>
<h2>reference text</h2><p>[1] P. K. Agarwal, S. Har-Peled, and K. R. Varadarajan. Geometric approximations via coresets. Combinatorial and Computational Geometry - MSRI Publications, 52:1–30, 2005.</p>
<p>[2] A. Czumaj and C. Sohler. Sublinear-time approximation algorithms for clustering via random sampling. Random Struct. Algorithms (RSA), 30(1-2):226–256, 2007.</p>
<p>[3] Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians. Annals of Applied Probability, 15(1A):69–92, 2005.</p>
<p>[4] D. Feldman and M. Langberg. A uniﬁed framework for approximating and clustering data. In Proc. 41th Annu. ACM Symp. on Theory of Computing (STOC), 2011.</p>
<p>[5] S. Har-Peled and A. Kushal. Smaller coresets for k-median and k-means clustering. Discrete & Computational Geometry, 37(1):3–19, 2007.</p>
<p>[6] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. In Proc. 36th Annu. ACM Symp. on Theory of Computing (STOC), pages 291–300, 2004.</p>
<p>[7] Jon Louis Bentley and James B. Saxe. Decomposable searching problems i: Static-to-dynamic transformation. J. Algorithms, 1(4):301–358, 1980.</p>
<p>[8] D. Feldman, M. Monemizadeh, and C. Sohler. A PTAS for k-means clustering based on weak coresets. In Proc. 23rd ACM Symp. on Computational Geometry (SoCG), pages 11–18, 2007.</p>
<p>[9] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simpliﬁed data processing on large clusters. In OSDI’04: Sixth Symposium on Operating System Design and Implementation, 2004.</p>
<p>[10] D. Feldman, A. Fiat, and M. Sharir. Coresets for weighted facilities and their applications. In Proc. 47th IEEE Annu. Symp. on Foundations of Computer Science (FOCS), pages 315–324, 2006.</p>
<p>[11] Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized information maximization. In Proc. Neural Information Processing Systems (NIPS), 2010.</p>
<p>[12] Matthew Faulkner, Michael Olson, Rishi Chandy, Jonathan Krause, K. Mani Chandy, and Andreas Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors. In In Proc. ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), 2011.</p>
<p>[13] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. J. Roy. Statist. Soc. Ser. B, 39:1–38, 1977.</p>
<p>[14] S. Dasgupta. Learning mixtures of gaussians. In Fortieth Annual IEEE Symposium on Foundations of Computer Science (FOCS), 1999.</p>
<p>[15] S. Dasgupta and L.J. Schulman. A two-round variant of em for gaussian mixtures. In Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2000.</p>
<p>[16] S. Vempala and G. Wang. A spectral algorithm for learning mixture models. In In Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002.</p>
<p>[17] J. Feldman, R. A. Servedio, and R. O’Donnell. Pac learning axis-aligned mixtures of gaussians with no separation assumption. In COLT, 2006.</p>
<p>[18] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In In Proc. Foundations of Computer Science (FOCS), 2010.</p>
<p>[19] M. Belkin and K. Sinha. Polynomial learning of distribution families. In In Proc. Foundations of Computer Science (FOCS), 2010.</p>
<p>[20] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Inf. Comput., 100(1):78–150, 1992.</p>
<p>[21] S. Har-Peled and K. R. Varadarajan. High-dimensional shape ﬁtting in linear time. Discrete & Computational Geometry, 32(2):269–288, 2004.</p>
<p>[22] M.W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697, 2009.</p>
<p>[23] G. Frahling and C. Sohler. Coresets in dynamic geometric data streams. In Proc. 37th Annu. ACM Symp. on Theory of Computing (STOC), pages 209–217, 2005.  9</p>
<br/>
<br/><br/><br/></body>
</html>
