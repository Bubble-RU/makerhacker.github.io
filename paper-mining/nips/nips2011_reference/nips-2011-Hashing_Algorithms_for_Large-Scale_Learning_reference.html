<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2011-Hashing Algorithms for Large-Scale Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-111" href="../nips2011/nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">nips2011-111</a> <a title="nips-2011-111-reference" href="#">nips2011-111-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2011-Hashing Algorithms for Large-Scale Learning</h1>
<br/><p>Source: <a title="nips-2011-111-pdf" href="http://papers.nips.cc/paper/4403-hashing-algorithms-for-large-scale-learning.pdf">pdf</a></p><p>Author: Ping Li, Anshumali Shrivastava, Joshua L. Moore, Arnd C. König</p><p>Abstract: Minwise hashing is a standard technique in the context of search for efﬁciently computing set similarities. The recent development of b-bit minwise hashing provides a substantial improvement by storing only the lowest b bits of each hashed value. In this paper, we demonstrate that b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression, to solve large-scale and high-dimensional statistical learning tasks, especially when the data do not ﬁt in memory. We compare b-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW) algorithms, which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is signiﬁcantly more accurate (at the same storage cost) than VW (and random projections) for binary data.</p><br/>
<h2>reference text</h2><p>[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671–687, 2003.</p>
<p>[2] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In Commun. ACM, volume 51, pages 117–122, 2008.</p>
<p>[3] Harald Baayen. Word Frequency Distributions, volume 18 of Text, Speech and Language Technology. Kulver Academic Publishers, 2001.</p>
<p>[4] Michael Bendersky and W. Bruce Croft. Finding text reuse on the web. In WSDM, pages 262–271, Barcelona, Spain, 2009.</p>
<p>[5] Leon Bottou. http://leon.bottou.org/projects/sgd.</p>
<p>[6] Andrei Z. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences, pages 21–29, Positano, Italy, 1997.</p>
<p>[7] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering of the web. In WWW, pages 1157 – 1166, Santa Clara, CA, 1997.</p>
<p>[8] Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogram-based image classiﬁcation. IEEE Trans. Neural Networks, 10(5):1055–1064, 1999.</p>
<p>[9] Ludmila Cherkasova, Kave Eshghi, Charles B. Morrey III, Joseph Tucek, and Alistair C. Veitch. Applying syntactic similarity algorithms for enterprise information management. In KDD, pages 1087–1096, Paris, France, 2009.</p>
<p>[10] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael Mitzenmacher, Alessandro Panconesi, and Prabhakar Raghavan. On compressing social networks. In KDD, pages 219–228, Paris, France, 2009.</p>
<p>[11] Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch and its applications. Journal of Algorithm, 55(1):58–75, 2005.</p>
<p>[12] Yon Dourisboure, Filippo Geraci, and Marco Pellegrini. Extraction and classiﬁcation of dense implicit communities in the web graph. ACM Trans. Web, 3(2):1–36, 2009.</p>
<p>[13] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, 2008.</p>
<p>[14] Dennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. A large-scale study of the evolution of web pages. In WWW, pages 669–678, Budapest, Hungary, 2003.</p>
<p>[15] George Forman, Kave Eshghi, and Jaap Suermondt. Efﬁcient detection of large-scale redundancy in enterprise ﬁle systems. SIGOPS Oper. Syst. Rev., 43(1):84–91, 2009.</p>
<p>[16] Sreenivas Gollapudi and Aneesh Sharma. An axiomatic approach for result diversiﬁcation. In WWW, pages 381–390, Madrid, Spain, 2009.</p>
<p>[17] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability measures. In AISTATS, pages 136–143, Barbados, 2005.</p>
<p>[18] Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th international conference on Machine learning, ICML, pages 408–415, 2008.</p>
<p>[19] Yugang Jiang, Chongwah Ngo, and Jun Yang. Towards optimal bag-of-features for object categorization and semantic video retrieval. In CIVR, pages 494–501, Amsterdam, Netherlands, 2007.</p>
<p>[20] Thorsten Joachims. Training linear svms in linear time. In KDD, pages 217–226, Pittsburgh, PA, 2006.</p>
<p>[21] Ping Li and Kenneth W. Church. Using sketches to estimate associations. In HLT/EMNLP, pages 708–715, Vancouver, BC, Canada, 2005 (The full paper appeared in Commputational Linguistics in 2007).</p>
<p>[22] Ping Li, Kenneth W. Church, and Trevor J. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, pages 873–880, Vancouver, BC, Canada, 2006 (Newer results appeared in NIPS 2008.</p>
<p>[23] Ping Li, Trevor J. Hastie, and Kenneth W. Church. Improving random projections using marginal information. In COLT, pages 635–649, Pittsburgh, PA, 2006.</p>
<p>[24] Ping Li, Trevor J. Hastie, and Kenneth W. Church. Very sparse random projections. In KDD, pages 287–296, Philadelphia, PA, 2006.</p>
<p>[25] Ping Li and Arnd Christian K¨ nig. Theory and applications b-bit minwise hashing. In Commun. ACM, 2011. o</p>
<p>[26] Ping Li and Arnd Christian K¨ nig. Accurate estimators for improving minwise hashing and b-bit minwise hashing. Technical report, o 2011 (arXiv:1108.0895).</p>
<p>[27] Ping Li and Arnd Christian K¨ nig. b-bit minwise hashing. In WWW, pages 671–680, Raleigh, NC, 2010. o</p>
<p>[28] Ping Li, Arnd Christian K¨ nig, and Wenhao Gui. b-bit minwise hashing for estimating three-way similarities. In NIPS, Vancouver, BC, o 2010.</p>
<p>[29] Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. Detecting Near-Duplicates for Web-Crawling. In WWW, Banff, Alberta, Canada, 2007.</p>
<p>[30] Marc Najork, Sreenivas Gollapudi, and Rina Panigrahy. Less is more: sampling the neighborhood graph makes salsa better and faster. In WSDM, pages 242–251, Barcelona, Spain, 2009.</p>
<p>[31] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML, pages 807–814, Corvalis, Oregon, 2007.</p>
<p>[32] Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research, 10:2615–2637, 2009.</p>
<p>[33] Simon Tong. Lessons learned developing a practical large scale http://googleresearch.blogspot.com/2010/04/lessons-learned-developing-practical.html, 2008.  machine  learning  system.</p>
<p>[34] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In ICML, pages 1113–1120, 2009.</p>
<p>[35] Hsiang-Fu Yu, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin. Large linear classiﬁcation when data cannot ﬁt in memory. In KDD, pages 833–842, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
