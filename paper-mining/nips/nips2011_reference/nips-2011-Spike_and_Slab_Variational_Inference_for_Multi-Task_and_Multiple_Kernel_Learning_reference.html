<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-269" href="../nips2011/nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">nips2011-269</a> <a title="nips-2011-269-reference" href="#">nips2011-269-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2011-269-pdf" href="http://papers.nips.cc/paper/4305-spike-and-slab-variational-inference-for-multi-task-and-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><br/>
<h2>reference text</h2><p>[1] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):1023–1032, 1988.</p>
<p>[2] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267–288, 1994.</p>
<p>[3] M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001.</p>
<p>[4] E.I. George and R.E. Mcculloch. Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88(423):881–889, 1993.</p>
<p>[5] M. West. Bayesian factor regression models in the ”large p, small n” paradigm. In Bayesian Statistics, pages 723–732. Oxford University Press, 2003.</p>
<p>[6] B. Efron. Microarrays, empirical Bayes and the two-groups model. Statistical Science, 23:1–22, 2008.</p>
<p>[7] C. Archambeau and F. Bach. Sparse probabilistic projections. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 73–80. 2009.</p>
<p>[8] F. Caron and A. Doucet. Sparse Bayesian nonparametric regression. In In 25th International Conference on Machine Learning (ICML). ACM, 2008.</p>
<p>[9] Matthias W. Seeger and Hannes Nickisch. Compressed sensing and Bayesian experimental design. In ICML, pages 912–919, 2008.</p>
<p>[10] C.M. Carvalho, N.G. Polson, and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika, 97:465–480, 2010.</p>
<p>[11] T. Damoulas and M.A. Girolami. Probabilistic multi-class multi-kernel learning: on protein fold recognition and remote homology detection. Bioinformatics, 24:1264–1270, 2008.</p>
<p>[12] M. Christoudias, R. Urtasun, and T. Darrell. Bayesian localized multiple kernel learning. Technical report, EECS Department, University of California, Berkeley, Jul 2009.</p>
<p>[13] C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 23 workshop on New Directions in Multiple Kernel Learning. 2010.</p>
<p>[14] Y.W. Teh, M. Seeger, and M.I. Jordan. Semiparametric latent factor models. In Proceedings of the International Workshop on Artiﬁcial Intelligence and Statistics, volume 10, 2005.</p>
<p>[15] E.V. Bonilla, K.M.A. Chai, and C.K.I. Williams. Multi-task Gaussian process prediction. In Advances Neural Information Processing Systems 20, 2008.</p>
<p>[16] P Boyle and M. Frean. Dependent Gaussian processes. In Advances in Neural Information Processing Systems 17, pages 217–224. MIT Press, 2005.</p>
<p>[17] M. Alvarez and N.D. Lawrence. Sparse convolved Gaussian processes for multi-output regression. In Advances in Neural Information Processing Systems 20, pages 57–64, 2008.</p>
<p>[18] R. Yoshida and M. West. Bayesian learning in sparse graphical factor models via variational mean-ﬁeld annealing. Journal of Machine Learning Research, 11:1771–1798, 2010.</p>
<p>[19] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Non-parametric Bayesian dictionary learning for sparse image represent ations. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2295–2303. 2009.</p>
<p>[20] M. L´ zaro-Gredilla and M. Titsias. Variational heteroscedastic Gaussian process regression. In 28th a International Conference on Machine Learning (ICML-11), pages 841–848, New York, NY, USA, June 2011. ACM.</p>
<p>[21] M.E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.</p>
<p>[22] M. Varma and D. Ray. Learning the discriminative power invariance trade-off. In International Conference on Computer Vision. 2007.</p>
<p>[23] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. IEEE Trans. Image Processing, 17, 2008.</p>
<p>[24] N.D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 601–608, 2009.</p>
<p>[25] K. Sharp and M. Rattray. Dense message passing for sparse principal component analysis. In 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 725–732, 2010.</p>
<p>[26] J.M. Hern´ ndez-Lobato, D. Hern´ ndez-Lobato, and A. Su´ rez. Network-based sparse Bayesian classiﬁa a a cation. Pattern Recognition, 44(4):886–900, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
