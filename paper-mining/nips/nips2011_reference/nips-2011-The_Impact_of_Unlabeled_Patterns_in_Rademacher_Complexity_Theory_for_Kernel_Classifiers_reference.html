<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-284" href="../nips2011/nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">nips2011-284</a> <a title="nips-2011-284-reference" href="#">nips2011-284-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</h1>
<br/><p>Source: <a title="nips-2011-284-pdf" href="http://papers.nips.cc/paper/4234-the-impact-of-unlabeled-patterns-in-rademacher-complexity-theory-for-kernel-classifiers.pdf">pdf</a></p><p>Author: Luca Oneto, Davide Anguita, Alessandro Ghio, Sandro Ridella</p><p>Abstract: We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classiﬁer. 1</p><br/>
<h2>reference text</h2><p>[1] V.N. Vapnik and A.Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16:264, 1971. 8</p>
<p>[2] P.L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. The Journal of Machine Learning Research, 3:463–482, 2003.</p>
<p>[3] P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statistics, 33(4):1497–1537, 2005.</p>
<p>[4] O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002.</p>
<p>[5] P.L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error estimation. Machine Learning, 48(1):85–113, 2002.</p>
<p>[6] D. Anguita, A. Ghio, and S. Ridella. Maximal discrepancy for support vector machines. Neurocomputing, 74(9):1436–1443, 2011.</p>
<p>[7] D. Anguita, A. Ghio, L. Oneto, and S. Ridella. Selecting the Hypothesis Space for Improving the Generalization Ability of Support Vector Machines. In The 2011 International Joint Conference on Neural Networks (IJCNN), San Jose, California. IEEE, 2011.</p>
<p>[8] S. Arlot and A. Celisse. A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40–79, 2010.</p>
<p>[9] B. Efron and R. Tibshirani. An introduction to the bootstrap. Chapman & Hall/CRC, 1993.</p>
<p>[10] D. Anguita, A. Ghio, L. Oneto, and S. Ridella. In-sample Model Selection for Support Vector Machines. In The 2011 International Joint Conference on Neural Networks (IJCNN), San Jose, California. IEEE, 2011.</p>
<p>[11] K.P. Bennett and A. Demiriz. Semi-supervised support vector machines. In Advances in neural information processing systems 11: proceedings of the 1998 conference, page 368. The MIT Press, 1999.</p>
<p>[12] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning. The MIT Press, page 528, 2010.</p>
<p>[13] V.N. Vapnik. The nature of statistical learning theory. Springer Verlag, 2000.</p>
<p>[14] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability. In Proceedings of the 23rd international conference on Machine learning, pages 201–208. ACM, 2006.</p>
<p>[15] C. McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148– 188, 1989.</p>
<p>[16] S. Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR Upper Saddle River, NJ, USA, 1994.</p>
<p>[17] S. Boucheron, G. Lugosi, and P. Massart. On concentration of self-bounding functions. Electronic Journal of Probability, 14:1884–1899, 2009.</p>
<p>[18] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities using the entropy method. The Annals of Probability, 31(3):1583–1614, 2003.</p>
<p>[19] G. Casella and R.L. Berger. Statistical inference. 2001.</p>
<p>[20] I. CPLEX. 11.0 users manual. ILOG SA, 2008.</p>
<p>[21] J. Wang, X. Shen, and W. Pan. On efﬁcient large margin semisupervised learning: Method and theory. Journal of Machine Learning Research, 10:719–742, 2009.</p>
<p>[22] J. Wang and X. Shen. Large margin semi–supervised learning. Journal of Machine Learning Research, 8:1867–1891, 2007.</p>
<p>[23] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Advances in Kernel MethodsSupport Vector Learning, 208:1–21, 1998.</p>
<p>[24] J. Shawe-Taylor and N. Cristianini. Margin distribution and soft margin. Advances in Large Margin Classiﬁers, pages 349–358, 2000.</p>
<p>[25] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In 24th ICML, pages 473–480, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
