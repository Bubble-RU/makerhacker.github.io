<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-70" href="../nips2011/nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">nips2011-70</a> <a title="nips-2011-70-reference" href="#">nips2011-70-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</h1>
<br/><p>Source: <a title="nips-2011-70-pdf" href="http://papers.nips.cc/paper/4336-dimensionality-reduction-using-the-sparse-linear-model.pdf">pdf</a></p><p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><br/>
<h2>reference text</h2><p>[1] M.A. Davenport, P.T. Boufounos, M.B. Wakin, and R.G. Baraniuk. Signal processing with compressive measurements. IEEE JSTSP, 2010.</p>
<p>[2] S.J. Koppal, I. Gkioulekas, T. Zickler, and G.L. Barrows. Wide-angle micro sensors for vision on a tight budget. CVPR, 2011.</p>
<p>[3] I. Jolliffe. Principal component analysis. Wiley, 1986.</p>
<p>[4] X. He and P. Niyogi. Locality Preserving Projections. NIPS, 2003.</p>
<p>[5] X. He, D. Cai, S. Yan, and H.J. Zhang. Neighborhood preserving embedding. ICCV, 2005.</p>
<p>[6] D. Cai, X. He, J. Han, and H.J. Zhang. Orthogonal laplacianfaces for face recognition. IEEE IP, 2006.</p>
<p>[7] D. Cai, X. He, and J. Han. Spectral regression for efﬁcient regularized subspace learning. ICCV, 2007.</p>
<p>[8] D. Cai, X. He, Y. Hu, J. Han, and T. Huang. Learning a spatially smooth subspace for face recognition. CVPR, 2007.</p>
<p>[9] X. He, D. Cai, and P. Niyogi. Tensor subspace analysis. NIPS, 2006.</p>
<p>[10] J. Ye, R. Janardan, and Q. Li. Two-dimensional linear discriminant analysis. NIPS, 2004.</p>
<p>[11] B. Scholkopf, A. Smola, and K.R. Muller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 1998.</p>
<p>[12] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 1997.</p>
<p>[13] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse representation. PAMI, 2008.</p>
<p>[14] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE IP, 2006.</p>
<p>[15] J.F. Cai, H. Ji, C. Liu, and Z. Shen. Blind motion deblurring from a single image using sparse approximation. CVPR, 2009.</p>
<p>[16] R. Raina, A. Battle, H. Lee, B. Packer, and A.Y. Ng. Self-taught learning: Transfer learning from unlabeled data. ICML, 2007.</p>
<p>[17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. NIPS, 2008.  8</p>
<p>[18] I. Ramirez, P. Sprechmann, and G. Sapiro. Classiﬁcation and clustering via dictionary learning with structured incoherence and shared features. CVPR, 2010.</p>
<p>[19] J. Yang, K. Yu, and T. Huang. Supervised translation-invariant sparse coding. CVPR, 2010.</p>
<p>[20] M.W. Seeger. Bayesian inference and optimal design for the sparse linear model. JMLR, 2008.</p>
<p>[21] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. NIPS, 2007.</p>
<p>[22] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. JMLR, 2010.</p>
<p>[23] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations. NIPS, 2009.</p>
<p>[24] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. ICCV, 2009.</p>
<p>[25] R. Tibshirani. Regression shrinkage and selection via the lasso. JRSS-B, 1996.</p>
<p>[26] A.M. Bruckstein, D.L. Donoho, and M. Elad. From sparse solutions of systems of equations to sparse modeling of signals and images. SIAM review, 2009.</p>
<p>[27] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of statistics, 2004.</p>
<p>[28] J. Ham, D.D. Lee, S. Mika, and B. Sch¨ lkopf. A kernel view of the dimensionality reduction of manifolds. o ICML, 2004.</p>
<p>[29] W.J. Fu. Penalized regressions: the bridge versus the lasso. JCGS, 1998.</p>
<p>[30] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. JRSS-B, 2005.</p>
<p>[31] S. Ji, Y. Xue, and L. Carin. Bayesian compressive sensing. IEEE SP, 2008.</p>
<p>[32] N. Srebro and T. Jaakkola. Weighted low-rank approximations. ICML, 2003.</p>
<p>[33] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Kluwer, 2004.</p>
<p>[34] V.I. Bogachev. Gaussian measures. AMS, 1998.</p>
<p>[35] J. Kuelbs, FM Larkin, and J.A. Williamson. Weak probability distributions on reproducing kernel hilbert spaces. Rocky Mountain J. Math, 1972.</p>
<p>[36] S. Gao, I. Tsang, and L.T. Chia. Kernel Sparse Representation for Image Classiﬁcation and Face Recognition. ECCV, 2010.</p>
<p>[37] J. Abernethy, F. Bach, T. Evgeniou, and J.P. Vert. A new approach to collaborative ﬁltering: Operator estimation with spectral regularization. JMLR, 2009.</p>
<p>[38] B. Scholkopf, R. Herbrich, and A. Smola. A generalized representer theorem. COLT, 2001.</p>
<p>[39] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination, and expression (PIE) database. IEEE ICAFGR, 2002.</p>
<p>[40] T. Randen and J.H. Husoy. Filtering for texture classiﬁcation: A comparative study. PAMI, 2002.</p>
<p>[41] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories. CVPR Workshops, 2004.</p>
<p>[42] P. Gehler and S. Nowozin. On feature combination for multiclass object classiﬁcation. ICCV, 2009.</p>
<p>[43] D. Dueck and B.J. Frey. Non-metric afﬁnity propagation for unsupervised image categorization. ICCV, 2007.</p>
<p>[44] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 2000.</p>
<p>[45] N.X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. JMLR, 2010.</p>
<p>[46] T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman & Hall, 2000.</p>
<p>[47] E.J. Cand` s and T. Tao. Decoding by linear programming. IEEE IT, 2005. e</p>
<p>[48] H. Rauhut, K. Schnass, and P. Vandergheynst. Compressed sensing and redundant dictionaries. IEEE IT, 2008.</p>
<p>[49] D.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE IT, 2001.</p>
<p>[50] M. Elad. Optimized projections for compressed sensing. IEEE SP, 2007.</p>
<p>[51] J.M. Duarte-Carvajalino and G. Sapiro. Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization. IEEE IP, 2009.</p>
<p>[52] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. NIPS, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
