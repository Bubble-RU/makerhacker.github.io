<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2011-Generalized Beta Mixtures of Gaussians</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-104" href="../nips2011/nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">nips2011-104</a> <a title="nips-2011-104-reference" href="#">nips2011-104-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>104 nips-2011-Generalized Beta Mixtures of Gaussians</h1>
<br/><p>Source: <a title="nips-2011-104-pdf" href="http://papers.nips.cc/paper/4439-generalized-beta-mixtures-of-gaussians.pdf">pdf</a></p><p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><br/>
<h2>reference text</h2><p>[1] A. Armagan. Variational bridge regression. JMLR: W&CP;, 5:17–24, 2009. 8</p>
<p>[2] A. Armagan, D. B. Dunson, and J. Lee. arXiv:1104.0861v2, 2011.  Generalized double Pareto shrinkage.</p>
<p>[3] C. Armero and M. J. Bayarri. Prior assessments for prediction in queues. The Statistician, 43(1):pp. 139–153, 1994.</p>
<p>[4] J. Berger. A robust generalized Bayes estimator and conﬁdence region for a multivariate normal mean. The Annals of Statistics, 8(4):pp. 716–761, 1980.</p>
<p>[5] C. M. Bishop and M. E. Tipping. Variational relevance vector machines. In UAI ’00: Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intelligence, pages 46–53, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.</p>
<p>[6] C. M. Carvalho, N. G. Polson, and J. G. Scott. Handling sparsity via the horseshoe. JMLR: W&CP;, 5, 2009.</p>
<p>[7] C. M. Carvalho, N. G. Polson, and J. G. Scott. The horseshoe estimator for sparse signals. Biometrika, 97(2):465–480, 2010.</p>
<p>[8] M. A. T. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25:1150–1159, 2003.</p>
<p>[9] E. I. George and R. E. McCulloch. Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88, 1993.</p>
<p>[10] M. Gordy. A generalization of generalized beta distributions. Finance and Economics Discussion Series 1998-18, Board of Governors of the Federal Reserve System (U.S.), 1998.</p>
<p>[11] J. E. Grifﬁn and P. J. Brown. Bayesian adaptive lassos with non-convex penalization. Technical Report, 2007.</p>
<p>[12] J. E. Grifﬁn and P. J. Brown. Inference with normal-gamma prior distributions in regression problems. Bayesian Analysis, 5(1):171–188, 2010.</p>
<p>[13] C. Hans. Bayesian lasso regression. Biometrika, 96:835–845, 2009.</p>
<p>[14] C. J. Hoggart, J. C. Whittaker, and David J. Balding M. De Iorio. Simultaneous analysis of all SNPs in genome-wide and re-sequencing association studies. PLoS Genetics, 4(7), 2008.</p>
<p>[15] H. Ishwaran and J. S. Rao. Spike and slab variable selection: Frequentist and Bayesian strategies. The Annals of Statistics, 33(2):pp. 730–773, 2005.</p>
<p>[16] I. M. Johnstone and B. W. Silverman. Needles and straw in haystacks: Empirical Bayes estimates of possibly sparse sequences. Annals of Statistics, 32(4):pp. 1594–1649, 2004.</p>
<p>[17] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. MIT Press, Cambridge, MA, USA, 1999.</p>
<p>[18] T. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):pp. 1023–1032, 1988.</p>
<p>[19] T. Park and G. Casella. The Bayesian lasso. Journal of the American Statistical Association, 103:681–686(6), 2008.</p>
<p>[20] N. G. Polson and J. G. Scott. Alternative global-local shrinkage rules using hypergeometricbeta mixtures. Discussion Paper 2009-14, Department of Statistical Science, Duke University, 2009.</p>
<p>[21] W. E. Strawderman. Proper Bayes minimax estimators of the multivariate normal mean. The Annals of Mathematical Statistics, 42(1):pp. 385–388, 1971.</p>
<p>[22] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996.</p>
<p>[23] L. Tierney and J. B. Kadane. Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393):82–86, 1986.</p>
<p>[24] M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
