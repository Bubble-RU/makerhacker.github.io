<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2011-Inverting Grice's Maxims to Learn Rules from Natural Language Extractions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-136" href="../nips2011/nips-2011-Inverting_Grice%27s_Maxims_to_Learn_Rules_from_Natural_Language_Extractions.html">nips2011-136</a> <a title="nips-2011-136-reference" href="#">nips2011-136-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 nips-2011-Inverting Grice's Maxims to Learn Rules from Natural Language Extractions</h1>
<br/><p>Source: <a title="nips-2011-136-pdf" href="http://papers.nips.cc/paper/4197-inverting-grices-maxims-to-learn-rules-from-natural-language-extractions.pdf">pdf</a></p><p>Author: Mohammad S. Sorower, Janardhan R. Doppa, Walker Orr, Prasad Tadepalli, Thomas G. Dietterich, Xiaoli Z. Fern</p><p>Abstract: We consider the problem of learning rules from natural language text sources. These sources, such as news articles and web texts, are created by a writer to communicate information to a reader, where the writer and reader share substantial domain knowledge. Consequently, the texts tend to be concise and mention the minimum information necessary for the reader to draw the correct conclusions. We study the problem of learning domain knowledge from such concise texts, which is an instance of the general problem of learning in the presence of missing data. However, unlike standard approaches to missing data, in this setting we know that facts are more likely to be missing from the text in cases where the reader can infer them from the facts that are mentioned combined with the domain knowledge. Hence, we can explicitly model this “missingness” process and invert it via probabilistic inference to learn the underlying domain knowledge. This paper introduces a mention model that models the probability of facts being mentioned in the text based on what other facts have already been mentioned and domain knowledge in the form of Horn clause rules. Learning must simultaneously search the space of rules and learn the parameters of the mention model. We accomplish this via an application of Expectation Maximization within a Markov Logic framework. An experimental evaluation on synthetic and natural text data shows that the method can learn accurate rules and apply them to new texts to make correct inferences. Experiments also show that the method out-performs the standard EM approach that assumes mentions are missing at random. 1</p><br/>
<h2>reference text</h2><p>[1] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr., and T.M. Mitchell. Toward an architecture for never-ending language learning. In Proceedings of the Conference on Artiﬁcial Intelligence (AAAI), pages 1306–1313. AAAI Press, 2010.</p>
<p>[2] A. Carlson, J. Betteridge, R. C. Wang, E. R. Hruschka, Jr., and T. M. Mitchell. Coupled semisupervised learning for information extraction. In Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM ’10, pages 101–110, New York, NY, USA, 2010. ACM.</p>
<p>[3] W. W. Cohen. WHIRL: A word-based information representation language. Artiﬁcial Intelligence, 118(1-2):163–196, 2000.</p>
<p>[4] J. R. Doppa, M. S. Sorower, M. Nasresfahani, J. Irvine, W. Orr, T. G. Dietterich, X. Fern, and P. Tadepalli. Learning rules from incomplete examples via implicit mention models. In Proceedings of the 2011 Asian Conference on Machine Learning, 2011.</p>
<p>[5] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld. Open information extraction from the web. Commun. ACM, 51(12):68–74, 2008.</p>
<p>[6] M. Freedman, E. Loper, E. Boschee, and R. Weischedel. Empirical Studies in Learning to Read. In Proceedings of Workshop on Formalisms and Methodology for Learning by Reading (NAACL-2010), pages 61–69, 2010.</p>
<p>[7] H. P. Grice. Logic and conversation. In Syntax and semantics: Speech acts, volume 3, pages 43–58. Academic Press, New York, 1975.</p>
<p>[8] S. Kok, M. Sumner, M. Richardson, P. Singla, H. Poon, D. Lowd, and P. Domingos. The Alchemy system for statistical relational AI. Technical report, Department of Computer Science and Engineering, University of Washington, Seattle, WA, 2007.</p>
<p>[9] L. Michael. Reading between the lines. In IJCAI, pages 1525–1530, 2009.</p>
<p>[10] L. Michael and L. G. Valiant. A ﬁrst experimental demonstration of massive knowledge infusion. In KR, pages 378–389, 2008.</p>
<p>[11] U. Y. Nahm and R. J. Mooney. A mutually beneﬁcial integration of data mining and information extraction. In Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence and the Twelfth Conference on Innovative Applications of Artiﬁcial Intelligence, pages 627–632. AAAI Press, 2000.</p>
<p>[12] NIST. Automatic Content Extraction 2008 Evaluation Plan.</p>
<p>[13] R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda. English Gigaword Fourth Edition. Linguistic Data Consortium, Philadelphia, 2009.</p>
<p>[14] L. Ramshaw, E. Boschee, M. Freedman, J. MacBride, R. Weischedel, and A.Zamanian. Serif language processing effective trainable language understanding. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. Springer, 2011.</p>
<p>[15] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62:107–136, February 2006.</p>
<p>[16] J. L. Schafer and M. K. Olsen. Multiple imputation for multivariate missing-data problems: a data analyst’s perspective. Multivariate Behavioral Research, 33:545–571, 1998.</p>
<p>[17] S. Schoenmackers, O. Etzioni, D. S. Weld, and J. Davis. Learning ﬁrst-order Horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1088–1098, Stroudsburg, PA, USA, 2010. Association for Computational Linguistics.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
