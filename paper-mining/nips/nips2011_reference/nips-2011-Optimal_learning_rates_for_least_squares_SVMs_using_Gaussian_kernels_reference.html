<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-207" href="../nips2011/nips-2011-Optimal_learning_rates_for_least_squares_SVMs_using_Gaussian_kernels.html">nips2011-207</a> <a title="nips-2011-207-reference" href="#">nips2011-207-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</h1>
<br/><p>Source: <a title="nips-2011-207-pdf" href="http://papers.nips.cc/paper/4216-optimal-learning-rates-for-least-squares-svms-using-gaussian-kernels.pdf">pdf</a></p><p>Author: Mona Eberts, Ingo Steinwart</p><p>Abstract: We prove a new oracle inequality for support vector machines with Gaussian RBF kernels solving the regularized least squares regression problem. To this end, we apply the modulus of smoothness. With the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method. Finally, it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions. 1</p><br/>
<h2>reference text</h2><p>[1] L. Gy¨ rﬁ, M. Kohler, A. Krzy˙ ak, and H. Walk. A Distribution-Free Theory of Nonparametric o z Regression. Springer-Verlag New York, 2002.</p>
<p>[2] I. Steinwart and A. Christmann. Support Vector Machines. Springer-Verlag, New York, 2008.</p>
<p>[3] R.A. DeVore and G.G. Lorentz. Constructive Approximation. Springer-Verlag Berlin Heidelberg, 1993.</p>
<p>[4] R.A. DeVore and V.A. Popov. Interpolation of Besov Spaces. AMS, Volume 305, 1988.</p>
<p>[5] H. Berens and R.A. DeVore. Quantitative Korovin theorems for positive linear operators on Lp -spaces. AMS, Volume 245, 1978.</p>
<p>[6] H. Johnen and K. Scherer. On the equivalence of the K-functional and moduli of continuity and some applications. In Lecture Notes in Math., volume 571, pages 119–140. Springer-Verlag Berlin, 1976.</p>
<p>[7] D.E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential 0perators. Cambridge University Press, 1996.</p>
<p>[8] E.M. Stein. Singular Integrals and Differentiability Properties of Functions. Princeton Univ. Press, 1970.</p>
<p>[9] R.A. Adams and J.J.F. Fournier. Sobolev Spaces. Academic Press, 2nd edition, 2003.</p>
<p>[10] H. Triebel. Theory of Function Spaces III. Birkh¨ user Verlag, 2006. a</p>
<p>[11] V. Temlyakov. Optimal estimators in learning theory. Banach Center Publications, Inst. Math. Polish Academy of Sciences, 72:341–366, 2006.</p>
<p>[12] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. Proceedings of the 22nd Annual Conference on Learning Theory, 2009.</p>
<p>[13] F. Cucker and S. Smale. On the mathematical foundations of learning. Bull. Amer. Math. Soc., 39:1–49, 2002.</p>
<p>[14] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized least-squares algorithm in learning theory. Found. Comput. Math., 5:59–85, 2005.</p>
<p>[15] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. Constr. Approx., 26:153–172, 2007.</p>
<p>[16] A. Caponnetto and E. De Vito. Optimal rates for regularized least squares algorithm. Found. Comput. Math., 7:331–368, 2007.</p>
<p>[17] S. Mendelson and J. Neeman. Regularization in kernel learning. Ann. Statist., 38:526–565, 2010.</p>
<p>[18] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl., Volume 1, 2003.</p>
<p>[19] I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Ann. Statist., 35:575–607, 2007.</p>
<p>[20] D.-H. Xiang and D.-X. Zhou. Classiﬁcation with Gaussians and convex loss. J. Mach. Learn. Res., 10:1447–1468, 2009.</p>
<p>[21] G.-B. Ye and D.-X. Zhou. Learning and approximation by Gaussians on Riemannian manifolds. Adv. Comput. Math., Volume 29, 2008.</p>
<p>[22] Y. Ying and D.-X. Zhou. Learnability of Gaussians with ﬂexible variances. J. Mach. Learn. Res. 8, 2007.</p>
<p>[23] C.A. Micchelli, M. Pontil, Q. Wu, and D.-X. Zhou. Error bounds for learning the kernel. 2005.</p>
<p>[24] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In S. Dasgupta and A. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
