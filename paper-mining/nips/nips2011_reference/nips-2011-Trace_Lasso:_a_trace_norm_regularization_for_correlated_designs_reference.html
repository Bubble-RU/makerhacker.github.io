<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-289" href="../nips2011/nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">nips2011-289</a> <a title="nips-2011-289-reference" href="#">nips2011-289-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</h1>
<br/><p>Source: <a title="nips-2011-289-pdf" href="http://papers.nips.cc/paper/4277-trace-lasso-a-trace-norm-regularization-for-correlated-designs.pdf">pdf</a></p><p>Author: Edouard Grave, Guillaume R. Obozinski, Francis R. Bach</p><p>Abstract: Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. 1</p><br/>
<h2>reference text</h2><p>[1] S.G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. Signal Processing, IEEE Transactions on, 41(12):3397–3415, 1993.</p>
<p>[2] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances in Neural Information Processing Systems, 22, 2008.</p>
<p>[3] M.W. Seeger. Bayesian inference and optimal design for the sparse linear model. The Journal of Machine Learning Research, 9:759–813, 2008.</p>
<p>[4] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996.</p>
<p>[5] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM journal on scientiﬁc computing, 20(1):33–61, 1999.</p>
<p>[6] P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.</p>
<p>[7] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006.</p>
<p>[8] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (Lasso). Information Theory, IEEE Transactions on, 55(5):2183–2202, 2009.</p>
<p>[9] G.H. Golub and C.F. Van Loan. Matrix computations. Johns Hopkins Univ Pr, 1996.</p>
<p>[10] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.</p>
<p>[11] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006.</p>
<p>[12] F. Bach. Consistency of the group Lasso and multiple kernel learning. The Journal of Machine Learning Research, 9:1179–1225, 2008.</p>
<p>[13] F. Bach. Bolasso: model consistent Lasso estimation through the bootstrap. In Proceedings of the 25th international conference on Machine learning, pages 33–40. ACM, 2008.</p>
<p>[14] H. Liu, K. Roeder, and L. Wasserman. Stability approach to regularization selection (stars) for high dimensional graphical models. Advances in Neural Information Processing Systems, 23, 2010.</p>
<p>[15] N. Meinshausen and P. B¨ hlmann. Stability selection. Journal of the Royal Statistical Society: u Series B (Statistical Methodology), 72(4):417–473, 2010.</p>
<p>[16] A. Tikhonov. Solution of incorrectly formulated problems and the regularization method. In Soviet Math. Dokl., volume 5, page 1035, 1963.</p>
<p>[17] A.E. Hoerl and R.W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67, 1970.</p>
<p>[18] G. Davis, S. Mallat, and M. Avellaneda. Adaptive greedy approximations. Constructive approximation, 13(1):57–98, 1997.</p>
<p>[19] E.J. Cand` s and T. Tao. Decoding by linear programming. Information Theory, IEEE Transe actions on, 51(12):4203–4215, 2005.</p>
<p>[20] A. Lorbert, D. Eis, V. Kostina, D. M. Blei, and P. J. Ramadge. Exploiting covariate similarity in sparse regression via the pairwise elastic net. JMLR - Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, 9:477–484, 2010.</p>
<p>[21] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning. 2001.</p>
<p>[22] E. Grave, G. Obozinski, and F. Bach. Trace lasso: a trace norm regularization for correlated designs. Technical report, arXiv:1109.1990, 2011.</p>
<p>[23] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. Advances in neural information processing systems, 19:41, 2007.</p>
<p>[24] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Pr, 2004.</p>
<p>[25] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. S. Sra, S. Nowozin, S. J. Wright., editors, Optimization for Machine Learning, 2011. 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
