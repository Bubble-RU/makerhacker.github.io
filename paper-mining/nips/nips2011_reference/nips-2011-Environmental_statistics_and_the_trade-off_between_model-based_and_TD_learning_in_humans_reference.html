<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-88" href="../nips2011/nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">nips2011-88</a> <a title="nips-2011-88-reference" href="#">nips2011-88-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</h1>
<br/><p>Source: <a title="nips-2011-88-pdf" href="http://papers.nips.cc/paper/4243-environmental-statistics-and-the-trade-off-between-model-based-and-td-learning-in-humans.pdf">pdf</a></p><p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><br/>
<h2>reference text</h2><p>[1] Bernard W. Balleine, Nathaniel D. Daw, and John P. O’Doherty. Multiple forms of value learning and the function of dopamine. In Paul W. Glimcher, Colin F. Camerer, Ernst Fehr, and Russell A. Poldrack, editors, Neuroeconomics: Decision Making and the Brain, chapter 24, pages 367–387. Academic Press, London, 2008.</p>
<p>[2] Antoine Bechara. Decision making, impulse control and loss of willpower to resist drugs: a neurocognitive perspective. Nat Neurosci, 8(11):1458–63, 2005.</p>
<p>[3] Frederick Toates. The interaction of cognitive and stimulus-response processes in the control of behaviour. Neuroscience & Biobehavioral Reviews, 22(1):59–83, 1997.</p>
<p>[4] Peter Dayan. Goal-directed control and its antipodes. Neural Netw, 22:213–219, 2009.</p>
<p>[5] Neal Schmitt, Bryan W. Coyle, and Larry King. Feedback and task predictability as determinants of performance in multiple cue probability learning tasks. Organ Behav Hum Perform, 16(2):388–402, 1976.</p>
<p>[6] Berndt Brehmer and Jan Kuylenstierna. Task information and performance in probabilistic inference tasks. Organ Behav Hum Perform, 22:445–464, 1978.</p>
<p>[7] B J Knowlton, L R Squire, and M A Gluck. Probabilistic classiﬁcation learning in amnesia. Learn Mem, 1(2):106–120, 1994.</p>
<p>[8] W. Todd Maddox and F. Gregory Ashby. Dissociating explicit and procedural-learning based systems of perceptual category learning. Behavioural Processes, 66(3):309–332, 2004.</p>
<p>[9] W. Todd Maddox, J. Vincent Filoteo, Kelli D. Hejl, and A. David Ing. Category number impacts rule-based but not information-integration category learning: Further evidence for dissociable categorylearning systems. J Exp Psychol Learn Mem Cogn, 30(1):227–245, 2004.</p>
<p>[10] R. A. Poldrack, J. Clark, E. J. Par´ -Blagoev, D. Shohamy, J. Creso Moyano, C. Myers, and M. A. Gluck. e Interactive memory systems in the human brain. Nature, 414(6863):546–550, 2001.</p>
<p>[11] Bernard W. Balleine and Anthony Dickinson. Goal-directed instrumental action: contingency and incentive learning and their cortical substrates. Neuropharmacology, 37(4–5):407–419, 1998.</p>
<p>[12] Kenji Doya. What are the computations of the cerebellum, the basal ganglia and the cerebral cortex? Neural Netw, 12(7–8):961–974, 1999.</p>
<p>[13] Nathaniel D. Daw, Yael Niv, and Peter Dayan. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nat Neurosci, 8(12):1704–1711, 2005.</p>
<p>[14] Ben Seymour, John P. O’Doherty, Peter Dayan, Martin Koltzenburg, Anthony K. Jones, Raymond J. Dolan, Karl J. Friston, and Richard S. Frackowiak. Temporal difference models describe higher-order learning in humans. Nature, 429(6992):664–667, 2004.</p>
<p>[15] John P. O’Doherty, Peter Dayan, Johannes Schultz, Ralf Deichmann, Karl Friston, and Raymond J. Dolan. Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science, 304(5669):452– 454, 2004.</p>
<p>[16] Adam Johnson and A. David Redish. Hippocampal replay contributes to within session learning in a temporal difference reinforcement learning model. Neural Netw, 18(9):1163–1171, 2005.</p>
<p>[17] M´ t´ Lengyel and Peter Dayan. Hippocampal contributions to control: The third way. In J.C. Platt, ae D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 889–896. MIT Press, Cambridge, MA, 2008.</p>
<p>[18] Mehdi Keramati, Amir Dezfouli, and Payam Piray. Speed/accuracy trade-off between the habitual and the goal-directed processes. PLoS Comput Biol, 7(5):e1002055, 2011.</p>
<p>[19] Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect algorithms. In Michael S. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information Processing Systems 11, volume 11, pages 996–1002. MIT Press, Cambridge, MA, 1999.</p>
<p>[20] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. J Basic Eng, 82(1):35–45, 1960.</p>
<p>[21] Nathaniel D Daw, S. J. Gershman, B. Seymour, P. Dayan, and R. J. Dolan. Model-based inﬂuences on humans’ choices and striatal prediction errors. Neuron, 69(6):1204–1215, 2011.</p>
<p>[22] Brian Lau and Paul W Glimcher. Dynamic response-by-response models of matching behavior in rhesus monkeys. J Exp Anal Behav, 84(3):555–579, 2005.</p>
<p>[23] Douglas Bates, Martin Maechler, and Ben Bolker. lme4: Linear mixed-effects models using S4 classes, 2011. R package version 0.999375-39.</p>
<p>[24] Saori C Tanaka, Kazuyuki Samejima, Go Okada, Kazutaka Ueda, Yasumasa Okamoto, Shigeto Yamawaki, and Kenji Doya. Brain mechanism of reward prediction under predictable and unpredictable environmental dynamics. Neural Netw, 19(8):1233–1241, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
