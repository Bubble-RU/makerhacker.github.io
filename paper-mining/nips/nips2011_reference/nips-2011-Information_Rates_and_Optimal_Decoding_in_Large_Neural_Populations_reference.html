<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-135" href="../nips2011/nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">nips2011-135</a> <a title="nips-2011-135-reference" href="#">nips2011-135-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</h1>
<br/><p>Source: <a title="nips-2011-135-pdf" href="http://papers.nips.cc/paper/4371-information-rates-and-optimal-decoding-in-large-neural-populations.pdf">pdf</a></p><p>Author: Kamiar R. Rad, Liam Paninski</p><p>Abstract: Many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of Shannon information rates in populations of spiking neurons. In this paper, we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities. We ﬁnd that for large neural populations carrying a ﬁnite total amount of information, the full spiking population response is asymptotically as informative as a single observation from a Gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties. The Gaussian form of this asymptotic sufﬁcient statistic allows us in certain cases to perform optimal Bayesian decoding by simple linear transformations, and to obtain closed-form expressions of the Shannon information carried by the network. One technical advantage of the theory is that it may be applied easily even to non-Poisson point process network models; for example, we ﬁnd that under some conditions, neural populations with strong history-dependent (non-Poisson) effects carry exactly the same information as do simpler equivalent populations of non-interacting Poisson neurons with matched ﬁring rates. We argue that our ﬁndings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design.</p><br/>
<h2>reference text</h2><p>[1] J. Atick. Could information theory provide an ecological theory of sensory processing? Network: Computation in Neural Systems, pages 213–251, May 1992.</p>
<p>[2] F. Attneave. Some informational aspects of visual perception. Psychological Review, 1954.</p>
<p>[3] H. B. Barlow. Possible principles underlying the transformation of sensory messages. Sensory Communication, pages 217–234, 1961.</p>
<p>[4] P. Berens, A. S. Ecker, S. Gerwinn, A. S. Tolias, and M. Bethge. Reassessing optimal neural population codes with neurometric functions. Proceedings of the National Academy of Sciences, 108:4423–4428, 2011.</p>
<p>[5] W. Bialek and A. Zee. Coding and computation with neural spike trains. Journal of Statistical Physics, 59:103–115, 1990.</p>
<p>[6] E. Brown, L. Frank, D. Tang, M. Quirk, and M. Wilson. A statistical paradigm for neural spike train decoding applied to position prediction from ensemble ﬁring patterns of rat hippocampal place cells. Journal of Neuroscience, 18:7411–7425, 1998.</p>
<p>[7] N. Brunel and J.-P. Nadal. Mutual information, ﬁsher information, and population coding. Neural Comput., 10(7):1731–1757, 1998.</p>
<p>[8] B. Clarke and A. Barron. Information-theoretic asymptotics of Bayes methods. IEEE Transactions on Information Theory, 36:453 – 471, 1990.</p>
<p>[9] T. Cover and J. Thomas. Elements of information theory. Wiley, New York, 1991.</p>
<p>[10] J. Durbin and S. Koopman. Time Series Analysis by State Space Methods. Oxford University Press, 2001.</p>
<p>[11] I. Ginzburg and H. Sompolinsky. Theory of correlations in stochastic neural networks. Phys Rev E, 50(4):3171–3191, 1994.</p>
<p>[12] V. Lawhern, W. Wu, N. Hastopoulos, and L. Paninski. Population decoding of motor cortical activity using a generalized linear model with hidden states. Journal of Neuroscience Methods, 2011.</p>
<p>[13] J. Macke, L. Sing, B. Cunningham, J.P. snd Yu, K. Shenoy, and M. Sahani. Modelling lowdimensional dynamics in recorded spiking populations. COSYNE, 2011.</p>
<p>[14] L. Paninski, Y. Ahmadian, D. Ferreira, S. Koyama, K. Rahnama Rad, M. Vidne, J. Vogelstein, and W. Wu. A new look at state-space models for neural data. Journal of Computational Neuroscience, 29(1):107–126, 2010.</p>
<p>[15] S. Panzeri, S. Schultz, A. Treves, and E. Rolls. Correlations and the encoding of information in the nervous system. Proceedings of the Royal Society London B, 266(1423):1001–1012, 1999.</p>
<p>[16] J. Pillow, Y. Ahmadian, and L. Paninski. Model-based decoding, information estimation, and change-point detection in multi-neuron spike trains. Neural Computation, 23(1):1–45, January 2011.</p>
<p>[17] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation, 11:305–345, 1999.</p>
<p>[18] E. Salinas and L. Abbott. Vector reconstruction from ﬁring rates. Journal of Computational Neuroscience, 1:89–107, 1994.</p>
<p>[19] H. S. Seung and H. Sompolinsky. Simple models for reading neuronal population codes. Proceedings of the National Academy of Sciences, 90:10749–10753, 1993.</p>
<p>[20] H. Snippe. Parameter extraction from population codes: A critical assesment. Neural Computation, 8:511–529, 1996.</p>
<p>[21] D. Snyder and M. Miller. Random Point Processes in Time and Space. Springer-Verlag, 1991.</p>
<p>[22] S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature, 381:520–522, 1996.</p>
<p>[23] T. Toyoizumi, K. Rahnama Rad, and L. Paninski. Mean-ﬁeld approximations for coupled populations of generalized linear model spiking neurons with Markov refractoriness. Neural Computation, 21:1203–1243, 2009.</p>
<p>[24] A. van der Vaart. Asymptotic statistics. Cambridge University Press, Cambridge, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
