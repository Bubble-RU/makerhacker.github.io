<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2011-A Convergence Analysis of Log-Linear Training</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-4" href="../nips2011/nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">nips2011-4</a> <a title="nips-2011-4-reference" href="#">nips2011-4-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>4 nips-2011-A Convergence Analysis of Log-Linear Training</h1>
<br/><p>Source: <a title="nips-2011-4-pdf" href="http://papers.nips.cc/paper/4421-a-convergence-analysis-of-log-linear-training.pdf">pdf</a></p><p>Author: Simon Wiesler, Hermann Ney</p><p>Abstract: Log-linear models are widely used probability models for statistical pattern recognition. Typically, log-linear models are trained according to a convex criterion. In recent years, the interest in log-linear models has greatly increased. The optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers. In this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. We verify our ﬁndings on two handwriting tasks. By making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach. 1</p><br/>
<h2>reference text</h2><p>[1] Bertolami, R., Bunke, H.: HMM-based Ensamble Methods for Ofﬂine Handwritten Text Line Recognition. Pattern Recogn. 41, 3452–3460 (2008)</p>
<p>[2] Bottou, L., Bousquet, O.: The tradeoffs of large scale learning. In: Advances in Neural Information Processing Systems. pp. 161–168 (2008)</p>
<p>[3] Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press (2004)</p>
<p>[4] Darroch, J., Ratcliff, D.: Generalized Iterative Scaling for Log-Linear Models. Ann. Math. Stat. 43(5), 1470–1480 (1972)</p>
<p>[5] Dreuw, P., Heigold, G., Ney, H.: Conﬁdence- and Margin-Based MMI/MPE Discriminative Training for Off-Line Handwriting Recognition. Int. J. Doc. Anal. Recogn. pp. 1–16 (2011)</p>
<p>[6] Espa˜ a-Boquera, S., Castro-Bleda, M., Gorbe-Moya, J., Zamora-Martinez, F.: Improving Ofn ﬂine Handwritten Text Recognition with Hybrid HMM/ANN Models. IEEE Trans. Pattern Anal. Mach. Intell. 33(4), 767 –779 (april 2011)</p>
<p>[7] Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., Schmidhuber, J.: A Novel Connectionist System for Unconstrained Handwriting Recognition. IEEE Trans. Pattern Anal. Mach. Intell. 31(5), 855–868 (May 2009)</p>
<p>[8] Horn, R., Johnson, C.: Topics in Matrix Analysis. Cambridge University Press (1994)</p>
<p>[9] Horn, R., Johnson, C.: Matrix Analysis. Cambridge University Press (2005)</p>
<p>[10] Lafferty, J., McCallum, A., Pereira, F.: Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In: Proceedings of the 18th International Conference on Machine Learning. pp. 282–289 (2001)</p>
<p>[11] LeCun, Y., Kanter, I., Solla, S.: Second order properties of error surfaces: Learning time and generalization. In: Advances in Neural Information Processing Systems. pp. 918–924. Morgan Kaufmann Publishers Inc. (1990)</p>
<p>[12] Liu, D., Nocedal, J.: On the Limited Memory BFGS Method for Large-Scale Optimization. Math. Program. 45(1), 503–528 (1989)</p>
<p>[13] Luenberger, D., Ye, Y.: Linear and Nonlinear Programming. Springer Verlag (2008)</p>
<p>[14] Malouf, R.: A comparison of algorithms for maximum entropy parameter estimation. In: Proceedings of the Sixth Conference on Natural Language Learning. pp. 49–55 (2002)</p>
<p>[15] Marti, U., Bunke, H.: The IAM-Database: An English Sentence Database for Ofﬂine Handwriting Recognition. Int. J. Doc. Anal. Recogn. 5(1), 39–46 (2002)</p>
<p>[16] McCallum, A., Freitag, D., Pereira, F.: Maximum entropy markov models for information extraction and segmentation. In: Proceedings of the 17th International Conference on Machine Learning. pp. 591–598 (2000)</p>
<p>[17] Minka, T.: Algorithms for maximum-likelihood logistic regression. Tech. rep., Carnegie Mellon University (2001)</p>
<p>[18] Nocedal, J., Wright, S.: Numerical Optimization. Springer (1999)</p>
<p>[19] Notay, Y.: Solving positive (semi)deﬁnite linear systems by preconditioned iterative methods. In: Preconditioned Conjugate Gradient Methods, Lecture Notes in Mathematics, vol. 1457, pp. 105–125. Springer (1990)</p>
<p>[20] Salakhutdinov, R., Roweis, S., Ghahramani, Z.: On the convergence of bound optimization algorithms. In: Uncertainty in Artiﬁcial Intelligence. vol. 19, pp. 509–516 (2003)</p>
<p>[21] Sha, F., Pereira, F.: Shallow parsing with conditional random ﬁelds. In: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 134–141 (2003)</p>
<p>[22] Sutton, C., McCallum, A.: An introduction to conditional random ﬁelds for relational learning. In: Getoor, L., Taskar, B. (eds.) Introduction to Statistical Relational Learning. MIT Press (2007)  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
