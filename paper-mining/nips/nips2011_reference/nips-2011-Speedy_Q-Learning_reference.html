<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>268 nips-2011-Speedy Q-Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-268" href="../nips2011/nips-2011-Speedy_Q-Learning.html">nips2011-268</a> <a title="nips-2011-268-reference" href="#">nips2011-268-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>268 nips-2011-Speedy Q-Learning</h1>
<br/><p>Source: <a title="nips-2011-268-pdf" href="http://papers.nips.cc/paper/4251-speedy-q-learning.pdf">pdf</a></p><p>Author: Mohammad Ghavamzadeh, Hilbert J. Kappen, Mohammad G. Azar, Rémi Munos</p><p>Abstract: We introduce a new convergent variant of Q-learning, called speedy Q-learning (SQL), to address the problem of slow convergence in the standard form of the Q-learning algorithm. We prove a PAC bound on the performance of SQL, which shows that for an MDP with n state-action pairs and the discount factor γ only T = O log(n)/(ǫ2 (1 − γ)4 ) steps are required for the SQL algorithm to converge to an ǫ-optimal action-value function with high probability. This bound has a better dependency on 1/ǫ and 1/(1 − γ), and thus, is tighter than the best available result for Q-learning. Our bound is also superior to the existing results for both modelfree and model-based instances of batch Q-value iteration that are considered to be more efﬁcient than the incremental methods like Q-learning. 1</p><br/>
<h2>reference text</h2><p>[1] A. Antos, R. Munos, and Cs. Szepesv´ ri. Fitted Q-iteration in continuous action-space MDPs. a In Proceedings of the 21st Annual Conference on Neural Information Processing Systems, 2007.</p>
<p>[2] M. Gheshlaghi Azar, R. Munos, M. Ghavamzadeh, and H.J. Kappen. Reinforcement learning with a near optimal rate of convergence. Technical Report inria-00636615, INRIA, 2011.</p>
<p>[3] P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence, 2009.</p>
<p>[4] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume II. Athena Scientiﬁc, Belmount, Massachusetts, third edition, 2007.</p>
<p>[5] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, Belmont, Massachusetts, 1996.</p>
<p>[6] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.</p>
<p>[7] E. Even-Dar, S. Mannor, and Y. Mansour. PAC bounds for multi-armed bandit and Markov decision processes. In 15th Annual Conference on Computational Learning Theory, pages 255–270, 2002.</p>
<p>[8] E. Even-Dar and Y. Mansour. Learning rates for Q-learning. Journal of Machine Learning Research, 5:1–25, 2003.</p>
<p>[9] W. Feller. An Introduction to Probability Theory and Its Applications, volume 1. Wiley, 1968.</p>
<p>[10] T. Jaakkola, M. I. Jordan, and S. Singh. On the convergence of stochastic iterative dynamic programming. Neural Computation, 6(6):1185–1201, 1994.</p>
<p>[11] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563–1600, 2010.</p>
<p>[12] M. Kearns and S. Singh. Finite-sample convergence rates for Q-learning and indirect algorithms. In Advances in Neural Information Processing Systems 12, pages 996–1002. MIT Press, 1999.</p>
<p>[13] R. Munos and Cs. Szepesv´ ri. Finite-time bounds for ﬁtted value iteration. Journal of Machine a Learning Research, 9:815–857, 2008.</p>
<p>[14] J. Peng and R. J. Williams. Incremental multi-step Q-learning. Machine Learning, 22(13):283–290, 1996.</p>
<p>[15] A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning in ﬁnite MDPs: PAC analysis. Journal of Machine Learning Research, 10:2413–2444, 2009.</p>
<p>[16] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts, 1998.</p>
<p>[17] Cs. Szepesv´ ri. The asymptotic convergence-rate of Q-learning. In Advances in Neural Infora mation Processing Systems 10, Denver, Colorado, USA, 1997, 1997.</p>
<p>[18] I. Szita and Cs. Szepesv´ ri. Model-based reinforcement learning with nearly tight exploration a complexity bounds. In Proceedings of the 27th International Conference on Machine Learning, pages 1031–1038. Omnipress, 2010.</p>
<p>[19] H. van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems 23, pages 2613–2621, 2010.</p>
<p>[20] C. Watkins. Learning from Delayed Rewards. PhD thesis, Kings College, Cambridge, England, 1989.  9</p>
<br/>
<br/><br/><br/></body>
</html>
