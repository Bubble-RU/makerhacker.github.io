<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-16" href="../nips2011/nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">nips2011-16</a> <a title="nips-2011-16-reference" href="#">nips2011-16-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</h1>
<br/><p>Source: <a title="nips-2011-16-pdf" href="http://papers.nips.cc/paper/4274-a-reinterpretation-of-the-policy-oscillation-phenomenon-in-approximate-policy-iteration.pdf">pdf</a></p><p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><br/>
<h2>reference text</h2><p>[1] C. Szepesv´ ri. Algorithms for reinforcement learning. Morgan & Claypool Publishers, 2010. a</p>
<p>[2] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 2005.</p>
<p>[3] L. Busoniu, R. Babuˇka, B. De Schutter, and D. Ernst. Reinforcement learning and dynamic programming ¸ s using function approximators. CRC Press, 2010.</p>
<p>[4] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.</p>
<p>[5] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms. Automatica, 45(11):2471–2482, 2009.</p>
<p>[6] V. R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143–1166, 2004.</p>
<p>[7] D. P. Bertsekas. Approximate policy iteration: A survey and some new methods. Technical report, Massachusetts Institute of Technology, Cambridge, US, 2010.</p>
<p>[8] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.</p>
<p>[9] R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 2000.</p>
<p>[10] C. Thiery and B. Scherrer. Building Controllers for Tetris. ICGA Journal, 32(1):3–11, 2009.</p>
<p>[11] I. Szita and A. L¨ rincz. Learning Tetris using the noisy cross-entropy method. Neural Computation, o 18(12):2936–2941, 2006.</p>
<p>[12] D. P. Bertsekas and S. Ioffe. Temporal differences-based policy iteration and applications in neurodynamic programming. Technical report, Massachusetts Institute of Technology, Cambridge, US, 1996.</p>
<p>[13] S. M. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, 2002.</p>
<p>[14] M. Petrik and B. Scherrer. Biasing approximate dynamic programming with a lower discount factor. In Advances in Neural Information Processing Systems, 2008.</p>
<p>[15] C. Thiery and B. Scherrer. Improvements on learning Tetris with cross entropy. ICGA Journal, 32(1):23– 33, 2009.</p>
<p>[16] V. Farias and B. Roy. Tetris: A study of randomized constraint sampling. In Probabilistic and Randomized Methods for Design Under Uncertainty, pages 189–201. Springer, 2006.</p>
<p>[17] V. Desai, V. Farias, and C. Moallemi. A smoothed approximate linear program. In Advances in Neural Information Processing Systems, 2009.</p>
<p>[18] G. J. Gordon. Reinforcement learning with function approximation converges to a region. In Advances in Neural Information Processing Systems, 2001.</p>
<p>[19] S. P. Singh, T. Jaakkola, and M. I. Jordan. Learning without state-estimation in partially observable markovian decision processes. In Proceedings of the Eleventh International Conference on Machine Learning, volume 31, page 37, 1994.</p>
<p>[20] M. D. Pendrith and M. J. McGarity. An analysis of direct reinforcement learning in non-markovian domains. In Proceedings of the Fifteenth International Conference on Machine Learning, 1998.</p>
<p>[21] T. J. Perkins. Action value based reinforcement learning for POMDPs. Technical report, University of Massachusetts, Amherst, MA, USA, 2001.</p>
<p>[22] T. J. Perkins and D. Precup. A convergent form of approximate policy iteration. In Advances in Neural Information Processing Systems, 2003.</p>
<p>[23] P. A. Crook and G. Hayes. Consistent exploration improves convergence of reinforcement learning on POMDPs. In AAMAS 2007 Workshop on Adaptive and Learning Agents, 2007.</p>
<p>[24] T. J. Perkins. Reinforcement learning for POMDPs based on action values and stochastic optimization. In Proceedings of the Eighteenth National Conference on Artiﬁcial Intelligence, pages 199–204. American Association for Artiﬁcial Intelligence, 2002.</p>
<p>[25] G. J. Gordon. Chattering in SARSA(λ). Technical report, Carnegie Mellon University, Pittsburgh, PA, USA, 1996.</p>
<p>[26] R. Parr, L. Li, G. Taylor, C. Painter-Wakeﬁeld, and M. L. Littman. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th International Conference on Machine learning, pages 752–759. ACM, 2008.</p>
<p>[27] D. P. Bertsekas and H. Yu. Q-learning and enhanced policy iteration in discounted dynamic programming. In Decision and Control (CDC), 2010 49th IEEE Conference on, pages 1409–1416. IEEE, 2010.</p>
<p>[28] A. Nedi´ and D. P. Bertsekas. Least squares policy evaluation algorithms with linear function approximac tion. Discrete Event Dynamic Systems: Theory and Applications, 13(1–2):79–110, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
