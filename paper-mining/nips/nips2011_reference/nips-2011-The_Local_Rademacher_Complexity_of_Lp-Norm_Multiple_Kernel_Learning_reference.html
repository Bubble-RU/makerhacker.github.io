<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-286" href="../nips2011/nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">nips2011-286</a> <a title="nips-2011-286-reference" href="#">nips2011-286-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2011-286-pdf" href="http://papers.nips.cc/paper/4259-the-local-rademacher-complexity-of-lp-norm-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proc. 21st ICML. ACM, 2004.</p>
<p>[2] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005.</p>
<p>[3] R. R. Bouckaert, E. Frank, M. A. Hall, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. WEKA– experiences with a java open-source project. Journal of Machine Learning Research, 11:2533–2541, 2010.</p>
<p>[4] C. Campbell and Y. Ying. Learning with Support Vector Machines. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2011.</p>
<p>[5] C. Cortes. Invited talk: Can learning kernels help performance? In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1:1–1:1, New York, NY, USA, 2009. ACM. Video http://videolectures.net/icml09_cortes_clkh/.</p>
<p>[6] C. Cortes, A. Gretton, G. Lanckriet, M. Mohri, and A. Rostamizadeh. Proceedings of the NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, 2008. URL http://videolectures. net/lkasok08_whistler/, Video http://www.cs.nyu.edu/learning_kernels.</p>
<p>[7] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence, 2009.</p>
<p>[8] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels. In Proceedings, 27th ICML, 2010.</p>
<p>[9] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.</p>
<p>[10] P. V. Gehler and S. Nowozin. Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel classiﬁers. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 06 2009.</p>
<p>[11] R. Ibragimov and S. Sharakhmetov. The best constant in the rosenthal inequality for nonnegative random variables. Statistics & Probability Letters, 55(4):367 – 376, 2001.</p>
<p>[12] J.-P. Kahane. Some random series of functions. Cambridge University Press, 2nd edition, 1985.</p>
<p>[13] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. M¨ ller, and A. Zien. Efﬁcient and accurate lp-norm u multiple kernel learning. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 997–1005. MIT Press, 2009.</p>
<p>[14] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. Lp-norm multiple kernel learning. Journal of Machine Learning Research, 12:953–997, Mar 2011.</p>
<p>[15] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of Statistics, 34(6):2593–2656, 2006.</p>
<p>[16] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. Annals of Statistics, 38(6):3660–3695, 2010.</p>
<p>[17] S. Kwapi´ n and W. A. Woyczy´ ski. Random Series and Stochastic Integrals: Single and Multiple. e n Birkh¨ user, Basel and Boston, M.A., 1992. a</p>
<p>[18] G. Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. I. Jordan. Learning the kernel matrix with semi-deﬁnite programming. JMLR, 5:27–72, 2004.</p>
<p>[19] S. Mendelson. On the performance of kernel classes. J. Mach. Learn. Res., 4:759–771, December 2003.</p>
<p>[20] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005.</p>
<p>[21] K.-R. M¨ ller, S. Mika, G. R¨ tsch, K. Tsuda, and B. Sch¨ lkopf. An introduction to kernel-based learning u a o algorithms. IEEE Neural Networks, 12(2):181–201, May 2001.</p>
<p>[22] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Journal of Machine Learning Research, 9:2491–2521, 2008.</p>
<p>[23] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. CoRR, abs/1008.3654, 2010.</p>
<p>[24] B. Sch¨ lkopf, A. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue problem. o u Neural Computation, 10:1299–1319, 1998.</p>
<p>[25] J. R. Searle. Minds, brains, and programs. Behavioral and Brain Sciences, 3(03):417–424, 1980.</p>
<p>[26] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. Journal of a a o Machine Learning Research, 7:1531–1565, July 2006.</p>
<p>[27] A. Tsybakov. Optimal rates of aggregation. In B. Sch¨ lkopf and M. Warmuth, editors, Computational o Learning Theory and Kernel Machines (COLT-2003), volume 2777 of Lecture Notes in Artiﬁcial Intelligence, pages 303–313. Springer, 2003.  9</p>
<br/>
<br/><br/><br/></body>
</html>
