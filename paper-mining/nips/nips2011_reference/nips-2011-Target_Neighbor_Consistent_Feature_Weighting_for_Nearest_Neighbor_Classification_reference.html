<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-279" href="../nips2011/nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">nips2011-279</a> <a title="nips-2011-279-reference" href="#">nips2011-279-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="nips-2011-279-pdf" href="http://papers.nips.cc/paper/4373-target-neighbor-consistent-feature-weighting-for-nearest-neighbor-classification.pdf">pdf</a></p><p>Author: Ichiro Takeuchi, Masashi Sugiyama</p><p>Abstract: We consider feature selection and weighting for nearest neighbor classiﬁers. A technical challenge in this scenario is how to cope with discrete update of nearest neighbors when the feature space metric is changed during the learning process. This issue, called the target neighbor change, was not properly addressed in the existing feature weighting and metric learning literature. In this paper, we propose a novel feature weighting algorithm that can exactly and efﬁciently keep track of the correct target neighbors via sequential quadratic programming. To the best of our knowledge, this is the ﬁrst algorithm that guarantees the consistency between target neighbors and the feature space metric. We further show that the proposed algorithm can be naturally combined with regularization path tracking, allowing computationally efﬁcient selection of the regularization parameter. We demonstrate the effectiveness of the proposed algorithm through experiments. 1</p><br/>
<h2>reference text</h2><p>[1] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: Scalable online collaborative ﬁltering. In Proceedings of the 16th International Conference on World Wide Web, pages 271–280. ACM, 2007.</p>
<p>[2] S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison of discrimination methods for the classiﬁcation of tumors using gene expression data. Journal of the American Statistical Association, 97(457):77–87, 2002.</p>
<p>[3] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 505–512. MIT Press, Cambridge MA, 2003.</p>
<p>[4] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520. MIT Press, Cambridge, MA, 2005.</p>
<p>[5] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Inforo mation Processing Systems 18, pages 1473–1480. MIT Press, Cambridge, MA, 2006.</p>
<p>[6] J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. In Proceedings of the 24th International Conference on Machine Learning, pages 209–216, 2007.</p>
<p>[7] K. Kira and L. Rendell. A practical approach to feature selection. In Proceedings of the 9-th International Conference on Machine Learning, pages 249–256, 1992.</p>
<p>[8] I. Kononenko. Estimating attributes: analysis and extensions of relief. In Proceedings of European Conference on Machine Learning, pages 171–182, 1994.</p>
<p>[9] R. Gilad-Bachrach, A. Navot, and N. Tishby. Margin based feature selection - theory and algorithms. In Proceedings of the 21st International Conference on Machine Learning, pages 43–50, 2004.</p>
<p>[10] Y. Sun and J. Li. Iterative relief for feature weighting. In Proceedings of the 23-rd International Conference on Machhine Learning, pages 913–920, 2006.</p>
<p>[11] Y. Sun, S. Todorovic, and S. Goodison. Local learning based feature selection for high dimensional data analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1610–1626, 2010.</p>
<p>[12] K. Wagsta, C. Cardie, S. Rogers, and S. Schroedl. Constrained k-means clustering with background knowledge. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 577–584, 2001.</p>
<p>[13] M. Sugiyama. Dimensionality reduction of multimodal labeled data by local ﬁsher discriminant analysis. Journal of Machine Learning Research, 8:1027–1061, 2007.</p>
<p>[14] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–1415, 2004.</p>
<p>[15] J. Nocedal and S. J. Wright. Numerical optimization. Springer, 1999.</p>
<p>[16] U. Alon, N. Barkia, D.A. Notterman, and K. Gish et al. Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. Proc. Natl. Acad. Sci. USA, 96:6745–6750, 1999.</p>
<p>[17] H. Sueltmann, A. Heydenbreck, W. Huber, and R. Kuner et al. Gene expression in kidney cancer is associated with novel tumor subtypes, cytogenetic abnormalities and metastasis formation. 8:1027–1061, 2007.</p>
<p>[18] T. R. Golub, D. K. Slonim, P. Tamayo, and C. Huard et al. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. Science, 286:531–537, 1999.</p>
<p>[19] D. Singh, P. G. Febbo, K. Ross, and D. G. Jackson et al. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1:203–209, 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
