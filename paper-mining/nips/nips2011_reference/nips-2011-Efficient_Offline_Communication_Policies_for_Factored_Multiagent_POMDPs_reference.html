<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-79" href="../nips2011/nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">nips2011-79</a> <a title="nips-2011-79-reference" href="#">nips2011-79-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</h1>
<br/><p>Source: <a title="nips-2011-79-pdf" href="http://papers.nips.cc/paper/4385-efficient-offline-communication-policies-for-factored-multiagent-pomdps.pdf">pdf</a></p><p>Author: João V. Messias, Matthijs Spaan, Pedro U. Lima</p><p>Abstract: Factored Decentralized Partially Observable Markov Decision Processes (DecPOMDPs) form a powerful framework for multiagent planning under uncertainty, but optimal solutions require a rigid history-based policy representation. In this paper we allow inter-agent communication which turns the problem in a centralized Multiagent POMDP (MPOMDP). We map belief distributions over state factors to an agent’s local actions by exploiting structure in the joint MPOMDP policy. The key point is that when sparse dependencies between the agents’ decisions exist, often the belief over its local state factors is sufﬁcient for an agent to unequivocally identify the optimal action, and communication can be avoided. We formalize these notions by casting the problem into convex optimization form, and present experimental results illustrating the savings in communication that we can obtain.</p><br/>
<h2>reference text</h2><p>[1] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4):819–840, 2002.</p>
<p>[2] Xavier Boyen and Daphne Koller. Tractable inference for complex stochastic processes. In Proc. of Uncertainty in Artiﬁcial Intelligence, 1998.</p>
<p>[3] X.G. Fang and G. Havas. On the worst-case complexity of integer gaussian elimination. In Proceedings of the 1997 international symposium on Symbolic and algebraic computation, pages 28–31. ACM, 1997.</p>
<p>[4] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.</p>
<p>[5] David A. McAllester and Satinder Singh. Approximate planning for factored POMDPs using belief state simpliﬁcation. In Proc. of Uncertainty in Artiﬁcial Intelligence, 1999.</p>
<p>[6] J.V. Messias, M.T.J. Spaan, and P. U. Lima. Supplementary material for “Efﬁcient Ofﬂine Communication Policies for Factored Multiagent POMDPs”. ISR/IST, 2011.</p>
<p>[7] Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos Vlassis. Dec-POMDPs with delayed communication. In Multi-agent Sequential Decision Making in Uncertain Domains, 2007. Workshop at AAMAS07.</p>
<p>[8] Frans A. Oliehoek, Matthijs T. J. Spaan, Shimon Whiteson, and Nikos Vlassis. Exploiting locality of interaction in factored Dec-POMDPs. In Proc. of Int. Conference on Autonomous Agents and Multi Agent Systems, 2008.</p>
<p>[9] P. Poupart and C. Boutilier. Value-directed belief state approximation for POMDPs. In Proc. of Uncertainty in Artiﬁcial Intelligence, volume 130, 2000.</p>
<p>[10] David V. Pynadath and Milind Tambe. The communicative multiagent team decision problem: Analyzing teamwork theories and models. Journal of Artiﬁcial Intelligence Research, 16:389– 423, 2002.</p>
<p>[11] M. Roth, R. Simmons, and M. Veloso. Decentralized communication strategies for coordinated multi-agent policies. In Multi-Robot Systems: From Swarms to Intelligent Automata, volume IV. Kluwer Academic Publishers, 2005.</p>
<p>[12] Maayan Roth, Reid Simmons, and Manuela Veloso. Exploiting factored representations for decentralized execution in multi-agent teams. In Proc. of Int. Conference on Autonomous Agents and Multi Agent Systems, 2007.</p>
<p>[13] Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlassis. Multiagent planning under uncertainty with stochastic communication delays. In Proc. of Int. Conf. on Automated Planning and Scheduling, pages 338–345, 2008.</p>
<p>[14] Chelsea C. White. Partially observed Markov decision processes: a survey. Annals of Operations Research, 32, 1991.</p>
<p>[15] Feng Wu, Shlomo Zilberstein, and Xiaoping Chen. Multi-agent online planning with communication. In Int. Conf. on Automated Planning and Scheduling, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
