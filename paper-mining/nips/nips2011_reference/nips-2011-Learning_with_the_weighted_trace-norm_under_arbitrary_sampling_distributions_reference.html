<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-159" href="../nips2011/nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">nips2011-159</a> <a title="nips-2011-159-reference" href="#">nips2011-159-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</h1>
<br/><p>Source: <a title="nips-2011-159-pdf" href="http://papers.nips.cc/paper/4303-learning-with-the-weighted-trace-norm-under-arbitrary-sampling-distributions.pdf">pdf</a></p><p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><br/>
<h2>reference text</h2><p>[1] M. Fazel. Matrix rank minimization with applications. PhD Thesis, Stanford University, 2002.</p>
<p>[2] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. Advances in Neural Information Processing Systems, 17, 2004.</p>
<p>[3] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in Neural Information Processing Systems, 20, 2007.</p>
<p>[4] F. Bach. Consistency of trace-norm minimization. Journal of Machine Learning Research, 9:1019–1048, 2008.</p>
<p>[5] E. Cand` s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE e Trans. Inform. Theory, 56(5):2053–2080, 2009.</p>
<p>[6] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. 18th Annual Conference on Learning Theory (COLT), pages 545–560, 2005.</p>
<p>[7] B. Recht. A simpler approach to matrix completion. arXiv:0910.0651, 2009.</p>
<p>[8] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:2057–2078, 2010.</p>
<p>[9] V. Koltchinskii, A. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal rates for noisy low rank matrix completion. arXiv:1011.6256, 2010.</p>
<p>[10] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. arXiv:1009.2118, 2010.</p>
<p>[11] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. 24th Annual Conference on Learning Theory (COLT), 2011.</p>
<p>[12] R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm. Advances in Neural Information Processing Systems, 23, 2010.</p>
<p>[13] O. Shamir and S. Shalev-Shwartz. Collaborative ﬁltering with the trace norm: Learning, bounding, and transducing. 24th Annual Conference on Learning Theory (COLT), 2011.</p>
<p>[14] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.</p>
<p>[15] J.A. Tropp. User-friendly tail bounds for sums of random matrices. arXiv:1004.4389, 2010.</p>
<p>[16] J. Bennett and S. Lanning. The netﬂix prize. In Proceedings of KDD Cup and Workshop, volume 2007, page 35. Citeseer, 2007.</p>
<p>[17] MovieLens Dataset. Available at http://www.grouplens.org/node/73. 2006.</p>
<p>[18] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. ACM Int. Conference on Knowledge Discovery and Data Mining (KDD’08), pages 426–434, 2008.</p>
<p>[19] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical Large-Scale Optimization for Max-Norm Regularization. Advances in Neural Information Processing Systems, 23, 2010.</p>
<p>[20] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287–2322, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
