<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-190" href="../nips2011/nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">nips2011-190</a> <a title="nips-2011-190-reference" href="#">nips2011-190-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2011-190-pdf" href="http://papers.nips.cc/paper/4420-nonlinear-inverse-reinforcement-learning-with-gaussian-processes.pdf">pdf</a></p><p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML ’04: Proceedings of the 21st International Conference on Machine Learning, 2004.</p>
<p>[2] M. P. Deisenroth, C. E. Rasmussen, and J. Peters. Gaussian process dynamic programming. Neurocomputing, 72(7–9):1508–1524, 2009.</p>
<p>[3] K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable MDPs. In ICML ’10: Proceedings of the 27th International Conference on Machine Learning, pages 335–342, 2010.</p>
<p>[4] Y. Engel, S. Mannor, and R. Meir. Reinforcement learning with Gaussian processes. In ICML ’05: Proceedings of the 22nd International Conference on Machine learning, pages 201–208, 2005.</p>
<p>[5] S. Levine, Z. Popovi´ , and V. Koltun. Feature construction for inverse reinforcement learning. c In Advances in Neural Information Processing Systems 23. 2010.</p>
<p>[6] G. Neu and C. Szepesv´ ri. Apprenticeship learning using inverse reinforcement learning and a gradient methods. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.</p>
<p>[7] A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In ICML ’00: Proceedings of the 17th International Conference on Machine Learning, pages 663–670, 2000.</p>
<p>[8] J. Qui˜ onero Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian n process regression. Journal of Machine Learning Research, 6:1939–1959, 2005.</p>
<p>[9] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In IJCAI’07: Proceedings of the 20th International Joint Conference on Artiﬁcal Intelligence, pages 2586–2591, 2007.</p>
<p>[10] C. E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In Advances in Neural Information Processing Systems 16, 2003.</p>
<p>[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005.</p>
<p>[12] N. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In ICML ’06: Proceedings of the 23rd International Conference on Machine Learning, pages 729–736, 2006.</p>
<p>[13] N. Ratliff, D. Bradley, J. A. Bagnell, and J. Chestnutt. Boosting structured prediction for imitation learning. In Advances in Neural Information Processing Systems 19, 2007.</p>
<p>[14] N. Ratliff, D. Silver, and J. A. Bagnell. Learning to search: Functional gradient techniques for imitation learning. Autonomous Robots, 27(1):25–53, 2009.</p>
<p>[15] U. Syed and R. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems 20, 2008.</p>
<p>[16] B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Carnegie Mellon University, 2010.</p>
<p>[17] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence (AAAI 2008), pages 1433–1438, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
