<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2011-Multiclass Boosting: Theory and Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-178" href="../nips2011/nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">nips2011-178</a> <a title="nips-2011-178-reference" href="#">nips2011-178-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>178 nips-2011-Multiclass Boosting: Theory and Algorithms</h1>
<br/><p>Source: <a title="nips-2011-178-pdf" href="http://papers.nips.cc/paper/4450-multiclass-boosting-theory-and-algorithms.pdf">pdf</a></p><p>Author: Mohammad J. Saberian, Nuno Vasconcelos</p><p>Abstract: The problem of multi-class boosting is considered. A new framework, based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
