<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 nips-2011-Multi-armed bandits on implicit metric spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-177" href="../nips2011/nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">nips2011-177</a> <a title="nips-2011-177-reference" href="#">nips2011-177-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>177 nips-2011-Multi-armed bandits on implicit metric spaces</h1>
<br/><p>Source: <a title="nips-2011-177-pdf" href="http://papers.nips.cc/paper/4332-multi-armed-bandits-on-implicit-metric-spaces.pdf">pdf</a></p><p>Author: Aleksandrs Slivkins</p><p>Abstract: The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting, an online algorithm has a ﬁxed set of alternatives (“arms”), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (inﬁnitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs. In this paper we consider the more realistic scenario in which the metric space is implicit – it is deﬁned by the available structure but not revealed to the algorithm directly. Speciﬁcally, we assume that an algorithm is given a tree-based classiﬁcation of arms. For any given problem instance such a classiﬁcation implicitly deﬁnes a similarity metric space, but the numerical similarity information is not available to the algorithm. We provide an algorithm for this setting, whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the Lipschitz MAB problem. 1</p><br/>
<h2>reference text</h2><p>[1] Rajeev Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33(6):1926– 1951, 1995.</p>
<p>[2] Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. o Machine Learning, 47(2-3):235–256, 2002. Preliminary version in 15th ICML, 1998.</p>
<p>[3] Peter Auer, Nicol` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed o bandit problem. SIAM J. Comput., 32(1):48–77, 2002. Preliminary version in 36th IEEE FOCS, 1995.</p>
<p>[4] Peter Auer, Ronald Ortner, and Csaba Szepesv´ ri. Improved Rates for the Stochastic Continuum-Armed a Bandit Problem. In 20th COLT, pages 454–468, 2007.</p>
<p>[5] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. of Computer and System Sciences, 74(1):97–114, February 2008. Preliminary version in 36th ACM STOC, 2004.</p>
<p>[6] Andrei Broder, Marcus Fontoura, Vanja Josifovski, and Lance Riedel. A semantic approach to contextual advertising. In 30th SIGIR, pages 559–566, 2007.</p>
<p>[7] S´ bastien Bubeck, R´ mi Munos, Gilles Stoltz, and Csaba Szepesvari. Online Optimization in X-Armed e e Bandits. J. of Machine Learning Research (JMLR), 12:1587–1627, 2011. Preliminary version in NIPS 2008.</p>
<p>[8] Nicol` Cesa-Bianchi and G´ bor Lugosi. Prediction, learning, and games. Cambridge Univ. Press, 2006. o a</p>
<p>[9] Eric Cope. Regret and convergence bounds for immediate-reward reinforcement learning with continuous action spaces. IEEE Trans. on Automatic Control, 54(6):1243–1253, 2009. A manuscript from 2004.</p>
<p>[10] Varsha Dani and Thomas P. Hayes. Robbing the bandit: less regret in online geometric optimization against an adaptive adversary. In 17th ACM-SIAM SODA, pages 937–943, 2006.</p>
<p>[11] Varsha Dani, Thomas P. Hayes, and Sham Kakade. The Price of Bandit Information for Online Optimization. In 20th NIPS, 2007.</p>
<p>[12] Abraham Flaxman, Adam Kalai, and H. Brendan McMahan. Online Convex Optimization in the Bandit Setting: Gradient Descent without a Gradient. In 16th ACM-SIAM SODA, pages 385–394, 2005.</p>
<p>[13] Sylvain Gelly and David Silver. Combining online and ofﬂine knowledge in UCT. In 24th ICML, 2007.</p>
<p>[14] Sylvain Gelly and David Silver. Achieving master level play in 9x9 computer go. In 23rd AAAI, 2008.</p>
<p>[15] Anupam Gupta, Robert Krauthgamer, and James R. Lee. Bounded geometries, fractals, and low– distortion embeddings. In 44th IEEE FOCS, pages 534–543, 2003.</p>
<p>[16] Sham M. Kakade, Adam T. Kalai, and Katrina Ligett. Playing Games with Approximation Algorithms. In 39th ACM STOC, 2007.</p>
<p>[17] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In 18th NIPS, 2004.</p>
<p>[18] Robert Kleinberg. Online Decision Problems with Large Strategy Sets. PhD thesis, MIT, 2005.</p>
<p>[19] Robert Kleinberg and Aleksandrs Slivkins. Sharp Dichotomies for Regret Minimization in Metric Spaces. In 21st ACM-SIAM SODA, 2010.</p>
<p>[20] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-Armed Bandits in Metric Spaces. In 40th ACM STOC, pages 681–690, 2008.</p>
<p>[21] Levente Kocsis and Csaba Szepesvari. Bandit Based Monte-Carlo Planning. In 17th ECML, pages 282– 293, 2006.</p>
<p>[22] T.L. Lai and Herbert Robbins. Asymptotically efﬁcient Adaptive Allocation Rules. Advances in Applied Mathematics, 6:4–22, 1985.</p>
<p>[23] H. Brendan McMahan and Avrim Blum. Online Geometric Optimization in the Bandit Setting Against an Adaptive Adversary. In 17th COLT, pages 109–123, 2004.</p>
<p>[24] R´ mi Munos and Pierre-Arnaud Coquelin. Bandit algorithms for tree search. In 23rd UAI, 2007. e</p>
<p>[25] Sandeep Pandey, Deepak Agarwal, Deepayan Chakrabarti, and Vanja Josifovski. Bandits for Taxonomies: A Model-based Approach. In SDM, 2007.</p>
<p>[26] Sandeep Pandey, Deepayan Chakrabarti, and Deepak Agarwal. Multi-armed Bandit Problems with Dependent Arms. In 24th ICML, 2007.</p>
<p>[27] Susan T. Dumais Paul N. Bennett, Krysta Marie Svore. Classiﬁcation-enhanced ranking. In 19th WWW, pages 111–120, 2010.</p>
<p>[28] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi-armed bandits. In 25th ICML, pages 784–791, 2008.</p>
<p>[29] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. Learning optimally diverse rankings over large document collections. In 27th ICML, pages 983–990, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
