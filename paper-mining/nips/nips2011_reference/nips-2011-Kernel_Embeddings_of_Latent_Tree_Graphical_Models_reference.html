<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-140" href="../nips2011/nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">nips2011-140</a> <a title="nips-2011-140-reference" href="#">nips2011-140-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</h1>
<br/><p>Source: <a title="nips-2011-140-pdf" href="http://papers.nips.cc/paper/4394-kernel-embeddings-of-latent-tree-graphical-models.pdf">pdf</a></p><p>Author: Le Song, Eric P. Xing, Ankur P. Parikh</p><p>Abstract: Latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision, bioinformatics and natural language processing problems. However, existing models are largely restricted to discrete and Gaussian variables due to computational constraints; furthermore, algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search. We present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-Gaussian variables. Our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efﬁcient inference. Experiments on simulated and real data show the advantage of our proposed approach. 1</p><br/>
<h2>reference text</h2><p>[1] A. Asuncion and D.J. Newman. Uci machine learning repository, 2007.</p>
<p>[2] D. Bickson. Gaussian belief propagation: Theory and application. Arxiv preprint arXiv:0811.2518, 2008.</p>
<p>[3] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. In NIPS, 2002.</p>
<p>[4] M.J. Choi, V.Y.F. Tan, A. Anandkumar, and A.S. Willsky. Learning latent tree graphical models. Arxiv preprint arXiv:1009.2722, 2010.</p>
<p>[5] A. Clark. Inference of haplotypes from pcr-ampliﬁed samples of diploid populations. Molecular Biology and Evolution, 7(2):111–122, 1990.</p>
<p>[6] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society B, 39(1):1–22, 1977.</p>
<p>[7] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. J. Mach. Learn. Res., 5:73–99, 2004.</p>
<p>[8] S. Harmeling and C.K.I. Williams. Greedy learning of binary latent trees. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010.</p>
<p>[9] K.A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of the 22nd international conference on Machine learning, pages 297–304. ACM, 2005.</p>
<p>[10] Peter D. Hoff, Adrian E. Raftery, and Mark S. Handcock. Latent space approaches to social network analysis. JASA, 97(460):1090–1098, 2002.</p>
<p>[11] D. Hsu, S. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models. In COLT, 2009.</p>
<p>[12] A. Ihler and D. McAllester. Particle belief propagation. In AISTATS, pages 256–263, 2009.</p>
<p>[13] Tamara. Kolda and Brett Bader. Tensor decompositions and applications. SIAM Review, 51(3):455–500, 2009.</p>
<p>[14] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. The Journal of Machine Learning Research, 10:2295–2328, 2009.</p>
<p>[15] T. Minka. Expectation Propagation for approximative Bayesian inference. PhD thesis, MIT Media Labs, Cambridge, USA, 2001.</p>
<p>[16] A. Parikh, L. Song, and E. Xing. A spectral algorithm for latent tree graphical models. In ICML, 2011.</p>
<p>[17] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2001.</p>
<p>[18] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE ASSP Magazine, 3(1):4–16, January 1986.</p>
<p>[19] M. Redmond and A. Baveja. A data-driven software tool for enabling cooperative information sharing among police departments. European Journal of Operational Research, 141(3):660–678, 2002.</p>
<p>[20] N. Saitou, M. Nei, et al. The neighbor-joining method: a new method for reconstructing phylogenetic trees. Mol Biol Evol, 4(4):406–425, 1987.</p>
<p>[21] C. Semple and M.A. Steel. Phylogenetics, volume 24. Oxford University Press, USA, 2003.</p>
<p>[22] B. W. Silverman. Density Estimation for Statistical and Data Analysis. Monographs on statistics and applied probability. Chapman and Hall, London, 1986.</p>
<p>[23] A.J. Smola, A. Gretton, L. Song, and B. Sch¨ lkopf. A hilbert space embedding for distributions. In E. o Takimoto, editor, Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer, 2007.</p>
<p>[24] L. Song, A. Gretton, and C. Guestrin. Nonparametric tree graphical models. In 13th Workshop on Artiﬁcial Intelligence and Statistics, volume 9 of JMLR workshop and conference proceedings, pages 765–772, 2010.</p>
<p>[25] Le Song, Byron Boots, Sajid Siddiqi, Geoffrey Gordon, and Alex Smola. Hilbert space embeddings of hidden markov models. In International Conference on Machine Learning, 2010.</p>
<p>[26] Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu. Hilbert space embeddings of conditional distributions. In ICML, 2009.</p>
<p>[27] E. Sudderth, A. Ihler, W. Freeman, and A. Willsky. Nonparametric belief propagation. In CVPR, 2003.</p>
<p>[28] N.L. Zhang. Hierarchical latent class models for cluster analysis. The Journal of Machine Learning Research, 5:697–723, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
