<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-278" href="../nips2011/nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">nips2011-278</a> <a title="nips-2011-278-reference" href="#">nips2011-278-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</h1>
<br/><p>Source: <a title="nips-2011-278-pdf" href="http://papers.nips.cc/paper/4472-td_gamma-re-evaluating-complex-backups-in-temporal-difference-learning.pdf">pdf</a></p><p>Author: George Konidaris, Scott Niekum, Philip S. Thomas</p><p>Abstract: We show that the λ-return target used in the TD(λ) family of algorithms is the maximum likelihood estimator for a speciﬁc model of how the variance of an nstep return estimate increases with n. We introduce the γ-return estimator, an alternative target based on a more accurate model of variance, which deﬁnes the TDγ family of complex-backup temporal difference learning algorithms. We derive TDγ , the γ-return equivalent of the original TD(λ) algorithm, which eliminates the λ parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm, TDγ (C), with a capacity parameter C. TDγ (C) requires C times more time and memory than TD(λ) and is incremental and online. We show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains, and that TDγ (C) performs as well as or better than TDγ for intermediate settings of C. 1</p><br/>
<h2>reference text</h2><p>[1] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[2] R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9–44, 1988.</p>
<p>[3] S. Singh and R.S. Sutton. Reinforcement learning with replacing eligibility traces. Machine Learning, 22:123–158, 1996.</p>
<p>[4] J.A. Boyan. Least squares temporal difference learning. In Proceedings of the 16th International Conference on Machine Learning, pages 49–56, 1999.</p>
<p>[5] H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artiﬁcial General Intelligence, 2010.</p>
<p>[6] C. Downey and S. Sanner. Temporal difference Bayesian model averaging: A Bayesian perspective on adapting lambda. In Proceedings of the 27th International Conference on Machine Learning, pages 311–318, 2010.</p>
<p>[7] K. Doya. Reinforcement learning in continuous time and space. 12(1):219–245, 2000.  Neural Computation,</p>
<p>[8] P. Cichosz. Truncating temporal differences: On the efﬁcient implementation of TD(λ) for reinforcement learning. Journal of Artiﬁcial Intelligence Research, 2:287–318, 1995.</p>
<p>[9] G.D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial Intelligence, pages 380–385, 2011.</p>
<p>[10] R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvari, and E. Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th International Conference on Machine Learning, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
