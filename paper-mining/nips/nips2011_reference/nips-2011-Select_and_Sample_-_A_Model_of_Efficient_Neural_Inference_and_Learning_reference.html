<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-243" href="../nips2011/nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">nips2011-243</a> <a title="nips-2011-243-reference" href="#">nips2011-243-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</h1>
<br/><p>Source: <a title="nips-2011-243-pdf" href="http://papers.nips.cc/paper/4346-select-and-sample-a-model-of-efficient-neural-inference-and-learning.pdf">pdf</a></p><p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><br/>
<h2>reference text</h2><p>[1] P. Dayan and L. F. Abbott. Theoretical Neuroscience. MIT Press, Cambridge, 2001.</p>
<p>[2] R. P. N. Rao, B. A. Olshausen, and M. S. Lewicki. Probabilistic Models of the Brain: Perception and Neural Function. MIT Press, 2002.</p>
<p>[3] J. Fiser, P. Berkes, G. Orban, and M. Lengye. Statistically optimal perception and learning: from behavior to neural representations. Trends in Cognitive Sciences, 14:119–130, 2010.</p>
<p>[4] M. D. Ernst and M. S. Banks. Humans integrate visual and haptic information in a statistically optimal fashion. Nature, 415:419–433, 2002.</p>
<p>[5] Y. Weiss, E. P. Simoncelli, and E. H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience, 5:598–604, 2002.</p>
<p>[6] K. P. Kording and D. M. Wolpert. Bayesian integration in sensorimotor learning. Nature, 427:244–247, 2004.</p>
<p>[7] J. M. Beck, W. J. Ma, R. Kiani, T. Hanksand A. K. Churchland, J. Roitman, M. N.. Shadlen, P. E. Latham, and A. Pouget. Probabilistic population codes for bayesian decision making. Neuron, 60(6), 2008.</p>
<p>[8] J. Trommersh¨ user, L. T. Maloney, and M. S. Landy. Decision making, movement planning and statistical a decision theory. Trends in Cognitive Science, 12:291–297, 2008.</p>
<p>[9] P. Berkes, G. Orban, M. Lengyel, and J. Fiser. Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331(6013):83–87, 2011.</p>
<p>[10] W. J. Ma, J. M. Beck, P. E. Latham, and A. Pouget. Bayesian inference with probabilistic population codes. Nature Neuroscience, 9:1432–1438, 2006.</p>
<p>[11] R. Turner, P. Berkes, and J. Fiser. Learning complex tasks with probabilistic population codes. In Frontiers in Neuroscience, 2011. Comp. and Systems Neuroscience 2011.</p>
<p>[12] T. S. Lee and D. Mumford. Hierarchical Bayesian inference in the visual cortex. Journal of the Optical Society of America A, 20(7):1434–1448, 2003.</p>
<p>[13] P. O. Hoyer and A. Hyvarinen. Interpreting neural response variability as Monte Carlo sampling from the posterior. In Adv. Neur. Inf. Proc. Syst. 16, pages 293–300. MIT Press, 2003.</p>
<p>[14] E. Vul, N. D. Goodman, T. L. Grifﬁths, and J. B. Tenenbaum. One and done? Optimal decisions from very few samples. In 31st Annual Meeting of the Cognitive Science Society, 2009.</p>
<p>[15] P. Berkes, R. Turner, and J. Fiser. The army of one (sample): the characteristics of sampling-based probabilistic neural representations. In Frontiers in Neuroscience, 2011. Comp. and Systems Neuroscience 2011.</p>
<p>[16] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 1958.</p>
<p>[17] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience, 211(11):1019 – 1025, 1999.</p>
<p>[18] V. A. F.. Lamme and P. R. Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. Trends in Neurosciences, 23(11):571 – 579, 2000.</p>
<p>[19] A. Yuille and D. Kersten. Vision as bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7):301–308, 2006.</p>
<p>[20] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The ‘wake-sleep’ algorithm for unsupervised neural networks. Science, 268:1158 – 1161, 1995.</p>
<p>[21] E. K¨ rner, M. O. Gewaltig, U. K¨ rner, A. Richter, and T. Rodemann. A model of computation in neocoro o tical architecture. Neural Networks, 12:989 – 1005, 1999.</p>
<p>[22] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996.</p>
<p>[23] H. Lee, A. Battle, R. Raina, and A. Ng. Efﬁcient sparse coding algorithms. NIPS, 20:801–808, 2007.</p>
<p>[24] Y. LeCun. Backpropagation applied to handwritten zip code recognition.</p>
<p>[25] M. Riesenhuber and T. Poggio. How visual cortex recognizes objects: The tale of the standard model. 2002.</p>
<p>[26] T. S. Lee and D. Mumford. Hierarchical bayesian inference in the visual cortex. J Opt Soc Am A Opt Image Sci Vis, 20(7):1434–1448, July 2003.</p>
<p>[27] J. L¨ cke and J. Eggert. Expectation Truncation And the Beneﬁts of Preselection in Training Generative u Models. Journal of Machine Learning Research, 2010.</p>
<p>[28] G. Puertas, J. Bornschein, and J. L¨ cke. The maximal causes of natural scenes are edge ﬁlters. NIPS, 23, u 2010.</p>
<p>[29] M. Henniges, G. Puertas, J. Bornschein, J. Eggert, and J. L¨ cke. Binary sparse coding. Latent Variable u Analysis and Signal Separation, 2010.</p>
<p>[30] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11, 2010.</p>
<p>[31] J. Hateren and A. Schaaf. Independent Component Filters of Natural Images Compared with Simple Cells in Primary Visual Cortex. Proc Biol Sci, 265(1394):359–366, 1998.</p>
<p>[32] D. L. Ringach. Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex. J Neurophysiol, 88:455–463, 2002.</p>
<p>[33] M. Haft, R. Hofman, and V. Tresp. Generative binary codes. Pattern Anal Appl, 6(4):269–284, 2004.</p>
<p>[34] P. O. Hoyer. Non-negative sparse coding. Neural Networks for Signal Processing XII: Proceedings of the IEEE Workshop, pages 557–565, 2002.</p>
<p>[35] J. L¨ cke. Receptive Field Self-Organization in a Model of the Fine Structure in V1 Cortical Columns. u Neural Computation, 2009.</p>
<p>[36] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computational Neuroscience Unit, University College London., 2003.</p>
<p>[37] Z. Tu and S. C. Zhu. Image Segmentation by Data-Driven Markov Chain Monte Carlo. PAMI, 24(5):657– 673, 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
