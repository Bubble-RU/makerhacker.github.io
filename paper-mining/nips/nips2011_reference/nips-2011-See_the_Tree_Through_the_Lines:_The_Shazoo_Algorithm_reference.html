<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>242 nips-2011-See the Tree Through the Lines: The Shazoo Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-242" href="../nips2011/nips-2011-See_the_Tree_Through_the_Lines%3A_The_Shazoo_Algorithm.html">nips2011-242</a> <a title="nips-2011-242-reference" href="#">nips2011-242-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>242 nips-2011-See the Tree Through the Lines: The Shazoo Algorithm</h1>
<br/><p>Source: <a title="nips-2011-242-pdf" href="http://papers.nips.cc/paper/4476-see-the-tree-through-the-lines-the-shazoo-algorithm.pdf">pdf</a></p><p>Author: Fabio Vitale, Nicolò Cesa-bianchi, Claudio Gentile, Giovanni Zappella</p><p>Abstract: Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains. Since graph sparsiﬁcation via spanning trees retains enough information while making the task much easier, trees are an important special case of this problem. Although it is known how to predict the nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm is not available yet. We ﬁll this hole and introduce an efﬁcient node predictor, S HAZOO, which is nearly optimal on any weighted tree. Moreover, we show that S HAZOO can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines. Experiments on real-world datasets conﬁrm that S HAZOO performs well in that it fully exploits the structure of the input tree, and gets very close to (and sometimes better than) less scalable energy minimization methods. 1</p><br/>
<h2>reference text</h2><p>[1] N. Alon, C. Avin, M. Kouck´ , G. Kozma, Z. Lotker, and M.R. Tuttle. Many random walks y are faster than one. In Proc. 20th Symp. on Parallel Algo. and Architectures, pages 119–128. Springer, 2008.</p>
<p>[2] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. In Proceedings of the 17th Annual Conference on Learning Theory, pages 624–638. Springer, 2004.</p>
<p>[3] Y. Bengio, O. Delalleau, and N. Le Roux. Label propagation and quadratic criterion. In SemiSupervised Learning, pages 193–216. MIT Press, 2006.</p>
<p>[4] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the 18th International Conference on Machine Learning. Morgan Kaufmann, 2001.</p>
<p>[5] N. Cesa-Bianchi, C. Gentile, and F. Vitale. Fast and optimal prediction of a labeled tree. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.</p>
<p>[6] N. Cesa-Bianchi, C. Gentile, F. Vitale, and G. Zappella. Random spanning trees and the prediction of weighted graphs. In Proceedings of the 27th International Conference on Machine Learning, 2010.</p>
<p>[7] C. Altaﬁni G. Iacono. Monotonicity, frustration, and ordered response: an analysis of the energy landscape of perturbed large-scale biological networks. BMC Systems Biology, 4(83), 2010.</p>
<p>[8] M. Herbster and G. Lever. Predicting the labelling of a graph via minimum p-seminorm interpolation. In Proceedings of the 22nd Annual Conference on Learning Theory. Omnipress, 2009.</p>
<p>[9] M. Herbster, G. Lever, and M. Pontil. Online prediction on large diameter graphs. In Advances in Neural Information Processing Systems 22. MIT Press, 2009.</p>
<p>[10] M. Herbster, M. Pontil, and S. Rojas-Galeano. Fast prediction on a tree. In Advances in Neural Information Processing Systems 22. MIT Press, 2009.</p>
<p>[11] F.R. Kschischang, B.J. Frey, and H.A. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47(2):498–519, 2001.</p>
<p>[12] R. Lyons and Y. Peres. Probability on trees and networks. Manuscript, 2008.</p>
<p>[13] S.T. McCormick, M.R. Rao, and G. Rinaldi. Easy and difﬁcult objective functions for max cut. Math. Program., 94(2-3):459–466, 2003.</p>
<p>[14] G. Pandey, M. Steinbach, R. Gupta, T. Garg, and V. Kumar. Association analysis-based transformations for protein interaction networks: a function prediction case study. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 540–549. ACM Press, 2007.</p>
<p>[15] Yahoo! Research (Barcelona) and Laboratory of Web Algorithmics (Univ. of Milan). Web spam collection. URL: barcelona.research.yahoo.net/webspam/datasets/.</p>
<p>[16] D. A. Spielman and N. Srivastava. Graph sparsiﬁcation by effective resistances. In Proc. of the 40th annual ACM symposium on Theory of computing (STOC 2008). ACM Press, 2008.</p>
<p>[17] D.B. Wilson. Generating random spanning trees more quickly than the cover time. In Proceedings of the 28th ACM Symposium on the Theory of Computing, pages 296–303. ACM Press, 1996.</p>
<p>[18] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International Conference on Machine Learning, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
