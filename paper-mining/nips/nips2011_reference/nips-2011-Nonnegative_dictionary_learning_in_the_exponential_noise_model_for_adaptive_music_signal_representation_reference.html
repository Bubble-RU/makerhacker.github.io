<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-191" href="../nips2011/nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">nips2011-191</a> <a title="nips-2011-191-reference" href="#">nips2011-191-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</h1>
<br/><p>Source: <a title="nips-2011-191-pdf" href="http://papers.nips.cc/paper/4273-nonnegative-dictionary-learning-in-the-exponential-noise-model-for-adaptive-music-signal-representation.pdf">pdf</a></p><p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><br/>
<h2>reference text</h2><p>[1] D. D. Lee and H. S. Seung. Learning the parts of objects with nonnegative matrix factorization. Nature, 401:788–791, 1999.</p>
<p>[2] C. F´ votte and A. T. Cemgil. Nonnegative matrix factorisations as probabilistic inference in e composite models. In Proc. 17th European Signal Processing Conference (EUSIPCO), pages 1913–1917, Glasgow, Scotland, Aug. 2009.</p>
<p>[3] C. F´ votte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakurae Saito divergence. With application to music analysis. Neural Computation, 21(3):793–830, Mar. 2009.</p>
<p>[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, Jan. 2003.</p>
<p>[5] Thomas Hofman. Probabilistic latent semantic indexing. In Proc. 22nd International Conference on Research and Development in Information Retrieval (SIGIR), 1999.</p>
<p>[6] E. Gaussier and C. Goutte. Relation between PLSA and NMF and implications. In Proc. 28th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR’05), pages 601–602, New York, NY, USA, 2005. ACM.</p>
<p>[7] M. Welling, C. Chemudugunta, and N. Sutter. Deterministic latent variable models and their pitfalls. In SIAM Conference on Data Mining (SDM), pages 196–207, 2008.</p>
<p>[8] W. L. Buntine and A. Jakulin. Discrete component analysis. In Lecture Notes in Computer Science, volume 3940, pages 1–33. Springer, 2006.</p>
<p>[9] John F. Canny. GaP: A factor model for discrete data. In Proceedings of the 27th ACM international Conference on Research and Development of Information Retrieval (SIGIR), pages 122–129, 2004.</p>
<p>[10] O. Dikmen and C. F´ votte. Maximum marginal likelihood estimation for nonnegative dictioe nary learning. In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP’11), Prague, Czech Republic, 2011.</p>
<p>[11] M. Hoffman, D. Blei, and P. Cook. Bayesian nonparametric matrix factorization for recorded music. In Proc. 27th International Conference on Machine Learning (ICML), Haifa, Israel, 2010.</p>
<p>[12] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician, 58:30 – 37, 2004.</p>
<p>[13] Y. Cao, P. P. B. Eggermont, and S. Terebey. Cross Burg entropy maximization and its application to ringing suppression in image reconstruction. IEEE Transactions on Image Processing, 8(2):286–292, Feb. 1999.</p>
<p>[14] C. M. Bishop. Pattern Recognition And Machine Learning. Springer, 2008. ISBN-13: 9780387310732.</p>
<p>[15] K. Katahira, K. Watanabe, and M. Okada. Deterministic annealing variant of variational Bayes method. In International Workshop on Statistical-Mechanical Informatics 2007 (IWSMI 2007), 2007.</p>
<p>[16] D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition into parts? In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, o Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004.</p>
<p>[17] D. J. C. Mackay. Probable networks and plausible predictions – a review of practical Bayesian models for supervised neural networks. Network: Computation in Neural Systems, 6(3):469– 505, 1995.</p>
<p>[18] C. M. Bishop. Bayesian PCA. In Advances in Neural Information Processing Systems (NIPS), pages 382–388, 1999.  9</p>
<br/>
<br/><br/><br/></body>
</html>
