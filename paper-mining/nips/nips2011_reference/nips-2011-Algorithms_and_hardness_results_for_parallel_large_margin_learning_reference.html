<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 nips-2011-Algorithms and hardness results for parallel large margin learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-29" href="../nips2011/nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">nips2011-29</a> <a title="nips-2011-29-reference" href="#">nips2011-29-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 nips-2011-Algorithms and hardness results for parallel large margin learning</h1>
<br/><p>Source: <a title="nips-2011-29-pdf" href="http://papers.nips.cc/paper/4444-algorithms-and-hardness-results-for-parallel-large-margin-learning.pdf">pdf</a></p><p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We study the fundamental problem of learning an unknown large-margin halfspace in the context of parallel computation. Our main positive result is a parallel algorithm for learning a large-margin halfspace that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations. We show that this algorithm learns an unknown γ-margin halfspace over n dimensions using poly(n, 1/γ) processors ˜ and runs in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have Ω(1/γ 2 ) runtime dependence on γ. Our main negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We give an information-theoretic proof that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized: the ability to call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. 1</p><br/>
<h2>reference text</h2><p>[1] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random projection. In Proc. 40th FOCS, pages 616–623, 1999.</p>
<p>[2] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–965, 1989.</p>
<p>[3] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge, 2004.</p>
<p>[4] J. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel coordinate descent for l1-regularized loss minimization. ICML, 2011.</p>
<p>[5] Joseph K. Bradley and Robert E. Schapire. Filterboost: Regression and classiﬁcation on large datasets. In NIPS, 2007.</p>
<p>[6] N. Bshouty, S. Goldman, and H.D. Mathias. Noise-tolerant parallel learning of geometric concepts. Inf. and Comput., 147(1):89 – 110, 1998.</p>
<p>[7] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, adaboost and bregman distances. Machine Learning, 48(1-3):253–285, 2002.</p>
<p>[8] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction. ICML, 2011.</p>
<p>[9] C. Domingo and O. Watanabe. MadaBoost: a modiﬁed version of AdaBoost. In Proc. 13th COLT, pages 180–189, 2000.</p>
<p>[10] Y. Freund. Boosting a weak learning algorithm by majority. Inf. and Comput., 121(2):256–285, 1995.</p>
<p>[11] Y. Freund. An adaptive version of the boost-by-majority algorithm. Mach. Learn., 43(3):293–318, 2001.</p>
<p>[12] R. Greenlaw, H.J. Hoover, and W.L. Ruzzo. Limits to Parallel Computation: P-Completeness Theory. Oxford University Press, New York, 1995.</p>
<p>[13] A. Kalai and R. Servedio. Boosting in the presence of noise. Journal of Computer & System Sciences, 71(3):266–290, 2005.</p>
<p>[14] N. Karmarkar. A new polynomial time algorithm for linear programming. Combinat., 4:373–395, 1984.</p>
<p>[15] M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In Proceedings of the Twenty-Eighth Annual Symposium on Theory of Computing, pages 459–468, 1996.</p>
<p>[16] M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA, 1994.</p>
<p>[17] N. Littlestone. From online to batch learning. In Proc. 2nd COLT, pages 269–284, 1989.</p>
<p>[18] P. Long and R. Servedio. Martingale boosting. In Proc. 18th Annual COLT, pages 79–94, 2005.</p>
<p>[19] P. Long and R. Servedio. Adaptive martingale boosting. In Proc. 22nd NIPS, pages 977–984, 2008.</p>
<p>[20] Y. Mansour and D. McAllester. Boosting using branching programs. Journal of Computer & System Sciences, 64(1):103–112, 2002.</p>
<p>[21] Y. Nesterov and A. Nemirovskii. Interior Point Polynomial Methods in Convex Programming: Theory and Applications. Society for Industrial and Applied Mathematics, Philadelphia, 1994.</p>
<p>[22] John H. Reif. O(log2 n) time efﬁcient parallel factorization of dense, sparse separable, and banded matrices. SPAA, 1994.</p>
<p>[23] J. Renegar. A polynomial-time algorithm, based on Newton’s method, for linear programming. Mathematical Programming, 40:59–93, 1988.</p>
<p>[24] James Renegar. A mathematical view of interior-point methods in convex optimization. Society for Industrial and Applied Mathematics, 2001.</p>
<p>[25] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–407, 1958.</p>
<p>[26] R. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990.</p>
<p>[27] R. Servedio. Smooth boosting and learning with malicious noise. JMLR, 4:633–648, 2003.</p>
<p>[28] S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: New relaxations and efﬁcient boosting algorithms. Machine Learning, 80(2):141–163, 2010.</p>
<p>[29] L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.</p>
<p>[30] V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.</p>
<p>[31] J. S. Vitter and J. Lin. Learning in parallel. Inf. Comput., 96(2):179–202, 1992.</p>
<p>[32] DIMACS 2011 Workshop. Parallelism: A 2020 Vision. 2011.</p>
<p>[33] NIPS 2009 Workshop. Large-Scale Machine Learning: Parallelism and Massive Datasets. 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
