<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 nips-2011-Autonomous Learning of Action Models for Planning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-41" href="../nips2011/nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">nips2011-41</a> <a title="nips-2011-41-reference" href="#">nips2011-41-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 nips-2011-Autonomous Learning of Action Models for Planning</h1>
<br/><p>Source: <a title="nips-2011-41-pdf" href="http://papers.nips.cc/paper/4267-autonomous-learning-of-action-models-for-planning.pdf">pdf</a></p><p>Author: Neville Mehta, Prasad Tadepalli, Alan Fern</p><p>Abstract: This paper introduces two new frameworks for learning action models for planning. In the mistake-bounded planning framework, the learner has access to a planner for the given model representation, a simulator, and a planning problem generator, and aims to learn a model with at most a polynomial number of faulty plans. In the planned exploration framework, the learner does not have access to a problem generator and must instead design its own problems, plan for them, and converge with at most a polynomial number of planning attempts. The paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks. A speciﬁc family of hypothesis spaces is shown to be efﬁciently learnable in both the frameworks. 1</p><br/>
<h2>reference text</h2><p>[1] R. Brafman and M. Tennenholtz. R-MAX — A General Polynomial Time Algorithm for NearOptimal Reinforcement Learning. Journal of Machine Learning Research, 3:213–231, 2002.</p>
<p>[2] M. Kearns and L. Valiant. Cryptographic Limitations on Learning Boolean Formulae and Finite Automata. In Annual ACM Symposium on Theory of Computing, 1989.</p>
<p>[3] L. Li. A Unifying Framework for Computational Reinforcement Learning Theory. PhD thesis, Rutgers University, 2009.</p>
<p>[4] L. Li, M. Littman, and T. Walsh. Knows What It Knows: A Framework for Self-Aware Learning. In ICML, 2008.</p>
<p>[5] N. Littlestone. Mistake Bounds and Logarithmic Linear-Threshold Learning Algorithms. PhD thesis, U.C. Santa Cruz, 1989.</p>
<p>[6] B. Marthi, S. Russell, and J. Wolfe. Angelic Semantics for High-Level Actions. In ICAPS, 2007.</p>
<p>[7] B. K. Natarajan. On Learning Boolean Functions. In Annual ACM Symposium on Theory of Computing, 1987.</p>
<p>[8] T. Walsh and M. Littman. Efﬁcient Learning of Action Schemas and Web-Service Descriptions. In AAAI, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
