<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-57" href="../nips2011/nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">nips2011-57</a> <a title="nips-2011-57-reference" href="#">nips2011-57-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</h1>
<br/><p>Source: <a title="nips-2011-57-pdf" href="http://papers.nips.cc/paper/4333-comparative-analysis-of-viterbi-training-and-maximum-likelihood-estimation-for-hmms.pdf">pdf</a></p><p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><br/>
<h2>reference text</h2><p>[1] A. E. Allahverdyan, Entropy of Hidden Markov Processes via Cycle Expansion, J. Stat. Phys. 133, 535 (2008).</p>
<p>[2] A.E. Allahverdyan and A. Galstyan, On Maximum a Posteriori Estimation of Hidden Markov Processes, Proc. of UAI, (2009).</p>
<p>[3] R. Artuso. E. Aurell and P. Cvitanovic, Recycling of strange sets, Nonlinearity 3, 325 (1990).</p>
<p>[4] P. Baldi and S. Brunak, Bioinformatics, MIT Press, Cambridge, USA (2001).</p>
<p>[5] L. E. Baum and T. Petrie, Statistical inference for probabilistic functions of ﬁnite state Markov chains, Ann. Math. Stat. 37, 1554 (1966).</p>
<p>[6] J.M. Benedi, J.A. Sanchez, Estimation of stochastic context-free grammars and their use as language models, Comp. Speech and Lang. 19, pp. 249-274 (2005).</p>
<p>[7] D. Blackwell and L. Koopmans, On the identiﬁability problem for functions of ﬁnite Markov chains, Ann. Math. Statist. 28, 1011 (1957).</p>
<p>[8] S. B. Cohen and N. A. Smith, Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization, Procs. of ACL (2010).</p>
<p>[9] Y. Ephraim and N. Merhav, Hidden Markov processes, IEEE Trans. Inf. Th., 48, 1518-1569, (2002).</p>
<p>[10] L.Y. Goldsheid and G.A. Margulis, Lyapunov indices of a product of random matrices, Russ. Math. Surveys 44, 11 (1989).</p>
<p>[11] R. N. Gutenkunst et al., Universally Sloppy Parameter Sensitivities in Systems Biology Models, PLoS Computational Biology, 3, 1871 (2007).</p>
<p>[12] G. Han and B. Marcus, Analyticity of entropy rate of hidden Markov chains, IEEE Trans. Inf. Th., 52, 5251 (2006).</p>
<p>[13] R. A. Horn and C. R. Johnson, Matrix Analysis (Cambridge University Press, New Jersey, USA, 1985).</p>
<p>[14] H. Ito, S. Amari, and K. Kobayashi, Identiﬁability of Hidden Markov Information Sources, IEEE Trans. Inf. Th., 38, 324 (1992).</p>
<p>[15] D. Janzing, On causally asymmetric versions of Occam’s Razor and their relation to thermodynamics, arXiv:0708.3411 (2007).</p>
<p>[16] B. H. Juang and L. R. Rabiner, The segmental k-means algorithm for estimating parameters of hidden Markov models, IEEE Trans. Acoustics, Speech, and Signal Processing, ASSP-38, no.9, pp.1639-1641, (1990).</p>
<p>[17] B. G. Leroux, Maximum-Likelihood Estimation for Hidden Markov Models, Stochastic Processes and Their Applications, 40, 127 (1992).</p>
<p>[18] N. Merhav and Y. Ephraim, Maximum likelihood hidden Markov modeling using a dominant sequence of states, IEEE Transactions on Signal Processing, vol.39, no.9, pp.2111-2115 (1991).</p>
<p>[19] F. Qin, Restoration of single-channel currents using the segmental k-means method based on hidden Markov modeling, Biophys J 86, 14881501 (2004).</p>
<p>[20] L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proc. IEEE, 77, 257 (1989).</p>
<p>[21] L. J. Rodriguez and I. Torres, Comparative Study of the Baum-Welch and Viterbi Training Algorithms, Pattern Recognition and Image Analysis, Lecture Notes in Computer Science, 2652/2003, 847 (2003).</p>
<p>[22] D. Ruelle, Statistical Mechanics, Thermodynamic Formalism, (Reading, MA: Addison-Wesley, 1978).</p>
<p>[23] J. Sanchez, J. Benedi, F. Casacuberta, Comparison between the inside-outside algorithm and the Viterbi algorithm for stochastic context-free grammars, in Adv. in Struct. and Synt. Pattern Recognition (1996).</p>
<p>[24] V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning, Viterbi Training Improves Unsupervised Dependency Parsing, in Proc. of the 14th Conference on Computational Natural Language Learning (2010).</p>
<p>[25] A. Vaswani, A. Pauls, and D. Chiang, Efﬁcient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging, in Proc. ACL (2010).  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
