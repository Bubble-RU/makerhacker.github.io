<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-170" href="../nips2011/nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">nips2011-170</a> <a title="nips-2011-170-reference" href="#">nips2011-170-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</h1>
<br/><p>Source: <a title="nips-2011-170-pdf" href="http://papers.nips.cc/paper/4285-message-passing-for-approximate-map-inference-with-latent-variables.pdf">pdf</a></p><p>Author: Jiarong Jiang, Piyush Rai, Hal Daume</p><p>Abstract: We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this. The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework. We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efﬁcacy of our proposed algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] Shankar Kumar, William Byrne, and Speech Processing. Minimum bayes-risk decoding for statistical machine translation. In HLT-NAACL, 2004.</p>
<p>[2] David Sontag and Tommi Jaakkola. New outer bounds on the marginal polytope. In In Advances in Neural Information Processing Systems, 2007.</p>
<p>[3] Amir Globerson and Tommi Jaakkola. Fixing max-product: Convergent message passing algorithms for map lp-relaxations. In NIPS, 2007.</p>
<p>[4] Pradeep Ravikumar, Alekh Agarwal, and Martin J. Wainwright. Message-passing for graph-structured linear programs: proximal projections, convergence and rounding schemes. In ICML, 2008.</p>
<p>[5] Qiang Liu and Alexander Ihler. Variational algorithms for marginal map. In UAI, 2011.</p>
<p>[6] James D. Park. MAP Complexity Results and Approximation Methods. In UAI, 2002.</p>
<p>[7] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.</p>
<p>[8] Shaul K. Bar-Lev, Daoud Bshouty, Peter Enis, Gerard Letac, I-Li Lu, and Donald Richards. The diagnonal multivariate natural exponential families and their classiﬁcation. In Journal of Theoretical Probability, pages 883–929, 1994.</p>
<p>[9] Vaibhava Goel and William J. Byrne. Minimum Bayes-risk automatic speech recognition. Computer Speech and Language, 14(2), 2000.</p>
<p>[10] M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends in Machine Learning, 2008.</p>
<p>[11] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.</p>
<p>[12] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In NIPS, 2000.</p>
<p>[13] Martin J. Wainwright, Tommi S. Jaakkola, and Alan S. Willsky. Exact map estimates by tree agreement. In NIPS, 2002.</p>
<p>[14] Martin J. Wainwright, Tommi S. Jaakkola, and Alan S. Willsky. Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment matching. In AISTATS, 2003.</p>
<p>[15] Mark Johnson. Why doesnt em ﬁnd good hmm pos-taggers. In EMNLP, pages 296–305, 2007.</p>
<p>[16] Pradeep Ravikumar, Martin J. Wainwright, and Alekh Agarwal. Message-passing for graph-structured linear programs: Proximal methods and rounding schemes, 2008.</p>
<p>[17] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM algorithm. Journal of The Royal Statistica Society, 1977.</p>
<p>[18] Radford M. Neal and Geoffrey E. Hinton. A View of the EM Algorithm that Justiﬁes Incremental, Sparse, and Other Variants. In Learning in graphical models, pages 355–368, 1999.</p>
<p>[19] Slav Petrov and Dan Klein. Discriminative log-linear grammars with latent variables. In NIPS, 2008.</p>
<p>[20] Ivan Titov and James Henderson. A latent variable model for generative dependency parsing. In IWPT, 2007.</p>
<p>[21] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. Learning structured prediction models: a large margin approach. 2004.</p>
<p>[22] Ioannis Tsochantaridis, Google Inc, Thorsten Joachims, Thomas Hofmann, Yasemin Altun, and Yoram Singer. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005.</p>
<p>[23] Chen Yanover, Talya Meltzer, and Yair Weiss. Linear programming relaxations and belief propagation – an empirical study. Journal of Machine Learning Research, 7:1887–1907, 2006.</p>
<p>[24] Chen Yanover, Ora Schueler-furman, and Yair Weiss. Minimizing and learning energy functions for side-chain prediction. In RECOMB2007, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
