<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-63" href="../nips2011/nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">nips2011-63</a> <a title="nips-2011-63-reference" href="#">nips2011-63-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</h1>
<br/><p>Source: <a title="nips-2011-63-pdf" href="http://papers.nips.cc/paper/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization.pdf">pdf</a></p><p>Author: Mark Schmidt, Nicolas L. Roux, Francis R. Bach</p><p>Abstract: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen ﬁxed error level on a set of structured sparsity problems. 1</p><br/>
<h2>reference text</h2><p>[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.</p>
<p>[2] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discussion Papers, (2007/76), 2007.</p>
<p>[3] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1):267–288, 1996.</p>
<p>[4] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.</p>
<p>[5] S.J. Wright, R.D. Nowak, and M.A.T. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009.</p>
<p>[6] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. In S. Sra, S. Nowozin, and S.J. Wright, editors, Optimization for Machine Learning. MIT Press, 2011.</p>
<p>[7] J. Fadili and G. Peyr´ . Total variation projection with ﬁrst order schemes. IEEE Transactions on Image e Processing, 20(3):657–669, 2011.</p>
<p>[8] X. Chen, S. Kim, Q. Lin, J.G. Carbonell, and E.P. Xing. Graph-structured multi-task regression and an efﬁcient optimization method for general fused Lasso. arXiv:1005.3579v1, 2010.</p>
<p>[9] J.-F. Cai, E.J. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM e Journal on Optimization, 20(4), 2010.</p>
<p>[10] S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1):321–353, 2011.  8</p>
<p>[11] L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. ICML, 2009.</p>
<p>[12] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. JMLR, 12:2297–2334, 2011.</p>
<p>[13] A. Barbero and S. Sra. Fast Newton-type methods for total variation regularization. ICML, 2011.</p>
<p>[14] J. Liu and J. Ye. Fast overlapping group Lasso. arXiv:1009.0306v1, 2010.</p>
<p>[15] M. Schmidt and K. Murphy. Convex structure learning in log-linear models: Beyond pairwise potentials. AISTATS, 2010.</p>
<p>[16] M. Patriksson. A uniﬁed framework of descent algorithms for nonlinear programs and variational inequalities. PhD thesis, Department of Mathematics, Link¨ ping University, Sweden, 1995. o</p>
<p>[17] P.L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged operators. Optimization, 53(5-6):475–504, 2004.</p>
<p>[18] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. JMLR, 10:2873–2898, 2009.</p>
<p>[19] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. JMLR, 10:777–801, 2009.</p>
<p>[20] A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171–1183, 2008.</p>
<p>[21] M. Baes. Estimate sequence methods: extensions and approximations. IFOR internal report, ETH Zurich, 2009.</p>
<p>[22] O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. CORE Discussion Papers, (2011/02), 2011.</p>
<p>[23] A. Nedic and D. Bertsekas. Convergence rate of incremental subgradient algorithms. Stochastic Optimization: Algorithms and Applications, pages 263–304, 2000.</p>
<p>[24] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: A general approach. Annals of Operations Research, 46-47(1):157–178, 1993.</p>
<p>[25] M.P. Friedlander and M. Schmidt. arXiv:1104.2373, 2011.  Hybrid deterministic-stochastic methods for data ﬁtting.</p>
<p>[26] R.T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14(5):877–898, 1976.</p>
<p>[27] O. G¨ ler. New proximal point algorithms for convex minimization. SIAM Journal on Optimization, u 2(4):649–664, 1992.</p>
<p>[28] S. Villa, S. Salzo, L. Baldassarre, and A. Verri. Accelerated and inexact forward-backward algorithms. Optimization Online, 2011.</p>
<p>[29] M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. arXiv:1109.2415v2, 2011.</p>
<p>[30] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.</p>
<p>[31] D.P. Bertsekas. Convex optimization theory. Athena Scientiﬁc, 2009.</p>
<p>[32] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization, 2008.</p>
<p>[33] J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. Convex and network ﬂow optimization for structured sparsity. JMLR, 12:2681–2720, 2011.</p>
<p>[34] H.H. Bauschke and P.L. Combettes. A Dykstra-like algorithm for two monotone operators. Paciﬁc Journal of Optimization, 4(3):383–391, 2008.</p>
<p>[35] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog., 103(1):127–152, 2005.</p>
<p>[36] P.L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In H.H. Bauschke, R.S. Burachik, P.L. Combettes, V. Elser, D.R. Luke, and H. Wolkowicz, editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages 185–212. Springer, 2011.</p>
<p>[37] M.J. Wainwright, T.S. Jaakkola, and A.S. Willsky. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching. AISTATS, 2003.</p>
<p>[38] J. Kivinen, A.J. Smola, and R.C. Williamson. Online learning with kernels. IEEE Transactions on Signal Processing, 52(8):2165–2176, 2004.</p>
<p>[39] A. d’Aspremont. Subsampling algorithms for semideﬁnite programming. arXiv:0803.1990v5, 2009.</p>
<p>[40] M. Schmidt, D. Kim, and S. Sra. Projected Newton-type methods in machine learning. In S. Sra, S. Nowozin, and S. Wright, editors, Optimization for Machine Learning. MIT Press, 2011.  9</p>
<br/>
<br/><br/><br/></body>
</html>
