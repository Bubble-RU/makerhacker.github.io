<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2011-Algorithms for Hyper-Parameter Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-30" href="../nips2011/nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">nips2011-30</a> <a title="nips-2011-30-reference" href="#">nips2011-30-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>30 nips-2011-Algorithms for Hyper-Parameter Optimization</h1>
<br/><p>Source: <a title="nips-2011-30-pdf" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">pdf</a></p><p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><br/>
<h2>reference text</h2><p>[1] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML 2007, pages 473–480, 2007.</p>
<p>[2] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.</p>
<p>[3] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Machine Learning Research, 11:3371–3408, 2010.</p>
<p>[4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.</p>
<p>[5] Nicolas Pinto, David Doukhan, James J. DiCarlo, and David D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Comput Biol, 5(11):e1000579, 11 2009.</p>
<p>[6] A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning. NIPS Deep Learning and Unsupervised Feature Learning Workshop, 2010.</p>
<p>[7] A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML11), 2010.</p>
<p>[8] F. Hutter. Automated Conﬁguration of Algorithms for Solving Hard Computational Problems. PhD thesis, University of British Columbia, 2009.</p>
<p>[9] F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In LION-5, 2011. Extended version as UBC Tech report TR-2010-10.</p>
<p>[10] D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21:345–383, 2001.</p>
<p>[11] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, 2006.</p>
<p>[12] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, 2010.</p>
<p>[13] J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum. In L.C.W. Dixon and G.P. Szego, editors, Towards Global Optimization, volume 2, pages 117–129. North Holland, New York, 1978.</p>
<p>[14] C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.</p>
<p>[15] D. Ginsbourger, D. Dupuy, A. Badea, L. Carraro, and O. Roustant. A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments. 25:115–131, 2009.</p>
<p>[16] R. Bardenet and B. K´ gl. Surrogating the surrogate: accelerating Gaussian Process optimization with e mixtures. In ICML, 2010.</p>
<p>[17] P. Larra˜ aga and J. Lozano, editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary n Computation. Springer, 2001.</p>
<p>[18] N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano, P. Larranaga, I. Inza, and E. Bengoetxea, editors, Towards a new evolutionary computation. Advances on estimation of distribution algorithms, pages 75–102. Springer, 2006.</p>
<p>[19] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Learning Workshop (Snowbird), 2011.</p>
<p>[20] A. Hyv¨ rinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Neta works, 13(4–5):411–430, 2000.</p>
<p>[21] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 2012. Accepted.</p>
<p>[22] C. Bishop. Neural networks for pattern recognition. 1995.</p>
<p>[23] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), June 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
