<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2011-Nonstandard Interpretations of Probabilistic Programs for Efficient Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-192" href="../nips2011/nips-2011-Nonstandard_Interpretations_of_Probabilistic_Programs_for_Efficient_Inference.html">nips2011-192</a> <a title="nips-2011-192-reference" href="#">nips2011-192-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 nips-2011-Nonstandard Interpretations of Probabilistic Programs for Efficient Inference</h1>
<br/><p>Source: <a title="nips-2011-192-pdf" href="http://papers.nips.cc/paper/4309-nonstandard-interpretations-of-probabilistic-programs-for-efficient-inference.pdf">pdf</a></p><p>Author: David Wingate, Noah Goodman, Andreas Stuhlmueller, Jeffrey M. Siskind</p><p>Abstract: Probabilistic programming languages allow modelers to specify a stochastic process using syntax that resembles modern programming languages. Because the program is in machine-readable format, a variety of techniques from compiler design and program analysis can be used to examine the structure of the distribution represented by the probabilistic program. We show how nonstandard interpretations of probabilistic programs can be used to craft efﬁcient inference algorithms: information about the structure of a distribution (such as gradients or dependencies) is generated as a monad-like side computation while executing the program. These interpretations can be easily coded using special-purpose objects and operator overloading. We implement two examples of nonstandard interpretations in two different languages, and use them as building blocks to construct inference algorithms: automatic differentiation, which enables gradient based methods, and provenance tracking, which enables efﬁcient construction of global proposals. 1</p><br/>
<h2>reference text</h2><p>[1] C. Bendtsen and O. Stauning. FADBAD, a ﬂexible C++ package for automatic differentiation. Technical Report IMM–REP–1996–17, Department of Mathematical Modelling, Technical University of Denmark, Lyngby, Denmark, Aug. 1996.</p>
<p>[2] C. H. Bischof, A. Carle, G. F. Corliss, A. Griewank, and P. D. Hovland. ADIFOR: Generating derivative codes from Fortran programs. Scientiﬁc Programming, 1(1):11–29, 1992.</p>
<p>[3] G. Corliss, C. Faure, A. Griewank, L. Hasco¨ t, and U. Naumann. Automatic Differentiation: From e Simulation to Optimization. Springer-Verlag, New York, NY, 2001.</p>
<p>[4] J. Eisner, E. Goldlust, and N. A. Smith. Compiling comp ling: Weighted dynamic programming and the Dyna language. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 281–290, Vancouver, October 2005.</p>
<p>[5] M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. J. R. Statist. Soc. B, 73(2):123–214, 2011.</p>
<p>[6] N. Goodman, V. Mansinghka, D. Roy, K. Bonawitz, and J. Tenenbaum. Church: a language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.</p>
<p>[7] A. Griewank. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Number 19 in Frontiers in Applied Mathematics. SIAM, 2000.</p>
<p>[8] A. Griewank, D. Juedes, and J. Utke. ADOL-C, a package for the automatic differentiation of algorithms written in C/C++. ACM Trans. Math. Software, 22(2):131–167, 1996.</p>
<p>[9] E. Herbst. Gradient and Hessian-based MCMC for DSGE models (job market paper), 2010.</p>
<p>[10] K. Kersting and L. D. Raedt. Bayesian logic programming: Theory and tool. In L. Getoor and B. Taskar, editors, An Introduction to Statistical Relational Learning. MIT Press, 2007.</p>
<p>[11] O. Kiselyov and C. Shan. Embedded probabilistic programming. In Domain-Speciﬁc Languages, pages 360–384, 2009.</p>
<p>[12] Y. LeCun and L. Bottou. Lush reference manual. Technical report, 2002. URL http://lush. sourceforge.net.</p>
<p>[13] B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic models with unknown objects. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1352–1359, 2005.</p>
<p>[14] M. B. Monagan and W. M. Neuenschwander. GRADIENT: Algorithmic differentiation in Maple. In International Symposium on Symbolic and Algebraic Computation (ISSAC), pages 68–76, July 1993.</p>
<p>[15] R. M. Neal. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte-Carlo (Steve Brooks, Andrew Gelman, Galin Jones and Xiao-Li Meng, Eds.), 2010.</p>
<p>[16] S. Pandolﬁ, F. Bartolucci, and N. Friel. A generalization of the multiple-try metropolis algorithm for bayesian estimation and model selection. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2010.</p>
<p>[17] B. A. Pearlmutter and J. M. Siskind. Lazy multivariate higher-order forward-mode AD. In Symposium on Principles of Programming Languages (POPL), pages 155–160, 2007. doi: 10.1145/1190215.1190242.</p>
<p>[18] A. Pfeffer. IBAL: A probabilistic rational programming language. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 733–740. Morgan Kaufmann Publ., 2001.</p>
<p>[19] Y. Qi and T. P. Minka. Hessian-based Markov chain Monte-Carlo algorithms (unpublished manuscript), 2002.</p>
<p>[20] P. J. Rossky, J. D. Doll, and H. L. Friedman. Brownian dynamics as smart monte carlo simulation. Journal of Chemical Physics, 69:4628–4633, 1978.</p>
<p>[21] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. 323:533–536, 1986.</p>
<p>[22] S. Rump. INTLAB - INTerval LABoratory. In Developments in Reliable Computing, pages 77–104. Kluwer Academic Publishers, Dordrecht, 1999.</p>
<p>[23] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Neural Information Processing Systems (NIPS), 2008.</p>
<p>[24] J. M. Siskind and B. A. Pearlmutter. First-class nonstandard interpretations by opening closures. In Symposium on Principles of Programming Languages (POPL), pages 71–76, 2007. doi: 10.1145/1190216. 1190230.</p>
<p>[25] B. Speelpenning. Compiling Fast Partial Derivatives of Functions Given by Algorithms. PhD thesis, Department of Computer Science, University of Illinois at Urbana-Champaign, Jan. 1980.</p>
<p>[26] B. Taylor. Methodus Incrementorum Directa et Inversa. London, 1715.</p>
<p>[27] R. E. Wengert. A simple automatic derivative evaluation program. Commun. ACM, 7(8):463–464, 1964.</p>
<p>[28] D. Wingate, A. Stuhlmueller, and N. D. Goodman. Lightweight implementations of probabilistic programming languages via transformational compilation. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
