<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-142" href="../nips2011/nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">nips2011-142</a> <a title="nips-2011-142-reference" href="#">nips2011-142-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</h1>
<br/><p>Source: <a title="nips-2011-142-pdf" href="http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf">pdf</a></p><p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><br/>
<h2>reference text</h2><p>[1] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct formulation of sparse PCA using semideﬁnite programming. SIAM Review, 49(3), 2007.</p>
<p>[2] A.A. Amini and M. Wainwright. High-dimensional analysis of semideﬁnite relaxations for sparse principal components. The Annals of Statistics, 37(5B):2877–2921, 2009.</p>
<p>[3] I. T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal of Applied Statistics, 22:29–35, 1995.</p>
<p>[4] J. Cadima and I. T. Jolliffe. Loadings and correlations in the interpretation of principal components. Journal of Applied Statistics, 22:203–214, 1995.</p>
<p>[5] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: exact and greedy algorithms. Advances in Neural Information Processing Systems, 18, 2006.</p>
<p>[6] A. d’Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9:1269–1294, 2008.</p>
<p>[7] I. T. Jolliffe, N.T. Trendaﬁlov, and M. Uddin. A modiﬁed principal component technique based on the LASSO. Journal of Computational and Graphical Statistics, 12:531–547, 2003.</p>
<p>[8] H. Zou, T. Hastie, and R. Tibshirani. Sparse Principal Component Analysis. Journal of Computational & Graphical Statistics, 15(2):265–286, 2006.</p>
<p>[9] Haipeng Shen and Jianhua Z. Huang. Sparse principal component analysis via regularized low rank matrix approximation. J. Multivar. Anal., 99:1015–1034, July 2008.</p>
<p>[10] M. Journ´ e, Y. Nesterov, P. Richt´ rik, and R. Sepulchre. Generalized power method for sparse e a principal component analysis. arXiv:0811.4724, 2008.</p>
<p>[11] Y. Zhang, A. d’Aspremont, and L. El Ghaoui. Sparse PCA: Convex relaxations, algorithms and applications. In M. Anjos and J.B. Lasserre, editors, Handbook on Semide nite, Cone and Polynomial Optimization: Theory, Algorithms, Software and Applications. Springer, 2011. To appear.</p>
<p>[12] L. El Ghaoui. On the quality of a semideﬁnite programming bound for sparse principal component analysis. arXiv:math/060144, February 2006.</p>
<p>[13] O.Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research, 9:485–516, March 2008.</p>
<p>[14] Zaiwen Wen, Donald Goldfarb, Shiqian Ma, and Katya Scheinberg. Row by row methods for semideﬁnite programming. Technical report, Dept of IEOR, Columbia University, 2009.</p>
<p>[15] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.</p>
<p>[16] A. Frank and A. Asuncion. UCI machine learning repository, 2010.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
