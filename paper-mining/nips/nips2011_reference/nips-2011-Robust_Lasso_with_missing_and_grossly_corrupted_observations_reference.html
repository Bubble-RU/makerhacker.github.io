<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>239 nips-2011-Robust Lasso with missing and grossly corrupted observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-239" href="../nips2011/nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">nips2011-239</a> <a title="nips-2011-239-reference" href="#">nips2011-239-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>239 nips-2011-Robust Lasso with missing and grossly corrupted observations</h1>
<br/><p>Source: <a title="nips-2011-239-pdf" href="http://papers.nips.cc/paper/4386-robust-lasso-with-missing-and-grossly-corrupted-observations.pdf">pdf</a></p><p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal, S. Negahban, and M. Wainwright. Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions. Proc. 28th Inter. Conf. Mach. Learn. (ICML-11), pages 1129–1136, 2011.  8  σ2  log n λe  −1      ,  Sublinear sparsity  0.6 0.4 p=128 p=256 p=512  0.2  0.2  0.4 0.6 Rescaled sample size θ  0.8  1  1  0.8  0.8  Probability of success  Probability of success  Probability of success  0.8  0 0  Fractional power sparsity  Linear sparsity 1  1  0.6 0.4 p=128 p=256 p=512  0.2 0 0  0.2  0.4 0.6 Rescaled sample size θ  0.8  1  0.6 0.4 p=128 p=256 p=512  0.2 0 0  0.2  0.4 0.6 Rescaled sample size θ  0.8  1  Figure 1: Probability of success in recovering the signed supports</p>
<p>[2] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of statistics, 37(4):1705–1732, 2009.</p>
<p>[3] E. J. Cand` s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Submitted for publicae tion, 2009.</p>
<p>[4] E. J. Cand` s and Y. Plan. Near-ideal model selection by l1 minimization. Annals of Statistics, 37:2145– e 2177, 2009.</p>
<p>[5] E. J. Cand` s and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals e of statistics, 35(6):2313–2351, 2007.</p>
<p>[6] E. Elhamifar and R. Vidal. Sparse subspace clustering. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2790–2797, 2009.</p>
<p>[7] J. N. Laska, M. A. Davenport, and R. G. Baraniuk. Exact signal recovery from sparsely corrupted measurements through the pursuit of justice. In Asilomar conference on Signals, Systems and Computers, pages 1556–1560, 2009.</p>
<p>[8] X. Li. Compressed sensing and matrix completion with constant proportion of corruptions. Preprint, 2011.</p>
<p>[9] Z. Li, F. Wu, and J. Wright. On the systematic measurement matrix for compressed sensing in the presence of gross error. In Data compression conference (DCC), pages 356–365, 2010.</p>
<p>[10] N. Meinshausen and P. Buehlmann. High dimensional graphs and variable selection with the lasso. Annals of statistics, 34(3):1436–1462, 2008.</p>
<p>[11] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of statistics, 37(1):2246–2270, 2009.</p>
<p>[12] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. Preprint, 2010.</p>
<p>[13] N. H. Nguyen and Trac. D. Tran. Exact recoverability from dense corrupted observations via l1 minimization. preprint, 2010.</p>
<p>[14] G. Raskutti, M. J. Wainwright, and B. Yu. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11:2241–2259, 2010.</p>
<p>[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, 1996.</p>
<p>[16] S. A. van de Geer and P. Buehlmann. On the conditions used to prove oracle results for the lasso. Electronic Journal of Statistics, 3(1360-1392), 2009.</p>
<p>[17] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1 -constrained quadratic programming ( lasso ). IEEE Trans. Information Theory, 55(5):2183–2202, 2009.</p>
<p>[18] J. Wright and Y. Ma. Dense error correction via l1 minimization. IEEE Transaction on Information Theory, 56(7):3540–3560, 2010.</p>
<p>[19] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transaction on Pattern Analysis and Machine Intelligence, 31(2):210–227, 2009.</p>
<p>[20] H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. Ad. Neural Infor. Proc. Sys. (NIPS), pages 2496–2504, 2010.</p>
<p>[21] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B, 68(1):49–67, 2006.</p>
<p>[22] T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization. Annals of statistics, 37(5):2109–2144, 2009.</p>
<p>[23] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
