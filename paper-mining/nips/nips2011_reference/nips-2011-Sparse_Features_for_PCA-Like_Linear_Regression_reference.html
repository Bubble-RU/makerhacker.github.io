<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>260 nips-2011-Sparse Features for PCA-Like Linear Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-260" href="../nips2011/nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">nips2011-260</a> <a title="nips-2011-260-reference" href="#">nips2011-260-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>260 nips-2011-Sparse Features for PCA-Like Linear Regression</h1>
<br/><p>Source: <a title="nips-2011-260-pdf" href="http://papers.nips.cc/paper/4196-sparse-features-for-pca-like-linear-regression.pdf">pdf</a></p><p>Author: Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail</p><p>Abstract: Principal Components Analysis (PCA) is often used as a feature extraction procedure. Given a matrix X ∈ Rn×d , whose rows represent n data points with respect to d features, the top k right singular vectors of X (the so-called eigenfeatures), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a small number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while provably achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efﬁcient, and we demonstrate their performance on several data sets.</p><br/>
<h2>reference text</h2><p>[1] J. Batson, D. Spielman, and N. Srivastava. Twice-ramanujan sparsiﬁers. In Proceedings of ACM STOC, pages 255–262, 2009.</p>
<p>[2] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column based matrix reconstruction. In Proceedings of IEEE FOCS, 2011.</p>
<p>[3] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. manuscript, 2011.</p>
<p>[4] C. Boutsidis and M. Magdon-Ismail. arXiv:1109.5664v1, 2011.  Sparse features for PCA-like linear regression.  Deterministic feature selection for k-means clustering.  8</p>
<p>[5] C. Boutsidis, M. W. Mahoney, and P. Drineas. An improved approximation algorithm for the column subset selection problem. In Proceedings of ACM -SIAM SODA, pages 968–977, 2009.</p>
<p>[6] C. Boutsidis, M. W. Mahoney, and P. Drineas. Unsupervised feature selection for the k-means clustering problem. In Proceedings of NIPS, 2009.</p>
<p>[7] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal components. Applied Statistics, 22:203–214, 1995.</p>
<p>[8] T. Chan and P. Hansen. Some applications of the rank revealing QR factorization. SIAM Journal on Scientiﬁc and Statistical Computing, 13:727–741, 1992.</p>
<p>[9] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proceedings of ACM STOC, 2008.</p>
<p>[10] A. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. Sampling algorithms and coresets for Lp regression. In Proceedings of ACM-SIAM SODA, 2008.</p>
<p>[11] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA using semideﬁnite programming. In Proceedings of NIPS, 2004.</p>
<p>[12] A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection. In Proceedings of ACM STOC, 2010.</p>
<p>[13] P. Drineas, R. Kannan, and M. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication. SIAM Journal of Computing, 36(1):132–157, 2006.</p>
<p>[14] P. Drineas, M. Mahoney, and S. Muthukrishnan. Polynomial time algorithm for column-row based relative-error low-rank matrix approximation. Technical Report 2006-04, DIMACS, March 2006.</p>
<p>[15] P. Drineas, M. Mahoney, and S. Muthukrishnan. Sampling algorithms for ℓ2 regression and applications. In Proceedings of ACM-SIAM SODA, pages 1127–1136, 2006.</p>
<p>[16] G. Golub. Numerical methods for solving linear least squares problems. Numerische Mathematik, 7:206– 216, 1965.</p>
<p>[17] G. Golub, P. Hansen, and D. O’Leary. Tikhonov regularization and total least squares. SIAM Journal on Matrix Analysis and Applications, 21(1):185–194, 2000.</p>
<p>[18] M. Gu and S. Eisenstat. Efﬁcient algorithms for computing a strong rank-revealing QR factorization. SIAM Journal on Scientiﬁc Computing, 17:848–869, 1996.</p>
<p>[19] I. Guyon and A. Elisseeff. Special issue on variable and feature selection. Journal of Machine Learning Research, 3, 2003.</p>
<p>[20] N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 2011.</p>
<p>[21] P. Hansen. The truncated SVD as a method for regularization. BIT Numerical Mathematics, 27(4):534– 553, 1987.</p>
<p>[22] I. Jolliffe. Discarding variables in Principal Component Analysis: asrtiﬁcial data. Applied Statistics, 21(2):160–173, 1972.</p>
<p>[23] R. Larsen. PROPACK: A software package for the symmetric eigenvalue problem and singular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization. http://soi.stanford.edu/∼rmunk/∼PROPACK/.</p>
<p>[24] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: exact and greedy algorithms. In Proceedings of NIPS, 2005.</p>
<p>[25] B. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227– 234, 1995.</p>
<p>[26] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric functional analysis. Journal of the ACM, 54, 2007.</p>
<p>[27] N. Srivastava and D. Spielman. Graph sparsiﬁcations by effective resistances. In Proceedings of ACM STOC, pages 563–568, 2008.</p>
<p>[28] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, pages 267–288, 1996.</p>
<p>[29] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):2231–2242, 2004.</p>
<p>[30] T. Zhang. Generating a d-dimensional linear subspace efﬁciently. In Adaptive forward-backward greedy algorithm for sparse learning with linear models, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
