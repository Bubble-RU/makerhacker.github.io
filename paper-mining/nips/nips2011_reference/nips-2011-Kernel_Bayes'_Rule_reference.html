<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2011-Kernel Bayes' Rule</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-139" href="../nips2011/nips-2011-Kernel_Bayes%27_Rule.html">nips2011-139</a> <a title="nips-2011-139-reference" href="#">nips2011-139-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>139 nips-2011-Kernel Bayes' Rule</h1>
<br/><p>Source: <a title="nips-2011-139-pdf" href="http://papers.nips.cc/paper/4414-kernel-bayes-rule.pdf">pdf</a></p><p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A nonparametric kernel-based method for realizing Bayes’ rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample. The kernel Bayes’ rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and ﬁltering with a nonparametric statespace model. A consistency rate for the posterior estimate is established. 1</p><br/>
<h2>reference text</h2><p>[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68(3):337–404, 1950.</p>
<p>[2] C.R. Baker. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273–289, 1973.</p>
<p>[3] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Kluwer Academic Publisher, 2004.</p>
<p>[4] A. Doucet, N. De Freitas, and N.J. Gordon. Sequential Monte Carlo Methods in Practice. Springer, 2001.</p>
<p>[5] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. JMLR, 2:243– 264, 2001.</p>
<p>[6] K. Fukumizu, F.R. Bach, and M.I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.</p>
<p>[7] K. Fukumizu, F.R. Bach, and M.I. Jordan. Kernel dimension reduction in regression. Anna. Stat., 37(4):1871–1905, 2009.</p>
<p>[8] K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨ lkopf. Kernel measures of conditional dependence. In o Advances in NIPS 20, pages 489–496. MIT Press, 2008.</p>
<p>[9] A. Gretton, K.M. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. Smola. A kernel method for the twoo sample-problem. In Advances in NIPS 19, pages 513–520. MIT Press, 2007.</p>
<p>[10] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch¨ lkopf, and A. Smola. A kernel statistical test of o independence. In Advances in NIPS 20, pages 585–592. MIT Press, 2008.</p>
<p>[11] S.J. Julier and J.K. Uhlmann. A new extension of the Kalman ﬁlter to nonlinear systems. In Proc. AeroSense: The 11th Intern. Symp. Aerospace/Defence Sensing, Simulation and Controls, 1997.</p>
<p>[12] P. Marjoram, Jo. Molitor, V. Plagnol, and S. Tavare. Markov chain monte carlo without likelihoods. PNAS, 100(26):15324–15328, 2003.</p>
<p>[13] S. Mika, B. Sch¨ lkopf, A. Smola, K.-R. M¨ ller, M. Scholz, and G. R¨ tsch. Kernel pca and de-noising in o u a feature spaces. In Advances in NIPS 11, pages 536–542. MIT Press, 1999.</p>
<p>[14] P. M¨ ller and F.A. Quintana. Nonparametric bayesian data analysis. Statistical Science, 19(1):95–110, u 2004.</p>
<p>[15] M. Rudemo. Empirical choice of histograms and kernel density estimators. Scandinavian J. Statistics, 9(2):pp. 65–78, 1982.</p>
<p>[16] B. Sch¨ lkopf and A.J. Smola. Learning with Kernels. MIT Press, 2002. o</p>
<p>[17] S. A. Sisson, Y. Fan, and M. M. Tanaka. Sequential monte carlo without likelihoods. PNAS, 104(6):1760– 1765, 2007.</p>
<p>[18] L. Song, A. Gretton., and C. Guestrin. Nonparametric tree graphical models via kernel embeddings. In AISTATS 2010, pages 765–772, 2010.</p>
<p>[19] L. Song, A. Gretton, D. Bickson, Y. Low, and C. Guestrin. Kernel belief propagation. In AISTATS 2011.</p>
<p>[20] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. Proc ICML2009, pages 961–968. 2009.</p>
<p>[21] L. Song and S. M. Siddiqi and G. Gordon and A. Smola. Hilbert Space Embeddings of Hidden Markov Models. Proc. ICML2010, 991–998. 2010.</p>
<p>[22] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨ lkopf, and G. R.G. Lanckriet. Hilbert space o embeddings and metrics on probability measures. JMLR, 11:1517–1561, 2010.</p>
<p>[23] I. Steinwart. On the Inﬂuence of the Kernel on the Consistency of Support Vector Machines. JMLR, 2:67–93, 2001.</p>
<p>[24] M. Sugiyama, I. Takeuchi, T. Suzuki, T. Kanamori, H. Hachiya, and D. Okanohara. Conditional density estimation via least-squares density ratio estimation. In AISTATS 2010, pages 781–788, 2010.</p>
<p>[25] S. Tavar´ , D.J. Balding, R.C. Grifﬁthis, and P. Donnelly. Inferring coalescence times from dna sequece e data. Genetics, 145:505–518, 1997.</p>
<p>[26] S. Thrun, J. Langford, and D. Fox. Monte carlo hidden markov models: Learning non-parametric models of partially observable stochastic processes. In ICML 1999, pages 415–424, 1999.</p>
<p>[27] V. Monbet , P. Ailliot, and P.F. Marteau. l1 -convergence of smoothing densities in non-parametric state space models. Statistical Inference for Stochastic Processes, 11:311–325, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
