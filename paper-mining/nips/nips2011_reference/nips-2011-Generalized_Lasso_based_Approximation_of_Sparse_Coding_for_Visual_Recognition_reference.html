<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-105" href="../nips2011/nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">nips2011-105</a> <a title="nips-2011-105-reference" href="#">nips2011-105-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</h1>
<br/><p>Source: <a title="nips-2011-105-pdf" href="http://papers.nips.cc/paper/4441-generalized-lasso-based-approximation-of-sparse-coding-for-visual-recognition.pdf">pdf</a></p><p>Author: Nobuyuki Morioka, Shin'ichi Satoh</p><p>Abstract: Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, 1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to 1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its effectiveness for large-scale visual recognition problems. 1</p><br/>
<h2>reference text</h2><p>[1] A. Adler, Y. Hel-Or, and M. Elad. A Shrinkage Learning Approach for Single Image SuperResolution with Overcomplete Representations. In ECCV, 2010.</p>
<p>[2] D.L. Donoho. For Most Large Underdetermined Systems of Linear Equations the Minimal L1norm Solution is also the Sparse Solution. Communications on Pure and Applied Mathematics, 2006.</p>
<p>[3] D.L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via L1 minimization. PNAS, 100(5):2197–2202, 2003.</p>
<p>[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least Angle Regression. Annals of Statistics, 2004.</p>
<p>[5] L. Fei-Fei, R. Fergus, and P. Perona. Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories. In CVPR Workshop, 2004.</p>
<p>[6] S. Gao, W. Tsang, L. Chia, and P. Zhao. Local Features Are Not Lonely - Laplacian Sparse Coding for Image Classiﬁcation. In CVPR, 2010.</p>
<p>[7] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In ICML, 2010.</p>
<p>[8] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 Object Category Dataset. Technical Report, California Institute of Technology, 2007.</p>
<p>[9] Y. Hel-Or and D. Shaked. A Discriminative Approach for Wavelet Denoising. TIP, 2008.</p>
<p>[10] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the Best Multi-Stage Architecture for Object Recognition. In ICCV, 2009.</p>
<p>[11] K Kavukcuoglu, M Ranzato, and Y Lecun. Fast inference in sparse coding algorithms with applications to object recognition. Technical rRport CBLL-TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008.</p>
<p>[12] S.-J. Kim, K. Koh, S. Boyd, and D. Gorinevsky. L1 trend ﬁltering. SIAM Review, 2009.</p>
<p>[13] S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. In CVPR, 2006.</p>
<p>[14] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2006.</p>
<p>[15] D.G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. IJCV, 2004.</p>
<p>[16] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised Dictionary Learning. In NIPS, 2008.</p>
<p>[17] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. Discriminative Sparse Image Models for Class-Speciﬁc Edge Detection and Image Interpretation. In ECCV, 2008.</p>
<p>[18] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37, 1997.</p>
<p>[19] M. Ranzato, F.J. Huang, Y. Boureau, and Y. LeCun. Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition. In CVPR, 2007.</p>
<p>[20] E. Shechtman and M. Irani. Matching Local Self-Similarities across Image and Videos. In CVPR, 2007.</p>
<p>[21] R. Tibshirani and J. Taylor. The Solution Path of the Generalized Lasso. The Annals of Statistics, 2010.</p>
<p>[22] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding for Image Classiﬁcation. In CVPR, 2010.</p>
<p>[23] J. Yang, J. Wright, T. Huang, and Y. Ma. Image Super-Resolution via Sparse Representation. TIP, 2010.</p>
<p>[24] J. Yang, K. Yu, Y. Gong, and T.S. Huang. Linear spatial pyramid matching using sparse coding for image classiﬁcation. In CVPR, 2009.</p>
<p>[25] J. Yang, K. Yu, and T. Huang. Supervised Translation-Invariant Sparse Coding. In CVPR, 2010.</p>
<p>[26] K. Yu, T. Zhang, and Y. Gong. Nonlinear Learning using Local Coordinate Coding. In NIPS, 2009. 9</p>
<br/>
<br/><br/><br/></body>
</html>
