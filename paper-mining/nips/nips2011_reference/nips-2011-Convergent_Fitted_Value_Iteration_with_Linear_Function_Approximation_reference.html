<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-65" href="../nips2011/nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">nips2011-65</a> <a title="nips-2011-65-reference" href="#">nips2011-65-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</h1>
<br/><p>Source: <a title="nips-2011-65-pdf" href="http://papers.nips.cc/paper/4298-convergent-fitted-value-iteration-with-linear-function-approximation.pdf">pdf</a></p><p>Author: Daniel J. Lizotte</p><p>Abstract: Fitted value iteration (FVI) with ordinary least squares regression is known to diverge. We present a new method, “Expansion-Constrained Ordinary Least Squares” (ECOLS), that produces a linear approximation but also guarantees convergence when used with FVI. To ensure convergence, we constrain the least squares regression operator to be a non-expansion in the ∞-norm. We show that the space of function approximators that satisfy this constraint is more rich than the space of “averagers,” we prove a minimax property of the ECOLS residual error, and we give an efﬁcient algorithm for computing the coefﬁcients of ECOLS based on constraint generation. We illustrate the algorithmic convergence of FVI with ECOLS in a suite of experiments, and discuss its properties. 1</p><br/>
<h2>reference text</h2><p>[1] A. Antos, R. Munos, and Cs. Szepesv´ ri. Fitted Q-iteration in continuous action-space MDPs. a In Advances in Neural Information Processing Systems 20, pages 9–16. MIT Press, 2008.</p>
<p>[2] L. Baird. Residual Algorithms: Reinforcement Learning with Function Approximation. In A. Prieditis and S. Russell, editors, Proceedings of the 25th International Conference on Machine Learning, pages 30–37. Morgan Kaufmann, 1995.</p>
<p>[3] D. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 2007.</p>
<p>[4] J. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value function. In Advances in neural information processing systems, pages 369–376, 1995.</p>
<p>[5] D. Ernst, P. Geurts, and L. Wehenkel. Tree-Based Batch Mode Reinforcement Learning. Journal of Machine Learning Research, 6:503–556, 2005.</p>
<p>[6] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv´ ri, and S. Mannor. Regularized ﬁtted Qa iteration for planning in continuous-space Markovian decision problems. In American Control Conference, pages 725–730, 2009.</p>
<p>[7] R. Fonteneau. Contributions to Batch Mode Reinforcement Learning. PhD thesis, University of Liege, 2011.</p>
<p>[8] G. J. Gordon. Approximate Solutions to Markov Decision Processes. PhD thesis, Carnegie Mellon University, 1999.</p>
<p>[9] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21. http://cvxr.com/cvx, Apr. 2011.</p>
<p>[10] M. C. Grant. Disciplined convex programming and the cvx modeling framework. Information Systems Journal, 2006.</p>
<p>[11] A. Guez, R. D. Vincent, M. Avoli, and J. Pineau. Adaptive treatment of epilepsy via batchmode reinforcement learning. In D. Fox and C. P. Gomes, editors, Innovative Applications of Artiﬁcial Intelligence, pages 1671–1678, 2008.</p>
<p>[12] IBM. IBM ILOG CPLEX Optimization Studio V12.2, 2011.</p>
<p>[13] S. Kalyanakrishnan and P. Stone. Batch reinforcement learning in a complex domain. In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems AAMAS 07, 2007.</p>
<p>[14] R. Munos and Cs. Szepesv´ ri. Finite time bounds for ﬁtted value iteration. Journal of Machine a Learning Research, 9:815–857, 2008.</p>
<p>[15] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine learning, 49(2):161– 178, 2002.</p>
<p>[16] M. Riedmiller. Neural ﬁtted Q iteration-ﬁrst experiences with a data efﬁcient neural reinforcement learning method. In ECML 2005, pages 317–328. Springer, 2005.</p>
<p>[17] J. Rust. Using randomization to break the curse of dimensionality. Econometrica, 65(3):pp. 487–516, 1997.</p>
<p>[18] G. A. F. Seber. A MATRIX HANDBOOK FOR STATISTICIANS. Wiley, 2007.</p>
<p>[19] S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy. Informing sequential clinical decision-making through reinforcement learning : an empirical study. Machine Learning, 2010.</p>
<p>[20] S. Siddiqi, B. Boots, and G. Gordon. A Constraint Generation Approach to Learning Stable Linear Dynamical Systems. In Advances in Neural Information Processing Systems 20, pages 1329–1336. MIT Press, 2008.</p>
<p>[21] Cs. Szepesv´ ri. Algorithms for Reinforcement Learning. Morgan and Claypool, 2010. a</p>
<p>[22] J. N. Tsitsiklis and B. van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
