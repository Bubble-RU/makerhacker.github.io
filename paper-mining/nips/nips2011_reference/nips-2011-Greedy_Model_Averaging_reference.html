<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2011-Greedy Model Averaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-109" href="../nips2011/nips-2011-Greedy_Model_Averaging.html">nips2011-109</a> <a title="nips-2011-109-reference" href="#">nips2011-109-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>109 nips-2011-Greedy Model Averaging</h1>
<br/><p>Source: <a title="nips-2011-109-pdf" href="http://papers.nips.cc/paper/4203-greedy-model-averaging.pdf">pdf</a></p><p>Author: Dong Dai, Tong Zhang</p><p>Abstract: This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression. It is known that if the models are mis-speciﬁed, model averaging is superior to model selection. Speciﬁcally, let n be the sample size, then the worst case regret of the former decays at the rate of O(1/n) while the worst √ case regret of the latter decays at the rate of O(1/ n). In the literature, the most important and widely studied model averaging method that achieves the optimal O(1/n) average regret is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations. The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA. We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples. 1</p><br/>
<h2>reference text</h2><p>[1] Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In NIPS’07, 2008.</p>
<p>[2] Olivier Catoni. Statistical learning theory and stochastic optimization. Springer-Verlag, 2004.</p>
<p>[3] Arnak Dalalyan and Joseph Salmon. Optimal aggregation of afﬁne estimators. In COLT’01, 2011.</p>
<p>[4] L.K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. Ann. Statist., 20(1):608–613, 1992.</p>
<p>[5] Anatoli Juditsky, Philippe Rigollet, and Alexandre Tsybakov. Learning by mirror averaging. The Annals of Statistics, 36:2183–2206, 2008.</p>
<p>[6] Gilbert Leung and A.R. Barron. Information theory and mixing least-squares regressions. Information Theory, IEEE Transactions on, 52(8):3396 –3410, aug. 2006.</p>
<p>[7] Philippe Rigollet. Kullback-leibler aggregation and misspeciﬁed generalized linear models. arXiv:0911.2919, November 2010.</p>
<p>[8] Pilippe Rigollet and Alexandre Tsybakov. Exponential Screening and optimal rates of sparse estimation. The Annals of Statistics, 39:731–771, 2011.</p>
<p>[9] Yuhong Yang. Adaptive regression by mixing. Journal of the American Statistical Association, 96:574–588, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
