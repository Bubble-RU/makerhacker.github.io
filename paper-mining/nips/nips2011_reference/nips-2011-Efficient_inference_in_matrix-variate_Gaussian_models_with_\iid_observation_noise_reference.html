<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-83" href="../nips2011/nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">nips2011-83</a> <a title="nips-2011-83-reference" href="#">nips2011-83-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</h1>
<br/><p>Source: <a title="nips-2011-83-pdf" href="http://papers.nips.cc/paper/4281-efficient-inference-in-matrix-variate-gaussian-models-with-iid-observation-noise.pdf">pdf</a></p><p>Author: Oliver Stegle, Christoph Lippert, Joris M. Mooij, Neil D. Lawrence, Karsten M. Borgwardt</p><p>Abstract: Inference in matrix-variate Gaussian models has major applications for multioutput prediction and joint learning of row and column covariances from matrixvariate data. Here, we discuss an approach for efﬁcient inference in such models that explicitly account for iid observation noise. Computational tractability can be retained by exploiting the Kronecker product between row and column covariance matrices. Using this framework, we show how to generalize the Graphical Lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples. We show practical utility on applications to biology, where we model covariances with more than 100,000 dimensions. We ﬁnd greater accuracy in recovering biological network structures and are able to better reconstruct the confounders. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Zhang and J. Schneider. Learning multiple tasks with a sparse matrix-normal penalty. In Advances in Neural Information Processing Systems, 2010.</p>
<p>[2] E. Bonilla, K.M. Chai, and C. Williams. Multi-task gaussian process prediction. Advances in Neural Information Processing Systems, 20:153–160, 2008.</p>
<p>[3] M.A. Alvarez and N.D. Lawrence. Computationally efﬁcient convolved multiple output gaussian processes. Journal of Machine Learning Research, 12:1425–1466, 2011.</p>
<p>[4] H. Wackernagel. Multivariate geostatistics: an introduction with applications. Springer Verlag, 2003.</p>
<p>[5] G.I. Allen and R. Tibshirani. Inference with transposable data: Modeling the effects of row and column correlations. Arxiv preprint arXiv:1004.0209, 2010.</p>
<p>[6] M. Lynch and B. Walsh. Genetics and Analysis of Quantitative Traits. Sinauer Associates Inc., U.S., 1998.</p>
<p>[7] P. Dutilleul. The MLE algorithm for the matrix normal distribution. Journal of Statistical Computation and Simulation, 64(2):105–123, 1999.</p>
<p>[8] K. Zhang, B. Sch¨ lkopf, and D. Janzing. Invariant gaussian process latent variable models and o application in causal discovery. In Uncertainty in Artiﬁcial Intelligence, 2010.</p>
<p>[9] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research, 9:485–516, 2008.</p>
<p>[10] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432, 2008.</p>
<p>[11] J.T. Leek and J.D. Storey. Capturing heterogeneity in gene expression studies by surrogate variable analysis. PLoS Genetics, 3(9):e161, 2007.</p>
<p>[12] O. Stegle, L. Parts, R. Durbin, and J. Winn. A bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eqtl studies. PLoS Computational Biology, 6(5):e1000770, 2010.</p>
<p>[13] C. Lippert, J. Listgarten, Y. Liu, C.M. Kadie, R.I. Davidson, and D. Heckerman. FaST linear mixed models for genome-wide association studies. Nature Methods, 8:833–835, 2011.</p>
<p>[14] P. Men´ ndez, Y.A.I. Kourmpetis, C.J.F. Ter Braak, and F.A. van Eeuwijk. Gene regulatory e networks from multifactorial perturbations using graphical lasso: Application to the dream4 challenge. PLoS One, 5(12):e14147, 2010.</p>
<p>[15] N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent variable models. Journal of Machine Learning Research, 6:1783–1816, 2005.</p>
<p>[16] K.Y. Yeung and W.L. Ruzzo. Principal component analysis for clustering gene expression data. Bioinformatics, 17(9):763, 2001.</p>
<p>[17] N. Meinshausen and P. B¨ hlmann. Stability selection. Journal of the Royal Statistical Society: u Series B (Statistical Methodology), 72(4):417–473, 2010.</p>
<p>[18] K. Sachs, O. Perez, D. Pe’er, D.A. Lauffenburger, and G.P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):523, 2005.</p>
<p>[19] E.N. Smith and L. Kruglyak. Gene–environment interaction in yeast gene expression. PLoS Biology, 6(4):e83, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
