<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-118" href="../nips2011/nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">nips2011-118</a> <a title="nips-2011-118-reference" href="#">nips2011-118-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</h1>
<br/><p>Source: <a title="nips-2011-118-pdf" href="http://papers.nips.cc/paper/4454-high-dimensional-regression-with-noisy-and-missing-data-provable-guarantees-with-non-convexity.pdf">pdf</a></p><p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><br/>
<h2>reference text</h2><p>[1] R. Little and D. B. Rubin. Statistical analysis with missing data. Wiley, New York, 1987.</p>
<p>[2] J. T. Hwang. Multiplicative errors-in-variables models with applications to recent data released by the U.S. Department of Energy. Journal of the American Statistical Association, 81(395):pp. 680–688, 1986.</p>
<p>[3] R. J. Carroll, D. Ruppert, and L. A. Stefanski. Measurement Error in Nonlinear Models. Chapman and Hall, 1995.</p>
<p>[4] S. J. Iturria, R. J. Carroll, and D. Firth. Polynomial regression and estimating functions in the presence of multiplicative measurement error. Journal of the Royal Statistical Society Series B - Statistical Methodology, 61:547–561, 1999.</p>
<p>[5] Q. Xu and J. You. Covariate selection for linear errors-in-variables regression models. Communications in Statistics - Theory and Methods, 36(2):375–386, 2007.</p>
<p>[6] N. St¨ dler and P. B¨ hlmann. Missing values: Sparse inverse covariance estimation and an extension to a u sparse regression. Statistics and Computing, pages 1–17, 2010.</p>
<p>[7] M. Rosenbaum and A. B. Tsybakov. Sparse recovery under matrix uncertainty. Annals of Statistics, 38:2620–2651, 2010.</p>
<p>[8] P. Loh and M.J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Technical report, UC Berkeley, September 2011. Available at http: //arxiv.org/abs/1109.3714.</p>
<p>[9] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996.</p>
<p>[10] S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.</p>
<p>[11] S. van de Geer. The deterministic Lasso. In Proceedings of Joint Statistical Meeting, 2007.</p>
<p>[12] P. J. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4):1705–1732, 2009.</p>
<p>[13] S. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the Lasso. Electronic Journal of Statistics, 3:1360–1392, 2009.</p>
<p>[14] G. Raskutti, M. J. Wainwright, and B. Yu. Restricted eigenvalue properties for correlated Gaussian designs. Journal of Machine Learning Research, 11:2241–2259, 2010.</p>
<p>[15] A. Agarwal, S. Negahban, and M.J. Wainwright. Fast global convergence of gradient methods for highdimensional statistical recovery. Technical report, UC Berkeley, April 2011. Available at http:// arxiv.org/abs/1104.4824.</p>
<p>[16] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the 1 -ball for learning in high dimensions. In International Conference on Machine Learning, pages 272–279, 2008.</p>
<p>[17] C. H. Zhang and J. Huang. The sparsity and bias of the Lasso selection in high-dimensional linear regression. Annals of Statistics, 36(4):1567–1594, 2008.</p>
<p>[18] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37(1):246–270, 2009.</p>
<p>[19] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for the analysis of regularized M -estimators. In Advances in Neural Information Processing Systems, 2009.</p>
<p>[20] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the Lasso. Annals u of Statistics, 34:1436–1462, 2006.</p>
<p>[21] M. Yuan. High-dimensional inverse covariance matrix estimation via linear programming. Journal of Machine Learning Research, 99:2261–2286, August 2010.</p>
<p>[22] A. J. Rothman, P. J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
