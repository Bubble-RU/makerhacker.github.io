<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>282 nips-2011-The Fast Convergence of Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-282" href="../nips2011/nips-2011-The_Fast_Convergence_of_Boosting.html">nips2011-282</a> <a title="nips-2011-282-reference" href="#">nips2011-282-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>282 nips-2011-The Fast Convergence of Boosting</h1>
<br/><p>Source: <a title="nips-2011-282-pdf" href="http://papers.nips.cc/paper/4343-the-fast-convergence-of-boosting.pdf">pdf</a></p><p>Author: Matus J. Telgarsky</p><p>Abstract: This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/✏2 )). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/✏)). Next, the (disjoint) conditions under which the inﬁmal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/✏)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/✏), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the inﬁmal empirical risk; the technique for overcoming this barrier may be of general interest. 1</p><br/>
<h2>reference text</h2><p>[1] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, July 1990.</p>
<p>[2] Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256–285, 1995.</p>
<p>[3] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.</p>
<p>[4] Leo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517, October 1999.</p>
<p>[5] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28, 1998.</p>
<p>[6] Peter J. Bickel, Yaacov Ritov, and Alon Zakai. Some theory for generalized boosting algorithms. Journal of Machine Learning Research, 7:705–732, 2006.</p>
<p>[7] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72:7–35, 1992.</p>
<p>[8] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1-3):253–285, 2002.</p>
<p>[9] Gunnar R¨ tsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of leveraging. a In NIPS, pages 487–494, 2001.</p>
<p>[10] Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In COLT, 2011.</p>
<p>[11] Robert E. Schapire. The convergence rate of AdaBoost. In COLT, 2010.</p>
<p>[12] Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In ICML, pages 322–330, 1997.</p>
<p>[13] Gunnar R¨ tsch and Manfred K. Warmuth. Maximizing the margin with boosting. In COLT, a pages 334–350, 2002.</p>
<p>[14] Manfred K. Warmuth, Karen A. Glocer, and Gunnar R¨ tsch. Boosting algorithms for maxia mizing the soft margin. In NIPS, 2007.</p>
<p>[15] Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separability: New relaxations and efﬁcient boosting algorithms. In COLT, pages 311–322, 2008.</p>
<p>[16] Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Functional gradient techniques for combining hypotheses. In A.J. Smola, P.L. Bartlett, B. Sch¨ lkopf, and D. Schuo urmans, editors, Advances in Large Margin Classiﬁers, pages 221–246, Cambridge, MA, 2000. MIT Press.</p>
<p>[17] Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[18] Jean-Baptiste Hiriart-Urruty and Claude Lemar´ chal. e Springer Publishing Company, Incorporated, 2001.  Fundamentals of Convex Analysis.</p>
<p>[19] Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer Publishing Company, Incorporated, 2000.</p>
<p>[20] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999.</p>
<p>[21] George B. Dantzig and Mukund N. Thapa. Linear Programming 2: Theory and Extensions. Springer, 2003.</p>
<p>[22] Adi Ben-Israel. Motzkin’s transposition theorem, and the related theorems of Farkas, Gordan and Stiemke. In M. Hazewinkel, editor, Encyclopaedia of Mathematics, Supplement III. 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
