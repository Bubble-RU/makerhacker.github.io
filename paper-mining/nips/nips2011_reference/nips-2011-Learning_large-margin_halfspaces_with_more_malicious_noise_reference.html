<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2011-Learning large-margin halfspaces with more malicious noise</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-153" href="../nips2011/nips-2011-Learning_large-margin_halfspaces_with_more_malicious_noise.html">nips2011-153</a> <a title="nips-2011-153-reference" href="#">nips2011-153-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 nips-2011-Learning large-margin halfspaces with more malicious noise</h1>
<br/><p>Source: <a title="nips-2011-153-pdf" href="http://papers.nips.cc/paper/4323-learning-large-margin-halfspaces-with-more-malicious-noise.pdf">pdf</a></p><p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We describe a simple algorithm that runs in time poly(n, 1/γ, 1/ε) and learns an unknown n-dimensional γ-margin halfspace to accuracy 1 − ε in the presence of malicious noise, when the noise rate is allowed to be as high as Θ(εγ log(1/γ)). Previous efﬁcient algorithms could only learn to accuracy ε in the presence of malicious noise of rate at most Θ(εγ). Our algorithm does not work by optimizing a convex loss function. We show that no algorithm for learning γ-margin halfspaces that minimizes a convex proxy for misclassiﬁcation error can tolerate malicious noise at a rate greater than Θ(εγ); this may partially explain why previous algorithms could not achieve the higher noise tolerance of our new algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] J. Aslam and S. Decatur. Speciﬁcation and simulation of statistical query algorithms for efﬁciency and noise tolerance. Journal of Computer and System Sciences, 56:191–208, 1998.</p>
<p>[2] P. Auer. Learning nested differences in the presence of malicious noise. Theor. Comp. Sci., 185(1):159– 175, 1997.</p>
<p>[3] P. Auer and N. Cesa-Bianchi. On-line learning with malicious noise and the closure algorithm. Annals of Mathematics and Artiﬁcial Intelligence, 23:83–99, 1998.  8</p>
<p>[4] E. B. Baum and D. Haussler. What size net gives valid generalization? Neural Comput., 1:151–160, 1989.</p>
<p>[5] H. Block. The Perceptron: a model for brain functioning. Reviews of Modern Physics, 34:123–135, 1962.</p>
<p>[6] A. Blum. Random Projection, Margins, Kernels, and Feature-Selection. In LNCS Volume 3940, pages 52–68, 2006.</p>
<p>[7] A. Blum and M.-F. Balcan. A discriminative model for semi-supervised learning. Journal of the ACM, 57(3), 2010.</p>
<p>[8] S. Decatur. Statistical queries and faulty PAC oracles. In Proc. 6th COLT, pages 262–268, 1993.</p>
<p>[9] C. Domingo and O. Watanabe. MadaBoost: a modiﬁed version of AdaBoost. In Proc. 13th COLT, pages 180–189, 2000.</p>
<p>[10] D. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algorithms. Cambridge University Press, Cambridge, 2009.</p>
<p>[11] A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of examples needed for learning. Information and Computation, 82(3):247–251, 1989.</p>
<p>[12] V. Feldman, P. Gopalan, S. Khot, and A. Ponnuswami. On agnostic learning of parities, monomials, and halfspaces. SIAM J. Comput., 39(2):606–645, 2009.</p>
<p>[13] W. Feller. Generalization of a probability limit theorem of Cram´ r. Trans. Am. Math. Soc., 54:361–372, e 1943.</p>
<p>[14] Y. Freund and R. Schapire. Large margin classiﬁcation using the Perceptron algorithm. In Proc. 11th COLT, pages 209–217., 1998.</p>
<p>[15] Y. Freund and R. Schapire. A short introduction to boosting. J. Japan. Soc. Artif. Intel., 14(5):771–780, 1999.</p>
<p>[16] D. Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. JMLR, 4:101– 117, 2003.</p>
<p>[17] C. Gentile and N. Littlestone. The robustness of the p-norm algorithms. In Proc. 12th COLT, pages 1–11, 1999.</p>
<p>[18] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. SIAM J. Comput., 39(2):742–765, 2009.</p>
<p>[19] D. Haussler, M. Kearns, N. Littlestone, and M. Warmuth. Equivalence of models for polynomial learnability. Information and Computation, 95(2):129–161, 1991.</p>
<p>[20] M. Kearns and M. Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807–837, 1993.</p>
<p>[21] R. Khardon and G. Wachman. Noise tolerant variants of the perceptron algorithm. JMLR, 8:227–248, 2007.</p>
<p>[22] A. Klivans, P. Long, and R. Servedio. Learning Halfspaces with Malicious Noise. JMLR, 10:2715–2740, 2009.</p>
<p>[23] P. Long and R. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine Learning, 78(3):287–304, 2010.</p>
<p>[24] Y. Mansour and M. Parnas. Learning conjunctions with noise under product distributions. Information Processing Letters, 68(4):189–196, 1998.</p>
<p>[25] R. Meir and G. R¨ tsch. An introduction to boosting and leveraging. In LNAI Advanced Lectures on a Machine Learning, pages 118–183, 2003.</p>
<p>[26] A. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on Mathematical Theory of Automata, volume XII, pages 615–622, 1962.</p>
<p>[27] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–407, 1958.</p>
<p>[28] R. Servedio. Smooth boosting and learning with malicious noise. JMLR, 4:633–648, 2003.</p>
<p>[29] J. Shawe-Taylor, P. Bartlett, R. Williamson, and M. Anthony. Structural risk minimization over datadependent hierarchies. IEEE Transactions on Information Theory, 44(5):1926–1940, 1998.</p>
<p>[30] L. Valiant. Learning disjunctions of conjunctions. In Proc. 9th IJCAI, pages 560–566, 1985.</p>
<p>[31] V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
