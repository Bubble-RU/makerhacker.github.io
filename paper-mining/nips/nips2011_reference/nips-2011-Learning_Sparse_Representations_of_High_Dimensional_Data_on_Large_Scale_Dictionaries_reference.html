<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-149" href="../nips2011/nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">nips2011-149</a> <a title="nips-2011-149-reference" href="#">nips2011-149-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</h1>
<br/><p>Source: <a title="nips-2011-149-pdf" href="http://papers.nips.cc/paper/4400-learning-sparse-representations-of-high-dimensional-data-on-large-scale-dictionaries.pdf">pdf</a></p><p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><br/>
<h2>reference text</h2><p>[1] M. Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. Springer, 2010.</p>
<p>[2] G. Cao and C.A. Bouman. Covariance estimation for high dimensional data vectors using the sparse matrix transform. In Advances in Neural Information Processing Systems, 2008.</p>
<p>[3] A.B. Lee, B. Nadler, and L. Wasserman. Treelets An adaptive multi-scale basis for sparse unordered data. The Annals of Applied Statistics, 2(2):435–471, 2008.</p>
<p>[4] M. Gavish, B. Nadler, and R.R. Coifman. Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning. In International Conference on Machine Learning, 2010.</p>
<p>[5] M. Belkin and P. Niyogi. Using manifold stucture for partially labeled classiﬁcation. In Advances in Neural Information Processing Systems, pages 953–960, 2003.</p>
<p>[6] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323, 2000.</p>
<p>[7] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision research, 37(23):3311–3325, 1997.</p>
<p>[8] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210–227, 2008.</p>
<p>[9] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Advances in Neural Information Processing Systems, volume 3, 2009.</p>
<p>[10] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, pages 407–451, 2004.</p>
<p>[11] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11:19–60, 2010.</p>
<p>[12] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. In Advances in Neural Information Processing Systems, volume 19, page 801, 2007.</p>
<p>[13] L.E. Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Arxiv preprint arXiv:1009.3515, 2010.</p>
<p>[14] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R.J. Tibshirani. Strong rules for discarding predictors in lasso-type problems. Arxiv preprint arXiv:1011.2234, 2010.</p>
<p>[15] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. Huang, and S. Yan. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE, 98(6):1031–1044, 2010.</p>
<p>[16] R.G. Baraniuk and M.B. Wakin. Random projections of smooth manifolds. Foundations of Computational Mathematics, 9(1):51–77, 2007.</p>
<p>[17] Y. Lin, T. Zhang, S. Zhu, and K. Yu. Deep coding network. In Advances in Neural Information Processing Systems, 2010.</p>
<p>[18] G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554, 2006.</p>
<p>[19] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In International Conference on Machine Learning, 2010.</p>
<p>[20] M.B. Wakin, D.L. Donoho, H. Choi, and R.G. Baraniuk. Highresolution navigation on non-differentiable image manifolds. In IEEE International Conference on Acoustics, Speech and Signal Processing, volume 5, pages 1073–1076, 2005.</p>
<p>[21] M.F. Duarte, M.A. Davenport, M.B. Wakin, JN Laska, D. Takhar, K.F. Kelly, and RG Baraniuk. Multiscale random projections for compressive classiﬁcation. In IEEE International Conference on Image Processing, volume 6, 2007.</p>
<p>[22] J.M. Shapiro. Embedded image coding using zerotrees of wavelet coefﬁcients. IEEE Transactions on Signal Processing, 41(12):3445–3462, 2002.</p>
<p>[23] S.A. Nene, S.K. Nayar, and H. Murase. Columbia object image library (coil-100). Techn. Rep. No. CUCS-006-96, dept. Comp. Science, Columbia University, 1996.</p>
<p>[24] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278 –2324, nov 1998.</p>
<p>[25] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643–660, 2002.</p>
<p>[26] K.C. Lee, J. Ho, and D.J. Kriegman. Acquiring linear subspaces for face recognition under variable lighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 684–698, 2005.</p>
<p>[27] R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J. Lin. LIBLINEAR: A library for large linear classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
