<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-294" href="../nips2011/nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">nips2011-294</a> <a title="nips-2011-294-reference" href="#">nips2011-294-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2011-294-pdf" href="http://papers.nips.cc/paper/4200-unifying-framework-for-fast-learning-rate-of-non-sparse-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Taiji Suzuki</p><p>Abstract: In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regularization instead of ℓ1 -mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp -MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization. 1</p><br/>
<h2>reference text</h2><p>[1] J. Aﬂalo, A. Ben-Tal, C. Bhattacharyya, J. S. Nath, and S. Raman. Variable sparsity kernel learning. Journal of Machine Learning Research, 12:565–592, 2011.</p>
<p>[2] A. Argyriou, R. Hauser, C. A. Micchelli, and M. Pontil. A DC-programming algorithm for kernel selection. In the 23st ICML, pages 41–48, 2006.</p>
<p>[3] F. R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008.</p>
<p>[4] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances in Neural Information Processing Systems 21, pages 105–112, 2009.</p>
<p>[5] F. R. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In the 21st ICML, pages 41–48, 2004.</p>
<p>[6] P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statistics, 33:1487–1537, 2005. ‡ In our assumption sm should be greater than 0. However we formally put sm = 0 (m > 1) for simplicity of discussion. For rigorous discussion, one might consider arbitrary small sm ≪ s.  8</p>
<p>[7] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, Boston, 1988.</p>
<p>[8] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In UAI 2009, 2009.</p>
<p>[9] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels. In Advances in Neural Information Processing Systems 22, pages 396–404, 2009.</p>
<p>[10] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels. In the 27th ICML, pages 247–254, 2010.</p>
<p>[11] D. E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential Operators. Cambridge University Press, Cambridge, 1996.</p>
<p>[12] G. S. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathematical Analysis and Applications, 33:82–95, 1971.</p>
<p>[13] M. Kloft and G. Blanchard. The local rademacher complexity of ℓp -norm multiple kernel learning, 2011. arXiv:1103.0790.</p>
<p>[14] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. M¨ ller, and A. Zien. Efﬁcient and accurate ℓp -norm u multiple kernel learning. In Advances in Neural Information Processing Systems 22, pages 997–1005, 2009.</p>
<p>[15] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. lp -norm multiple kernel learning. Journal of Machine Learning Research, 12:953–997, 2011.</p>
<p>[16] M. Kloft, U. R¨ ckert, and P. L. Bartlett. A unifying view of multiple kernel learning. In ECML/PKDD, u 2010.</p>
<p>[17] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34:2593–2656, 2006.</p>
<p>[18] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In COLT, pages 229–238, 2008.</p>
<p>[19] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics, 38(6):3660– 3695, 2010.</p>
<p>[20] G. Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with semi-deﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004.</p>
<p>[21] L. Meier, S. van de Geer, and P. B¨ hlmann. High-dimensional additive modeling. The Annals of Statistics, u 37(6B):3779–3821, 2009.</p>
<p>[22] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005.</p>
<p>[23] C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine Learning Research, 6:1043–1071, 2005.</p>
<p>[24] G. Raskutti, M. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Technical report, 2010. arXiv:1008.3654.</p>
<p>[25] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[26] J. Shawe-Taylor. Kernel learning for novelty detection. In NIPS 2008 Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, Whistler, 2008.</p>
<p>[27] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels. In COLT, pages 169–183, 2006.</p>
<p>[28] I. Steinwart. Support Vector Machines. Springer, 2008.</p>
<p>[29] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT, 2009.</p>
<p>[30] T. Suzuki and R. Tomioka. Spicymkl: A fast algorithm for multiple kernel learning with thousands of kernels. Machine Learning, 85(1):77–108, 2011.</p>
<p>[31] R. Tomioka and T. Suzuki. Sparsity-accuracy trade-off in MKL. In NIPS 2009 Workshop: Understanding Multiple Kernel Learning Methods, Whistler, 2009.</p>
<p>[32] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.</p>
<p>[33] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 1996.</p>
<p>[34] M. Varma and B. R. Babu. More generality in efﬁcient multiple kernel learning. In the 26th ICML, pages 1065–1072, 2009.</p>
<p>[35] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In COLT, 2009.</p>
<p>[36] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of The Royal Statistical Society Series B, 68(1):49–67, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
