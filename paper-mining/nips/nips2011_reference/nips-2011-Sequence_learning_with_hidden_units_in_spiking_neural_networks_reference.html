<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 nips-2011-Sequence learning with hidden units in spiking neural networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-249" href="../nips2011/nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">nips2011-249</a> <a title="nips-2011-249-reference" href="#">nips2011-249-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 nips-2011-Sequence learning with hidden units in spiking neural networks</h1>
<br/><p>Source: <a title="nips-2011-249-pdf" href="http://papers.nips.cc/paper/4383-sequence-learning-with-hidden-units-in-spiking-neural-networks.pdf">pdf</a></p><p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><br/>
<h2>reference text</h2><p>[1] D. Ackley and G. E. Hinton. A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147– 169, 1985.</p>
<p>[2] D. Barber. Learning in spiking neural assemblies. Advances in Neural Information Processing Systems, 15, 2003.</p>
<p>[3] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2011. In press.</p>
<p>[4] L. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical Statistics, 41(1):164–171, 1970.</p>
<p>[5] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38, 1977.</p>
<p>[6] A. D¨ ring, A. Coolen, and D. Sherrington. Phase diagram and storage capacity of sequence processing u neural networks. Journal of Physics A: Mathematical and General, 31:8607, 1998.</p>
<p>[7] W. Gerstner and W. M. Kistler. Spiking neuron models: single neurons, populations, plasticity. Cambridge University Press, 2002.</p>
<p>[8] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–800, 2002.</p>
<p>[9] G. E. Hinton and A. Brown. Spiking boltzmann machines. Advances in Neural Information Processing Systems, 12, 2000.</p>
<p>[10] J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences of the United States of America, 79(8):2554, 1982.</p>
<p>[11] P. Latham and J. W. Pillow. Neural characterization in partially observed populations of spiking neurons. Advances in Neural Information Processing Systems, 20:1161–1168, 2008.</p>
<p>[12] M. Lengyel, J. Kwag, O. Paulsen, and P. Dayan. Matching storage and recall: hippocampal spike timingdependent plasticity and phase response curves. Nature Neuroscience, 8(12):1677–83, 2005.</p>
<p>[13] M. Lukoˇeviˇ ius and H. Jaeger. Reservoir computing approaches to recurrent neural network training. s c Computer Science Review, 3(3):127–149, 2009.</p>
<p>[14] W. Maass, T. Natschl¨ ger, and H. Markram. Real-time computing without stable states: a new framework a for neural computation based on perturbations. Neural Computation, 14(11):2531–60, 2002.</p>
<p>[15] D. J. C. MacKay. Information Theory, Inference & Learning Algorithms. Cambridge University Press, 2002.</p>
<p>[16] G. McLachlan and T. Krishnan. The EM Algorithm and Extensions. John Wiley and Sons, 1997.</p>
<p>[17] Y. Mishchenko and L. Paninski. Efﬁcient methods for sampling spike trains in networks of coupled neurons. The Annals of Applied Statistics, 5(3):1893–1919, 2011.</p>
<p>[18] J.-P. Pﬁster, T. Toyoizumi, D. Barber, and W. Gerstner. Optimal spike-timing-dependent plasticity for precise action potential ﬁring in supervised learning. Neural Computation, 18(6):1318–1348, 2006.</p>
<p>[19] L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–86, 1989.</p>
<p>[20] H. Sompolinsky and I. Kanter. Temporal association in asymmetric neural networks. Physical Review Letters, 57(22):2861–64, 1986.</p>
<p>[21] I. Sutskever, G. E. Hinton, and G. Taylor. The Recurrent Temporal Restricted Boltzmann Machine. Advances in Neural Information Processing Systems, 21:1601–08, 2009.</p>
<p>[22] G. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using binary latent variables. Advances in Neural Information Processing Systems, 19:1345–52, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
