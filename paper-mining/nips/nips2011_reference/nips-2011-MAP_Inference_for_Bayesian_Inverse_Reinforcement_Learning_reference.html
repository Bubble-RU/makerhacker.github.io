<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-163" href="../nips2011/nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">nips2011-163</a> <a title="nips-2011-163-reference" href="#">nips2011-163-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2011-163-pdf" href="http://papers.nips.cc/paper/4479-map-inference-for-bayesian-inverse-reinforcement-learning.pdf">pdf</a></p><p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] S. Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of COLT, 1998.</p>
<p>[2] P. R. Montague and G. S. Berns. Neural economics and the biological substrates of valuation. Neuron, 36(2), 2002.</p>
<p>[3] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5), 2009.</p>
<p>[4] Y. Niv. Reinforcement learning in the brain. Journal of Mathematical Psychology, 53(3), 2009.</p>
<p>[5] E. Hopkins. Adaptive learning models of consumer behavior. Journal of Economic Behavior and Organization, 64(3–4), 2007.</p>
<p>[6] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proceedings of ICML, 2000.</p>
<p>[7] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of ICML, 2004.</p>
<p>[8] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In Proceedings of ICML, 2006.</p>
<p>[9] G. Neu and C. Szepesv´ ri. Apprenticeship learning using inverse reinforcement learning and gradient a methods. In Proceedings of UAI, 2007.</p>
<p>[10] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proceedings of NIPS, 2008.</p>
<p>[11] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI, 2008.</p>
<p>[12] G. Neu and C. Szepesv´ ri. Training parsers by inverse reinforcement learning. Machine Learning, 77(2), a 2009.</p>
<p>[13] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proceedings of IJCAI, 2007.</p>
<p>[14] A. Boularias and B. Chaib-Draa. Bootstrapping apprenticeship learning. In Proceedings of NIPS, 2010.</p>
<p>[15] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. In Proceedings of IJCAI, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
