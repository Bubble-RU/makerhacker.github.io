<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2011-How biased are maximum entropy models?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-123" href="../nips2011/nips-2011-How_biased_are_maximum_entropy_models%3F.html">nips2011-123</a> <a title="nips-2011-123-reference" href="#">nips2011-123-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>123 nips-2011-How biased are maximum entropy models?</h1>
<br/><p>Source: <a title="nips-2011-123-pdf" href="http://papers.nips.cc/paper/4357-how-biased-are-maximum-entropy-models.pdf">pdf</a></p><p>Author: Jakob H. Macke, Iain Murray, Peter E. Latham</p><p>Abstract: Maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mutual information in biological systems. However, maximum entropy models ﬁt to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations. However, in practice, the true distribution is usually outside the model class, and we show here that this misspeciﬁcation can lead to much larger bias. We provide a perturbative approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data. 1</p><br/>
<h2>reference text</h2><p>[1] C.E. Shannon and W. Weaver. The mathematical theory of communication. University of Illinois Press, 1949.</p>
<p>[2] T.M. Cover, J.A. Thomas, J. Wiley, et al. Elements of information theory, volume 6. Wiley Online Library, 1991.</p>
<p>[3] F. Rieke, D. Warland, R. de R uytervansteveninck, and W. Bialek. Spikes: exploring the neural code (computational neuroscience). The MIT Press, 1999.  8</p>
<p>[4] A. Borst and F. E. Theunissen. Information theory and neural coding. Nat Neurosci, 2(11):947–957, 1999 Nov.</p>
<p>[5] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191–1253, 2003.</p>
<p>[6] B. B. Averbeck, P. E. Latham, and A. Pouget. Neural correlations, population coding and computation. Nature Reviews Neuroscience, 7(5):358–66, 2006.</p>
<p>[7] R. Quian Quiroga and S. Panzeri. Extracting information from neuronal populations: information theory and decoding approaches. Nat Rev Neurosci, 10(3):173–185, 2009.</p>
<p>[8] G. Miller. Note on the bias of information estimates. In Information Theory in Psychology II-B, chapter 95-100. Free Press, Glencole, IL, 1955.</p>
<p>[9] A. Treves and S. Panzeri. The upward bias in measures of information derived from limited data samples. Neural Computation, 7(2):399–407, 1995.</p>
<p>[10] S. Panzeri, R. Senatore, M. A. Montemurro, and R. S. Petersen. Correcting for the sampling bias problem in spike train information measures. J Neurophysiol, 98(3):1064–1072, 2007.</p>
<p>[11] Robin A A Ince, Alberto Mazzoni, Rasmus S Petersen, and Stefano Panzeri. Open source tools for the information theoretic analysis of neural data. Front Neurosci, 4, 2010.</p>
<p>[12] E. Ising. Beitrag zur Theorie des Ferromagnetismus. Z. Phys, 31:253, 1925.</p>
<p>[13] E. Schneidman, M. J. 2nd Berry, R. Segev, and W. Bialek. Weak pairwise correlations imply strongly correlated network states in a neural population. Nature, 440(7087):1007–12, 2006.</p>
<p>[14] J. Shlens, G. D. Field, J. L. Gauthier, M. I. Grivich, D. Petrusca, A. Sher, A. M. Litke, and E. J. Chichilnisky. The structure of multi-neuron ﬁring patterns in primate retina. J Neurosci, 26(32):8254–66, 2006.</p>
<p>[15] I. E. Ohiorhenuan, F. Mechler, K. P. Purpura, A. M. Schmid, Q. Hu, and J. D. Victor. Sparse coding and high-order correlations in ﬁne-scale cortical networks. Nature, 466(7306):617–621, 2010.</p>
<p>[16] G. Tkacik, E. Schneidman, M. J. Berry, II, and W. Bialek. Spin glass models for a network of real neurons. arXiv:q-bio/0611072v2, 2009.</p>
<p>[17] Y. Roudi, J. Tyrcha, and J. Hertz. Ising model for neural data: model quality and approximate methods for extracting functional connectivity. Phys Rev E Stat Nonlin Soft Matter Phys, 79(5 Pt 1):051915, May 2009.</p>
<p>[18] Y. Roudi, E. Aurell, and J. A. Hertz. Statistical physics of pairwise probability models. Front Comput Neurosci, 3:22, 2009.</p>
<p>[19] T. Mora, A. M. Walczak, W. Bialek, and C. G. Jr Callan. Maximum entropy models for antibody diversity. Proc Natl Acad Sci U S A, 107(12):5405–5410, 2010.</p>
<p>[20] A.W. Van der Vaart. Asymptotic statistics. Cambridge Univ Pr, 2000.</p>
<p>[21] J.H. Macke, P. Berens, A.S. Ecker, A.S. Tolias, and M. Bethge. Generating spike trains with speciﬁed correlation coefﬁcients. Neural Computation, 21(2):397–423, 2009.</p>
<p>[22] J.H. Macke, M. Opper, and M. Bethge. Common input explains higher-order correlations and entropy in a simple model of neural population activity. Physical Review Letters, 106(20):208102, 2011.</p>
<p>[23] I. Nemenman, W. Bialek, and R.D.R. Van Steveninck. Entropy and information in neural spike trains: Progress on the sampling problem. Physical Review E, 69(5):056111, 2004.</p>
<p>[24] N.A. Ahmed and D. V. Gokhale. Entropy expressions and their estimators for multivariate distributions. Information Theory, IEEE Transactions on, 35(3):688–692, 1989.</p>
<p>[25] O. Oyman, R. U. Nabar, H. Bolcskei, and A. J. Paulraj. Characterizing the statistical properties of mutual information in MIMO channels: insights into diversity-multiplexing tradeoff. In Signals, Systems and Computers, 2002. Conference Record of the Thirty-Sixth Asilomar Conference on, volume 1, pages 521– 525. IEEE, 2002.</p>
<p>[26] N. Misra, H. Singh, and E. Demchuk. Estimation of the entropy of a multivariate normal distribution. Journal of multivariate analysis, 92(2):324–342, 2005.</p>
<p>[27] G. Marrelec and H. Benali. Large-sample asymptotic approximations for the sampling and posterior distributions of differential entropy for multivariate normal distributions. Entropy, 13(4):805–819, 2011.</p>
<p>[28] S. Srivastava and M.R. Gupta. Bayesian estimation of the entropy of the multivariate Gaussian. In Information Theory, 2008. ISIT 2008. IEEE International Symposium on, pages 1103–1107. IEEE, 2008.</p>
<p>[29] N.R. Goodman. The distribution of the determinant of a complex Wishart distributed matrix. The Annals of mathematical statistics, 34(1):178–180, 1963.</p>
<p>[30] M. Gupta and S. Srivastava. Parametric Bayesian estimation of differential entropy and relative entropy. Entropy, 12(4):818–843, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
