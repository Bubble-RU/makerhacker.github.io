<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-183" href="../nips2011/nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">nips2011-183</a> <a title="nips-2011-183-reference" href="#">nips2011-183-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</h1>
<br/><p>Source: <a title="nips-2011-183-pdf" href="http://papers.nips.cc/paper/4317-neural-reconstruction-with-approximate-message-passing-neuramp.pdf">pdf</a></p><p>Author: Alyson K. Fletcher, Sundeep Rangan, Lav R. Varshney, Aniruddha Bhargava</p><p>Abstract: Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear ﬁltering stage that produces a lowdimensional signal that drives subsequent nonlinear stages. This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive ﬁelds in sensory neurons. The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method. The GAMP method is based on Gaussian approximations of loopy belief propagation. In the neural connectivity problem, the GAMP-based method is shown to be computational efﬁcient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and signiﬁcantly outperforms previous compressed-sensing methods. For the receptive ﬁeld estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights. The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive ﬁelds of salamander retinal ganglion cells. 1</p><br/>
<h2>reference text</h2><p>[1] Peter Dayan and L. F. Abbott. Theoretical Neuroscience. Computational and Mathematical Modeling of Neural Systems. MIT Press, 2001.</p>
<p>[2] Odelia Schwartz, Jonathan W. Pillow, Nicole C. Rust, and Eero P. Simoncelli. Spike-triggered neural characterization. J. Vis., 6(4):13, July 2006.</p>
<p>[3] Liam Paninski, Jonathan W. Pillow, and Eero P. Simoncelli. Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Encoding Model. Neural Computation, 16(12):2533– 2561, December 2004.</p>
<p>[4] Tao Hu and Dmitri B. Chklovskii. Reconstruction of sparse circuits using multi-neuronal excitation (RESCUME). In Yoshua Bengio, Dale Schuurmans, John Lafferty, Chris Williams, and Aron Culotta, editors, Advances in Neural Information Processing Systems 22, pages 790– 798. MIT Press, Cambridge, MA, 2009.</p>
<p>[5] James R. Anderson, Bryan W. Jones, Carl B. Watt, Margaret V. Shaw, Jia-Hui Yang, David DeMill, James S. Lauritzen, Yanhua Lin, Kevin D. Rapp, David Mastronarde, Pavel Koshevoy, Bradley Grimm, Tolga Tasdizen, Ross Whitaker, and Robert E. Marc. Exploring the retinal connectome. Mol. Vis, 17:355–379, February 2011.</p>
<p>[6] Elad Ganmor, Ronen Segev, and Elad Schneidman. The architecture of functional interaction networks in the retina. J. Neurosci., 31(8):3044–3054, February 2011.</p>
<p>[7] Lav R. Varshney, Per Jesper Sj¨ str¨ m, and Dmitri B. Chklovskii. Optimal information storage o o in noisy synapses under resource constraints. Neuron, 52(3):409–423, November 2006.</p>
<p>[8] E. J. Cand` s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruce tion from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489– 509, February 2006.</p>
<p>[9] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, April 2006. 8</p>
<p>[10] E. J. Cand` s and T. Tao. Near-optimal signal recovery from random projections: Universal e encoding strategies? IEEE Trans. Inform. Theory, 52(12):5406–5425, December 2006.</p>
<p>[11] S. Rangan. Generalized Approximate Message Passing for Estimation with Random Linear Mixing. arXiv:1010.5141 [cs.IT]., October 2010.</p>
<p>[12] S. Rangan, A.K. Fletcher, V.K.Goyal, and P. Schniter. Hybrid Approximate Message Passing with Applications to Group Sparsity . arXiv, 2011.</p>
<p>[13] D. Guo and C.-C. Wang. Random sparse linear systems observed via arbitrary channels: A decoupling principle. In Proc. IEEE Int. Symp. Inform. Th., pages 946 – 950, Nice, France, June 2007.</p>
<p>[14] David L. Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. PNAS, 106(45):18914–18919, September 2009.</p>
<p>[15] David H. Hubel. Eye, Brain, and Vision. W. H. Freeman, 2nd edition, 1995.</p>
<p>[16] Toshihiko Hosoya, Stephen A. Baccus, and Markus Meister. Dynamic predictive coding by the retina. Nature, 436(7047):71–77, July 2005.</p>
<p>[17] E. J. Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Computation in Neural Systems., 12:199–213, 2001.</p>
<p>[18] L. Paninski. Convergence properties of some spike-triggered analysis techniques. Network: Computation in Neural Systems, 14:437–464, 2003.</p>
<p>[19] S. Bakin. Adaptive regression and model selection in data mining problems. PhD thesis, Australian National University, Canberra, 1999.</p>
<p>[20] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Royal Statist. Soc., 68:49–67, 2006.</p>
<p>[21] Lukas Meier, Sara van de Geer, and Peter B¨ hlmann. Model selection and estimation in reu gression with grouped variables. J. Royal Statist. Soc., 70:53–71, 2008. ´</p>
<p>[22] Aur´ lie C. Lozano, Grzegorz Swirszcz, and Naoki Abe. Group orthogonal matching pursuit e for variable selection and prediction. In Proc. NIPS, Vancouver, Canada, December 2008.</p>
<p>[23] C. M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, New York, NY, 2006.</p>
<p>[24] Markus Meister, Jerome Pine, and Denis A. Baylor. Multi-neuronal signals from the retina: acquisition and analysis. J. Neurosci. Methods, 51(1):95–106, January 1994.</p>
<p>[25] Joaquin Rapela, Jerry M. Mendel, and Norberto M. Grzywacz. Estimating nonlinear receptive ﬁelds from natural images. J. Vis., 6(4):11, May 2006.</p>
<p>[26] D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Appl. Comput. Harm. Anal., 26(3):301–321, May 2009.</p>
<p>[27] W. Dai and O. Milenkovic. Subspace pursuit for compressive sensing signal reconstruction. IEEE Trans. Inform. Theory, 55(5):2230–2249, May 2009.</p>
<p>[28] Dmitri B. Chklovskii, Bartlett W. Mel, and Karel Svoboda. Cortical rewiring and information storage. Nature, 431(7010):782–788, October 2004.</p>
<p>[29] Tai Sing Lee and David Mumford. Hierarchical bayesian inference in the visual cortex. J. Opt. Soc. Am. A, 20(7):1434–1448, July 2003.</p>
<p>[30] Karl Friston. The free-energy principle: a uniﬁed brain theory? Nat. Rev. Neurosci., 11(2):127– 138, February 2010.</p>
<p>[31] Guy Isely, Christopher J. Hillar, and Friedrich T. Sommer. Decyphering subsampled data: Adaptive compressive sampling as a principle of brain communication. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 910–918. MIT Press, Cambridge, MA, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
