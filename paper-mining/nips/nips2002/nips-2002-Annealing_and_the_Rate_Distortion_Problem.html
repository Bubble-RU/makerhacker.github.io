<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2002-Annealing and the Rate Distortion Problem</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-30" href="#">nips2002-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2002-Annealing and the Rate Distortion Problem</h1>
<br/><p>Source: <a title="nips-2002-30-pdf" href="http://papers.nips.cc/paper/2264-annealing-and-the-rate-distortion-problem.pdf">pdf</a></p><p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>Reference: <a title="nips-2002-30-reference" href="../nips2002_reference/nips-2002-Annealing_and_the_Rate_Distortion_Problem_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yn', 0.59), ('bifurc', 0.507), ('qk', 0.419), ('distort', 0.265), ('dk', 0.115), ('maxq', 0.111), ('def', 0.099), ('subcrit', 0.095), ('sm', 0.082), ('montan', 0.079), ('branch', 0.07), ('pointw', 0.069), ('gedeon', 0.063), ('ker', 0.063), ('minq', 0.063), ('symmetry', 0.053), ('unresolv', 0.05), ('yk', 0.05), ('imax', 0.047), ('singul', 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="30-tfidf-1" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>2 0.20419489 <a title="30-tfidf-2" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>3 0.14425406 <a title="30-tfidf-3" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>Author: Sumio Watanabe, Shun-ichi Amari</p><p>Abstract: A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coeﬃcient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general. 1</p><p>4 0.086493976 <a title="30-tfidf-4" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The problem of extracting the relevant aspects of data, in face of multiple conﬂicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.</p><p>5 0.070867158 <a title="30-tfidf-5" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>Author: Dmitri B. Chklovskii, Armen Stepanyants</p><p>Abstract: What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d</p><p>6 0.059084199 <a title="30-tfidf-6" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>7 0.051909208 <a title="30-tfidf-7" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>8 0.045142181 <a title="30-tfidf-8" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>9 0.0422251 <a title="30-tfidf-9" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<p>10 0.034408115 <a title="30-tfidf-10" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>11 0.029496882 <a title="30-tfidf-11" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>12 0.029474741 <a title="30-tfidf-12" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>13 0.028989337 <a title="30-tfidf-13" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>14 0.028590916 <a title="30-tfidf-14" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<p>15 0.028587947 <a title="30-tfidf-15" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>16 0.02830964 <a title="30-tfidf-16" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>17 0.027955515 <a title="30-tfidf-17" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>18 0.027866958 <a title="30-tfidf-18" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>19 0.027758934 <a title="30-tfidf-19" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>20 0.027227217 <a title="30-tfidf-20" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.08), (1, 0.023), (2, -0.014), (3, 0.002), (4, -0.03), (5, -0.019), (6, -0.049), (7, 0.014), (8, -0.018), (9, -0.012), (10, 0.018), (11, -0.011), (12, -0.065), (13, -0.049), (14, 0.026), (15, 0.006), (16, 0.006), (17, 0.016), (18, -0.004), (19, 0.039), (20, 0.063), (21, -0.039), (22, -0.089), (23, 0.318), (24, 0.071), (25, -0.161), (26, 0.292), (27, 0.121), (28, -0.047), (29, 0.034), (30, 0.206), (31, -0.049), (32, -0.192), (33, 0.02), (34, -0.159), (35, -0.038), (36, -0.263), (37, 0.024), (38, 0.056), (39, -0.066), (40, -0.049), (41, 0.105), (42, 0.033), (43, -0.01), (44, 0.014), (45, 0.098), (46, 0.115), (47, 0.104), (48, 0.054), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9685896 <a title="30-lsi-1" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>2 0.76585358 <a title="30-lsi-2" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>3 0.47655743 <a title="30-lsi-3" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>Author: Sumio Watanabe, Shun-ichi Amari</p><p>Abstract: A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coeﬃcient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general. 1</p><p>4 0.32280251 <a title="30-lsi-4" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The problem of extracting the relevant aspects of data, in face of multiple conﬂicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.</p><p>5 0.2372528 <a title="30-lsi-5" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>Author: Matthias Bethge, David Rotermund, Klaus Pawelzik</p><p>Abstract: Here we derive optimal gain functions for minimum mean square reconstruction from neural rate responses subjected to Poisson noise. The shape of these functions strongly depends on the length T of the time window within which spikes are counted in order to estimate the underlying firing rate. A phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero. For a particular function class, we were able to prove the existence of a second-order phase transition analytically. The critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical results. We conclude that under most circumstances relevant to information processing in the brain, rate coding can be better ascribed to a binary (low-entropy) code than to the other extreme of rich analog coding. 1 Optimal neuronal gain functions for short decoding time windows The use of action potentials (spikes) as a means of communication is the striking feature of neurons in the central nervous system. Since the discovery by Adrian [1] that action potentials are generated by sensory neurons with a frequency that is substantially determined by the stimulus, the idea of rate coding has become a prevalent paradigm in neuroscience [2]. In particular, today the coding properties of many neurons from various areas in the cortex have been characterized by tuning curves, which describe the average firing rate response as a function of certain stimulus parameters. This way of description is closely related to the idea of analog coding, which constitutes the basis for many neural network models. Reliabl v inference from the observed number of spikes about the underlying firing rate of a neuronal response, however, requires a sufficiently long time interval, while integration times of neurons in vivo [3] as well as reaction times of humans or animals when performing classification tasks [4, 5] are known to be rather short. Therefore, it is important to understand, how neural rate coding is affected by a limited time window available for decoding. While rate codes are usually characterized by tuning functions relating the intensity of the ,f * http://www.neuro.urn-bremen.dermbethge neuronal response to a particular stimulus parameter, the question, how relevant the idea of analog coding actually is does not depend on the particular entity represented by a neuron. Instead it suffices to determine the shape of the gain function, which displays the mean firing rate as a function of the actual analog signal to be sent to subsequent neurons. Here we seek for optimal gain functions that minimize the minimum average squared reconstruction error for a uniform source signal transmitted through a Poisson channel as a function of the maximum mean number of spikes. In formal terms, the issue is to optimally encode a real random variable x in the number of pulses emitted by a neuron within a certain time window. Thereby, x stands for the intended analog output of the neuron that shall be signaled to subsequent neurons. The latter, however, can only observe a number of spikes k integrated within a time interval of length T. The statistical dependency between x and k is specified by the assumption of Poisson noise p(kIJL(x)) = (JL~))k exp{ -JL(X)} , (1) and the choice of the gain function f(x), which together with T determines the mean spike count J.L(x) == T f(x) . An important additional constraint is the limited output range of the neuronal firing rate, which can be included by the requirement of a bounded gain function (fmin :::; f (x) :::; f max, VX). Since inhibition can reliably prevent a neuron from firing, we will here consider the case f min == 0 only. Instead of specifying f max, we impose a bound directly on the mean spike count (i.e. J.L(x) :::; /l), because f max constitutes a meaningful constraint only in conjunction with a fixed time window length T. As objective function we consider the minimum mean squared error (MMSE) with respect to Lebesgue measure for x E [0, 1], ~ 2 X _ E x2 _ E (i2 _ _ [jt( )] - [] [] - 3 X ~ (Xl (J01 xp(kIJL(x)) dx r J01p(kIJL(x)) dx' (2) where x(k) == E[xlk] denotes the mean square estimator, which is the conditional expectation (see e.g. [6]). 1.1 Tunings and errors As derived in [7] on the basis of Fisher information the optimal gain function for a single neuron in the asymptotic limit T -+ 00 has a parabolic shape: fasymp(x) == fmaxx2 . (3) For any finite /l, however, this gain function is not necessarily optimal, and in the limit T -+ 0, it is straight forward to show that the optimal tuning curve is a step function f step (xl'19) == fmax 8 (x - {)) , (4) where 8(z) denotes the Heaviside function that equals one, if z > 0 and zero if z < O. The optimal threshold 'l9(p,) of the step tuning curve depends on /l and can be determined analytically 11(-) =1_ It 3 - V8e-J.' +1 4(1 - e- il ) (5) as well as the corresponding MMSE [8]: 2 2[fste p] _ 1 ( 3'19 (p,) ) X - 12 1 - [(1 -11(p))(l - e-iL)]-1 - 1 . (6) 1 S +1 0.5 CJ;) o ........ '------'-----'---'---'--'~----'----'-- ~---'---'---'--'~ 10-1 ~---,.---,---.,...............---.----.---.---.-.......-.-.--.-~ ...............~ Figure 1: The upper panel shows a bifurcation plot for {}(Jt) - wand {}(Jt) + w of the optimal gain function in 51 as a function of {t illustrating the phase transition from binary to continuous encoding. The dotted line separates the regions before and after the phase transition in all three panels. Left from this line (i.e. for Jt < Jt C) the step function given by Eq. 4+5 is optimal. The middle panel shows the MMSE of this step function (dashed) and of the optimal gain function in 52 (solid), which becomes smaller than the first one after the phase transition. The relative deviation between the minimal errors of 51 and 52 (i.e. (X~l - X~2)/X~2) is displayed in the lower panel and has a maximum below 0.035. The binary shape for small {t and the continuous parabolic shape for large {t implies that there has to be a transition from discrete to analog encoding with increasing {to Unfortunately it is not possible to determine the optimal gain function within the set of all bounded functions B :== {fli : [0, 1] -+ [0, fmax]} and hence, one has to choose a certain parameterized function space 5 c B in advance that is feasible for the optimization. In [8], we investigated various such function-'spaces and for {t < 2.9, we did not find any gain function with an error smaller than the MMSE of the step function. Furthermore, we always observed a phase transition from binary to analog encoding at a critical {t C that depends only slightly on the function space. As one can see in Fig. 1 (upper) pc is approximately three. In this paper, we consider two function classes 51, 52, which both contain the binary gain function as well as the asymptotic optimal parabolic function as special cases. Furthermore 51 is a proper subset of 52. Our interest in 51 results from the fact that we can analyze the phase transition in this subset analytically, while 52 is the most general parameterization for which we have. determined the optimal encoding numerically. The latter has six free parameters a :::; b :::; c E [0, 1], fmid E (0, fmax), a, f3 E [0,00) and the parameterization of the gain functions is given by o fS2 (xla, b, c, fmid, a, (3) fmid ( ~=: == , O</p><p>6 0.22139327 <a title="30-lsi-6" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>7 0.19706036 <a title="30-lsi-7" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>8 0.16812211 <a title="30-lsi-8" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>9 0.15265441 <a title="30-lsi-9" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>10 0.14617869 <a title="30-lsi-10" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<p>11 0.14215411 <a title="30-lsi-11" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>12 0.13911754 <a title="30-lsi-12" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>13 0.1385676 <a title="30-lsi-13" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>14 0.1350895 <a title="30-lsi-14" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>15 0.13502026 <a title="30-lsi-15" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>16 0.12919039 <a title="30-lsi-16" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>17 0.12867114 <a title="30-lsi-17" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>18 0.11547727 <a title="30-lsi-18" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>19 0.10986064 <a title="30-lsi-19" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>20 0.10799587 <a title="30-lsi-20" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.02), (22, 0.086), (26, 0.027), (39, 0.031), (47, 0.063), (48, 0.086), (54, 0.015), (63, 0.012), (66, 0.05), (72, 0.036), (79, 0.4), (93, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76296788 <a title="30-lda-1" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>2 0.36665651 <a title="30-lda-2" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>3 0.36493063 <a title="30-lda-3" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>4 0.36479649 <a title="30-lda-4" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>5 0.36278707 <a title="30-lda-5" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>6 0.35963181 <a title="30-lda-6" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>7 0.35937554 <a title="30-lda-7" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>8 0.35677516 <a title="30-lda-8" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>9 0.35650286 <a title="30-lda-9" href="./nips-2002-A_Differential_Semantics_for_Jointree_Algorithms.html">4 nips-2002-A Differential Semantics for Jointree Algorithms</a></p>
<p>10 0.35508209 <a title="30-lda-10" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>11 0.35432953 <a title="30-lda-11" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>12 0.35409552 <a title="30-lda-12" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>13 0.35406128 <a title="30-lda-13" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>14 0.35293353 <a title="30-lda-14" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>15 0.35277766 <a title="30-lda-15" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>16 0.3526051 <a title="30-lda-16" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>17 0.35259062 <a title="30-lda-17" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>18 0.35185561 <a title="30-lda-18" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>19 0.35164842 <a title="30-lda-19" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>20 0.35101694 <a title="30-lda-20" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
