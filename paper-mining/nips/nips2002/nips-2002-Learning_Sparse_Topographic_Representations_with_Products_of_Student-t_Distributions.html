<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-127" href="#">nips2002-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</h1>
<br/><p>Source: <a title="nips-2002-127-pdf" href="http://papers.nips.cc/paper/2177-learning-sparse-topographic-representations-with-products-of-student-t-distributions.pdf">pdf</a></p><p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>Reference: <a title="nips-2002-127-reference" href="../nips2002_reference/nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  ¡     Abstract We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. [sent-6, score-0.291]
</p><p>2 If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. [sent-8, score-0.33]
</p><p>3 Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. [sent-9, score-0.334]
</p><p>4 Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images. [sent-10, score-0.202]
</p><p>5 1 Introduction Historically, two different classes of statistical model have been used for natural images. [sent-11, score-0.09]
</p><p>6 “Energy-based” models assign to each image a global energy, , that is the sum of a number of local contributions and they deﬁne the probability of an image to be proportional to . [sent-12, score-0.287]
</p><p>7 It is difﬁcult to perform maximum likelihood ﬁtting on most energy-based models because of the normalization term (the partition function) that is required to convert to a probability. [sent-14, score-0.109]
</p><p>8 The normalization term is a sum over all possible images and its derivative w. [sent-15, score-0.079]
</p><p>9 ¢   ¢   ¢   ¨ ¦ ¤ ©§¥£   ¨ ¦ ¤ ©§£  The other class of model uses a “causal” directed acyclic graph in which the lowest level nodes correspond to pixels and the probability distribution at a node (in the absence of any observations) depends only on its parents. [sent-20, score-0.12]
</p><p>10 When the graph is singly or very sparsely  connected there are efﬁcient algorithms for maximum likelihood ﬁtting but if nodes have many parents, it is hard to perform maximum likelihood ﬁtting because this requires the intractable posterior distribution over non-leaf nodes given the pixel values. [sent-21, score-0.324]
</p><p>11 There is much debate about which class of model is the most appropriate for natural images. [sent-22, score-0.09]
</p><p>12 Is a particular image best characterized by the states of some hidden variables in a causal generative model? [sent-23, score-0.416]
</p><p>13 In this paper we treat violations of constraints as contributions to a global energy and we show how to learn a large set of constraints each of which is normally satisﬁed fairly accurately but occasionally violated by a lot. [sent-27, score-0.271]
</p><p>14 The ability to learn efﬁciently without ever having to generate equilibrium samples from the model and without having to confront the intractable partition function removes a major obstacle to the use of energy-based models. [sent-28, score-0.228]
</p><p>15 The distribution represented by a PoE is simply the normalized product of all the distributions represented by the individual “experts”: §  (1)   ©  ¨ ©  ¡  ¥ ¨     ¡ ¢¨  £ ¤ ¦     © ¦  ©  ¨ ©  ¡  are un-normalized experts and denotes the overall normalization constant. [sent-30, score-0.297]
</p><p>16 where In the product of Student-t (PoT) model, un-normalized experts have the following form,     ¥  (2)  3 42© 1 ( & )' 0  "  ¡    $© # %¢¨    ! [sent-31, score-0.297]
</p><p>17 £  ©  5  6  $© #  ¡  , the energy of the PoT model becomes "     ¡  # $© ¢¨  § ¥ F  ! [sent-35, score-0.136]
</p><p>18 ¥  ¨  D B A EC)© 0    ©  @ £ 9  ¦  8 ' 7 ¨  7 ¨    ¢   ¨ ¦ ¤ ©§£  £   7 ¢¨     Deﬁning  (3)  ¢  Viewed this way, the model takes the form of a maximum entropy distribution with weights on real-valued “features” of the image. [sent-36, score-0.226]
</p><p>19 Unlike previous maximum entropy models, however, we can ﬁt both the weights and the features at the same time. [sent-37, score-0.184]
</p><p>20 ©  0  When the number of input dimensions is equal to the number of experts, the normally intractable partition function becomes a determinant and the PoT model becomes equivalent to a noiseless ICA model with Student-t prior distributions [2]. [sent-38, score-0.303]
</p><p>21 So noiseless ICA can be viewed as an energy-based model even though it is usually interpeted as a causal generative model in which the posterior over the hidden variables collapses to a point. [sent-40, score-0.419]
</p><p>22 However, when we consider more experts than input dimensions (i. [sent-41, score-0.341]
</p><p>23 an overcomplete representation), the energy-based view and the causal generative view lead to different generalizations of ICA. [sent-43, score-0.322]
</p><p>24 The natural causal generalization retains the independence of the hidden variables in the prior by assuming independent sources. [sent-44, score-0.342]
</p><p>25 In contrast, the PoT model simply multiplies together more experts than input dimensions and re-normalizes to get the total probability. [sent-45, score-0.43]
</p><p>26   I P#  £ HG  3 Training the PoT Model with Contrastive Divergence When training energy-based models we need to shape the energy function so that observed images have low energy and empty regions in the space of all possible images have high energy. [sent-46, score-0.346]
</p><p>27 The maximum likelihood learning rule is given by, (4)  ¤ ¥£  ¢  & $  ¦ '"%"#""! [sent-47, score-0.072]
</p><p>28 A much more efﬁcient way to ﬁt the model is to use the data distribution itself to initialize a Markov Chain which then starts moving towards the model’s equilibrium distribution. [sent-49, score-0.097]
</p><p>29 This is done by lowering the energy of the data and raising the energy of the “confabulations” produced by a few steps of MCMC. [sent-51, score-0.188]
</p><p>30 To speed up learning we need a Markov chain that mixes rapidly so that the confabulations will be some distance away from the data. [sent-55, score-0.16]
</p><p>31 Rapid mixing can be achieved by alternately Gibbs sampling a set of hidden variables given the random variables under consideration and vice versa. [sent-56, score-0.269]
</p><p>32 the variables can be interpreted as precision variables in the transformed space In this respect our model resembles a “Gaussian scale mixture” (GSM) [8] which also multiplies a positive scaling variable with a normal variate. [sent-63, score-0.389]
</p><p>33 But GSM is a causal model while PoT is energy-based. [sent-64, score-0.149]
</p><p>34 ¡  $  #  £    B  q  The (in)dependency relations between the variables in a PoT model are depicted graphically in ﬁgure (1a,b). [sent-65, score-0.124]
</p><p>35 The hidden variables are independent given , which allows them to be Gibbs-sampled in parallel. [sent-66, score-0.187]
</p><p>36 ¡  To learn the parameters of the PoT model we thus propose to iterate the following steps:      B  1) Sample given the data distribution (7). [sent-68, score-0.089]
</p><p>37 (b)-Expanded graph where the deterministic   ¨ ©    relation (dashed lines) between the random variable and the activities of the ﬁlters is made explicit. [sent-70, score-0.111]
</p><p>38 (d)-Filters with large (decreasing from left to right) weights into a particular top level unit . [sent-72, score-0.109]
</p><p>39 Top level units have learned to connect to ﬁlters similar in frequency, location and orientation. [sent-73, score-0.177]
</p><p>40 for every  ¡  3) Update the parameters according to (5) where the “k-step samples” are now given by the reconstructions , the energy is given by (3), and the parameters are given . [sent-76, score-0.146]
</p><p>41 by    ¡  ¡  @ A   " #  #  £    4 Overcomplete Representations The above learning rules are still valid for overcomplete representations. [sent-77, score-0.215]
</p><p>42 In contrast, for the overcomplete case we ought to perform a Cholesky factorization on for each data-vector separately. [sent-81, score-0.254]
</p><p>43 #  $  $  PV   I  $ I  #  I $ ¤#  PV   I  $ #     #  From experiments we have also found that in the overcomplete case we should ﬁx the norm of the ﬁlters, , in order to prevent some of them from decaying to zero. [sent-83, score-0.246]
</p><p>44 Since controlling the norm removes the ability of the experts to adapt to scale it is necessary to whiten the data ﬁrst. [sent-85, score-0.362]
</p><p>45 1 Experiment: Overcomplete Representations for Natural Images ¥  ¥  @  We randomly generated patches of pixels from images of natural scenes 1 . [sent-87, score-0.225]
</p><p>46 The patches were centered and sphered using PCA and the DC component (eigen-vector with largest variance) was removed. [sent-88, score-0.141]
</p><p>47 The algorithm for overcomplete representations using the pseudo-inverse was used to train experts, i. [sent-89, score-0.26]
</p><p>48 We ﬁxed the weights to have and the the ﬁlters to have a -norm of . [sent-92, score-0.109]
</p><p>49 ﬁ/projects/ica/data/images  E E GFD)3 ) 3  3  1  %  (  5 Topographically Ordered Features In [6] it was shown that linear ﬁltering of natural images is not enough to remove all higher order dependencies. [sent-100, score-0.127]
</p><p>50 In particular, it was argued that there are residual dependencies among the activities of the ﬁltered inputs. [sent-101, score-0.156]
</p><p>51 It is therefore desirable to model those dependencies within the PoT model. [sent-102, score-0.12]
</p><p>52 By inspection of ﬁgure (1b) we note that these dependencies can be modelled through a non-negative weight matrix , which connects . [sent-103, score-0.121]
</p><p>53 The resultant model is depicted in ﬁgure the hidden variables with the activities (1c). [sent-104, score-0.307]
</p><p>54 Depending on how many nonzero weights emanate from a hidden unit (say ), each expert now occupies input dimensions instead of just one. [sent-105, score-0.31]
</p><p>55 The expressions for these richer experts can be obtained from (2) by replacing, . [sent-106, score-0.297]
</p><p>56 We have found that learning is assisted by ﬁxing the -norm of the weights ( ). [sent-107, score-0.109]
</p><p>57     The larger the value for  (9)  Joint and conditional distributions over hidden variables are obtained through similar replacements in eqn. [sent-111, score-0.187]
</p><p>58 Sampling the reconstructions given the states of the hidden variables proceeds by ﬁrst sampling from independent generalized Laplace with precision parameters which are distributions subsequently transformed into . [sent-113, score-0.303]
</p><p>59 Learning in this model therefore proceeds with only minor modiﬁcations to the algorithm described in the previous section. [sent-114, score-0.072]
</p><p>60 "  ©    ¡  When we learn the weight matrix from image data we ﬁnd that a particular hidden develops weights to the activities of ﬁlters similar in frequency, location and variable orientation. [sent-117, score-0.514]
</p><p>61 The variables therefore integrate information from these ﬁlters and as a result develop certain invariances that resemble the behavior of complex cells. [sent-118, score-0.082]
</p><p>62 A similar approach was studied in [4] using a related causal model 2 in which a number of scale variables generate correlated variances for conditionally Gaussian experts. [sent-119, score-0.297]
</p><p>63 This results in topography when the scale-generating variables are non-adaptive and connect to a local neighborhood of ﬁlters only. [sent-120, score-0.214]
</p><p>64 ¡ ¢©     B  ©    We will now argue that ﬁxed local weights also give rise to topography in the PoT model. [sent-121, score-0.152]
</p><p>65 The reason is that averaging the squares of randomly chosen ﬁlter outputs (eqn. [sent-122, score-0.081]
</p><p>66 However, this “smoothing effect” may be largely avoided by averaging squared ﬁlter outputs that are highly correlated (i. [sent-124, score-0.081]
</p><p>67 Since the averaging is local, this results in a topographic layout of the ﬁlters. [sent-127, score-0.142]
</p><p>68 1 Experiment: Topographic Representations for Natural Images 3 ¥  3 ¥  @  image patches of size pixels in the same For this experiment we collected way as described in section (4. [sent-129, score-0.251]
</p><p>69 The image data were sphered and reduced to dimensions by removing low variance and high variance (DC) direction. [sent-131, score-0.254]
</p><p>70 We learned an overcomplete representation with experts which were organized on a square grid. [sent-132, score-0.547]
</p><p>71 Each expert connects with a ﬁxed weight of to itself and all its neighbors, where periodic boundary conditions were imposed for the experts on the boundary. [sent-133, score-0.428]
</p><p>72 (a)  (b)  Figure 2: (a)-Small subset of the  learned ﬁlters from a times overcomplete representation for natural image patches. [sent-135, score-0.42]
</p><p>73 The weights were ﬁxed and connect to neighbors only, using periodic boundary conditions. [sent-137, score-0.234]
</p><p>74 Neighboring ﬁlters have learned to be similar in frequency, location and orientation. [sent-138, score-0.088]
</p><p>75 We note that the weights topographic ordering on the experts, where location, scale and frequency of the Gabor-like ﬁlters all change smoothly across the map. [sent-142, score-0.332]
</p><p>76   £  "  £  "  #  %    3 ¥  In another experiment we used the same data to train a complete representation of experts where we learned the weights ( -norm ), and the ﬁlters (unconstrained), but with a ﬁxed value of . [sent-143, score-0.472]
</p><p>77 The weights and were kept positive by adapting their can now connect to any other expert we do not expect logarithm. [sent-144, score-0.25]
</p><p>78 To study whether the weights were modelling the dependencies between we ordered the ﬁlters for each complex cell the energies of the ﬁlter outputs according to the strength of the weights connecting to it. [sent-146, score-0.438]
</p><p>79 Since the cells connect to similar ﬁlters we may conclude that the weights are indeed learning the dependencies between the activities of the ﬁlter outputs. [sent-148, score-0.354]
</p><p>80 E  #  ¥  "    "  %    ¥      £      ©  "  $©  ¡      #  ¨  B  ¥  3    6 Denoising Images: The Iterated Wiener Filter If the PoT model provides an accurate description of the statistics of natural image data it ought to be a good prior for cleaning up noisy images. [sent-149, score-0.251]
</p><p>81 In the following we will apply this idea to denoise images contaminated with Gaussian pixel noise. [sent-150, score-0.114]
</p><p>82 We follow the standard Bayesian approach which states that the optimal estimate of the original image is given by the maximum a posteriori (MAP) estimate of , where denotes the noisy image. [sent-151, score-0.159]
</p><p>83 Since the second equation is just a Wiener ﬁlter with noise covariance and a Gaussian prior with covariance we have named the above denoising equations the iterated Wiener ﬁlter (IWF). [sent-163, score-0.194]
</p><p>84 1 Experiment: Denoising To test the iterated Wiener ﬁlter, we trained a complete set of experts on the data described in section (4. [sent-170, score-0.366]
</p><p>85 The norm of the ﬁlters was unconstrained, the were free to adapt, but we did not include any weights . [sent-172, score-0.14]
</p><p>86 The image shown in ﬁgure (3a) was corrupted with Gaussian noise with standard deviation , which resulted in a PSNR of dB (ﬁgure (3b)). [sent-173, score-0.122]
</p><p>87 The denoised image using adaptive Wiener ﬁltering has a PSNR of dB and is shown in ﬁgure (3c). [sent-176, score-0.192]
</p><p>88 Because the ﬁlters were trained on sphered data without a DC component, the same transformations have to be applied to the test patches before IWF is applied. [sent-178, score-0.141]
</p><p>89 The denoised image using dB, which is a signiﬁcant improvement of IWF is shown in (3d) and has a PSNR of dB over Wiener ﬁltering. [sent-179, score-0.192]
</p><p>90 It is our hope that the use of overcomplete representations and weights will further improve those results. [sent-180, score-0.369]
</p><p>91 E5  (  3 4F  3  ¥  )  3  )  (  ¥  $ #  ¥  5  (  F  F  5  ¥    7 Discussion It is well known that a wavelet transform de-correlates natural image data in good approximation. [sent-181, score-0.244]
</p><p>92 In [6] it was found that in the marginal distribution the wavelet coefﬁcients are sparsely distributed but that there are signiﬁcant residual dependencies among their energies . [sent-182, score-0.275]
</p><p>93 In this paper we have shown that the PoT model can learn highly overcomplete ﬁlters with sparsely distributed outputs. [sent-183, score-0.369]
</p><p>94 With a second hidden layer that is locally connected, it captures the dependencies between ﬁlter outputs by learning topographic representations. [sent-184, score-0.336]
</p><p>95 In the PoT model the hidden variables are conditionally independent so perceptual inference is very easy and does not require iterative settling even when the model is overcomplete. [sent-188, score-0.303]
</p><p>96 There is a fairly simple and efﬁcient procedure for learning all the parameters, including the weights connecting top-level units to ﬁlter outputs. [sent-189, score-0.109]
</p><p>97 Finally, the model leads to an elegant denoising algorithm which involves iterating a Wiener-ﬁlter. [sent-190, score-0.167]
</p><p>98 We thank Yee-Whye Teh for ﬁrst suggesting a related model and Peter Dayan for encouraging us to apply products of experts to topography. [sent-192, score-0.393]
</p><p>99 Modeling the joint statistics of images in the wavelet domain. [sent-231, score-0.153]
</p><p>100 Image denoising using a local Gaussian scale mixture model in the wavelet domain. [sent-237, score-0.275]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pot', 0.528), ('lters', 0.308), ('experts', 0.297), ('overcomplete', 0.215), ('wiener', 0.161), ('lter', 0.149), ('denoising', 0.125), ('image', 0.122), ('iwf', 0.117), ('weights', 0.109), ('topographic', 0.107), ('causal', 0.107), ('hidden', 0.105), ('energy', 0.094), ('connect', 0.089), ('gsm', 0.088), ('psnr', 0.088), ('sphered', 0.088), ('variables', 0.082), ('images', 0.079), ('dependencies', 0.078), ('activities', 0.078), ('wavelet', 0.074), ('gure', 0.073), ('denoised', 0.07), ('iterated', 0.069), ('sparsely', 0.065), ('contrastive', 0.065), ('confabulations', 0.059), ('jx', 0.059), ('poe', 0.059), ('spie', 0.059), ('pv', 0.058), ('energies', 0.058), ('equilibrium', 0.055), ('products', 0.054), ('location', 0.053), ('patches', 0.053), ('expert', 0.052), ('reconstructions', 0.052), ('mixes', 0.051), ('frequency', 0.051), ('ica', 0.05), ('normally', 0.05), ('chain', 0.05), ('natural', 0.048), ('intractable', 0.047), ('db', 0.047), ('learn', 0.047), ('multiplies', 0.047), ('shrinkage', 0.047), ('hyvarinen', 0.047), ('welling', 0.047), ('eca', 0.047), ('outputs', 0.046), ('representations', 0.045), ('pixels', 0.045), ('dimensions', 0.044), ('connects', 0.043), ('topography', 0.043), ('contributions', 0.043), ('model', 0.042), ('noiseless', 0.041), ('simon', 0.041), ('logarithm', 0.041), ('della', 0.041), ('unconstrained', 0.039), ('pietra', 0.039), ('ought', 0.039), ('teh', 0.039), ('entropy', 0.038), ('ordered', 0.038), ('meeting', 0.037), ('violated', 0.037), ('partition', 0.037), ('maximum', 0.037), ('dc', 0.036), ('resembles', 0.036), ('periodic', 0.036), ('likelihood', 0.035), ('pixel', 0.035), ('averaging', 0.035), ('learned', 0.035), ('gatsby', 0.034), ('boltzmann', 0.034), ('scale', 0.034), ('transformed', 0.034), ('inverse', 0.034), ('ef', 0.034), ('graph', 0.033), ('conditionally', 0.032), ('mcmc', 0.032), ('normal', 0.032), ('markov', 0.031), ('smoothly', 0.031), ('isotropic', 0.031), ('experiment', 0.031), ('norm', 0.031), ('proceeds', 0.03), ('toronto', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="127-tfidf-1" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>2 0.17039926 <a title="127-tfidf-2" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>3 0.15751739 <a title="127-tfidf-3" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>4 0.1460779 <a title="127-tfidf-4" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>Author: Max Welling, Richard S. Zemel, Geoffrey E. Hinton</p><p>Abstract: Boosting algorithms and successful applications thereof abound for classiﬁcation and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random ﬁeld model by training them to improve classiﬁcation performance between the data and an equal-sized sample of “negative examples” generated from the model’s current estimate of the data density. Training in each boosting round proceeds in three stages: ﬁrst we sample negative examples from the model’s current Boltzmann distribution. Next, a feature is trained to improve classiﬁcation performance between data and negative examples. Finally, a coefﬁcient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data.</p><p>5 0.12997369 <a title="127-tfidf-5" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image. This might be attempted by using simple ﬁlters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images. In this paper we describe an approach which lies between the two extremes. It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves. We use a dynamic tree-like architecture to model the high resolution data. Approximate conditioning on the low resolution image is achieved through a mean ﬁeld approach. 1</p><p>6 0.12781434 <a title="127-tfidf-6" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>7 0.11467677 <a title="127-tfidf-7" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>8 0.11285195 <a title="127-tfidf-8" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>9 0.11186958 <a title="127-tfidf-9" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>10 0.10343777 <a title="127-tfidf-10" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>11 0.1031128 <a title="127-tfidf-11" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>12 0.099030271 <a title="127-tfidf-12" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>13 0.096225083 <a title="127-tfidf-13" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>14 0.094670609 <a title="127-tfidf-14" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>15 0.08871121 <a title="127-tfidf-15" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>16 0.086739585 <a title="127-tfidf-16" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>17 0.085595652 <a title="127-tfidf-17" href="./nips-2002-Theory-Based_Causal_Inference.html">198 nips-2002-Theory-Based Causal Inference</a></p>
<p>18 0.085321352 <a title="127-tfidf-18" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>19 0.081559956 <a title="127-tfidf-19" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>20 0.078078419 <a title="127-tfidf-20" href="./nips-2002-Automatic_Alignment_of_Local_Representations.html">36 nips-2002-Automatic Alignment of Local Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.235), (1, 0.018), (2, -0.057), (3, 0.264), (4, -0.009), (5, 0.038), (6, -0.011), (7, 0.086), (8, 0.041), (9, -0.037), (10, 0.051), (11, -0.002), (12, 0.104), (13, 0.099), (14, 0.039), (15, 0.091), (16, 0.027), (17, -0.021), (18, -0.054), (19, -0.001), (20, -0.091), (21, -0.142), (22, 0.01), (23, 0.042), (24, -0.0), (25, -0.034), (26, 0.142), (27, 0.067), (28, -0.012), (29, -0.111), (30, -0.04), (31, -0.028), (32, -0.004), (33, -0.04), (34, 0.007), (35, 0.012), (36, -0.013), (37, 0.2), (38, -0.109), (39, 0.036), (40, -0.078), (41, -0.063), (42, -0.009), (43, -0.062), (44, -0.018), (45, 0.049), (46, -0.052), (47, -0.094), (48, -0.012), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94945049 <a title="127-lsi-1" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>2 0.64340508 <a title="127-lsi-2" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>3 0.62001359 <a title="127-lsi-3" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>Author: Anat Levin, Assaf Zomet, Yair Weiss</p><p>Abstract: Certain simple images are known to trigger a percept of transparency: the input image I is perceived as the sum of two images I(x, y) = I1 (x, y) + I2 (x, y). This percept is puzzling. First, why do we choose the “more complicated” description with two images rather than the “simpler” explanation I(x, y) = I1 (x, y) + 0 ? Second, given the inﬁnite number of ways to express I as a sum of two images, how do we compute the “best” decomposition ? Here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative ﬁlters and “corner detectors” in natural scenes and use this model to ﬁnd the most probable decomposition of a novel image. The optimization is performed using loopy belief propagation. We show that our model computes perceptually “correct” decompositions on synthetic images and discuss its application to real images. 1</p><p>4 0.59414595 <a title="127-lsi-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.53725797 <a title="127-lsi-5" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>6 0.52924961 <a title="127-lsi-6" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>7 0.51467592 <a title="127-lsi-7" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>8 0.48562518 <a title="127-lsi-8" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>9 0.46808982 <a title="127-lsi-9" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>10 0.4489803 <a title="127-lsi-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.44874734 <a title="127-lsi-11" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>12 0.4480651 <a title="127-lsi-12" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>13 0.4372074 <a title="127-lsi-13" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>14 0.42677087 <a title="127-lsi-14" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>15 0.41495827 <a title="127-lsi-15" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>16 0.40307605 <a title="127-lsi-16" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>17 0.39076024 <a title="127-lsi-17" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>18 0.38884842 <a title="127-lsi-18" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>19 0.38535264 <a title="127-lsi-19" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>20 0.37320986 <a title="127-lsi-20" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.011), (8, 0.113), (11, 0.059), (23, 0.026), (33, 0.019), (42, 0.117), (54, 0.141), (55, 0.038), (57, 0.014), (64, 0.022), (67, 0.025), (68, 0.042), (74, 0.106), (87, 0.024), (92, 0.033), (98, 0.108)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93013859 <a title="127-lda-1" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>same-paper 2 0.92589891 <a title="127-lda-2" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>3 0.89790726 <a title="127-lda-3" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>Author: Ralf Schoknecht</p><p>Abstract: There are several reinforcement learning algorithms that yield approximate solutions for the problem of policy evaluation when the value function is represented with a linear function approximator. In this paper we show that each of the solutions is optimal with respect to a specific objective function. Moreover, we characterise the different solutions as images of the optimal exact value function under different projection operations. The results presented here will be useful for comparing the algorithms in terms of the error they achieve relative to the error of the optimal approximate solution. 1</p><p>4 0.87057859 <a title="127-lda-4" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>5 0.86704999 <a title="127-lda-5" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>6 0.86535269 <a title="127-lda-6" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>7 0.86150134 <a title="127-lda-7" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>8 0.85620475 <a title="127-lda-8" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>9 0.85290802 <a title="127-lda-9" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>10 0.85290468 <a title="127-lda-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.85159266 <a title="127-lda-11" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>12 0.8514151 <a title="127-lda-12" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>13 0.84932566 <a title="127-lda-13" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>14 0.84701246 <a title="127-lda-14" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>15 0.84480703 <a title="127-lda-15" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>16 0.84417903 <a title="127-lda-16" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>17 0.8433212 <a title="127-lda-17" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>18 0.8430568 <a title="127-lda-18" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>19 0.84280396 <a title="127-lda-19" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>20 0.84191406 <a title="127-lda-20" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
