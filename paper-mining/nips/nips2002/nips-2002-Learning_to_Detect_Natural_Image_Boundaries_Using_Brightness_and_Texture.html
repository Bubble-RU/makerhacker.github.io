<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-132" href="#">nips2002-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</h1>
<br/><p>Source: <a title="nips-2002-132-pdf" href="http://papers.nips.cc/paper/2217-learning-to-detect-natural-image-boundaries-using-brightness-and-texture.pdf">pdf</a></p><p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>Reference: <a title="nips-2002-132-reference" href="../nips2002_reference/nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  Abstract The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. [sent-7, score-0.532]
</p><p>2 We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. [sent-8, score-0.689]
</p><p>3 In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. [sent-9, score-0.391]
</p><p>4 We present precision-recall curves showing that the resulting detector outperforms existing approaches. [sent-10, score-0.301]
</p><p>5 1 Introduction Consider the image patches in Figure 1. [sent-11, score-0.185]
</p><p>6 The goal of this paper is to use features extracted from the image patch to estimate the posterior probability of a boundary passing through the center point. [sent-13, score-0.497]
</p><p>7 Such a local boundary model is integral to higher-level segmentation algorithms, whether based on grouping pixels into regions [21, 8] or grouping edge fragments into contours [22, 16]. [sent-14, score-0.569]
</p><p>8 The traditional approach to this problem is to look for discontinuities in image brightness. [sent-15, score-0.185]
</p><p>9 For example, the widely employed Canny detector [2] models boundaries as brightness step edges. [sent-16, score-0.45]
</p><p>10 The image patches show that this is an inadequate model for boundaries in natural images, due to the ubiquitous phenomenon of texture. [sent-17, score-0.339]
</p><p>11 The Canny detector will ﬁre wildly inside textured regions where high-contrast contours are present but no boundary exists. [sent-18, score-0.586]
</p><p>12 In addition, it is unable to detect the boundary between textured regions when there is only a subtle change in average image brightness. [sent-19, score-0.553]
</p><p>13 These signiﬁcant problems have lead researchers to develop boundary detectors that explicitly model texture. [sent-20, score-0.249]
</p><p>14 Texture descriptors over local windows that straddle a boundary have different statistics from windows contained in either of the neighboring regions. [sent-22, score-0.305]
</p><p>15 Clearly, boundaries in natural images are marked by changes in both texture and brightness. [sent-24, score-0.693]
</p><p>16 Evidence from psychophysics [18] suggests that humans make combined use of these two cues to improve detection and localization of boundaries. [sent-25, score-0.226]
</p><p>17 There has been limited work in computational vision on addressing the difﬁcult problem of cue combination. [sent-26, score-0.196]
</p><p>18 For example, the authors of [8] associate a measure of texturedness with each point in an image in order to suppress contour processing in textured regions and vice versa. [sent-27, score-0.349]
</p><p>19 A large dataset of natural images that have been manually segmented by multiple human subjects [10] provides the ground truth label for each pixel as being on- or off-boundary. [sent-30, score-0.55]
</p><p>20 The task is then to model the probability of a pixel being on-boundary conditioned on some set of locally measured image features. [sent-31, score-0.238]
</p><p>21 This sort of quantitative approach to learning and evaluating boundary detectors is similar to the work of Konishi et al. [sent-32, score-0.296]
</p><p>22 Our work is distinguished by an explicit treatment of texture and brightness, enabling superior performance on a more diverse collection of natural images. [sent-34, score-0.511]
</p><p>23 In Section 2 we describe the oriented energy and texture gradient features used as input to our algorithm. [sent-36, score-0.743]
</p><p>24 Section 4 presents our evaluation methodology along with a quantitative comparison of our method to existing boundary detection methods. [sent-38, score-0.411]
</p><p>25 1 Oriented Energy In natural images, brightness edges are more than simple steps. [sent-41, score-0.215]
</p><p>26 The oriented energy (OE) approach [12] can be used to detect and localize these composite edges [14]. [sent-43, score-0.395]
</p><p>27 We compute OE at 3 half-octave scales starting at the image diagonal. [sent-49, score-0.185]
</p><p>28 The ﬁlters are elongated by a ratio of 3:1 along the putative boundary direction. [sent-50, score-0.249]
</p><p>29 2 Texture Gradient We would like a directional operator that measures the degree to which texture varies at a location in direction . [sent-52, score-0.518]
</p><p>30 A natural way to operationalize this is to consider a disk of radius centered on , and divided in two along a diameter at orientation . [sent-53, score-0.143]
</p><p>31 We can then compare the texture in the two half discs with some texture dissimilarity measure. [sent-54, score-0.863]
</p><p>32 Oriented texture processing along these lines has been pursued by [19]. [sent-55, score-0.448]
</p><p>33 &  &  @8 6 A97  @8 6 C9B  '  What texture dissimilarity measure should one use? [sent-56, score-0.448]
</p><p>34 There is an emerging consensus that for texture analysis, an image should ﬁrst be convolved with a bank of ﬁlters tuned to various orientations and spatial frequencies [4, 9]. [sent-57, score-0.688]
</p><p>35 After ﬁltering, a texture descriptor is then constructed using the empirical distribution of ﬁlter responses in the neighborhood of a pixel. [sent-58, score-0.415]
</p><p>36 This approach has been shown to be very powerful both for texture synthesis [5] as well as texture discrimination [15]. [sent-59, score-0.83]
</p><p>37 [15] evaluate a wide range of texture descriptors in this framework. [sent-61, score-0.457]
</p><p>38 I  E HG  E FD  ¡  ¢   ¡ ¢   Boundaries  Non-Boundaries  ¢ £¡  Intensity  ¢  £¡  Image  Figure 1: Local image features. [sent-67, score-0.185]
</p><p>39 In each row, the ﬁrst panel shows the image patch. [sent-68, score-0.249]
</p><p>40 The features are raw image intensity, raw oriented energy , localized oriented energy , raw texture gradient , and localized texture gradient . [sent-70, score-2.132]
</p><p>41 The challenge is to combine these features in order to detect and localize boundaries. [sent-72, score-0.275]
</p><p>42 © ¨  ¥§ ¦¤  ¥ ¦¤  @8 6 C9B EHG E $  5D G D "   " E G E D   ¨ 8  "   ©§ ¨  We deﬁne the texture gradient (TG) to be the  distance between these two histograms:  The texture gradient is computed at each pixel of the image diagonal. [sent-73, score-1.178]
</p><p>43 3 Localization The underlying function we are trying to learn is tightly peaked around the location of image boundaries marked by humans. [sent-75, score-0.422]
</p><p>44 The texture gradient is particularly prone to this effect, since the texture in a window straddling the boundary is distinctly different than the textures on either side of the boundary. [sent-78, score-1.178]
</p><p>45 This often results in a wide plateau or even double peaks in the texture gradient. [sent-79, score-0.488]
</p><p>46 Since each pixel is classiﬁed independently, these spatially extended features are particularly problematic as both on-boundary pixels and nearby off-boundary pixels will have large OE and TG. [sent-80, score-0.358]
</p><p>47 In order to make this spatial structure available to the classiﬁer we transform the raw OE and TG signals in order to emphasize local maxima. [sent-81, score-0.152]
</p><p>48 Given a feature  6 7   6  deﬁned over spatial coordinate orthogonal to the edge orientation, consider the derived , where is the ﬁrst-order approximation feature of the distance to the nearest maximum of . [sent-82, score-0.156]
</p><p>49 By incorporating the localization term, will have narrower peaks than the raw . [sent-84, score-0.297]
</p><p>50 1 This transformation is applied to the oriented energy and texture gradient signals at each orientation and scale separately. [sent-87, score-0.709]
</p><p>51 This yields a 6-element consists of these localized signals feature vector at 12 orientations at each pixel. [sent-92, score-0.232]
</p><p>52 0 1)  ' (&  3 Cue Combination Using Classiﬁers We would like to combine the cues given by the local feature vector in order to estimate the posterior probability of a boundary at each image location . [sent-93, score-0.63]
</p><p>53 Previous work on learning boundary models includes [11, 7]. [sent-94, score-0.216]
</p><p>54 For example, one may wish to ignore brightness edges inside high-contrast textures where OE is high and TG is low. [sent-98, score-0.206]
</p><p>55 75  1  Recall  Figure 2: Performance of raw (left) and localized features (right). [sent-152, score-0.322]
</p><p>56 The precision and recall axes are described in Section 4. [sent-153, score-0.182]
</p><p>57 The left plot shows the performance of the raw OE and TG features using the logistic regression classiﬁer. [sent-157, score-0.437]
</p><p>58 The right plot shows the performance of the features after applying the localization process of Equation 1. [sent-158, score-0.263]
</p><p>59 It is clear that the localization function greatly improves the quality of the individual features, especially the texture gradient. [sent-159, score-0.534]
</p><p>60 The top curve in each graph shows the performance of the features in combination. [sent-160, score-0.183]
</p><p>61 ¢  ¡  The ground truth boundary data is based on the dataset of [10] which provides 5-6 human segmentations for each of 1000 natural images from the Corel image database. [sent-168, score-0.902]
</p><p>62 The authors of [10] show that the segmentations of a single image by the different subjects are highly consistent, so we consider all humanmarked boundaries valid. [sent-171, score-0.415]
</p><p>63 We declare an image location to be on-boundary if it is within =2 pixels and =30 degrees of any human-marked boundary. [sent-172, score-0.313]
</p><p>64 Note that a high degree of class overlap in any local feature space is inevitable because the human subjects make use of both global constraints and high-level information to resolve locally ambiguous boundaries. [sent-177, score-0.267]
</p><p>65 4 Results The output of each classiﬁer is a set of oriented images, which provide the probability of a boundary at each image location based on local information. [sent-178, score-0.616]
</p><p>66 The left panel shows the performance of different combinations of the localized features using the logistic regression classiﬁer: the 3 OE features (oe*), the 3 TG features (tg*), the best performing single OE and TG features (oe2+tg1), and all 6 features together. [sent-205, score-0.94]
</p><p>67 Based on performance, simplicity, and low computation cost, we favor the logistic regression and its variants. [sent-210, score-0.188]
</p><p>68 classiﬁers we consider, the image provides actual posterior probabilities, which is particularly appropriate for the local measurement model in higher-level vision applications. [sent-211, score-0.408]
</p><p>69 ¥ ¤  ¥ ¦¤  In order to evaluate the boundary model against the human ground truth, we use the precision-recall framework, a standard evaluation technique in the information retrieval community [17]. [sent-213, score-0.439]
</p><p>70 It is closely related to the ROC curves used for by [1] to evaluate boundary models. [sent-214, score-0.266]
</p><p>71 The precision-recall curve captures the trade-off between accuracy and noise as the detector threshold is varied. [sent-215, score-0.253]
</p><p>72 These are computed using a distance tolerance of 2 pixels to allow for small localization errors in both the machine and human boundary maps. [sent-217, score-0.607]
</p><p>73 The precision-recall curve is particularly meaningful in the context of boundary detection when we consider applications that make use of boundary maps, such as stereo or object recognition. [sent-218, score-0.578]
</p><p>74 The location of the maximum F-measure along the curve provides the optimal threshold given , which we set to 0. [sent-223, score-0.16]
</p><p>75 ¤      ¢ ¢  $ ¤     £¤ ¨ ¡     Figure 2 shows the performance of the raw and localized features. [sent-225, score-0.274]
</p><p>76 This provides a clear quantitative justiﬁcation for the localization process described in Section 2. [sent-226, score-0.197]
</p><p>77 Figure 3a shows the performance of various linear combinations of the localized features. [sent-228, score-0.208]
</p><p>78 5  3  Tolerance (in pixels)  Figure 4: The left panel shows precision-recall curves for a variety of boundary detection schemes, along with the precision and recall of the human segmentations when compared with each other. [sent-248, score-0.826]
</p><p>79 The right panel shows the F-measure of each detector as the distance tolerance for measuring precision and recall varies. [sent-249, score-0.534]
</p><p>80 We take the Canny detector as the baseline due to its widespread use. [sent-250, score-0.245]
</p><p>81 Our detector outperforms the learning-based Nitzberg detector proposed by Konishi et al. [sent-251, score-0.465]
</p><p>82 The results presented so far use the logistic regression classiﬁer. [sent-253, score-0.188]
</p><p>83 The plain logistic regression model performs extremely well, with the variants of logistic regression – quadratic, boosted, and HME – performing only slightly better. [sent-257, score-0.376]
</p><p>84 3 2  Figure 4 shows the performance of our detector compared to two other approaches. [sent-261, score-0.262]
</p><p>85 Because of its widespread use, MATLAB’s implementation of the classic Canny [2] detector forms the baseline. [sent-262, score-0.245]
</p><p>86 We also consider the Nitzberg detector [13, 7], since it is based on a similar supervised learning approach, and Konishi et al. [sent-263, score-0.214]
</p><p>87 The Nitzberg detector generates a feature vector containing eigenvalues of the 2nd moment matrix; we train a classiﬁer on these 2 features using logistic regression. [sent-267, score-0.498]
</p><p>88 Figure 4 also shows the performance of the human data as an upper-bound for the algorithms. [sent-268, score-0.175]
</p><p>89 The human precision-recall points are computed for each segmentation by comparing it to the other segmentations of the same image. [sent-269, score-0.302]
</p><p>90 The approach of this paper is a clear improvement over the state of the art in boundary detection, but it will take the addition of high-level and global information to close the gap between the machine and human performance. [sent-270, score-0.343]
</p><p>91 C  3  C  3  5 Conclusion We have deﬁned a novel set of brightness and texture cues appropriate for constructing a local boundary model. [sent-280, score-0.846]
</p><p>92 By using a very large dataset of human-labeled boundaries in natural images, we have formulated the task of cue combination for local boundary detection as a supervised learning problem. [sent-281, score-0.606]
</p><p>93 This approach models the true posterior probability of a boundary at every image location and orientation, which is particularly useful for higherlevel algorithms. [sent-282, score-0.496]
</p><p>94 Based on a quantitative evaluation on 100 natural images, our detector outperforms existing methods. [sent-283, score-0.392]
</p><p>95 Fundamental bounds on edge detection: an information theoretic evaluation of different edge cues. [sent-334, score-0.142]
</p><p>96 A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. [sent-358, score-0.392]
</p><p>97 Feature detection in human vision: a phase dependent energy model. [sent-373, score-0.262]
</p><p>98 Detecting and localizing edges composed of steps, peaks and roofs. [sent-388, score-0.174]
</p><p>99 Non-parametric similarity measures for unsupervised texture segmentation and image retrieval. [sent-397, score-0.69]
</p><p>100 A probabilistic multi-scale model for contour completion based on image statistics. [sent-402, score-0.284]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('texture', 0.415), ('oe', 0.296), ('boundary', 0.216), ('detector', 0.214), ('canny', 0.213), ('tg', 0.195), ('nitzberg', 0.187), ('image', 0.185), ('logistic', 0.134), ('brightness', 0.13), ('human', 0.127), ('classi', 0.121), ('localized', 0.121), ('localization', 0.119), ('oriented', 0.111), ('konishi', 0.107), ('vision', 0.107), ('boundaries', 0.106), ('raw', 0.105), ('features', 0.096), ('localize', 0.093), ('precision', 0.092), ('recall', 0.09), ('segmentation', 0.09), ('cue', 0.089), ('images', 0.085), ('segmentations', 0.085), ('tolerance', 0.074), ('peaks', 0.073), ('ers', 0.072), ('pixels', 0.071), ('detection', 0.069), ('energy', 0.066), ('contour', 0.065), ('panel', 0.064), ('malik', 0.064), ('localizing', 0.064), ('orientation', 0.062), ('lters', 0.06), ('textured', 0.059), ('orientations', 0.057), ('contours', 0.057), ('experts', 0.057), ('location', 0.057), ('gradient', 0.055), ('regression', 0.054), ('feature', 0.054), ('fowlkes', 0.053), ('libsvm', 0.053), ('detect', 0.053), ('pixel', 0.053), ('int', 0.051), ('lter', 0.051), ('ground', 0.05), ('curves', 0.05), ('edge', 0.048), ('natural', 0.048), ('performance', 0.048), ('quantitative', 0.047), ('local', 0.047), ('directional', 0.046), ('hme', 0.046), ('evaluation', 0.046), ('truth', 0.044), ('descriptors', 0.042), ('segmented', 0.042), ('regions', 0.04), ('textures', 0.039), ('marked', 0.039), ('subjects', 0.039), ('pro', 0.039), ('curve', 0.039), ('combinations', 0.039), ('cues', 0.038), ('boosting', 0.038), ('density', 0.038), ('particularly', 0.038), ('edges', 0.037), ('puzicha', 0.037), ('lr', 0.037), ('outperforms', 0.037), ('tightly', 0.035), ('boosted', 0.035), ('composite', 0.035), ('intensity', 0.034), ('er', 0.034), ('completion', 0.034), ('combine', 0.033), ('quadratic', 0.033), ('along', 0.033), ('detectors', 0.033), ('dissimilarity', 0.033), ('roc', 0.033), ('trees', 0.032), ('bank', 0.031), ('widespread', 0.031), ('provides', 0.031), ('dataset', 0.031), ('martin', 0.03), ('spatially', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="132-tfidf-1" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>2 0.21353358 <a title="132-tfidf-2" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>3 0.18232279 <a title="132-tfidf-3" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>4 0.16479039 <a title="132-tfidf-4" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>Author: Anat Levin, Assaf Zomet, Yair Weiss</p><p>Abstract: Certain simple images are known to trigger a percept of transparency: the input image I is perceived as the sum of two images I(x, y) = I1 (x, y) + I2 (x, y). This percept is puzzling. First, why do we choose the “more complicated” description with two images rather than the “simpler” explanation I(x, y) = I1 (x, y) + 0 ? Second, given the inﬁnite number of ways to express I as a sum of two images, how do we compute the “best” decomposition ? Here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative ﬁlters and “corner detectors” in natural scenes and use this model to ﬁnd the most probable decomposition of a novel image. The optimization is performed using loopy belief propagation. We show that our model computes perceptually “correct” decompositions on synthetic images and discuss its application to real images. 1</p><p>5 0.14436685 <a title="132-tfidf-5" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>6 0.1353564 <a title="132-tfidf-6" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>7 0.12781434 <a title="132-tfidf-7" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>8 0.12655644 <a title="132-tfidf-8" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>9 0.12632796 <a title="132-tfidf-9" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>10 0.1199066 <a title="132-tfidf-10" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>11 0.10509691 <a title="132-tfidf-11" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>12 0.099478476 <a title="132-tfidf-12" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>13 0.095814802 <a title="132-tfidf-13" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>14 0.094856501 <a title="132-tfidf-14" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>15 0.091682583 <a title="132-tfidf-15" href="./nips-2002-Theory-Based_Causal_Inference.html">198 nips-2002-Theory-Based Causal Inference</a></p>
<p>16 0.088128857 <a title="132-tfidf-16" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>17 0.086542889 <a title="132-tfidf-17" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>18 0.086216152 <a title="132-tfidf-18" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>19 0.086091027 <a title="132-tfidf-19" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>20 0.082886055 <a title="132-tfidf-20" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.263), (1, -0.037), (2, 0.038), (3, 0.279), (4, 0.111), (5, -0.025), (6, 0.107), (7, -0.05), (8, 0.003), (9, -0.049), (10, -0.064), (11, 0.056), (12, 0.031), (13, 0.1), (14, 0.059), (15, 0.013), (16, -0.019), (17, -0.044), (18, 0.033), (19, -0.025), (20, -0.148), (21, 0.048), (22, -0.017), (23, 0.05), (24, 0.034), (25, 0.049), (26, 0.067), (27, -0.055), (28, -0.006), (29, -0.108), (30, 0.066), (31, -0.114), (32, -0.127), (33, -0.044), (34, 0.02), (35, -0.024), (36, 0.039), (37, 0.063), (38, 0.059), (39, 0.053), (40, 0.088), (41, -0.099), (42, -0.155), (43, 0.001), (44, -0.006), (45, -0.039), (46, 0.118), (47, -0.081), (48, 0.045), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96067083 <a title="132-lsi-1" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>2 0.72502655 <a title="132-lsi-2" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>3 0.66496551 <a title="132-lsi-3" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>Author: Anat Levin, Assaf Zomet, Yair Weiss</p><p>Abstract: Certain simple images are known to trigger a percept of transparency: the input image I is perceived as the sum of two images I(x, y) = I1 (x, y) + I2 (x, y). This percept is puzzling. First, why do we choose the “more complicated” description with two images rather than the “simpler” explanation I(x, y) = I1 (x, y) + 0 ? Second, given the inﬁnite number of ways to express I as a sum of two images, how do we compute the “best” decomposition ? Here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative ﬁlters and “corner detectors” in natural scenes and use this model to ﬁnd the most probable decomposition of a novel image. The optimization is performed using loopy belief propagation. We show that our model computes perceptually “correct” decompositions on synthetic images and discuss its application to real images. 1</p><p>4 0.62894487 <a title="132-lsi-4" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>5 0.54197615 <a title="132-lsi-5" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>6 0.53396773 <a title="132-lsi-6" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>7 0.50324076 <a title="132-lsi-7" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>8 0.50002056 <a title="132-lsi-8" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>9 0.47390747 <a title="132-lsi-9" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>10 0.46811655 <a title="132-lsi-10" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>11 0.46804392 <a title="132-lsi-11" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>12 0.46661997 <a title="132-lsi-12" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>13 0.45716789 <a title="132-lsi-13" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>14 0.45135242 <a title="132-lsi-14" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>15 0.40569255 <a title="132-lsi-15" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>16 0.39594522 <a title="132-lsi-16" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>17 0.39446652 <a title="132-lsi-17" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>18 0.385748 <a title="132-lsi-18" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>19 0.38510999 <a title="132-lsi-19" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>20 0.37010038 <a title="132-lsi-20" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.034), (14, 0.034), (15, 0.188), (23, 0.037), (42, 0.087), (54, 0.116), (55, 0.033), (67, 0.011), (68, 0.036), (74, 0.168), (87, 0.014), (92, 0.028), (98, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89502299 <a title="132-lda-1" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>2 0.76601952 <a title="132-lda-2" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>3 0.7635963 <a title="132-lda-3" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image. This might be attempted by using simple ﬁlters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images. In this paper we describe an approach which lies between the two extremes. It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves. We use a dynamic tree-like architecture to model the high resolution data. Approximate conditioning on the low resolution image is achieved through a mean ﬁeld approach. 1</p><p>4 0.75949365 <a title="132-lda-4" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>5 0.75709283 <a title="132-lda-5" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>6 0.75693649 <a title="132-lda-6" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>7 0.75445777 <a title="132-lda-7" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>8 0.75301319 <a title="132-lda-8" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>9 0.75298935 <a title="132-lda-9" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>10 0.75297272 <a title="132-lda-10" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>11 0.75296128 <a title="132-lda-11" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>12 0.75228602 <a title="132-lda-12" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>13 0.75109255 <a title="132-lda-13" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>14 0.74868798 <a title="132-lda-14" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>15 0.74844247 <a title="132-lda-15" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>16 0.74659854 <a title="132-lda-16" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>17 0.74551022 <a title="132-lda-17" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>18 0.7452845 <a title="132-lda-18" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>19 0.74150217 <a title="132-lda-19" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>20 0.74102199 <a title="132-lda-20" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
