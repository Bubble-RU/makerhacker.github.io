<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-48" href="#">nips2002-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</h1>
<br/><p>Source: <a title="nips-2002-48-pdf" href="http://papers.nips.cc/paper/2252-categorization-under-complexity-a-unified-mdl-account-of-human-learning-of-regular-and-irregular-categories.pdf">pdf</a></p><p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>Reference: <a title="nips-2002-48-reference" href="../nips2002_reference/nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). [sent-5, score-0.888]
</p><p>2 In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. [sent-6, score-0.537]
</p><p>3 This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. [sent-7, score-0.634]
</p><p>4 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. [sent-8, score-0.062]
</p><p>5 It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. [sent-9, score-0.123]
</p><p>6 While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). [sent-10, score-0.575]
</p><p>7 (a)  (b)  (c)  (d)  (e)  Figure 1: Categories similar to those previously studied. [sent-11, score-0.024]
</p><p>8 html  Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. [sent-17, score-0.528]
</p><p>9 For example, Nosofsky has previously examined the "criss-cross" category of Figure 1(d) and a diagonal category similar to Concept 3 of Figure 2, as well as some other multimodal categories [5J [6J. [sent-18, score-0.718]
</p><p>10 While these individual category structures are no doubt theoretically important, they in no way exhaust the range of possible concept structures. [sent-19, score-0.729]
</p><p>11 Indeed, if we view n-dimensional Cartesian space as the canvas upon which a category may be represented, then any set of manifolds in that space may be considered as a potential category [7]. [sent-20, score-0.548]
</p><p>12 It is therefore natural to ask whether one such category-manifold is in principle easier or more difficult to learn than another. [sent-21, score-0.07]
</p><p>13 Since previous investigations have never considered any reasonably broad range of category structures, they have never been in a position to answer this question. [sent-22, score-0.352]
</p><p>14 In this paper we present a theory for human categorization, based on the MDL principle, that is much better equipped to answer questions about . [sent-23, score-0.12]
</p><p>15 the intrinsic leamability of both structurally regular and structurally irregular categories. [sent-24, score-0.316]
</p><p>16 In support of this theory we briefly present an experiment testing human subjects' learning of a range of concept types defined over a continuous two-dimensional feature space, including both highly regular and highly irregular structures. [sent-25, score-0.834]
</p><p>17 We find that our MDL-based theory gives a good account of human learning for these concepts, and that descriptive complexity accurately predicts the subjective difficulty of the various concept types tested. [sent-26, score-0.78]
</p><p>18 2 Previous Investigations of Category Structure The role of category structure in determining leamability has not been overlooked entirely in the literature; in fact, the intrinsic structure of binary-featured categories has been investigated quite thoroughly. [sent-27, score-0.596]
</p><p>19 [8J showed that human performance in learning such Boolean categories varies greatly depending on the intrinsic logical structure of the concept. [sent-29, score-0.371]
</p><p>20 More recently, we have shown that this performance is well-predicted by the intrinsic Boolean complexity of each concept, given by the length of the shortest Boolean formula that describes the objects in the category [9]. [sent-30, score-0.538]
</p><p>21 This result suggests that a principle of simplicity or parsimony, manifested as a minimization of complexity, might play an important role in human category learning. [sent-31, score-0.494]
</p><p>22 The details of Boolean complexity analysis do not generalize easily to the type of continuous feature spaces we wish to investigate here. [sent-32, score-0.159]
</p><p>23 Thus a new approach is required, similar in general spirit but differing in the mathematics. [sent-33, score-0.029]
</p><p>24 3 Experiment While the MDL principle that we plan to employ is applicable to concepts of any dimension, for reasons of convenience this experiment is limited to category structures that can be formed within a two-dimensional feature space. [sent-35, score-0.645]
</p><p>25 This feature space is discretized into a 4 x 4 grid from which a legitimate category can be specified by the selection of any four grid squares. [sent-36, score-0.453]
</p><p>26 Our motivation for discretizing the feature space is to place a constraint on possible category structure that will facilitate the computation of a complexity measure; this does not restrict the range ofpossible feature values that can be adopted by stimuli. [sent-37, score-0.494]
</p><p>27 In principle, feature values are limited only by machine precision, but as a matter of convenience  we restrict features to adopting one of 1000 possible values in the range [0,1]. [sent-38, score-0.099]
</p><p>28 Concept 1  Concept 2  Concept 3  Concept 4  Concept 5  Concept 6  Concept 7  Concept 8  Concept 9  Concept 10  Concept 11  Concept 12  Figure 2: Abstract concepts used in experiment. [sent-39, score-0.179]
</p><p>29 The particular 12 abstract category structures ("concepts") examined in the experiment are shown in Figure 2. [sent-40, score-0.34]
</p><p>30 These concepts were considered to be individually interesting (from a cross-theoretical perspective) and jointly representative of the broader range of available concepts. [sent-41, score-0.251]
</p><p>31 The two categories in each concept are referred to as "positive" and "negative. [sent-42, score-0.498]
</p><p>32 " The positive category is represented by the dark-shaded regions, and the corresponding negative category is its complement. [sent-43, score-0.578]
</p><p>33 Note that in many cases the categories are "disconnected" or multimodal. [sent-44, score-0.146]
</p><p>34 Nevertheless, these categories are not in any sense "probabilistic" or "ill-de:fil1. [sent-45, score-0.146]
</p><p>35 ed"; a given point in feature space is ahvays either p_ositive or negative. [sent-46, score-0.032]
</p><p>36 During the experiment, each stimulus is drawn randomly from the feature space and is labeled "positive" or "negative" based on the region from which it was drawn. [sent-47, score-0.087]
</p><p>37 Uniform sampling is used, so all 12 categories of Figure 2 have the same base rate for positives, . [sent-48, score-0.146]
</p><p>38 ) 4 1 P( posItIve == 16 == 4' The experiment itself was clothed as a video game that required subj ects to discriminate between two classes of spaceships, "Ally" and "Enemy," by destroying Enemy ships and quick-landing Allied ships. [sent-50, score-0.087]
</p><p>39 Each subject (14 total) played 12 five-minute games in which the distribution ofAllies and Enemies corresponded (in random order) to the 12 concepts of Figure 2. [sent-51, score-0.233]
</p><p>40 The physical features of the spaceships in all cases were the height of the "tube" and the radius of the "pod. [sent-52, score-0.047]
</p><p>41 " As shown in Figure 3, these physical features are mapped randomly onto the abstract feature space such that the experimental concepts may be any rigid rotation or reflection of the abstract concepts in Figure 2. [sent-53, score-0.39]
</p><p>42 4 Derivation of the MDL Principle The MDL principle is largely due to Rissanen [10] and is easily shown to be a consequence of optimal Bayesian inference [11]. [sent-54, score-0.099]
</p><p>43 While several Bayesian algorithms have previously been proposed as models of human concept learning [3][4], the implications of the MDL principle for human learning have only recently come under scrutiny [12][13]. [sent-55, score-0.686]
</p><p>44 According to Bayes rule, a learner ought to select the category hypothesis H that maximizes  Pod Radius  (a)  (b)  (c)  (d)  Figure 3: (a) A spaceship. [sent-57, score-0.497]
</p><p>45 Thus, besides the merits ofbrevity for its own sake, we see that maximal descriptive compactness also corresponds to maximal inferential power. [sent-61, score-0.071]
</p><p>46 It is this equivalence between description length and inference that leads us to investigate the role of descriptive complexity in the domain of concept learning. [sent-62, score-0.746]
</p><p>47 5  Theory  In order to investigate the complexity of the 12 concepts of Figure 2, Equation 4 indicates that we need to analyze (1) the description length of a hypothesis for each concept, DL (H), and (2) the description length ofthe concept given the hypothesis, DL(D I H). [sent-63, score-1.212]
</p><p>48 within which hypotheses about the category structure can be expressed. [sent-67, score-0.338]
</p><p>49 We choose to use the "rectangle language" whose alphabet (Table 1) consists of 10 classes of symbols representing the 10 different sizes of rectangle that can be composited within a 4x4 grid: 1x 1, 1 x2, 1x3, 1 x4, 2x2, 2x3, 2x4, 3x3, 3x4, and 4x4. [sent-68, score-0.286]
</p><p>50 2 Each member of the class "m x n" is an m x n or n·x m rectangle situated at a particular position in the 4 x 4 grid. [sent-69, score-0.318]
</p><p>51 We allow a given hypothesis to be represented by up to four distinct rectangles (i. [sent-70, score-0.295]
</p><p>52 Having specified a language, the issue is now the length of the hypothesis code. [sent-73, score-0.275]
</p><p>53 The derivation above suggests that a codelength of -log P(x) be assigned to each symbol x, which corresponds to the so-called Shannon code. [sent-74, score-0.095]
</p><p>54 We therefore proceed to compute the Shannon codelengths for the rectangle alphabet of Table 1. [sent-75, score-0.381]
</p><p>55 The particular choice of language (model class) is obviously an important determinant of the ultimate hypothesis description length. [sent-77, score-0.339]
</p><p>56 2The class "m x n" contains all rectangles of dimension m x nand n x m. [sent-82, score-0.1]
</p><p>57 3 We use the noninteger value - log P (x) rather than the integer log P ( x)l. [sent-83, score-0.058]
</p><p>58 The third and fourth columns show the probability that the source generates a given member ofthe class "m x n" and the corresponding codelength. [sent-86, score-0.126]
</p><p>59 1  (gI0 )  -log  (lo)  -log  1 (4 0)  -log (4~) -log (1~)  Computing these codelengths requires t~at we specify the probability mass function of a source, P(x). [sent-96, score-0.095]
</p><p>60 It is convenient for this purpose (and compatible with the subject's perspective) to imagine that the concepts in Figure 2 are produced by a "concept generator," an information source whose parameters are essentially unknown. [sent-97, score-0.206]
</p><p>61 A reasonable assumption is that the source randomly selects a rectangle class with uniform probability, and then selects an individual member of the chosen class also with uniform probability. [sent-98, score-0.451]
</p><p>62 Since there are 10 classes, the assumption regarding class selection places a prior on each rectangle class of P(m x n) == 1~. [sent-99, score-0.315]
</p><p>63 Moreover, the assumption of uniform within-class sampling means that in order to encode any individual rectangle, we need only consider the cardinality of the class to which it belongs. [sent-100, score-0.087]
</p><p>64 We now recall that the individual rectangles of the class "m x n" differ only in their positions within the 4 x 4 grid. [sent-101, score-0.124]
</p><p>65 ( rnXn The corresponding Shannon codelengths are shown next to these probabilities in Table 1. [sent-103, score-0.095]
</p><p>66 The description length of a particular hypothesis is the summed codeword lengths for all the rectangles (up to four) that are comprised by the hypothesis. [sent-104, score-0.463]
</p><p>67 2  The Likelihood Description Length, DL(D I H)  The second part of the two-part MDL code is the description of the concept with respect to the selected hypothesis, corresponding to the Bayes likelihood. [sent-106, score-0.5]
</p><p>68 We recall that a hypothesis H is composed of up to four rectangular regions. [sent-108, score-0.26]
</p><p>69 Computing DL(D I H) therefore involves describing that portion of the positive category that falls within each rectangular hypothesis region. [sent-109, score-0.527]
</p><p>70 This is conceptually the same problem that we faced in computing DL(H) above, except that the region of interest for DL(H) was fixed  Table 2: Minimum description lengths for the 12 abstract concepts. [sent-110, score-0.179]
</p><p>71 Since DL(D I H) must capture just the positive squares in the hypothesis region (a maximum of four squares), the only rectangle classes needed in the alphabet are those of size four: 1x 1, 1x2, 1x 3, 1x4, and 2x2. [sent-128, score-0.574]
</p><p>72 6 Minimum Description Lengths for Experimental Concepts Applying the MDL analysis above to the concepts in Figure 2 requires that we compute the total description length DL(D I H) + DL(H) corresponding to all viable hypothe, ses for each concept. [sent-129, score-0.346]
</p><p>73 The hypothesis H corresponding to the shortest total codelength DL(D I H) + DL(H) for each concept is the MDL hypothesis. [sent-130, score-0.678]
</p><p>74 4 The MDL hypotheses for all 12 concepts are shown in Table 2 along with the corresponding minimum codelengths. [sent-131, score-0.252]
</p><p>75 4Note that the MDL hypothesis is not in general the most compact hypothesis, i. [sent-133, score-0.192]
</p><p>76 Rather, the MDL hypothesis is the one for which the sum DL(D I H) + DL(H) is minimum. [sent-136, score-0.192]
</p><p>77 7 Results For each game played by the subject (i. [sent-137, score-0.082]
</p><p>78 , each concept in Figure 2), an overall measure of performance (d') is computed. [sent-139, score-0.352]
</p><p>79 5 Figure 4 shows performance for all subjects and all concepts as a function of the concept complexities (MDL codelengths) in Table 2. [sent-140, score-0.589]
</p><p>80 There is an evident decrease in performance with increasing complexity, which a regression analysis shows to be highly significant (R 2 == . [sent-141, score-0.024]
</p><p>81 000001), meaning that the linear trend in the plot is very unlikely to be a statistical accident. [sent-144, score-0.033]
</p><p>82 Thus, the MDL complexity predicts the subjective difficulty ofleaming across a broad range of concepts. [sent-145, score-0.246]
</p><p>83 The d' performance for each concept is indicated by a '+' and the mean d' for each concept is indicated by an '0'. [sent-159, score-0.704]
</p><p>84 We mention that the MDL approach described here can be further modified to make "realtime" predictions of how subjects will categorize each new stimulus. [sent-160, score-0.093]
</p><p>85 In the most simplistic approach, the prediction for each new stimulus x is made based on the MDL hypothesis prevailing at the time that stimulus is observed. [sent-161, score-0.244]
</p><p>86 Correlation between this MDL prediction and the subject's actual decision is found to be highly significant (p :::; . [sent-162, score-0.024]
</p><p>87 :: ~ step 7  step 9  Step 19  step 59  step 113  Step 169  step 190  Figure 5: Real-time MDL hypothesis evolution for actual Concept 11 data. [sent-184, score-0.317]
</p><p>88 shown in Step 169 and the two-rectangle (1 x3) hypothesis shown in Step 190. [sent-186, score-0.192]
</p><p>89 l 5 d (discriminability) gives a measure of subjects' intrinsic capacity to discriminate categories, i. [sent-187, score-0.11]
</p><p>90 The data presented above suggest that human learners are indeed guided by something very much like Rissanen's principle when classifying objects. [sent-191, score-0.219]
</p><p>91 While it is premature to conclude that humans construct anything precisely corresponding to the two-part code of Equation 4, it seems likely that they employ some closely related complexity-minimization principle-and an associated "cognitive code" still to be discovered. [sent-192, score-0.063]
</p><p>92 This finding is consistent with many earlier observations of minimum principles guiding human inference, especially in perception (e. [sent-193, score-0.177]
</p><p>93 Moreover, our findings suggest a principled approach to predicting the subjective difficulty of concepts defined over continuous features. [sent-196, score-0.358]
</p><p>94 As we had previously found with Boolean concepts, subjective difficulty correlates with intrinsic complexity: That which is incompressible is) in turn) incomprehensible. [sent-197, score-0.219]
</p><p>95 The MDL approach is an elegant framework in which to make this observation rigorous and concrete, and one which apparently accords well with human performance. [sent-198, score-0.12]
</p><p>96 , "The adaptive nature of human categorization," Psychological Review, Vol. [sent-215, score-0.12]
</p><p>97 , "Bayesian modeling of human concept learning," Advances in Neural Information Processing Systems, edited by M. [sent-221, score-0.522]
</p><p>98 , "Optimal performance and exemplar models of classification," Rational Models of Cognition, edited by M. [sent-231, score-0.05]
</p><p>99 , "Minimization of Boolean complexity in human concept learning," Nature, Vol. [sent-256, score-0.561]
</p><p>100 , "Categorization by simplicity: A minimum description length approach to unsupervised clustering," Similarity and Categorization, edited by U. [sent-270, score-0.25]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mdl', 0.495), ('concept', 0.352), ('dl', 0.315), ('category', 0.274), ('rectangle', 0.247), ('hypothesis', 0.192), ('concepts', 0.179), ('bits', 0.147), ('categories', 0.146), ('human', 0.12), ('description', 0.112), ('categorization', 0.095), ('codelength', 0.095), ('codelengths', 0.095), ('nosofsky', 0.095), ('rissanen', 0.095), ('complexity', 0.089), ('boolean', 0.083), ('intrinsic', 0.081), ('irregular', 0.078), ('descriptive', 0.071), ('principle', 0.07), ('psychology', 0.066), ('rectangles', 0.066), ('feldman', 0.062), ('difficulty', 0.06), ('subjects', 0.058), ('oxford', 0.057), ('length', 0.055), ('subjective', 0.054), ('edited', 0.05), ('shannon', 0.05), ('enemy', 0.047), ('leamability', 0.047), ('ofmathematical', 0.047), ('piscataway', 0.047), ('spaceships', 0.047), ('range', 0.043), ('grid', 0.041), ('shepard', 0.041), ('chater', 0.041), ('jacob', 0.041), ('hypotheses', 0.04), ('regular', 0.04), ('shortest', 0.039), ('alphabet', 0.039), ('investigate', 0.038), ('lengths', 0.038), ('findings', 0.038), ('rutgers', 0.038), ('four', 0.037), ('member', 0.037), ('code', 0.036), ('structures', 0.036), ('pearson', 0.035), ('structurally', 0.035), ('categorize', 0.035), ('investigations', 0.035), ('language', 0.035), ('types', 0.034), ('class', 0.034), ('meaning', 0.033), ('minimum', 0.033), ('bayesian', 0.032), ('table', 0.032), ('feature', 0.032), ('ought', 0.031), ('rectangular', 0.031), ('briefly', 0.03), ('experiment', 0.03), ('positive', 0.03), ('minimization', 0.03), ('log', 0.029), ('region', 0.029), ('inference', 0.029), ('discriminate', 0.029), ('broader', 0.029), ('cognition', 0.029), ('differing', 0.029), ('cardinality', 0.029), ('guided', 0.029), ('ofthe', 0.028), ('specified', 0.028), ('game', 0.028), ('played', 0.028), ('defined', 0.027), ('source', 0.027), ('humans', 0.027), ('stimulus', 0.026), ('subject', 0.026), ('cognitive', 0.026), ('step', 0.025), ('structure', 0.024), ('highly', 0.024), ('individual', 0.024), ('previously', 0.024), ('perception', 0.024), ('convenience', 0.024), ('selects', 0.024), ('nj', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="48-tfidf-1" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>2 0.15697044 <a title="48-tfidf-2" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>Author: Neville E. Sanjana, Joshua B. Tenenbaum</p><p>Abstract: We argue that human inductive generalization is best explained in a Bayesian framework, rather than by traditional models based on similarity computations. We go beyond previous work on Bayesian concept learning by introducing an unsupervised method for constructing ﬂexible hypothesis spaces, and we propose a version of the Bayesian Occam’s razor that trades off priors and likelihoods to prevent under- or over-generalization in these ﬂexible spaces. We analyze two published data sets on inductive reasoning as well as the results of a new behavioral study that we have carried out.</p><p>3 0.094859511 <a title="48-tfidf-3" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>4 0.073172383 <a title="48-tfidf-4" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>5 0.072662972 <a title="48-tfidf-5" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>6 0.063279659 <a title="48-tfidf-6" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>7 0.058555827 <a title="48-tfidf-7" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>8 0.057632215 <a title="48-tfidf-8" href="./nips-2002-Theory-Based_Causal_Inference.html">198 nips-2002-Theory-Based Causal Inference</a></p>
<p>9 0.056407917 <a title="48-tfidf-9" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>10 0.054762378 <a title="48-tfidf-10" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>11 0.054003406 <a title="48-tfidf-11" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>12 0.053228009 <a title="48-tfidf-12" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>13 0.052085549 <a title="48-tfidf-13" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>14 0.047939368 <a title="48-tfidf-14" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>15 0.047171444 <a title="48-tfidf-15" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>16 0.044340957 <a title="48-tfidf-16" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>17 0.044106219 <a title="48-tfidf-17" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>18 0.043290146 <a title="48-tfidf-18" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>19 0.041922159 <a title="48-tfidf-19" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>20 0.041571863 <a title="48-tfidf-20" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, -0.025), (2, -0.001), (3, -0.001), (4, 0.002), (5, 0.034), (6, -0.076), (7, -0.088), (8, -0.019), (9, -0.059), (10, -0.007), (11, -0.009), (12, 0.068), (13, -0.026), (14, -0.068), (15, -0.065), (16, 0.005), (17, -0.006), (18, 0.039), (19, -0.142), (20, -0.072), (21, -0.134), (22, 0.039), (23, -0.091), (24, -0.027), (25, 0.043), (26, -0.091), (27, -0.046), (28, 0.045), (29, 0.085), (30, 0.022), (31, 0.054), (32, -0.031), (33, 0.158), (34, 0.108), (35, 0.139), (36, 0.096), (37, 0.004), (38, 0.046), (39, 0.093), (40, -0.043), (41, 0.134), (42, -0.272), (43, -0.009), (44, 0.151), (45, -0.1), (46, -0.06), (47, 0.077), (48, -0.135), (49, 0.268)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97950912 <a title="48-lsi-1" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>2 0.73318946 <a title="48-lsi-2" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>Author: Neville E. Sanjana, Joshua B. Tenenbaum</p><p>Abstract: We argue that human inductive generalization is best explained in a Bayesian framework, rather than by traditional models based on similarity computations. We go beyond previous work on Bayesian concept learning by introducing an unsupervised method for constructing ﬂexible hypothesis spaces, and we propose a version of the Bayesian Occam’s razor that trades off priors and likelihoods to prevent under- or over-generalization in these ﬂexible spaces. We analyze two published data sets on inductive reasoning as well as the results of a new behavioral study that we have carried out.</p><p>3 0.55465537 <a title="48-lsi-3" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>4 0.4863084 <a title="48-lsi-4" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>5 0.42439896 <a title="48-lsi-5" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>6 0.33342102 <a title="48-lsi-6" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>7 0.29202205 <a title="48-lsi-7" href="./nips-2002-Theory-Based_Causal_Inference.html">198 nips-2002-Theory-Based Causal Inference</a></p>
<p>8 0.27445674 <a title="48-lsi-8" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>9 0.26907119 <a title="48-lsi-9" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>10 0.25336015 <a title="48-lsi-10" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>11 0.25156498 <a title="48-lsi-11" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>12 0.24797401 <a title="48-lsi-12" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>13 0.24303105 <a title="48-lsi-13" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>14 0.22915447 <a title="48-lsi-14" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>15 0.22569318 <a title="48-lsi-15" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>16 0.22257824 <a title="48-lsi-16" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>17 0.22179931 <a title="48-lsi-17" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>18 0.21226025 <a title="48-lsi-18" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>19 0.20880717 <a title="48-lsi-19" href="./nips-2002-Rational_Kernels.html">167 nips-2002-Rational Kernels</a></p>
<p>20 0.20857683 <a title="48-lsi-20" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.243), (11, 0.019), (23, 0.024), (42, 0.064), (54, 0.13), (55, 0.033), (57, 0.028), (67, 0.021), (68, 0.032), (74, 0.137), (87, 0.031), (92, 0.036), (98, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84212071 <a title="48-lda-1" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>2 0.81944495 <a title="48-lda-2" href="./nips-2002-Rational_Kernels.html">167 nips-2002-Rational Kernels</a></p>
<p>Author: Corinna Cortes, Patrick Haffner, Mehryar Mohri</p><p>Abstract: We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition. We show that rational kernels can be computed efﬁciently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm. We also describe several general families of positive deﬁnite symmetric rational kernels. These general kernels can be combined with Support Vector Machines to form efﬁcient and powerful techniques for spoken-dialog classiﬁcation: highly complex kernels become easy to design and implement and lead to substantial improvements in the classiﬁcation accuracy. We also show that the string kernels considered in applications to computational biology are all speciﬁc instances of rational kernels.</p><p>3 0.67406315 <a title="48-lda-3" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>4 0.6726023 <a title="48-lda-4" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>Author: Rong Jin, Zoubin Ghahramani</p><p>Abstract: In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naIve methods degrade rapidly as more ambiguity is introduced into the labels. 1</p><p>5 0.67228389 <a title="48-lda-5" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present a class of algorithms for learning the structure of graphical models from data. The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtained from Mercer kernels. Thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type. We explore the computational properties of our approach, showing how to use the kernel trick to compute the relevant statistics in linear time. We illustrate our framework with experiments involving discrete and continuous data.</p><p>6 0.6708771 <a title="48-lda-6" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>7 0.66846085 <a title="48-lda-7" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>8 0.66635823 <a title="48-lda-8" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>9 0.66626072 <a title="48-lda-9" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>10 0.66592729 <a title="48-lda-10" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>11 0.66531712 <a title="48-lda-11" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>12 0.66527247 <a title="48-lda-12" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>13 0.66403258 <a title="48-lda-13" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>14 0.66260362 <a title="48-lda-14" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>15 0.6615203 <a title="48-lda-15" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>16 0.66065407 <a title="48-lda-16" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>17 0.65981585 <a title="48-lda-17" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>18 0.65975088 <a title="48-lda-18" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>19 0.6597209 <a title="48-lda-19" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>20 0.65852541 <a title="48-lda-20" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
