<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-88" href="#">nips2002-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</h1>
<br/><p>Source: <a title="nips-2002-88-pdf" href="http://papers.nips.cc/paper/2321-feature-selection-and-classification-on-matrix-data-from-large-margins-to-small-covering-numbers.pdf">pdf</a></p><p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>Reference: <a title="nips-2002-88-reference" href="../nips2002_reference/nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. [sent-4, score-0.866]
</p><p>2 We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. [sent-5, score-0.495]
</p><p>3 Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. [sent-6, score-0.448]
</p><p>4 The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. [sent-7, score-0.086]
</p><p>5 We then consider the case that row objects are interpreted as features. [sent-8, score-0.607]
</p><p>6 We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. [sent-9, score-0.788]
</p><p>7 Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. [sent-10, score-1.227]
</p><p>8 Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. [sent-11, score-0.218]
</p><p>9 Our new method extracts a sparse set of genes and provides superior classiﬁcation results. [sent-12, score-0.117]
</p><p>10 1  Introduction  Many properties of sets of objects can be described by matrices, whose rows and columns correspond to objects and whose elements describe the relationship between them. [sent-13, score-0.895]
</p><p>11 One typical case are so-called pairwise data, where rows as well as columns of the matrix represent the objects of the dataset (Fig. [sent-14, score-0.692]
</p><p>12 1a) and where the entries of the matrix denote similarity values which express the relationships between objects. [sent-15, score-0.113]
</p><p>13 2  Figure 1: Two typical examples of matrix data (see text). [sent-244, score-0.113]
</p><p>14 Column objects (A-G) diﬀer form row objects (α - λ). [sent-248, score-0.951]
</p><p>15 Another typical case occurs, if objects are described by a set of features (Fig. [sent-250, score-0.453]
</p><p>16 In this case, the column objects are the objects to be characterized, the row objects correspond to their features and the matrix elements denote the strength with which a feature is expressed in a particular object. [sent-252, score-1.844]
</p><p>17 In the following we consider the task of learning a classiﬁcation problem on matrix data. [sent-253, score-0.113]
</p><p>18 We consider the case that class labels are assigned to the column objects of the training set. [sent-254, score-0.509]
</p><p>19 Given the matrix and the class labels we then want to construct a classiﬁer with good generalization properties. [sent-255, score-0.16]
</p><p>20 From all the possible choices we select classiﬁers from the support vector machine (SVM) family [1, 2] and we use the principle of structural risk minimization [15] for model selection - because of its recent success [11] and its theoretical properties [15]. [sent-256, score-0.375]
</p><p>21 Previous work on large margin classiﬁers for datasets, where objects are described by feature vectors and where SVMs operate on the column vectors of the matrix, is abundant. [sent-257, score-0.962]
</p><p>22 However, there is one serious problem which arise when the number of features becomes large and comparable to the number of objects: Without feature selection, SVMs are prone to overﬁtting, despite the complexity regularization which is implicit in the learning method [3]. [sent-258, score-0.287]
</p><p>23 Rather than being sparse in the number of support vectors, the classiﬁer should be sparse in the number of features used for classiﬁcation. [sent-259, score-0.169]
</p><p>24 This relates to the result [15] that the number of features provide an upper bound on the number of “essential” support vectors. [sent-260, score-0.135]
</p><p>25 Previous work on large margin classiﬁers for datasets, where objects are described by their mutual similarities, was centered around the idea that the matrix of similarities can be interpreted as a Gram matrix (see e. [sent-261, score-0.806]
</p><p>26 In this contribution we extend the Gram matrix approach to matrix data, where row and column objects belong to diﬀerent sets. [sent-265, score-0.88]
</p><p>27 Since we can no longer expect that the matrices are positive deﬁnite (or even square), a new objective function must be derived. [sent-266, score-0.086]
</p><p>28 This is done in the next section, where an algorithm for the construction of linear classiﬁers is derived using the principle of structural risk minimization. [sent-267, score-0.206]
</p><p>29 Section 3 is concerned with the question under what conditions matrix elements can indeed be interpreted as vector products in some feature space. [sent-268, score-0.404]
</p><p>30 The method is specialized to pairwise data in Section 4. [sent-269, score-0.137]
</p><p>31 A sparseness constraint for feature selection is introduced in Section 5. [sent-270, score-0.362]
</p><p>32 Section 6, ﬁnally, contains an evaluation of the new method for DNA microarray data as well as benchmark results with standard classiﬁers which are based on standard feature selection procedures. [sent-271, score-0.42]
</p><p>33 2  Large Margin Classiﬁers for Matrix Data  In the following we consider two sets X and Z of objects, which are described by feature vectors x and z. [sent-272, score-0.253]
</p><p>34 Based on the feature vectors x we construct a linear classiﬁer deﬁned through the classiﬁcation function f (x) = w, x + b,  (1)  where . [sent-273, score-0.253]
</p><p>35 The hyperplane’s margin γ with respect to X is given by ˆ γ = min | w, x + b/ w x∈X  2  |. [sent-277, score-0.166]
</p><p>36 (2)  Setting γ = w −1 allows us to treat normal vectors w which are not normalized, 2 if the margin is normalized to 1. [sent-278, score-0.2]
</p><p>37 The hyperplane with largest margin is then obtained by minimizing w 2 for a margin which equals 1. [sent-280, score-0.28]
</p><p>38 (1), can be bounded from above with probability 1 − δ by the bound B, B(L, a/γ, δ)  =  2 L  log2 EN  γ , F, 2L 2a  + log2  4La δγ  ,  (3)  provided that the training classiﬁcation error is zero and f (x) is bounded by −a ≤ f (x) ≤ a for all x drawn iid from the (unknown) distribution of objects. [sent-282, score-0.165]
</p><p>39 L denotes the number of training objects x, γ denotes the margin and EN ( , F, L) the expected -covering number of a class F of functions that map data objects from T to [0, 1] (see Theorem 7. [sent-283, score-0.924]
</p><p>40 a is not known in general, however, because the probability distribution of objects (in particular its support) is not known. [sent-286, score-0.403]
</p><p>41 5 maxi w, xi − mini w, xi of values in the training set and minimize the quantity B(L, m/γ, δ) instead of eq. [sent-288, score-0.137]
</p><p>42 , xL be the matrix of feature vectors of L objects from the set X and Z := z 1 , z 2 , . [sent-293, score-0.769]
</p><p>43 , z P be the matrix of feature vectors of P objects from the set Z. [sent-296, score-0.769]
</p><p>44 The objects of set X are labeled, and we summarize all labels using a label matrix Y : [Y ]ij := y i δij ∈ RL×L , where δ is the Kronecker-Delta. [sent-297, score-0.516]
</p><p>45 Let us consider the case that the feature vectors X and Z are unknown, but that we are given the matrix K := X T Z of the corresponding scalar products. [sent-298, score-0.415]
</p><p>46 The training set is then given by the data matrix K and the corresponding label matrix Y . [sent-299, score-0.226]
</p><p>47 The principle of structural risk minimization is implemented by minimizing an upper bound on  2  (m/γ) given by X T w 2 , as can be seen from m/γ ≤ 2  w  2  ˆ maxi | w, xi | ≤  2  ( w, xi ) = X T w 2 . [sent-300, score-0.381]
</p><p>48 The constraints f (xi ) = y i imposed by the training + set are taken into account using the expressions 1 − ξi ≤ y i w, xi + b ≤ − + − 1 + ξi , where ξi , ξi ≥ 0 are slack variables which should also be minimized. [sent-301, score-0.09]
</p><p>49 ˜ α In the following we set M + = M − = M and C := M Y X T Z −1 so that row ˜ α ∞ ≤ C implies α ∞ ≤ Y X T Z row α ∞ ≤ M , where . [sent-315, score-0.29]
</p><p>50 (5), and as long as a = m holds true for all possible objects x (which are assumed to be drawn iid), the generalization error is bounded by eq. [sent-324, score-0.489]
</p><p>51 (7) 2L But note, that not all outliers are misclassiﬁed, and the trivial bound on the generalization error is still of the order L−1 . [sent-330, score-0.127]
</p><p>52 P {| w, x | > m} ≤  3  Kernel Functions, Measurements and Scalar Products  In the last section we have assumed that the matrix K is derived from scalar products between the feature vectors x and z which describe the objects from the sets X and Z. [sent-331, score-0.879]
</p><p>53 The feature vectors are not known and it is even unclear whether they exist. [sent-333, score-0.253]
</p><p>54 In order to apply the results of Section 2 to practical problems the following question remains to be answered: What are the conditions under which the measurement operator k(. [sent-334, score-0.124]
</p><p>55 , z) can indeed be interpreted as a scalar product between feature vectors and under which the matrix K can be interpreted as a matrix of kernel evaluations? [sent-335, score-0.691]
</p><p>56 Let L2 (H) denote the set of functions h from H with h2 (x)dx < ∞ and 2 the set of inﬁnite vectors (a1 , a2 , . [sent-337, score-0.082]
</p><p>57 Let α be from L2 (H1 ) and let k be a kernel from L2 (H2 , H1 ) which deﬁnes a HilbertSchmidt operator Tk : H1 → H2 (Tk α)(x) = f (x) =  k(x, z) α(z) dz . [sent-342, score-0.211]
</p><p>58 (8)  Then there exists an expansion k(x, z) = n sn en (z) gn (x) which converges in the L2 -sense. [sent-343, score-0.524]
</p><p>59 The sn ≥ 0 are the singular values of Tk , and en ∈ H1 , gn ∈ H2 are the corresponding orthonormal functions. [sent-344, score-0.535]
</p><p>60 Corollary 1 (Linear Classiﬁcation in 2 ) Let the assumptions of Theorem 1 hold and let H1 (k(x, z))2 dz ≤ K 2 for all x. [sent-345, score-0.124]
</p><p>61 Then the following holds true: • w, φ(x) ∈ • f  2 H2  =  2  , where w  ∗ Tk Tk α, α  2  H1 ,  2  =  α  2 H1 ,  and  ∗ where Tk is the adjoint operator of Tk ,  and the following sum convergences absolutely and uniformly: f (x) =  w, φ(x)  2  =  sn α, en  H1  gn (x) . [sent-355, score-0.614]
</p><p>62 We deﬁne a second mapping from H1 to the feature space by ω (z) := (e1 (z), e2 (z), . [sent-359, score-0.171]
</p><p>63 Theorem 1 tells us that any measurement kernel k applied to objects x and z can be expressed for almost all x and z as k(x, z) = φ (x) , ω (z) , where . [sent-367, score-0.496]
</p><p>64 deﬁnes a dot product in some feature space for almost all x, z. [sent-368, score-0.226]
</p><p>65 Hence, we can deﬁne the a matrix X := φ x1 , φ x2 , . [sent-369, score-0.113]
</p><p>66 , φ xL of feature vectors for the L column objects and a matrix Z := ω z 1 , ω z 2 , . [sent-372, score-0.875]
</p><p>67 , ω z P of feature vectors for the P row objects and apply the results of Section 2. [sent-375, score-0.835]
</p><p>68 4  Pairwise Data  An interesting special case occurs if row and column objects coincide. [sent-376, score-0.695]
</p><p>69 This kind of data is known as pairwise data [5, 4, 8] where the objects to be classiﬁed serve as features and vice versa. [sent-377, score-0.59]
</p><p>70 Like in Section 3 we can expand the measurement kernel via singular value decomposition but that would introduce two diﬀerent mappings (φ and ω) into the feature space. [sent-378, score-0.325]
</p><p>71 We will use one map for row and column objects and perform an eigenvalue decomposition. [sent-379, score-0.654]
</p><p>72 Then there exists an 2 expansion k(x, z) = n νn en (z) en (x) which converges in the L -sense. [sent-383, score-0.637]
</p><p>73 The νn are the eigenvalues of Tk with the corresponding orthonormal eigenfunctions en . [sent-384, score-0.313]
</p><p>74 Then the following holds true: w  2  2 S  =  n  |νn | α, en  sign(νn )  2  2  =  H  2  n νn 2  α, en  2 H  =  Tk α, α  H,  φ(x) 2 = = k(x, x) in the L sense, and the following sum n νn en (x) S convergences absolutely and uniformly: f (x) =  w, φ(x)  2 S  =  νn α, en  H  en (x) . [sent-396, score-1.507]
</p><p>75 If k is both continuous and positive deﬁnite and if H is compact, then the sum converges uniformly and absolutely for all x (Mercer). [sent-403, score-0.185]
</p><p>76 We choose the regularization term such that it enforces sparseness and that it also can be used for feature selection. [sent-405, score-0.306]
</p><p>77 The dual optimization problem is then given by 1 T α+ − α − K T K α+ − α − − (11) min α 2 1T Y K α+ − α − + 1 T α+ + α − s. [sent-408, score-0.094]
</p><p>78 i=1 αi − αi This saves on the number of measurements z i , u for new objects and yields to improved classiﬁcation performance due to the reduced number of features z i [15]. [sent-414, score-0.453]
</p><p>79 6  Application to DNA Microarray Data  We apply our new method to the DNA microarray data published in [9]. [sent-415, score-0.127]
</p><p>80 Column objects are samples from diﬀerent brain tumors of the medullablastoma kind. [sent-416, score-0.447]
</p><p>81 The samples were obtained from 60 patients, which were treated in a similar way and the samples were labeled according to whether a patient responded well to chemoor radiation therapy. [sent-417, score-0.088]
</p><p>82 Transcriptions of 7,129 genes were tagged with ﬂuorescent dyes and used as a probe in a binding assay. [sent-419, score-0.081]
</p><p>83 For every sample-gene pair, the ﬂuorescence of the bound transcripts - a snapshot of the level of gene expression - was measured. [sent-420, score-0.191]
</p><p>84 This gave rise to a 60 × 7, 129 real valued sample-gene matrix where each entry represents the level of gene expression in the corresponding sample. [sent-421, score-0.266]
</p><p>85 Therefore, feature selection is a prerequisite for good generalization [6, 16]. [sent-425, score-0.34]
</p><p>86 In a ﬁrst step, we apply our new method on a 59 × 7, 129 matrix, where one column object was withhold to avoid biased feature selection. [sent-427, score-0.311]
</p><p>87 In a second step, we use the selected features only and apply our method once more on the reduced sample-gene matrix, but now with a small value of . [sent-429, score-0.136]
</p><p>88 Feature Selection / Classiﬁcation TrkC statistic / SVM statistic / Comb1 statistic / KNN statistic / Comb2  # F 1 8  # E 20 15 14 13 12  Feature Selection / Classiﬁcation P-SVM / C-SVM P-SVM / C-SVM P-SVM / P-SVM  C 1. [sent-431, score-0.364]
</p><p>89 1  # F 40/45/50 40/45/50 40/45/50  # E 5/4/5 5/5/5 4/4/5  Table 1: Benchmark results for DNA microarray data (for explanations see text). [sent-434, score-0.093]
</p><p>90 The table shows the classiﬁcation error given by the number of wrong classiﬁcations (“E”) for diﬀerent numbers of selected features (“F”) and for diﬀerent values of the parameter C. [sent-435, score-0.175]
</p><p>91 The feature selection method is signal-to-noise-statistic and t-statitic denoted by “statistic” or our method P-SVM. [sent-436, score-0.293]
</p><p>92 Table 1 shows the result of a leave-one-out cross-validation procedure, where the classiﬁcation error is given for diﬀerent numbers of selected features. [sent-439, score-0.087]
</p><p>93 Our method (P-SVM) is compared with “TrkC”-Gene classiﬁcation (one gene classiﬁcation), standard SVMs, weighted “TrkC”/SVM-classiﬁcation, K nearest neighbor (KNN), and a combined SVM/TrkC/KNN classiﬁer. [sent-440, score-0.11]
</p><p>94 For the latter methods, feature selection was based on the correlation of features with classes using signal-to-noisestatistics and t-statistics [3]. [sent-441, score-0.343]
</p><p>95 5 for feature selection in step one which gave rise to 10 − 1000 selected features. [sent-445, score-0.345]
</p><p>96 The feature selection procedure (also a classiﬁer) had its lowest misclassiﬁcation rate between 20 and 40 features. [sent-446, score-0.293]
</p><p>97 Our feature selection method clearly outperforms standard methods — the number of misclassiﬁcation is down by a factor of 3 (for 45 selected genes). [sent-449, score-0.345]
</p><p>98 Molecular classiﬁcation of cancer: Class discovery and class prediction by gene expression monitoring. [sent-488, score-0.153]
</p><p>99 Gene selection for cancer classiﬁcation using support vector machines. [sent-517, score-0.207]
</p><p>100 Prediction of central nervous system embryonal tumour outcome based on gene expression. [sent-578, score-0.144]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('objects', 0.403), ('classi', 0.341), ('en', 0.274), ('tk', 0.209), ('feature', 0.171), ('trkc', 0.166), ('row', 0.145), ('pairwise', 0.137), ('er', 0.136), ('cation', 0.129), ('selection', 0.122), ('margin', 0.118), ('matrix', 0.113), ('gene', 0.11), ('column', 0.106), ('erent', 0.102), ('gn', 0.1), ('dna', 0.098), ('microarray', 0.093), ('statistic', 0.091), ('structural', 0.089), ('hochreiter', 0.087), ('risk', 0.084), ('di', 0.083), ('vectors', 0.082), ('genes', 0.081), ('absolutely', 0.079), ('knn', 0.079), ('dz', 0.074), ('theorem', 0.072), ('gram', 0.071), ('sparseness', 0.069), ('gaasenbeek', 0.067), ('mesirov', 0.067), ('tamayo', 0.067), ('regularization', 0.066), ('singular', 0.061), ('products', 0.061), ('obermayer', 0.061), ('sn', 0.061), ('ers', 0.06), ('interpreted', 0.059), ('mukherjee', 0.058), ('minkowski', 0.058), ('convergences', 0.058), ('guyon', 0.058), ('misclassi', 0.055), ('dot', 0.055), ('expansion', 0.055), ('selected', 0.052), ('proximity', 0.052), ('sign', 0.051), ('correspond', 0.05), ('corollary', 0.05), ('let', 0.05), ('features', 0.05), ('poggio', 0.049), ('iid', 0.049), ('maxi', 0.049), ('scalar', 0.049), ('matrices', 0.049), ('min', 0.048), ('measurement', 0.048), ('generalization', 0.047), ('support', 0.047), ('svms', 0.046), ('slack', 0.046), ('dual', 0.046), ('kernel', 0.045), ('xi', 0.044), ('samples', 0.044), ('bengio', 0.044), ('hyperplane', 0.044), ('expression', 0.043), ('graepel', 0.042), ('outliers', 0.042), ('operator', 0.042), ('occurs', 0.041), ('weston', 0.041), ('williamson', 0.041), ('lecun', 0.041), ('bartlett', 0.04), ('bounded', 0.039), ('herbrich', 0.039), ('orthonormal', 0.039), ('rows', 0.039), ('bound', 0.038), ('wrong', 0.038), ('cancer', 0.038), ('datasets', 0.037), ('positive', 0.037), ('nite', 0.036), ('sparse', 0.036), ('uniformly', 0.035), ('numbers', 0.035), ('outcome', 0.034), ('benchmark', 0.034), ('converges', 0.034), ('apply', 0.034), ('principle', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="88-tfidf-1" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>2 0.23481631 <a title="88-tfidf-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.2096979 <a title="88-tfidf-3" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>4 0.19248277 <a title="88-tfidf-4" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>Author: Baback Moghaddam, Gregory Shakhnarovich</p><p>Abstract: We introduce a novel learning algorithm for binary classiﬁcation with hyperplane discriminants based on pairs of training points from opposite classes (dyadic hypercuts). This algorithm is further extended to nonlinear discriminants using kernel functions satisfying Mercer’s conditions. An ensemble of simple dyadic hypercuts is learned incrementally by means of a conﬁdence-rated version of AdaBoost, which provides a sound strategy for searching through the ﬁnite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classiﬁers was found to be comparable to that of SVMs and k-NN classiﬁers. Furthermore, the computational cost of classiﬁcation (at run time) was found to be similar to, or better than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we oﬀer an on-line and incremental learning machine for building kernel discriminants whose complexity (number of kernel evaluations) can be directly controlled (traded oﬀ for accuracy). 1</p><p>5 0.1761978 <a title="88-tfidf-5" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>6 0.17530936 <a title="88-tfidf-6" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>7 0.16522339 <a title="88-tfidf-7" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>8 0.16514319 <a title="88-tfidf-8" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>9 0.15539891 <a title="88-tfidf-9" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>10 0.15152149 <a title="88-tfidf-10" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>11 0.15127738 <a title="88-tfidf-11" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>12 0.14948842 <a title="88-tfidf-12" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>13 0.1447276 <a title="88-tfidf-13" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>14 0.14123635 <a title="88-tfidf-14" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>15 0.13656782 <a title="88-tfidf-15" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>16 0.13569258 <a title="88-tfidf-16" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>17 0.13079387 <a title="88-tfidf-17" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>18 0.12909205 <a title="88-tfidf-18" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>19 0.12904529 <a title="88-tfidf-19" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>20 0.12825735 <a title="88-tfidf-20" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.33), (1, -0.234), (2, 0.146), (3, -0.068), (4, 0.262), (5, -0.059), (6, 0.029), (7, -0.152), (8, -0.048), (9, 0.058), (10, -0.024), (11, 0.047), (12, -0.041), (13, 0.001), (14, 0.104), (15, -0.106), (16, 0.012), (17, 0.02), (18, -0.013), (19, 0.047), (20, -0.106), (21, 0.034), (22, 0.096), (23, 0.02), (24, -0.081), (25, 0.028), (26, 0.024), (27, 0.007), (28, 0.004), (29, 0.118), (30, -0.04), (31, 0.099), (32, -0.148), (33, -0.059), (34, -0.093), (35, -0.14), (36, 0.099), (37, 0.076), (38, -0.029), (39, 0.04), (40, 0.015), (41, 0.005), (42, 0.059), (43, -0.003), (44, 0.071), (45, -0.003), (46, -0.103), (47, 0.021), (48, 0.051), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97949851 <a title="88-lsi-1" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>2 0.83455068 <a title="88-lsi-2" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>Author: Elzbieta Pekalska, David Tax, Robert Duin</p><p>Abstract: Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identiﬁcation or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classiﬁer for dissimilarity representations. By the use of linear programming an efﬁcient one-class description can be found, based on a small number of prototype objects. This classiﬁer can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classiﬁer by Campbell and Bennett is given.</p><p>3 0.70136118 <a title="88-lsi-3" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>Author: Baback Moghaddam, Gregory Shakhnarovich</p><p>Abstract: We introduce a novel learning algorithm for binary classiﬁcation with hyperplane discriminants based on pairs of training points from opposite classes (dyadic hypercuts). This algorithm is further extended to nonlinear discriminants using kernel functions satisfying Mercer’s conditions. An ensemble of simple dyadic hypercuts is learned incrementally by means of a conﬁdence-rated version of AdaBoost, which provides a sound strategy for searching through the ﬁnite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classiﬁers was found to be comparable to that of SVMs and k-NN classiﬁers. Furthermore, the computational cost of classiﬁcation (at run time) was found to be similar to, or better than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we oﬀer an on-line and incremental learning machine for building kernel discriminants whose complexity (number of kernel evaluations) can be directly controlled (traded oﬀ for accuracy). 1</p><p>4 0.70119262 <a title="88-lsi-4" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>5 0.68784249 <a title="88-lsi-5" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>Author: Stan Z. Li, Zhenqiu Zhang, Heung-yeung Shum, Hongjiang Zhang</p><p>Abstract: AdaBoost [3] minimizes an upper error bound which is an exponential function of the margin on the training set [14]. However, the ultimate goal in applications of pattern classiﬁcation is always minimum error rate. On the other hand, AdaBoost needs an effective procedure for learning weak classiﬁers, which by itself is difﬁcult especially for high dimensional data. In this paper, we present a novel procedure, called FloatBoost, for learning a better boosted classiﬁer. FloatBoost uses a backtrack mechanism after each iteration of AdaBoost to remove weak classiﬁers which cause higher error rates. The resulting ﬂoat-boosted classiﬁer consists of fewer weak classiﬁers yet achieves lower error rates than AdaBoost in both training and test. We also propose a statistical model for learning weak classiﬁers, based on a stagewise approximation of the posterior using an overcomplete set of scalar features. Experimental comparisons of FloatBoost and AdaBoost are provided through a difﬁcult classiﬁcation problem, face detection, where the goal is to learn from training examples a highly nonlinear classiﬁer to differentiate between face and nonface patterns in a high dimensional space. The results clearly demonstrate the promises made by FloatBoost over AdaBoost.</p><p>6 0.6808902 <a title="88-lsi-6" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>7 0.66853809 <a title="88-lsi-7" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>8 0.64988708 <a title="88-lsi-8" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>9 0.63631964 <a title="88-lsi-9" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>10 0.63338506 <a title="88-lsi-10" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>11 0.62774068 <a title="88-lsi-11" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>12 0.62170398 <a title="88-lsi-12" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>13 0.59896344 <a title="88-lsi-13" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>14 0.59521145 <a title="88-lsi-14" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>15 0.55663168 <a title="88-lsi-15" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>16 0.54308319 <a title="88-lsi-16" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>17 0.53740031 <a title="88-lsi-17" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>18 0.52983153 <a title="88-lsi-18" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>19 0.52904576 <a title="88-lsi-19" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>20 0.52461815 <a title="88-lsi-20" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.066), (23, 0.028), (38, 0.022), (42, 0.081), (54, 0.161), (55, 0.059), (68, 0.019), (74, 0.102), (86, 0.16), (92, 0.067), (98, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96687722 <a title="88-lda-1" href="./nips-2002-Minimax_Differential_Dynamic_Programming%3A_An_Application_to_Robust_Biped_Walking.html">144 nips-2002-Minimax Differential Dynamic Programming: An Application to Robust Biped Walking</a></p>
<p>Author: Jun Morimoto, Christopher G. Atkeson</p><p>Abstract: We developed a robust control policy design method in high-dimensional state space by using differential dynamic programming with a minimax criterion. As an example, we applied our method to a simulated ﬁve link biped robot. The results show lower joint torques from the optimal control policy compared to a hand-tuned PD servo controller. Results also show that the simulated biped robot can successfully walk with unknown disturbances that cause controllers generated by standard differential dynamic programming and the hand-tuned PD servo to fail. Learning to compensate for modeling error and previously unknown disturbances in conjunction with robust control design is also demonstrated.</p><p>2 0.94992524 <a title="88-lda-2" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>Author: Maxim Likhachev, Sven Koenig</p><p>Abstract: In this paper, we introduce an efﬁcient replanning algorithm for nondeterministic domains, namely what we believe to be the ﬁrst incremental heuristic minimax search algorithm. We apply it to the dynamic discretization of continuous domains, resulting in an efﬁcient implementation of the parti-game reinforcement-learning algorithm for control in high-dimensional domains.</p><p>same-paper 3 0.89891428 <a title="88-lda-3" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>4 0.8348406 <a title="88-lda-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.83026689 <a title="88-lda-5" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>Author: Peter Meinicke, Thorsten Twellmann, Helge Ritter</p><p>Abstract: We propose a framework for classiﬁer design based on discriminative densities for representation of the differences of the class-conditional distributions in a way that is optimal for classiﬁcation. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximization of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability density functions, the resulting maximum contrast classiﬁers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vector Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approximate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classiﬁcation performance.</p><p>6 0.82894492 <a title="88-lda-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.82814205 <a title="88-lda-7" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>8 0.82812196 <a title="88-lda-8" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>9 0.82769108 <a title="88-lda-9" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>10 0.82397962 <a title="88-lda-10" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>11 0.82373166 <a title="88-lda-11" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>12 0.82307285 <a title="88-lda-12" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>13 0.82284379 <a title="88-lda-13" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>14 0.8223474 <a title="88-lda-14" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>15 0.81988758 <a title="88-lda-15" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>16 0.81754464 <a title="88-lda-16" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>17 0.81738567 <a title="88-lda-17" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>18 0.81559682 <a title="88-lda-18" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>19 0.81521773 <a title="88-lda-19" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>20 0.81500065 <a title="88-lda-20" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
