<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-109" href="#">nips2002-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</h1>
<br/><p>Source: <a title="nips-2002-109-pdf" href="http://papers.nips.cc/paper/2291-improving-a-page-classifier-with-anchor-extraction-and-link-analysis.pdf">pdf</a></p><p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>Reference: <a title="nips-2002-109-reference" href="../nips2002_reference/nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Most text categorization systems use simple models of documents and document collections. [sent-3, score-0.197]
</p><p>2 In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. [sent-4, score-2.71]
</p><p>3 On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. [sent-5, score-0.134]
</p><p>4 The system uses a variant of co-training to exploit unlabeled data from a new site. [sent-6, score-0.204]
</p><p>5 1 Introduction Most text categorization systems use simple models of documents and document collections. [sent-8, score-0.197]
</p><p>6 For instance, it is common to model documents as “bags of words”, and to model a collection as a set of documents drawn from some ﬁxed distribution. [sent-9, score-0.148]
</p><p>7 An interesting question is how to exploit more detailed information about the structure of individual documents, or the structure of a collection of documents. [sent-10, score-0.168]
</p><p>8 For web page categorization, a frequently-used approach is to use hyperlink information to improve classiﬁcation accuracy (e. [sent-11, score-1.098]
</p><p>9 Often hyperlink structure is used to “smooth” the predictions of a learned classiﬁer, so that documents that (say) are pointed to by the same “hub” page will be more likely to have the same classiﬁcation after smoothing. [sent-14, score-0.934]
</p><p>10 This smoothing can be done either explicitly [15] or implicitly (for instance, by representing examples so that the distance between examples depends on hyperlink connectivity [7, 9]). [sent-15, score-0.265]
</p><p>11 The structure of individual pages, as represented by HTML markup structure or linguis-  tic structure, is less commonly used in web page classiﬁcation: however, page structure is often used in extracting information from web pages. [sent-16, score-1.868]
</p><p>12 Page structure seems to be particularly important in ﬁnding site-speciﬁc extraction rules (“wrappers”), since on a given site, formatting information is frequently an excellent indication of content [6, 10, 12]. [sent-17, score-0.189]
</p><p>13 This paper is based on two practical observations about web page classiﬁcation. [sent-18, score-0.812]
</p><p>14 , product pages, job-posting pages, and press releases) many sites contain “hub” or index pages that point to essentially all pages in that category on a site. [sent-21, score-0.341]
</p><p>15 These hubs rarely link exclusively to pages of a single category—instead the hubs will contain a number of additional links, such as links back to a home page and links to related hubs. [sent-22, score-1.213]
</p><p>16 However, the page structure of a hub page often gives strong indications of which links are to pages from the “main” category associated with the hub, and which are ancillary links that exist for other (e. [sent-23, score-2.042]
</p><p>17 Links to pages in the main category associated with this hub (previous NIPS conference homepages) are in the left-hand column of the table, and hence can be easily identiﬁed by the page structure. [sent-27, score-1.147]
</p><p>18 The second observation is that it is relatively easy to learn to extract links from hub pages to main-category pages using existing wrapper-learning methods [8, 6]. [sent-28, score-0.896]
</p><p>19 Wrapper-learning techniques interactively learn to extract data of some type from a single site using userprovided training examples. [sent-29, score-0.225]
</p><p>20 Our experience in a number of domains indicates that maincategory links on hub pages (like the NIPS-homepage links from Figure 1) can almost always be learned from two or three positive examples. [sent-30, score-0.888]
</p><p>21 Exploiting these observations, we describe in this paper a web page categorization system that exploits link structure within a site, as well as page structure within hub pages, to improve classiﬁcation accuracy of a traditional bag-of-words classiﬁer on pages from a previously unseen site. [sent-31, score-2.299]
</p><p>22 The system uses a variant of co-training [3] to exploit unlabeled data from a new, previously unseen site. [sent-32, score-0.25]
</p><p>23 Speciﬁcally, pages are labeled using a simple bag-of-words classiﬁer, and the results are used by a restricted wrapper-learner to propose potential “main-category link wrappers”. [sent-33, score-0.285]
</p><p>24 These wrappers are then used as features by a decision tree learner to ﬁnd a categorization of the pages on the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer. [sent-34, score-1.108]
</p><p>25 2 One-step co-training and hyperlink structure Consider a binary bag-of-words classiﬁer f that has been learned from some set of labeled web pages D . [sent-35, score-0.755]
</p><p>26 We wish to improve the performance of f on pages from an unknown web site S, by smoothing its predictions in a way that is plausible given the hyperlink of S, and the page structure of potential hub pages in S. [sent-36, score-1.973]
</p><p>27 As background for the algorithm, let us consider ﬁrst co-training, a well-studied approach for improving classiﬁer performance using unlabeled data [3]. [sent-37, score-0.144]
</p><p>28 edu) created and maintained these web pages from 1994 until 1996. [sent-51, score-0.45]
</p><p>29 NIPS*2000  NIPS 13, the conference proceedings for 2000 (”Advances in Neural Information Processing Systems 13”, edited by Leen, Todd K. [sent-60, score-0.065]
</p><p>30 ∗ Abstracts and papers from this forthcoming volume are available on-line. [sent-63, score-0.112]
</p><p>31 ∗ BibTeX entries for all papers from this forthcoming volume are available on-line. [sent-64, score-0.112]
</p><p>32 Abstracts and papers from this volume are available on-line. [sent-66, score-0.076]
</p><p>33 Abstracts and (some) papers from this volume are available on-line. [sent-68, score-0.076]
</p><p>34 Links to pages in the main category associated with this hub are in the left-hand column of the table. [sent-73, score-0.602]
</p><p>35 In this setting, a large amount of unlabeled data Du can be used to improve the accuracy of a small set of labeled data D , as follows. [sent-74, score-0.296]
</p><p>36 Then, use f1 to label the examples in Du , and use A2 to learn from this training set. [sent-76, score-0.096]
</p><p>37 Given the assumptions above, f1 ’s errors on Du will appear to A2 as random, uncorrelated noise, and A2 can in principle learn an arbitrarily good approximation to f , given enough unlabeled data in Du . [sent-77, score-0.182]
</p><p>38 Now, consider a set DS of unlabeled pages from a unseen web site S. [sent-79, score-0.762]
</p><p>39 It seems not unreasonable to assume that the words x1 on a page x ∈ S and the hub pages x2 ∈ S that hyperlink to x are independent, given the class of x. [sent-80, score-1.275]
</p><p>40 Let S be a web site, f1 be a bag-of-words page classiﬁer, and DS be the pages on the site S. [sent-83, score-1.09]
</p><p>41 For each page xi ∈ DS , represent xi as a vector of all pages in S that hyperlink to xi . [sent-86, score-1.005]
</p><p>42 Use a learner A2 to learn f2 from the labeled examples D2 = {(xi , y i )}i . [sent-91, score-0.217]
</p><p>43 Use f2 (x) as the ﬁnal label for each page x ∈ DS . [sent-94, score-0.547]
</p><p>44 In experimental studies, co-training is usually done iteratively, alternating between using f1 and f2 for tagging the unlabeled data. [sent-96, score-0.144]
</p><p>45 The one-step version seems more appropriate in this setting, in which there are a limited number of unlabeled examples over which each x2 is deﬁned. [sent-97, score-0.173]
</p><p>46 1 Learning to extract anchors from web pages Algorithm 1 has some shortcomings. [sent-99, score-0.557]
</p><p>47 Co-training assumes a large pool of unlabeled data: however, if the informative hubs for pages on S are mostly within S (a very plausible assumption) then the amount of useful unlabeled data is limited by the size of S. [sent-100, score-0.493]
</p><p>48 With limited amounts of unlabeled data, it is very important that A2 has a strong (and appropriate) statistical bias, and that A2 has some effective method for avoiding overﬁtting. [sent-101, score-0.144]
</p><p>49 As suggested by Figure 1, the informativeness of hub features can be improved by using knowledge of the structure of hub pages themselves. [sent-102, score-1.082]
</p><p>50 To make use of hub page structure, we used a wrapper-learning system called WL2 , which has experimentally proven to be effective at learning substructures of web pages [6]. [sent-103, score-1.385]
</p><p>51 The output of WL 2 is an extraction predicate: a binary relation p between pages x and substrings a within x. [sent-104, score-0.235]
</p><p>52 As an example, WL2 might output p = {(x, a) : x is the page of Figure 1 and a is an anchor appearing in the ﬁrst column of the table}. [sent-105, score-0.641]
</p><p>53 (An anchor is a substring of a web page that deﬁnes a hyperlink. [sent-106, score-0.968]
</p><p>54 ) This suggests a modiﬁcation of Algorithm 1, in which one-step co-training is carried out on the problem of extracting anchors rather than the problem of labeling web pages. [sent-107, score-0.488]
</p><p>55 Speciﬁcally, one might map f1 ’s predictions from web pages to anchors, by giving a positive label to anchor a iff a links to a page x such that f1 (x) = 1; then use WL2 algorithm A2 to learn a predicate p2 ; and ﬁnally, map the predictions of p2 from anchors back to web pages. [sent-108, score-1.862]
</p><p>56 Another problem is that it unclear how to map class labels from anchors back to web pages, since a page might be pointed to by many different anchors. [sent-110, score-0.994]
</p><p>57 2 Bridging the gap between anchors and pages Based on these observations we modiﬁed Algorithm 1 as follows. [sent-112, score-0.239]
</p><p>58 As suggested, we map the predictions about page labels made by f1 to anchors. [sent-113, score-0.55]
</p><p>59 Using these anchor labels, we then produce many small training sets that are passed to WL2 . [sent-114, score-0.123]
</p><p>60 Finally, we use the many wrappers produced by WL2 as features in a representation of a page x, and again use a learner to combine the wrapper-features and produce a single classiﬁcation for a page. [sent-116, score-0.788]
</p><p>61 Let S be a web site, f1 be a bag-of-words page classiﬁer, and DS be the pages on the site. [sent-119, score-0.927]
</p><p>62 For each anchor a on a page x ∈ S, label a as tentatively-positive  if a points to a page x such that x ∈ S and f1 (x ) = 1. [sent-122, score-1.188]
</p><p>63 Let P be the set of all pairs (x, a) where a is a tentativelypositive link and x is the page on which a is found. [sent-125, score-0.615]
</p><p>64 , Dk containing such pairs, and for each subset Di , use WL2 to produce a number of possible extraction predicates pi,1 , . [sent-129, score-0.15]
</p><p>65 We will say that the “wrapper predicate” p ij links to x iff pij includes some pair (x , a) such that x ∈ DS and a is a hyperlink to page x. [sent-136, score-0.94]
</p><p>66 For each page xi ∈ DS , represent xi as a vector of all wrappers pij that link to x. [sent-137, score-0.933]
</p><p>67 Use a learner A2 to learn f2 from the labeled examples DS = {(xi , y i )}i . [sent-142, score-0.217]
</p><p>68 Use f2 (x) as the ﬁnal label for each page x ∈ DS . [sent-145, score-0.547]
</p><p>69 In this case, in building a page classiﬁer, one would like to exploit knowledge about the related problem of link extraction. [sent-147, score-0.651]
</p><p>70 The bagging-like approach of “feeding” WL 2 many small training sets, and the use of a second learning algorithm to aggregate the results of WL 2 , are a means of exploiting prior experimental results, in lieu of more precise statistical assumptions. [sent-153, score-0.069]
</p><p>71 4 Experimental results To evaluate the technique, we used the task of categorizing web pages from company sites as executive biography or other. [sent-154, score-0.586]
</p><p>72 We selected nine company web sites with non-trivial hub structures. [sent-155, score-0.885]
</p><p>73 These were crawled using a heuristic spidering strategy intended to ﬁnd executive biography pages with high recall. [sent-156, score-0.197]
</p><p>74 1 The crawl found 879 pages, of which 128 were labeled positive. [sent-157, score-0.073]
</p><p>75 A simple bag-of-words classiﬁer f1 was trained using a disjoint set of sites (different from the nine above), obtaining an average accuracy of 91. [sent-158, score-0.171]
</p><p>76 Algorithm 2 improves over the baseline classiﬁer f1 on six of the nine sites, and obtains the same accuracy on two more. [sent-164, score-0.139]
</p><p>77 This difference is signiﬁcant at the 98% level with a 2-tailed paired sign test, and at the 95% level with a 2-tailed paired t test. [sent-165, score-0.068]
</p><p>78 5-like decision tree learning algorithm [14] for learner A2 . [sent-167, score-0.101]
</p><p>79 5 Related work The introduction discusses the relationship between this work and a number of previous techniques for using hyperlink structure in web page classiﬁcation [7, 9, 15]. [sent-232, score-1.085]
</p><p>80 The WL 2 based method for ﬁnding document structure has antecedents in other techniques for learning [10, 12] and automatically detecting [4, 5] structure in web pages. [sent-233, score-0.46]
</p><p>81 Blei et al do not address the speciﬁc problem considered here, of using both page structure and hyperlink structure in web page classiﬁcation. [sent-235, score-1.727]
</p><p>82 However, they do apply their technique to two closely related problems: they augment a page classiﬁcation method with local features based on the page’s URL, and also augment content-based classiﬁcation of “text nodes” (speciﬁc substrings of a web page) with page-structure-based local features. [sent-236, score-0.949]
</p><p>83 (In fact, Blei et al generated page-structure-based features for their extraction task in exactly this way: the only difference is that WL2 was parameterized differently. [sent-238, score-0.185]
</p><p>84 6 Conclusions We have described a technique that improves a simple web page classiﬁer by exploiting link structure within a site, as well as page structure within hub pages. [sent-241, score-2.092]
</p><p>85 The system uses a variant of co-training called “one-step co-training” to exploit unlabeled data from a new site. [sent-242, score-0.227]
</p><p>86 Next, results of this labeling are propogated to links to labeled pages, and these labeled links are used by a wrapper-learner called WL2 to propose potential “main-category link wrappers”. [sent-244, score-0.628]
</p><p>87 Finally, these wrappers  are used as features by another learner A2 to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer. [sent-245, score-0.993]
</p><p>88 On a real-world benchmark problem, this technique substantially improved the accuracy of a simple bag-of-words classiﬁer, reducing error rate by about half. [sent-247, score-0.107]
</p><p>89 Appendix A: Details on “Wrapper Proposal” Extraction predicates are constructed by WL2 using a rule-learning algorithm and a conﬁgurable set of components called builders. [sent-250, score-0.101]
</p><p>90 Each builder B corresponds to a language L B of extraction predicates. [sent-251, score-0.132]
</p><p>91 Given a set of pairs D = {(x i , ai )} such that each ai is a substring of xi , LGGB (D) is the least general p ∈ LB such that (x, a) ∈ D ⇒ (x, a) ∈ p. [sent-253, score-0.088]
</p><p>92 Depending on B, these properties might be membership in a particular syntactic HTML structure (e. [sent-255, score-0.066]
</p><p>93 These heuristics were based on the observation that in most extraction tasks, the items to be extracted are close together. [sent-261, score-0.122]
</p><p>94 (We also note that these heuristics were initially developed to support a different set of experiments [1], and were not substantially modiﬁed for the experiments in this paper. [sent-263, score-0.056]
</p><p>95 In our use of WL2 , we simply applied each builder Bj to a dataset Di , to get the set of predicates {pij } = {LGGBj (Di )}, instead of running the full WL2 learning algorithm. [sent-265, score-0.09]
</p><p>96 Learning with scope, with application to information extraction and classiﬁcation. [sent-270, score-0.096]
</p><p>97 Automatically extracting features for concept learning from the web. [sent-280, score-0.077]
</p><p>98 Learning page-independent heuristics for extracting data from web pages. [sent-285, score-0.366]
</p><p>99 The missing link - a probabilistic model of document content and hypertext connectivity. [sent-293, score-0.189]
</p><p>100 A structured wrapper induction system for extracting information from semi-structured documents. [sent-299, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('page', 0.518), ('hub', 0.435), ('web', 0.294), ('hyperlink', 0.207), ('links', 0.169), ('site', 0.163), ('wrappers', 0.162), ('unlabeled', 0.144), ('classi', 0.142), ('anchors', 0.124), ('anchor', 0.123), ('pages', 0.115), ('wrapper', 0.108), ('ds', 0.103), ('link', 0.097), ('extraction', 0.096), ('er', 0.095), ('learner', 0.077), ('blei', 0.076), ('documents', 0.074), ('labeled', 0.073), ('william', 0.071), ('structure', 0.066), ('categorization', 0.065), ('nips', 0.065), ('du', 0.063), ('hubs', 0.062), ('nine', 0.061), ('sites', 0.059), ('wl', 0.058), ('xi', 0.055), ('predicates', 0.054), ('winnow', 0.054), ('category', 0.052), ('accuracy', 0.051), ('papers', 0.051), ('html', 0.049), ('predicate', 0.049), ('abstracts', 0.049), ('extracting', 0.046), ('pij', 0.046), ('unseen', 0.046), ('exploiting', 0.045), ('cohen', 0.043), ('biography', 0.041), ('builders', 0.041), ('executive', 0.041), ('lggb', 0.041), ('maintained', 0.041), ('learn', 0.038), ('proceedings', 0.038), ('pointed', 0.037), ('forthcoming', 0.036), ('company', 0.036), ('builder', 0.036), ('exploit', 0.036), ('al', 0.035), ('paired', 0.034), ('agrees', 0.034), ('document', 0.034), ('substring', 0.033), ('di', 0.032), ('predictions', 0.032), ('se', 0.032), ('features', 0.031), ('hypertext', 0.031), ('cation', 0.031), ('substantially', 0.03), ('induction', 0.029), ('label', 0.029), ('examples', 0.029), ('improve', 0.028), ('assumes', 0.028), ('augment', 0.028), ('douglas', 0.028), ('content', 0.027), ('conference', 0.027), ('improves', 0.027), ('technique', 0.026), ('heuristics', 0.026), ('largely', 0.026), ('baker', 0.025), ('available', 0.025), ('substrings', 0.024), ('labeling', 0.024), ('algorithm', 0.024), ('variant', 0.024), ('text', 0.024), ('extract', 0.024), ('jensen', 0.024), ('attributes', 0.024), ('instance', 0.023), ('andrew', 0.023), ('et', 0.023), ('called', 0.023), ('june', 0.022), ('international', 0.022), ('cally', 0.022), ('back', 0.021), ('subsets', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="109-tfidf-1" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>2 0.12946475 <a title="109-tfidf-2" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>3 0.12858841 <a title="109-tfidf-3" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>4 0.10384692 <a title="109-tfidf-4" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>5 0.10330726 <a title="109-tfidf-5" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>6 0.09427914 <a title="109-tfidf-6" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>7 0.077268802 <a title="109-tfidf-7" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>8 0.072419368 <a title="109-tfidf-8" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>9 0.067652367 <a title="109-tfidf-9" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>10 0.066992514 <a title="109-tfidf-10" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>11 0.064194836 <a title="109-tfidf-11" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>12 0.063557483 <a title="109-tfidf-12" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>13 0.063275509 <a title="109-tfidf-13" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>14 0.055544853 <a title="109-tfidf-14" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>15 0.055441897 <a title="109-tfidf-15" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>16 0.055108823 <a title="109-tfidf-16" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>17 0.054677814 <a title="109-tfidf-17" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>18 0.054143526 <a title="109-tfidf-18" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>19 0.053849217 <a title="109-tfidf-19" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>20 0.053233534 <a title="109-tfidf-20" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.164), (1, -0.093), (2, 0.05), (3, -0.021), (4, 0.037), (5, 0.043), (6, -0.053), (7, -0.138), (8, 0.019), (9, -0.055), (10, -0.128), (11, 0.024), (12, 0.005), (13, 0.008), (14, 0.063), (15, -0.036), (16, 0.009), (17, 0.023), (18, 0.049), (19, 0.052), (20, 0.018), (21, 0.044), (22, -0.02), (23, -0.04), (24, 0.066), (25, -0.028), (26, -0.125), (27, -0.05), (28, -0.049), (29, -0.011), (30, -0.022), (31, 0.044), (32, 0.004), (33, 0.132), (34, -0.031), (35, 0.033), (36, 0.038), (37, 0.047), (38, 0.017), (39, 0.15), (40, 0.02), (41, 0.227), (42, -0.129), (43, 0.0), (44, 0.112), (45, 0.013), (46, 0.074), (47, 0.026), (48, -0.073), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93347013 <a title="109-lsi-1" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>2 0.77636421 <a title="109-lsi-2" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>3 0.56894666 <a title="109-lsi-3" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>4 0.48376817 <a title="109-lsi-4" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>5 0.4535833 <a title="109-lsi-5" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>6 0.45332801 <a title="109-lsi-6" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>7 0.43990102 <a title="109-lsi-7" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>8 0.4394201 <a title="109-lsi-8" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>9 0.41282219 <a title="109-lsi-9" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>10 0.39668667 <a title="109-lsi-10" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>11 0.38545573 <a title="109-lsi-11" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>12 0.38026774 <a title="109-lsi-12" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>13 0.3782447 <a title="109-lsi-13" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>14 0.3639051 <a title="109-lsi-14" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>15 0.35883322 <a title="109-lsi-15" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>16 0.35771307 <a title="109-lsi-16" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>17 0.35518768 <a title="109-lsi-17" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>18 0.35412994 <a title="109-lsi-18" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>19 0.33194527 <a title="109-lsi-19" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>20 0.32134867 <a title="109-lsi-20" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.014), (14, 0.013), (21, 0.271), (23, 0.014), (42, 0.086), (54, 0.091), (55, 0.041), (57, 0.022), (67, 0.021), (68, 0.025), (74, 0.134), (87, 0.01), (92, 0.026), (98, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81073344 <a title="109-lda-1" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>2 0.63328874 <a title="109-lda-2" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>Author: Rong Jin, Zoubin Ghahramani</p><p>Abstract: In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naIve methods degrade rapidly as more ambiguity is introduced into the labels. 1</p><p>3 0.62179238 <a title="109-lda-3" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>4 0.61808622 <a title="109-lda-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.61740232 <a title="109-lda-5" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>Author: Joaquin Quiñonero-candela, Ole Winther</p><p>Abstract: In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean ﬁeld approach to the intractable classiﬁcation model that is expected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 − 104 ) examples. The results indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.</p><p>6 0.61684042 <a title="109-lda-6" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>7 0.61618429 <a title="109-lda-7" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>8 0.6152482 <a title="109-lda-8" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>9 0.61492276 <a title="109-lda-9" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>10 0.61311758 <a title="109-lda-10" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>11 0.61256105 <a title="109-lda-11" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>12 0.612499 <a title="109-lda-12" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>13 0.61126447 <a title="109-lda-13" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>14 0.61064589 <a title="109-lda-14" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>15 0.61020958 <a title="109-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.61020559 <a title="109-lda-16" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>17 0.60803068 <a title="109-lda-17" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>18 0.60779744 <a title="109-lda-18" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>19 0.6074239 <a title="109-lda-19" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>20 0.60596466 <a title="109-lda-20" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
