<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 nips-2002-Unsupervised Color Constancy</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-202" href="#">nips2002-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 nips-2002-Unsupervised Color Constancy</h1>
<br/><p>Source: <a title="nips-2002-202-pdf" href="http://papers.nips.cc/paper/2323-unsupervised-color-constancy.pdf">pdf</a></p><p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>Reference: <a title="nips-2002-202-reference" href="../nips2002_reference/nips-2002-Unsupervised_Color_Constancy_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. [sent-6, score-1.209]
</p><p>2 We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. [sent-7, score-1.104]
</p><p>3 Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. [sent-8, score-0.767]
</p><p>4 This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. [sent-9, score-0.287]
</p><p>5 We show results on image matching and shadow removal and detection. [sent-10, score-0.444]
</p><p>6 Light sources, shadows, camera aperture, exposure time, transducer non-linearities, and camera processing (such as auto-gain-control and color balancing) can all affect the ﬁnal image of a scene. [sent-12, score-1.141]
</p><p>7 These effects have a signiﬁcant impact on the images obtained with cameras and hence on image processing algorithms, often hampering or eliminating our ability to produce reliable recognition algorithms. [sent-13, score-0.436]
</p><p>8 Addressing the variability of images due to these photic parameters has been an important problem in machine vision. [sent-14, score-0.252]
</p><p>9 We distinguish photic parameters from geometric parameters, such as camera orientation or blurring, that affect which parts of the scene a particular pixel represents. [sent-15, score-0.326]
</p><p>10 We also note that photic parameters are more general than “lighting parameters” and include anything which affects the ﬁnal RGB values in an image given that the geometric parameters and the objects in the scene have been ﬁxed. [sent-16, score-0.475]
</p><p>11 We present a statistical linear model of color change space that is learned by observing how the colors in static images change jointly under common, naturally occurring lighting changes. [sent-17, score-1.271]
</p><p>12 Such a model can be used for a number of tasks, including synthesis of images of new objects under different lighting conditions, image matching, and shadow detection. [sent-18, score-0.745]
</p><p>13 The model uses no prior knowledge of lighting conditions, surface reﬂectances, or other parameters during data collection and modeling. [sent-22, score-0.249]
</p><p>14 It also has no built-in knowledge of the physics of image acquisition or “typical” image color  changes, such as brightness changes. [sent-23, score-1.282]
</p><p>15 While it may not apply to all scenes equally well, it is a model of frequently occurring joint color changes, which is meant to apply to all scenes. [sent-25, score-0.744]
</p><p>16 Third, while our model is linear in color change space, each joint color change that we model (a 3-D vector ﬁeld) is completely arbitrary, and is not itself restricted to being linear. [sent-26, score-1.571]
</p><p>17 After discussing previous work in Section 2, we introduce the color ﬂow model and how it is obtained from observations in Section 3. [sent-28, score-0.727]
</p><p>18 In Section 4, we show how the model and a single observed image can be used to generate a large family of related images. [sent-29, score-0.274]
</p><p>19 In Section 5 we give preliminary results for image matching (object recognition) and shadow detection. [sent-31, score-0.444]
</p><p>20 2 Previous work The color constancy literature contains a large body of work on estimating surface reﬂectances and various photic parameters from images. [sent-32, score-0.951]
</p><p>21 Another technique is to estimate the relative illuminant or mapping of colors under an unknown illuminant to a canonical one. [sent-38, score-0.296]
</p><p>22 The intersection of the mappings for each pixel in an image is used to choose a “best” mapping. [sent-40, score-0.332]
</p><p>23 [7] trained a back-propagation multi-layer neural network to estimate the parameters of a linear color mapping. [sent-41, score-0.745]
</p><p>24 The approach in [8] works in the log color spectra space where the effect of a relative illuminant is a set of constant shifts in the scalar coefﬁcients of linear models for the image colors and illuminant. [sent-42, score-1.25]
</p><p>25 [9] bypasses the need to predict speciﬁc scene properties by proving that the set of images of a gray Lambertian convex object under all lighting conditions form a convex cone. [sent-44, score-0.499]
</p><p>26 1 We wanted a model which, based upon a single image (instead of three required by [9]), could make useful predictions about other images of the same scene. [sent-45, score-0.413]
</p><p>27 3 Color ﬂows In the following, let C = {(r, g, b)T ∈ R3 : 0 ≤ r ≤ 255, 0 ≤ g ≤ 255, 0 ≤ b ≤ 255} be the set of all possible observable image color 3-vectors. [sent-47, score-0.987]
</p><p>28 Let the vector-valued color of an image pixel p be denoted by c(p) ∈ C. [sent-48, score-1.038]
</p><p>29 Suppose we are given two P -pixel RGB color images I1 and I2 of the same scene taken under two different photic parameters θ1 and θ2 (the images are registered). [sent-49, score-1.182]
</p><p>30 The authors’ results on color images also do not address the issue of metamers, and assume that light is composed of only the wavelengths red, green, and blue. [sent-51, score-0.886]
</p><p>31 a  b  c  d  e  f  Figure 1: Matching non-linear color changes. [sent-52, score-0.727]
</p><p>32 corresponding image pixels pk and pk , 1 ≤ k ≤ P , in the two images represents a single1 2 color mapping c(pk ) → c(pk ) that is conveniently represented by the vector difference: 1 2 d(pk , pk ) = c(pk ) − c(pk ). [sent-56, score-1.496]
</p><p>33 1 2 2 1  (1)  By computing P vector differences (one for each pair of pixels) and placing each at the point c(pk ) in color space C, we have a partially observed color ﬂow: 1 Φ (c(pk )) = d(pk , pk ), 1 1 2  1≤k≤P  (2)  deﬁned at points in C for which there are colors in image I1 . [sent-57, score-1.984]
</p><p>34 a vector ﬁeld Φ deﬁned at all points in C) from a partially observed color ﬂow Φ , we must address two issues. [sent-60, score-0.741]
</p><p>35 Second, there may be multiple pixels of a particular color in image I1 that are mapped to different colors in image I2 . [sent-62, score-1.398]
</p><p>36 We use a radial basis function estimator which deﬁnes the ﬂow at a color point (r, g, b) T as the weighted proximity-based average of nearby observed “ﬂow vectors”. [sent-63, score-0.759]
</p><p>37 Note that color ﬂows are deﬁned so that a color point with only a single nearby neighbor will inherit a ﬂow vector that is nearly parallel to its neighbor. [sent-65, score-1.472]
</p><p>38 The idea is that if a particular color, under a photic parameter change θ1 → θ2 , is observed to get a little bit darker and a little bit bluer, for example, then its neighbors in color space are also deﬁned to exhibit this behavior. [sent-66, score-0.963]
</p><p>39 1 Structure in the space of color ﬂows Consider a ﬂat Lambertian surface that may have different reﬂectances as a function of the wavelength. [sent-68, score-0.793]
</p><p>40 While in principle it is possible for a change in lighting to map any color from such a surface to any other color independently of all other colors 2 , we know from experience that many such joint maps are unlikely. [sent-69, score-1.904]
</p><p>41 This suggests that while the marginal distribution of mappings for a particular color is broadly distributed, the space of possible joint color maps (i. [sent-70, score-1.511]
</p><p>42 In learning a statistical model of color ﬂows, many common color ﬂows can be anticipated such as ones that make colors a little darker, lighter, or more red. [sent-73, score-1.578]
</p><p>43 These types of ﬂows can be well modeled with a simple global 3x3 matrix A that maps a color c 1 in image I1 to a color c2 in image I2 via c2 = Ac1 . [sent-74, score-1.993]
</p><p>44 Such photic changes will tend 2 By carefully choosing properties such as the surface reﬂectance of a point as a function of wave˜ length and lighting any mapping Φ can, in principle, be observed even on a ﬂat Lambertian surface. [sent-77, score-0.408]
</p><p>45 The rightmost image is an ideal quotient image, corresponding to a Figure 3: Effects of the ﬁrst three eigenﬂows. [sent-82, score-0.313]
</p><p>46 to leave the bright and dim parts of the image alone, while spreading the central colors of color space toward the margins. [sent-85, score-1.111]
</p><p>47 For a linear imaging process, the ratio of the brightnesses of two images, or quotient image [12], should vary smoothly except at surface normal boundaries. [sent-86, score-0.471]
</p><p>48 However as shown in Figure 2, the quotient image is a function not only of surface normal, but also of albedo– direct evidence of a non-linear imaging process. [sent-87, score-0.396]
</p><p>49 Another pair of images exhibiting a nonlinear color ﬂow is shown in Figures 1a and b. [sent-88, score-0.864]
</p><p>50 Notice that the brighter areas of the original image get brighter and the darker portions get darker. [sent-89, score-0.356]
</p><p>51 2 Color eigenﬂows We wanted to capture the structure in color ﬂow space by observing real-world data in an unsupervised fashion. [sent-91, score-0.743]
</p><p>52 A one square meter color palette was printed on standard non-glossy plotter paper using every color that could be produced by a Hewlett Packard DesignJet 650C. [sent-92, score-1.483]
</p><p>53 Images of the poster were captured using the video camera under a wide variety of lighting conditions, including various intervals during sunrise, sunset, at midday, and with various combinations of ofﬁce lights and outdoor lighting (controlled by adjusting blinds). [sent-96, score-0.541]
</p><p>54 j j We chose image pairs I j = (I1 , I2 ), 1 ≤ j ≤ 800, by randomly and independently selecting individual images from the set of raw images. [sent-99, score-0.397]
</p><p>55 Each image pair was then used to estimate a full color ﬂow Φ(I j ). [sent-100, score-0.987]
</p><p>56 We call the principal components of the color ﬂow data “color eigenﬂows”, or just eigenﬂows, 4 for short. [sent-103, score-0.751]
</p><p>57 We emphasize that these principal components of color ﬂows have nothing to do with the distribution of colors in images, but only model the distribution of changes in color. [sent-104, score-0.905]
</p><p>58 Our work is very different from approaches that compute principal components in the intensity or color space itself [14, 15]. [sent-106, score-0.751]
</p><p>59 25  color flow linear diagonal gray world  rms error  20  15  10  5  a  0  1  2  3  image  4  b  Figure 4: Image matching. [sent-108, score-1.139]
</p><p>60 Bottom row: best approximation to original images using eigenﬂows and the source image a. [sent-110, score-0.426]
</p><p>61 4 Using color ﬂows to synthesize novel images How do we generate a new image from a source image and a color ﬂow Φ? [sent-113, score-2.14]
</p><p>62 For each pixel p in the new image, its color c (p) can be computed as c c (p) = c(p) + αΦ(ˆ(p)),  (4)  where c(p) is color in the source image and α is a scalar multiplier that represents the “quantity of ﬂow”. [sent-114, score-1.794]
</p><p>63 ˆ(p) is interpreted to be the color vector closest to c(p) (in color space) c at which Φ has been computed. [sent-115, score-1.454]
</p><p>64 Figure 3 shows the effect of the ﬁrst three eigenﬂows on an image of a face. [sent-117, score-0.26]
</p><p>65 The original image is in the middle of each row while the other images show the application of each eigenﬂow with α values between ±4 standard deviations. [sent-118, score-0.413]
</p><p>66 We stress that the eigenﬂows were only computed once (on the color palette data), and that they were applied to the face image without any knowledge of the parameters under which the face image was taken. [sent-125, score-1.344]
</p><p>67 1 Flowing one image to another Suppose we have two images and we pose the question of whether they are images of the same object or scene. [sent-127, score-0.576]
</p><p>68 We suggest that if we can “ﬂow” one image to another then the images are likely to be of the same scene. [sent-128, score-0.397]
</p><p>69 Let us treat an image I as a function that takes a color ﬂow and returns a difference image D by placing at each (x,y) pixel in D the color change vector Φ(c(p x,y )). [sent-129, score-2.111]
</p><p>70 The difference image basis for I and set of eigenﬂows Ψi , 1 ≤ i ≤ E, is Di = I(Ψi ). [sent-130, score-0.288]
</p><p>71 The set of images S that can be formed using a source image and a set of eigenﬂows is S = {S : S = E I + i=1 γi Di }, where the γi ’s are scalars, and here I is just an image, and not a function. [sent-131, score-0.426]
</p><p>72 We can only ﬂow image I1 to another image I2 if it is possible to represent the difference image as a linear combination of the Di ’s, i. [sent-133, score-0.826]
</p><p>73 We ﬁnd the optimal (in the least-squares sense) γi ’s by solving the system E  D=  γ i Di , i=1  (5)  a  b  e  c  d  f  Figure 5: Modeling lighting changes with color ﬂows. [sent-136, score-0.94]
</p><p>74 While clipping can only reduce the error between a synthetic image and a target image, it may change which solution is optimal in some cases. [sent-153, score-0.384]
</p><p>75 1 Image matching One use of the color change model is for image matching. [sent-155, score-1.086]
</p><p>76 We ﬁrst examined our ability to ﬂow a source image to a matching target image under different photic parameters. [sent-157, score-0.74]
</p><p>77 The linear method ﬁnds the matrix A in Equation 3 that minimizes the L2 error between the synthetic and target images; diagonal does the same with a diagonal A; gray world linearly matches the mean R, G, B values of the synthetic and target images. [sent-159, score-0.204]
</p><p>78 In a second experiment (Figure 4), we matched images of a face taken under various camera parameters but with constant lighting. [sent-161, score-0.248]
</p><p>79 2 Local ﬂows In another test, the source and target images were taken under very different lighting conditions. [sent-164, score-0.367]
</p><p>80 Furthermore, shadowing effects and lighting direction changed between the two images. [sent-165, score-0.208]
</p><p>81 a  b  c  d  Figure 6: Backgrounding with color ﬂows. [sent-174, score-0.727]
</p><p>82 c For each of the two regions (from background subtraction), a “ﬂow” was done between the original image and the new image based on the pixels in each region. [sent-177, score-0.561]
</p><p>83 d The color ﬂow of the original image using the eigenﬂow coefﬁcients recovered from the shadow region. [sent-178, score-1.113]
</p><p>84 The color ﬂow using the coefﬁcients from the non-shadow region are unable to give a reasonable reconstruction of the new image. [sent-179, score-0.754]
</p><p>85 We performed one experiment to measure the over-ﬁtting of our method versus the others by trying to ﬂow an original image to its reﬂection (Figure 5). [sent-181, score-0.26]
</p><p>86 Note that while our method had lower error (which is undesirable), there was still a signiﬁcant spread between matching images and non-matching images. [sent-187, score-0.21]
</p><p>87 We believe we can improve differentiation between matching and non-matching image pairs by assigning a cost to the change in γ i across each image patch. [sent-188, score-0.619]
</p><p>88 For matching images, sharp changes would only be necessary at shadow boundaries or changes in the surface orientation relative to directional light sources. [sent-190, score-0.332]
</p><p>89 Shadows can also move across an image and appear as moving objects. [sent-194, score-0.26]
</p><p>90 Many of these problems could be eliminated if we could recognize that a particular region of an image is equivalent to a previously seen version of the scene, but under a different lighting. [sent-195, score-0.287]
</p><p>91 Figure 6a shows how color ﬂows may be used to distinguish between a new object and a shadow by ﬂowing both regions. [sent-196, score-0.895]
</p><p>92 A constant color ﬂow across an entire region may not model the image change well. [sent-197, score-1.055]
</p><p>93 That is, we can ﬁnd the best least squares ﬁt of the difference image allowing our γ estimates to vary linearly or quadratically over the image. [sent-199, score-0.379]
</p><p>94 We implemented this technique by computing ﬂows γx,y between corresponding image patches (indexed by x and y), and then minimizing the following form: (γx,y − M cx,y )T Σ−1 (γx,y − M cx,y ). [sent-200, score-0.279]
</p><p>95 Allowing the γ’s to vary over the image greatly increases the capacity of a matcher, but by limiting this variation to linear or quadratic variation, the capacity is still not able to qualitatively match “non-matching” images. [sent-204, score-0.406]
</p><p>96 Note that this smooth variation in eigenﬂow coefﬁcients can model either a nearby light source or a smoothly curving surface, since either of these conditions will result in a smoothly varying lighting change. [sent-205, score-0.38]
</p><p>97 8  Table 1: Error residuals for shadow and non-shadow regions after color ﬂows. [sent-212, score-0.853]
</p><p>98 Much larger experiments need to be performed to establish the utility of the color change model for particular applications. [sent-217, score-0.768]
</p><p>99 However, since the color change model represents a compact description of lighting changes, including nonlinearities, we are optimistic about these applications. [sent-218, score-0.951]
</p><p>100 Color correction of face images under different illuminants by rgb eigenfaces. [sent-336, score-0.28]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('color', 0.727), ('ows', 0.298), ('image', 0.26), ('ow', 0.228), ('lighting', 0.183), ('eigen', 0.171), ('images', 0.137), ('shadow', 0.126), ('colors', 0.124), ('photic', 0.115), ('pk', 0.115), ('illuminant', 0.086), ('rgb', 0.08), ('camera', 0.077), ('shadows', 0.068), ('surface', 0.066), ('scene', 0.066), ('matching', 0.058), ('ectance', 0.053), ('quotient', 0.053), ('pixel', 0.051), ('poster', 0.05), ('constancy', 0.043), ('ectances', 0.043), ('object', 0.042), ('change', 0.041), ('darker', 0.038), ('lambertian', 0.038), ('brightness', 0.035), ('quadratically', 0.034), ('coef', 0.034), ('face', 0.034), ('capacity', 0.032), ('smoothly', 0.031), ('cients', 0.03), ('changes', 0.03), ('flow', 0.03), ('varying', 0.029), ('aperture', 0.029), ('backgrounding', 0.029), ('brainard', 0.029), ('brighter', 0.029), ('clipping', 0.029), ('illuminants', 0.029), ('palette', 0.029), ('source', 0.029), ('rms', 0.029), ('gray', 0.028), ('difference', 0.028), ('pixels', 0.027), ('surfaces', 0.027), ('region', 0.027), ('vary', 0.026), ('ce', 0.026), ('re', 0.025), ('tieu', 0.025), ('wall', 0.025), ('lights', 0.025), ('effects', 0.025), ('principal', 0.024), ('video', 0.023), ('ijcv', 0.023), ('synthesis', 0.022), ('light', 0.022), ('mappings', 0.021), ('synthetic', 0.021), ('variation', 0.02), ('patches', 0.019), ('maps', 0.019), ('spectra', 0.019), ('nearby', 0.018), ('di', 0.018), ('quadratic', 0.018), ('linear', 0.018), ('target', 0.018), ('illumination', 0.018), ('objects', 0.017), ('geometric', 0.017), ('joint', 0.017), ('linearly', 0.017), ('imaging', 0.017), ('placing', 0.017), ('conditions', 0.017), ('elds', 0.016), ('miller', 0.016), ('wanted', 0.016), ('diagonal', 0.016), ('spectral', 0.016), ('shifts', 0.016), ('row', 0.016), ('world', 0.016), ('error', 0.015), ('allowing', 0.014), ('background', 0.014), ('bit', 0.014), ('berkeley', 0.014), ('top', 0.014), ('recognition', 0.014), ('observed', 0.014), ('convex', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="202-tfidf-1" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>2 0.42350176 <a title="202-tfidf-2" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>Author: B. Caputo, Gy. Dorkó</p><p>Abstract: This paper presents a kernel method that allows to combine color and shape information for appearance-based object recognition. It doesn't require to define a new common representation, but use the power of kernels to combine different representations together in an effective manner. These results are achieved using results of statistical mechanics of spin glasses combined with Markov random fields via kernel functions. Experiments show an increase in recognition rate up to 5.92% with respect to conventional strategies. 1</p><p>3 0.31979755 <a title="202-tfidf-3" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>4 0.28377172 <a title="202-tfidf-4" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>Author: Javier R. Movellan, Thomas Wachtler, Thomas D. Albright, Terrence Sejnowski</p><p>Abstract: We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive ﬁeld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations. In this paper we introduce the notion of Morton-style factorial coding and illustrate how it may help analyze information integration and perceptual organization in the brain. In the neurosciences factorial codes are often studied in the context of mean tuning curves. A tuning curve is called separable if it can be expressed as the product of terms selectively inﬂuenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. Morton (1969) analyzed a wide variety of psychophysical experiments on word perception and showed that they could be explained using a model in which stimulus and context have separable effects on perception. More precisely, in Mortons’ model the joint effect of stimulus and context on a perceptual representation can be obtained by multiplying terms selectively controlled by stimulus and by context, i.e.,  £ © # #</p><p>5 0.16524257 <a title="202-tfidf-5" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>Author: Michael E. Tipping, Christopher M. Bishop</p><p>Abstract: The extraction of a single high-quality image from a set of lowresolution images is an important problem which arises in fields such as remote sensing, surveillance, medical imaging and the extraction of still images from video. Typical approaches are based on the use of cross-correlation to register the images followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution images, using regularization to resolve the ill-posed nature of the inversion process. In this paper we develop a Bayesian treatment of the super-resolution problem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown high-resolution image. This approach allows us to estimate the unknown point spread function, and is rendered tractable through the introduction of a Gaussian process prior over images. Results indicate a significant improvement over techniques based on MAP (maximum a-posteriori) point optimization of the high resolution image and associated registration parameters. 1</p><p>6 0.16504554 <a title="202-tfidf-6" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>7 0.16368429 <a title="202-tfidf-7" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>8 0.12337255 <a title="202-tfidf-8" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>9 0.10426193 <a title="202-tfidf-9" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>10 0.098810531 <a title="202-tfidf-10" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>11 0.093328863 <a title="202-tfidf-11" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>12 0.086542889 <a title="202-tfidf-12" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>13 0.08637739 <a title="202-tfidf-13" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>14 0.085278019 <a title="202-tfidf-14" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>15 0.081977762 <a title="202-tfidf-15" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>16 0.079324916 <a title="202-tfidf-16" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>17 0.071821705 <a title="202-tfidf-17" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>18 0.060186405 <a title="202-tfidf-18" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>19 0.057449397 <a title="202-tfidf-19" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>20 0.055335723 <a title="202-tfidf-20" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.157), (1, 0.042), (2, 0.011), (3, 0.367), (4, 0.0), (5, -0.146), (6, 0.314), (7, -0.044), (8, -0.088), (9, -0.042), (10, -0.074), (11, -0.044), (12, -0.114), (13, -0.184), (14, -0.079), (15, -0.226), (16, -0.337), (17, -0.058), (18, -0.009), (19, -0.164), (20, 0.205), (21, -0.015), (22, 0.009), (23, -0.0), (24, -0.039), (25, -0.074), (26, -0.06), (27, 0.1), (28, 0.047), (29, -0.073), (30, -0.024), (31, 0.029), (32, 0.161), (33, 0.005), (34, 0.014), (35, 0.011), (36, 0.025), (37, -0.001), (38, -0.036), (39, -0.005), (40, -0.017), (41, -0.0), (42, 0.035), (43, -0.092), (44, -0.002), (45, 0.014), (46, -0.005), (47, -0.001), (48, -0.015), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9563328 <a title="202-lsi-1" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>2 0.79710931 <a title="202-lsi-2" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>Author: B. Caputo, Gy. Dorkó</p><p>Abstract: This paper presents a kernel method that allows to combine color and shape information for appearance-based object recognition. It doesn't require to define a new common representation, but use the power of kernels to combine different representations together in an effective manner. These results are achieved using results of statistical mechanics of spin glasses combined with Markov random fields via kernel functions. Experiments show an increase in recognition rate up to 5.92% with respect to conventional strategies. 1</p><p>3 0.69821721 <a title="202-lsi-3" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>4 0.56704861 <a title="202-lsi-4" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><p>5 0.50519156 <a title="202-lsi-5" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>Author: Javier R. Movellan, Thomas Wachtler, Thomas D. Albright, Terrence Sejnowski</p><p>Abstract: We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive ﬁeld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations. In this paper we introduce the notion of Morton-style factorial coding and illustrate how it may help analyze information integration and perceptual organization in the brain. In the neurosciences factorial codes are often studied in the context of mean tuning curves. A tuning curve is called separable if it can be expressed as the product of terms selectively inﬂuenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. Morton (1969) analyzed a wide variety of psychophysical experiments on word perception and showed that they could be explained using a model in which stimulus and context have separable effects on perception. More precisely, in Mortons’ model the joint effect of stimulus and context on a perceptual representation can be obtained by multiplying terms selectively controlled by stimulus and by context, i.e.,  £ © # #</p><p>6 0.36374182 <a title="202-lsi-6" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>7 0.35687786 <a title="202-lsi-7" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>8 0.30686963 <a title="202-lsi-8" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>9 0.29685974 <a title="202-lsi-9" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>10 0.28333467 <a title="202-lsi-10" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>11 0.28261542 <a title="202-lsi-11" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>12 0.27698693 <a title="202-lsi-12" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>13 0.27402931 <a title="202-lsi-13" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>14 0.24661089 <a title="202-lsi-14" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>15 0.23514089 <a title="202-lsi-15" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>16 0.23093624 <a title="202-lsi-16" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>17 0.22826606 <a title="202-lsi-17" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>18 0.21010014 <a title="202-lsi-18" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>19 0.19406715 <a title="202-lsi-19" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>20 0.18845728 <a title="202-lsi-20" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.011), (5, 0.326), (11, 0.018), (23, 0.02), (42, 0.067), (54, 0.117), (55, 0.028), (67, 0.027), (68, 0.042), (74, 0.115), (92, 0.024), (98, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80839723 <a title="202-lda-1" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>2 0.75883055 <a title="202-lda-2" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>3 0.52522194 <a title="202-lda-3" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>4 0.50935698 <a title="202-lda-4" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>5 0.50623953 <a title="202-lda-5" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present a class of algorithms for learning the structure of graphical models from data. The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtained from Mercer kernels. Thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type. We explore the computational properties of our approach, showing how to use the kernel trick to compute the relevant statistics in linear time. We illustrate our framework with experiments involving discrete and continuous data.</p><p>6 0.50502539 <a title="202-lda-6" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>7 0.50453413 <a title="202-lda-7" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>8 0.50406414 <a title="202-lda-8" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>9 0.50350738 <a title="202-lda-9" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>10 0.50252998 <a title="202-lda-10" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>11 0.50215757 <a title="202-lda-11" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>12 0.50132668 <a title="202-lda-12" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>13 0.50098979 <a title="202-lda-13" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>14 0.49925879 <a title="202-lda-14" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>15 0.49699926 <a title="202-lda-15" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>16 0.49683943 <a title="202-lda-16" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>17 0.4963842 <a title="202-lda-17" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>18 0.49598414 <a title="202-lda-18" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>19 0.49567634 <a title="202-lda-19" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>20 0.49440548 <a title="202-lda-20" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
