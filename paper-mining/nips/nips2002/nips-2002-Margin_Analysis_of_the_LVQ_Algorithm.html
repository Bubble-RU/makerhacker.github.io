<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2002-Margin Analysis of the LVQ Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-140" href="#">nips2002-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2002-Margin Analysis of the LVQ Algorithm</h1>
<br/><p>Source: <a title="nips-2002-140-pdf" href="http://papers.nips.cc/paper/2261-margin-analysis-of-the-lvq-algorithm.pdf">pdf</a></p><p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>Reference: <a title="nips-2002-140-reference" href="../nips2002_reference/nips-2002-Margin_Analysis_of_the_LVQ_Algorithm_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prototyp', 0.55), ('lvq', 0.545), ('xt', 0.247), ('loss', 0.244), ('xl', 0.214), ('margin', 0.213), ('hing', 0.152), ('closest', 0.127), ('sl', 0.122), ('dl', 0.097), ('hypothes', 0.079), ('label', 0.067), ('pak', 0.065), ('brok', 0.064), ('wl', 0.061), ('neighbo', 0.058), ('rul', 0.058), ('yt', 0.058), ('bound', 0.056), ('nearest', 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="140-tfidf-1" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>2 0.17987418 <a title="140-tfidf-2" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>3 0.1551594 <a title="140-tfidf-3" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>4 0.15349568 <a title="140-tfidf-4" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>5 0.1313404 <a title="140-tfidf-5" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>Author: Yasemin Altun, Thomas Hofmann, Mark Johnson</p><p>Abstract: This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields. Applications areas for the presented technique range from natural language processing and information extraction to computational biology. We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 1</p><p>6 0.10747681 <a title="140-tfidf-6" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>7 0.098100223 <a title="140-tfidf-7" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>8 0.097915925 <a title="140-tfidf-8" href="./nips-2002-Field-Programmable_Learning_Arrays.html">91 nips-2002-Field-Programmable Learning Arrays</a></p>
<p>9 0.090448841 <a title="140-tfidf-9" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>10 0.087779336 <a title="140-tfidf-10" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>11 0.086337805 <a title="140-tfidf-11" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>12 0.083585858 <a title="140-tfidf-12" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>13 0.083065487 <a title="140-tfidf-13" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>14 0.083056822 <a title="140-tfidf-14" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>15 0.080577597 <a title="140-tfidf-15" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>16 0.075575069 <a title="140-tfidf-16" href="./nips-2002-A_Hierarchical_Bayesian_Markovian_Model_for_Motifs_in_Biopolymer_Sequences.html">7 nips-2002-A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences</a></p>
<p>17 0.074832305 <a title="140-tfidf-17" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>18 0.072937846 <a title="140-tfidf-18" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>19 0.072593622 <a title="140-tfidf-19" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>20 0.070156358 <a title="140-tfidf-20" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, 0.103), (2, -0.009), (3, 0.027), (4, 0.048), (5, -0.116), (6, -0.147), (7, 0.007), (8, -0.055), (9, -0.102), (10, 0.003), (11, 0.026), (12, 0.004), (13, 0.046), (14, 0.023), (15, 0.021), (16, 0.005), (17, -0.025), (18, -0.035), (19, 0.132), (20, 0.075), (21, 0.225), (22, -0.062), (23, -0.172), (24, -0.089), (25, 0.134), (26, 0.002), (27, 0.114), (28, 0.043), (29, 0.103), (30, 0.172), (31, -0.061), (32, 0.08), (33, -0.038), (34, -0.168), (35, -0.032), (36, -0.009), (37, 0.073), (38, 0.001), (39, 0.152), (40, -0.037), (41, 0.052), (42, 0.114), (43, -0.02), (44, -0.044), (45, 0.047), (46, 0.095), (47, 0.075), (48, -0.037), (49, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90342325 <a title="140-lsi-1" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>2 0.63669002 <a title="140-lsi-2" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>3 0.59181094 <a title="140-lsi-3" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>4 0.57044804 <a title="140-lsi-4" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>5 0.43573669 <a title="140-lsi-5" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>Author: Yasemin Altun, Thomas Hofmann, Mark Johnson</p><p>Abstract: This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields. Applications areas for the presented technique range from natural language processing and information extraction to computational biology. We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 1</p><p>6 0.4015255 <a title="140-lsi-6" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>7 0.39569581 <a title="140-lsi-7" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>8 0.38869017 <a title="140-lsi-8" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>9 0.36838618 <a title="140-lsi-9" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>10 0.36702427 <a title="140-lsi-10" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>11 0.35899261 <a title="140-lsi-11" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>12 0.3450616 <a title="140-lsi-12" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>13 0.33958083 <a title="140-lsi-13" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>14 0.33604452 <a title="140-lsi-14" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>15 0.33243826 <a title="140-lsi-15" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>16 0.32799757 <a title="140-lsi-16" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>17 0.32170752 <a title="140-lsi-17" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>18 0.3138974 <a title="140-lsi-18" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>19 0.30156446 <a title="140-lsi-19" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>20 0.29076543 <a title="140-lsi-20" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.019), (22, 0.052), (26, 0.026), (39, 0.141), (47, 0.129), (48, 0.076), (54, 0.031), (57, 0.222), (66, 0.057), (72, 0.053), (93, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93359035 <a title="140-lda-1" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>Author: Robert B. Gramacy, Manfred K. Warmuth, Scott A. Brandt, Ismail Ari</p><p>Abstract: We are constructing caching policies that have 13-20% lower miss rates than the best of twelve baseline policies over a large variety of request streams. This represents an improvement of 49–63% over Least Recently Used, the most commonly implemented policy. We achieve this not by designing a speciﬁc new policy but by using on-line Machine Learning algorithms to dynamically shift between the standard policies based on their observed miss rates. A thorough experimental evaluation of our techniques is given, as well as a discussion of what makes caching an interesting on-line learning problem.</p><p>same-paper 2 0.81151748 <a title="140-lda-2" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>3 0.75458777 <a title="140-lda-3" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>4 0.70225632 <a title="140-lda-4" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: We describe a new algorithmic framework for learning multiclass categorization problems. In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space. In this space each instance is assigned the label it is nearest to. We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data. A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC). Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching. We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems.</p><p>5 0.69781578 <a title="140-lda-5" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>Author: Wim Wiegerinck, Tom Heskes</p><p>Abstract: We consider loopy belief propagation for approximate inference in probabilistic graphical models. A limitation of the standard algorithm is that clique marginals are computed as if there were no loops in the graph. To overcome this limitation, we introduce fractional belief propagation. Fractional belief propagation is formulated in terms of a family of approximate free energies, which includes the Bethe free energy and the naive mean-ﬁeld free as special cases. Using the linear response correction of the clique marginals, the scale parameters can be tuned. Simulation results illustrate the potential merits of the approach.</p><p>6 0.68630755 <a title="140-lda-6" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>7 0.68484837 <a title="140-lda-7" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>8 0.68409216 <a title="140-lda-8" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>9 0.67954063 <a title="140-lda-9" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>10 0.67872381 <a title="140-lda-10" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>11 0.67544925 <a title="140-lda-11" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>12 0.67528629 <a title="140-lda-12" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>13 0.67343593 <a title="140-lda-13" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>14 0.67280614 <a title="140-lda-14" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>15 0.67274368 <a title="140-lda-15" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>16 0.6697709 <a title="140-lda-16" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>17 0.66691434 <a title="140-lda-17" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>18 0.66444224 <a title="140-lda-18" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>19 0.66364557 <a title="140-lda-19" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>20 0.66205746 <a title="140-lda-20" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
