<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-102" href="#">nips2002-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</h1>
<br/><p>Source: <a title="nips-2002-102-pdf" href="http://papers.nips.cc/paper/2254-hidden-markov-model-of-cortical-synaptic-plasticity-derivation-of-the-learning-rule.pdf">pdf</a></p><p>Author: Michael Eisele, Kenneth D. Miller</p><p>Abstract: Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synaptic plasticity does not associate individual spikes, but rather whole ﬁring episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Standard methods from hidden Markov models are used to deﬁne what “ﬁring episodes” are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002).</p><p>Reference: <a title="nips-2002-102-reference" href="../nips2002_reference/nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. [sent-10, score-2.364]
</p><p>2 We study the hypothesis that cortical synaptic plasticity does not associate individual spikes, but rather whole ﬁring episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. [sent-11, score-0.99]
</p><p>3 Standard methods from hidden Markov models are used to deﬁne what “ﬁring episodes” are. [sent-13, score-0.068]
</p><p>4 Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. [sent-14, score-0.288]
</p><p>5 We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. [sent-15, score-1.139]
</p><p>6 We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002). [sent-16, score-0.699]
</p><p>7 1 Introduction Cortical synaptic plasticity agrees with the Hebbian learning principle: Neurons that ﬁre together, wire together. [sent-17, score-0.466]
</p><p>8 But many features of cortical plasticity go beyond this simple principle, such as the dependence on spike-timing or the nonlinear dependence on spike frequency (see [1] or [2] for review). [sent-18, score-0.642]
</p><p>9 Studying these features may produce a better understanding of which neurons wire together in the neocortex. [sent-19, score-0.097]
</p><p>10 Previous models of cortical synaptic plasticity [3]-[5] differed in their details, but they agreed that nonlinear learning rules are needed to model cortical plasticity. [sent-20, score-0.64]
</p><p>11 In linear learning rules, the weight change induced by a presynatic spike would depend only on the postsynaptic spikes, but not on all the other presynaptic spikes. [sent-21, score-1.183]
</p><p>12 In the cortex, by contrast, the contribution from a presynaptic spike is stronger when it occurs alone than when it occurs right after another presynaptic spike [5]. [sent-22, score-0.869]
</p><p>13 Consequently, the weight change depends in a complex way on the whole temporal pattern of pre- and postsynaptic spikes. [sent-24, score-0.786]
</p><p>14 Even though this nonlinear dependence can be modeled phenomenologically [3]-[5], its biological function remains unknown. [sent-25, score-0.065]
</p><p>15 They produce long-term potentiation (LTP) when the presynaptic spike (pre) precedes the postsynaptic spike (post), and long-term depression (LTD) if the order is reversed. [sent-27, score-1.212]
</p><p>16 When several pre- and postsynaptic spikes are interleaved in time, the outcome depends in a complicated way on the whole spike pattern (LTP or LTD). [sent-28, score-1.223]
</p><p>17 B: In our model, pre- and postsynaptic spikes are paired only indirectly. [sent-29, score-0.879]
</p><p>18 Each spike train is used to estimate when ﬁring episodes start and end. [sent-30, score-0.526]
</p><p>19 C: These ﬁring episodes are then associated, with LTP being induced if the presynaptic ﬁring episode starts before the postsynaptic one and LTD if the order is reversed and if the episodes are short. [sent-31, score-1.272]
</p><p>20 D: Hidden Markov model used to estimate when ﬁring episodes occur. [sent-32, score-0.249]
</p><p>21 function may be easier to understand in future studies. [sent-33, score-0.083]
</p><p>22 2 Basic learning principle The basic principle behind our model is illustrated in ﬁg. [sent-34, score-0.076]
</p><p>23 We propose that the learning rule does not associate pre- and postsynaptic spikes directly, but rather uses them to estimate whether the pre- or postsynaptic neuron is currently in a period of rapid ﬁring (’ﬁring episode’) or a period of little or no ﬁring. [sent-36, score-1.669]
</p><p>24 When the per- and postsynaptic ﬁring episodes overlap, the synapse is strengthened or weakened depending on which one started ﬁrst, but independent of the precise temporal patterns of spikes within a ﬁring episode. [sent-38, score-1.152]
</p><p>25 As a consequence, the contribution of each spike to synaptic plasticity will depend on whether it occurs alone, or surrounded by other spikes, and the learning rule will be nonlinear. [sent-39, score-0.89]
</p><p>26 For the right parameter choice, the nonlinear features of this rule will agree well with nonlinear features of cortical synaptic plasticity. [sent-40, score-0.57]
</p><p>27 Implementation of this rule will be done in two steps. [sent-41, score-0.168]
</p><p>28 Secondly, we will associate the pre- and postsynaptic ﬁring episodes. [sent-43, score-0.564]
</p><p>29 The ﬁrst step uses standard methods from hidden Markov models (see e. [sent-44, score-0.068]
</p><p>30 The pre- and postsynaptic neuron will each be described by a Markov model with three states (ﬁg. [sent-47, score-0.601]
</p><p>31 1D), which correspond to ﬁring episodes (state 2; ﬁring probability ), to the silence between responses (state 0; ﬁring probability ), and to the ﬁrst spike of a new ﬁring episode (state 1; ﬁring probability ; duration = 1 time step). [sent-48, score-0.614]
</p><p>32 As usual, the parameters of the Markov model are the transition probabilities , which determine how long ﬁring episodes and silent periods are expected to last, and the emission rates , which determine the ﬁring rates. [sent-49, score-0.347]
</p><p>33 is the binary observable at time step ( at spikes and otherwise), is the ﬁring probability per time step in state , and . [sent-50, score-0.499]
</p><p>34 In general, the pre- and postsynaptic neuron will have different parameters and . [sent-51, score-0.563]
</p><p>35 C $"    %' ¦£  ¤  ¨¦£    ¥0 © § ¥0 %    ¥0 % © § ¥0 ' ¦£  & ¨¦£  for for otherwise  (1)   ¦     ¥0  ¨¥0 ¡ ¦£ ¦¤££ ¢  ©§  where and are the amplitudes of synaptic potentiation and depression. [sent-56, score-0.222]
</p><p>36 In general, the states are not known with certainty, only their probabilities are, and the actual weight change is therefore deﬁned as:  ( )! [sent-57, score-0.253]
</p><p>37 TRSRR  W(  ¡ (  ( V ¦ CRRR 1( UTSQ£ @ & H P    H    ¥0 £ F @ 8 © § ( C 6 © § ¥0 £ £ @ 8 6 ¡ 3 Q¥ ( I9  ¦¨GE¦ ¨¥ 1D' B¨¦AE¨9¦ 9  7£ 5  4 12  (2)  where the sum is over all possible pre- and postsynaptic states and is the probability given the whole spike sequence . [sent-59, score-0.883]
</p><p>38 2 shows, this straightforward learning rule produces weight changes that are similar to those seen in cortex [5]. [sent-61, score-0.388]
</p><p>39 (One can show that this particular Markov model depends on the parameters and only through the two and where is the combinations time step. [sent-62, score-0.071]
</p><p>40 To ﬁt the data on spike pairs and triplets [5], we set 15ms, 34ms, 20ms, 70ms, 96Hz , and . [sent-63, score-0.449]
</p><p>41 ¡  £    ¥ f baY c4` X    ©§ ¨¥ f  This learning rule is, however, not biologically plausible, because it violates causality. [sent-66, score-0.21]
</p><p>42 The estimates of state probabilities depend not only on past, but also on future observables, while real synaptic plasticity can depend only on past spikes. [sent-67, score-0.76]
</p><p>43 To solve this causality problem, we will rewrite the learning rule, essentially deriving a new algorithm in place of the familiar hidden Markov algorithms. [sent-68, score-0.114]
</p><p>44 We will derive this causal learning rule not only for this speciﬁc 3-state model, but for general Markov models. [sent-69, score-0.246]
</p><p>45 Ideally, we would like to set the weight at time equal to the expectation value of , given the spike trains and . [sent-73, score-0.529]
</p><p>46 But only part of these spike trains are known at time . [sent-74, score-0.327]
</p><p>47 Of the sequence the synapse has already seen the past values , . [sent-75, score-0.166]
</p><p>48 model  2/1 triplets; hidden Markov model  2/1 triplets; linear rule  0. [sent-80, score-0.236]
</p><p>49 5  examples of 2/1 triplets  25  5  0 −5 −25 t2 (ms)  25  5  0 −5 −25 t2 (ms)  −25 0 −5 25 5 t1 (ms)  5  0 −5 −25 t2 (ms)  −25 0 −5 25 5 t1 (ms)  −25 0 −5 25 5 t1 (ms)  5  t2 (ms)  25  25 0 −5 −25 25 5 0 −5 −25 t1 (ms)  1/2 triplets; phen. [sent-86, score-0.194]
</p><p>50 model  1/2 triplets; hidden Markov model  1/2 triplets; linear rule  0. [sent-87, score-0.236]
</p><p>51 5  dw  1  dw  1  dw  1  0  examples of 1/2 triplets  0  0 −25  −0. [sent-90, score-0.512]
</p><p>52 5  0  5 −5 0 −25 t1 (ms)  5 25 t2 (ms)  0 5 25  25  −25 −5 0 5 25 t1 (ms)  Figure 2: Weight change produced by spike triplets in various models. [sent-93, score-0.492]
</p><p>53 It certainly agrees better than a purely linear rule (third column). [sent-95, score-0.229]
</p><p>54 Parameters were set so that all three models produce the same results for spike pairs (1 presynaptic and 1 postsynaptic spike). [sent-96, score-0.922]
</p><p>55 Upper row: Weight change produced by 2 presynaptic and 1 postsynaptic spikes (2/1 triplet). [sent-97, score-1.064]
</p><p>56 Lower row: 1 and are the times between prepresynaptic and 2 postsynaptic spikes (1/2 triplet). [sent-98, score-0.879]
</p><p>57 The small boxes on the right show examples of spike patterns for positive and negative and  % Qa  a  ¤ F  a  a  ¡ ¥  ¡# ©§¥ B¨v(  # # ©§¥ ©§ ¨w ¨0¥  ¡ 0 (  (  it has not yet seen the future sequence , , . [sent-100, score-0.358]
</p><p>58 All one can do is to make some assumption about what the future spikes will be, set accordingly, and correct in the future, when the real spike sequence becomes known. [sent-104, score-0.712]
</p><p>59 The condition that all future spikes are 0 is written as and . [sent-106, score-0.437]
</p><p>60 One could make other assumptions about the future spikes, but all these assumptions would affect only when the weight changes, but not how much it changes in the long run. [sent-107, score-0.283]
</p><p>61 As grows, most weight changes will lie in the distant past and depend only weakly on our assumptions about future spikes. [sent-109, score-0.454]
</p><p>62 4  4  Next we will show how to compute the expectation value in eq. [sent-110, score-0.038]
</p><p>63 (5) without having to store the past spike trains . [sent-111, score-0.445]
</p><p>64 To simplify the notation, we will regard each pair of pre- and  (  (  P ¦£  B¨¦ F£    ¥r © § ¥r  r£  postsynaptic states as a state of a combined pre- and postsynaptic Markov model. [sent-112, score-1.156]
</p><p>65 We will also combine the pre- and postsynaptic spikes , each of which can take the two values 0 or 1, to a single observable , which can take 4 values. [sent-113, score-0.902]
</p><p>66 The desired weight is then equal to:  P rw(  ¨w@F    ¥ © § ¥r ( 0  t ¡ 4r ¤££ ¢  ¦  r  scr 1 d  ¡  (  q p£ 0 ¡   ¥  with  (7)     ( 0 (  (      ¡¡  C q p£ 0 ¡    ( 1  #  ¡   0  3. [sent-114, score-0.209]
</p><p>67 2 Running estimate of state probabilities , it is helpful to ﬁrst compute the probabilities  (   ( %( 0  ¦      C  £  @  ( 1 9  #   % £ 0  ¡ 0  ¢   ¦4£&  To compute  (8)  of the states given the past and present spikes and assuming that there are no future spikes. [sent-115, score-0.751]
</p><p>68 The can be computed recursively, in terms of (this is similar to the familiar forward algorithm for hidden Markov models). [sent-116, score-0.094]
</p><p>69 As long as the end of the Markov chain is far enough in the future, this equation reduces to an eigenvalue problem with the solution , where is the largest eigenvalue of the matrix with elements and is the corresponding eigenvector. [sent-120, score-0.078]
</p><p>70 As the matrix elements are positive, will be real, and the eigenvector will be unique up to a constant factor (except for quite exceptional, disconnected Markov chains, in which it may depend on the choice of end state). [sent-121, score-0.077]
</p><p>71 If there is no pre- or postsynaptic spike at time ( ), the normalization factor is equal to 1, and no longer depends on or . [sent-125, score-0.886]
</p><p>72 (16) is a linear equation with constant coefﬁcients, which can be integrated analytically from one spike to the next, thereby speeding up the numerical simulation. [sent-127, score-0.255]
</p><p>73 At pre- or postsynaptic spikes ( ), can be computed by summing eq. [sent-128, score-0.9]
</p><p>74 In between spikes, the weight therefore no pre- or postsynaptic spike at time ( changes as: (22)  (  At the time of spikes, the weight change is more complex, because earlier weight changes have to be modiﬁed according to the new state information given by the spikes. [sent-131, score-1.426]
</p><p>75 (16)), this expression is equal to  ¦ ¤ ¨  (  ( %@£ 0 (  & F$    as before. [sent-136, score-0.035]
</p><p>76 Putting everything together, one gets the update rule for  ¦4£&  ¢ I¦ 8  C 4£ $  ¦ §¤ C 4 £ $  8 I¦  ¨ ©  (   ( %( £ '$ 0 &   $  d 4 & 4 9 ¡ 1 e¦ £ ¢ 8 ¦  £ 5    ¦4£&  & '$ ¨    with the same  (28) :  (29)  Together with eqs. [sent-137, score-0.253]
</p><p>77 (16), (17), (19), and (24) this constitutes our learning rule. [sent-138, score-0.02]
</p><p>78 It is causal, because it depends only on past, not on future signals, but in the long run it will give the same weight change as the standard hidden Markov rule (2). [sent-139, score-0.559]
</p><p>79 (29) evolve according to linear rules, and the weight changes according to the simple rule (22). [sent-142, score-0.344]
</p><p>80 These simpliﬁcations are a consequence of assuming, in the deﬁnition of , that there are no future spikes. [sent-143, score-0.083]
</p><p>81 ¢  ¨  ¢  ¨  ¡  0  ¥  (  (    ¡¡   01(1C q p£ 0 ¡   0  0  ¡  0  ¡  ¡  0  ¡  ¡  0  ¡  This learning rule still has a rather unusual form. [sent-145, score-0.207]
</p><p>82 Usually, one writes as the sum of plus some weight change. [sent-146, score-0.149]
</p><p>83 It occurs because a new spike changes the probability estimates of previous states, and thereby the desired weight. [sent-150, score-0.329]
</p><p>84 4 Summary of the learning algorithm To simplify notation, we combined the pre- and postsynaptic Markov models into a single one. [sent-152, score-0.545]
</p><p>85 How does the learning rule look in terms of the original pre- and postsynaptic paramstates and the postsynaptic one , then the eters? [sent-153, score-1.238]
</p><p>86 If the presynaptic model has combined model has states. [sent-154, score-0.142]
</p><p>87 At each time step, we have to update not only the weight but signal traces , which we will now write as , where denotes the presynaptic and the postsynaptic state. [sent-155, score-0.902]
</p><p>88 However, one needs to update only of the signal traces , because they factorize into a pre- and a postsynaptic part: . [sent-156, score-0.604]
</p><p>89 Deﬁne the weight change for all possible state pairs. [sent-158, score-0.24]
</p><p>90 4 Conclusion This demonstrates that the basic principle of associating not individual spikes, but whole ﬁring episodes, can be implemented in a causal learning rule, which depends only on past signals. [sent-161, score-0.34]
</p><p>91 This rule does not have to store the time of all past spikes, but only a few signal traces and , and may thus be biologically plausible. [sent-162, score-0.406]
</p><p>92 For the right parameter choice, it agrees well with some nonlinear features of cortical synaptic plasticity (ﬁg. [sent-163, score-0.573]
</p><p>93 This does not imply that actual synaptic plasticity follows the same rule, but only that these particular features are consistent with our basic principle. [sent-165, score-0.377]
</p><p>94 Based on the predictions of this rule, one could design more precise experimental tests of whether cortical synaptic plasticity associates individual spikes or whole ﬁring episodes. [sent-166, score-0.915]
</p><p>95 Sejnowski for his comments on a similar type of learning rules, which he suggested to call ”hidden Hebbian learning”. [sent-169, score-0.02]
</p><p>96 The second author (KM) would like to emphasize that his contribution to this paper was limited to assistance in writing. [sent-170, score-0.021]
</p><p>97 An algorithm for modifying neurotransmitter release probability based on pre- and postsynaptic spike timing. [sent-191, score-0.78]
</p><p>98 Rate, timing, and cooperativity jointly determine cortical synaptic plasticity. [sent-201, score-0.288]
</p><p>99 A tutorial on hidden Markov models and selected applications in speech recognition. [sent-212, score-0.068]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('postsynaptic', 0.525), ('spikes', 0.354), ('spike', 0.255), ('episodes', 0.249), ('ring', 0.226), ('triplets', 0.194), ('synaptic', 0.187), ('rule', 0.168), ('plasticity', 0.167), ('presynaptic', 0.142), ('ms', 0.141), ('weight', 0.129), ('markov', 0.129), ('past', 0.122), ('ltp', 0.116), ('dw', 0.106), ('cortical', 0.101), ('ltd', 0.099), ('future', 0.083), ('episode', 0.083), ('ae', 0.074), ('hidden', 0.068), ('state', 0.068), ('timing', 0.066), ('post', 0.062), ('agrees', 0.061), ('causal', 0.058), ('pre', 0.051), ('changes', 0.047), ('trains', 0.045), ('depend', 0.045), ('eisele', 0.045), ('froemke', 0.045), ('integrative', 0.045), ('scr', 0.045), ('trsrr', 0.045), ('whole', 0.045), ('traces', 0.044), ('depends', 0.044), ('probabilities', 0.043), ('change', 0.043), ('cb', 0.04), ('associate', 0.039), ('certainty', 0.039), ('expectation', 0.038), ('associates', 0.038), ('neuron', 0.038), ('states', 0.038), ('ba', 0.037), ('keck', 0.035), ('potentiation', 0.035), ('update', 0.035), ('equal', 0.035), ('nonlinear', 0.034), ('uc', 0.032), ('eigenvector', 0.032), ('emission', 0.031), ('wire', 0.031), ('triplet', 0.031), ('dependence', 0.031), ('rules', 0.03), ('last', 0.029), ('super', 0.028), ('weakly', 0.028), ('principle', 0.028), ('occurs', 0.027), ('everything', 0.027), ('time', 0.027), ('sequences', 0.027), ('eigenvalue', 0.027), ('chains', 0.026), ('rat', 0.026), ('familiar', 0.026), ('putting', 0.026), ('hebbian', 0.026), ('induced', 0.024), ('long', 0.024), ('synapse', 0.024), ('cortex', 0.024), ('features', 0.023), ('gets', 0.023), ('layers', 0.023), ('store', 0.023), ('individual', 0.023), ('observable', 0.023), ('together', 0.022), ('start', 0.022), ('biologically', 0.022), ('evolution', 0.021), ('contribution', 0.021), ('neurons', 0.021), ('summing', 0.021), ('francisco', 0.02), ('sequence', 0.02), ('backward', 0.02), ('learning', 0.02), ('plus', 0.02), ('eters', 0.019), ('unusual', 0.019), ('enumerator', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="102-tfidf-1" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>Author: Michael Eisele, Kenneth D. Miller</p><p>Abstract: Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synaptic plasticity does not associate individual spikes, but rather whole ﬁring episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Standard methods from hidden Markov models are used to deﬁne what “ﬁring episodes” are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002).</p><p>2 0.32262829 <a title="102-tfidf-2" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>Author: Arunava Banerjee, Alexandre Pouget</p><p>Abstract: If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.</p><p>3 0.30297831 <a title="102-tfidf-3" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>Author: R. J. Vogelstein, Francesco Tenore, Ralf Philipp, Miriam S. Adlerstein, David H. Goldberg, Gert Cauwenberghs</p><p>Abstract: Address-event representation (AER), originally proposed as a means to communicate sparse neural events between neuromorphic chips, has proven efﬁcient in implementing large-scale networks with arbitrary, conﬁgurable synaptic connectivity. In this work, we further extend the functionality of AER to implement arbitrary, conﬁgurable synaptic plasticity in the address domain. As proof of concept, we implement a biologically inspired form of spike timing-dependent plasticity (STDP) based on relative timing of events in an AER framework. Experimental results from an analog VLSI integrate-and-ﬁre network demonstrate address domain learning in a task that requires neurons to group correlated inputs.</p><p>4 0.23200184 <a title="102-tfidf-4" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>Author: Luk Chong Yeung, Brian S. Blais, Leon N. Cooper, Harel Z. Shouval</p><p>Abstract: A uniﬁed, biophysically motivated Calcium-Dependent Learning model has been shown to account for various rate-based and spike time-dependent paradigms for inducing synaptic plasticity. Here, we investigate the properties of this model for a multi-synapse neuron that receives inputs with diﬀerent spike-train statistics. In addition, we present a physiological form of metaplasticity, an activity-driven regulation mechanism, that is essential for the robustness of the model. A neuron thus implemented develops stable and selective receptive ﬁelds, given various input statistics 1</p><p>5 0.20462838 <a title="102-tfidf-5" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>Author: Brian Taba, Kwabena A. Boahen</p><p>Abstract: We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2] . This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modeling largely unelucidated and exceedingly plastic higher processing centers in cortex. Instead of an explicit blueprint for every cortical area, what is needed is a developmental rule that can wire complex circuits from minimal specifications. One candidate is the famous</p><p>6 0.18144783 <a title="102-tfidf-6" href="./nips-2002-Neuromorphic_Bisable_VLSI_Synapses_with_Spike-Timing-Dependent_Plasticity.html">154 nips-2002-Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity</a></p>
<p>7 0.168193 <a title="102-tfidf-7" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>8 0.16716887 <a title="102-tfidf-8" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>9 0.16549982 <a title="102-tfidf-9" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>10 0.16274172 <a title="102-tfidf-10" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>11 0.16118562 <a title="102-tfidf-11" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>12 0.132174 <a title="102-tfidf-12" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>13 0.13112201 <a title="102-tfidf-13" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>14 0.11782787 <a title="102-tfidf-14" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>15 0.10397485 <a title="102-tfidf-15" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>16 0.10131622 <a title="102-tfidf-16" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>17 0.092568867 <a title="102-tfidf-17" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>18 0.086694591 <a title="102-tfidf-18" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>19 0.079024933 <a title="102-tfidf-19" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>20 0.076426014 <a title="102-tfidf-20" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, 0.332), (2, 0.031), (3, -0.18), (4, 0.053), (5, 0.228), (6, 0.127), (7, -0.022), (8, -0.023), (9, -0.072), (10, 0.04), (11, -0.084), (12, -0.005), (13, -0.009), (14, -0.002), (15, 0.009), (16, -0.124), (17, -0.009), (18, -0.108), (19, 0.083), (20, -0.255), (21, -0.089), (22, -0.012), (23, 0.051), (24, -0.086), (25, -0.081), (26, -0.019), (27, -0.046), (28, -0.045), (29, -0.123), (30, 0.059), (31, -0.019), (32, 0.143), (33, -0.05), (34, -0.143), (35, -0.01), (36, -0.008), (37, -0.066), (38, 0.006), (39, 0.082), (40, 0.01), (41, 0.066), (42, 0.027), (43, -0.102), (44, -0.036), (45, 0.151), (46, -0.029), (47, 0.033), (48, -0.012), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97534353 <a title="102-lsi-1" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>Author: Michael Eisele, Kenneth D. Miller</p><p>Abstract: Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synaptic plasticity does not associate individual spikes, but rather whole ﬁring episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Standard methods from hidden Markov models are used to deﬁne what “ﬁring episodes” are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002).</p><p>2 0.80501133 <a title="102-lsi-2" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>Author: R. J. Vogelstein, Francesco Tenore, Ralf Philipp, Miriam S. Adlerstein, David H. Goldberg, Gert Cauwenberghs</p><p>Abstract: Address-event representation (AER), originally proposed as a means to communicate sparse neural events between neuromorphic chips, has proven efﬁcient in implementing large-scale networks with arbitrary, conﬁgurable synaptic connectivity. In this work, we further extend the functionality of AER to implement arbitrary, conﬁgurable synaptic plasticity in the address domain. As proof of concept, we implement a biologically inspired form of spike timing-dependent plasticity (STDP) based on relative timing of events in an AER framework. Experimental results from an analog VLSI integrate-and-ﬁre network demonstrate address domain learning in a task that requires neurons to group correlated inputs.</p><p>3 0.71180868 <a title="102-lsi-3" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>Author: Luk Chong Yeung, Brian S. Blais, Leon N. Cooper, Harel Z. Shouval</p><p>Abstract: A uniﬁed, biophysically motivated Calcium-Dependent Learning model has been shown to account for various rate-based and spike time-dependent paradigms for inducing synaptic plasticity. Here, we investigate the properties of this model for a multi-synapse neuron that receives inputs with diﬀerent spike-train statistics. In addition, we present a physiological form of metaplasticity, an activity-driven regulation mechanism, that is essential for the robustness of the model. A neuron thus implemented develops stable and selective receptive ﬁelds, given various input statistics 1</p><p>4 0.69288236 <a title="102-lsi-4" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>Author: Brian Taba, Kwabena A. Boahen</p><p>Abstract: We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2] . This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modeling largely unelucidated and exceedingly plastic higher processing centers in cortex. Instead of an explicit blueprint for every cortical area, what is needed is a developmental rule that can wire complex circuits from minimal specifications. One candidate is the famous</p><p>5 0.6846835 <a title="102-lsi-5" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>Author: Arunava Banerjee, Alexandre Pouget</p><p>Abstract: If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.</p><p>6 0.61020964 <a title="102-lsi-6" href="./nips-2002-Neuromorphic_Bisable_VLSI_Synapses_with_Spike-Timing-Dependent_Plasticity.html">154 nips-2002-Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity</a></p>
<p>7 0.54687023 <a title="102-lsi-7" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>8 0.54208112 <a title="102-lsi-8" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>9 0.48795247 <a title="102-lsi-9" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>10 0.44241312 <a title="102-lsi-10" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>11 0.41976747 <a title="102-lsi-11" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>12 0.40030146 <a title="102-lsi-12" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>13 0.34031755 <a title="102-lsi-13" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>14 0.28228599 <a title="102-lsi-14" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>15 0.27390277 <a title="102-lsi-15" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>16 0.26523083 <a title="102-lsi-16" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>17 0.26267812 <a title="102-lsi-17" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>18 0.25977305 <a title="102-lsi-18" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>19 0.23311245 <a title="102-lsi-19" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>20 0.23280954 <a title="102-lsi-20" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.012), (11, 0.01), (23, 0.025), (42, 0.072), (54, 0.086), (55, 0.037), (57, 0.012), (67, 0.013), (68, 0.109), (74, 0.064), (92, 0.019), (95, 0.272), (98, 0.176)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82739019 <a title="102-lda-1" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>Author: Michael Eisele, Kenneth D. Miller</p><p>Abstract: Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synaptic plasticity does not associate individual spikes, but rather whole ﬁring episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Standard methods from hidden Markov models are used to deﬁne what “ﬁring episodes” are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002).</p><p>2 0.65040708 <a title="102-lda-2" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>Author: David Barber</p><p>Abstract: The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the beneﬁts of a tractable probabilistic model. 1</p><p>3 0.64868593 <a title="102-lda-3" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>Author: Bernd Fischer, Johann Schumann, Wray Buntine, Alexander G. Gray</p><p>Abstract: Machine learning has reached a point where many probabilistic methods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e.g., as different instances of the EM algorithm. This enables the systematic derivation of algorithms customized for different models. Here, we describe the AUTO BAYES system which takes a high-level statistical model speciﬁcation, uses powerful symbolic techniques based on schema-based program synthesis and computer algebra to derive an efﬁcient specialized algorithm for learning that model, and generates executable code implementing that algorithm. This capability is far beyond that of code collections such as Matlab toolboxes or even tools for model-independent optimization such as BUGS for Gibbs sampling: complex new algorithms can be generated without new programming, algorithms can be highly specialized and tightly crafted for the exact structure of the model and data, and efﬁcient and commented code can be generated for different languages or systems. We present automatically-derived algorithms ranging from closed-form solutions of Bayesian textbook problems to recently-proposed EM algorithms for clustering, regression, and a multinomial form of PCA. 1 Automatic Derivation of Statistical Algorithms Overview. We describe a symbolic program synthesis system which works as a “statistical algorithm compiler:” it compiles a statistical model speciﬁcation into a custom algorithm design and from that further down into a working program implementing the algorithm design. This system, AUTO BAYES, can be loosely thought of as “part theorem prover, part Mathematica, part learning textbook, and part Numerical Recipes.” It provides much more ﬂexibility than a ﬁxed code repository such as a Matlab toolbox, and allows the creation of efﬁcient algorithms which have never before been implemented, or even written down. AUTO BAYES is intended to automate the more routine application of complex methods in novel contexts. For example, recent multinomial extensions to PCA [2, 4] can be derived in this way. The algorithm design problem. Given a dataset and a task, creating a learning method can be characterized by two main questions: 1. What is the model? 2. What algorithm will optimize the model parameters? The statistical algorithm (i.e., a parameter optimization algorithm for the statistical model) can then be implemented manually. The system in this paper answers the algorithm question given that the user has chosen a model for the data,and continues through to implementation. Performing this task at the state-of-the-art level requires an intertwined meld of probability theory, computational mathematics, and software engineering. However, a number of factors unite to allow us to solve the algorithm design problem computationally: 1. The existence of fundamental building blocks (e.g., standardized probability distributions, standard optimization procedures, and generic data structures). 2. The existence of common representations (i.e., graphical models [3, 13] and program schemas). 3. The formalization of schema applicability constraints as guards. 1 The challenges of algorithm design. The design problem has an inherently combinatorial nature, since subparts of a function may be optimized recursively and in different ways. It also involves the use of new data structures or approximations to gain performance. As the research in statistical algorithms advances, its creative focus should move beyond the ultimately mechanical aspects and towards extending the abstract applicability of already existing schemas (algorithmic principles like EM), improving schemas in ways that generalize across anything they can be applied to, and inventing radically new schemas. 2 Combining Schema-based Synthesis and Bayesian Networks Statistical Models. Externally, AUTO BAYES has the look and feel of 2 const int n_points as ’nr. of data points’ a compiler. Users specify their model 3 with 0 < n_points; 4 const int n_classes := 3 as ’nr. classes’ of interest in a high-level speciﬁcation 5 with 0 < n_classes language (as opposed to a program6 with n_classes << n_points; ming language). The ﬁgure shows the 7 double phi(1..n_classes) as ’weights’ speciﬁcation of the mixture of Gaus8 with 1 = sum(I := 1..n_classes, phi(I)); 9 double mu(1..n_classes); sians example used throughout this 9 double sigma(1..n_classes); paper.2 Note the constraint that the 10 int c(1..n_points) as ’class labels’; sum of the class probabilities must 11 c ˜ disc(vec(I := 1..n_classes, phi(I))); equal one (line 8) along with others 12 data double x(1..n_points) as ’data’; (lines 3 and 5) that make optimization 13 x(I) ˜ gauss(mu(c(I)), sigma(c(I))); of the model well-deﬁned. Also note 14 max pr(x| phi,mu,sigma ) wrt phi,mu,sigma ; the ability to specify assumptions of the kind in line 6, which may be used by some algorithms. The last line speciﬁes the goal inference task: maximize the conditional probability pr with respect to the parameters , , and . Note that moving the parameters across to the left of the conditioning bar converts this from a maximum likelihood to a maximum a posteriori problem. 1 model mog as ’Mixture of Gaussians’; ¡   £  £  £ §¤¢ £ © ¨ ¦ ¥ ©   ¡     ¡ £ £ £ ¨ Computational logic and theorem proving. Internally, AUTO BAYES uses a class of techniques known as computational logic which has its roots in automated theorem proving. AUTO BAYES begins with an initial goal and a set of initial assertions, or axioms, and adds new assertions, or theorems, by repeated application of the axioms, until the goal is proven. In our context, the goal is given by the input model; the derived algorithms are side effects of constructive theorems proving the existence of algorithms for the goal. 1 Schema guards vary widely; for example, compare Nead-Melder simplex or simulated annealing (which require only function evaluation), conjugate gradient (which require both Jacobian and Hessian), EM and its variational extension [6] (which require a latent-variable structure model). 2 Here, keywords have been underlined and line numbers have been added for reference in the text. The as-keyword allows annotations to variables which end up in the generated code’s comments. Also, n classes has been set to three (line 4), while n points is left unspeciﬁed. The class variable and single data variable are vectors, which deﬁnes them as i.i.d. Computer algebra. The ﬁrst core element which makes automatic algorithm derivation feasible is the fact that we can mechanize the required symbol manipulation, using computer algebra methods. General symbolic differentiation and expression simpliﬁcation are capabilities fundamental to our approach. AUTO BAYES contains a computer algebra engine using term rewrite rules which are an efﬁcient mechanism for substitution of equal quantities or expressions and thus well-suited for this task.3 Schema-based synthesis. The computational cost of full-blown theorem proving grinds simple tasks to a halt while elementary and intermediate facts are reinvented from scratch. To achieve the scale of deduction required by algorithm derivation, we thus follow a schema-based synthesis technique which breaks away from strict theorem proving. Instead, we formalize high-level domain knowledge, such as the general EM strategy, as schemas. A schema combines a generic code fragment with explicitly speciﬁed preconditions which describe the applicability of the code fragment. The second core element which makes automatic algorithm derivation feasible is the fact that we can use Bayesian networks to efﬁciently encode the preconditions of complex algorithms such as EM. First-order logic representation of Bayesian netNclasses works. A ﬁrst-order logic representation of Bayesian µ σ networks was developed by Haddawy [7]. In this framework, random variables are represented by functor symbols and indexes (i.e., speciﬁc instances φ x c of i.i.d. vectors) are represented as functor arguments. discrete gauss Nclasses Since unknown index values can be represented by Npoints implicitly universally quantiﬁed Prolog variables, this approach allows a compact encoding of networks involving i.i.d. variables or plates [3]; the ﬁgure shows the initial network for our running example. Moreover, such networks correspond to backtrack-free datalog programs, allowing the dependencies to be efﬁciently computed. We have extended the framework to work with non-ground probability queries since we seek to determine probabilities over entire i.i.d. vectors and matrices. Tests for independence on these indexed Bayesian networks are easily developed in Lauritzen’s framework which uses ancestral sets and set separation [9] and is more amenable to a theorem prover than the double negatives of the more widely known d-separation criteria. Given a Bayesian network, some probabilities can easily be extracted by enumerating the component probabilities at each node: § ¥ ¨¦¡ ¡ ¢© Lemma 1. Let be sets of variables over a Bayesian network with . Then descendents and parents hold 4 in the corresponding dependency graph iff the following probability statement holds: £ ¤  ¡ parents B % % 9 C0A@ ! 9  @8 § ¥   ¢   2 ' % % 310  parents    ©¢   £ ¡ !    ' % #!  </p><p>4 0.64291918 <a title="102-lda-4" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>Author: Luk Chong Yeung, Brian S. Blais, Leon N. Cooper, Harel Z. Shouval</p><p>Abstract: A uniﬁed, biophysically motivated Calcium-Dependent Learning model has been shown to account for various rate-based and spike time-dependent paradigms for inducing synaptic plasticity. Here, we investigate the properties of this model for a multi-synapse neuron that receives inputs with diﬀerent spike-train statistics. In addition, we present a physiological form of metaplasticity, an activity-driven regulation mechanism, that is essential for the robustness of the model. A neuron thus implemented develops stable and selective receptive ﬁelds, given various input statistics 1</p><p>5 0.63269401 <a title="102-lda-5" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>Author: Shih-Chii Liu, Malte Boegershausen, Pascal Suter</p><p>Abstract: We describe a model of short-term synaptic depression that is derived from a silicon circuit implementation. The dynamics of this circuit model are similar to the dynamics of some present theoretical models of shortterm depression except that the recovery dynamics of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency. The equations describing the steady-state and transient responses of this synaptic model ﬁt the experimental results obtained from a fabricated silicon network consisting of leaky integrate-and-ﬁre neurons and different types of synapses. We also show experimental data demonstrating the possible computational roles of depression. One possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential.</p><p>6 0.62965459 <a title="102-lda-6" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>7 0.62642777 <a title="102-lda-7" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>8 0.62476742 <a title="102-lda-8" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>9 0.62315083 <a title="102-lda-9" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>10 0.62160677 <a title="102-lda-10" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>11 0.61666059 <a title="102-lda-11" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>12 0.61421436 <a title="102-lda-12" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>13 0.60688835 <a title="102-lda-13" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>14 0.60640764 <a title="102-lda-14" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>15 0.60533643 <a title="102-lda-15" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>16 0.6047684 <a title="102-lda-16" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>17 0.60214502 <a title="102-lda-17" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>18 0.59940851 <a title="102-lda-18" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>19 0.59817463 <a title="102-lda-19" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>20 0.59574533 <a title="102-lda-20" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
