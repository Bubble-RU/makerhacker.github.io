<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-143" href="#">nips2002-143</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</h1>
<br/><p>Source: <a title="nips-2002-143-pdf" href="http://papers.nips.cc/paper/2250-mean-field-approach-to-a-probabilistic-model-in-information-retrieval.pdf">pdf</a></p><p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>Reference: <a title="nips-2002-143-reference" href="../nips2002_reference/nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). [sent-6, score-0.239]
</p><p>2 Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. [sent-7, score-0.025]
</p><p>3 The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. [sent-8, score-0.252]
</p><p>4 The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR. [sent-9, score-0.028]
</p><p>5 1 Introduction The area of information retrieval (IR) studies the representation, organization and access of information in an information repository. [sent-10, score-0.256]
</p><p>6 With the advent and boom of the Internet, especially the World Wide Web (WWW), more and more information is available to be shared online. [sent-11, score-0.023]
</p><p>7 In this respect, probabilistic models have become very useful in empowering information searches [1, 2]. [sent-13, score-0.075]
</p><p>8 In fact, information searches themselves contain rich information, which can be recorded and fruitfully used to improve the performance of subsequent retrievals. [sent-14, score-0.035]
</p><p>9 This is an extension of the process of relevance feedback [3], which incorporates the relevance assessments supplied by the user to construct new representations for queries, during the procedure of the users interactive document retrieval. [sent-15, score-0.564]
</p><p>10 In the process, the feedback information helps to reﬁne the queries continuously, but the effects pertain only to the particular retrieval session. [sent-16, score-0.746]
</p><p>11 On the other hand, our objective is to reﬁne the representations of documents and queries with the help of relevancy data, so that subsequent retrieval sessions can be beneﬁted. [sent-17, score-1.245]
</p><p>12 (Here we assume a model in which categories are represented by queries. [sent-20, score-0.025]
</p><p>13 Since documents and queries are represented by high dimensional vectors in a vector space model, a mean-ﬁeld approach will be adopted. [sent-22, score-0.804]
</p><p>14 mean-ﬁeld methods were commonly used to study magnetic systems in statistical physics, but thanks to their ability to deal with high dimensional systems, they are increasingly applied to many areas of information processing recently [6]. [sent-23, score-0.025]
</p><p>15 After introducing the parametric model in Section 2, the mean-ﬁeld approach will be used in two steps. [sent-25, score-0.054]
</p><p>16 First, in Section 3, the true representations of documents and queries will be estimated by maximizing the total probability of observation. [sent-26, score-0.854]
</p><p>17 It results in a set of meanﬁeld equations, which can be solved by a fast iterative algorithm. [sent-27, score-0.068]
</p><p>18 Respectively, the estimated true documents and queries will then be used for ad hoc information retrieval and document routing. [sent-28, score-1.792]
</p><p>19 Secondly, the model depends on a few hyperparameters which are conventionally determined by the cross-validation method. [sent-29, score-0.159]
</p><p>20 Here, as described in Section 4, the mean-ﬁeld approach can be used again to accelerate the otherwise tedious leave-one-out cross-validation procedure. [sent-30, score-0.042]
</p><p>21 In Section 6, we compare the model with the standard tf-idf [8] and latent semantic indexing (LSI) [9] on benchmark test collections. [sent-32, score-0.163]
</p><p>22 As we shall see, the validity of our model is well supported by its superior performance. [sent-33, score-0.025]
</p><p>23 Assume that a set of documents and queries is available to us. [sent-36, score-0.777]
</p><p>24 In the vector space model, each document and query is represented by an dimensional vector. [sent-37, score-0.537]
</p><p>25 The vectors are denoted by ( ), which are referred to as the true meaning of the document (query). [sent-38, score-0.398]
</p><p>26 In other words, the document is generated from its true meaning . [sent-40, score-0.371]
</p><p>27 ¦      ¦ ¨ ¦©  ¨ ¢¦  ¨ ¦  ¦  that the user actually submits is also distributed around the true (b) Similarly, the query query vector according to the probability distribution distribution . [sent-41, score-0.589]
</p><p>28  ©   ¨ §  §  (c) There is some relation between the document and query, called relevancy assessment. [sent-43, score-0.5]
</p><p>29 We denote this relation with a binary variable for each pair of document and query. [sent-44, score-0.315]
</p><p>30 If , we say the document is relevant to the query, that is, the document is what the user wants. [sent-45, score-0.601]
</p><p>31 Otherwise, and the document is irrelevant to the query. [sent-46, score-0.284]
</p><p>32 Suppose we have some relevancy relations between documents and queries (through historical records, from experts, etc. [sent-47, score-0.962]
</p><p>33 Then we hypothesize that the true documents and queries are distributed , that is, the true representation of documents and according to the distribution queries should satisfy their relevancy relations. [sent-49, score-1.881]
</p><p>34 £ ¡ ¤¢   ¦ ¡ §¥     ¨ ¤   § ¦ ©©  We summarize the idea through a probabilistic meta-structure shown in Figure 1. [sent-50, score-0.04]
</p><p>35 fQ (Q 0 | Q)  Q B  Q0  fB (D,Q | B ) fD (D 0 | D)  D data  D0 data  unknown parameters Figure 1: Probabilistic meta-structure  In order to complete the model, we need to hypothesize the form of the distribution functions. [sent-51, score-0.042]
</p><p>36 In this paper, we restrict the documents and queries to a hypersphere, since usually only the cosines of the angles between documents and queries are used to determine the similarity between documents and queries. [sent-52, score-1.933]
</p><p>37 Hence, we assume the following distribution functions: given its true location  ¦  ¨ ¦  (a) The distribution of each observed document  :  4   28& ¨ § ' 7 ¨ 3 § 6§ £ ! [sent-53, score-0.334]
</p><p>38 G £ 0 ( & $ £ 0 ( & $    ¨ "¦  ©  (b) The distribution of each observed query  given its true location  (1)  : (2)   ¨ "§! [sent-57, score-0.303]
</p><p>39  ©  (c) The prior distribution of the documents and queries, given the relevance relation between them:  ¨ ¨ 3  3 3  ¨© $  UT $ ¡  DB A H 'GF   EC§ 75¦ @9 ¨ ©  is the Dirac -function, and , and are normalization constants of respectively, and are hence independent of and . [sent-58, score-0.451]
</p><p>40 PA ¡ 3 ¥G £ B £ A B A £ ¡ ¢  Y  where  (5) (6)  X  and denotes all hyperparameters . [sent-62, score-0.134]
</p><p>41 There is now an appealing correspondence between the present model and spin models in statistical physics. [sent-63, score-0.025]
</p><p>42 As a byproduct, we can also obtain the estimation of the true queries , which in turn can be used in document routing: new documents should be compared with to determine whether it belongs to this category or not. [sent-67, score-1.174]
</p><p>43 So our model gives a unifying procedure for both ad hoc retrieval and routing. [sent-68, score-0.706]
</p><p>44 ©¦  ©§  ©¦  ¨ ¦  ©§  3 Parameter Estimation In this section, we derive a fast iterative algorithm for parameter estimation. [sent-69, score-0.068]
</p><p>45 £ B £ A   ¡ e##07   B § £ A ¦ £ "  ¡   B ¡ A ¡    Y 3 i  $  i  i  (7)  i  "  where formula, we have changed the integration to the imaginary axis. [sent-72, score-0.022]
</p><p>46 In writing this  "¦  , and , when the integration can be Mean-ﬁeld theory works in the limit of large well approximated by taking the saddle point of . [sent-74, score-0.071]
</p><p>47 This is obtained by equating the partial derivatives of with respect to , , and to zero, yielding (8) (9) (10) (11)  This set of equations is referred to as the mean-ﬁeld equations, since ﬂuctuations around the mean values of the parameters have been neglected. [sent-75, score-0.055]
</p><p>48 Due to its simple form, it can be solved by an iterative scheme. [sent-76, score-0.034]
</p><p>49 Though we have not studied the theoretical convergence of the iterative scheme, its effectiveness can be seen from the following arguments. [sent-77, score-0.034]
</p><p>50 (9) by the respective values of and at the saddle point, then the iteration process becomes a linear one. [sent-80, score-0.078]
</p><p>51 Hence after this linear iteration problem by scale factors of using Eqs. [sent-83, score-0.029]
</p><p>52 (10) and (11), the problem is equivalent to rescaling the lengths of the iterated  3B   ©  (3 B  A ©  64 5 5 3A 3A 4 B  B ©  A ©  £ ¡ (& C§ & B  £ )('&F;¦ & ¡ A  vectors back to the hypersphere deﬁned by and . [sent-84, score-0.13]
</p><p>53 This alternate operation of linear iteration and rescaling back to the hypersphere makes it a very stable algorithm. [sent-85, score-0.132]
</p><p>54 The complexity of the algorithm is linear in the number of documents and queries. [sent-86, score-0.356]
</p><p>55 Alternatively, one may use the Augmented Lagrangian method to ﬁnd the saddle point of , whose convergence is guaranteed, but is computationally more complex [10]. [sent-88, score-0.049]
</p><p>56 "  4 Hyperparameter Estimation  £  ¡     G  In our model, the parameters , and determine the shape of the distributions , and , and inﬂuence the parameter estimation described in Section 3. [sent-89, score-0.063]
</p><p>57 They have to be chosen so that the model performs optimally when new queries are raised to retrieve documents, or when new documents are routed. [sent-91, score-0.802]
</p><p>58 © ©  ¨©  A standard method for hyperparameter estimation in machine learning is leave-one-out cross-validation [11]. [sent-92, score-0.181]
</p><p>59 Then each time we pick one data as the validation set and train the model with the rest of the examples. [sent-94, score-0.025]
</p><p>60 The hyperparameters are chosen as the ones that give the optimal performance averaged over the test examples. [sent-95, score-0.158]
</p><p>61 £ 0           The exact leave-one-out cross-validation is very tedious, especially for multiple hyperparameters, because of the need to train the model times for each combination of hyperparameters. [sent-96, score-0.117]
</p><p>62 For this model, we propose an approximate leave-one-out procedure based on the cavity method [7]. [sent-97, score-0.084]
</p><p>63 The solution can be further simpliﬁed by using the mean-ﬁeld argument that the changes induced by removing the query on documents can be decoupled. [sent-99, score-0.609]
</p><p>64 Hence we can neglect the off-diagonal terms, yielding (15)    ¢A © ¦     & '( 4 %$$ "(! [sent-100, score-0.026]
</p><p>65 # ¨   ¤¥£ B $ ( G 0 A ©  ¡ A  ¨ §   A QG      §  ¦  ¦  0  A ¦ ¡   A © ¦ D ©B © 7A © 9 ¢  Note that have been known in the systemwide training. [sent-101, score-0.053]
</p><p>66 The similarities between and are then used to predict the leave-one-out ad hoc retrieval performance of the model. [sent-103, score-0.733]
</p><p>67 ¢A © ¦    §  A  Note that we need to train the model only once, and the leave-one-out estimation of documents and queries can be obtained in one step. [sent-105, score-0.865]
</p><p>68 Amazingly, it also gives reasonable estimations of hyperparameters, as shown in the following experiments. [sent-107, score-0.053]
</p><p>69 We remark that the mean-ﬁeld technique can be applied to distributions of documents, queries and relevance feedbacks other than those described by Eqs. [sent-108, score-0.485]
</p><p>70 (1-3), our model is similar to the Gaussian model, if the spherical constraint on ’s and ’s are replaced by a spherical Gaussian prior. [sent-111, score-0.089]
</p><p>71 On the other hand, the mean-ﬁeld estimation greatly simpliﬁes the process by neglecting the off-diagonal elements. [sent-113, score-0.063]
</p><p>72 ¦  §  5 Experimental Results We have applied the proposed method to ad hoc retrieval and routing for the test collections of Cranﬁeld and CISI. [sent-114, score-0.828]
</p><p>73 Because we treat both tasks identically, we use the same evaluation criterion: the recall precision curve and the average retrieval precision. [sent-115, score-0.417]
</p><p>74 We have run two versions of our algorithm: (a) in the original dimension, the observed documents and queries are represented by the original tf-idf weights; (b) in the reduced dimension of 100, in which the original vectors are reduced by singular value decomposition (SVD) in LSI. [sent-116, score-0.94]
</p><p>75 2 (a-b), we show the recall precision curves at the optimal hyperparameters. [sent-118, score-0.132]
</p><p>76 The mean-ﬁeld estimates are compared with the baseline results of LSI. [sent-119, score-0.025]
</p><p>77 It is clear that our method gives signiﬁcant gains in retrieval precision. [sent-120, score-0.256]
</p><p>78 Comparisons using the original dimension or the Cranﬁeld collection, not shown here due to space limitations, yield equally satisfactory results. [sent-121, score-0.048]
</p><p>79 3, we have plotted the average precision versus the two hyperparameters, as computed by the two methods. [sent-138, score-0.148]
</p><p>80 This demonstrates the usefulness of the mean-ﬁeld approximation in hyperparameter estimation. [sent-140, score-0.118]
</p><p>81 In Table 1, we obtain the values of the optimal hyperparameters from the mean-ﬁeld leave-  one-out method, and the average precisions of the exact leave-one-out are then computed using these optimal hyperparameters. [sent-141, score-0.274]
</p><p>82 These are compared with the results of the exact leave-one-out and listed in Table 1. [sent-142, score-0.069]
</p><p>83 For the hyperparameter estimation in the original dimension, the exact leave-one-out is not available since it is too tedious. [sent-143, score-0.25]
</p><p>84 Instead, we compare the hyperparameters with the ones from the -fold cross-validation. [sent-144, score-0.158]
</p><p>85 Whether we compare the mean-ﬁeld with the exact leave-one-out or -fold cross-validation, the optimal hperparameters are comparable in most cases, and when there are discrepancies, one can observe that the average precisions are essentially the same. [sent-145, score-0.14]
</p><p>86 Figure 3: Average retrieval precision versus hyperparameters for ad hoc retrieval in reduced ; dimension for CISI: (a) mean-ﬁeld leave-one-out, peaked at (b) exact leave-one-out. [sent-146, score-1.379]
</p><p>87   £  ¦  C G 5 £ 'HG 5 ¡   ¡  ¡  Table 1: The average retrieval precision for leave-one-out cross-validation in reduced dimension: mean-ﬁeld versus exact. [sent-149, score-0.448]
</p><p>88 CISI Cranﬁeld Average precision Average precision ad hoc retrieval LSI – – 0. [sent-150, score-0.863]
</p><p>89 356  G5£  G5¡   G5£  G5¡   6 Conclusion We have considered a probabilistic model of documents, queries and relevancy assessments. [sent-178, score-0.671]
</p><p>90 Fast algorithms are derived for parameter and hyperparameter estimations. [sent-179, score-0.118]
</p><p>91 Significant improvement is achieved for both ad hoc retrieval and routing compared with tf-idf and LSI. [sent-180, score-0.828]
</p><p>92 In another paper [12], we have compared the model with other heuristic methods such as Rocchio heuristics [3] and Bartell’s Multidimensional Scaling [13], and the mean-ﬁeld method still outperforms them. [sent-181, score-0.047]
</p><p>93 These successes illustrate the potentials of the mean-ﬁeld approach, which is especially suitable for systems with high dimensions and  numerous mutually interacting components, such as those in IR. [sent-182, score-0.023]
</p><p>94 Hence we anticipate that mean-ﬁeld methods will have increasing applications in many other probabilistic models in IR. [sent-183, score-0.063]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('queries', 0.421), ('documents', 0.356), ('document', 0.284), ('retrieval', 0.256), ('hoc', 0.254), ('query', 0.253), ('relevancy', 0.185), ('eld', 0.175), ('ad', 0.171), ('routing', 0.147), ('hyperparameters', 0.134), ('rba', 0.133), ('hyperparameter', 0.118), ('lsi', 0.111), ('hong', 0.098), ('precision', 0.091), ('kong', 0.084), ('cavity', 0.084), ('bodoff', 0.08), ('buckley', 0.08), ('cisi', 0.08), ('cran', 0.08), ('fuhr', 0.08), ('ab', 0.07), ('hypersphere', 0.069), ('wong', 0.069), ('exact', 0.069), ('relevance', 0.064), ('estimation', 0.063), ('mf', 0.059), ('bartell', 0.053), ('estimations', 0.053), ('rocchio', 0.053), ('salton', 0.053), ('systemwide', 0.053), ('similarities', 0.052), ('true', 0.05), ('saddle', 0.049), ('dimension', 0.048), ('feedback', 0.046), ('assessments', 0.046), ('indexing', 0.044), ('reduced', 0.044), ('water', 0.042), ('hypothesize', 0.042), ('precisions', 0.042), ('tedious', 0.042), ('recall', 0.041), ('probabilistic', 0.04), ('bay', 0.039), ('qg', 0.039), ('semantic', 0.039), ('uni', 0.039), ('meaning', 0.037), ('internet', 0.037), ('uctuations', 0.035), ('searches', 0.035), ('fast', 0.034), ('ir', 0.034), ('rescaling', 0.034), ('iterative', 0.034), ('user', 0.033), ('ranked', 0.032), ('spherical', 0.032), ('relation', 0.031), ('average', 0.029), ('parametric', 0.029), ('iteration', 0.029), ('pa', 0.029), ('equations', 0.029), ('peaked', 0.028), ('versus', 0.028), ('benchmark', 0.028), ('representations', 0.027), ('vectors', 0.027), ('latent', 0.027), ('american', 0.026), ('yielding', 0.026), ('dietterich', 0.026), ('increasingly', 0.025), ('model', 0.025), ('baseline', 0.025), ('ones', 0.024), ('technology', 0.023), ('multidimensional', 0.023), ('acm', 0.023), ('mcgill', 0.023), ('cosines', 0.023), ('nocedal', 0.023), ('facilitating', 0.023), ('fq', 0.023), ('byproduct', 0.023), ('anticipate', 0.023), ('cliffs', 0.023), ('pertain', 0.023), ('especially', 0.023), ('wu', 0.023), ('physics', 0.023), ('heuristics', 0.022), ('integration', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="143-tfidf-1" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>2 0.29350895 <a title="143-tfidf-2" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>3 0.17321685 <a title="143-tfidf-3" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>4 0.16048867 <a title="143-tfidf-4" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>Author: Jaz Kandola, Nello Cristianini, John S. Shawe-taylor</p><p>Abstract: The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such similarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain positive results. 1</p><p>5 0.15344816 <a title="143-tfidf-5" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>6 0.15013053 <a title="143-tfidf-6" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>7 0.10812385 <a title="143-tfidf-7" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>8 0.099774688 <a title="143-tfidf-8" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>9 0.099037595 <a title="143-tfidf-9" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>10 0.077168472 <a title="143-tfidf-10" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>11 0.056474924 <a title="143-tfidf-11" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>12 0.056286078 <a title="143-tfidf-12" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>13 0.054677814 <a title="143-tfidf-13" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>14 0.051951267 <a title="143-tfidf-14" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>15 0.04713922 <a title="143-tfidf-15" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>16 0.041522969 <a title="143-tfidf-16" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>17 0.04031134 <a title="143-tfidf-17" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>18 0.038601715 <a title="143-tfidf-18" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>19 0.038036808 <a title="143-tfidf-19" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>20 0.038024586 <a title="143-tfidf-20" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.15), (1, -0.046), (2, 0.015), (3, -0.005), (4, -0.248), (5, 0.09), (6, -0.098), (7, -0.191), (8, 0.077), (9, -0.163), (10, -0.285), (11, -0.085), (12, 0.039), (13, -0.086), (14, 0.123), (15, 0.007), (16, -0.03), (17, -0.055), (18, 0.009), (19, 0.065), (20, 0.048), (21, 0.11), (22, 0.029), (23, -0.015), (24, -0.059), (25, 0.121), (26, 0.042), (27, 0.107), (28, 0.009), (29, -0.057), (30, -0.052), (31, -0.013), (32, -0.035), (33, 0.001), (34, -0.039), (35, 0.002), (36, -0.177), (37, 0.069), (38, 0.063), (39, 0.064), (40, -0.007), (41, -0.044), (42, 0.038), (43, -0.01), (44, -0.072), (45, -0.076), (46, 0.056), (47, -0.001), (48, 0.018), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9669463 <a title="143-lsi-1" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>2 0.80002189 <a title="143-lsi-2" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>3 0.78109479 <a title="143-lsi-3" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>4 0.65753168 <a title="143-lsi-4" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>5 0.65375453 <a title="143-lsi-5" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>6 0.61671436 <a title="143-lsi-6" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>7 0.45664233 <a title="143-lsi-7" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>8 0.4312208 <a title="143-lsi-8" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>9 0.4065958 <a title="143-lsi-9" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>10 0.36292163 <a title="143-lsi-10" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>11 0.28478649 <a title="143-lsi-11" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>12 0.28335235 <a title="143-lsi-12" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>13 0.27966967 <a title="143-lsi-13" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>14 0.27801943 <a title="143-lsi-14" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>15 0.21008942 <a title="143-lsi-15" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>16 0.20088649 <a title="143-lsi-16" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>17 0.19685891 <a title="143-lsi-17" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>18 0.1948387 <a title="143-lsi-18" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>19 0.19381242 <a title="143-lsi-19" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>20 0.18611427 <a title="143-lsi-20" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.021), (14, 0.019), (23, 0.019), (42, 0.133), (48, 0.321), (54, 0.108), (55, 0.042), (57, 0.013), (67, 0.012), (68, 0.022), (74, 0.073), (92, 0.04), (98, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79325259 <a title="143-lda-1" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>2 0.54338938 <a title="143-lda-2" href="./nips-2002-Manifold_Parzen_Windows.html">138 nips-2002-Manifold Parzen Windows</a></p>
<p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: The similarity between objects is a fundamental element of many learning algorithms. Most non-parametric methods take this similarity to be ﬁxed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices. Experiments in density estimation show signiﬁcant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classiﬁers, yielding classiﬁcation rates similar to SVMs and much superior to the Parzen classiﬁer.</p><p>3 0.5391258 <a title="143-lda-3" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>Author: Saharon Rosset, Eran Segal</p><p>Abstract: Several authors have suggested viewing boosting as a gradient descent search for a good ﬁt in function space. We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation. We show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well. We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models.</p><p>4 0.53376031 <a title="143-lda-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.53325862 <a title="143-lda-5" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>6 0.53291392 <a title="143-lda-6" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>7 0.53289002 <a title="143-lda-7" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>8 0.53130186 <a title="143-lda-8" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>9 0.53105658 <a title="143-lda-9" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>10 0.53001279 <a title="143-lda-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.52730507 <a title="143-lda-11" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>12 0.52213246 <a title="143-lda-12" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>13 0.51967704 <a title="143-lda-13" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>14 0.5185203 <a title="143-lda-14" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>15 0.51840371 <a title="143-lda-15" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>16 0.5181489 <a title="143-lda-16" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>17 0.51718128 <a title="143-lda-17" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>18 0.51712143 <a title="143-lda-18" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>19 0.51669854 <a title="143-lda-19" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>20 0.51523966 <a title="143-lda-20" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
