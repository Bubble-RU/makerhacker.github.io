<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-100" href="#">nips2002-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</h1>
<br/><p>Source: <a title="nips-2002-100-pdf" href="http://papers.nips.cc/paper/2183-half-lives-of-eigenflows-for-spectral-clustering.pdf">pdf</a></p><p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>Reference: <a title="nips-2002-100-reference" href="../nips2002_reference/nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  Abstract Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. [sent-4, score-0.373]
</p><p>2 The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. [sent-5, score-0.215]
</p><p>3 An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. [sent-6, score-0.145]
</p><p>4 The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. [sent-8, score-0.545]
</p><p>5 We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion. [sent-9, score-0.217]
</p><p>6 datapoints) in each cluster should be connected with high-afﬁnity edges, while different clusters are either not connected or are connnected only by a few edges with low afﬁnity. [sent-13, score-0.38]
</p><p>7 The practical problem is to identify these tightly coupled clusters, and cut the inter-cluster edges. [sent-14, score-0.179]
</p><p>8 Here we use the random walk formulation of [4], where the edge weights are used to construct a Markov deﬁnes a random walk on the graph to transition probability matrix, . [sent-16, score-0.375]
</p><p>9 The eigenvalues and eigenvectors of provide the basis for deciding on a particular segmentation. [sent-18, score-0.189]
</p><p>10 In particular, it has been shown that for weakly coupled clusters, the leading eigenvectors of will be roughly piecewise constant [4, 13, 5]. [sent-19, score-0.377]
</p><p>11 This result motivates many of the current spectral clustering algorithms. [sent-20, score-0.162]
</p><p>12 For example in [5], the number of clusters must be known a priori, and the -means algorithm is used on the leading eigenvectors of in an attempt to identify the appropriate piecewise constant regions. [sent-21, score-0.445]
</p><p>13 ¢  ¢  ¢  £  ¢  £  £  £  ¢  £  In this paper we investigate the form of the leading eigenvectors of the Markov matrix . [sent-22, score-0.244]
</p><p>14 Using some simple image segmentation examples we conﬁrm that the leading eigenvectors of are roughly piecewise constant for problems with well separated clusters. [sent-23, score-0.396]
</p><p>15 However, we observe that for several segmentation problems that we might wish to solve, the coupling between the clusters is signiﬁcantly stronger and, as a result, the piecewise constant approximation breaks down. [sent-24, score-0.288]
</p><p>16 ¢  ¢  Unlike the piecewise constant approximation, a perfectly general view is that the eigenvectors of determine particular ﬂows of probability along the edges in the graph. [sent-25, score-0.411]
</p><p>17 Instead of measuring the decay rate in terms of the eigenvalue , we ﬁnd it more convenient to use the ﬂow’s halﬂife , which is simply deﬁned by . [sent-27, score-0.124]
</p><p>18 ¢     ¡  §  ©§ ¥ ¢ ¨¦¤ £ ¢     ¡     From the perspective of eigenﬂows, a graph representing a set of weakly coupled clusters produces eigenﬂows between the various clusters which decay with long halﬂives. [sent-30, score-0.445]
</p><p>19 In order to identify clusters we therefore consider the eigenﬂows with long halﬂives. [sent-32, score-0.16]
</p><p>20 Given such a slowly decaying eigenﬂow, we identify particular bottleneck regions in the graph which critically restrict the ﬂow (cf. [sent-33, score-0.229]
</p><p>21 To identify these bottlenecks we propose computing the sensitivity of the ﬂow’s halﬂife with respect to perturbations in the edge weights. [sent-35, score-0.447]
</p><p>22 We implement a simple spectral graph partitioning algorithm which is based on these ideas. [sent-36, score-0.193]
</p><p>23 We ﬁrst compute the eigenvectors for the Markov transition matrix, and select those with long halﬂives. [sent-37, score-0.259]
</p><p>24 For each such eigenvector, we identify bottlenecks by computing the sensitivity of the ﬂow’s halﬂife with respect to perturbations in the edge weights. [sent-38, score-0.447]
</p><p>25 In the current algorithm, we simply select one of these eigenvectors in which a bottleneck has been identiﬁed, and cut edges within the bottleneck. [sent-39, score-0.517]
</p><p>26 The algorithm recomputes the eigenvectors and eigenvalues for the modiﬁed graph, and continues this iterative process until no further edges are cut. [sent-40, score-0.339]
</p><p>27 2 From Afﬁnities to Markov Chains Following the formulation in [4], we consider an undirected graph with vertices , for , and edges with non-negative weights . [sent-41, score-0.343]
</p><p>28 The edge afﬁnities are assumed to be symmetric, . [sent-43, score-0.117]
</p><p>29 A Markov chain is deﬁned using these afﬁnities by setting the transition that is, probability from vertex to vertex to be proportional to the edge afﬁnity, . [sent-44, score-0.458]
</p><p>30 In matrix notation, the afﬁnities are represented by a symmetric matrix , with elements , and the transition probability matrix is given by    ) 1 0 '  ) ( 0 '  )    $! [sent-46, score-0.283]
</p><p>31 (1)  deﬁnes the random walk of a particle on the graph This transition probability matrix . [sent-54, score-0.323]
</p><p>32 Suppose the initial probability of the particle being at vertex is , for . [sent-55, score-0.196]
</p><p>33 Then, the probability of the particle being initially at vertex and taking edge is . [sent-56, score-0.313]
</p><p>34 In matrix notation, the probability of the particle ending up any of the vertices after one step is given by the distribution , where . [sent-57, score-0.226]
</p><p>35 The matrix therefore has the same spectrum as and any eigenvector of must correspond to an eigenvector of with the same eigenvalue. [sent-66, score-0.274]
</p><p>36 Note that , and therefore is a symmetric matrix since is symmetric while is diagonal. [sent-67, score-0.16]
</p><p>37 ¢  ¢  c  ¢  ¢  ¢    The advantage of considering the matrix over is that the symmetric eigenvalue problem is more stable to small perturbations, and is computationally much more tractable. [sent-68, score-0.205]
</p><p>38 Since the matrix is symmetric, it has an orthogonal decomposition of the form: (2) ¢   e    ¥ fdF    (a)  (b)  (c)  (d)  (e)  Figure 1: (a-c) Three random images each having an occluder in front of a textured background. [sent-69, score-0.095]
</p><p>39 ¢  x  § Y¥ G      hC"##vsh B     C#"#    B  g ¥ x x  £ x £   ¡    are the eigenvectors and is a diagonal matrix of eigenvalsorted in decreasing order. [sent-71, score-0.22]
</p><p>40 While the eigenvectors have unit length, , the eigenvalues are real and have an absolute value bounded by 1, . [sent-72, score-0.223]
</p><p>41   ¤ ¥  § ¦ ¢  ¢   G  ¢  where ues    The eigenvector representation provides a simple way to capture the Markovian relaxation process [12]. [sent-73, score-0.14]
</p><p>42 The , can be represented as: transition matrix after iterations, namely  ¡   c ! [sent-75, score-0.116]
</p><p>43 As , the Markov chain approaches the stationary distribution , . [sent-77, score-0.095]
</p><p>44 Assuming the graph is connected with edges having non-zero weights, it is convenient to interpret the Markovian relaxation process as perturbations to the stationary distribution, , where is associated with the stationary distribution and . [sent-78, score-0.465]
</p><p>45 By the deﬁnition of the Markov chain, recall that the probability of making the transition from vertex to is the probability of starting in vertex , times the given that the particle is at vertex , namely conditional probability of taking edge . [sent-80, score-0.597]
</p><p>46 The net ﬂow of probability mass along edge from to is therefore the difference . [sent-82, score-0.168]
</p><p>47 It then follows that the net ﬂow of probability mass from vertex to is given by , where is the -element of the matrix     )     )     ) ( & '  ) 5  r) ' sq ) 1 3    R pR S  (4)  ! [sent-83, score-0.222]
</p><p>48 Therefore, the ﬂow is caused by the eigenvectors with , and hence we analyze the rate of decay of these eigenﬂows . [sent-95, score-0.203]
</p><p>49 ) 0S )  ('S ('§ §  (a)  (b)  (c)  Figure 2: (a) Eigenmode (b) corresponding eigenﬂow (c) gray value at each pixel corresponds to the maximum of the absolute sensitivities of all the weights on edges connected to a pixel (not including itself). [sent-104, score-0.426]
</p><p>50 A graph clustering problem is formed where each pixel in a test image is associated with a vertex of the graph . [sent-106, score-0.438]
</p><p>51 The edges in are deﬁned by the standard 8-neighbourhood of each pixel (with pixels at the edges and corners of the image only having 5 and 3 neighbours, respectively). [sent-107, score-0.426]
</p><p>52 The edge weight between neighbouring vertices and is given by the afﬁnity , where is the test image brightness and is a grey-level standard deviation. [sent-108, score-0.267]
</p><p>53 We use , where is the median at pixel absolute difference of gray levels between all neighbouring pixels and . [sent-109, score-0.147]
</p><p>54 §    ¡    ©  X ¤¢  ¨ ¨ ¦  a  x ¨ §¦X  ¥£¡¥ ) (' x 0  a   6©  wa )  X a X x  ©      This generative process provides an ensemble of clustering problems which we feel are representative of the structure of typical image segmentation problems. [sent-111, score-0.197]
</p><p>55 This latter property ensures that there are some edges with signiﬁcant weights between the two clusters in the graph associated with the foreground and background pixels. [sent-115, score-0.449]
</p><p>56 In Figure 2 we plot one eigenvector, , of the matrix Notice that the displayed eigenmode is not in general piecewise constant. [sent-117, score-0.325]
</p><p>57 Rather, the eigenvector is more like vibrational mode of a non-uniform membrane (in fact, they can be modeled in precisely that way). [sent-118, score-0.146]
</p><p>58 Also, for all but the stationary distribution, there is a signiﬁcant net ﬂow between neighbours, especially in regions where the magnitude of the spatial gradient of the eigenmode is larger. [sent-119, score-0.266]
</p><p>59 a u X x  ¢    x    4 Perturbation Analysis of EigenFlows As discussed in the introduction, we seek to identify bottlenecks in the eigenﬂows associated with long halﬂives. [sent-121, score-0.174]
</p><p>60 This notion of identifying bottlenecks is similar to the well-known max-ﬂow, min-cut theorem. [sent-122, score-0.131]
</p><p>61 In particular, for a graph whose edge weights represent maximum ﬂow capacities between pairs of vertices, instead of the current conditional transition probabilities, the bottleneck edges can be identiﬁed as precisely those edges across which the maximum ﬂow is equal to their maximum capacity. [sent-123, score-0.681]
</p><p>62 However, in the Markov framework, the ﬂow of probability across an edge is only maximal in the extreme cases for which the initial probability of being at one of the edge’s endpoints is equal to one, and zero at the other endpoint. [sent-124, score-0.14]
</p><p>63 Instead, we show that the desired bottleneck edges can be conveniently identiﬁed by considering the sensitivity of the ﬂow’s halﬂife to perturbations of the edge weights (see Fig. [sent-126, score-0.528]
</p><p>64 Intuitively, this sensitivity arises because the ﬂow across a bottleneck will have fewer alternative routes to take and therefore will be particularly sensitive to changes in the edge weights within the bottleneck. [sent-128, score-0.359]
</p><p>65 In comparison, the ﬂow between two vertices in a strongly coupled cluster will have many alternative routes and therefore will not be particularly  sensitive on the precise weight of any single edge. [sent-129, score-0.195]
</p><p>66 r ¡  ¥ a 1X ¡     a ¢£  hX ¢ x  ar ¡  ¡    ¡ (X  ¤ ¢ ¥£¡  Suppose we have an eigenvector of , with eigenvalue . [sent-133, score-0.167]
</p><p>67 In Figure 2, for a given eigenvector and its ﬂow, we plot the maximum of absolute sensitivities of all the weights on edges connected to a pixel (not including itself). [sent-138, score-0.49]
</p><p>68 Note that the sensitivities are large in the bottlenecks at the border of the foreground and background. [sent-139, score-0.318]
</p><p>69 a)8 8 5%%X  Here  (5)  a t X   5  E IGEN C UTS : A Basic Clustering Algorithm We select a simple clustering algorithm to test our proposal of using the derivative of the eigenmode’s halﬂife for identifying bottleneck edges. [sent-140, score-0.2]
</p><p>70 Set , and set a scale factor to be the median of Form the symmetric matrix . [sent-146, score-0.109]
</p><p>71 For each eigenvector of with halﬂife , compute the halﬂife sensitivities, for each edge in the graph. [sent-152, score-0.225]
</p><p>72 That is, suppress if there is a strictly more negative value or for some vertex the sensitivity in the neighbourhood of , or some in the neighbourhood of . [sent-156, score-0.275]
</p><p>73 for which  x    over all non-suppressed edges  4 F2  of . [sent-163, score-0.15]
</p><p>74 {  © ¥ §  }   |  '  r h| ¡  ~  a 1X ¡     r ¡  r %| ¡  z @  §   ¡  In step 5 we perform a non-maximal suppression on the sensitivities for the eigenvector. [sent-171, score-0.119]
</p><p>75 We have observed that at strong borders the computed sensitivities can be less than in a band along the border few pixels thick. [sent-172, score-0.156]
</p><p>76   g  ~  In step 6 we wish to select one particular eigenmode to base the edge cutting on at this iteration. [sent-175, score-0.38]
</p><p>77 The reason for not considering all the modes simultaneously is that we have found the locations of the cuts can vary by a few pixels for different modes. [sent-176, score-0.131]
</p><p>78 If nearby edges are cut as a result of different eigenmodes, then small isolated fragments can result in the ﬁnal clustering. [sent-177, score-0.267]
</p><p>79 Therefore we wish to select just one eigenmode to base cuts on each iteration. [sent-178, score-0.259]
</p><p>80 The particular eigenmode selected can, of course, vary from one iteration to the next. [sent-179, score-0.194]
</p><p>81 That is, we compute , where is the change of afﬁnities for any edge left to otherwise. [sent-181, score-0.117]
</p><p>82 When the process does terminate, the selected succession of cuts provides a modiﬁed afﬁnity matrix which has well separated clusters. [sent-184, score-0.11]
</p><p>83 For the ﬁnal clustering result, we can use either a connected components algorithm or the -means algorithm of [5] with set to the number of modes having large halﬂives. [sent-185, score-0.159]
</p><p>84 ¨ V  £  £  6 Experiments We compare the quality of E IGEN C UTS with two other methods: a -means based spectral clustering algorithm of [5] and an efﬁcient segmentation algorithm proposed in [1] based on a pairwise region comparison function. [sent-186, score-0.234]
</p><p>85 Our strategy was to select thresholds that are likely to generate a small number of stable partitions. [sent-187, score-0.106]
</p><p>86 To allow for comparison with -means, we needed to determine the number of clusters a priori. [sent-189, score-0.117]
</p><p>87 We therefore set to be the same as the number of clusters that E IGEN C UTS generated. [sent-190, score-0.117]
</p><p>88 A crucial observation with E IGEN C UTS is that, although the number of clusters changed slightly with a change in , the regions they deﬁned were qualitatively preserved across the thresholds and corresponded to a naive observer’s intuitive segmentation of the image. [sent-194, score-0.275]
</p><p>89 The performance on the eye images is also interesting in that the largely uniform regions around the center of the eye remain as part of one cluster. [sent-196, score-0.093]
</p><p>90 r ¡  In comparison, both the -means algorithm and the image segmentation algorithm of [1] (rows 3-6 in Fig. [sent-197, score-0.111]
</p><p>91 £  7 Discussion We have demonstrated that the common piecewise constant approximation to eigenvectors arising in spectral clustering problems limits the applicability of previous methods to situations in which the clusters are only relatively weakly coupled. [sent-199, score-0.587]
</p><p>92 We have proposed a new edge cutting criterion which avoids this piecewise constant approximation. [sent-200, score-0.272]
</p><p>93 Bottleneck edges between distinct clusters are identiﬁed through the observed sensitivity of an eigenﬂow’s halﬂife on changes in the edges’ afﬁnity weights. [sent-201, score-0.355]
</p><p>94 The basic algorithm we propose is computationally demanding in that the eigenvectors of the Markov matrix must be recomputed after each iteration of edge cutting. [sent-202, score-0.363]
</p><p>95 However, the point of this algorithm is to simply demonstrate the partitioning that can be achieved through the computation of the sensitivity of eigenﬂow halﬂives to changes in edge weights. [sent-203, score-0.244]
</p><p>96 More efﬁcient updates of the eigenvalue computation, taking advantage of low-rank changes in the matrix from one iteration to the next, or a multi-scale technique, are important areas for further study. [sent-204, score-0.143]
</p><p>97 Pairs of rows correspond to results from applying: E IGEN C UTS with and (Rows 1&2), -Means spectral clustering where , the number of clusters, is determined by the results of E IGEN C UTS (Rows 3&4) and Falsenszwalb & Huttenlocher (Rows 5&6). [sent-207, score-0.206]
</p><p>98 Longuet-Higgins Feature grouping by relocalization of eigenvectors of the proximity matrix. [sent-264, score-0.162]
</p><p>99 Appendix We compute the derivative of the log of half-life of an eigenvalue with respect to an element of the afﬁnity matrix . [sent-280, score-0.117]
</p><p>100 So eigenvectors with half-lives smaller than are effectively ignored. [sent-285, score-0.162]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hal', 0.486), ('ife', 0.281), ('ow', 0.269), ('eigen', 0.222), ('ccd', 0.195), ('wcd', 0.193), ('eigenmode', 0.168), ('eigenvectors', 0.162), ('af', 0.158), ('edges', 0.15), ('igen', 0.15), ('bottlenecks', 0.131), ('ows', 0.125), ('edge', 0.117), ('clusters', 0.117), ('uts', 0.114), ('nities', 0.114), ('vertex', 0.113), ('nity', 0.11), ('eigenvector', 0.108), ('piecewise', 0.099), ('cut', 0.091), ('sensitivity', 0.088), ('sensitivities', 0.087), ('clustering', 0.086), ('vertices', 0.085), ('particle', 0.083), ('graph', 0.078), ('spectral', 0.076), ('bottleneck', 0.075), ('ives', 0.075), ('foreground', 0.074), ('segmentation', 0.072), ('perturbations', 0.068), ('markov', 0.064), ('eigenvalue', 0.059), ('transition', 0.058), ('matrix', 0.058), ('chain', 0.057), ('cutting', 0.056), ('eigenflows', 0.056), ('cuts', 0.052), ('symmetric', 0.051), ('dq', 0.05), ('weakly', 0.047), ('walk', 0.046), ('coupled', 0.045), ('pixel', 0.044), ('rows', 0.044), ('identify', 0.043), ('pixels', 0.043), ('decay', 0.041), ('cluster', 0.039), ('select', 0.039), ('image', 0.039), ('partitioning', 0.039), ('stationary', 0.038), ('mode', 0.038), ('gxfp', 0.037), ('huttenlocher', 0.037), ('occluder', 0.037), ('wbe', 0.037), ('zheng', 0.037), ('neighbourhood', 0.037), ('connected', 0.037), ('stable', 0.037), ('modes', 0.036), ('wb', 0.036), ('diag', 0.034), ('absolute', 0.034), ('regions', 0.033), ('eigenmodes', 0.033), ('identi', 0.032), ('suppression', 0.032), ('relaxation', 0.032), ('ng', 0.03), ('thresholds', 0.03), ('eye', 0.03), ('vx', 0.03), ('weights', 0.03), ('qx', 0.03), ('sw', 0.028), ('shi', 0.028), ('markovian', 0.028), ('eigenvalues', 0.027), ('notice', 0.027), ('net', 0.027), ('routes', 0.026), ('terminate', 0.026), ('neighbouring', 0.026), ('neighbours', 0.026), ('fragments', 0.026), ('border', 0.026), ('iteration', 0.026), ('leading', 0.024), ('mass', 0.024), ('convenient', 0.024), ('across', 0.023), ('characterized', 0.023), ('weiss', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="100-tfidf-1" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>2 0.16504554 <a title="100-tfidf-2" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>3 0.10953733 <a title="100-tfidf-3" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>4 0.10843387 <a title="100-tfidf-4" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>5 0.093521543 <a title="100-tfidf-5" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>6 0.087735668 <a title="100-tfidf-6" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>7 0.081577592 <a title="100-tfidf-7" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>8 0.070304856 <a title="100-tfidf-8" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>9 0.065546028 <a title="100-tfidf-9" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>10 0.058700748 <a title="100-tfidf-10" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>11 0.057190023 <a title="100-tfidf-11" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>12 0.056794997 <a title="100-tfidf-12" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>13 0.056746706 <a title="100-tfidf-13" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>14 0.054138783 <a title="100-tfidf-14" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>15 0.053887565 <a title="100-tfidf-15" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>16 0.053716827 <a title="100-tfidf-16" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>17 0.051592559 <a title="100-tfidf-17" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>18 0.051453628 <a title="100-tfidf-18" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>19 0.049028337 <a title="100-tfidf-19" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>20 0.047602233 <a title="100-tfidf-20" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, -0.018), (2, -0.019), (3, 0.108), (4, -0.068), (5, 0.026), (6, 0.07), (7, -0.059), (8, -0.082), (9, 0.081), (10, 0.053), (11, 0.032), (12, -0.041), (13, 0.006), (14, -0.064), (15, -0.092), (16, -0.073), (17, -0.056), (18, -0.03), (19, -0.04), (20, -0.012), (21, 0.146), (22, 0.028), (23, 0.118), (24, 0.055), (25, -0.018), (26, -0.057), (27, 0.056), (28, -0.042), (29, -0.023), (30, 0.092), (31, -0.14), (32, 0.073), (33, 0.127), (34, 0.003), (35, -0.091), (36, 0.136), (37, -0.023), (38, -0.043), (39, -0.104), (40, -0.058), (41, 0.024), (42, -0.03), (43, -0.206), (44, -0.089), (45, 0.047), (46, 0.148), (47, -0.197), (48, 0.092), (49, -0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95716172 <a title="100-lsi-1" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>2 0.52866203 <a title="100-lsi-2" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>3 0.45247394 <a title="100-lsi-3" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>4 0.45164075 <a title="100-lsi-4" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>5 0.43348339 <a title="100-lsi-5" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>6 0.3744919 <a title="100-lsi-6" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>7 0.36802253 <a title="100-lsi-7" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>8 0.32596457 <a title="100-lsi-8" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>9 0.32271528 <a title="100-lsi-9" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>10 0.31205818 <a title="100-lsi-10" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>11 0.30829632 <a title="100-lsi-11" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>12 0.29493141 <a title="100-lsi-12" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>13 0.28884715 <a title="100-lsi-13" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>14 0.27967209 <a title="100-lsi-14" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>15 0.2638756 <a title="100-lsi-15" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>16 0.25915849 <a title="100-lsi-16" href="./nips-2002-Manifold_Parzen_Windows.html">138 nips-2002-Manifold Parzen Windows</a></p>
<p>17 0.25775191 <a title="100-lsi-17" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>18 0.25491321 <a title="100-lsi-18" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>19 0.24920483 <a title="100-lsi-19" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>20 0.24629287 <a title="100-lsi-20" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.044), (14, 0.06), (40, 0.277), (42, 0.081), (54, 0.091), (55, 0.033), (67, 0.026), (68, 0.025), (74, 0.131), (87, 0.011), (92, 0.025), (98, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87171042 <a title="100-lda-1" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>same-paper 2 0.80534738 <a title="100-lda-2" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>3 0.56520623 <a title="100-lda-3" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The problem of extracting the relevant aspects of data, in face of multiple conﬂicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.</p><p>4 0.56013572 <a title="100-lda-4" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>Author: David A. Ross, Richard S. Zemel</p><p>Abstract: We propose a model that can learn parts-based representations of highdimensional data. Our key assumption is that the dimensions of the data can be separated into several disjoint subsets, or factors, which take on values independently of each other. We assume each factor has a small number of discrete states, and model it using a vector quantizer. The selected states of each factor represent the multiple causes of the input. Given a set of training examples, our model learns the association of data dimensions with factors, as well as the states of each VQ. Inference and learning are carried out efﬁciently via variational algorithms. We present applications of this model to problems in image decomposition, collaborative ﬁltering, and text classiﬁcation.</p><p>5 0.54394507 <a title="100-lda-5" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>6 0.54295331 <a title="100-lda-6" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>7 0.54254174 <a title="100-lda-7" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>8 0.53974271 <a title="100-lda-8" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>9 0.53776509 <a title="100-lda-9" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>10 0.53452748 <a title="100-lda-10" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>11 0.53439784 <a title="100-lda-11" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>12 0.5343383 <a title="100-lda-12" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>13 0.53366995 <a title="100-lda-13" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>14 0.53317684 <a title="100-lda-14" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>15 0.53125668 <a title="100-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.52887839 <a title="100-lda-16" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>17 0.52625495 <a title="100-lda-17" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>18 0.5260846 <a title="100-lda-18" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>19 0.52495825 <a title="100-lda-19" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>20 0.52220821 <a title="100-lda-20" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
