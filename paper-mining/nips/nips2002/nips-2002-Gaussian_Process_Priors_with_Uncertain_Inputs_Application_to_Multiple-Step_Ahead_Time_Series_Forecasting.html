<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-95" href="#">nips2002-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</h1>
<br/><p>Source: <a title="nips-2002-95-pdf" href="http://papers.nips.cc/paper/2313-gaussian-process-priors-with-uncertain-inputs-application-to-multiple-step-ahead-time-series-forecasting.pdf">pdf</a></p><p>Author: Agathe Girard, Carl Edward Rasmussen, Joaquin Quiñonero Candela, Roderick Murray-Smith</p><p>Abstract: We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.   ¡ % # ¢ ¡     ¢ ¡¨ ¦ ¤ ¢ $</p><p>Reference: <a title="nips-2002-95-reference" href="../nips2002_reference/nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. [sent-12, score-1.098]
</p><p>2 -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. [sent-13, score-1.713]
</p><p>3 For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. [sent-14, score-0.199]
</p><p>4 In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction. [sent-15, score-0.278]
</p><p>5  ©§¥£¡     ' (&  1 Introduction One of the main objectives in time series analysis is forecasting and in many real life problems, one has to predict ahead in time, up to a certain time horizon (sometimes called lead time or prediction horizon). [sent-17, score-1.373]
</p><p>6 Furthermore, knowledge of the uncertainty of the prediction is important. [sent-18, score-0.282]
</p><p>7 Currently, the multiple-step ahead prediction task is achieved by either explic-     itly training a direct model to predict steps ahead, or by doing repeated one-step ahead predictions up to the desired horizon, which we call the iterative method. [sent-19, score-2.02]
</p><p>8 There are a number of reasons why the iterative method might be preferred to the ‘direct’ one. [sent-20, score-0.091]
</p><p>9 Firstly, the direct method makes predictions for a ﬁxed horizon only, making it computationally demanding if one is interested in different horizons. [sent-21, score-0.233]
</p><p>10 Furthermore, the larger , the more training data we need in order to achieve a good predictive performance, because . [sent-22, score-0.13]
</p><p>11 On the other hand, the iterated of the larger number of ‘missing’ data between and method provides any -step ahead forecast, up to the desired horizon, as well as the joint probability distribution of the predicted points. [sent-23, score-0.939]
</p><p>12 '   &  &     In the Gaussian process modelling approach, one computes predictive distributions whose means serve as output estimates. [sent-24, score-0.284]
</p><p>13 Gaussian processes (GPs) for regression have historically been ﬁrst introduced by O’Hagan [1] but started being a popular non-parametric modelling approach after the publication of [7]. [sent-25, score-0.141]
</p><p>14 In [10], it is shown that GPs can achieve a predictive performance comparable to (if not better than) other modelling approaches like neural networks or local learning methods. [sent-26, score-0.192]
</p><p>15 We will show that for a -step ahead prediction which ignores the accumulating prediction variance, the model is not conservative enough, with unrealistically small uncertainty attached to the forecast. [sent-27, score-1.319]
</p><p>16 An alternative solution is presented for iterative -step ahead prediction, with propagation of the prediction uncertainty. [sent-28, score-1.06]
</p><p>17 2 Gaussian Process modelling We brieﬂy recall some fundamentals of Gaussian processes. [sent-29, score-0.062]
</p><p>18 For a comprehensive introduction, please refer to [5], [11], or the more recent review [12]. [sent-30, score-0.023]
</p><p>19 1 The GP prior model Formally, the random function, or stochastic process, is a Gaussian process, with mean and covariance function , if its values at a ﬁnite number of points, , are seen as the components of a normally distributed random vector. [sent-32, score-0.114]
</p><p>20 If we further assume that the process is stationary: it has a constant mean and a covariance function only depending on the distance between the inputs . [sent-33, score-0.208]
</p><p>21    %  ¡©¦   ¨ % £¨ ¢       £      %    ¨   %  ¨   ©¦    %     £     ¤    ¨ ©¦    ¡  giving the covariance between the points and , which is a function of the inputs corresponding to the same cases and . [sent-35, score-0.149]
</p><p>22 A common choice of covariance function is the Gaussian kernel 1  % ¨   ©0  1  H  R  (2)    %  P 0 ¨  H  @  H   P  Q  ¤ "©¡©¦  % %% ¨  ¨  ¨ ©¦ ¨  ¥   )¡  ¤  ' % # (&$  % ¨  ¨ ©£©¦  ! [sent-36, score-0.068]
</p><p>23 ¨ "¥  %  ¨ ©¦  ¥   )¡  2  B @ CA9  I AGH  ¨    ¥ ¡  ¨  ¥   )£  with  (1)  ¤ %  7 5 3 864  E FD    ¨    ¨    0 ¥ ¡    ¤  where is the input dimension. [sent-37, score-0.028]
</p><p>24 The parameters (correlation length) allow a different distance measure for each input dimension . [sent-38, score-0.028]
</p><p>25 Q  H  S  Q  T  The role of the covariance function in the GP framework is similar to that of the kernels used in the Support Vector Machines community. [sent-40, score-0.068]
</p><p>26 It accounts for a high correlation between the outputs of cases with nearby inputs. [sent-42, score-0.058]
</p><p>27 ¦  1 This choice was motivated by the fact that, in [8], we were aiming at uniﬁed expressions for the GPs and the Relevance Vector Machines models which employ such a kernel. [sent-43, score-0.026]
</p><p>28 More discussion about possible covariance functions can be found in [5]. [sent-44, score-0.068]
</p><p>29 2 Predicting with Gaussian Processes I  ©¨£§¦¥ ¤ ¢   £ £¡  Given this prior on the function and a set of data , our aim, in this Bayesian setting, is to get the predictive distribution of the function value corresponding to a new (given) input . [sent-46, score-0.158]
</p><p>30    %   ¡    ¨ ©¦    £  ¤    0     ¦       If we assume an additive uncorrelated Gaussian white noise, with variance , relates the targets (observations) to the function outputs, the distribution over the targets is Gaussian, with zero mean and covariance matrix such that . [sent-47, score-0.323]
</p><p>31 8 Q         Q  ¤ §  % ¥  E  ¨ 1 (©  ¤ ¥¨ %  %  In this framework, for a new , the predictive distribution is simply obtained by conditioning on the training data. [sent-51, score-0.13]
</p><p>32 §  F %6 F    %     %     ¨ ©¦ ¨    £  1  where  ¨  %    £  ¤     If we now assume that the input distribution is Gaussian, distribution is now obtain by integrating over    £ P  ¨  ¨    ¡  1  3 Prediction at a random input  with uncer. [sent-54, score-0.056]
</p><p>33 %  %  % ¡©¦ ¨   ¨     The predictive mean serves as a point estimate of the function output, tainty . [sent-55, score-0.176]
</p><p>34 And it is also a point estimate for the target, , with variance  '  %  B  @  is the  (4)   %       where the new point and the training targets and (2). [sent-56, score-0.131]
</p><p>35 1 Gaussian approximation    F %6   Given that this integral is analytically intractable ( is a complicated function of ), we opt for an analytical Gaussian approximation and only compute the mean and variance of . [sent-58, score-0.245]
</p><p>36 A ﬁrst and second order Taylor expansions of and respectively, around , led to  %  (8)  y xx U U xx  v h  A v   (9)    I  %  ¨    £      I  %  ¨    ¡    '    I  ¨    £ P  %  ¨    ¡  %  %  B  ¨    ¡ P    P     D  '  %  ¨  ¨  P  % ¤  ¤  UVT #U¥A  T VUT #U¥A T   %  ! [sent-64, score-0.07]
</p><p>37 In [8], we derived the exact expressions of the ﬁrst and second moments. [sent-67, score-0.115]
</p><p>38    P 6 $ TU 6 Sh2'f £ T U ¥A #UT #U¥A T 4 C    A W VUT #U9T A %     £ ¤  7 85 3  '  £  ¨  ¤ §  4    £  '  where  W  and is the  In the same manner, we obtain for the variance  with     ¨    £  . [sent-74, score-0.081]
</p><p>39 2 Monte-Carlo alternative Equation (5) can be solved by performing a numerical approximation of the integral, using a simple Monte-Carlo approach:   %6  %     ¨ ¨ ©¦ (1    ¡  I G   ¢  B  B       %6   @ A    T  %  ¨ (1 %    £  . [sent-76, score-0.099]
</p><p>40 Consider the time series and the state-space model where is the state at time (we assume that the lag is known) and the (white) noise has variance . [sent-78, score-0.244]
</p><p>41 That way, only the output estimates are used and the uncertainty induced by each successive prediction is not accounted for. [sent-81, score-0.307]
</p><p>42 P     Using the results derived in the previous section, we suggest to formally incorporate the uncertainty information about the intermediate regressor. [sent-82, score-0.141]
</p><p>43 That is, as we predict ahead in time, we now view the lagged outputs as random variables. [sent-83, score-0.933]
</p><p>44 In this framework, the input    ¡&  at time is a random vector with mean formed by the predicted means of the lagged outputs , , given by (11). [sent-84, score-0.322]
</p><p>45 The input covariance matrix has the different predicted variances on its diagonal (with the estimated noise variance added to them), computed with (12), and the off-diagonal elements are given by, in the case of the exact solution, , where is as deﬁned previously and with . [sent-85, score-0.397]
</p><p>46 We use the second example, inspired from real-life problems, to show that iteratively predicting ahead in time without taking account of the uncertainties induced by each succesive prediction leads to inaccurate results, with unrealistically small error bars. [sent-89, score-1.112]
</p><p>47 1 Forecasting the Mackey-Glass time series H H  The Mackey-Glass chaotic time series constitutes a wellknown benchmark and a challenge for the multiple-step ahead prediction task, due to its strong non-linearity [4]: . [sent-93, score-1.18]
</p><p>48 We choose for the number of lagged outputs in the and the targets, , are corrupted by a state vector, white noise with variance . [sent-96, score-0.23]
</p><p>49 "'  ¤   @  &  % ¨  3 4   points, taken at random We train a GP model with a Gaussian kernel such as (2) on from a series of points. [sent-99, score-0.069]
</p><p>50 Figure 1 shows the mean predictions with their uncertainties, samples from the Monte-Carlo nugiven by the exact and approximate methods, and merical approximation, from to steps ahead, for different starting points. [sent-100, score-0.34]
</p><p>51 -step ahead mean predictions (left) and their uncerFigure 2 shows the plot of the tainties (right), given by the exact and approximate methods, as well as the sample mean and sample variance obtained with the numerical solution (average over points). [sent-101, score-1.243]
</p><p>52 & C&  & & & 66CD  & @E  E  B  ¤  & 6&  D  B     & C&  ¤     B  & 6E  These ﬁgures show the better performance of the exact method on the approximate one. [sent-102, score-0.154]
</p><p>53 Also, they allow us to validate the Gaussian approximation, noticing that the error bars encompass the samples from the true distribution. [sent-103, score-0.125]
</p><p>54 Table 1: Average (over test points) absolute error ( ), squared error ( ) and minus log predictive density ( ) of the -step ahead predictions obtained using the exact method ( ), the approximate one ( ) and the sampling from the true distribution ( ). [sent-105, score-1.226]
</p><p>55 From 1 to 100 steps ahead  From 1 to 100 steps ahead  3  2. [sent-107, score-1.658]
</p><p>56 5 100 250  260  270  280  290  300  310  320  330  340  350  B  −3  B  Figure 1: Iterative method in action: simulation from to steps ahead for different starting points in the test series. [sent-115, score-0.885]
</p><p>57 Mean predictions with error bars given by the exact (dash) and approximate (dot) methods. [sent-116, score-0.256]
</p><p>58 & C&  E  D  & 6E  100−step ahead predicted variances  100−step ahead predicted means  3. [sent-118, score-1.798]
</p><p>59 5  600  0 100  150  200  250  300  350  400  450  500  550  Figure 2: -step ahead mean predictions (left) and uncertainties (right. [sent-128, score-0.975]
</p><p>60 ) obtained using the exact method (dash), the approximate (dot) and the sample mean and variance of the numerical solution (dash-dot). [sent-129, score-0.345]
</p><p>61 2 Prediction of a pH process simulation     We now compare the iterative -step ahead prediction results obtained when propagating the uncertainty (using the approximate method) and when using the output estimates only (the naive approach). [sent-132, score-1.361]
</p><p>62 For doing so, we use the pH neutralisation process benchmark presented in [3]. [sent-133, score-0.126]
</p><p>63 The training and test data consist of pH values (outputs of the process) and a control input signal ( ). [sent-134, score-0.086]
</p><p>64 ¡     With a model of the form examples and consider a test set of  , we train our GP on points (all data have been normalized). [sent-135, score-0.032]
</p><p>65   ¥ ¢¢ ©¦  ¤ ¢ ¡  D  B  D D  B  Figure 3 shows the -step ahead predicted means and variances obtained when propagating the uncertainty and when using information on the past predicted means only. [sent-138, score-1.197]
</p><p>66 The losses calculated are the following: , and for the approximate method and for the naive one! [sent-139, score-0.169]
</p><p>67 &  D 6&  D  1    & (¤  )    P  G  E H&  &  1 ) B RB  ¤  P  P    &   ¤ D P  B  ¤  & D 66P    P  10−step ahead predicted means  2  2  true  1. [sent-140, score-0.92]
</p><p>68 5 10  20  30  40  50  60  70  80  70  80  10−step ahead predicted variances  5  10  Naive m +/− 2σ  2  0  10  0 −2  −5  10  −4  k=10  k=1 26  28  30  32  B  24  −10  34  10  10  20  30  40  50  60  B  −6 22  B  Figure 3: Predictions from to steps ahead (left). [sent-149, score-1.745]
</p><p>69 -step ahead mean predictions with the corresponding variances, when propagating the uncertainty (dot) and when using the previous point estimates only (dash). [sent-150, score-1.084]
</p><p>70 &  &  5 Conclusions We have presented a novel approach which allows us to use knowledge of the variance on inputs to Gaussian process models to achieve more realistic prediction variance in the case of noisy inputs. [sent-151, score-0.425]
</p><p>71 Iterating this approach allows us to use it as a method for efﬁcient propagation of uncertainty in the multi-step ahead prediction task of non-linear time-series. [sent-152, score-1.13]
</p><p>72 Note that this method is also useful in its own right in the case of noisy model inputs, assuming they have a Gaussian distribution. [sent-155, score-0.024]
</p><p>73 The authors gratefully acknowledge the support of the Multi-Agent Control Research Training Network - EC TMR grant HPRN-CT-1999-00107 and RM-S is grateful for EPSRC grant Modern statistical approaches to off-equilibrium modelling for nonlinear system control GR/M76379/01. [sent-157, score-0.12]
</p><p>74 (1994) Adaptive nonlinear control of a pH neutralisation process. [sent-172, score-0.111]
</p><p>75 (2002) Nonlinear adaptive control using non-parametric Gaussian process prior models. [sent-187, score-0.103]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ahead', 0.785), ('prediction', 0.169), ('predictive', 0.13), ('horizon', 0.118), ('forecasting', 0.115), ('vut', 0.115), ('uncertainty', 0.113), ('tu', 0.11), ('gaussian', 0.109), ('ut', 0.095), ('girard', 0.092), ('predictions', 0.091), ('exact', 0.089), ('variance', 0.081), ('candela', 0.08), ('glasgow', 0.079), ('predicted', 0.075), ('rasmussen', 0.074), ('dash', 0.069), ('series', 0.069), ('covariance', 0.068), ('iterative', 0.067), ('naive', 0.067), ('numerical', 0.064), ('lagged', 0.063), ('gps', 0.063), ('modelling', 0.062), ('vu', 0.058), ('control', 0.058), ('outputs', 0.058), ('processes', 0.056), ('variances', 0.056), ('agathe', 0.053), ('neutralisation', 0.053), ('onero', 0.053), ('unrealistically', 0.053), ('utt', 0.053), ('xxx', 0.053), ('uncertainties', 0.053), ('gp', 0.052), ('ph', 0.052), ('targets', 0.05), ('inputs', 0.049), ('propagating', 0.049), ('hagan', 0.046), ('qui', 0.046), ('mean', 0.046), ('process', 0.045), ('uncertain', 0.044), ('steps', 0.044), ('approx', 0.042), ('approximate', 0.041), ('propagation', 0.039), ('true', 0.038), ('denmark', 0.037), ('edward', 0.037), ('losses', 0.037), ('xx', 0.035), ('bars', 0.035), ('approximation', 0.035), ('lag', 0.034), ('dot', 0.033), ('points', 0.032), ('iterated', 0.031), ('time', 0.03), ('ignores', 0.03), ('samples', 0.029), ('law', 0.028), ('minus', 0.028), ('white', 0.028), ('input', 0.028), ('formally', 0.028), ('repeated', 0.028), ('benchmark', 0.028), ('predict', 0.027), ('expressions', 0.026), ('williams', 0.026), ('output', 0.025), ('desired', 0.024), ('analytical', 0.024), ('integral', 0.024), ('method', 0.024), ('fe', 0.023), ('congress', 0.023), ('epsrc', 0.023), ('hamilton', 0.023), ('ifac', 0.023), ('ireland', 0.023), ('maynooth', 0.023), ('please', 0.023), ('mike', 0.023), ('encompass', 0.023), ('hgf', 0.023), ('historically', 0.023), ('jqc', 0.023), ('mackey', 0.023), ('hg', 0.023), ('predicting', 0.022), ('machines', 0.022), ('means', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="95-tfidf-1" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>Author: Agathe Girard, Carl Edward Rasmussen, Joaquin Quiñonero Candela, Roderick Murray-Smith</p><p>Abstract: We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.   ¡ % # ¢ ¡     ¢ ¡¨ ¦ ¤ ¢ $</p><p>2 0.11937989 <a title="95-tfidf-2" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>Author: E. Solak, R. Murray-smith, W. E. Leithead, D. J. Leith, Carl E. Rasmussen</p><p>Abstract: Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.</p><p>3 0.1184185 <a title="95-tfidf-3" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>Author: Joaquin Quiñonero-candela, Ole Winther</p><p>Abstract: In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean ﬁeld approach to the intractable classiﬁcation model that is expected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 − 104 ) examples. The results indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.</p><p>4 0.10676968 <a title="95-tfidf-4" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>Author: Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We ﬁnd that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.</p><p>5 0.08761292 <a title="95-tfidf-5" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>6 0.080938891 <a title="95-tfidf-6" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>7 0.069015071 <a title="95-tfidf-7" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>8 0.064112023 <a title="95-tfidf-8" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>9 0.060301974 <a title="95-tfidf-9" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>10 0.05011759 <a title="95-tfidf-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.04991854 <a title="95-tfidf-11" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>12 0.049377833 <a title="95-tfidf-12" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>13 0.049132146 <a title="95-tfidf-13" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>14 0.044712905 <a title="95-tfidf-14" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>15 0.043528691 <a title="95-tfidf-15" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>16 0.042655081 <a title="95-tfidf-16" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>17 0.04264316 <a title="95-tfidf-17" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>18 0.041486461 <a title="95-tfidf-18" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>19 0.041121073 <a title="95-tfidf-19" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>20 0.040899906 <a title="95-tfidf-20" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, 0.005), (2, -0.024), (3, 0.008), (4, -0.034), (5, 0.002), (6, -0.104), (7, 0.074), (8, 0.046), (9, 0.046), (10, 0.009), (11, -0.043), (12, 0.147), (13, 0.041), (14, 0.052), (15, -0.023), (16, -0.062), (17, 0.01), (18, 0.09), (19, 0.03), (20, 0.138), (21, 0.036), (22, 0.05), (23, 0.086), (24, 0.033), (25, -0.132), (26, 0.111), (27, 0.069), (28, 0.014), (29, 0.071), (30, 0.163), (31, 0.129), (32, 0.1), (33, -0.032), (34, -0.022), (35, -0.01), (36, -0.173), (37, -0.077), (38, 0.036), (39, 0.003), (40, 0.182), (41, 0.022), (42, -0.069), (43, 0.078), (44, -0.089), (45, 0.008), (46, 0.036), (47, -0.018), (48, 0.089), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95894074 <a title="95-lsi-1" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>Author: Agathe Girard, Carl Edward Rasmussen, Joaquin Quiñonero Candela, Roderick Murray-Smith</p><p>Abstract: We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.   ¡ % # ¢ ¡     ¢ ¡¨ ¦ ¤ ¢ $</p><p>2 0.79384232 <a title="95-lsi-2" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>Author: E. Solak, R. Murray-smith, W. E. Leithead, D. J. Leith, Carl E. Rasmussen</p><p>Abstract: Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.</p><p>3 0.71808583 <a title="95-lsi-3" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp</p><p>Abstract: Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves signiﬁcantly better accuracy, yet at a higher computational cost.</p><p>4 0.61296844 <a title="95-lsi-4" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>Author: Joaquin Quiñonero-candela, Ole Winther</p><p>Abstract: In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean ﬁeld approach to the intractable classiﬁcation model that is expected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 − 104 ) examples. The results indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.</p><p>5 0.58376545 <a title="95-lsi-5" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>Author: Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We ﬁnd that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.</p><p>6 0.5344193 <a title="95-lsi-6" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>7 0.48118207 <a title="95-lsi-7" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>8 0.45947239 <a title="95-lsi-8" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>9 0.36318642 <a title="95-lsi-9" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>10 0.35168901 <a title="95-lsi-10" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>11 0.35071814 <a title="95-lsi-11" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>12 0.3429521 <a title="95-lsi-12" href="./nips-2002-Manifold_Parzen_Windows.html">138 nips-2002-Manifold Parzen Windows</a></p>
<p>13 0.33725658 <a title="95-lsi-13" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>14 0.32507843 <a title="95-lsi-14" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>15 0.32296392 <a title="95-lsi-15" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>16 0.31746671 <a title="95-lsi-16" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>17 0.31552845 <a title="95-lsi-17" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>18 0.30557546 <a title="95-lsi-18" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>19 0.29990843 <a title="95-lsi-19" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>20 0.298893 <a title="95-lsi-20" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.023), (37, 0.02), (42, 0.089), (51, 0.024), (54, 0.077), (55, 0.372), (57, 0.026), (67, 0.016), (68, 0.032), (74, 0.051), (92, 0.037), (98, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9473834 <a title="95-lda-1" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>same-paper 2 0.90766287 <a title="95-lda-2" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>Author: Agathe Girard, Carl Edward Rasmussen, Joaquin Quiñonero Candela, Roderick Murray-Smith</p><p>Abstract: We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.   ¡ % # ¢ ¡     ¢ ¡¨ ¦ ¤ ¢ $</p><p>3 0.8853417 <a title="95-lda-3" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>4 0.87466276 <a title="95-lda-4" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>5 0.73060739 <a title="95-lda-5" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>6 0.68768728 <a title="95-lda-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.68335849 <a title="95-lda-7" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>8 0.66590059 <a title="95-lda-8" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>9 0.64764607 <a title="95-lda-9" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>10 0.635423 <a title="95-lda-10" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>11 0.61752474 <a title="95-lda-11" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>12 0.61667871 <a title="95-lda-12" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>13 0.61163336 <a title="95-lda-13" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>14 0.60833669 <a title="95-lda-14" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>15 0.60147715 <a title="95-lda-15" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>16 0.60137933 <a title="95-lda-16" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>17 0.60027564 <a title="95-lda-17" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>18 0.60024148 <a title="95-lda-18" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>19 0.59953368 <a title="95-lda-19" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>20 0.59935981 <a title="95-lda-20" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
