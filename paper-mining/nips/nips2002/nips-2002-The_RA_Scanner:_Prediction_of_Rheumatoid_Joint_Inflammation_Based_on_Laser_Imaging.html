<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-196" href="#">nips2002-196</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</h1>
<br/><p>Source: <a title="nips-2002-196-pdf" href="http://papers.nips.cc/paper/2175-the-ra-scanner-prediction-of-rheumatoid-joint-inflammation-based-on-laser-imaging.pdf">pdf</a></p><p>Author: Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, Gerhard A. Müller</p><p>Abstract: We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various linear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably perform parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage.</p><p>Reference: <a title="nips-2002-196-reference" href="../nips2002_reference/nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. [sent-13, score-0.283]
</p><p>2 The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. [sent-14, score-0.614]
</p><p>3 Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. [sent-15, score-0.496]
</p><p>4 Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage. [sent-18, score-0.337]
</p><p>5 1 Introduction Rheumatoid arthritis (RA) is the most common inﬂammatory arthropathy with 1–2% of the population being affected. [sent-19, score-0.249]
</p><p>6 This chronic, mostly progressive disease often leads to early disability and joint deformities. [sent-20, score-0.15]
</p><p>7 Recent studies have convincingly shown that early treatment and therefore an early diagnosis is mandatory to prevent or at least delay joint destruction [2]. [sent-21, score-0.216]
</p><p>8 Unfortunately, long-term medication with disease modifying anti-rheumatic drugs (DMARDs) often acts very slowly on clinical parameters of inﬂammation, making it difﬁcult to ﬁnd the right drug for a patient within adequate time. [sent-22, score-0.095]
</p><p>9 Conventional radiology,  such as magnetic resonance imaging (MRI) and ultrasound, may provide information on soft tissue changes, yet these techniques are time-consuming and—in the case of MRI— costly. [sent-23, score-0.367]
</p><p>10 New imaging techniques for RA diagnosis should thus be non-invasive, of low cost, examiner independent and easy to use. [sent-24, score-0.219]
</p><p>11 Following recent experiments on absorption and scattering coefﬁcients of laser light in joint tissue [6], a prototype laser imaging technique was developed [7]. [sent-25, score-0.978]
</p><p>12 As part of the prototype development, it became necessary to analyze if the rheumatic status of a ﬁnger joint can be reliably classiﬁed on the basis of the laser images. [sent-26, score-0.495]
</p><p>13 Employing different linear and kernel-based classiﬁers, we will investigate the performance of the laser imaging technique to predict the status of the rheumatic joint inﬂammation. [sent-28, score-0.652]
</p><p>14 Provided that the accuracy of the overall system is sufﬁciently high, the imaging technique and the automatic inﬂammation classiﬁcation can be combined into a novel device that allows an inexpensive and objective assessment of inﬂammatory joint changes. [sent-29, score-0.297]
</p><p>15 2 we describe the RA scanner in more detail, as well as the process of data acquisition. [sent-32, score-0.309]
</p><p>16 2 The RA Scanner The rheumatoid arthritis (RA) scanner provides a new medical imaging technique, developed speciﬁcally for the diagnosis of RA in ﬁnger joints. [sent-41, score-1.002]
</p><p>17 The RA scanner [7] allows the in vivo trans-illumination of ﬁnger joints with laser light in the near infrared wavelength range. [sent-42, score-0.748]
</p><p>18 The scattered light distribution is detected by a camera and is used to assess the inﬂammatory status of the ﬁnger joint. [sent-43, score-0.192]
</p><p>19 Example images, taken from an inﬂamed joint and from a healthy control, are shown in Fig. [sent-44, score-0.119]
</p><p>20 Starting out from the laser images, image pre-processing is used to obtain a description of each laser image by nine numerical features. [sent-46, score-0.466]
</p><p>21 Furthermore for each ﬁnger joint examined, the circumference is measured using a conventional measuring tape. [sent-49, score-0.141]
</p><p>22 The nine image features plus the joint circumference make up the data that is used in the classiﬁcation step of the RA scanner to predict the inﬂammatory status of the joint. [sent-50, score-0.585]
</p><p>23 1 Data Acquisition One of the clinically important questions is to know as early as possible if a prescribed medication improves the state of rheumatoid arthritis. [sent-52, score-0.262]
</p><p>24 Therefore the goal of the classiﬁcation step in the RA scanner is to decide—based on features extracted from the laser images—if there was an improvement of arthritis activity or if the joint inﬂammation remained unchanged or worsened. [sent-53, score-0.927]
</p><p>25 The data for the development of the RA scanner stems from a study on 22 patients with rheumatoid arthritis. [sent-54, score-0.567]
</p><p>26 Data from 72 ﬁnger joints were used for the study. [sent-55, score-0.183]
</p><p>27 All of these 72 ﬁnger joints were examined at baseline and during a follow-up visit after a mean duration of 42 days. [sent-56, score-0.3]
</p><p>28 Earlier data from an additional 20 patients had to be discarded since experimental conditions were not controlled properly. [sent-57, score-0.067]
</p><p>29 Each joint was examined and the clinical arthritis activity was classiﬁed from 0 (inactive, not swollen, tender or warm) to 3 (very active) by a rheumatologist. [sent-58, score-0.408]
</p><p>30 The characteristics of joint tissue was recorded by the above described laser imaging technique. [sent-59, score-0.615]
</p><p>31 In a preprocess-  (a) Laser image of a healthy ﬁnger joint  (b) Laser image of an inﬂamed ﬁnger joint. [sent-60, score-0.119]
</p><p>32 The inﬂammation changes the joint tissue’s absorption coefﬁcient, giving a darker image. [sent-61, score-0.129]
</p><p>33 Figure 1: Two examples of the light distribution captured by the RA scanner. [sent-62, score-0.094]
</p><p>34 A laser beam is sent through the ﬁnger joint (the ﬁnger tip is to the right, the palm is on the left), the light distribution below the joint is captured by a CCD element. [sent-63, score-0.463]
</p><p>35 To calculate the features, ﬁrst a horizontal line near the vertical center of the ﬁnger joint is selected. [sent-64, score-0.091]
</p><p>36 The distribution of light intensity along that line is bell-shaped. [sent-65, score-0.098]
</p><p>37 The features used in the classiﬁcation task are the maximum light intensity, the curvature of the light intensity at the maximum and seven additional features based on higher moments of the intensity curve. [sent-66, score-0.254]
</p><p>38 ing step nine features were derived from the distribution of the scattered laser light (see Fig. [sent-67, score-0.374]
</p><p>39 The tenth feature is the circumference of the ﬁnger joint. [sent-69, score-0.107]
</p><p>40 Since there are high inter-individual variations in optical joint characteristics, it is not possible to tell the inﬂammatory status of a joint from one single image. [sent-70, score-0.279]
</p><p>41 For every joint examined, data from baseline and follow-up visit were compared and changes in arthritis activity were rated as improvement, unchanged or worsening. [sent-72, score-0.452]
</p><p>42 This rating divided the data into two classes: Class 1 contains the joints where an improvement of arthritis activity was observed (a total of 46 joints), and class 1 are the joints that remained unchanged or worsened (a total of 26 joints). [sent-73, score-0.673]
</p><p>43 For all joints, the differences in feature values between baseline and follow-up visit were computed. [sent-74, score-0.136]
</p><p>44 1 Gaussian Process Classiﬁcation (GPC) In Gaussian processes, a function  j 1  x xj Θ  § ¢  ∑ w jK  (1)  £ §  M  ¦  f x  ¤ ¥£ ¢  is described as a superposition of M kernel functions K x x j Θ , deﬁned for each of the M training data points x j , with weight w j . [sent-77, score-0.115]
</p><p>45 In two-class Gaussian process classiﬁcation, the logistic transfer function σ f x 1 e f x 1 is applied to the prediction of a Gaussian process to produce an output which can be interpreted as π x , the probability of the input x belonging to class 1 [10]. [sent-79, score-0.154]
</p><p>46 For training the Gaussian process classiﬁer 1 d (that is, determining the posterior probabilities of the parameters w 1 wM θ0 θd ) we used a full Bayesian approach, implemented with Readford Neal’s freely available FBM software. [sent-82, score-0.058]
</p><p>47 The disadvantage is that the GPR prediction cannot be treated as a posterior class probability; the advantage is that the fast and non-iterative training algorithms for GPR can be applied. [sent-88, score-0.061]
</p><p>48 Ci 0 is a constant that determines the weight of errors on the training data, and K is an M M matrix containing the amplitudes of the kernel functions at the training data, i. [sent-102, score-0.118]
</p><p>49 One particular feature of the SVM is the sparsity of the solution vector w, that is, many elements w i are zero. [sent-107, score-0.057]
</p><p>50 ¥  ¡ §  ©      £ §  § ¢  ¤  £¢       In the experiments, we used both an SVM with linear kernel (“SVM linear”) and an SVM with a Gaussian kernel (“SVM Gaussian”), equivalent to the Gaussian process kernel Eq. [sent-108, score-0.209]
</p><p>51   £  1 As a prior distribution for kernel parameter θ 0  To compensate for the unbalanced distribution of classes, the penalty term C i was chosen to be 0 8 for the examples from the larger class and 1 for the smaller class. [sent-119, score-0.081]
</p><p>52 This was found empirically to give the best balance of sensitivity and speciﬁcity (cf. [sent-120, score-0.102]
</p><p>53 The the natural link function [1] is the logistic transfer function σ f x overall output of the GLM σ f x computes π x , the probability of the input x belonging to class 1. [sent-128, score-0.076]
</p><p>54  £     £   ¢ ¤ £ ¢ ¢  ¤ ¥£ ¢  £ ¢  £ £ ¢ ¢  4 Training and Evaluation One of the challenges in developing the classiﬁcation system for the RA scanner is the low number of training examples available. [sent-130, score-0.346]
</p><p>55 Data was collected through an extensive medical study, but only data from 72 ﬁngers were found to be suitable for further use. [sent-131, score-0.058]
</p><p>56 Since we wish to make use of as much training data as possible, N 36 seemed the appropriate choice 2 , giving test sets with two examples in each iteration. [sent-135, score-0.093]
</p><p>57 For some of the methods model parameter needed to be tuned (for example, choosing SVM kernel width), where again cross-validation is employed. [sent-136, score-0.08]
</p><p>58 We use here a test that is best suited for small test sets, since it takes into account the outcome on the test examples one by one, thus matching our above described 36-fold cross validation scheme perfectly. [sent-138, score-0.152]
</p><p>59 Basis of the test are two counts b (how many examples in the test set were correctly classiﬁed by method B, but misclassiﬁed by method A) and c (number of examples misclassiﬁed by B, correctly classiﬁed by A). [sent-140, score-0.12]
</p><p>60 “Reduced feature set” indicates experiments where a priori feature selection has been done counts b and c as the sufﬁcient statistics of a binomial random variable with parameter θ, where θ is the proportion of cases where method A performs better than method B. [sent-146, score-0.141]
</p><p>61 The null hypothesis H0 is that the parameter θ 0 5, that is, both methods A and B have the same performance. [sent-147, score-0.116]
</p><p>62 The test statistics under the null hypothesis is the Binomial distribution Bi i b c θ) with parameter θ 0 5. [sent-149, score-0.119]
</p><p>63 We reject the null hypothesis if the probability of observing a count k c under the null hypothesis Pk c ∑b cc Bi i b c θ 0 5 is sufﬁciently small. [sent-150, score-0.176]
</p><p>64 i  ©  ©  ©  ¤  §    ¤  §     ¨  ¢  £ ©     ¤ §  ¨  ¢  ©  ¤ £  ¦    ¢  ROC Curves In medical diagnosis, biometrics and other areas, the common means of assessing a classiﬁcation method is the receiver operating characteristics (ROC) curve. [sent-151, score-0.147]
</p><p>65 An ROC curve plots sensitivity versus 1-speciﬁcity 3 for different thresholds of the classiﬁer output. [sent-152, score-0.111]
</p><p>66 Based on the ROC curve it can be decided how many false positives resp. [sent-153, score-0.115]
</p><p>67 false negatives one is willing to tolerate, thus helping to tune the classiﬁer threshold to best suit a certain application. [sent-154, score-0.149]
</p><p>68 Acquiring the ROC curve typically requires the classiﬁer output on an independent test set. [sent-155, score-0.094]
</p><p>69 Here the above mentioned threshold on the classiﬁer output is chosen such that sensitivity equals speciﬁcity. [sent-159, score-0.096]
</p><p>70 We attribute the good performance of GPR to its inherent feature relevance detection, which is done by adapting the length scales θi in the covariance function Eq. [sent-164, score-0.057]
</p><p>71 a large θ i means that the i-th feature is essentially ignored. [sent-167, score-0.057]
</p><p>72 In an additional experiment we wanted to ﬁnd out if classiﬁcation results could be improved speciﬁcity ¡  true positives true positives false negatives  true negatives true negatives false positives ¢  ¢  ¡  3 sensitivity  1 0. [sent-171, score-0.428]
</p><p>73 2  GPR Bayesian GLM, reduced feature set SVM linear, reduced feature set  0. [sent-179, score-0.218]
</p><p>74 8  1  Figure 2: ROC curves of the best classiﬁcation methods, both on the full data set and on a reduced data set where a priori feature selection was used to retain only the three most relevant features. [sent-184, score-0.168]
</p><p>75 Integrating the area under the ROC curves gives similar results for all three methods, with an area of 0 86 for SVM linear and GLM, and 0 84 for GPR Bayesian  ©  ©  by using only a subset of input features 4 . [sent-185, score-0.086]
</p><p>76 We found that only the performance of the two linear classiﬁers (GLM and SVM linear) could be improved by the input feature selection. [sent-186, score-0.085]
</p><p>77 ©  Signiﬁcance Tests Using the statistical hypothesis test described in the previous section, we compared all classiﬁcation methods pairwise. [sent-189, score-0.098]
</p><p>78 It turned out the three best methods (GPR Bayesian, and GLM and SVM linear with reduced feature set) perform better than all other methods at a conﬁdence level of 90% or more. [sent-190, score-0.223]
</p><p>79 ROC Curves For the three best classiﬁcation methods (GPR Bayesian, and GLM and SVM linear with reduced feature set), we have plotted the receiver operating characteristics (ROC) curve in Fig. [sent-192, score-0.323]
</p><p>80 According to the ROC curve a sensitivity of 80% can be achieved with a speciﬁcity at around 90%. [sent-194, score-0.111]
</p><p>81 Summary To summarize, when the full set of features was used, best performance was obtained with GPR Bayesian. [sent-196, score-0.059]
</p><p>82 Comparable yet slightly worse results could be achieved by performing feature selection a priori and reducing the number of input features to the three most signiﬁcant ones. [sent-198, score-0.086]
</p><p>83 In particular, the error rates of linear classiﬁers (GLM and linear SVM) improved by this feature selection, whereas more complex classiﬁers did not beneﬁt. [sent-199, score-0.113]
</p><p>84 We can draw the important conclusion that, using the best classiﬁers, a sensitivity of 80% can be reached at a speciﬁcity of approximately 90%. [sent-200, score-0.102]
</p><p>85 6 Conclusions In this paper we have reported results of the analysis of a prototype medical imaging system, the RA scanner. [sent-201, score-0.269]
</p><p>86 Aim of the RA scanner is to detect soft tissue changes in ﬁnger joints, 4 This was done with the input relevance detection algorithm of the neural network tool SENN, a variant of sequential backward elimination where the feature that least affects the neural network output is removed. [sent-202, score-0.514]
</p><p>87 The feature set was reduced to the three most relevant ones. [sent-203, score-0.109]
</p><p>88 which occur in early stages of rheumatoid arthritis (RA). [sent-204, score-0.473]
</p><p>89 Basis of the RA scanner is a novel laser imaging technique that is sensitive to inﬂammatory soft tissue changes. [sent-205, score-0.855]
</p><p>90 We have analyzed whether the laser images are suitable for an accurate prediction of the inﬂammatory status of a ﬁnger joint, and which classiﬁcation methods are best suited for this task. [sent-206, score-0.399]
</p><p>91 Out of a set of linear and kernel-based classiﬁcation methods, Gaussian processes regression performed best, followed closely by generalized linear models and the linear support vector machine, the latter two operating on a reduced feature set. [sent-207, score-0.245]
</p><p>92 For the RA prediction task, we achieved a sensitivity of 80% at a speciﬁcity of approximately 90%. [sent-209, score-0.1]
</p><p>93 These results show that a further development of the RA scanner is desirable. [sent-210, score-0.284]
</p><p>94 In the present study the inﬂammatory status is assessed by a rheumatologist, taking into account the patients subjective degree of pain. [sent-211, score-0.164]
</p><p>95 Further developments of the classiﬁcation system in the RA scanner will thus incorporate information from established medical imaging systems such as magnetic resonance imaging (MRI). [sent-213, score-0.718]
</p><p>96 MRI is known to provide accurate information about soft tissue changes in ﬁnger joints, yet is too costly to be routinely used for RA diagnosis. [sent-214, score-0.149]
</p><p>97 When does rheumatoid arthritis begin and why do we need to know? [sent-225, score-0.441]
</p><p>98 Monte carlo implementation of gaussian process models for bayesian regression and classiﬁcation. [sent-254, score-0.144]
</p><p>99 Assessment of proximal ﬁnger joint inﬂammation in patients with rheumatoid arthritis, using a novel laser-based imaging technique. [sent-277, score-0.532]
</p><p>100 Bayesian framework for least-squares support vector machine classiﬁers, gaussian processes and kernel ﬁsher discriminant analysis. [sent-291, score-0.095]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nger', 0.334), ('gpr', 0.334), ('ra', 0.292), ('scanner', 0.284), ('arthritis', 0.249), ('laser', 0.216), ('glm', 0.2), ('classi', 0.196), ('rheumatoid', 0.192), ('joints', 0.183), ('ammatory', 0.173), ('imaging', 0.158), ('roc', 0.152), ('svm', 0.132), ('tissue', 0.117), ('ammation', 0.115), ('status', 0.097), ('joint', 0.091), ('cation', 0.077), ('uller', 0.077), ('mri', 0.076), ('sensitivity', 0.072), ('city', 0.068), ('patients', 0.067), ('gpc', 0.067), ('light', 0.065), ('diagnosis', 0.061), ('medical', 0.058), ('crossval', 0.058), ('fbm', 0.058), ('feature', 0.057), ('negatives', 0.054), ('prototype', 0.053), ('reduced', 0.052), ('bayesian', 0.052), ('kernel', 0.052), ('ers', 0.052), ('circumference', 0.05), ('null', 0.049), ('neal', 0.044), ('gaussian', 0.043), ('visit', 0.043), ('positives', 0.042), ('cv', 0.04), ('curve', 0.039), ('hypothesis', 0.039), ('absorption', 0.038), ('amed', 0.038), ('krause', 0.038), ('medication', 0.038), ('rheumatic', 0.038), ('rheumatism', 0.038), ('scheel', 0.038), ('ttingen', 0.038), ('examined', 0.038), ('baseline', 0.036), ('false', 0.034), ('nine', 0.034), ('mayer', 0.033), ('olkopf', 0.033), ('training', 0.033), ('characteristics', 0.033), ('intensity', 0.033), ('unchanged', 0.033), ('early', 0.032), ('misclassi', 0.032), ('soft', 0.032), ('tune', 0.031), ('classifier', 0.031), ('test', 0.031), ('scattered', 0.03), ('wm', 0.03), ('clinical', 0.03), ('resonance', 0.03), ('magnetic', 0.03), ('best', 0.03), ('xj', 0.03), ('examples', 0.029), ('er', 0.029), ('curves', 0.029), ('features', 0.029), ('healthy', 0.028), ('receiver', 0.028), ('operating', 0.028), ('prediction', 0.028), ('methods', 0.028), ('linear', 0.028), ('transfer', 0.028), ('binomial', 0.027), ('graz', 0.027), ('disease', 0.027), ('remained', 0.025), ('cance', 0.025), ('process', 0.025), ('regression', 0.024), ('stems', 0.024), ('technique', 0.024), ('output', 0.024), ('logistic', 0.024), ('novel', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="196-tfidf-1" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, Gerhard A. Müller</p><p>Abstract: We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various linear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably perform parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage.</p><p>2 0.1310288 <a title="196-tfidf-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.12843227 <a title="196-tfidf-3" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>4 0.12588651 <a title="196-tfidf-4" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>5 0.11760385 <a title="196-tfidf-5" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>6 0.11610119 <a title="196-tfidf-6" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>7 0.11267551 <a title="196-tfidf-7" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>8 0.1119616 <a title="196-tfidf-8" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>9 0.10546837 <a title="196-tfidf-9" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>10 0.10277379 <a title="196-tfidf-10" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>11 0.1013143 <a title="196-tfidf-11" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>12 0.098140053 <a title="196-tfidf-12" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>13 0.084665641 <a title="196-tfidf-13" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>14 0.083585337 <a title="196-tfidf-14" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>15 0.080883294 <a title="196-tfidf-15" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>16 0.079303131 <a title="196-tfidf-16" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>17 0.077577017 <a title="196-tfidf-17" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>18 0.072464146 <a title="196-tfidf-18" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>19 0.071876176 <a title="196-tfidf-19" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>20 0.071134783 <a title="196-tfidf-20" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.207), (1, -0.089), (2, 0.105), (3, -0.014), (4, 0.152), (5, -0.026), (6, 0.012), (7, -0.043), (8, 0.062), (9, 0.019), (10, -0.081), (11, 0.108), (12, 0.093), (13, 0.075), (14, 0.116), (15, -0.125), (16, -0.053), (17, 0.04), (18, 0.032), (19, 0.033), (20, 0.035), (21, -0.004), (22, 0.015), (23, -0.014), (24, -0.028), (25, -0.02), (26, -0.023), (27, -0.03), (28, 0.032), (29, 0.042), (30, 0.117), (31, 0.0), (32, -0.03), (33, 0.027), (34, 0.019), (35, 0.044), (36, 0.028), (37, -0.035), (38, -0.076), (39, -0.035), (40, 0.085), (41, -0.042), (42, -0.082), (43, 0.086), (44, -0.02), (45, 0.023), (46, 0.085), (47, 0.064), (48, 0.081), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93348455 <a title="196-lsi-1" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, Gerhard A. Müller</p><p>Abstract: We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various linear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably perform parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage.</p><p>2 0.72966337 <a title="196-lsi-2" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>3 0.71074355 <a title="196-lsi-3" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>4 0.693124 <a title="196-lsi-4" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Recently, interest is growing to develop an effective communication interface connecting the human brain to a computer, the ’Brain-Computer Interface’ (BCI). One motivation of BCI research is to provide a new communication channel substituting normal motor output in patients with severe neuromuscular disabilities. In the last decade, various neurophysiological cortical processes, such as slow potential shifts, movement related potentials (MRPs) or event-related desynchronization (ERD) of spontaneous EEG rhythms, were shown to be suitable for BCI, and, consequently, different independent approaches of extracting BCI-relevant EEG-features for single-trial analysis are under investigation. Here, we present and systematically compare several concepts for combining such EEG-features to improve the single-trial classiﬁcation. Feature combinations are evaluated on movement imagination experiments with 3 subjects where EEG-features are based on either MRPs or ERD, or both. Those combination methods that incorporate the assumption that the single EEG-features are physiologically mutually independent outperform the plain method of ’adding’ evidence where the single-feature vectors are simply concatenated. These results strengthen the hypothesis that MRP and ERD reﬂect at least partially independent aspects of cortical processes and open a new perspective to boost BCI effectiveness.</p><p>5 0.64393997 <a title="196-lsi-5" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>6 0.63436091 <a title="196-lsi-6" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>7 0.60172755 <a title="196-lsi-7" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>8 0.60143656 <a title="196-lsi-8" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>9 0.60137033 <a title="196-lsi-9" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>10 0.5888837 <a title="196-lsi-10" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>11 0.57718545 <a title="196-lsi-11" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>12 0.55133837 <a title="196-lsi-12" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>13 0.54964936 <a title="196-lsi-13" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>14 0.52189606 <a title="196-lsi-14" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>15 0.52188909 <a title="196-lsi-15" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>16 0.52126831 <a title="196-lsi-16" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>17 0.51983219 <a title="196-lsi-17" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>18 0.5187254 <a title="196-lsi-18" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>19 0.48786572 <a title="196-lsi-19" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>20 0.47076541 <a title="196-lsi-20" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.01), (14, 0.021), (23, 0.036), (30, 0.353), (42, 0.055), (54, 0.096), (55, 0.028), (57, 0.015), (64, 0.012), (67, 0.017), (68, 0.025), (74, 0.075), (87, 0.013), (92, 0.028), (98, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76963997 <a title="196-lda-1" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, Gerhard A. Müller</p><p>Abstract: We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various linear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably perform parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage.</p><p>2 0.46709263 <a title="196-lda-2" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>3 0.4652836 <a title="196-lda-3" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>4 0.46458602 <a title="196-lda-4" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>Author: Joaquin Quiñonero-candela, Ole Winther</p><p>Abstract: In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean ﬁeld approach to the intractable classiﬁcation model that is expected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 − 104 ) examples. The results indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.</p><p>5 0.46436197 <a title="196-lda-5" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>6 0.46414405 <a title="196-lda-6" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>7 0.46359754 <a title="196-lda-7" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>8 0.46228543 <a title="196-lda-8" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>9 0.46190023 <a title="196-lda-9" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>10 0.4618673 <a title="196-lda-10" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>11 0.461748 <a title="196-lda-11" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>12 0.46086776 <a title="196-lda-12" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>13 0.46030331 <a title="196-lda-13" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>14 0.45985219 <a title="196-lda-14" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>15 0.45945811 <a title="196-lda-15" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>16 0.45926064 <a title="196-lda-16" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>17 0.45889318 <a title="196-lda-17" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>18 0.4585433 <a title="196-lda-18" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>19 0.45761797 <a title="196-lda-19" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>20 0.45759034 <a title="196-lda-20" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
