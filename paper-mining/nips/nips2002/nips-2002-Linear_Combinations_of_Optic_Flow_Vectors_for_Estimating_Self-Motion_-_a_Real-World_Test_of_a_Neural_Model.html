<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-136" href="#">nips2002-136</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</h1>
<br/><p>Source: <a title="nips-2002-136-pdf" href="http://papers.nips.cc/paper/2247-linear-combinations-of-optic-flow-vectors-for-estimating-self-motion-a-real-world-test-of-a-neural-model.pdf">pdf</a></p><p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>Reference: <a title="nips-2002-136-reference" href="../nips2002_reference/nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. [sent-9, score-0.881]
</p><p>2 In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. [sent-10, score-0.536]
</p><p>3 We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. [sent-11, score-0.71]
</p><p>4 The estimator is tested on a gantry carrying an omnidirectional vision sensor. [sent-12, score-0.514]
</p><p>5 The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. [sent-13, score-0.643]
</p><p>6 1  Introduction  The tangential neurons in the ﬂy brain are known to respond in a directionally selective manner to wide-ﬁeld motion stimuli. [sent-14, score-0.684]
</p><p>7 A detailed mapping of their local motion sensitivities and preferred motion directions shows a striking similarity to certain self-motion-induced ﬂow ﬁelds (an example is shown in Fig. [sent-15, score-0.418]
</p><p>8 This suggests a possible involvement of these neurons in the extraction of self-motion parameters from the optic ﬂow, which might be useful, for instance, for stabilizing the ﬂy’s head during ﬂight manoeuvres. [sent-17, score-0.534]
</p><p>9 A recent study [2] has shown that a simpliﬁed computational model of the tangential neurons as a weighted sum of ﬂow measurements was able to reproduce the observed response ﬁelds. [sent-18, score-0.775]
</p><p>10 The weights were chosen according to an optimality principle which minimizes the output variance of the model caused by noise and distance variability between different scenes. [sent-19, score-0.188]
</p><p>11 We again use linear combinations of local ﬂow measurements but, instead of prescribing a ﬁxed motion axis and minimizing the output variance, we require that the quadratic error in the estimated self-motion parameters be as small as possible. [sent-22, score-0.466]
</p><p>12 The orientation of each arrow gives the local preferred direction (LPD), and its length denotes the relative local motion sensitivity (LMS). [sent-26, score-0.367]
</p><p>13 VS7 responds maximally to rotation around an axis at an azimuth of about 30◦ and an elevation of about −15◦ (after [1]). [sent-27, score-0.58]
</p><p>14 optimization principle, we derive weight sets that lead to motion sensitivities similar to those observed in tangential neurons. [sent-28, score-0.539]
</p><p>15 In contrast to the previous model, this approach also yields the preferred motion directions and the motion axes to which the neural models are tuned. [sent-29, score-0.334]
</p><p>16 We subject the obtained linear estimator to a rigorous real-world test on a gantry carrying an omnidirectional vision sensor. [sent-30, score-0.543]
</p><p>17 1  Modeling ﬂy tangential neurons as optimal linear estimators for self-motion Sensor and neuron model  In order to simplify the mathematical treatment, we assume that the N elementary motion detectors (EMDs) of our model eye are arranged on the unit sphere. [sent-32, score-0.817]
</p><p>18 The viewing direction of a particular EMD with index i is denoted by the radial unit vector di . [sent-33, score-0.249]
</p><p>19 At each viewing direction, we deﬁne a local two-dimensional coordinate system on the sphere consisting of two orthogonal tangential unit vectors ui and vi (Fig. [sent-34, score-0.695]
</p><p>20 Formally, this means that we obtain at each viewing direction two measurements xi and yi along ui and vi , respectively, given by xi = pi · ui + nx,i  and  yi = pi · vi + ny,i ,  (1)  where nx,i and ny,i denote additive noise components and pi the local optic ﬂow vector. [sent-37, score-1.373]
</p><p>21 When the spherical sensor translates with T while rotating with R about an axis through the origin, the self-motion-induced image ﬂow pi at di is [3] pi = −µi (T − (T · di )di ) − R × di . [sent-38, score-0.665]
</p><p>22 (2)  µi is the inverse distance between the origin and the object seen in direction di , the socalled “nearness”. [sent-39, score-0.272]
</p><p>23 The entire collection of ﬂow measurements xi and yi comprises the  a. [sent-40, score-0.221]
</p><p>24 y  optic flow vectors  LPD unit vectors  ui  LMSs  summation  w11  di pi  w12  vi  +  w13  z x  Figure 2: a. [sent-42, score-0.727]
</p><p>25 Sensor model: At each viewing direction di , there are two measurements xi and yi of the optic ﬂow pi along two directions ui and vi on the unit sphere. [sent-43, score-1.053]
</p><p>26 Simpliﬁed model of a tangential neuron: The optic ﬂow and the local noise signal are projected onto a unit vector ﬁeld. [sent-45, score-0.773]
</p><p>27 The weighted projections are linearly integrated to give the estimator output. [sent-46, score-0.179]
</p><p>28 input to the simpliﬁed neural model of a tangential neuron which consists of a weighted sum of all local measurements (Fig. [sent-47, score-0.605]
</p><p>29 In this model, the local motion sensitivity (LMS) is deﬁned as wi = (wx,i , wy,i ) , the local preferred motion direction (LPD) is parallel to the 1 vector wi (wx,i , wy,i ). [sent-49, score-0.49]
</p><p>30 The resulting LMSs and LPDs can be compared to measurements on real tangential neurons. [sent-50, score-0.487]
</p><p>31 As our basic hypothesis, we assume that the output of such model neurons is used to estimate the self-motion of the sensor. [sent-51, score-0.252]
</p><p>32 Since the output is a scalar, we need in the simplest case an ensemble of six neurons to encode all six rotational and translational degrees of freedom. [sent-52, score-0.592]
</p><p>33 The local weights of each neuron are chosen to yield an optimal linear estimator for the respective self-motion component. [sent-53, score-0.326]
</p><p>34 2  Prior knowledge  An estimator for self-motion consisting of a linear combination of ﬂow measurements necessarily has to neglect the dependence of the optic ﬂow on the object distances. [sent-55, score-0.667]
</p><p>35 As a consequence, the estimator output will be different from scene to scene, depending on the current distance and noise characteristics. [sent-56, score-0.481]
</p><p>36 The best the estimator can do is to add up as many ﬂow measurements as possible hoping that the individual distance deviations of the current scene from the average will cancel each other. [sent-57, score-0.538]
</p><p>37 Clearly, viewing directions with low distance variability and small noise content should receive a higher weight in this process. [sent-58, score-0.299]
</p><p>38 In this way, prior knowledge about the distance and noise statistics of the sensor and its environment can improve the reliability of the estimate. [sent-59, score-0.316]
</p><p>39 If the current nearness at viewing direction di differs from the the average nearness µi over ¯ all scenes by ∆µi , the measurement xi can be written as ( see Eqns. [sent-60, score-0.632]
</p><p>40 (1) and (2)) xi = −(¯i ui , (ui × di ) ) µ  T R  + nx,i − ∆µi ui T,  (4)  where the last two terms vary from scene to scene, even when the sensor undergoes exactly the same self-motion. [sent-61, score-0.543]
</p><p>41 The entire µ ensemble of measurements over the sensor can thus be written as x = F θ + n. [sent-76, score-0.297]
</p><p>42 (5)  Assuming that T, nx,i , ny,i and µi are uncorrelated, the covariance matrix C of the scenedependent measurement component n is given by Cij = Cn,ij + Cµ,ij ui CT uj  (6)  with Cn being the covariance of n, Cµ of µ and CT of T. [sent-77, score-0.219]
</p><p>43 These three covariance matrices, together with the average nearness µi , constitute the prior knowledge required for deriving ¯ the optimal estimator. [sent-78, score-0.181]
</p><p>44 The optimal weight matrix is chosen to minimize the mean square error e of the estimator given by ˆ e = E( θ − θ 2 ) = tr[W CW ] (8) where E denotes the expectation. [sent-84, score-0.214]
</p><p>45 When computed for the typical inter-scene covariances of a ﬂying animal, the resulting weight sets are able to reproduce the characteristics of the LMS and LPD distribution of the tangential neurons [2]. [sent-91, score-0.667]
</p><p>46 Having shown the good correspondence between model neurons and measurement, the question remains whether the output of such an ensemble of neurons can be used for some real-world task. [sent-92, score-0.525]
</p><p>47 This is by no means evident given the fact that - in contrast to most approaches in computer vision - the distance distribution of the current scene is completely ignored by the linear estimator. [sent-93, score-0.277]
</p><p>48 1  Experiments Linear estimator for an ofﬁce robot  As our test scenario, we consider the situation of a mobile robot in an ofﬁce environment. [sent-95, score-0.509]
</p><p>49 This scenario allows for measuring the typical motion patterns and the associated distance statistics which otherwise would be difﬁcult to obtain for a ﬂying agent. [sent-96, score-0.285]
</p><p>50 )  60  90  120  150  180  Figure 3: Distance statistics of an indoor robot (0 azimuth corresponds to forward direction): a. [sent-151, score-0.431]
</p><p>51 The distance statistics were recorded using a rotating laser scanner. [sent-157, score-0.239]
</p><p>52 The 26 measurement points were chosen along typical trajectories of a mobile robot while wandering around and avoiding obstacles in an ofﬁce environment. [sent-158, score-0.338]
</p><p>53 The recorded distance statistics therefore reﬂect properties both of the environment and of the speciﬁc movement patterns of the robot. [sent-159, score-0.203]
</p><p>54 From these measurements, the average nearness µi and its covariance Cµ were ¯ computed (cf. [sent-160, score-0.181]
</p><p>55 3, we used distance instead of nearness for easier interpretation). [sent-162, score-0.255]
</p><p>56 The distance statistics show a pronounced anisotropy which can be attributed to three main causes: (1) Since the robot tries to turn away from the obstacles, the distance in front and behind the robot tends to be larger than on its sides (Fig. [sent-163, score-0.544]
</p><p>57 (2) The camera on the robot usually moves at a ﬁxed height above ground on a ﬂat surface. [sent-165, score-0.239]
</p><p>58 When the robot follows the corridor while avoiding obstacles, distance variations in the frontal region of the visual ﬁeld are very large (Fig. [sent-169, score-0.296]
</p><p>59 The estimation of the translation covariance CT is straightforward since our robot can only translate in forward direction, i. [sent-171, score-0.438]
</p><p>60 CT is therefore 0 everywhere except the lower right diagonal entry which is the square of the average forward speed of the robot (here: 0. [sent-174, score-0.185]
</p><p>61 )  150  180  Figure 4: Model neurons computed as part of the linear estimator. [sent-183, score-0.242]
</p><p>62 The depicted region of the visual ﬁeld extends from −15◦ to 180◦ azimuth and from −75◦ to 75◦ elevation. [sent-186, score-0.212]
</p><p>63 Examples of the optimal weight sets for the model neurons (corresponding to a row of W ) are shown in Fig. [sent-199, score-0.248]
</p><p>64 The resulting model neurons show very similar characteristics to those observed in real tangential neurons, however, with speciﬁc adaptations to the indoor robot scenario. [sent-201, score-0.755]
</p><p>65 All model neurons have in common that image regions near the rotation or translation axis receive less weight. [sent-202, score-0.811]
</p><p>66 Equation (11) predicts that the estimator will preferably sample in image regions with smaller distance variations. [sent-204, score-0.385]
</p><p>67 In our measurements, this is mainly the case at the ground around the robot (Fig. [sent-205, score-0.186]
</p><p>68 The rotation-selective model neurons weight image regions with larger distances more highly, since distance variations at large distances have a smaller effect. [sent-207, score-0.572]
</p><p>69 In our example, distances are largest in front and behind the robot so that the rotation-selective neurons assign the highest weights to these regions (Fig. [sent-208, score-0.46]
</p><p>70 2  Gantry experiments  The self-motion estimates from the model neuron ensemble were tested on a gantry with three translational and one rotational (yaw) degree of freedom. [sent-211, score-0.632]
</p><p>71 Since the gantry had a position accuracy below 1mm, the programmed position values were taken as ground truth for evaluating the estimator’s accuracy. [sent-212, score-0.234]
</p><p>72 Such a large ﬁeld of view considerably improves the estimator’s performance since the individual distance deviations in the scene are more likely to be averaged out. [sent-215, score-0.22]
</p><p>73 In each experiment, the camera was moved to 10 different start positions in the lab with largely varying distance distributions. [sent-217, score-0.184]
</p><p>74 After recording an image of the scene at the start position, the gantry translated and rotated at various prescribed speeds and directions and took a second image. [sent-218, score-0.466]
</p><p>75 After the recorded image pairs (10 for each type of movement) were unwarped, we computed the optic ﬂow input for the model neurons using a standard gradient-based scheme [5]. [sent-219, score-0.598]
</p><p>76 20  150  estimator response [%]  estimated self-motion  18  rotation  16  14  12  translation 10  100  50  8  6  4  4  6  8  10  12  14  16  18  20  0  22  true self-motion  2  3  4  5  d. [sent-222, score-0.69]
</p><p>77 1  1  2  3  0  1  2  3  Figure 5: Gantry experiments: Results are given in arbitrary units, true rotation values are denoted by a dashed line, translation by a dash-dot line. [sent-236, score-0.447]
</p><p>78 Grey bars denote translation estimates, white bars rotation estimates a. [sent-237, score-0.545]
</p><p>79 The average error of the rotation rate estimates over all trials (N=450) was 0. [sent-243, score-0.324]
</p><p>80 5a), the error in the estimated translation speeds (N=420) was 8. [sent-247, score-0.307]
</p><p>81 The estimated rotation axis had an average error of magnitude 1. [sent-251, score-0.311]
</p><p>82 The larger error of the translation estimates is mainly caused by the direct dependence of the translational ﬂow on distance (see Eq. [sent-254, score-0.58]
</p><p>83 (2)) whereas the rotation estimates are only indirectly affected by distance errors via the current translational ﬂow component which is largely ﬁltered out by the LPD arrangement. [sent-255, score-0.584]
</p><p>84 The larger sensitivity of the translation estimates can be seen by moving the sensor at the same translation and rotation speeds in various locations. [sent-256, score-0.951]
</p><p>85 The rotation estimates remain consistent over all locations whereas the translation estimates show a higher variance and also a location-dependent bias, e. [sent-257, score-0.643]
</p><p>86 A second problem for translation estimation comes from the different properties of rotational and translational ﬂow ﬁelds: Due to its distance dependence, the translational ﬂow ﬁeld shows a much wider range of values than a rotational ﬂow ﬁeld. [sent-261, score-0.743]
</p><p>87 The smaller translational ﬂow vectors are often swamped by simultaneous rotation or noise, and the larger ones tend to be in the upper saturation range of the used optic ﬂow algorithm. [sent-262, score-0.716]
</p><p>88 Again, rotation estimates remain consistent while translation estimates are strongly affected by rotation (Fig. [sent-264, score-0.894]
</p><p>89 4  Conclusion  Our experiments show that it is indeed possible to obtain useful self-motion estimates from an ensemble of linear model neurons. [sent-266, score-0.187]
</p><p>90 Although a linear approach necessarily has to ignore the distances of the currently perceived scene, an appropriate choice of local weights and a large ﬁeld of view are capable of reducing the inﬂuence of noise and the particular scene distances on the estimates. [sent-267, score-0.355]
</p><p>91 In particular, rotation estimates were highly accurate - in a range comparable to gyroscopic estimates - and consistent across different scenes and different simultaneous translations. [sent-268, score-0.495]
</p><p>92 The components of the estimator are simpliﬁed model neurons which have been shown to reproduce the essential receptive ﬁeld properties of the ﬂy’s tangential neurons [2]. [sent-270, score-0.998]
</p><p>93 Our study suggests that the output of such neurons could be directly used for self-motion estimation by simply combining them linearly at a later integration stage. [sent-271, score-0.252]
</p><p>94 Finally, we have to point out a basic limitation of the proposed theory: It assumes linear EMDs as input to the neurons (see Eq. [sent-273, score-0.242]
</p><p>95 In this range, the tangential neuron can only indicate the presence and the sign of a particular self-motion component, not the current rotation or translation velocity. [sent-277, score-0.862]
</p><p>96 In addition, a detailed comparison between the linear model and real neurons shows characteristic differences indicating that tangential neurons usually operate in the plateau range rather than in the linear range of the EMDs [2]. [sent-279, score-0.871]
</p><p>97 Acknowledgments The gantry experiments were done at the Center of Visual Sciences in Canberra. [sent-282, score-0.199]
</p><p>98 Dendritic structure and receptive ﬁeld organization of optic low processing interneurons in the ﬂy. [sent-294, score-0.294]
</p><p>99 Wide-ﬁeld, motion-sensitive neurons and matched ﬁlters for optic ﬂow ﬁelds. [sent-301, score-0.507]
</p><p>100 An image-interpolation technique for the computation of optic ﬂow and egomotion. [sent-323, score-0.294]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tangential', 0.348), ('ow', 0.322), ('optic', 0.294), ('rotation', 0.226), ('translation', 0.221), ('neurons', 0.213), ('gantry', 0.199), ('estimator', 0.179), ('azimuth', 0.173), ('robot', 0.151), ('nearness', 0.149), ('measurements', 0.139), ('elevation', 0.13), ('translational', 0.129), ('lpd', 0.124), ('motion', 0.123), ('scene', 0.114), ('ui', 0.107), ('distance', 0.106), ('emds', 0.099), ('estimates', 0.098), ('sensor', 0.098), ('eld', 0.079), ('rotational', 0.079), ('di', 0.078), ('viewing', 0.077), ('emd', 0.075), ('lms', 0.075), ('omnidirectional', 0.075), ('rotating', 0.075), ('ct', 0.073), ('pi', 0.072), ('neuron', 0.067), ('image', 0.063), ('ensemble', 0.06), ('obstacles', 0.059), ('srinivasan', 0.059), ('distances', 0.059), ('direction', 0.057), ('camera', 0.053), ('cn', 0.052), ('speeds', 0.052), ('axis', 0.051), ('local', 0.051), ('preferred', 0.05), ('chahl', 0.05), ('franz', 0.05), ('hengstenberg', 0.05), ('javaan', 0.05), ('krapp', 0.05), ('lmss', 0.05), ('measurement', 0.048), ('vi', 0.046), ('reproduce', 0.045), ('ce', 0.044), ('yi', 0.043), ('indoor', 0.043), ('ying', 0.043), ('noise', 0.043), ('plateau', 0.039), ('environment', 0.039), ('output', 0.039), ('xi', 0.039), ('visual', 0.039), ('directions', 0.038), ('simpli', 0.038), ('simultaneous', 0.038), ('regions', 0.037), ('cw', 0.037), ('unit', 0.037), ('six', 0.036), ('scenes', 0.035), ('sensitivity', 0.035), ('ground', 0.035), ('weight', 0.035), ('flow', 0.035), ('estimated', 0.034), ('forward', 0.034), ('carrying', 0.033), ('sensitivities', 0.033), ('darker', 0.033), ('covariance', 0.032), ('tr', 0.031), ('origin', 0.031), ('statistics', 0.03), ('response', 0.03), ('consequence', 0.03), ('linear', 0.029), ('vectors', 0.029), ('vision', 0.028), ('presenting', 0.028), ('mobile', 0.028), ('recorded', 0.028), ('head', 0.027), ('along', 0.026), ('uncorrelated', 0.026), ('typical', 0.026), ('dependence', 0.026), ('varying', 0.025), ('affected', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="136-tfidf-1" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>2 0.15379892 <a title="136-tfidf-2" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>3 0.12337255 <a title="136-tfidf-3" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>4 0.11894488 <a title="136-tfidf-4" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>Author: Patrik O. Hoyer, Aapo Hyvärinen</p><p>Abstract: The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signiﬁcantly from trial to trial. This variability is most often interpreted as ‘noise’, purely detrimental to the sensory system. In this paper, we propose an alternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speciﬁcally, the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representational scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.</p><p>5 0.11732961 <a title="136-tfidf-5" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>Author: Duane Q. Nykamp</p><p>Abstract: We present a method to distinguish direct connections between two neurons from common input originating from other, unmeasured neurons. The distinction is computed from the spike times of the two neurons in response to a white noise stimulus. Although the method is based on a highly idealized linear-nonlinear approximation of neural response, we demonstrate via simulation that the approach can work with a more realistic, integrate-and-ﬁre neuron model. We propose that the approach exempliﬁed by this analysis may yield viable tools for reconstructing stimulus-driven neural networks from data gathered in neurophysiology experiments.</p><p>6 0.11200577 <a title="136-tfidf-6" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>7 0.10953733 <a title="136-tfidf-7" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>8 0.10947926 <a title="136-tfidf-8" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>9 0.10251918 <a title="136-tfidf-9" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>10 0.094137102 <a title="136-tfidf-10" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>11 0.091145374 <a title="136-tfidf-11" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>12 0.087340347 <a title="136-tfidf-12" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>13 0.080396727 <a title="136-tfidf-13" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>14 0.078606769 <a title="136-tfidf-14" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>15 0.07703016 <a title="136-tfidf-15" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>16 0.076011509 <a title="136-tfidf-16" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>17 0.073634543 <a title="136-tfidf-17" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>18 0.073442213 <a title="136-tfidf-18" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>19 0.07264486 <a title="136-tfidf-19" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>20 0.070645757 <a title="136-tfidf-20" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.206), (1, 0.132), (2, -0.012), (3, 0.091), (4, 0.022), (5, -0.008), (6, 0.06), (7, 0.024), (8, 0.045), (9, 0.119), (10, -0.059), (11, -0.001), (12, 0.022), (13, -0.116), (14, -0.106), (15, -0.017), (16, 0.004), (17, -0.023), (18, -0.013), (19, 0.009), (20, 0.093), (21, 0.145), (22, 0.076), (23, 0.094), (24, 0.104), (25, 0.05), (26, -0.212), (27, 0.136), (28, -0.013), (29, -0.027), (30, 0.125), (31, -0.029), (32, -0.017), (33, 0.045), (34, -0.032), (35, 0.014), (36, 0.084), (37, -0.067), (38, -0.086), (39, -0.093), (40, -0.155), (41, 0.048), (42, 0.122), (43, -0.135), (44, 0.045), (45, -0.134), (46, 0.028), (47, -0.108), (48, 0.037), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9600175 <a title="136-lsi-1" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>2 0.55630291 <a title="136-lsi-2" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>3 0.54427987 <a title="136-lsi-3" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>4 0.53746456 <a title="136-lsi-4" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>5 0.37860537 <a title="136-lsi-5" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>6 0.37550095 <a title="136-lsi-6" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>7 0.36060861 <a title="136-lsi-7" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>8 0.35628435 <a title="136-lsi-8" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>9 0.34454387 <a title="136-lsi-9" href="./nips-2002-Minimax_Differential_Dynamic_Programming%3A_An_Application_to_Robust_Biped_Walking.html">144 nips-2002-Minimax Differential Dynamic Programming: An Application to Robust Biped Walking</a></p>
<p>10 0.34432194 <a title="136-lsi-10" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>11 0.34163949 <a title="136-lsi-11" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>12 0.32857129 <a title="136-lsi-12" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>13 0.31912971 <a title="136-lsi-13" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>14 0.31544423 <a title="136-lsi-14" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>15 0.31122461 <a title="136-lsi-15" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>16 0.30339605 <a title="136-lsi-16" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>17 0.29788214 <a title="136-lsi-17" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>18 0.29217255 <a title="136-lsi-18" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>19 0.28264558 <a title="136-lsi-19" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>20 0.27574316 <a title="136-lsi-20" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.286), (11, 0.011), (14, 0.016), (23, 0.038), (42, 0.06), (54, 0.131), (55, 0.05), (64, 0.038), (67, 0.016), (68, 0.038), (74, 0.079), (92, 0.03), (98, 0.118)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82958353 <a title="136-lda-1" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>2 0.69098401 <a title="136-lda-2" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>Author: Ralf Schoknecht</p><p>Abstract: There are several reinforcement learning algorithms that yield approximate solutions for the problem of policy evaluation when the value function is represented with a linear function approximator. In this paper we show that each of the solutions is optimal with respect to a specific objective function. Moreover, we characterise the different solutions as images of the optimal exact value function under different projection operations. The results presented here will be useful for comparing the algorithms in terms of the error they achieve relative to the error of the optimal approximate solution. 1</p><p>3 0.67022038 <a title="136-lda-3" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>4 0.59830344 <a title="136-lda-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.58877057 <a title="136-lda-5" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>6 0.58801931 <a title="136-lda-6" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>7 0.58783424 <a title="136-lda-7" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>8 0.58627468 <a title="136-lda-8" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>9 0.58582145 <a title="136-lda-9" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>10 0.58432078 <a title="136-lda-10" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>11 0.58371341 <a title="136-lda-11" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>12 0.58356774 <a title="136-lda-12" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>13 0.58196849 <a title="136-lda-13" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>14 0.58139193 <a title="136-lda-14" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>15 0.581029 <a title="136-lda-15" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>16 0.5796814 <a title="136-lda-16" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>17 0.57949746 <a title="136-lda-17" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>18 0.57861596 <a title="136-lda-18" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>19 0.57829946 <a title="136-lda-19" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>20 0.5762766 <a title="136-lda-20" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
