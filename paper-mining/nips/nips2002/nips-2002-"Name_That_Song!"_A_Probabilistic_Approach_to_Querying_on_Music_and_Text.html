<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-1" href="#">nips2002-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</h1>
<br/><p>Source: <a title="nips-2002-1-pdf" href="http://papers.nips.cc/paper/2262-name-that-song-a-probabilistic-approach-to-querying-on-music-and-text.pdf">pdf</a></p><p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>Reference: <a title="nips-2002-1-reference" href="../nips2002_reference/nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We present a novel, ﬂexible statistical approach for modelling music and text jointly. [sent-6, score-0.722]
</p><p>2 The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. [sent-7, score-0.085]
</p><p>3 The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs. [sent-8, score-2.438]
</p><p>4 DJs play a short excerpt from a song and listeners phone in to guess the name of the song. [sent-10, score-0.235]
</p><p>5 Of course, callers often get it right when DJs provide extra contextual clues (such as lyrics, or a piece of trivia about the song or band). [sent-11, score-0.354]
</p><p>6 We are attempting to reproduce this ability in the context of information retrieval (IR). [sent-12, score-0.053]
</p><p>7 In this paper, we present a method for querying with words and/or music. [sent-13, score-0.161]
</p><p>8 We focus on monophonic and polyphonic musical pieces of known structure (MIDI ﬁles, full music notation, etc. [sent-14, score-0.863]
</p><p>9 Retrieving these pieces in multimedia databases, such as the Web, is a problem of growing interest [1, 2]. [sent-16, score-0.164]
</p><p>10 A signiﬁcant step was taken by Downie [3], who applied standard text IR techniques to retrieve music by, initially, converting music to text format. [sent-17, score-1.444]
</p><p>11 Most research (including [3]) has, however, focused on plain music retrieval. [sent-18, score-0.474]
</p><p>12 To the best of our knowledge, there has been no attempt to model text and music jointly. [sent-19, score-0.722]
</p><p>13 We propose a joint probabilistic model for documents with music and/or text. [sent-20, score-0.639]
</p><p>14 It allows users to query multimedia databases using text and/or music as input. [sent-22, score-0.985]
</p><p>15 It is well-suited for browsing applications as it organizes the documents into “soft” clusters. [sent-23, score-0.201]
</p><p>16 The document of highest probability in each cluster serves as a music thumbnail for automated music summarisation. [sent-24, score-1.191]
</p><p>17 The model allows one to query with an entire text document to automatically annotate the document with musical pieces. [sent-25, score-0.87]
</p><p>18 It can be used to automatically recommend or identify similar songs. [sent-26, score-0.087]
</p><p>19 The interested reader may further wish to consult [4], in which we discuss an application of our model to the problem of jointly  modelling music, as well as text and images. [sent-28, score-0.248]
</p><p>20 2 Model speciﬁcation The training data consists of documents with text (lyrics or information about the song) and musical scores in GUIDO notation [5]. [sent-29, score-0.773]
</p><p>21 (GUIDO is a powerful language for representing musical scores in an HTML-like notation. [sent-30, score-0.347]
</p><p>22 ) We model the data with a Bayesian multi-modal mixture model. [sent-32, score-0.048]
</p><p>23 Words and scores are assumed to be conditionally independent given the mixture component label. [sent-33, score-0.19]
</p><p>24 We model musical scores with ﬁrst-order Markov chains, in which each state corresponds to a note, rest, or the start of a new voice. [sent-34, score-0.384]
</p><p>25 Notes’ pitches are represented by the interval change (in semitones) from the previous note, rather than by absolute pitch, so that a score or query transposed to a different key will still have the same Markov chain. [sent-35, score-0.215]
</p><p>26 Rest states are represented similarly, save that pitch is not represented. [sent-37, score-0.065]
</p><p>27 Polyphonic scores are represented by chaining the beginning of a new voice to the end of a previous one. [sent-39, score-0.217]
</p><p>28 In order to ensure that the ﬁrst note in each voice appears in both the row and column of the Markov transition matrix, a special “new voice” state with no interval or rhythm serves as a dummy state marking the beginning of a new voice. [sent-40, score-0.224]
</p><p>29 The ﬁrst note of a voice has a distinguishing “ﬁrst note” interval value and the ﬁrst note or rest has a duration value of one. [sent-41, score-0.146]
</p><p>30 From top: GUIDO notation, standard musical notation (generated automatically from GUIDO notation), and as a series of states in a ﬁrst-order Markov chain (also generated automatically from GUIDO notation). [sent-43, score-0.334]
</p><p>31 The Markov chain representation of a piece of music is then mapped to a sparse transition frequency table , where denotes the number of times we observe the transition from state to state in document . [sent-44, score-0.725]
</p><p>32 We use to denote the initial state of the Markov chain. [sent-45, score-0.037]
</p><p>33 The associated text is modeled using a standard sparse term frequency vector , where denotes the number of times word appears in document . [sent-46, score-0.341]
</p><p>34 For notational simplicity, we group the music and text variable as follows: . [sent-47, score-0.722]
</p><p>35 In essence, this Markovian approach is akin to a text bigram model, save that the states are transitions between musical notes and rests rather than words. [sent-48, score-0.544]
</p><p>36  ('$  &  Our multi-modal mixture model is as follows: IP  "# ¡  (1) 24  E  )  E F¦  H '! [sent-50, score-0.048]
</p><p>37          ¡  ©  ¡  0 ¢  where encompasses all the model parameters and where if the ﬁrst entry of belongs to state and is otherwise. [sent-55, score-0.037]
</p><p>38 The threedimensional matrix denotes the estimated probability of transitioning from state to state in cluster , the matrix denotes the initial probabilities of being in state , given membership in cluster . [sent-56, score-0.443]
</p><p>39 The matrix denotes the probability of the word in cluster . [sent-58, score-0.15]
</p><p>40 The mixture model is deﬁned on the standard probability simplex for all and . [sent-59, score-0.048]
</p><p>41 We introduce the latent allocation variables to indicate that a particular sequence belongs to a speciﬁc cluster . [sent-60, score-0.301]
</p><p>42 It is also easy to formulate the model in terms of aspects and clusters as suggested in [7, 8]. [sent-83, score-0.037]
</p><p>43 1 Prior speciﬁcation ¢  We follow a hierarchical Bayesian strategy, where the unknown parameters and the allocation variables are regarded as being drawn from appropriate prior distributions. [sent-85, score-0.108]
</p><p>44 We place a conjugate to be drawn from a multinomial distribution, Dirichlet prior on the mixing coefﬁcients . [sent-88, score-0.039]
</p><p>45   )     The posterior for the allocation variables will be required. [sent-96, score-0.108]
</p><p>46 Q9      ¤dQ9 f    § ¦   e x `  3 Computation The parameters of the mixture model cannot be computed analytically unless one knows the mixture indicator variables. [sent-104, score-0.126]
</p><p>47 One can implement a Gibbs sampler to compute the parameters and allocation variables. [sent-106, score-0.108]
</p><p>48 This is done by sampling the parameters from their Dirichlet posteriors and the allocation variables from their multinomial posterior. [sent-107, score-0.147]
</p><p>49 Instead we opt for expectation maximization (EM) algorithms to compute the maximum likelihood (ML) and maximum a posteriori (MAP) point estimates of the mixture model. [sent-109, score-0.085]
</p><p>50 E step: Compute the expectation of the complete log-likelihood with respect to the distribution of the allocation variables ML , old  4         ¢ £¡  ©  ¨  §  ¦¤ ¥        T  7  0  )  ¨   % $ # ! [sent-112, score-0.14]
</p><p>51  &  $ ¦        '( %     RQ    ¡  ©  § ¦        & '¦   9  ©       ML  IP  The  ¢¡  old    where  T     ¡  In the E step, we have to compute using equation (3). [sent-119, score-0.032]
</p><p>52 Q    `  & '¦  1  & 2'¦   (&   T  `  &'¦ `  & '¦  ) ¡    `     r sg V       ¡     ¦S   9     Q  © ¦      9  0               &  #      Qh ¡  (8)  ` T  4 ! [sent-124, score-0.032]
</p><p>53 2 Maximum a posteriori estimation with the EM algorithm The EM formulation for MAP estimation is straightforward. [sent-127, score-0.037]
</p><p>54   )     )    SONG Moby – Porcelain Nine Inch Nails – Terrible Lie other – ’Addams Family’ theme . [sent-134, score-0.1]
</p><p>55 2753  Figure 2: Representative probabilistic cluster allocations using MAP estimation. [sent-181, score-0.18]
</p><p>56 These expressions can also be derived by considering the posterior modes and by replacing the cluster indicator variable with its posterior estimate . [sent-182, score-0.18]
</p><p>57   ¡      4 Experiments To test the model with text and music, we clustered a database of musical scores with associated text documents. [sent-184, score-0.967]
</p><p>58 The database is composed of various types of musical scores – jazz, classical, television theme songs, and contemporary pop music – as well as associated text ﬁles. [sent-185, score-1.242]
</p><p>59 The associated text ﬁles are a song’s lyrics, where applicable, or textual commentary on the score for instrumental pieces, all of which were extracted from the World Wide Web. [sent-187, score-0.284]
</p><p>60 The experimental database contains 100 scores, each with a single associated text document. [sent-188, score-0.321]
</p><p>61 There is nothing in the model, however, that requires this one-to-one association of text documents and scores – this was done solely for testing simplicity and efﬁciency. [sent-189, score-0.571]
</p><p>62 In a deployment such as the world wide web, one would routinely expect one-to-many or many-to-many mappings between the scores and text. [sent-190, score-0.142]
</p><p>63 Figure 2 shows some representative cluster probability assignments obtained with MAP estimation. [sent-194, score-0.15]
</p><p>64   T    4  X  V  T    4  X  V  T W  4  V  T    By and large, the MAP clusters are intuitive. [sent-195, score-0.037]
</p><p>65 Bach each have very high ( ) probabilities of membership in the same cluster. [sent-198, score-0.032]
</p><p>66 The Beatles’ song The Yellow Submarine is included in the same cluster as the Bach pieces, though all the other Beatles songs in the database are assigned to other clusters. [sent-200, score-0.574]
</p><p>67 1 Demonstrating the utility of multi-modal queries A major intended use of the text-score model is for searching documents on a combination of text and music. [sent-202, score-0.431]
</p><p>68 Consider a hypothetical example, using our database: A music fan is struggling to recall a dimly-remembered song with a strong repeating single-pitch, dotted-eight-note/sixteenthnote bass line, and lyrics containing the words come on, come on, get down. [sent-203, score-1.041]
</p><p>69 A search on the text portion alone turns up four documents which contain the lyrics. [sent-204, score-0.383]
</p><p>70 A search on the notes alone returns seven documents which have matching transitions. [sent-205, score-0.254]
</p><p>71 But a combined search returns only the correct document (ﬁgure 3). [sent-206, score-0.093]
</p><p>72 2 Precision and recall We evaluated our retrieval system with randomly generated queries. [sent-210, score-0.053]
</p><p>73 A query is composed of a random series of 1 to 5 note transitions, and 1 to 5 words, . [sent-211, score-0.144]
</p><p>74 We then determine the actual number of matches in the database, where a match is deﬁned as a song such that all elements of and have a frequency of 1 or greater. [sent-212, score-0.232]
</p><p>75 In order to avoid skewing the results unduly, we reject any query that has or . [sent-213, score-0.144]
</p><p>76 ¢ £  X    ¡   ¨ ©  g  ¦ ¤ §¥ g  ¢  g             0  To perform a query, we simply sample probabilistically without replacement from the clusters. [sent-214, score-0.033]
</p><p>77 If a cluster contains no items or later becomes empty, it is assigned a sampling probability of zero, and the probabilities of the remaining clusters are re-normalized. [sent-216, score-0.187]
</p><p>78 ¡    Q9  In each iteration , a cluster is selected, and the matching criteria are applied against each ! [sent-217, score-0.211]
</p><p>79 piece of music that has been assigned to that cluster until a match is found. [sent-218, score-0.708]
</p><p>80 If no match is found, an arbitrary piece is selected. [sent-219, score-0.084]
</p><p>81 The selected piece is returned as the rank- result. [sent-220, score-0.084]
</p><p>82 Once all the matches have been returned, we compute the standard precision-recall curve [9], as shown in Figure 4. [sent-221, score-0.032]
</p><p>83 ¡  ¢  Our querying method enjoys a high precision until recall is approximately , and experiences a relatively modest deterioration of precision thereafter. [sent-223, score-0.166]
</p><p>84 By choosing clusters before £ ¤X  Figure 4: Precision-recall curve showing average results, over 1000 randomly-generated queries, combining music and text matching criteria. [sent-224, score-0.82]
</p><p>85 For example, river banks and money banks appear in separate clusters. [sent-226, score-0.1]
</p><p>86 3 Association The probabilistic nature of our approach allows us the ﬂexibility to use our techniques and database for tasks beyond traditional querying. [sent-229, score-0.103]
</p><p>87 One of the more promising avenues of exploration is associating documents with each other probabilistically. [sent-230, score-0.172]
</p><p>88 This could be used, for example, to ﬁnd suitable songs for web sites or presentations (matching on text), or for recommending songs similar to one a user enjoys (matching on scores). [sent-231, score-0.388]
</p><p>89 Given an input document, , we ﬁrst cluster by ﬁnding the most likely cluster as determined by computing (equation 3). [sent-232, score-0.3]
</p><p>90 Input documents containing text or music only can be clustered using only those components of the database. [sent-233, score-0.908]
</p><p>91 Input documents that combine text and music are clustered using all the data. [sent-234, score-0.908]
</p><p>92 We can then ﬁnd the closest association by computing the distance from the input document to the other document vectors in the cluster using a similarity metric such as Euclidean distance, or cosine measures after carrying out latent semantic indexing [10]. [sent-235, score-0.462]
</p><p>93         5 Conclusions We feel that the probabilistic approach to querying on music and text presented here is powerful, ﬂexible, and novel, and suggests many interesting areas of future research. [sent-239, score-0.878]
</p><p>94 Bach – Invention #5 Nine Inch Nails – I Do Not Want This The Cure – One Hundred Years  Figure 5: The results of associating songs in the database with other text and/or musical input. [sent-246, score-0.714]
</p><p>95 The input is clustered probabilistically and then associated with the existing song that has the least Euclidean distance in that cluster. [sent-247, score-0.284]
</p><p>96 The association of The Wasteland with The Cure’s thematically similar One Hundred Years is likely due to the high co-occurance of relatively uncommon words such as water, death, and year(s). [sent-248, score-0.081]
</p><p>97 This will permit querying by singing, humming, or via recorded music. [sent-250, score-0.126]
</p><p>98 There are a number of ways of combining our method with images [6, 4], opening up room for novel applications in multimedia [11]. [sent-251, score-0.144]
</p><p>99 A comparison of language modeling and probabilistic text information retrieval approaches to monophonic music retrieval. [sent-259, score-0.855]
</p><p>100 The sound of an album cover: Probabilistic multimedia and IR. [sent-265, score-0.08]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('music', 0.474), ('beatles', 0.252), ('text', 0.248), ('musical', 0.205), ('song', 0.2), ('lyrics', 0.176), ('guido', 0.153), ('songs', 0.151), ('cluster', 0.15), ('query', 0.144), ('scores', 0.142), ('documents', 0.135), ('moby', 0.126), ('querying', 0.126), ('bach', 0.123), ('ml', 0.121), ('allocation', 0.108), ('inch', 0.101), ('invention', 0.101), ('nails', 0.101), ('submarine', 0.101), ('theme', 0.1), ('document', 0.093), ('piece', 0.084), ('pieces', 0.084), ('multimedia', 0.08), ('yellow', 0.08), ('bodyrock', 0.076), ('cure', 0.076), ('downie', 0.076), ('voice', 0.075), ('database', 0.073), ('barnard', 0.066), ('browsing', 0.066), ('dirichlet', 0.064), ('matching', 0.061), ('nine', 0.059), ('come', 0.059), ('notes', 0.058), ('got', 0.056), ('retrieval', 0.053), ('clustered', 0.051), ('banks', 0.05), ('brochu', 0.05), ('hoos', 0.05), ('midi', 0.05), ('monophonic', 0.05), ('moon', 0.05), ('polyphonic', 0.05), ('porcelain', 0.05), ('freitas', 0.05), ('map', 0.05), ('life', 0.048), ('queries', 0.048), ('mixture', 0.048), ('association', 0.046), ('web', 0.046), ('annotate', 0.044), ('djs', 0.044), ('nando', 0.044), ('recommend', 0.044), ('notation', 0.043), ('automatically', 0.043), ('latent', 0.043), ('enjoys', 0.04), ('rhythm', 0.04), ('databases', 0.039), ('multinomial', 0.039), ('em', 0.038), ('les', 0.038), ('get', 0.038), ('associating', 0.037), ('posteriori', 0.037), ('semantic', 0.037), ('clusters', 0.037), ('state', 0.037), ('rest', 0.036), ('score', 0.036), ('name', 0.035), ('british', 0.035), ('columbia', 0.035), ('words', 0.035), ('interval', 0.035), ('opening', 0.033), ('probabilistically', 0.033), ('save', 0.033), ('old', 0.032), ('markov', 0.032), ('contextual', 0.032), ('ir', 0.032), ('membership', 0.032), ('pitch', 0.032), ('sg', 0.032), ('vancouver', 0.032), ('exible', 0.032), ('matches', 0.032), ('hundred', 0.031), ('room', 0.031), ('indicator', 0.03), ('probabilistic', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="1-tfidf-1" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>2 0.17321685 <a title="1-tfidf-2" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>3 0.14501138 <a title="1-tfidf-3" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>Author: Gil-jin Jang, Te-Won Lee</p><p>Abstract: We present a new technique for achieving source separation when given only a single channel recording. The main idea is based on exploiting the inherent time structure of sound sources by learning a priori sets of basis ﬁlters in time domain that encode the sources in a statistically efﬁcient manner. We derive a learning algorithm using a maximum likelihood approach given the observed single channel data and sets of basis ﬁlters. For each time point we infer the source signals and their contribution factors. This inference is possible due to the prior knowledge of the basis ﬁlters and the associated coefﬁcient densities. A ﬂexible model for density estimation allows accurate modeling of the observation and our experimental results exhibit a high level of separation performance for mixtures of two music signals as well as the separation of two voice signals.</p><p>4 0.14309068 <a title="1-tfidf-4" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>5 0.11644772 <a title="1-tfidf-5" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>6 0.1152181 <a title="1-tfidf-6" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>7 0.11129015 <a title="1-tfidf-7" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>8 0.10828818 <a title="1-tfidf-8" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>9 0.10820072 <a title="1-tfidf-9" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>10 0.10478497 <a title="1-tfidf-10" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>11 0.079762302 <a title="1-tfidf-11" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>12 0.073096342 <a title="1-tfidf-12" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>13 0.068838045 <a title="1-tfidf-13" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>14 0.061774947 <a title="1-tfidf-14" href="./nips-2002-Real_Time_Voice_Processing_with_Audiovisual_Feedback%3A_Toward_Autonomous_Agents_with_Perfect_Pitch.html">170 nips-2002-Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch</a></p>
<p>15 0.061536424 <a title="1-tfidf-15" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>16 0.056923557 <a title="1-tfidf-16" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>17 0.052164026 <a title="1-tfidf-17" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>18 0.051935505 <a title="1-tfidf-18" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>19 0.05183683 <a title="1-tfidf-19" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>20 0.05166525 <a title="1-tfidf-20" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, -0.052), (2, 0.003), (3, 0.025), (4, -0.246), (5, 0.094), (6, -0.112), (7, -0.169), (8, 0.089), (9, -0.113), (10, -0.103), (11, -0.02), (12, -0.073), (13, -0.039), (14, 0.063), (15, -0.021), (16, -0.039), (17, -0.04), (18, 0.014), (19, 0.079), (20, 0.016), (21, 0.02), (22, 0.003), (23, 0.048), (24, -0.07), (25, 0.02), (26, -0.053), (27, 0.076), (28, 0.018), (29, -0.064), (30, -0.056), (31, -0.052), (32, -0.05), (33, 0.095), (34, -0.004), (35, 0.022), (36, -0.175), (37, 0.082), (38, 0.017), (39, 0.092), (40, -0.046), (41, 0.072), (42, 0.061), (43, -0.033), (44, -0.033), (45, -0.017), (46, 0.099), (47, 0.007), (48, -0.001), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96191418 <a title="1-lsi-1" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>2 0.81798935 <a title="1-lsi-2" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>3 0.7464866 <a title="1-lsi-3" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>4 0.65422648 <a title="1-lsi-4" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>5 0.65405792 <a title="1-lsi-5" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>6 0.54422724 <a title="1-lsi-6" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>7 0.51048785 <a title="1-lsi-7" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>8 0.44189087 <a title="1-lsi-8" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>9 0.3827768 <a title="1-lsi-9" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>10 0.38162908 <a title="1-lsi-10" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>11 0.35450152 <a title="1-lsi-11" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>12 0.35310218 <a title="1-lsi-12" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>13 0.34978682 <a title="1-lsi-13" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>14 0.34819731 <a title="1-lsi-14" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>15 0.32806903 <a title="1-lsi-15" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>16 0.31576702 <a title="1-lsi-16" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>17 0.30625951 <a title="1-lsi-17" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>18 0.30524874 <a title="1-lsi-18" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>19 0.2997973 <a title="1-lsi-19" href="./nips-2002-A_Hierarchical_Bayesian_Markovian_Model_for_Motifs_in_Biopolymer_Sequences.html">7 nips-2002-A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences</a></p>
<p>20 0.29232392 <a title="1-lsi-20" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.046), (14, 0.014), (23, 0.022), (40, 0.366), (42, 0.081), (54, 0.085), (55, 0.022), (57, 0.011), (67, 0.031), (68, 0.03), (74, 0.097), (92, 0.024), (98, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80728781 <a title="1-lda-1" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>2 0.70067507 <a title="1-lda-2" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>3 0.42752796 <a title="1-lda-3" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>4 0.42620003 <a title="1-lda-4" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers</p><p>Abstract: We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 1</p><p>5 0.42336321 <a title="1-lda-5" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>6 0.42162892 <a title="1-lda-6" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>7 0.41876426 <a title="1-lda-7" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>8 0.41848442 <a title="1-lda-8" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>9 0.41832003 <a title="1-lda-9" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>10 0.41729409 <a title="1-lda-10" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>11 0.41625738 <a title="1-lda-11" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>12 0.4154768 <a title="1-lda-12" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>13 0.41457501 <a title="1-lda-13" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>14 0.41226923 <a title="1-lda-14" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>15 0.41122851 <a title="1-lda-15" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>16 0.41072211 <a title="1-lda-16" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>17 0.41023675 <a title="1-lda-17" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>18 0.41005981 <a title="1-lda-18" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>19 0.40961581 <a title="1-lda-19" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>20 0.40929514 <a title="1-lda-20" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
