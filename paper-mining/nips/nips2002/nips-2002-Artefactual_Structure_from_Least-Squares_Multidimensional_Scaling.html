<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-34" href="#">nips2002-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</h1>
<br/><p>Source: <a title="nips-2002-34-pdf" href="http://papers.nips.cc/paper/2239-artefactual-structure-from-least-squares-multidimensional-scaling.pdf">pdf</a></p><p>Author: Nicholas P. Hughes, David Lowe</p><p>Abstract: We consider the problem of illusory or artefactual structure from the visualisation of high-dimensional structureless data. In particular we examine the role of the distance metric in the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling. We show that the use of a squared Euclidean metric (i.e. the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution, and we provide a theoretical justiﬁcation for this observation.</p><p>Reference: <a title="nips-2002-34-reference" href="../nips2002_reference/nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We consider the problem of illusory or artefactual structure from the visualisation of high-dimensional structureless data. [sent-8, score-0.727]
</p><p>2 In particular we examine the role of the distance metric in the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling. [sent-9, score-0.479]
</p><p>3 We show that the use of a squared Euclidean metric (i. [sent-10, score-0.214]
</p><p>4 the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution, and we provide a theoretical justiﬁcation for this observation. [sent-12, score-0.426]
</p><p>5 One approach to the aforementioned problem then is to ﬁnd a faithful1 representation of the data in a lower dimensional space. [sent-15, score-0.059]
</p><p>6 Typically this space is chosen to be two- or three-dimensional, thus facilitating the visualisation and exploratory analysis of the intrinsic low-dimensional structure in the data (which would otherwise be masked by the dimensionality of the data space). [sent-16, score-0.436]
</p><p>7 In this context then, an effective dimensionality reduction algorithm should seek to extract the underlying relationships in the data with minimum loss of information. [sent-17, score-0.107]
</p><p>8 Conversely, any interesting patterns which are present in the visualisation space should be representative of similar patterns in the original data space, and not artefacts of the dimensionality reduction process. [sent-18, score-0.41]
</p><p>9 1  By “faithful” we mean that the underlying geometric structure in the data space, which characterises the informative relationships in the data, is preserved in the visualisation space. [sent-19, score-0.391]
</p><p>10 Although much effort has been focused on the former problem of optimal structure elucidation (see [7, 10] for recent approaches to dimensionality reduction), comparatively little work has been undertaken on the latter (and equally important) problem of artefactual structure. [sent-20, score-0.354]
</p><p>11 This shortcoming was recently highlighted in a controversial example of the application of visualisation techniques to neuroanatomical connectivity data derived from the primate visual cortex [12, 9, 13, 3]. [sent-21, score-0.376]
</p><p>12 In this paper we attempt to redress the balance by considering the visualisation of highdimensional structureless data through the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling (MDS). [sent-22, score-0.839]
</p><p>13 This is an important class of mappings which have recently been brought into the neural network domain [5], and have signiﬁcant connections to modern kernel-based algorithms such as kernel PCA [11]. [sent-23, score-0.097]
</p><p>14 The organisation of the remainder of this paper is as follows: In section 2 we introduce the technique of multidimensional scaling and relate this to the ﬁeld of topographic mappings. [sent-24, score-0.296]
</p><p>15 In section 3 we show how under certain conditions such mappings can give rise to artefactual structure. [sent-25, score-0.399]
</p><p>16 A theoretical analysis of this effect is then presented in section 4. [sent-26, score-0.026]
</p><p>17 2 Multidimensional Scaling and Topographic Mappings The visualisation of experimental data which is characterised by pairwise proximity values is a common problem in areas such as psychology, molecular biology and linguistics. [sent-27, score-0.408]
</p><p>18 Multidimensional scaling (MDS) is a statistical technique which can be used to construct a spatial conﬁguration of points in a (typically) two- or three-dimensional space given a matrix of pairwise proximity values between objects. [sent-28, score-0.263]
</p><p>19 The proximity matrix provides a measure of the similarity or dissimilarity between the objects, and the geometric layout of the resulting MDS conﬁguration reﬂects the relationships between the objects as deﬁned by this matrix. [sent-29, score-0.22]
</p><p>20 In this way the information contained within the proximity matrix can be captured by a more succinct spatial model which aids visualisation of the data and improves understanding of the processes that generated it. [sent-30, score-0.406]
</p><p>21 In many situations, the raw dissimilarities will not be representative of actual inter-point distances between the objects, and thus will not be suitable for embedding in a lowdimensional space. [sent-31, score-0.256]
</p><p>22 The aim of metric MDS then is that the transformed dissimilarities should correspond as closely as possible to the inter-point disin the resulting conﬁguration2. [sent-33, score-0.243]
</p><p>23 tances ¥£ ¡ ¢  ¥ £    Metric MDS can be formulated as a continuous optimisation problem through the deﬁnition of an appropriate error function. [sent-34, score-0.096]
</p><p>24 In particular, least squares scaling algorithms directly seek to minimise the sum-of-squares error between the disparities and the inter-point distances. [sent-35, score-0.367]
</p><p>25   §  S TRESS  (1)  2 This is in contrast to nonmetric MDS which requires that only the ordering of the disparities corresponds to the ordering of the inter-point distances (and thus that the disparities are some arbitrary monotonically increasing function of the distances). [sent-37, score-0.412]
</p><p>26 ¥ £  where the term is a normalising constant which reduces the sensitivity of the measure to the number of points and the scaling of the disparities, and the are the weighting factors. [sent-40, score-0.243]
</p><p>27 It is straightforward to differentiate this S TRESS measure with respect to the conﬁguration points and minimise the error through the use of standard nonlinear optimisation techniques. [sent-41, score-0.261]
</p><p>28 ¥ £  (  £    ¡  ¢  An alternative and commonly used error function, which is referred to as SS TRESS, is given by: 5 4 ¦"¥ £  2 "¥ £ ¡ ¢ 0   £ $ "¥ £ ¡ ¢ ¥ £ ! [sent-42, score-0.049]
</p><p>29   £ % '&¥$  §  SS TRESS  (2)  which represents the sum-of-squares error between squared disparities and squared distances. [sent-43, score-0.37]
</p><p>30 The primary advantage of the SS TRESS measure is that it can be efﬁciently minimised through the use of an alternating least squares procedure4 [1]. [sent-44, score-0.144]
</p><p>31 Closely related to the ﬁeld of Metric MDS is Sammon’s mapping [8], which takes as its input a set of high-dimensional vectors and seeks to produce a set of lower dimensional vectors such that the following error measure is minimised: 5   ¦£ ¥   ¥ £    ¥ 2 £       £ $  ¥ £ ¥£ ! [sent-45, score-0.164]
</p><p>32   £ &¥ $ %   ¨ ¨ ¦¤ § ©§¥£    ¥  ¥  2 £  § £   are the inter-point Euclidean distances in the data space: , are the corresponding inter-point Euclidean distances in the feature or map . [sent-46, score-0.344]
</p><p>33 § ¥ £ ¥  £    Ignoring the normalising constant, Sammon’s mapping is thus equivalent to least squares metric MDS with the disparities taken to be the raw inter-point distances in the data space and the weighting factors given by . [sent-48, score-0.59]
</p><p>34 Lowe (1993) termed such a mapping based on the minimisation of an error measure of the form a topographic mapping, since this constraint “optimally preserves the geometric structure in the data” [5]. [sent-49, score-0.349]
</p><p>35 ¥ £    "    §  ¥ £  (  Interestingly the choice of the S TRESS or SS TRESS measure in MDS has a more natural interpretation when viewed within the framework of Sammon’s mapping. [sent-51, score-0.047]
</p><p>36 In particular, S TRESS corresponds to the use of the standard Euclidean distance metric whereas SS TRESS corresponds to the use of the squared Euclidean distance metric. [sent-52, score-0.274]
</p><p>37 In the next section we show that this choice of metric can lead to markedly different results when the input data is sampled from a high-dimensional isotropic distribution. [sent-53, score-0.308]
</p><p>38 Such data can be generated by sampling from an isotropic distribution (such as a spherical Gaussian), which is characterised by a covariance matrix that is proportional to the identity matrix, and a skewness of zero. [sent-55, score-0.317]
</p><p>39 We created four structureless data sets by randomly sampling 1000 i. [sent-56, score-0.155]
</p><p>40 points from unit hypercubes of dimensions 5, 10, 30 and 100. [sent-59, score-0.14]
</p><p>41 For each data set, we generated a pair §  #  4  The SS TRESS measure now forms the basis of the ALSCAL implementation of MDS, which is included as part of the SPSS software package for statistical data analysis. [sent-60, score-0.109]
</p><p>42 2  Figure 1: Final map conﬁgurations produced by S TRESS mappings of data uniformly randomly distributed in unit hypercubes of dimension . [sent-88, score-0.327]
</p><p>43 #  of 2-D conﬁgurations by minimising5 S TRESS and SS TRESS error measures of the form and respectively. [sent-89, score-0.049]
</p><p>44 The process was repeated ﬁfty times (for each individual error function and data set) using different initial conﬁgurations of the map points, and the conﬁguration with the lowest ﬁnal error was retained. [sent-90, score-0.246]
</p><p>45 ¥ £  As previously noted, the choice of the S TRESS or SS TRESS error measure is best viewed as a choice of distance metric, where S TRESS corresponds to the standard Euclidean metric and SS TRESS corresponds to the squared Euclidean metric. [sent-93, score-0.34]
</p><p>46 It is clear that each conﬁguration has captured the isotropic nature of the associated data set, and there are no spurious patterns or clusters evident in the ﬁnal visualisation plots. [sent-95, score-0.52]
</p><p>47 2  Figure 2: Final map conﬁgurations produced by SS TRESS mappings of data uniformly randomly distributed in unit hypercubes of dimension . [sent-131, score-0.327]
</p><p>48 The conﬁgurations exhibit signiﬁcant artefactual structure, which is characterised by a tendency for the map points to cluster in a circular fashion. [sent-133, score-0.648]
</p><p>49 Furthermore, the degree of clustering increases with increasing dimensionality of the data space (and is clearly evident for as low as 10). [sent-134, score-0.141]
</p><p>50 #  #  Although the tendency for SS TRESS conﬁgurations to cluster in a circular fashion has been noted in the MDS literature [2], the connection between artefactual structure and the choice of distance metric has not been made. [sent-135, score-0.65]
</p><p>51 Indeed, in the next section we show analytically that the use of the squared Euclidean metric leads to a globally optimal solution corresponding to an annular structure. [sent-136, score-0.288]
</p><p>52 To date, the most signiﬁcant work on this problem is that of Klock and Buhmann [4], who proposed a novel transformation of the dissimilarities (i. [sent-137, score-0.112]
</p><p>53 the squared inter-point distances 5  We used a conjugate gradients optimisation algorithm. [sent-139, score-0.227]
</p><p>54 in the data space) such that “the ﬁnal disparities are more suitable for Euclidean embedding”. [sent-140, score-0.211]
</p><p>55 However this transformation assumes that the input data are drawn from a spherical Gaussian distribution6 , which is inappropriate for most real-world data sets of interest. [sent-141, score-0.153]
</p><p>56 4 Theoretical Analysis of Artefactual Structure In this section we present a theoretical analysis of the artefactual structure problem. [sent-142, score-0.343]
</p><p>57 A dimensional map conﬁguration is considered to be the result of a SS TRESS mapping of a data set of i. [sent-143, score-0.216]
</p><p>58 points drawn from a dimensional isotropic distribution (where ). [sent-146, score-0.285]
</p><p>59 T The set of data points is given by the x matrix and similarly T the set of map points is given by the x matrix . [sent-147, score-0.374]
</p><p>60 ¥$  ¢  £    2    ¡  2 6£  ¢  GI H £ ¢  £  ¢  2 £ £   In this case the squared inter-point distances will follow a  T   ¥  £  § 6 ¥  £   £  ¢  " £ ! [sent-167, score-0.18]
</p><p>61 ¥$ § 8£ 2 ¢  ¢  T  T  ¥   £  6  T  ¢ ¢  Thus at a stationary point of the error (i. [sent-169, score-0.1]
</p><p>62 ¥$  T  T  ¢ £  T  T  ¥  T  ¥  Equation (4) can therefore be expanded to: T     We begin by deﬁning the derivative of the SS TRESS error measure with respect to a particular map vector :  ¢  1  2 6£  " ! [sent-173, score-0.213]
</p><p>63 ¥$ £  GI ¢  2   %  §  ¡  £  Since the error is a function of the inter-point distances only, we can centre both the data points and the map points on the origin without loss of generality. [sent-177, score-0.457]
</p><p>64 For large we have:  ¥  ¥   ¨ ¦ ©¡  T  tr   ¨ ¦  ¥  ¥   ¡  " ! [sent-178, score-0.092]
</p><p>65 ¥$ £  T    ¦   ¥   ¡     ¨ ¦   ¡  ¥  ¡  ¨ ¦ ©§¡  ¡  ¢ £ ¥  ¥  ¢ ¥  ¡  tr  ¦ ! [sent-182, score-0.092]
</p><p>66 ¨  where is the x zero matrix, is the covariance matrix of the map vectors, is the covariance matrix of the map vectors and the data vectors, and tr is the matrix trace operator. [sent-187, score-0.453]
</p><p>67 ¥$  £   ¨ ¦   2     # $£    tr  T  ¢    # ¡  £  ¢  £  ¢  £  T  ¥  % 2 46£   ¨ ¦  2 £ £  T  ¢ ¢  %  ¨ ¦  §  This represents a general expression for the value of the map vector at a stationary point of the SS TRESS error, regardless of the nature of the input data distribution. [sent-191, score-0.337]
</p><p>68 However we are interested in the case where the input data is drawn from a high-dimensional isotropic distribution. [sent-192, score-0.207]
</p><p>69 ¢  If the data space is isotropic then a stationary point of the error will correspond to a similarly isotropic map space7 . [sent-193, score-0.561]
</p><p>70 Thus, at a stationary point, we have for large :  2  ) # #  "  ) #  "  and  "  "  %   ¡  (¦   " #  ¤ ! [sent-194, score-0.051]
</p><p>71 £E ¡ ¨ ¦ ¢ & # ¡ ¨ '% $"©¦ % #  2  ¨  ©¦   ¢  where is the x identity matrix, and the data space respectively. [sent-195, score-0.054]
</p><p>72 ¢  tr  ¡  tr  are the variances in the map space and       &  Finally, consider the expression:  2 ! [sent-196, score-0.355]
</p><p>73 ¥$  2     ¡  The ﬁrst term is the third order moment, which is zero for an isotropic distribution [6]. [sent-199, score-0.145]
</p><p>74 ¥$    2    ¡  7  T  (7)  This is true regardless of the initial distribution of the map points, although a highly non-uniform initial conﬁguration would take signiﬁcantly longer to reach a local minimum of the error function. [sent-206, score-0.189]
</p><p>75 ¢ F§ E  T  (8)  ¤  % #  Thus, for large , the variance of the map points is related to the variance of the data points by a factor of . [sent-208, score-0.31]
</p><p>76 Table 1 shows the values of the observed and predicted map variances for 1000 data points sampled randomly from uniform distributions in the interval (i. [sent-209, score-0.283]
</p><p>77 Clearly as the dimension of the data space increases, so too does the accuracy of the approximation given by equation (7), and therefore the accuracy of equation (8). [sent-212, score-0.077]
</p><p>78 823  "  % #  ¡     3    §  #  #  Number of points 1000 1000 1000 1000  ¤  ¦ ¤ ¦#¥ ¢  #  "  ) #  Dimension 5 10 30 100  predicted 0. [sent-217, score-0.104]
</p><p>79 4%  Table 1: A comparison of the predicted and observed map variances. [sent-225, score-0.14]
</p><p>80 We can show that this mismatch in variances in the two spaces results in the map points clustering in a circular fashion by considering the expected squared distance of the map of the annulus): points from the origin (i. [sent-226, score-0.652]
</p><p>81 % "#  ¦    ¢      §  &%  '#$ 5   5¦   separates since and will be uncorrelated due to the where the expectation over isotropic nature of . [sent-231, score-0.168]
</p><p>82 In general for a -dimensional map space we have that . [sent-232, score-0.14]
</p><p>83 the optimal conﬁguration will be an annulus or ring shape, as observed  5 Conclusions We have investigated the problem or artefactual or illusory structure from topographic mappings based upon least squares scaling algorithms from multidimensional scaling. [sent-235, score-0.846]
</p><p>84 In particular we have shown that the use of a squared Euclidean distance metric (i. [sent-236, score-0.244]
</p><p>85 the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution. [sent-238, score-0.4]
</p><p>86 A theoretical analysis of this problem was presented and a simple relationship between the variance of the map and the data points was derived. [sent-239, score-0.255]
</p><p>87 Finally we showed that this relationship results in an optimal conﬁguration which is characterised by the map points clustering in a circular fashion. [sent-240, score-0.367]
</p><p>88 An evaluation of the use of multidimensional scaling for understanding brain connectivity. [sent-262, score-0.206]
</p><p>89 Neuroscale: Novel topographic feature extraction with radial basis function networks. [sent-280, score-0.09]
</p><p>90 On a connection between kernel PCA and metric multidimensional scaling. [sent-340, score-0.289]
</p><p>91 Objective analysis of the topological organization of the primate cortical visual system. [sent-351, score-0.049]
</p><p>92 Non-metric multidimensional scaling in the analysis of neuroanatomical connection data and the organization of the primate cortical visual system. [sent-364, score-0.362]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tress', 0.624), ('artefactual', 0.272), ('ss', 0.262), ('visualisation', 0.247), ('mds', 0.173), ('disparities', 0.157), ('isotropic', 0.145), ('guration', 0.136), ('metric', 0.132), ('multidimensional', 0.13), ('structureless', 0.124), ('map', 0.117), ('gurations', 0.103), ('euclidean', 0.102), ('distances', 0.098), ('mappings', 0.097), ('tr', 0.092), ('con', 0.091), ('topographic', 0.09), ('squared', 0.082), ('dissimilarities', 0.082), ('points', 0.081), ('characterised', 0.079), ('scaling', 0.076), ('annular', 0.074), ('sammon', 0.074), ('circular', 0.066), ('lowe', 0.065), ('hypercubes', 0.059), ('proximity', 0.051), ('stationary', 0.051), ('error', 0.049), ('annulus', 0.049), ('klock', 0.049), ('minimised', 0.049), ('neuroanatomical', 0.049), ('simmen', 0.049), ('primate', 0.049), ('squares', 0.048), ('optimisation', 0.047), ('measure', 0.047), ('structure', 0.045), ('highdimensional', 0.044), ('goodhill', 0.043), ('mapping', 0.04), ('illusory', 0.039), ('normalising', 0.039), ('relationships', 0.039), ('dimensionality', 0.037), ('philosophical', 0.037), ('minimise', 0.037), ('pq', 0.033), ('tendency', 0.033), ('matrix', 0.032), ('variances', 0.031), ('embedding', 0.031), ('drawn', 0.031), ('editors', 0.031), ('data', 0.031), ('transformation', 0.03), ('spherical', 0.03), ('rise', 0.03), ('distance', 0.03), ('geometric', 0.029), ('transformed', 0.029), ('dimensional', 0.028), ('connection', 0.027), ('termed', 0.027), ('theoretical', 0.026), ('evident', 0.026), ('gi', 0.026), ('patterns', 0.025), ('discovery', 0.025), ('final', 0.024), ('clustering', 0.024), ('eld', 0.024), ('space', 0.023), ('nature', 0.023), ('predicted', 0.023), ('regardless', 0.023), ('suitable', 0.023), ('captured', 0.023), ('fashion', 0.023), ('cant', 0.023), ('dimension', 0.023), ('objects', 0.022), ('ma', 0.022), ('noted', 0.022), ('transactions', 0.022), ('manifold', 0.022), ('raw', 0.022), ('aston', 0.022), ('birmingham', 0.022), ('aids', 0.022), ('artefacts', 0.022), ('facilitating', 0.022), ('kent', 0.022), ('mardia', 0.022), ('minimisation', 0.022), ('multichannel', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="34-tfidf-1" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>Author: Nicholas P. Hughes, David Lowe</p><p>Abstract: We consider the problem of illusory or artefactual structure from the visualisation of high-dimensional structureless data. In particular we examine the role of the distance metric in the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling. We show that the use of a squared Euclidean metric (i.e. the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution, and we provide a theoretical justiﬁcation for this observation.</p><p>2 0.12440532 <a title="34-tfidf-2" href="./nips-2002-Global_Versus_Local_Methods_in_Nonlinear_Dimensionality_Reduction.html">97 nips-2002-Global Versus Local Methods in Nonlinear Dimensionality Reduction</a></p>
<p>Author: Vin D. Silva, Joshua B. Tenenbaum</p><p>Abstract: Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.</p><p>3 0.11316101 <a title="34-tfidf-3" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>4 0.09497112 <a title="34-tfidf-4" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>Author: Martin J. Wainwright, Tommi S. Jaakkola, Alan S. Willsky</p><p>Abstract: We describe a method for computing provably exact maximum a posteriori (MAP) estimates for a subclass of problems on graphs with cycles. The basic idea is to represent the original problem on the graph with cycles as a convex combination of tree-structured problems. A convexity argument then guarantees that the optimal value of the original problem (i.e., the log probability of the MAP assignment) is upper bounded by the combined optimal values of the tree problems. We prove that this upper bound is met with equality if and only if the tree problems share an optimal conﬁguration in common. An important implication is that any such shared conﬁguration must also be the MAP conﬁguration for the original problem. Next we develop a tree-reweighted max-product algorithm for attempting to ﬁnd convex combinations of tree-structured problems that share a common optimum. We give necessary and sufﬁcient conditions for a ﬁxed point to yield the exact MAP estimate. An attractive feature of our analysis is that it generalizes naturally to convex combinations of hypertree-structured distributions.</p><p>5 0.088588931 <a title="34-tfidf-5" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>Author: Volker Roth, Julian Laub, Klaus-Robert Müller, Joachim M. Buhmann</p><p>Abstract: Pairwise data in empirical sciences typically violate metricity, either due to noise or due to fallible estimates, and therefore are hard to analyze by conventional machine learning technology. In this paper we therefore study ways to work around this problem. First, we present an alternative embedding to multi-dimensional scaling (MDS) that allows us to apply a variety of classical machine learning and signal processing algorithms. The class of pairwise grouping algorithms which share the shift-invariance property is statistically invariant under this embedding procedure, leading to identical assignments of objects to clusters. Based on this new vectorial representation, denoising methods are applied in a second step. Both steps provide a theoretically well controlled setup to translate from pairwise data to the respective denoised metric representation. We demonstrate the practical usefulness of our theoretical reasoning by discovering structure in protein sequence data bases, visibly improving performance upon existing automatic methods. 1</p><p>6 0.066071965 <a title="34-tfidf-6" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>7 0.064711034 <a title="34-tfidf-7" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<p>8 0.056931816 <a title="34-tfidf-8" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>9 0.05678523 <a title="34-tfidf-9" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>10 0.054890849 <a title="34-tfidf-10" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>11 0.054586183 <a title="34-tfidf-11" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>12 0.048084334 <a title="34-tfidf-12" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>13 0.047003329 <a title="34-tfidf-13" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>14 0.045107935 <a title="34-tfidf-14" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>15 0.04475205 <a title="34-tfidf-15" href="./nips-2002-Automatic_Alignment_of_Local_Representations.html">36 nips-2002-Automatic Alignment of Local Representations</a></p>
<p>16 0.044547107 <a title="34-tfidf-16" href="./nips-2002-Manifold_Parzen_Windows.html">138 nips-2002-Manifold Parzen Windows</a></p>
<p>17 0.042559374 <a title="34-tfidf-17" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>18 0.041251533 <a title="34-tfidf-18" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>19 0.04120332 <a title="34-tfidf-19" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>20 0.039579619 <a title="34-tfidf-20" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, -0.024), (2, 0.009), (3, 0.025), (4, -0.057), (5, 0.038), (6, -0.029), (7, -0.06), (8, -0.088), (9, 0.144), (10, 0.06), (11, 0.006), (12, -0.017), (13, -0.028), (14, 0.043), (15, 0.112), (16, -0.048), (17, 0.039), (18, -0.001), (19, -0.083), (20, -0.042), (21, 0.082), (22, -0.004), (23, 0.012), (24, -0.028), (25, -0.065), (26, 0.1), (27, 0.053), (28, -0.018), (29, -0.025), (30, -0.017), (31, -0.011), (32, -0.034), (33, 0.106), (34, 0.038), (35, -0.072), (36, -0.069), (37, 0.01), (38, -0.081), (39, 0.06), (40, 0.008), (41, -0.083), (42, 0.184), (43, 0.183), (44, 0.073), (45, -0.105), (46, -0.108), (47, 0.125), (48, -0.121), (49, 0.187)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94292921 <a title="34-lsi-1" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>Author: Nicholas P. Hughes, David Lowe</p><p>Abstract: We consider the problem of illusory or artefactual structure from the visualisation of high-dimensional structureless data. In particular we examine the role of the distance metric in the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling. We show that the use of a squared Euclidean metric (i.e. the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution, and we provide a theoretical justiﬁcation for this observation.</p><p>2 0.59545922 <a title="34-lsi-2" href="./nips-2002-Global_Versus_Local_Methods_in_Nonlinear_Dimensionality_Reduction.html">97 nips-2002-Global Versus Local Methods in Nonlinear Dimensionality Reduction</a></p>
<p>Author: Vin D. Silva, Joshua B. Tenenbaum</p><p>Abstract: Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.</p><p>3 0.51804161 <a title="34-lsi-3" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>4 0.50994682 <a title="34-lsi-4" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>Author: Balázs Kégl</p><p>Abstract: We propose a new algorithm to estimate the intrinsic dimension of data sets. The method is based on geometric properties of the data and requires neither parametric assumptions on the data generating model nor input parameters to set. The method is compared to a similar, widelyused algorithm from the same family of geometric techniques. Experiments show that our method is more robust in terms of the data generating distribution and more reliable in the presence of noise. 1</p><p>5 0.49018842 <a title="34-lsi-5" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>Author: Elzbieta Pekalska, David Tax, Robert Duin</p><p>Abstract: Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identiﬁcation or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classiﬁer for dissimilarity representations. By the use of linear programming an efﬁcient one-class description can be found, based on a small number of prototype objects. This classiﬁer can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classiﬁer by Campbell and Bennett is given.</p><p>6 0.47226727 <a title="34-lsi-6" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>7 0.40855911 <a title="34-lsi-7" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>8 0.3955828 <a title="34-lsi-8" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>9 0.38149995 <a title="34-lsi-9" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>10 0.37074447 <a title="34-lsi-10" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>11 0.36842459 <a title="34-lsi-11" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>12 0.35436919 <a title="34-lsi-12" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<p>13 0.33917484 <a title="34-lsi-13" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>14 0.30223835 <a title="34-lsi-14" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>15 0.29340094 <a title="34-lsi-15" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>16 0.2918739 <a title="34-lsi-16" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>17 0.27985111 <a title="34-lsi-17" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>18 0.27322394 <a title="34-lsi-18" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>19 0.2716659 <a title="34-lsi-19" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>20 0.26011613 <a title="34-lsi-20" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.036), (23, 0.015), (42, 0.097), (43, 0.268), (54, 0.19), (55, 0.035), (67, 0.011), (68, 0.031), (74, 0.078), (92, 0.024), (98, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91819459 <a title="34-lda-1" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>Author: Jürgen Schmidhuber</p><p>Abstract: Given is a problem sequence and a probability distribution (the bias) on programs computing solution candidates. We present an optimally fast way of incrementally solving each task in the sequence. Bias shifts are computed by program preﬁxes that modify the distribution on their sufﬁxes by reusing successful code for previous tasks (stored in non-modiﬁable memory). No tested program gets more runtime than its probability times the total search time. In illustrative experiments, ours becomes the ﬁrst general system to learn a universal solver for arbitrary disk Towers of Hanoi tasks (minimal solution size ). It demonstrates the advantages of incremental learning by proﬁting from previously solved, simpler tasks involving samples of a simple context free language.   ¦ ¤ ¢ §¥£¡ 1 Brief Introduction to Optimal Universal Search Consider an asymptotically optimal method for tasks with quickly veriﬁable solutions: ¦ ¦  ©  £ £¨ © © ©  © ¦ ¦ ¦   Method 1.1 (L SEARCH ) View the -th binary string as a potential program for a universal Turing machine. Given some problem, for all do: every steps on average execute (if possible) one instruction of the -th program candidate, until one of the programs has computed a solution.   !     © © © ¢</p><p>same-paper 2 0.8296771 <a title="34-lda-2" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>Author: Nicholas P. Hughes, David Lowe</p><p>Abstract: We consider the problem of illusory or artefactual structure from the visualisation of high-dimensional structureless data. In particular we examine the role of the distance metric in the use of topographic mappings based on the statistical ﬁeld of multidimensional scaling. We show that the use of a squared Euclidean metric (i.e. the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution, and we provide a theoretical justiﬁcation for this observation.</p><p>3 0.68196535 <a title="34-lda-3" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>Author: Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to deﬁne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “ﬁnance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.</p><p>4 0.68109965 <a title="34-lda-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.6809898 <a title="34-lda-5" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>6 0.67791802 <a title="34-lda-6" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>7 0.67569834 <a title="34-lda-7" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>8 0.67565042 <a title="34-lda-8" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>9 0.67485946 <a title="34-lda-9" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>10 0.67381251 <a title="34-lda-10" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>11 0.67341876 <a title="34-lda-11" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>12 0.67322135 <a title="34-lda-12" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>13 0.67316735 <a title="34-lda-13" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>14 0.6725142 <a title="34-lda-14" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>15 0.67132127 <a title="34-lda-15" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>16 0.6687004 <a title="34-lda-16" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>17 0.66846573 <a title="34-lda-17" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>18 0.66840059 <a title="34-lda-18" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>19 0.66769803 <a title="34-lda-19" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>20 0.66681218 <a title="34-lda-20" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
