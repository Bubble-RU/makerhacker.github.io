<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-96" href="#">nips2002-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</h1>
<br/><p>Source: <a title="nips-2002-96-pdf" href="http://papers.nips.cc/paper/2144-generalized2-linear2-models.pdf">pdf</a></p><p>Author: Geoffrey J. Gordon</p><p>Abstract: We introduce the Generalized 2 Linear 2 Model, a statistical estimator which combines features of nonlinear regression and factor analysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation j(g(A)h(B)). Here A and Bare low-rank matrices, while j, g, and h are link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family peA, the infomax formulation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new. 1</p><p>Reference: <a title="nips-2002-96-reference" href="../nips2002_reference/nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uv', 0.491), ('gl', 0.411), ('link', 0.271), ('loss', 0.264), ('corrid', 0.245), ('believ', 0.208), ('svd', 0.169), ('convex', 0.133), ('newton', 0.123), ('zi', 0.113), ('reconstruct', 0.104), ('yi', 0.086), ('pca', 0.084), ('robot', 0.084), ('match', 0.076), ('tanh', 0.073), ('fix', 0.071), ('matrix', 0.062), ('afrl', 0.062), ('bso', 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="96-tfidf-1" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: We introduce the Generalized 2 Linear 2 Model, a statistical estimator which combines features of nonlinear regression and factor analysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation j(g(A)h(B)). Here A and Bare low-rank matrices, while j, g, and h are link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family peA, the infomax formulation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new. 1</p><p>2 0.31362692 <a title="96-tfidf-2" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>3 0.12738736 <a title="96-tfidf-3" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>Author: Anat Levin, Assaf Zomet, Yair Weiss</p><p>Abstract: Certain simple images are known to trigger a percept of transparency: the input image I is perceived as the sum of two images I(x, y) = I1 (x, y) + I2 (x, y). This percept is puzzling. First, why do we choose the “more complicated” description with two images rather than the “simpler” explanation I(x, y) = I1 (x, y) + 0 ? Second, given the inﬁnite number of ways to express I as a sum of two images, how do we compute the “best” decomposition ? Here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative ﬁlters and “corner detectors” in natural scenes and use this model to ﬁnd the most probable decomposition of a novel image. The optimization is performed using loopy belief propagation. We show that our model computes perceptually “correct” decompositions on synthetic images and discuss its application to real images. 1</p><p>4 0.12145925 <a title="96-tfidf-4" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>5 0.12084243 <a title="96-tfidf-5" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>Author: Yasemin Altun, Thomas Hofmann, Mark Johnson</p><p>Abstract: This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields. Applications areas for the presented technique range from natural language processing and information extraction to computational biology. We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 1</p><p>6 0.10576189 <a title="96-tfidf-6" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>7 0.10276953 <a title="96-tfidf-7" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>8 0.10054359 <a title="96-tfidf-8" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>9 0.099099584 <a title="96-tfidf-9" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>10 0.095634244 <a title="96-tfidf-10" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>11 0.09500137 <a title="96-tfidf-11" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>12 0.087779336 <a title="96-tfidf-12" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>13 0.083998613 <a title="96-tfidf-13" href="./nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">111 nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>14 0.083280422 <a title="96-tfidf-14" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>15 0.081278265 <a title="96-tfidf-15" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>16 0.076083504 <a title="96-tfidf-16" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>17 0.073655322 <a title="96-tfidf-17" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>18 0.073584825 <a title="96-tfidf-18" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>19 0.072766446 <a title="96-tfidf-19" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>20 0.069585457 <a title="96-tfidf-20" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, 0.089), (2, -0.088), (3, 0.016), (4, 0.032), (5, -0.031), (6, -0.079), (7, -0.072), (8, -0.021), (9, -0.021), (10, -0.192), (11, -0.003), (12, -0.022), (13, -0.027), (14, 0.126), (15, -0.038), (16, 0.024), (17, -0.023), (18, -0.058), (19, 0.049), (20, -0.026), (21, 0.024), (22, 0.077), (23, 0.039), (24, -0.312), (25, 0.017), (26, 0.115), (27, 0.045), (28, 0.156), (29, -0.148), (30, 0.124), (31, -0.212), (32, -0.067), (33, -0.124), (34, 0.119), (35, -0.089), (36, 0.247), (37, 0.028), (38, 0.049), (39, -0.061), (40, 0.13), (41, -0.029), (42, -0.054), (43, -0.007), (44, -0.016), (45, -0.015), (46, 0.065), (47, 0.03), (48, -0.078), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94054914 <a title="96-lsi-1" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: We introduce the Generalized 2 Linear 2 Model, a statistical estimator which combines features of nonlinear regression and factor analysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation j(g(A)h(B)). Here A and Bare low-rank matrices, while j, g, and h are link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family peA, the infomax formulation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new. 1</p><p>2 0.75253707 <a title="96-lsi-2" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>3 0.62133938 <a title="96-lsi-3" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We examine the problem of generating state-space compressions of POMDPs in a way that minimally impacts decision quality. We analyze the impact of compressions on decision quality, observing that compressions that allow accurate policy evaluation (prediction of expected future reward) will not affect decision quality. We derive a set of sufﬁcient conditions that ensure accurate prediction in this respect, illustrate interesting mathematical properties these confer on lossless linear compressions, and use these to derive an iterative procedure for ﬁnding good linear lossy compressions. We also elaborate on how structured representations of a POMDP can be used to ﬁnd such compressions.</p><p>4 0.39555657 <a title="96-lsi-4" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>Author: William W. Cohen</p><p>Abstract: Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that improves a simple web page classiﬁer’s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signiﬁcantly and substantially improves the accuracy of a bag-of-words classiﬁer, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classiﬁer; the results are used by a restricted wrapper-learner to propose potential “main-category anchor wrappers”; and ﬁnally, these wrappers are used as features by a third learner to ﬁnd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classiﬁer.</p><p>5 0.39220047 <a title="96-lsi-5" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>Author: Yasemin Altun, Thomas Hofmann, Mark Johnson</p><p>Abstract: This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields. Applications areas for the presented technique range from natural language processing and information extraction to computational biology. We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 1</p><p>6 0.38767707 <a title="96-lsi-6" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>7 0.36733544 <a title="96-lsi-7" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>8 0.35192442 <a title="96-lsi-8" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>9 0.33742478 <a title="96-lsi-9" href="./nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">111 nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>10 0.33568671 <a title="96-lsi-10" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>11 0.33385164 <a title="96-lsi-11" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>12 0.32393381 <a title="96-lsi-12" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>13 0.32215628 <a title="96-lsi-13" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>14 0.30943266 <a title="96-lsi-14" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>15 0.30731666 <a title="96-lsi-15" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>16 0.30282477 <a title="96-lsi-16" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>17 0.29639173 <a title="96-lsi-17" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>18 0.2954689 <a title="96-lsi-18" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>19 0.29294226 <a title="96-lsi-19" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>20 0.29236311 <a title="96-lsi-20" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.015), (9, 0.026), (18, 0.189), (22, 0.037), (26, 0.065), (39, 0.083), (47, 0.096), (48, 0.135), (54, 0.024), (58, 0.019), (63, 0.01), (66, 0.038), (72, 0.056), (93, 0.125)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89831376 <a title="96-lda-1" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>Author: Brian Taba, Kwabena A. Boahen</p><p>Abstract: We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2] . This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modeling largely unelucidated and exceedingly plastic higher processing centers in cortex. Instead of an explicit blueprint for every cortical area, what is needed is a developmental rule that can wire complex circuits from minimal specifications. One candidate is the famous</p><p>same-paper 2 0.85958165 <a title="96-lda-2" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: We introduce the Generalized 2 Linear 2 Model, a statistical estimator which combines features of nonlinear regression and factor analysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation j(g(A)h(B)). Here A and Bare low-rank matrices, while j, g, and h are link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family peA, the infomax formulation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new. 1</p><p>3 0.76316535 <a title="96-lda-3" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>Author: Luis E. Ortiz, Michael Kearns</p><p>Abstract: We introduce NashProp, an iterative and local message-passing algorithm for computing Nash equilibria in multi-player games represented by arbitrary undirected graphs. We provide a formal analysis and experimental evidence demonstrating that NashProp performs well on large graphical games with many loops, often converging in just a dozen iterations on graphs with hundreds of nodes. NashProp generalizes the tree algorithm of (Kearns et al. 2001), and can be viewed as similar in spirit to belief propagation in probabilistic inference, and thus complements the recent work of (Vickrey and Koller 2002), who explored a junction tree approach. Thus, as for probabilistic inference, we have at least two promising general-purpose approaches to equilibria computation in graphs.</p><p>4 0.76307535 <a title="96-lda-4" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>5 0.76182443 <a title="96-lda-5" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>6 0.75967062 <a title="96-lda-6" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>7 0.75937253 <a title="96-lda-7" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>8 0.75686502 <a title="96-lda-8" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>9 0.75632989 <a title="96-lda-9" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>10 0.75549293 <a title="96-lda-10" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>11 0.75446451 <a title="96-lda-11" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>12 0.75435734 <a title="96-lda-12" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>13 0.75304651 <a title="96-lda-13" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>14 0.75192368 <a title="96-lda-14" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>15 0.75167602 <a title="96-lda-15" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>16 0.75073421 <a title="96-lda-16" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>17 0.74928653 <a title="96-lda-17" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>18 0.74915141 <a title="96-lda-18" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>19 0.74893224 <a title="96-lda-19" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>20 0.74836677 <a title="96-lda-20" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
