<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-17" href="#">nips2002-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</h1>
<br/><p>Source: <a title="nips-2002-17-pdf" href="http://papers.nips.cc/paper/2185-a-statistical-mechanics-approach-to-approximate-analytical-bootstrap-averages.pdf">pdf</a></p><p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>Reference: <a title="nips-2002-17-reference" href="../nips2002_reference/nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bootstrap', 0.782), ('dc', 0.383), ('udc', 0.219), ('replic', 0.19), ('replica', 0.137), ('pq', 0.091), ('resampl', 0.073), ('efron', 0.071), ('av', 0.066), ('ri', 0.066), ('phys', 0.062), ('abalon', 0.061), ('retrain', 0.061), ('bd', 0.057), ('malzahn', 0.055), ('gp', 0.054), ('formula', 0.052), ('gib', 0.05), ('energy', 0.049), ('disord', 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="17-tfidf-1" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>2 0.2335804 <a title="17-tfidf-2" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>3 0.10317671 <a title="17-tfidf-3" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>4 0.061449431 <a title="17-tfidf-4" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert function Spaces. We introduce a concept of scalesensitive effective data dimension, and show that it characterizes the convergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in ﬁnite dimensional spaces to non-parametric kernel learning methods. We derive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.</p><p>5 0.057412669 <a title="17-tfidf-5" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>6 0.054148685 <a title="17-tfidf-6" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>7 0.051289741 <a title="17-tfidf-7" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>8 0.047636066 <a title="17-tfidf-8" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>9 0.045189951 <a title="17-tfidf-9" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>10 0.043177504 <a title="17-tfidf-10" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>11 0.039992895 <a title="17-tfidf-11" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>12 0.039563447 <a title="17-tfidf-12" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>13 0.039295811 <a title="17-tfidf-13" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>14 0.038942352 <a title="17-tfidf-14" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>15 0.037642621 <a title="17-tfidf-15" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>16 0.03709456 <a title="17-tfidf-16" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>17 0.036681101 <a title="17-tfidf-17" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>18 0.036658756 <a title="17-tfidf-18" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>19 0.03651455 <a title="17-tfidf-19" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>20 0.035901349 <a title="17-tfidf-20" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.113), (1, 0.036), (2, -0.003), (3, -0.004), (4, 0.003), (5, -0.007), (6, -0.063), (7, -0.044), (8, -0.007), (9, 0.02), (10, -0.034), (11, -0.009), (12, -0.015), (13, -0.039), (14, -0.018), (15, -0.038), (16, 0.085), (17, 0.078), (18, 0.045), (19, -0.017), (20, 0.054), (21, -0.005), (22, -0.021), (23, 0.067), (24, 0.035), (25, -0.017), (26, 0.028), (27, 0.051), (28, -0.304), (29, 0.081), (30, 0.21), (31, 0.021), (32, 0.077), (33, -0.259), (34, -0.106), (35, 0.169), (36, 0.148), (37, 0.033), (38, -0.197), (39, -0.163), (40, 0.0), (41, 0.104), (42, 0.123), (43, 0.087), (44, -0.025), (45, -0.188), (46, -0.13), (47, -0.149), (48, 0.023), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89730793 <a title="17-lsi-1" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>2 0.80415541 <a title="17-lsi-2" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>3 0.32768583 <a title="17-lsi-3" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>4 0.28047648 <a title="17-lsi-4" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert function Spaces. We introduce a concept of scalesensitive effective data dimension, and show that it characterizes the convergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in ﬁnite dimensional spaces to non-parametric kernel learning methods. We derive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.</p><p>5 0.2727423 <a title="17-lsi-5" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>Author: Finnegan Southey, Dale Schuurmans, Ali Ghodsi</p><p>Abstract: Greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for modes in the estimation objective. Previous work has demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domains. In this paper we present a reformulation of greedy importance sampling that eliminates the free parameters from the original estimator, and introduces a new regularization strategy that further reduces variance without compromising unbiasedness. The resulting estimator is shown to be effective for difﬁcult estimation problems arising in Markov random ﬁeld inference. In particular, improvements are achieved over standard MCMC estimators when the distribution has multiple peaked modes.</p><p>6 0.26773962 <a title="17-lsi-6" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>7 0.26085618 <a title="17-lsi-7" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>8 0.2399305 <a title="17-lsi-8" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>9 0.23810676 <a title="17-lsi-9" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>10 0.23504668 <a title="17-lsi-10" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>11 0.22552019 <a title="17-lsi-11" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>12 0.22186235 <a title="17-lsi-12" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>13 0.21292651 <a title="17-lsi-13" href="./nips-2002-Approximate_Inference_and_Protein-Folding.html">32 nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>14 0.21220782 <a title="17-lsi-14" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>15 0.21167988 <a title="17-lsi-15" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>16 0.20746526 <a title="17-lsi-16" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>17 0.20239019 <a title="17-lsi-17" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>18 0.19645879 <a title="17-lsi-18" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>19 0.19044285 <a title="17-lsi-19" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>20 0.18913577 <a title="17-lsi-20" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (19, 0.017), (21, 0.289), (22, 0.105), (26, 0.03), (37, 0.029), (39, 0.053), (47, 0.085), (48, 0.091), (54, 0.021), (66, 0.049), (72, 0.045), (93, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67594308 <a title="17-lda-1" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>2 0.54015946 <a title="17-lda-2" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>3 0.53704792 <a title="17-lda-3" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>4 0.53311229 <a title="17-lda-4" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>5 0.53179914 <a title="17-lda-5" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>6 0.52719396 <a title="17-lda-6" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>7 0.52295882 <a title="17-lda-7" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>8 0.52231091 <a title="17-lda-8" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>9 0.5211069 <a title="17-lda-9" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>10 0.52104437 <a title="17-lda-10" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>11 0.51996088 <a title="17-lda-11" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>12 0.51919889 <a title="17-lda-12" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>13 0.51913351 <a title="17-lda-13" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>14 0.51893187 <a title="17-lda-14" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>15 0.51779532 <a title="17-lda-15" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>16 0.51495123 <a title="17-lda-16" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>17 0.51405823 <a title="17-lda-17" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>18 0.51362741 <a title="17-lda-18" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>19 0.51348007 <a title="17-lda-19" href="./nips-2002-Analysis_of_Information_in_Speech_Based_on_MANOVA.html">29 nips-2002-Analysis of Information in Speech Based on MANOVA</a></p>
<p>20 0.51322412 <a title="17-lda-20" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
