<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-193" href="#">nips2002-193</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</h1>
<br/><p>Source: <a title="nips-2002-193-pdf" href="http://papers.nips.cc/paper/2184-temporal-coherence-natural-image-sequences-and-the-visual-cortex.pdf">pdf</a></p><p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>Reference: <a title="nips-2002-193-reference" href="../nips2002_reference/nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ﬁ  Abstract We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. [sent-6, score-1.048]
</p><p>2 The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. [sent-7, score-1.066]
</p><p>3 In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. [sent-8, score-0.48]
</p><p>4 This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). [sent-9, score-0.299]
</p><p>5 Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. [sent-10, score-1.276]
</p><p>6 The second approach is based on a two-layer statistical generative model of natural image sequences. [sent-11, score-0.353]
</p><p>7 In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. [sent-12, score-0.877]
</p><p>8 Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. [sent-13, score-0.837]
</p><p>9 In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. [sent-14, score-0.242]
</p><p>10 This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori. [sent-15, score-0.165]
</p><p>11 1 Introduction The functional role of simple and complex cells has puzzled scientists since their response properties were ﬁrst mapped by Hubel and Wiesel in the 1950s (see, e. [sent-16, score-0.168]
</p><p>12 In 1996 a major advance was achieved when Olshausen and Field showed that simple-cell-like receptive ﬁelds emerge when sparse coding is applied to natural image data [3]. [sent-22, score-0.747]
</p><p>13 In the case of image data, independent component analysis is closely related to sparse coding [5]. [sent-24, score-0.305]
</p><p>14 In this paper we show that a principle called temporal coherence [6, 7, 8, 9] leads to the emergence of major properties of the primary visual cortex from natural image sequences. [sent-25, score-0.971]
</p><p>15 Temporal coherence is based on the idea that when processing temporal input, the representation changes as little as possible over time. [sent-26, score-0.576]
</p><p>16 We apply the principle of temporal coherence to natural input, and at the level of early vision, in two different ways. [sent-30, score-0.756]
</p><p>17 In the ﬁrst approach we show that when the input consists of natural image sequences, the maximization of temporal response strength correlation of cell output leads to receptive ﬁelds which are similar to simple cell receptive ﬁelds. [sent-31, score-1.749]
</p><p>18 These results show that temporal coherence is an alternative to sparse coding, in that they both result in the emergence of simple-cell-like receptive ﬁelds from natural input data. [sent-32, score-1.038]
</p><p>19 Whereas earlier research has focused on establishing a link between temporal coherence and complex cells, our results demonstrate that such a connection exists even on the simple cell level. [sent-33, score-0.869]
</p><p>20 We will also show how this approach can be interpreted as estimation of a linear latent variable model in which the latent signals have varying variances. [sent-34, score-0.161]
</p><p>21 In the second approach we use the principle of temporal coherence to formulate a two-layer generative model of natural image sequences. [sent-35, score-0.976]
</p><p>22 In addition to single-cell temporal coherence, this model also captures inter-cell temporal dependencies. [sent-36, score-0.516]
</p><p>23 We show that when this model is estimated from natural image sequence data, the results include both simple-cell-like receptive ﬁelds, and a complex-cell-like pooling of simple cell outputs. [sent-37, score-0.971]
</p><p>24 Whereas in earlier research learning two-layer statistical models of early vision has required ﬁxing one of the layers beforehand, in our model both layers are learned simultaneously. [sent-38, score-0.327]
</p><p>25 This situation is analogous to sparse coding, because measures of sparseness can also be used to estimate linear generative models with non-Gaussian independent sources [5]. [sent-40, score-0.342]
</p><p>26 We ﬁrst describe our measure of temporal coherence, and then provide the link to latent variable models. [sent-41, score-0.286]
</p><p>27 Linear simple cell models are commonly used in studies concerning the connections between visual input statistics and simple cell receptive ﬁelds [3, 4]. [sent-43, score-0.833]
</p><p>28 A vectorization of image patches can be done by scanning images column-wise into vectors – for windows of size N × N this yields vectors with dimension N 2 . [sent-50, score-0.266]
</p><p>29 The output of the kth ﬁlter at time t, denoted by signal yk (t), T is given by yk (t) = wk x(t). [sent-51, score-0.547]
</p><p>30 Temporal response strength correlation, the objective function, is deﬁned by K  f (W) =  Et {g(yk (t))g(yk (t − ∆t))} ,  (2)  k=1  where the nonlinearity g is strictly convex, even (rectifying), and differentiable. [sent-54, score-0.235]
</p><p>31 The nonlinearity g measures the strength (amplitude) of the response of the ﬁlter, and emphasizes large responses over small ones (see [10] for  A  B 9  0  y2 (t)  y(t)  3  6 3 0  −3 0  200  400  time index  0  200  400  time index  Figure 1: Illustration of nonstationarity of variance. [sent-56, score-0.356]
</p><p>32 (A) A temporally uncorrelated signal y(t) with nonstationary variance. [sent-57, score-0.24]
</p><p>33 A set of ﬁlters which has a large temporal response strength correlation is such that the same ﬁlters often respond strongly at consecutive time points, outputting large (either positive or negative) values. [sent-61, score-0.468]
</p><p>34 This means that the same ﬁlters will respond strongly over short periods of time, thereby expressing temporal coherence of a population code. [sent-62, score-0.625]
</p><p>35 A detailed discussion of the difference between temporal response strength correlation and sparseness, including several control experiments, can be found in [10]. [sent-63, score-0.419]
</p><p>36 To keep the outputs of the ﬁlters bounded we enforce the unit variance constraint on each of the output signals yk (t). [sent-64, score-0.332]
</p><p>37 The interpretation of maximization of objective function (2) as estimation of a generative model is based on the concept of sources with nonstationary variances [11, 12]. [sent-69, score-0.484]
</p><p>38 (3)  Here A = [a1 · · · aK ] denotes a matrix which relates the image patch x(t) to the activities of the simple cells, so that each column ak , k = 1, . [sent-71, score-0.218]
</p><p>39 The nonstationarity of the variances of sources y(t) means that their variances change over time, and the variance of a signal is correlated at nearby time points. [sent-77, score-0.256]
</p><p>40 An example of a signal with nonstationary variance is shown in Figure 1. [sent-78, score-0.183]
</p><p>41 It can be shown [12] that optimization of a cumulant-based criterion, similar to equation (2), can separate independent sources with nonstationary variances. [sent-79, score-0.18]
</p><p>42 Thus, the maximization of the objective function can also be interpreted as estimation of generative models in which the activity levels of the sources vary over time, and are temporally correlated over time. [sent-80, score-0.388]
</p><p>43 As was noted above, this situation is analogous to the application of measures of sparseness to estimate linear generative models with non-Gaussian sources. [sent-81, score-0.23]
</p><p>44 The algorithm was applied to natural image sequence data, which was sampled from a subset of image sequences used in [14]. [sent-82, score-0.412]
</p><p>45 Preprocessing consisted of temporal decorrelation, subtraction of local mean, and normalization [10], and dimensionality reduction from 256 to 160 using principal component analysis [5] (this degree of reduction  Figure 2: Basis vectors estimated using the principle of temporal coherence. [sent-84, score-0.651]
</p><p>46 The vectors were estimated from natural image sequences by optimizing temporal response strength correlation (2) under unit energy and uncorrelatedness constraints (here nonlinearity g(α) = ln cosh α). [sent-85, score-0.934]
</p><p>47 The basis vectors have been ordered according to Et {g(yk (t))g(yk (t − ∆t))} , that is, according to their “contribution” into the ﬁnal objective value (vectors with largest values top left). [sent-86, score-0.157]
</p><p>48 Figure 2 shows the basis vectors (columns of matrix A) which emerge when temporal response strength correlation is maximized for this data. [sent-88, score-0.669]
</p><p>49 These are the main features of simple cell receptive ﬁelds [1]. [sent-90, score-0.529]
</p><p>50 A quantitative analysis, showing that the resulting receptive ﬁelds are similar to those obtained using sparse coding, can be found in [10], where the details of the experiments are also described. [sent-91, score-0.346]
</p><p>51 3 Inter-cell temporal dependencies yield simple cell output pooling 3. [sent-92, score-0.739]
</p><p>52 1 Model Temporal response strength correlation, equation (2), measures the temporal coherence of individual simple cells. [sent-93, score-0.771]
</p><p>53 In terms of the generative model described above, this means that the nonstationary variances of different yk (t)’s have no interdependencies. [sent-94, score-0.485]
</p><p>54 In this section we add another layer to the generative model presented above to extend the theory to simple cell interactions, and to the level of complex cells. [sent-95, score-0.471]
</p><p>55 Like in the generative model described at the end of the previous section, the output layer of the model (see Figure 3) is linear, and maps signed cell responses to image features. [sent-96, score-0.719]
</p><p>56 But in contrast to the previous section, or models used in independent component analysis [5] or basic sparse coding [3], we do not assume that the components of y(t) are independent. [sent-97, score-0.174]
</p><p>57 Instead, we model the dependencies between these components with a multivariate autoreT gressive model in the ﬁrst layer of our model. [sent-98, score-0.172]
</p><p>58 Let abs (y(t)) = [|y1 (t)| · · · |yK (t)|] , let v(t) denote a driving noise signal, and let M denote a K × K matrix. [sent-99, score-0.536]
</p><p>59 Our model is a multidimensional ﬁrst-order autoregressive process, deﬁned by abs (y(t)) = M abs (y(t − ∆t)) + v(t). [sent-100, score-1.092]
</p><p>60 (4)  As in independent component analysis, we also need to ﬁx the scale of the latent variables 2 by deﬁning Et yk (t) = 1 for k = 1, . [sent-101, score-0.296]
</p><p>61 abs (y(t)) v(t)  abs (y(t)) = M abs (y(t − ∆t)) + v(t)  y(t)  ×  x(t)  x(t) = Ay(t)  random signs T  Figure 3: The two layers of the generative model. [sent-105, score-1.747]
</p><p>62 Let abs (y(t)) = [|y1 (t)| · · · |yK (t)|] denote the amplitudes of simple cell responses. [sent-106, score-0.778]
</p><p>63 In the ﬁrst layer, the driving noise signal v(t) generates the amplitudes of simple cell responses via an autoregressive model. [sent-107, score-0.448]
</p><p>64 The signs of the responses are generated randomly between the ﬁrst and second layer to yield signed responses y(t). [sent-108, score-0.29]
</p><p>65 In the second layer, natural video x(t) is generated linearly from simple cell responses. [sent-109, score-0.349]
</p><p>66 In addition to the relations shown here, the generation of v(t) is affected by M abs (y(t − ∆t)) to ensure non-negativity of abs (y(t)) . [sent-110, score-0.994]
</p><p>67 There are dependencies between the driving noise v(t) and output strengths abs (y(t)) , caused by the non-negativity of abs (y(t)) . [sent-112, score-1.15]
</p><p>68 We deﬁne v(t) = max (−M abs (y(t − ∆t)) , u(t)) , where, for vectors a and b, max (a, b) = T [max(a1 , b1 ) · · · max(an , bn )] . [sent-115, score-0.612]
</p><p>69 We assume that u(t) and abs (y(t)) are uncorrelated. [sent-116, score-0.497]
</p><p>70 To make the generative model complete, a mechanism for generating the signs of cell responses y(t) must be included. [sent-117, score-0.467]
</p><p>71 We specify that the signs are generated randomly with equal probability for plus or minus after the strengths of the responses have been generated. [sent-118, score-0.159]
</p><p>72 Note that one consequence of this is that the different yk (t)’s are uncorrelated. [sent-119, score-0.204]
</p><p>73 In equation (4), a large positive matrix element M(i, j), or M(j, i), indicates that there is strong temporal coherence between the output strengths of cells i and j. [sent-122, score-0.766]
</p><p>74 Thinking in terms of grouping temporally coherent cells together, matrix M can be thought of as containing similarities (reciprocals of distances) between different cells. [sent-123, score-0.296]
</p><p>75 We will use this property in the experimental section to derive a topography of simple cell receptive ﬁelds from M. [sent-124, score-0.599]
</p><p>76 Therefore, instead of deriving an objective function from ﬁrst principles, we derived an objective function heuristically, and veriﬁed through simulations that the objective function is capable of estimating the two-layer model. [sent-138, score-0.183]
</p><p>77 Figure 4 illustrates the basis vectors – that is, columns of A – laid out at spatial coordinates derived from M in a way explained below. [sent-147, score-0.181]
</p><p>78 The resulting basis vectors are again oriented, localized and multiscale, as in the previous experiment. [sent-148, score-0.15]
</p><p>79 The two-dimensional coordinates of the basis vectors were determined from M using multidimensional scaling (see ﬁgure caption for details). [sent-149, score-0.175]
</p><p>80 The temporal coherence between the outputs of two cells i and j is reﬂected in the distance between the corresponding receptive ﬁelds: the larger the elements M(i, j) and M(j, i) are, the closer the receptive ﬁelds are to each other. [sent-150, score-1.272]
</p><p>81 We can see that local topography emerges in the results: those basis vectors which are close to each other seem to be mostly coding for similarly oriented features at nearby spatial positions. [sent-151, score-0.421]
</p><p>82 This kind of grouping is characteristic of pooling of simple cell outputs at complex cell level [1]. [sent-152, score-0.759]
</p><p>83 1 Thus, the estimation of our two-layer model from natural image sequences yields both simple-cell-like receptive ﬁelds, and grouping similar to the pooling of simple cell outputs. [sent-153, score-1.129]
</p><p>84 Linear receptive ﬁelds emerge in the second layer (matrix A), and cell output grouping emerges in the ﬁrst layer (matrix M). [sent-154, score-0.955]
</p><p>85 This is a signiﬁcant improvement on earlier statistical models of early vision, because no a priori ﬁxing of either of these layers is needed. [sent-156, score-0.163]
</p><p>86 4 Conclusions We have shown in this paper that when the principle of temporal coherence is applied to natural image sequences, both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs emerge. [sent-157, score-1.609]
</p><p>87 These results were obtained with two different approaches 1  Some global topography also emerges: those basis vectors which code for horizontal features are on the left in the ﬁgure, while those that code for vertical features are on the right. [sent-158, score-0.166]
</p><p>88 Figure 4: Results of estimating the two-layer generative model from natural image sequences. [sent-159, score-0.353]
</p><p>89 Basis vectors (columns of A) plotted at spatial coordinates given by applying multidimensional scaling to M. [sent-160, score-0.181]
</p><p>90 Some of the resulting coordinates were very close to each other, so tight cell clusters were magniﬁed for purposes of visual display. [sent-163, score-0.311]
</p><p>91 The ﬁrst used temporally coherent simple cell outputs, and the second was based on a temporal two-layer generative model of natural image sequences. [sent-166, score-0.993]
</p><p>92 Simple-cell-like receptive ﬁelds emerge in both cases, and the output pooling emerges as a local topographic property in the case of the two-layer generative model. [sent-167, score-0.776]
</p><p>93 First, to our knowledge this is the ﬁrst time that localized and oriented receptive ﬁelds with different scales have been shown to emerge from natural data using the principle of temporal coherence. [sent-169, score-0.902]
</p><p>94 In some models of invariant visual representations [8, 16] simple cell receptive ﬁelds are obtained as by-products, but learning is strongly modulated by complex cells, and the receptive ﬁelds seem to lack the important properties of spatial localization and multiresolution. [sent-170, score-0.925]
</p><p>95 This is not needed in our two-layer model, because both layers emerge simultaneously in a completely unsupervised manner from the natural input data. [sent-172, score-0.306]
</p><p>96 Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. [sent-184, score-0.413]
</p><p>97 Simple-cell-like receptive ﬁelds maximize temporal coherence in natural video. [sent-208, score-0.944]
</p><p>98 A two-layer sparse coding model learns simple and complex cell receptive ﬁelds and topography from natural images. [sent-219, score-0.839]
</p><p>99 Independent component analysis of natural image sequences yields spatio-temporal ﬁlters similar to simple cells in primary visual cortex. [sent-224, score-0.496]
</p><p>100 A two-layer dynamic generative model of natural image sequences. [sent-227, score-0.353]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('abs', 0.497), ('coherence', 0.33), ('receptive', 0.274), ('temporal', 0.246), ('cell', 0.224), ('elds', 0.217), ('yk', 0.204), ('pooling', 0.165), ('image', 0.131), ('emerge', 0.126), ('aapo', 0.113), ('hyv', 0.113), ('nonstationary', 0.113), ('generative', 0.104), ('natural', 0.094), ('layer', 0.088), ('layers', 0.086), ('cells', 0.081), ('temporally', 0.081), ('lters', 0.08), ('coding', 0.077), ('emerges', 0.07), ('topography', 0.07), ('strength', 0.07), ('outputs', 0.067), ('signs', 0.066), ('nonstationarity', 0.062), ('jarmo', 0.062), ('oriented', 0.061), ('objective', 0.061), ('coherent', 0.058), ('estimation', 0.057), ('sequences', 0.056), ('wk', 0.056), ('hurri', 0.056), ('rinen', 0.056), ('response', 0.056), ('vectors', 0.055), ('vision', 0.054), ('localized', 0.054), ('responses', 0.049), ('emergence', 0.049), ('visual', 0.049), ('grouping', 0.048), ('nonlinearity', 0.048), ('uncorrelatedness', 0.047), ('spatial', 0.047), ('principle', 0.047), ('et', 0.047), ('correlation', 0.047), ('signal', 0.046), ('sparse', 0.045), ('strengths', 0.044), ('sources', 0.044), ('multidimensional', 0.041), ('pseudoinverse', 0.041), ('basis', 0.041), ('maximization', 0.041), ('latent', 0.04), ('variances', 0.04), ('early', 0.039), ('xing', 0.039), ('driving', 0.039), ('measures', 0.038), ('coordinates', 0.038), ('earlier', 0.038), ('signed', 0.038), ('ms', 0.037), ('output', 0.037), ('sparseness', 0.037), ('dependencies', 0.036), ('multiscale', 0.035), ('bruno', 0.035), ('emphasizes', 0.033), ('autoregressive', 0.033), ('simple', 0.031), ('cosh', 0.031), ('terrence', 0.031), ('max', 0.03), ('olshausen', 0.03), ('component', 0.029), ('estimated', 0.028), ('blind', 0.028), ('ak', 0.028), ('matrix', 0.028), ('details', 0.027), ('ay', 0.026), ('spatiotemporal', 0.026), ('amplitudes', 0.026), ('strongly', 0.026), ('situation', 0.026), ('cortex', 0.025), ('energy', 0.025), ('estimate', 0.025), ('yields', 0.025), ('model', 0.024), ('variance', 0.024), ('rst', 0.023), ('respond', 0.023), ('independent', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="193-tfidf-1" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>2 0.23954251 <a title="193-tfidf-2" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>3 0.18997273 <a title="193-tfidf-3" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>Author: Christian K. Machens, Michael Wehr, Anthony M. Zador</p><p>Abstract: How do cortical neurons represent the acoustic environment? This question is often addressed by probing with simple stimuli such as clicks or tone pips. Such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to uncover complex or higher-order neuronal response properties. Here we adopt an alternative approach, probing neuronal responses with complex acoustic stimuli, including animal vocalizations and music. We have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential ﬂuctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit, instead of just its output—a sparse binary spike train—as in conventional single unit physiological recordings. Whole cell recording thus provides a much richer source of information about the neuron’s response. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component—the spectrotemporal receptive ﬁeld (STRF)—of the transformation from the sound (as represented by its time-varying spectrogram) to the neuron’s membrane potential. We ﬁnd that the STRF has a rich dynamical structure, including excitatory regions positioned in general accord with the prediction of the simple tuning curve. We also ﬁnd that in many cases, much of the neuron’s response, although deterministically related to the stimulus, cannot be predicted by the linear component, indicating the presence of as-yet-uncharacterized nonlinear response properties.</p><p>4 0.16292791 <a title="193-tfidf-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.16192466 <a title="193-tfidf-5" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>6 0.1510922 <a title="193-tfidf-6" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>7 0.13651949 <a title="193-tfidf-7" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>8 0.12678787 <a title="193-tfidf-8" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>9 0.11186704 <a title="193-tfidf-9" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>10 0.099030271 <a title="193-tfidf-10" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>11 0.086216152 <a title="193-tfidf-11" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>12 0.08319553 <a title="193-tfidf-12" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>13 0.083010599 <a title="193-tfidf-13" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>14 0.082453817 <a title="193-tfidf-14" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>15 0.080571391 <a title="193-tfidf-15" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>16 0.078994945 <a title="193-tfidf-16" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>17 0.077146798 <a title="193-tfidf-17" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>18 0.075193502 <a title="193-tfidf-18" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>19 0.072018571 <a title="193-tfidf-19" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>20 0.069234781 <a title="193-tfidf-20" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, 0.15), (2, 0.032), (3, 0.194), (4, -0.022), (5, -0.107), (6, 0.043), (7, -0.018), (8, -0.013), (9, -0.023), (10, 0.01), (11, -0.026), (12, 0.026), (13, 0.101), (14, 0.034), (15, 0.182), (16, 0.282), (17, -0.05), (18, -0.087), (19, 0.001), (20, 0.052), (21, -0.053), (22, 0.099), (23, -0.121), (24, -0.045), (25, -0.046), (26, -0.069), (27, 0.045), (28, -0.008), (29, -0.042), (30, -0.036), (31, 0.074), (32, 0.064), (33, -0.016), (34, 0.052), (35, 0.032), (36, 0.066), (37, 0.077), (38, 0.014), (39, 0.02), (40, 0.012), (41, -0.027), (42, -0.031), (43, 0.076), (44, -0.009), (45, -0.051), (46, 0.041), (47, -0.058), (48, 0.016), (49, -0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96421134 <a title="193-lsi-1" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>2 0.87231225 <a title="193-lsi-2" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>3 0.70220232 <a title="193-lsi-3" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>4 0.684825 <a title="193-lsi-4" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>5 0.63702148 <a title="193-lsi-5" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>6 0.5400995 <a title="193-lsi-6" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>7 0.53337497 <a title="193-lsi-7" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>8 0.51288718 <a title="193-lsi-8" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>9 0.48186228 <a title="193-lsi-9" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>10 0.47272658 <a title="193-lsi-10" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>11 0.43641591 <a title="193-lsi-11" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>12 0.42647257 <a title="193-lsi-12" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>13 0.41760486 <a title="193-lsi-13" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>14 0.40448913 <a title="193-lsi-14" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>15 0.36846179 <a title="193-lsi-15" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>16 0.35154635 <a title="193-lsi-16" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>17 0.34651825 <a title="193-lsi-17" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>18 0.33124545 <a title="193-lsi-18" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>19 0.3094421 <a title="193-lsi-19" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>20 0.30869362 <a title="193-lsi-20" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.021), (14, 0.011), (23, 0.02), (42, 0.068), (46, 0.172), (54, 0.112), (55, 0.119), (64, 0.078), (67, 0.034), (68, 0.039), (74, 0.107), (92, 0.023), (98, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87002784 <a title="193-lda-1" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>2 0.82486928 <a title="193-lda-2" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>3 0.77505445 <a title="193-lda-3" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>4 0.76804191 <a title="193-lda-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.76623785 <a title="193-lda-5" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>6 0.75171483 <a title="193-lda-6" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>7 0.74909782 <a title="193-lda-7" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>8 0.74649036 <a title="193-lda-8" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>9 0.74043202 <a title="193-lda-9" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>10 0.73750067 <a title="193-lda-10" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>11 0.7237525 <a title="193-lda-11" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>12 0.71602833 <a title="193-lda-12" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>13 0.71250665 <a title="193-lda-13" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>14 0.71057725 <a title="193-lda-14" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>15 0.71030247 <a title="193-lda-15" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>16 0.71015406 <a title="193-lda-16" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>17 0.70894146 <a title="193-lda-17" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>18 0.70609206 <a title="193-lda-18" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>19 0.70600766 <a title="193-lda-19" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>20 0.70584249 <a title="193-lda-20" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
