<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2002-Real-Time Particle Filters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-169" href="#">nips2002-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 nips-2002-Real-Time Particle Filters</h1>
<br/><p>Source: <a title="nips-2002-169-pdf" href="http://papers.nips.cc/paper/2305-real-time-particle-filters.pdf">pdf</a></p><p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>Reference: <a title="nips-2002-169-reference" href="../nips2002_reference/nips-2002-Real-Time_Particle_Filters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu £  Abstract Particle ﬁlters estimate the state of dynamical systems from sensor information. [sent-7, score-0.305]
</p><p>2 In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. [sent-8, score-0.888]
</p><p>3 The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. [sent-9, score-0.901]
</p><p>4 In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. [sent-10, score-0.871]
</p><p>5 This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. [sent-11, score-0.387]
</p><p>6 The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. [sent-12, score-0.365]
</p><p>7 Thereby, our approach focuses computational resources (samples) on valuable sensor information. [sent-13, score-0.439]
</p><p>8 Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches. [sent-14, score-0.277]
</p><p>9 1 Introduction Due to their sample-based representation, particle ﬁlters are well suited to estimate the state of non-linear dynamic systems. [sent-15, score-0.492]
</p><p>10 Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems including visual tracking, speech recognition, and mobile robotics [1]. [sent-16, score-0.619]
</p><p>11 The increased representational power of particle ﬁlters, however, comes at the cost of higher computational complexity. [sent-17, score-0.512]
</p><p>12 The application of particle ﬁlters to online, real-time estimation raises new research questions. [sent-18, score-0.528]
</p><p>13 The key question in this context is: How can we deal with situations in which the rate of incoming sensor data is higher than the update rate of the particle ﬁlter? [sent-19, score-0.831]
</p><p>14 The prevalent approach in real time applications is to update the ﬁlter as often as possible and to discard sensor information that arrives during the update process. [sent-21, score-0.512]
</p><p>15 Obviously, this approach is prone to losing valuable sensor information. [sent-22, score-0.348]
</p><p>16 At ﬁrst sight, the sample based representation of particle ﬁlters suggests an alternative approach similar to an any-time implementation: Whenever a new observation arrives, sampling is interrupted and the next observation is processed. [sent-23, score-0.704]
</p><p>17 In this paper we introduce real-time particle ﬁlters (RTPF) to deal with constraints imposed by limited computational resources. [sent-25, score-0.511]
</p><p>18 (b) Aggregate observations within a window and integrate them in one step. [sent-36, score-0.395]
</p><p>19 samples among the different observations arriving during a ﬁlter update. [sent-38, score-0.286]
</p><p>20 The weights of the mixture components are computed so as to minimize the approximation error introduced by the mixture representation. [sent-40, score-0.365]
</p><p>21 The resuling approach naturally focuses computational resources (samples) on valuable sensor information. [sent-41, score-0.439]
</p><p>22 The remainder of this paper is organized as follows: In the next section we outline the basics of particle ﬁlters in the context of real-time constraints. [sent-42, score-0.454]
</p><p>23 Then, in Section 3, we introduce our novel technique to real-time particle ﬁlters. [sent-43, score-0.454]
</p><p>24 ¦ §¥ £¡ ¤¢   (1)    ¨£¡ ¦ ¥©¤¢   Here is a sensor measurement and is control information measuring the dynamics of the system. [sent-46, score-0.275]
</p><p>25 The basic form of the particle ﬁlter realizes the recursive Bayes ﬁlter according to a sampling procedure, often referred to as sequential importance sampling with resampling (SISR): 1. [sent-49, score-0.61]
</p><p>26 Under realtime conditions, however, it is possible that the update cannot be completed before the next sensor measurement arrives. [sent-59, score-0.309]
</p><p>27 This can be the case for computationally complex sensor models or whenever the underlying posterior requires large sample sets [2]. [sent-60, score-0.408]
</p><p>28 The majority of ﬁltering approaches deals with this problem by skipping sensor information that arrives during the update of the ﬁlter. [sent-61, score-0.421]
</p><p>29 While this approach works reasonably well in many situations, it is prone to miss valuable sensor information. [sent-62, score-0.348]
</p><p>30 zt 1 St1  zt 3  zt 2 St2 α1  St3 α2  zt+11 St+11  zt+12 St+12  St+13  α2  α1  ’  ’  α3  z t+13  α3  ’  Estimation window t+1  Estmation window t  Figure 2: Real time particle ﬁlters. [sent-63, score-1.181]
</p><p>31 The samples are distributed among the observations within one estimation interval (window size three in this example). [sent-64, score-0.399]
</p><p>32 The belief is a mixture of the individual sample sets. [sent-65, score-0.391]
</p><p>33 Let be the number of samples required by the particle ﬁlter. [sent-70, score-0.58]
</p><p>34 Assume that the resulting update cycle of the particle ﬁlter takes and is called the estimation interval or estimation window. [sent-71, score-0.709]
</p><p>35 The -th observation and state within window are denoted and , respectively. [sent-76, score-0.343]
</p><p>36 1 illustrates different approaches to dealing with window sizes larger than one. [sent-78, score-0.346]
</p><p>37 Here, observations arriving during the update of the sample set are discarded, which has the obvious disadvantage that valuable sensor information might get lost. [sent-81, score-0.645]
</p><p>38 For example, it assumes that observations can be aggregated optimally, and that the integration of an aggregated observation can be performed as efﬁciently as the integration of individual observations, which is often not the case. [sent-85, score-0.297]
</p><p>39 1(c), simply stops generating new samples whenever an observation is made (hence each sample set contains only samples). [sent-87, score-0.302]
</p><p>40 While this approach takes advantage of the any-time capabilities of particle ﬁlters, it is susceptible to ﬁlter divergence due to an insufﬁcent number of samples [2, 1]. [sent-88, score-0.626]
</p><p>41 & ) 10R  3 Real time particle ﬁlters In this paper we propose real time particle ﬁlters (RTPFs), a novel approach to dealing with limited computational resources. [sent-89, score-1.083]
</p><p>42 The key idea of RTPFs is to consider all sensor measurements by distributing the samples among the observations within an update window. [sent-90, score-0.571]
</p><p>43 Additionally, by weighting the different sample sets within a window, our approach focuses the computational resources (samples) on the most valuable observations. [sent-91, score-0.388]
</p><p>44 If needed, however, the complete belief can be generated by considering the dynamics between the individual mixture components. [sent-97, score-0.335]
</p><p>45 The belief state that is propagated by RTPF to the next estimation interval is a mixture distribution where each mixture component is represented by one of the sample sets, all generated independently from the previous window. [sent-106, score-0.658]
</p><p>46 Thus, the belief state propagation is simulated by sample trajectories, that for computational convenience are represented at the points in time where the observations are integrated. [sent-107, score-0.441]
</p><p>47 The key idea is to choose the weights that minimize the KL-divergence between the mixture belief and the optimal belief. [sent-110, score-0.342]
</p><p>48 The optimal belief is the belief we would get if there was enough time to compute the full posterior within the update window. [sent-111, score-0.417]
</p><p>49 The optimal belief at the end of an estimation window results from iterative application of the Bayes ﬁlter update on each obseration [3]:  &  (2)  ¤ ¡  2  ¦ ¥ 4 16 5 ¦ ¥ 4  ¦ "¥©¤I    §©)  ¦ ¥   ¦ "¥¨ §© ¦ ¥ # ¦ ¨  6 6 ¦ 0(    ¨£¡  6   ¤ © ¡     6 16   ¦ "¥¨ 3 7I  6 ¦ £¡ ! [sent-114, score-0.504]
</p><p>50 In essence, (2) computes the belief by integrating over all trajectories through the estimation interval, where the start position of the trajectories is drawn from the previous belief . [sent-117, score-0.552]
</p><p>51 Now let denote the belief resulting from integrating only the observation within the estimation window. [sent-119, score-0.295]
</p><p>52 In contrast to (2), however, each trajectory selectively integrates only one of the observations within the estimation interval1. [sent-127, score-0.272]
</p><p>53 2 Optimizing the mixture weights We will now turn to the problem of ﬁnding the weights of the mixture. [sent-129, score-0.267]
</p><p>54 Optimizing the weights of mixture approximations can be done using EM [6] or (constrained) gradient descent [7]. [sent-139, score-0.295]
</p><p>55 3 Monte Carlo gradient estimation The exact computation of the gradients in (6) requires the computation of the different beliefs, each in turn requiring several particle ﬁlter updates (see (2), (3)), and integreation over all states . [sent-152, score-0.577]
</p><p>56 The approach is based on the observation that the beliefs in (6) share the same trajectories through space and differ only in the observations they integrate. [sent-155, score-0.345]
</p><p>57 Therefore, we ﬁrst generate sample trajectories through the estimation window without considering the observations, and then use importance sampling to generate the beliefs needed for the gradient from a sample estimation. [sent-156, score-0.787]
</p><p>58 Trajectory generation is done as follows: we draw a sample is given by the set of the previous mixture belief, where the probability of chosing a set mixture weights . [sent-157, score-0.432]
</p><p>59 This sample is then moved forward in time by consecutively drawing samples from the distributions at each time step . [sent-158, score-0.324]
</p><p>60 The number of independent samples needed to represent the belief, the update rate of incoming sensor data, and the available processing power determine the size of the estimation window and hence the number of mixture components. [sent-175, score-0.943]
</p><p>61 RTPF computes the optimal weights of the mixture distribution at the end of each estimation window. [sent-176, score-0.297]
</p><p>62 The resulting weights are used to generate samples for the individual sample sets of the next estimation window. [sent-178, score-0.408]
</p><p>63 The task of the robot was to determine its position using data collected by two distance measuring devices, one pointing to its left, the other pointing to its right. [sent-183, score-0.292]
</p><p>64 4 Experiments In this section we evaluate the effectiveness of RTPF against the alternatives, using data collected from a mobile robot in a real-world environment. [sent-184, score-0.249]
</p><p>65 The task of the robot was to determine its position within the map, using data collected by two laser-beams, one pointing to its left, the other pointing to its right. [sent-186, score-0.313]
</p><p>66 Between each observation the robot moved approximately 50cm (see [3] for details on robot localization and sensor models). [sent-188, score-0.889]
</p><p>67 Localization performance was measured by the average distance between the samples and the reference robot positions, which were computed ofﬂine. [sent-190, score-0.317]
</p><p>68 In the experiments, our real-time algorithm, RTPF, is compared to particle ﬁlters with skipping observations, called “Skip data” (Figure 1a), and particle ﬁlters with insufﬁcient samples, called “Naive” (Figure 1c). [sent-191, score-0.964]
</p><p>69 First, we ﬁx the sample set size which is sufﬁcient for the robot to localize itself. [sent-198, score-0.281]
</p><p>70 In our experiment is set empirically to 20,000 (the particle ﬁlters may fail at lower , see also [2]). [sent-199, score-0.454]
</p><p>71 We then vary the computational resources, resulting in different window sizes . [sent-200, score-0.316]
</p><p>72 Larger window size means lower computational power, and the ). [sent-201, score-0.277]
</p><p>73 number of samples that can be generated for each observation decreases to (  R  & 0R )  R  &  R  Figure 4 shows the evolutions of average localization errors over time, using different window sizes. [sent-202, score-0.613]
</p><p>74 Furthermore, RTPF shows the least degradation with limited computational power (larger window sizes). [sent-207, score-0.308]
</p><p>75 The key advantage of RTPF over “Uniform” lies in the mixture weighting, which allows our approach to focus computational resources on valuable sensor information, for example when the robot passes an informative feature in one of the hallways. [sent-208, score-0.763]
</p><p>76 4(a)), this advantage is not very strong since in this environment, most features can be detected in several consecutive sensor measurements. [sent-210, score-0.288]
</p><p>77 4(a)-(c): Performance of the different algorithms for window sizes of 4, 8, and 12 respectively. [sent-221, score-0.286]
</p><p>78 The -axis plots the localization error measured in average distance from the reference position. [sent-223, score-0.25]
</p><p>79 4(d) represents the localization speedup of RTPF over “Skip data” for various window sizes. [sent-227, score-0.501]
</p><p>80 To see this, consider one estimation window of length . [sent-233, score-0.297]
</p><p>81 In such a situation, “Uniform” considers this landmark every time the robot passes it. [sent-235, score-0.287]
</p><p>82 In contrast to both approaches, RTPF detects all landmarks and generates more samples for the landmark detections, thereby gaining the best of both worlds, and Figures 4(a)–(c) show this is indeed the case. [sent-240, score-0.304]
</p><p>83 &  &  R  & 0R )  &  In Figure 4(d) we summarize the performance gain of RTPF over “Skip data” for different window sizes in terms of localization time. [sent-241, score-0.51]
</p><p>84 We considered the robot to be localized if the average localization error remains below 200 cm over a period of 10 seconds. [sent-242, score-0.47]
</p><p>85 The -axis represents the window size and the -axis the localization speedup. [sent-244, score-0.45]
</p><p>86 For each window size speedups were determined using -tests on the localization times for the 30 pairs of data runs. [sent-245, score-0.45]
</p><p>87 At small window sizes the speedup is 20-50%, but it goes up to 2. [sent-250, score-0.361]
</p><p>88 7 times for larger windows, demonstrating the beneﬁts of the RTPF approach over traditional particle ﬁlters. [sent-251, score-0.454]
</p><p>89 Ultimately, for very large window sizes, the speedup decreases again, which is due to the fact that none of the approaches is able to reduce the error below 200cm within the run time of an experiment. [sent-252, score-0.371]
</p><p>90 ¡  (  ¥  5 Conclusions In this paper we tackled the problem of particle ﬁltering under the constraint of limited computing resources. [sent-253, score-0.481]
</p><p>91 Our approach makes near-optimal use of sensor information by dividing sample sets between all available observations and then representing the state as a mixture of sample sets. [sent-254, score-0.744]
</p><p>92 We showed that RTPF produces signiﬁcant performance improvements in a robot localization task. [sent-257, score-0.398]
</p><p>93 7 times faster than the original particle ﬁlter approach, which skips sensor data. [sent-260, score-0.695]
</p><p>94 Based on these results, we expect our method to be highly valuable in a wide range of real-time applications of particle ﬁlters. [sent-261, score-0.54]
</p><p>95 RTPF yields maximal performance gain for data streams containing highly valuable sensor data occuring at unpredictable time points. [sent-262, score-0.356]
</p><p>96 So far RTPF uses ﬁxed sample sizes and ﬁxed window sizes. [sent-266, score-0.376]
</p><p>97 For example, by the method of [2] we can change the sample size on-the-ﬂy, which in turn allows us to change the window size. [sent-268, score-0.337]
</p><p>98 Ongoing experiments suggest that this combination yields further performance improvements: When the state uncertainty is high, many samples are used and these samples are spread out over multiple observations. [sent-269, score-0.313]
</p><p>99 On the other hand, when the uncertainty is low, the number of samples is very small and RTPF becomes identical to the vanilla particle ﬁlter with one update (sample set) per observation. [sent-270, score-0.671]
</p><p>100 Branching and interacting particle systems approximations of feynamkac formulae with applications to non linear ﬁltering. [sent-293, score-0.454]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rtpf', 0.508), ('particle', 0.454), ('sensor', 0.241), ('window', 0.223), ('localization', 0.203), ('lters', 0.169), ('robot', 0.167), ('skip', 0.167), ('belief', 0.139), ('mixture', 0.139), ('samples', 0.126), ('observations', 0.115), ('lter', 0.108), ('st', 0.095), ('trajectories', 0.09), ('sample', 0.09), ('valuable', 0.086), ('zt', 0.084), ('beliefs', 0.079), ('speedup', 0.075), ('estimation', 0.074), ('landmark', 0.07), ('update', 0.068), ('weights', 0.064), ('sizes', 0.063), ('observation', 0.061), ('dealing', 0.06), ('rtpfs', 0.056), ('skipping', 0.056), ('cm', 0.056), ('arrives', 0.056), ('importance', 0.054), ('mobile', 0.053), ('resources', 0.053), ('baseline', 0.053), ('moved', 0.05), ('aggregated', 0.049), ('gradient', 0.049), ('weighting', 0.048), ('pointing', 0.048), ('detects', 0.045), ('arriving', 0.045), ('insuf', 0.044), ('uniform', 0.044), ('descent', 0.043), ('trajectory', 0.04), ('naive', 0.04), ('interval', 0.039), ('sampling', 0.038), ('state', 0.038), ('carlo', 0.038), ('integrate', 0.036), ('monte', 0.035), ('dynamics', 0.034), ('landmarks', 0.033), ('boyen', 0.033), ('sets', 0.031), ('computational', 0.03), ('thereby', 0.03), ('mixtures', 0.03), ('loop', 0.03), ('collected', 0.029), ('time', 0.029), ('focuses', 0.029), ('situations', 0.028), ('fox', 0.028), ('walls', 0.028), ('improvements', 0.028), ('power', 0.028), ('limited', 0.027), ('sec', 0.026), ('doucet', 0.026), ('resampling', 0.026), ('discard', 0.026), ('dynamical', 0.026), ('advantage', 0.026), ('laser', 0.025), ('whenever', 0.025), ('prevalent', 0.024), ('reference', 0.024), ('size', 0.024), ('error', 0.023), ('individual', 0.023), ('arrive', 0.023), ('uncertainty', 0.023), ('environment', 0.022), ('integrates', 0.022), ('overhead', 0.022), ('grouped', 0.022), ('localized', 0.021), ('summarize', 0.021), ('prone', 0.021), ('posterior', 0.021), ('runs', 0.021), ('passes', 0.021), ('consecutive', 0.021), ('within', 0.021), ('computes', 0.02), ('divergence', 0.02), ('rate', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="169-tfidf-1" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>2 0.19959189 <a title="169-tfidf-2" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>3 0.18291885 <a title="169-tfidf-3" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>Author: Rubén Morales-menéndez, Nando D. Freitas, David Poole</p><p>Abstract: This paper discusses the application of particle ﬁltering algorithms to fault diagnosis in complex industrial processes. We consider two ubiquitous processes: an industrial dryer and a level tank. For these applications, we compared three particle ﬁltering variants: standard particle ﬁltering, Rao-Blackwellised particle ﬁltering and a version of RaoBlackwellised particle ﬁltering that does one-step look-ahead to select good sampling regions. We show that the overhead of the extra processing per particle of the more sophisticated methods is more than compensated by the decrease in error and variance.</p><p>4 0.12779708 <a title="169-tfidf-4" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classiﬁcation that combines the computational advantage of a parametric solution with the ﬂexibility of sequential sampling techniques. We regard the parameters of the classiﬁer as latent states in a ﬁrst order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman ﬁltering. The variational Kalman ﬁlter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classiﬁers both in stationary and non-stationary environments. Although we focus on classiﬁcation, the algorithm is easily extended to other generalized nonlinear models.</p><p>5 0.11846624 <a title="169-tfidf-5" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>Author: Bernd Porr, Florentin Wörgötter</p><p>Abstract: We develop a systems theoretical treatment of a behavioural system that interacts with its environment in a closed loop situation such that its motor actions inﬂuence its sensor inputs. The simplest form of a feedback is a reﬂex. Reﬂexes occur always “too late”; i.e., only after a (unpleasant, painful, dangerous) reﬂex-eliciting sensor event has occurred. This deﬁnes an objective problem which can be solved if another sensor input exists which can predict the primary reﬂex and can generate an earlier reaction. In contrast to previous approaches, our linear learning algorithm allows for an analytical proof that this system learns to apply feedforward control with the result that slow feedback loops are replaced by their equivalent feed-forward controller creating a forward model. In other words, learning turns the reactive system into a pro-active system. By means of a robot implementation we demonstrate the applicability of the theoretical results which can be used in a variety of different areas in physics and engineering.</p><p>6 0.11485138 <a title="169-tfidf-6" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>7 0.1031128 <a title="169-tfidf-7" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>8 0.095947474 <a title="169-tfidf-8" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>9 0.092960954 <a title="169-tfidf-9" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>10 0.088911057 <a title="169-tfidf-10" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>11 0.088451967 <a title="169-tfidf-11" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>12 0.087348625 <a title="169-tfidf-12" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>13 0.08725889 <a title="169-tfidf-13" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>14 0.08352375 <a title="169-tfidf-14" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>15 0.07703016 <a title="169-tfidf-15" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>16 0.07497175 <a title="169-tfidf-16" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>17 0.070911832 <a title="169-tfidf-17" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>18 0.069314152 <a title="169-tfidf-18" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>19 0.068342932 <a title="169-tfidf-19" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>20 0.063315891 <a title="169-tfidf-20" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.187), (1, 0.017), (2, -0.162), (3, 0.08), (4, 0.006), (5, 0.078), (6, -0.125), (7, 0.182), (8, 0.09), (9, 0.105), (10, -0.113), (11, 0.076), (12, 0.01), (13, -0.05), (14, 0.029), (15, 0.014), (16, -0.026), (17, -0.102), (18, -0.078), (19, 0.016), (20, 0.052), (21, -0.029), (22, -0.022), (23, 0.247), (24, -0.024), (25, -0.067), (26, -0.089), (27, -0.079), (28, -0.13), (29, -0.104), (30, 0.103), (31, -0.019), (32, -0.108), (33, -0.01), (34, 0.029), (35, -0.019), (36, 0.172), (37, 0.169), (38, -0.02), (39, 0.118), (40, -0.128), (41, -0.044), (42, -0.022), (43, 0.044), (44, -0.065), (45, 0.053), (46, 0.042), (47, -0.113), (48, 0.099), (49, 0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96237236 <a title="169-lsi-1" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>2 0.79068041 <a title="169-lsi-2" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>Author: Rubén Morales-menéndez, Nando D. Freitas, David Poole</p><p>Abstract: This paper discusses the application of particle ﬁltering algorithms to fault diagnosis in complex industrial processes. We consider two ubiquitous processes: an industrial dryer and a level tank. For these applications, we compared three particle ﬁltering variants: standard particle ﬁltering, Rao-Blackwellised particle ﬁltering and a version of RaoBlackwellised particle ﬁltering that does one-step look-ahead to select good sampling regions. We show that the overhead of the extra processing per particle of the more sophisticated methods is more than compensated by the decrease in error and variance.</p><p>3 0.51267123 <a title="169-lsi-3" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>4 0.48903093 <a title="169-lsi-4" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>Author: Bernd Porr, Florentin Wörgötter</p><p>Abstract: We develop a systems theoretical treatment of a behavioural system that interacts with its environment in a closed loop situation such that its motor actions inﬂuence its sensor inputs. The simplest form of a feedback is a reﬂex. Reﬂexes occur always “too late”; i.e., only after a (unpleasant, painful, dangerous) reﬂex-eliciting sensor event has occurred. This deﬁnes an objective problem which can be solved if another sensor input exists which can predict the primary reﬂex and can generate an earlier reaction. In contrast to previous approaches, our linear learning algorithm allows for an analytical proof that this system learns to apply feedforward control with the result that slow feedback loops are replaced by their equivalent feed-forward controller creating a forward model. In other words, learning turns the reactive system into a pro-active system. By means of a robot implementation we demonstrate the applicability of the theoretical results which can be used in a variety of different areas in physics and engineering.</p><p>5 0.40250939 <a title="169-lsi-5" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>6 0.38060248 <a title="169-lsi-6" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>7 0.37912267 <a title="169-lsi-7" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>8 0.37112999 <a title="169-lsi-8" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>9 0.36918086 <a title="169-lsi-9" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>10 0.35722008 <a title="169-lsi-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.33853248 <a title="169-lsi-11" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>12 0.33803841 <a title="169-lsi-12" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>13 0.33120424 <a title="169-lsi-13" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>14 0.32969797 <a title="169-lsi-14" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>15 0.32032922 <a title="169-lsi-15" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>16 0.3119944 <a title="169-lsi-16" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>17 0.29705024 <a title="169-lsi-17" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>18 0.29648426 <a title="169-lsi-18" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>19 0.29148224 <a title="169-lsi-19" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>20 0.27132368 <a title="169-lsi-20" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.053), (14, 0.035), (23, 0.042), (42, 0.099), (54, 0.151), (55, 0.044), (57, 0.025), (68, 0.028), (74, 0.092), (92, 0.029), (98, 0.09), (99, 0.219)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84131444 <a title="169-lda-1" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>2 0.71027476 <a title="169-lda-2" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>3 0.70862424 <a title="169-lda-3" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>4 0.70825839 <a title="169-lda-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.70616359 <a title="169-lda-5" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>6 0.7019372 <a title="169-lda-6" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>7 0.70060897 <a title="169-lda-7" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>8 0.70021975 <a title="169-lda-8" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>9 0.6987797 <a title="169-lda-9" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>10 0.69482702 <a title="169-lda-10" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>11 0.6933592 <a title="169-lda-11" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>12 0.69269061 <a title="169-lda-12" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>13 0.69200116 <a title="169-lda-13" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>14 0.69139981 <a title="169-lda-14" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>15 0.6898123 <a title="169-lda-15" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>16 0.68964887 <a title="169-lda-16" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>17 0.68948424 <a title="169-lda-17" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>18 0.68872923 <a title="169-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.68836898 <a title="169-lda-19" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>20 0.68769705 <a title="169-lda-20" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
