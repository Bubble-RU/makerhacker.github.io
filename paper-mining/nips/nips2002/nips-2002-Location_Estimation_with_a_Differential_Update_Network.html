<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>137 nips-2002-Location Estimation with a Differential Update Network</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-137" href="#">nips2002-137</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>137 nips-2002-Location Estimation with a Differential Update Network</h1>
<br/><p>Source: <a title="nips-2002-137-pdf" href="http://papers.nips.cc/paper/2341-location-estimation-with-a-differential-update-network.pdf">pdf</a></p><p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>Reference: <a title="nips-2002-137-reference" href="../nips2002_reference/nips-2002-Location_Estimation_with_a_Differential_Update_Network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. [sent-2, score-0.576]
</p><p>2 The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. [sent-3, score-0.352]
</p><p>3 We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. [sent-4, score-0.293]
</p><p>4 To update a trajectory of length t, the update takes O(t). [sent-5, score-0.409]
</p><p>5 When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement. [sent-6, score-0.128]
</p><p>6 Given a sequence of pairwise measurements between the elements of the chain (for example, their differences, corrupted by noise) we are asked to reﬁne our estimate of their values online, as these pairwise measurements become available. [sent-8, score-0.664]
</p><p>7 We use this mechanism to recover the trajectory of a robot given noisy measurements of its movement between points in its trajecotry. [sent-10, score-0.548]
</p><p>8 These pairwise displacements are thought of as noise corrupted measurements between the true but unknown poses to be recovered. [sent-11, score-0.529]
</p><p>9 The recovered trajectories are consistent in the sense that when the camera returns to an already visited position, its estimated pose is consistent with the pose recovered on the earlier visit. [sent-12, score-0.86]
</p><p>10 Pose change measurements between two points on the trajectory are obtained by bringing images of the environment acquired at each pose into registration with each other. [sent-13, score-0.919]
</p><p>11 The required transformation to affect the registration is the pose change measurement. [sent-14, score-0.443]
</p><p>12 There is a rich literature on computing pose changes from a pair of scans from an optical sensor: 2D [5, 6] and 3D transformations [7, 8, 9] from monocular cameras, or 3D transformations from range imagery [10, 11, 12] are a few examples. [sent-15, score-0.377]
</p><p>13 These have been used by [1, 2] in 3D model acquisition and by [3, 4] in robot navigation. [sent-16, score-0.149]
</p><p>14 The trajectory of the robot is deﬁned as the unknown pose from which each frame was acquired, and is maintained in a state vector which is updated as pose changes are measured. [sent-17, score-1.053]
</p><p>15 Figure 1: Independence structure of a differential update network. [sent-18, score-0.231]
</p><p>16 An alternative method estimates the pose of the robot with respect to ﬁxed features in the world. [sent-19, score-0.45]
</p><p>17 These methods represent the world as a set of features, such as corners, lines, and other geometric shapes in 3D [13, 14, 15] and match features between a scan at the current pose and the acquired world representation. [sent-20, score-0.373]
</p><p>18 However, measurements are still pairwise, since they depend on a feature and the poses of the camera. [sent-21, score-0.334]
</p><p>19 Because both the feature list and the poses are maintained in the state vector, the differential Update Framework can be applied to both scan-based methods and feature-based methods. [sent-22, score-0.247]
</p><p>20 Our algorithm incorporates each pose change measurement by updating the pose associated with every frame encountered. [sent-23, score-0.821]
</p><p>21 To insure that each update can happen in time linear to the length of the trajectory, the correlation structure of the state vector is approximated with a simpler Markov chain after each measurement. [sent-24, score-0.228]
</p><p>22 In addition, we focus on frame-based trajectory estimation due to the ready availability of pose change estimators, and to avoid the complexity of maintaining an explicit feature map. [sent-28, score-0.552]
</p><p>23 Sections 3 and 4 sketch existing batch and online methods for obtaining globally consistent trajectories. [sent-30, score-0.27]
</p><p>24 Section 5 derives the update rules for our algorithm, which is then applied to a 2D trajectory estimation in section 6. [sent-31, score-0.328]
</p><p>25 We assume the hidden variables xt have a Markov structure with known transition densities: T  p(xt |xt−1 ). [sent-33, score-0.708]
</p><p>26 p(X) = t=1  Pairwise measurements appear on the chain one by one. [sent-34, score-0.243]
</p><p>27 Conditioned on the hidden variables, these measurements are assumed to be independent: t p(ys |xs , xt ),  p(Y |X) = (s,t)∈M  where M is the set of pairs of hidden variables which have been measured. [sent-35, score-0.968]
</p><p>28 To apply this network to robot localization, let X = {xt }t=1. [sent-36, score-0.183]
</p><p>29 T be the trajectory of the robot up to time T , with each xt denoting its pose at time t. [sent-38, score-1.229]
</p><p>30 These poses can be represented using any parametrization of pose, for example as 3D rotations and translation, 2D  translations (which is what we use in section 6, or even non-rigid deformations such as afﬁne. [sent-39, score-0.124]
</p><p>31 (1)  t As the robot moves, the pose change estimator computes the motion y s of the robot from two scans of the environment. [sent-41, score-0.768]
</p><p>32 Given the true poses, we assume that these measurements t are independent of each other even when they share a common scan. [sent-42, score-0.21]
</p><p>33 We model each y s as being drawn from a Gaussian centered around xt − xs : t t p(ys |xs , xt ) = N (ys |xt − xs , Λy|xx)  (2) t ys  The online global estimation problem requires us to update p(X|Y ) as each in Y becomes available. [sent-43, score-1.984]
</p><p>34 The following section reviews a batch solution for computing p(X|Y ) using this model. [sent-44, score-0.134]
</p><p>35 Section 4 discusses a recursive approach with a similar running time as the batch version. [sent-45, score-0.194]
</p><p>36 Section 5 presents our approach, which performs these updates much faster by simplifying the output of the recursive solution after incorporating each measurement. [sent-46, score-0.097]
</p><p>37 Because the pose dynamics are Markovian, the inverse covariance Λ −1 is tri-diagonal. [sent-48, score-0.351]
</p><p>38 According X t to equation (2), the observations are drawn from ys = As,t X + ωs,t = xt − xs + ωs,t , with ωs,t white and Gaussian with covariance λs,t . [sent-49, score-1.115]
</p><p>39 X Y |X  (6)  If there are M measurements and T hidden variables, this computation will take O(T 2 M ) if performed naively. [sent-51, score-0.266]
</p><p>40 Note that if M > T , as is the case in the robot mapping problem, the alternate equations (5) and (6) can be used to obtain a running time of O(T 3 ). [sent-52, score-0.149]
</p><p>41 4 Online Linear Gaussian Solution Lu and Milios [3] proposed a recursive update for updating the trajectory X|Y old after t obtaining a new measurement ys . [sent-53, score-0.847]
</p><p>42 Because each measurement is independent of past measurements given the X’s, the update is: Bayes  t t (7) p(X|Y old , ys ) ∝ p(ys |X)p(X|Y old ). [sent-54, score-0.97]
</p><p>43 t 2 Using equations (3) and (4) to perform this update for one ys takes O(T ). [sent-55, score-0.382]
</p><p>44 After integrating M measurements, this yields the same ﬁnal cost as the batch update. [sent-56, score-0.134]
</p><p>45 Unfortunately, as measurements are incorporated, Λ −1 X|new becomes denser due to the accumulation of the rank 1 terms in equation (9), rendering this approach less effective. [sent-60, score-0.291]
</p><p>46 5 Approximate Online Solution To implement this idea in the general case, we resort to Assumed Density Filtering (ADF) [16]: we approximate p(X|Y old ) with a simpler distribution q(X|Y old ). [sent-63, score-0.323]
</p><p>47 To incorporate a t new measurement ys , we apply the update p(X|Y new )  Bayes  ∝  t p(ys |xs , xt )q(X|Y old ). [sent-64, score-1.214]
</p><p>48 new  (10) old  This new p(X|Y ) has a more complicated independence structure than q(X|Y ), so incorporating subsequent measurements would require more work and the resulting posterior would be even hairier. [sent-65, score-0.519]
</p><p>49 So we approximate it again with a q(X|Y new ) that has a simpler independence structure. [sent-66, score-0.106]
</p><p>50 Subsequent measurements can again be incorporated easily using this new q. [sent-67, score-0.235]
</p><p>51 2 shows how to incorporate a pairwise measurement on the resulting Markov chain using equation (10). [sent-72, score-0.263]
</p><p>52 1 Simplifying the independence structure We would like to approximate an arbitrary distribution which factors according to p(X) = t pt (xt |Pa[xt ]), using one which factors into q(X) = t qt (xt |Qa[xt ]). [sent-74, score-0.262]
</p><p>53 Here, Pa[xt ] are the parents of node xt in the graph prescribed by p(X), and Qa[xt ][xt ] = xt−1 are the parents of node xt as prescribed by q(X). [sent-75, score-1.31]
</p><p>54 The objective is to minimize: q ∗ = arg min KL q  pt  qt  =  p(X) ln x  p(X) . [sent-76, score-0.151]
</p><p>55 qt (xt |Qa[xt ]) i  (11)  After some manipulation, it can be shown that: ∗ qt = p(xt |Qa[xt ]). [sent-77, score-0.194]
</p><p>56 (12)  This says that the best conditional qt is built up from the corresponding pt by marginalizing out the conditions that were removed in the graph. [sent-78, score-0.202]
</p><p>57 2 Computing posterior transitions on a graph with a single loop This result suggests a simpliﬁcation to the update of equation (10). [sent-81, score-0.28]
</p><p>58 Because the ultimate goal is to compute q(X|Y new ), not p(X|Y new ), we only need to compute the posterior transitions p(xt |xt−1 , Y new ). [sent-82, score-0.163]
</p><p>59 We propose computing these transitions in three steps, one for the transitions to the left of xs , another for the loop, and the third for transitions to the right of x t . [sent-84, score-0.321]
</p><p>60 t For every s < τ < t, notice that p(y, xτ −1 , xt )p(xτ |xτ −1 , xt ) = p(y, xτ −1 , xτ , xt ), (13) because according to ﬁgure 5, p(xτ |xτ −1 , xt ) = p(xτ |xτ −1 , xt , y). [sent-89, score-2.95]
</p><p>61 If we could ﬁnd this joint distribution for all τ , we could ﬁnd p(xτ |xτ −1 , y) by marginalizing out xt and normalizing. [sent-90, score-0.641]
</p><p>62 We could also ﬁnd p(xτ |y) by marginalizing out both xt and xτ −1 , then normalizing. [sent-91, score-0.641]
</p><p>63 Finally, we could compute p(y, xτ , xt ) for the next τ in the iteration. [sent-92, score-0.622]
</p><p>64 So there are two missing pieces: The ﬁrst is p(y, xs , xt ) for starting the recursion. [sent-93, score-0.761]
</p><p>65 Computing this term is easy, because p(y|xs , xt ) is the given measurement model, and p(xs , xt ) can be obtained easily from the prior by successively applying the total probability theorem. [sent-94, score-1.286]
</p><p>66 Note that this quantity does not depend on the measurements and could be computed ofﬂine if we wanted to. [sent-96, score-0.21]
</p><p>67 The recursion for calculating it is: p(xτ |xτ −1 , xt )  Bayes  ∝  p(xt |xτ )  p(xt |xτ )p(xτ |xτ −1 ) dxi+1 p(xt |xi+1 )p(xτ +1 |xτ )  =  (14) (15)  The second equation describes a recursion which starts from t and goes down to s. [sent-97, score-0.706]
</p><p>68 Because of the backward nature of (15), p(xτ |xτ −1 , xt ) has to be computed using a pass which runs in the opposite direction of the process of (13). [sent-101, score-0.59]
</p><p>69 s Starting from τ = s − 1, compute p(y|xτ ) p(xτ |y)  dxτ +1 p(y|xτ +1 )p(xτ +1 |xτ )  = Bayes  ∝  p(y|xτ )p(xτ )  Bayes  p(xτ |xτ −1 , y) ∝ p(y|xτ )p(xτ |xτ −1 ) The recursion ﬁrst computes the inﬂuence of xτ on the observation, then computes the marginal and the transition probability. [sent-106, score-0.144]
</p><p>70 T Starting from τ = t, compute p(xτ |y)  =  dxτ −1 p(xτ |xτ −1 , y)p(xτ −1 |y)  p(xτ |xτ −1 , y) = p(xτ |xτ −1 ) The second identity follows from the independence structure on the right side of observed nodes. [sent-111, score-0.118]
</p><p>71 The robot took about 3 minutes to trace each path, producing about 6000 frames of data for each experiment. [sent-114, score-0.184]
</p><p>72 The trajectory was pre-marked on the ﬂoor so we could revisit speciﬁc locations (see the rightmost diagrams of ﬁgures 6(a,b)). [sent-115, score-0.189]
</p><p>73 The trajectory estimation worked at frame-rate, although it was processed ofﬂine to simplify data acquisition. [sent-117, score-0.218]
</p><p>74 In these experiments, the pose parameters were (x, y) locations on the ﬂoor. [sent-118, score-0.301]
</p><p>75 For each new frame, pose changes were computed with respect to at most three base frames. [sent-120, score-0.339]
</p><p>76 The selection of base frames was based on a measure of appearance between the current frame and all past frames. [sent-121, score-0.153]
</p><p>77 The pose change estimator was a Lucas-Kanade optical ﬂow tracker [24]. [sent-122, score-0.359]
</p><p>78 To compute pose displacements, we computed a robust average of the ﬂow vectors using an iterative outlier rejection scheme. [sent-123, score-0.333]
</p><p>79 We used the number of inlier ﬂow vectors as a crude estimate of the precision of t p(ys |xs , xt ). [sent-124, score-0.59]
</p><p>80 The middle plots compare our algorithm (blue) against the batch algorithm which uses equations (5) and (6) (black). [sent-126, score-0.134]
</p><p>81 Although our recovered trajectories don’t coincide exactly with the batch solutions, like the batch solutions, ours are smooth and consistent. [sent-127, score-0.36]
</p><p>82 Estimating the motion of each frame with respect to only the previous base frame yields an unsmooth trajectory (green). [sent-129, score-0.437]
</p><p>83 Furthermore, loops can’t be closed correctly (for example, the robot is not found to return to the origin). [sent-130, score-0.263]
</p><p>84 The red trajectory shows what happens when we assume individual poses are independent. [sent-132, score-0.344]
</p><p>85 This corresponds to using a diagonal matrix to represent the correlation between the poses (instead of the tri-diagonal inverse covariance matrix our algorithm uses). [sent-133, score-0.202]
</p><p>86 Notice that the resulting trajectory is not smooth, and loops are not well closed. [sent-134, score-0.251]
</p><p>87 By taking into account a minimum amount of correlation between frame poses, loops have been closed correctly and the trajectory is correctly found to be smooth. [sent-135, score-0.411]
</p><p>88 7 Conclusion We have presented a method for approximately computing the posterior distribution of a set of variables for which only pairwise measurements are available. [sent-136, score-0.382]
</p><p>89 We call the resulting structure a Differential Update Network and showed how to use Assumed Density Filtering to update the posterior as pairwise measurements become available. [sent-137, score-0.492]
</p><p>90 The two key insights were 1) how to approximate the posterior at each step to minimize KL divergence, and 2) how to compute transition densities on a graph with a single loop in closed form. [sent-138, score-0.197]
</p><p>91 We showed how to estimate globally consistent trajectories for a camera using this framework. [sent-139, score-0.209]
</p><p>92 Although the example used pose change measurements between scans of the environment, our framework can be applied to feature-based mapping and localization as well. [sent-141, score-0.626]
</p><p>93 (a)  (b)  Figure 3: Left, naive accumulation (green) and projecting trajectory to diagonal covariance (red). [sent-147, score-0.347]
</p><p>94 Loops are not closed well, and trajectory is not smooth. [sent-148, score-0.241]
</p><p>95 The zoomed areas show that in both naive approaches, there are large jumps in the trajectory, and the pose estimate is incorrect at revisited locations. [sent-149, score-0.332]
</p><p>96 Like the batch solution, our solution generates smooth and consistent trajectories. [sent-151, score-0.168]
</p><p>97 3d pose tracking with linear depth and brightness constraints. [sent-196, score-0.301]
</p><p>98 Robot pose estimation in unknown environments by matching 2d range scans. [sent-200, score-0.356]
</p><p>99 An iterative image registration technique with an application to stereo vision. [sent-283, score-0.109]
</p><p>100 Automatic camera recovery for closed or open image sequences. [sent-287, score-0.117]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xt', 0.59), ('pose', 0.301), ('ys', 0.272), ('measurements', 0.21), ('trajectory', 0.189), ('mx', 0.175), ('xs', 0.171), ('robot', 0.149), ('old', 0.136), ('batch', 0.134), ('poses', 0.124), ('update', 0.11), ('registration', 0.109), ('measurement', 0.106), ('qt', 0.097), ('pairwise', 0.092), ('differential', 0.09), ('frame', 0.08), ('rahimi', 0.073), ('qa', 0.072), ('camera', 0.065), ('loops', 0.062), ('trajectories', 0.059), ('hidden', 0.056), ('independence', 0.055), ('lu', 0.054), ('pt', 0.054), ('closed', 0.052), ('filtering', 0.052), ('eccv', 0.051), ('marginalizing', 0.051), ('scans', 0.051), ('globally', 0.051), ('online', 0.051), ('transitions', 0.05), ('motion', 0.05), ('covariance', 0.05), ('posterior', 0.049), ('accumulation', 0.049), ('bayes', 0.046), ('adf', 0.043), ('recursion', 0.042), ('kl', 0.041), ('thought', 0.04), ('robotics', 0.039), ('loop', 0.039), ('darrell', 0.039), ('february', 0.039), ('ijcv', 0.039), ('environment', 0.039), ('acquired', 0.038), ('incorporating', 0.038), ('base', 0.038), ('displacements', 0.036), ('markovian', 0.036), ('computes', 0.035), ('ow', 0.035), ('frames', 0.035), ('network', 0.034), ('consistent', 0.034), ('scan', 0.034), ('recursive', 0.034), ('chain', 0.033), ('change', 0.033), ('recovered', 0.033), ('maintained', 0.033), ('prescribed', 0.033), ('equation', 0.032), ('node', 0.032), ('compute', 0.032), ('markov', 0.031), ('blue', 0.031), ('red', 0.031), ('localization', 0.031), ('naive', 0.031), ('structure', 0.031), ('variables', 0.031), ('pages', 0.03), ('green', 0.03), ('autonomous', 0.03), ('ine', 0.03), ('estimation', 0.029), ('intelligence', 0.028), ('projecting', 0.028), ('mobile', 0.028), ('correlation', 0.028), ('finding', 0.027), ('corrupted', 0.027), ('andrew', 0.027), ('discusses', 0.026), ('environments', 0.026), ('kalman', 0.026), ('filter', 0.026), ('simpler', 0.026), ('divergence', 0.026), ('assumed', 0.025), ('approximate', 0.025), ('simplifying', 0.025), ('incorporated', 0.025), ('optical', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="137-tfidf-1" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>2 0.22022472 <a title="137-tfidf-2" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>Author: Samy Bengio</p><p>Abstract: This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same event. It is based on two other Markovian models, namely Asynchronous Input/ Output Hidden Markov Models and Pair Hidden Markov Models. An EM algorithm to train the model is presented, as well as a Viterbi decoder that can be used to obtain the optimal state sequence as well as the alignment between the two sequences. The model has been tested on an audio-visual speech recognition task using the M2VTS database and yielded robust performances under various noise conditions. 1</p><p>3 0.15259257 <a title="137-tfidf-3" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>4 0.1372003 <a title="137-tfidf-4" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>Author: Christopher G. Atkeson, Jun Morimoto</p><p>Abstract: A longstanding goal of reinforcement learning is to develop nonparametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of dimensionality. We have developed a trajectory-based approach, in which policies and value functions are represented nonparametrically along trajectories. These trajectories, policies, and value functions are updated as the value function becomes more accurate or as a model of the task is updated. We have applied this approach to periodic tasks such as hopping and walking, which required handling discount factors and discontinuities in the task dynamics, and using function approximation to represent value functions at discontinuities. We also describe extensions of the approach to make the policies more robust to modeling error and sensor noise.</p><p>5 0.10947926 <a title="137-tfidf-5" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>6 0.090656325 <a title="137-tfidf-6" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>7 0.090096101 <a title="137-tfidf-7" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>8 0.087436125 <a title="137-tfidf-8" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>9 0.08352375 <a title="137-tfidf-9" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>10 0.082978368 <a title="137-tfidf-10" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>11 0.082650952 <a title="137-tfidf-11" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>12 0.079476483 <a title="137-tfidf-12" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>13 0.078479946 <a title="137-tfidf-13" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>14 0.078248322 <a title="137-tfidf-14" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>15 0.071192451 <a title="137-tfidf-15" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>16 0.068647489 <a title="137-tfidf-16" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>17 0.067137823 <a title="137-tfidf-17" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>18 0.061035782 <a title="137-tfidf-18" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>19 0.060880959 <a title="137-tfidf-19" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>20 0.060324736 <a title="137-tfidf-20" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.179), (1, -0.003), (2, -0.115), (3, 0.058), (4, 0.023), (5, 0.074), (6, -0.032), (7, 0.1), (8, 0.175), (9, 0.122), (10, -0.016), (11, 0.012), (12, -0.036), (13, -0.161), (14, -0.199), (15, -0.032), (16, -0.003), (17, 0.073), (18, -0.007), (19, 0.041), (20, 0.082), (21, 0.03), (22, 0.081), (23, 0.111), (24, -0.12), (25, 0.206), (26, -0.092), (27, -0.008), (28, 0.087), (29, 0.002), (30, 0.098), (31, -0.089), (32, -0.034), (33, 0.079), (34, -0.195), (35, -0.02), (36, 0.05), (37, -0.019), (38, 0.132), (39, 0.03), (40, 0.019), (41, -0.059), (42, -0.089), (43, 0.045), (44, 0.063), (45, 0.048), (46, -0.087), (47, -0.044), (48, -0.109), (49, -0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96843135 <a title="137-lsi-1" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>2 0.67343605 <a title="137-lsi-2" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>Author: Samy Bengio</p><p>Abstract: This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same event. It is based on two other Markovian models, namely Asynchronous Input/ Output Hidden Markov Models and Pair Hidden Markov Models. An EM algorithm to train the model is presented, as well as a Viterbi decoder that can be used to obtain the optimal state sequence as well as the alignment between the two sequences. The model has been tested on an audio-visual speech recognition task using the M2VTS database and yielded robust performances under various noise conditions. 1</p><p>3 0.46856624 <a title="137-lsi-3" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>Author: Shinji Watanabe, Yasuhiro Minami, Atsushi Nakamura, Naonori Ueda</p><p>Abstract: In this paper, we propose a Bayesian framework, which constructs shared-state triphone HMMs based on a variational Bayesian approach, and recognizes speech based on the Bayesian prediction classiﬁcation; variational Bayesian estimation and clustering for speech recognition (VBEC). An appropriate model structure with high recognition performance can be found within a VBEC framework. Unlike conventional methods, including BIC or MDL criterion based on the maximum likelihood approach, the proposed model selection is valid in principle, even when there are insufﬁcient amounts of data, because it does not use an asymptotic assumption. In isolated word recognition experiments, we show the advantage of VBEC over conventional methods, especially when dealing with small amounts of data.</p><p>4 0.41971067 <a title="137-lsi-4" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><p>5 0.38883442 <a title="137-lsi-5" href="./nips-2002-A_Hierarchical_Bayesian_Markovian_Model_for_Motifs_in_Biopolymer_Sequences.html">7 nips-2002-A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Richard M. Karp, Stuart Russell</p><p>Abstract: We propose a dynamic Bayesian model for motifs in biopolymer sequences which captures rich biological prior knowledge and positional dependencies in motif structure in a principled way. Our model posits that the position-speciﬁc multinomial parameters for monomer distribution are distributed as a latent Dirichlet-mixture random variable, and the position-speciﬁc Dirichlet component is determined by a hidden Markov process. Model parameters can be ﬁt on training motifs using a variational EM algorithm within an empirical Bayesian framework. Variational inference is also used for detecting hidden motifs. Our model improves over previous models that ignore biological priors and positional dependence. It has much higher sensitivity to motifs during detection and a notable ability to distinguish genuine motifs from false recurring patterns.</p><p>6 0.37551805 <a title="137-lsi-6" href="./nips-2002-Minimax_Differential_Dynamic_Programming%3A_An_Application_to_Robust_Biped_Walking.html">144 nips-2002-Minimax Differential Dynamic Programming: An Application to Robust Biped Walking</a></p>
<p>7 0.37135202 <a title="137-lsi-7" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>8 0.36624029 <a title="137-lsi-8" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>9 0.36372867 <a title="137-lsi-9" href="./nips-2002-Handling_Missing_Data_with_Variational_Bayesian_Learning_of_ICA.html">101 nips-2002-Handling Missing Data with Variational Bayesian Learning of ICA</a></p>
<p>10 0.35576829 <a title="137-lsi-10" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>11 0.34748763 <a title="137-lsi-11" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>12 0.33969948 <a title="137-lsi-12" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>13 0.31832379 <a title="137-lsi-13" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>14 0.3173542 <a title="137-lsi-14" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>15 0.30956334 <a title="137-lsi-15" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>16 0.30804253 <a title="137-lsi-16" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>17 0.30738688 <a title="137-lsi-17" href="./nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">168 nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>18 0.3041428 <a title="137-lsi-18" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>19 0.30297795 <a title="137-lsi-19" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>20 0.27498406 <a title="137-lsi-20" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.015), (11, 0.016), (14, 0.016), (23, 0.048), (35, 0.241), (41, 0.016), (42, 0.067), (54, 0.136), (55, 0.047), (57, 0.025), (67, 0.022), (68, 0.051), (74, 0.09), (87, 0.014), (92, 0.027), (98, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84613144 <a title="137-lda-1" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>same-paper 2 0.83650744 <a title="137-lda-2" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>3 0.78176188 <a title="137-lda-3" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>4 0.64524502 <a title="137-lda-4" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>5 0.64483035 <a title="137-lda-5" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>6 0.64470917 <a title="137-lda-6" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>7 0.64467418 <a title="137-lda-7" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>8 0.64332509 <a title="137-lda-8" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>9 0.64321983 <a title="137-lda-9" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>10 0.64210129 <a title="137-lda-10" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>11 0.64182514 <a title="137-lda-11" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>12 0.64036953 <a title="137-lda-12" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>13 0.64026368 <a title="137-lda-13" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>14 0.63994002 <a title="137-lda-14" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>15 0.63899827 <a title="137-lda-15" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>16 0.63820273 <a title="137-lda-16" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>17 0.63790607 <a title="137-lda-17" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>18 0.6377368 <a title="137-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.63517898 <a title="137-lda-19" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>20 0.63512808 <a title="137-lda-20" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
