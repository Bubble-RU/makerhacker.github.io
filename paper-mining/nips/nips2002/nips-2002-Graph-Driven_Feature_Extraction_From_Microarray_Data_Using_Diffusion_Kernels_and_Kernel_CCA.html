<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-99" href="#">nips2002-99</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</h1>
<br/><p>Source: <a title="nips-2002-99-pdf" href="http://papers.nips.cc/paper/2273-graph-driven-feature-extraction-from-microarray-data-using-diffusion-kernels-and-kernel-cca.pdf">pdf</a></p><p>Author: Jean-philippe Vert, Minoru Kanehisa</p><p>Abstract: We present an algorithm to extract features from high-dimensional gene expression proﬁles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression proﬁles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. Function prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classiﬁer uses the vector of features instead of the original expression proﬁle to predict the functional class of a gene.</p><p>Reference: <a title="nips-2002-99-reference" href="../nips2002_reference/nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract We present an algorithm to extract features from high-dimensional gene expression proﬁles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. [sent-6, score-1.488]
</p><p>2 Function prediction experiments for the genes of the yeast S. [sent-8, score-0.583]
</p><p>3 Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classiﬁer uses the vector of features instead of the original expression proﬁle to predict the functional class of a gene. [sent-9, score-0.41]
</p><p>4 1 Introduction Microarray technology (DNA chips) is quickly becoming a major data provider in the postgenomics era, enabling the monitoring of the quantity of messenger RNA present in a cell for several thousands genes simultaneously. [sent-10, score-0.479]
</p><p>5 By submitting cells to various experimental conditions and comparing the expression proﬁles of different genes, a better understanding of the regulation mechanisms and functions of each gene is expected. [sent-11, score-0.3]
</p><p>6 Independently of microarray technology, decades of research in molecular biology have characterized the roles played by many genes as catalyzing chemical reactions in the cell. [sent-13, score-0.991]
</p><p>7 This information has now been integrated into databases such as KEGG [8], where series of successive chemical reactions arranged into pathways are represented, together with the genes catalyzing them. [sent-14, score-0.919]
</p><p>8 In particular one can extract from such a database a graph of genes, where two genes are linked whenever they catalyze two successive reactions. [sent-15, score-0.97]
</p><p>9 The question motivating this report is whether the knowledge of this graph can help improve the performance of gene function prediction algorithms based on microarray data  only. [sent-16, score-0.494]
</p><p>10 Our approach consists in translating this intuition as a regularized version of canonical component analysis between the genes mapped to two reproducible kernel Hilbert spaces, deﬁned respectively by a diffusion kernel [9] on the graph and a linear kernel on the expression proﬁles. [sent-18, score-1.587]
</p><p>11 This formulation leads to a well-posed problem equivalent to a generalized eigenvector problem [1]. [sent-19, score-0.101]
</p><p>12 2 Problem formulation  ¤ ¢ ¥£¡   ¡     The set of genes is represented by a discrete set of cardinality . [sent-20, score-0.479]
</p><p>13 The set of expression proﬁles is a mapping , where is the number of measurements and is the expression proﬁle of gene . [sent-21, score-0.486]
</p><p>14 In the sequel we assume that the set of proﬁles has been centered, i. [sent-22, score-0.048]
</p><p>15 ¦   The graph of genes extracted from the pathway database is represented by a simple graph , with the genes as vertices. [sent-26, score-1.503]
</p><p>16 Our goal is use this graph to extract features from the expression proﬁles. [sent-27, score-0.535]
</p><p>17 To this end we formally deﬁne a feature to be a real-valued mapping on the set of genes , and we denote by the set of possible . [sent-28, score-0.559]
</p><p>18 The set of centered features is denoted by  c bVA a` Y13$¢2! [sent-30, score-0.152]
</p><p>19 XAT'(%W#4VUTS(QIHE  "§ E R AP ¢ G ¢ '  FE  97  @8  6¢ 5   © § DC  BA  In particular linear features extracted from expression proﬁles are deﬁned, for any , by , for any (here and often in the sequel we use matrix notations, where is a column vector and its transpose). [sent-31, score-0.463]
</p><p>20 The normalized variance of a linear feature is deﬁned by:  E i G Bph  (1)    f dR   g  ! [sent-33, score-0.084]
</p><p>21  c b` 8(&$"  It is a ﬁrst indicator of the possible relevance of a linear vector. [sent-35, score-0.085]
</p><p>22 Linear features with a large normalized variance (1) are called relevant in the sequel, as opposed to irrelevant features. [sent-37, score-0.277]
</p><p>23 While the normalized variance (1) is an intrinsic property of the set of proﬁles, the knowledge of the graph suggests another criterion to judge “good” features. [sent-39, score-0.201]
</p><p>24 As genes linked together in the graph are supposed to participate in successive reactions in the cell, it is likely that the activation/inhibition of a biochemical pathway has a characteristic expression pattern shared by clusters of genes in the graph. [sent-40, score-1.764]
</p><p>25 More globally, the graph deﬁnes a structure on the set of genes, and therefore a notion of smoothness for any feature . [sent-41, score-0.339]
</p><p>26 A feature is called smooth if it varies slowly between adjacent nodes in the graph, and rugged otherwise. [sent-42, score-0.135]
</p><p>27 As just stated, features of interest are more likely to be smooth than other features. [sent-43, score-0.236]
</p><p>28 5  E R SA  We therefore end up with two criteria for extracting “good” features: they should simultaneously be relevant and smooth, the latter being deﬁned with respect to the gene graph. [sent-44, score-0.202]
</p><p>29 One way to extract such features is to look for pairs of features, , such that be smooth, be a relevant linear feature, and the correlation between and be as large as possible. [sent-45, score-0.314]
</p><p>30 vA   A h  E R  v A 7 u A  vA  E §    © H   &A;  for any feature, and a Suppose we can deﬁne a smoothness functional relevance functional for linear features, in such a way that lower values of  h v   X© ! [sent-47, score-0.411]
</p><p>31   where is a regularization parameter, is a way to extract smooth and relevant features. [sent-54, score-0.211]
</p><p>32 Irrelevance and ruggedness penalize any candidate pair through the functionals and , and controls the trade-off between relevance and smoothness on the one hand, and correlation on the other hand. [sent-55, score-0.334]
</p><p>33 amounts to ﬁnding and as correlated as possible (which is obtained by taking ), while forces to be relevant and to be smooth. [sent-56, score-0.058]
</p><p>34 Let us therefore show how to build two RKHS on the set of genes whose norms are smoothness (Section 4) and relevance (Section 5) functionals, respectively. [sent-58, score-0.728]
</p><p>35 v      3 Reproducible kernel Hilbert spaces and smoothness functionals Let us brieﬂy review basic properties of RKHS relevant for the sequel. [sent-59, score-0.416]
</p><p>36  R  0 6 p 7 7 54 ' 7   (%  2 a # ¥ 2 a # 10 ¢ 0   © v  § 0  Let be a Mercer kernel in the sense that the matrix symmetric positive semideﬁnite. [sent-61, score-0.213]
</p><p>37 Let be the linear span of consider a decomposition of as:  be , and  E i 3  i i 7 q0 f w¢ v y¡¡ A y¡¡ E R 8pi 3 R sSA 3 ` '   (%  2 a # ¥ ¢ e A i  0 t 0 f g¢ u 7 gf vu t 1 i r 7 c 9 v E R f7 1 v 3 r TsR 7 A  t i i 0 ¢ qprA ` A 0   i  0  i ' %  7 0g! [sent-62, score-0.033]
</p><p>38 The decomposition of any on this basis can be expressed as , where is the multiplicity of as an eigenvalue. [sent-64, score-0.166]
</p><p>39 An inner product can be deﬁned in as follows: (4)  The resulting Hilbert space is called a reproducing kernel Hilbert space, due to the following reproducing property: (5)  The inner product in can be easily expressed in a dual form as follows. [sent-65, score-0.515]
</p><p>40 Each can be decomposed as , where is unique up to the addition of an element of the null space of and is called the dual coordinate of . [sent-66, score-0.208]
</p><p>41 in regularization theory [14, 5], and we now adapt it to the discrete setting. [sent-70, score-0.041]
</p><p>42 A  x¡¡ A x¡¡  4 Smoothness functional on a graph A natural way to quantify the smoothness of a feature on a graph is by its energy at high frequency, as computed from its Fourier transform. [sent-71, score-0.608]
</p><p>43 Fourier transforms on graphs is a classical tool of spectral graph analysis [3, 11] which we brieﬂy recall now. [sent-72, score-0.167]
</p><p>44 Let be the adjacency matrix of the graph ( if there is an edge between and , otherwise) and the diagonal matrix of vertex degrees. [sent-73, score-0.229]
</p><p>45 Then the matrix is called the Laplacian of , and is known to share many properties with the continuous Laplacian [11]. [sent-74, score-0.064]
</p><p>46 The eigenvector belongs to the eigenvalue , whose multiplicity is equal to the number of connected components of . [sent-76, score-0.146]
</p><p>47 ¢  ¤ T¤   3 ¢u  6 ¤7    £ 7 ¢ 7 4  @D £ 7    7 ©£ § ¥ ¢ ¨¦gI  £ ¤¢  ¤ ¨¤   ¢  2a #  5  3 $¢   8  ¥  5  5  B  the eigenvalues of and an Let us denote by orthonormal set of associated eigenvectors. [sent-77, score-0.031]
</p><p>48 This basis is a discrete Fourier basis [3], and it is known that oscillates more and more on the graph as increases. [sent-78, score-0.231]
</p><p>49 The Fourier is the expansion in terms of this basis: decomposition of any feature    I  7  @  @D  A  B  B G    G  g3@ D ¢  ¢  8  A 7    7 & A  T A  A ¢ A 8 @9 E R TSA  is called the discrete Fourier transform of . [sent-79, score-0.116]
</p><p>50 0 6 3 4 X ©   §    For any monotonic decreasing mapping tion deﬁned by:  , let us now consider the func-  (10)  The mapping being assumed to take only positive values, the matrix is deﬁnite positive and is therefore a Mercer kernel on the set . [sent-84, score-0.341]
</p><p>51 The corresponding RKHS is the set of features , with norm given by: (11)      As increases, increases so decreases. [sent-85, score-0.198]
</p><p>52 As a result the norm (11) has a higher value on features which have a lot of energy at high frequency, and is therefore a natural smoothing functional. [sent-86, score-0.198]
</p><p>53 In that case we recover the diffusion kernel introduced and discussed in [9]. [sent-88, score-0.318]
</p><p>54 Considering other mapping would be beyond the scope of this report, so we restrict ourselves to this diffusion kernel in the sequel. [sent-89, score-0.348]
</p><p>55 Observe that it can be expressed using the matrix exponential as . [sent-90, score-0.08]
</p><p>56 1     I ) © £ ¦I00 § 2 1 ¢  )  5 Relevance functional   6 R )  4 7 10V¦ 4  a `d a`  c bWA ¢ c b(A   e R d  Gd  If has a projection onto the linear span of then . [sent-91, score-0.102]
</p><p>57 As a result the set of linear features can be parametrized by directions of the form , where is called the dual coordinate of and is deﬁned up to the addition of an element of the null space of the Gram matrix . [sent-92, score-0.391]
</p><p>58 The RKHS associated with this semideﬁnite positive matrix consists of the set of features of the form , where . [sent-93, score-0.212]
</p><p>59 ¢  h  d  E R t  10V@10 t (W# " ¢ d  ¦  ' % vu¦ f 10¦ ¢ 2 a # 0    v x¡¡ ` c ab` A x¡¡ d x¡¡ ¢ t v 0 0f t ¢ ¢ W ac Wu s `A y¡¡ c b` A ¡x¡ a '%# a t f t v 10 c b` A (W$" h SA R h ¢ 3   a` c A H 7 0g! [sent-95, score-0.032]
</p><p>60  t (&# " ¢ XA ¢  0  ' %  E i 3  can be expressed by (1), (6) and (8) as follows:  `   '¥      y¡¡    ! [sent-96, score-0.049]
</p><p>61  t (W# "  ¦  ' %  The variance of a feature   ' ¥  x¡¡ A x¡¡  As a result, a natural relevance functional to balance the term in (2) is the norm in the RKHS: , where is the RKHS associated with the linear kernel . [sent-98, score-0.436]
</p><p>62 4¢ u 7 0g0  ¦  y¡¡ c b` A x¡¡ ¢ ac ` u f v  A a  6 Extracting smooth correlations  `  1uV¦ 1U¢  ¦ I ) f £ 1 ¢ § 2  `  denote the diffusion kernel and denote the linear kernel , with associated RKHS and respectively. [sent-100, score-0.555]
</p><p>63 Taking as a smoothness function for any , and as a relevance func, we can express the maximization Problem (2) in a dual tional for any linear feature form as:  v 0  ¢ v E  x¡¡ A x¡¡ 2 v uA3  83 8R A   ¢ A  ¨ uW! [sent-101, score-0.365]
</p><p>64   70  Let  x¡¡ A x¡¡ u ¨  v 0  h R 8SA  ` R v A 7  A v 0 %" v v F $ $ 0 0 i  f t   i  0 %" v  F f 1 ¢ £ % ¡  a ¥ v ¨ ¨ t t ©7 i   ¤¢  t 0  0f i ¦  (12)  ¤ ¥  £  ¡ ¢  At ﬁrst sight it seems that (12) is the dual formulation of an optimization over , and not as in (2). [sent-102, score-0.108]
</p><p>65 E R i  0 ¢ (A  h  G E  ¡  ' ¥  y¡&(A x¡¡  ¡ ¨ x¡W(A x¡¡  h E ¢ v ¢  3  3  h T G E     3  A  A  Formulated as (12) the problem appears to be a generalization of canonical correlation analysis (CCA) known as kernel-CCA, discussed in [1]. [sent-105, score-0.114]
</p><p>66 Moreover, solving (13) provides a series of pairs of features , where , with decreasing values of for which the gradient is null, equivalent to the extraction of successive canonical directions with decreasing correlation in classical CCA. [sent-107, score-0.473]
</p><p>67 The resulting features and are therefore a set of features likely to have decreasing biological relevance when increases, and are the features we propose to extract in this report. [sent-108, score-0.711]
</p><p>68 ¤  1 7 0 ¤    ¢ ¤    ¢       ¤ £  ¡    ¢  7  @  ©  v 0 ¢ a v A  with  (13)  on the diagonal of the As discussed in [1] we regularize the problem (13) by adding matrix on the right-side, to be able to perform the Cholesky decomposition necessary to  v$      solve this problem. [sent-109, score-0.064]
</p><p>69 If is an generalized eigenvector solution of (14) belonging to the generalized eigenvalue , then belong to . [sent-111, score-0.17]
</p><p>70  '    3 $¢  @  ©  8  ©  ©  ¤  §  ¤ ¥    ©  8 §7 8 7 t i § f7 a© ©  ©    ©    7  ©  §  7  ©  7 Experiments We extracted from the LIGAND database of chemical compounds of reactions in biological pathways [6] a graph made of 774 genes of the budding yeast S. [sent-113, score-1.301]
</p><p>71 Cerevisiae, linked through 16,650 edges, where two genes are linked when they have the possibility to catalyze two successive reactions in the LIGAND database (i. [sent-114, score-1.015]
</p><p>72 e, two reactions such that the main product of the ﬁrst one be the main substrate of the second one). [sent-115, score-0.21]
</p><p>73 Concatenating several publicly available data, we ended up with 330 measurements for 6075 genes of the yeast, i. [sent-117, score-0.479]
</p><p>74 Following [4, 2] we work with the normalized logarithm of the ratio of expression levels of the genes between two experimental conditions. [sent-120, score-0.669]
</p><p>75 The functional classes of the yeast genes we consider are the one deﬁned by the January 10, 2002 version of the Comprehensive Yeast Genome Database (CYGD) [10], which is a comprehensive classiﬁcation of 3,936 genes into 259 categories. [sent-121, score-1.243]
</p><p>76 The 669 genes in the gene graph with known expression proﬁles were ﬁrst used to perform the feature extraction process described in this report. [sent-122, score-1.051]
</p><p>77 The resulting linear features were then extracted from the expression proﬁles of the disjoint set of 2,688 genes which are in the CYGD functional catalogue but not in the pathway database. [sent-123, score-1.024]
</p><p>78 We then performed functional classiﬁcation experiments on this set of 2,688 genes, using either the proﬁles themselves or the features extracted. [sent-124, score-0.254]
</p><p>79 All functional classes with more than 20 members in this set were tested (which amount to 115 categories). [sent-125, score-0.148]
</p><p>80 All vectors were scaled to unit length before being sent to the SVM, and all SVM use a radial basis kernel with unit width, i. [sent-127, score-0.185]
</p><p>81 £  v x¡&§  x¡© £ ¡ u ¡ § 2  1  ¢ ¨ u 7 0   ¦  Preliminary experiments to tune the two parameters of the algorithm, namely the width of the diffusion kernel and the regularization parameter , showed that and provide good performances. [sent-131, score-0.359]
</p><p>82 For these values we ﬁrst tested whether there exists an optimal number of features to be extracted for optimal gene function prediction. [sent-132, score-0.372]
</p><p>83 Figure 1 shows the performance of SVM using different numbers of features, in terms of ROC index averaged over all 115 classes. [sent-133, score-0.119]
</p><p>84 The ROC index is the area under the curve of false negative vs true positive, normalized to for a perfect classiﬁer and for a random classiﬁer. [sent-134, score-0.182]
</p><p>85 For each category the ROC index was averaged over random splitting of the data into training and test set, in the proportion . [sent-135, score-0.119]
</p><p>86 It appears that the more features are included, the better the performance averaged over all categories. [sent-136, score-0.189]
</p><p>87 A more precise analysis of the different classes shows however that some classes don’t follow the average trend and are better predicted by a smaller number of features, as shown on Figure 2 for categories best predicted by less than features. [sent-137, score-0.167]
</p><p>88 Finally Figure 3 compares, for each of the 115 categories, the ROC index for a SVM using the original expression proﬁles with a SVM using the vectors of 330 features. [sent-138, score-0.238]
</p><p>89 First of all, the performance reached for some classes such as heavy ion metal transporters shows that a ROC above 80% can be expected for several classes. [sent-141, score-0.204]
</p><p>90 Second, while many classes are apparently not learned by the SVM based on expression proﬁles (ROC around 50), the ROC based on extracted features of the same classes is around 60. [sent-142, score-0.476]
</p><p>91 This shows that there is hope to be able to predict more functional classes than previously thought [2] from microarray data, which is a good news since the amount of microarray data is expected to explode in the coming years. [sent-143, score-0.514]
</p><p>92 The method presented in this paper can be seen as an attempt to explore the possibilities of data mining and analysis provided by kernel methods. [sent-144, score-0.182]
</p><p>93 Few studies have used kernel methods other than SVM, and have used kernels other than Gaussian or polynomial kernels. [sent-145, score-0.197]
</p><p>94 In this report we tried to show how “exotic” kernels such as the diffusion kernel, and “exotic” methods such as kernel-CCA, can be adapted to particular problems, graph-driven feature extraction in our case. [sent-146, score-0.314]
</p><p>95 Exploring other possibilities of kernel methods in the data-rich ﬁeld of computational biology is among our future plans. [sent-147, score-0.182]
</p><p>96 100  ROC index based on expression profiles  90  80  70  60  50  40  30 40  50  60 70 80 ROC index based on extracted features  90  100  Figure 3: ROC index of a SVM classiﬁer based on expression proﬁles (y axis) or extracted features (x axis). [sent-148, score-1.014]
</p><p>97 Knowledge-based analysis of microarray gene expression data by using support vector machines. [sent-162, score-0.483]
</p><p>98 Spectral graph theory, volume 92 of CBMS Regional Conference Series. [sent-171, score-0.167]
</p><p>99 LIGAND: database of chemical compounds and reactions in biological pathways. [sent-192, score-0.438]
</p><p>100 MIPS: a database for genomes and protein u sequences. [sent-226, score-0.076]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('genes', 0.479), ('roc', 0.24), ('pro', 0.219), ('reactions', 0.21), ('microarray', 0.183), ('les', 0.172), ('graph', 0.167), ('diffusion', 0.165), ('rkhs', 0.156), ('expression', 0.156), ('kernel', 0.153), ('features', 0.152), ('gene', 0.144), ('smoothness', 0.122), ('dual', 0.108), ('yeast', 0.104), ('functional', 0.102), ('svm', 0.094), ('reproducible', 0.091), ('relevance', 0.085), ('fourier', 0.084), ('functionals', 0.083), ('index', 0.082), ('kanehisa', 0.079), ('ligand', 0.079), ('extracted', 0.076), ('database', 0.076), ('categories', 0.075), ('successive', 0.074), ('hilbert', 0.072), ('canonical', 0.07), ('acid', 0.07), ('cca', 0.068), ('chemical', 0.067), ('null', 0.067), ('nucleic', 0.067), ('va', 0.067), ('vs', 0.066), ('eigenvector', 0.063), ('linked', 0.062), ('extract', 0.06), ('pathway', 0.059), ('relevant', 0.058), ('vu', 0.058), ('extraction', 0.055), ('catalyze', 0.052), ('catalyzing', 0.052), ('cygd', 0.052), ('multiplicity', 0.052), ('transporters', 0.052), ('smooth', 0.052), ('xa', 0.052), ('semide', 0.05), ('feature', 0.05), ('expressed', 0.049), ('sequel', 0.048), ('classes', 0.046), ('reproducing', 0.046), ('norm', 0.046), ('exotic', 0.046), ('eisen', 0.046), ('kegg', 0.046), ('compounds', 0.046), ('participate', 0.046), ('regional', 0.046), ('kernels', 0.044), ('correlation', 0.044), ('cerevisiae', 0.042), ('goto', 0.042), ('metal', 0.042), ('norms', 0.042), ('regularization', 0.041), ('inner', 0.04), ('biological', 0.039), ('decreasing', 0.039), ('generalized', 0.038), ('averaged', 0.037), ('bach', 0.037), ('pathways', 0.037), ('classi', 0.035), ('laplacian', 0.035), ('normalized', 0.034), ('called', 0.033), ('comprehensive', 0.033), ('heavy', 0.033), ('decomposition', 0.033), ('likely', 0.032), ('sa', 0.032), ('ra', 0.032), ('ac', 0.032), ('coordinates', 0.032), ('basis', 0.032), ('eigenvalue', 0.031), ('ion', 0.031), ('orthonormal', 0.031), ('matrix', 0.031), ('mapping', 0.03), ('positive', 0.029), ('possibilities', 0.029), ('brown', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="99-tfidf-1" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>Author: Jean-philippe Vert, Minoru Kanehisa</p><p>Abstract: We present an algorithm to extract features from high-dimensional gene expression proﬁles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression proﬁles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. Function prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classiﬁer uses the vector of features instead of the original expression proﬁle to predict the functional class of a gene.</p><p>2 0.18117872 <a title="99-tfidf-2" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>Author: Eleazar Eskin, Jason Weston, William S. Noble, Christina S. Leslie</p><p>Abstract: We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classiﬁcation problem. These kernels measure sequence similarity based on shared occurrences of -length subsequences, counted with up to mismatches, and do not rely on any generative model for the positive training sequences. We compute the kernels efﬁciently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classiﬁer performs as well as the Fisher kernel, the most successful method for remote homology detection, while achieving considerable computational savings. ¡ ¢</p><p>3 0.1756566 <a title="99-tfidf-3" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>Author: Cheng S. Ong, Robert C. Williamson, Alex J. Smola</p><p>Abstract: We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves deﬁning a Reproducing Kernel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.</p><p>4 0.16522339 <a title="99-tfidf-4" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>5 0.16444935 <a title="99-tfidf-5" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classiﬁcation.</p><p>6 0.15246451 <a title="99-tfidf-6" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>7 0.14465259 <a title="99-tfidf-7" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>8 0.12875979 <a title="99-tfidf-8" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>9 0.11570843 <a title="99-tfidf-9" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>10 0.11461932 <a title="99-tfidf-10" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>11 0.11281651 <a title="99-tfidf-11" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>12 0.10277379 <a title="99-tfidf-12" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>13 0.088599212 <a title="99-tfidf-13" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>14 0.088357307 <a title="99-tfidf-14" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>15 0.088020675 <a title="99-tfidf-15" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>16 0.087442607 <a title="99-tfidf-16" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>17 0.083560705 <a title="99-tfidf-17" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>18 0.083293431 <a title="99-tfidf-18" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>19 0.083268106 <a title="99-tfidf-19" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>20 0.076885782 <a title="99-tfidf-20" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.234), (1, -0.135), (2, 0.127), (3, -0.092), (4, -0.094), (5, -0.045), (6, 0.096), (7, 0.06), (8, 0.002), (9, 0.021), (10, -0.006), (11, 0.103), (12, 0.026), (13, 0.109), (14, 0.054), (15, -0.008), (16, -0.021), (17, -0.077), (18, -0.057), (19, 0.018), (20, -0.073), (21, -0.007), (22, 0.035), (23, -0.063), (24, -0.113), (25, 0.028), (26, 0.037), (27, 0.02), (28, 0.098), (29, 0.089), (30, 0.048), (31, -0.128), (32, 0.029), (33, -0.024), (34, -0.083), (35, -0.068), (36, 0.127), (37, 0.009), (38, -0.076), (39, 0.027), (40, 0.044), (41, 0.042), (42, -0.082), (43, 0.095), (44, 0.012), (45, -0.068), (46, 0.121), (47, 0.063), (48, 0.097), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9620204 <a title="99-lsi-1" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>Author: Jean-philippe Vert, Minoru Kanehisa</p><p>Abstract: We present an algorithm to extract features from high-dimensional gene expression proﬁles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression proﬁles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. Function prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classiﬁer uses the vector of features instead of the original expression proﬁle to predict the functional class of a gene.</p><p>2 0.64329094 <a title="99-lsi-2" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>Author: Eleazar Eskin, Jason Weston, William S. Noble, Christina S. Leslie</p><p>Abstract: We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classiﬁcation problem. These kernels measure sequence similarity based on shared occurrences of -length subsequences, counted with up to mismatches, and do not rely on any generative model for the positive training sequences. We compute the kernels efﬁciently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classiﬁer performs as well as the Fisher kernel, the most successful method for remote homology detection, while achieving considerable computational savings. ¡ ¢</p><p>3 0.63394433 <a title="99-lsi-3" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>Author: Cheng S. Ong, Robert C. Williamson, Alex J. Smola</p><p>Abstract: We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves deﬁning a Reproducing Kernel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.</p><p>4 0.61822337 <a title="99-lsi-4" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>Author: Olivier Bousquet, Daniel Herrmann</p><p>Abstract: We investigate data based procedures for selecting the kernel when learning with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors ﬁx. This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to overﬁtting. We thus propose a suitable way of constraining the class. We use an efﬁcient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach.</p><p>5 0.60239577 <a title="99-lsi-5" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classiﬁcation.</p><p>6 0.57413256 <a title="99-lsi-6" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>7 0.57295769 <a title="99-lsi-7" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>8 0.52882493 <a title="99-lsi-8" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>9 0.52818108 <a title="99-lsi-9" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>10 0.50986207 <a title="99-lsi-10" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>11 0.49631226 <a title="99-lsi-11" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>12 0.48970854 <a title="99-lsi-12" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>13 0.47582871 <a title="99-lsi-13" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>14 0.47078964 <a title="99-lsi-14" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>15 0.44369769 <a title="99-lsi-15" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>16 0.44214255 <a title="99-lsi-16" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>17 0.44193587 <a title="99-lsi-17" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>18 0.4324103 <a title="99-lsi-18" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>19 0.38366744 <a title="99-lsi-19" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>20 0.37706831 <a title="99-lsi-20" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.036), (23, 0.016), (38, 0.281), (42, 0.066), (54, 0.157), (55, 0.048), (67, 0.02), (68, 0.03), (74, 0.122), (79, 0.01), (92, 0.035), (98, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85141098 <a title="99-lda-1" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>Author: Jean-philippe Vert, Minoru Kanehisa</p><p>Abstract: We present an algorithm to extract features from high-dimensional gene expression proﬁles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression proﬁles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. Function prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classiﬁer uses the vector of features instead of the original expression proﬁle to predict the functional class of a gene.</p><p>2 0.77697808 <a title="99-lda-2" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>Author: Nuno Vasconcelos</p><p>Abstract: We address the question of feature selection in the context of visual recognition. It is shown that, besides efﬁcient from a computational standpoint, the infomax principle is nearly optimal in the minimum Bayes error sense. The concept of marginal diversity is introduced, leading to a generic principle for feature selection (the principle of maximum marginal diversity) of extreme computational simplicity. The relationships between infomax and the maximization of marginal diversity are identiﬁed, uncovering the existence of a family of classiﬁcation procedures for which near optimal (in the Bayes error sense) feature selection does not require combinatorial search. Examination of this family in light of recent studies on the statistics of natural images suggests that visual recognition problems are a subset of it.</p><p>3 0.62920856 <a title="99-lda-3" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>4 0.62629914 <a title="99-lda-4" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classiﬁcation problems. The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model. This paper gives a theoretical analysis about how class information is preserved in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions. When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions. So we will develop a novel but simple clustering algorithm specialized for the Fisher score, which can exploit important dimensions. This algorithm is successfully tested in experiments with artiﬁcial data and real data (amino acid sequences).</p><p>5 0.62421072 <a title="99-lda-5" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>6 0.62393385 <a title="99-lda-6" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>7 0.62353581 <a title="99-lda-7" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>8 0.62057728 <a title="99-lda-8" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>9 0.62048376 <a title="99-lda-9" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>10 0.61894155 <a title="99-lda-10" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>11 0.61816096 <a title="99-lda-11" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>12 0.61810714 <a title="99-lda-12" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>13 0.61769921 <a title="99-lda-13" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>14 0.61758941 <a title="99-lda-14" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>15 0.61734283 <a title="99-lda-15" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>16 0.61701185 <a title="99-lda-16" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>17 0.61532599 <a title="99-lda-17" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>18 0.61488283 <a title="99-lda-18" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>19 0.61482942 <a title="99-lda-19" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>20 0.61476707 <a title="99-lda-20" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
