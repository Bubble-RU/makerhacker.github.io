<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-16" href="#">nips2002-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</h1>
<br/><p>Source: <a title="nips-2002-16-pdf" href="http://papers.nips.cc/paper/2277-a-prototype-for-automatic-recognition-of-spontaneous-facial-actions.pdf">pdf</a></p><p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>Reference: <a title="nips-2002-16-reference" href="../nips2002_reference/nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu       Abstract We present ongoing work on a project for automatic recognition of spontaneous facial actions. [sent-11, score-1.013]
</p><p>2 Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. [sent-12, score-1.107]
</p><p>3 Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. [sent-13, score-0.959]
</p><p>4 Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. [sent-14, score-1.181]
</p><p>5 We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. [sent-16, score-0.357]
</p><p>6 This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. [sent-17, score-0.797]
</p><p>7 The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). [sent-18, score-0.837]
</p><p>8 We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. [sent-19, score-0.879]
</p><p>9 1 Introduction Much of the early work on computer vision applied to facial expressions focused on recognizing a few prototypical expressions of emotion produced on command (e. [sent-21, score-1.064]
</p><p>10 Extending these systems to spontaneous facial behavior is a critical step forward for applications of this technology. [sent-25, score-0.881]
</p><p>11 Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. [sent-26, score-1.107]
</p><p>12 Spontaneous facial expressions are mediated by a distinct neural pathway from posed expressions. [sent-27, score-0.873]
</p><p>13 The pyramidal motor system, originating in the cortical motor strip, drives voluntary facial actions, whereas involuntary, emotional facial expressions appear to originate in a subcortical motor circuit involving  the basal ganglia, limbic system, and the cingulate motor area (e. [sent-28, score-1.633]
</p><p>14 Psychophysical work has shown that spontaneous facial expressions differ from posed expressions in a number of ways [6]. [sent-31, score-1.274]
</p><p>15 Subjects often contract different facial muscles when asked to pose an emotion such as fear versus when they are actually experiencing fear. [sent-32, score-0.852]
</p><p>16 Spontaneous expressions have a fast and smooth onset, with apex coordination, in which muscle contractions in different parts of the face peak at the same time. [sent-35, score-0.423]
</p><p>17 In posed expressions, the onset tends to be slow and jerky, and the muscle contractions typically do not peak simultaneously. [sent-36, score-0.189]
</p><p>18 Spontaneous facial expressions often contain much information beyond what is conveyed by basic emotion categories, such as happy, sad, or surprised. [sent-37, score-0.873]
</p><p>19 Instead of classifying expressions into a few basic emotion categories, the work presented here attempts to measure the full range of facial behavior by recognizing facial animation units that comprise facial expressions. [sent-39, score-2.226]
</p><p>20 FACS [7] is the leading method for measuring facial movement in behavioral science. [sent-41, score-0.763]
</p><p>21 In FACS, human coders decompose facial expressions into action units (AUs) that roughly correspond to independent muscle movements in the face (see Figure 1). [sent-43, score-1.177]
</p><p>22 Ekman and Friesen described 46 independent facial movements, or ”facial actions” (Figure 1). [sent-44, score-0.647]
</p><p>23 These facial actions are analogous to phonemes for facial expression. [sent-45, score-1.384]
</p><p>24 Over 7000 distinct combinations of such movements have been observed in spontaneous behavior. [sent-46, score-0.284]
</p><p>25 AU1  Inner Brow Raiser (Central Frontalis)  1+2  AU2 Outer Brow Raiser (Lateral Frontalis)  1+4  AU4 Brow Lower (Corrugator, Depressor Supercilli, Depressor Glaballae)  1+2+4  Figure 1: The Facial Action Coding System decomposes facial expressions into component actions. [sent-47, score-0.814]
</p><p>26 The three individual brow region actions and selected combinations are illustrated. [sent-48, score-0.4]
</p><p>27 When subjects pose fear they often perform 1+2 (top right), whereas spontaneous fear reliably elicits 1+2+4 (bottom right) [6]. [sent-49, score-0.467]
</p><p>28 It does not apply interpretive labels to expressions but rather a description of physical changes in the face. [sent-51, score-0.167]
</p><p>29 This enables studies of new relationships between facial movement and internal state, such as the facial signals of stress or fatigue. [sent-52, score-1.345]
</p><p>30 FACS codes for all independent motions of the face observed by behavioral psychologists over 20 years of study. [sent-54, score-0.214]
</p><p>31 Automated facial action coding would be effective for human-computer interaction tools and low bandwidth facial animation coding, and would have a tremendous impact on behavioral science by making objective measurement more accessible. [sent-57, score-1.56]
</p><p>32 There has been an emergence of groups that analyze facial expressing into elementary movements. [sent-58, score-0.647]
</p><p>33 For example, Essa and Pentland [8] and Yacoob and Davis [16] proposed methods to analyze expressions into elementary movements using an animation style coding system inspired by FACS. [sent-59, score-0.368]
</p><p>34 Eric Petajan’s group has also worked for many years on  methods for automatic coding of facial expressions in the style of MPEG4 [5], which codes movement of a set of facial feature points. [sent-60, score-1.66]
</p><p>35 It also does not encode the wrinkles and bulges that are critical for distinguishing some facial muscle activations that are difﬁcult to differentiate using motion alone yet can have different behavioral implications (e. [sent-62, score-0.771]
</p><p>36 ) One other group has focused on automatic FACS recognition as a tool for behavioral research, lead by Jeff Cohn and Takeo Kanade. [sent-65, score-0.197]
</p><p>37 2 Factorizing rigid head motion from nonrigid facial deformations The most difﬁcult technical challenge that came with spontaneous behavior was the presence of out-of-plane rotations due to the fact that people often nod or turn their head as they communicate with others. [sent-68, score-1.312]
</p><p>38 Our approach to expression recognition is based on statistical methods applied directly to ﬁlter bank image representations. [sent-69, score-0.176]
</p><p>39 We ﬁt 3D face models to the image plane, texture those models using the original image frame, then rotate the model to frontal views, warp it to a canonical face geometry, and then render the model back into the image plane. [sent-72, score-0.507]
</p><p>40 This allowed us to factor out image variation due to rigid head rotations from variations due to nonrigid face deformations. [sent-74, score-0.44]
</p><p>41 These parameters are retained for analysis of the relation of rigid head dynamics to emotional and cognitive state. [sent-76, score-0.248]
</p><p>42 Since our goal was to explore the use of 3D models to handle out-of-plane rotations for expression recognition, we ﬁrst tested the system using hand-labeling to give the position of 8 facial landmarks. [sent-77, score-0.753]
</p><p>43 Although human labeling can be highly precise, the labels employed here had substantial error due to inattention when the face moved. [sent-79, score-0.188]
</p><p>44 Hence it may be realistic to suppose that a fully automatic head pose tracker may achieve at least this level of accuracy. [sent-82, score-0.298]
</p><p>45 First camera parameters and face geometry are jointly estimated using an iterative least squares technique b. [sent-87, score-0.176]
</p><p>46 Next head pose is estimated in each frame using stochastic particle ﬁltering. [sent-88, score-0.357]
</p><p>47 Each particle is a head model at a particular orientation and scale. [sent-89, score-0.194]
</p><p>48 When landmark positions in the image plane are known, the problem of 3D pose estimation is relatively easy to solve. [sent-90, score-0.161]
</p><p>49 We begin with a canonical wire-mesh face model and adapt it to the face of a particular individual by using 30 image frames in which 8 facial features have been labeled by hand. [sent-91, score-1.005]
</p><p>50 A scattered data interpolation technique is then used to modify the canonical 3D face model so that it ﬁts the 8 feature positions [14]. [sent-93, score-0.181]
</p><p>51 Once camera parameters and 3D face geometry are known, we use a stochastic particle ﬁltering approach [11] to estimate the most likely rotation and translation parameters of the 3D face model in each video frame. [sent-94, score-0.477]
</p><p>52 3 Action unit recognition Database of spontaneous facial expressions. [sent-96, score-0.94]
</p><p>53 We employed a dataset of spontaneous facial expressions from freely behaving individuals. [sent-97, score-1.095]
</p><p>54 The video sequences contained out of plane head rotation up to 75 degrees. [sent-99, score-0.333]
</p><p>55 The facial behaviors in one minute of video per subject were scored frame by frame by 2 teams experts on the FACS system, one lead by Mark Frank at Rutgers, and another lead by Jeffrey Cohn at U. [sent-102, score-0.894]
</p><p>56 While the database we used was rather large for current digital video storage standards, in practice the number of spontaneous examples of each action unit in the database was relatively small. [sent-104, score-0.521]
</p><p>57 These three facial actions have relevance to applications such as monitoring of alertness, anxiety, and confusion. [sent-107, score-0.737]
</p><p>58 The system presented here employs general purpose learning mechanisms that can be applied to recognition of any facial action once sufﬁcient training data is available. [sent-108, score-0.863]
</p><p>59 There is no need to develop special purpose feature measures to recognize additional facial actions. [sent-109, score-0.647]
</p><p>60 First, head pose is estimated, and images are warped to frontal views and canonical face geometry. [sent-111, score-0.596]
</p><p>61 The warped images are then passed through a bank of Gabor ﬁlters. [sent-112, score-0.186]
</p><p>62 SVM’s are then trained to classify facial actions from the Gabor representation in individual video frames. [sent-113, score-0.876]
</p><p>63 The output trajectories of the SVM’s for full video sequences are then channeled to hidden Markov models. [sent-114, score-0.262]
</p><p>64 Head pose was estimated in the video sequences using a particle ﬁlter with 100 particles. [sent-117, score-0.291]
</p><p>65 Face images were then warped onto a face model with canonical face geometry, rotated to frontal, and then projected back into the image plane. [sent-118, score-0.501]
</p><p>66 This alignment was used to deﬁne and crop a subregion of the face image containing the eyes and brows. [sent-119, score-0.229]
</p><p>67 Nonlinear SVM’s were trained to recognize facial actions in individual video frames. [sent-128, score-0.876]
</p><p>68 The training samples for the SVM’s were the action peaks as identiﬁed by the FACS experts, and negative examples were randomly selected frames matched by subject. [sent-129, score-0.185]
</p><p>69 Trajectories of SVM outputs for the full video sequence of test subjects were then channeled to hidden Markov models (HMM’s). [sent-132, score-0.261]
</p><p>70 The HMM’s were trained to classify facial actions without using information about which frame contained the action peak. [sent-133, score-0.948]
</p><p>71 The face on the bottom right is an original frame from the dataset. [sent-136, score-0.202]
</p><p>72 The curve shows the output of the blink detector for the video sequence. [sent-139, score-0.226]
</p><p>73 SVM’s were ﬁrst trained to discriminate images containing the peak of blink sequences from randomly selected images containing no blinks. [sent-142, score-0.376]
</p><p>74 The SVM outputs provide in-  formation about FACS dynamics that was previously unavailable by human coding due to time constraints. [sent-152, score-0.181]
</p><p>75 Current coding methods provide only the beginning and end of the action, along with the location and magnitude of the action unit peak. [sent-153, score-0.166]
</p><p>76 Brow raise trajectories of SVM outputs for one subject. [sent-164, score-0.166]
</p><p>77 HMM’s were trained to classify action units from the trajectories of SVM outputs. [sent-166, score-0.177]
</p><p>78 HMM’s addressed the case in which the frame containing the action unit peak is unknown. [sent-167, score-0.208]
</p><p>79 Two hidden Markov models, one for Blinks and one for random sequences matched by subject and length, were trained and tested using leave-one-out cross-validation. [sent-168, score-0.164]
</p><p>80 Separate HMM’s were also trained to perform each of the 2-category brow movement discriminations in image sequences. [sent-182, score-0.456]
</p><p>81 Figure 5c shows example output trajectories for the SVM trained to discriminate Brow Raise from Random matched sequences. [sent-184, score-0.208]
</p><p>82 As with the blinks, we see that despite not being trained to indicate AU intensity, an emergent property of the SVM output was the magnitude of the brow raise. [sent-185, score-0.412]
</p><p>83 A similar pattern was obtained for the brow movements, except that nonlinear SVMs applied directly to difference images did not perform as well as nonlinear SVM’s applied to Gabors. [sent-211, score-0.428]
</p><p>84 5 Conclusions We explored an approach for handling out-of-plane head rotations in automatic recognition of spontaneous facial expressions from freely behaving individuals. [sent-213, score-1.382]
</p><p>85 The approach ﬁts a 3D model of the face and rotates it back to a canonical pose (e. [sent-214, score-0.268]
</p><p>86 We found that machine learning techniques applied directly to the warped images is a promising approach for automatic coding of spontaneous facial expressions. [sent-217, score-1.172]
</p><p>87 This approach employed general purpose learning mechanisms that can be applied to the recognition of any facial action. [sent-218, score-0.756]
</p><p>88 The approach is parsimonious and does not require deﬁning a different set of feature parameters or image operations for each facial action. [sent-219, score-0.698]
</p><p>89 While the database we used was rather large for current digital video storage standards, in practice the number of spontaneous examples of each action unit in the database was relatively small. [sent-220, score-0.521]
</p><p>90 Based on these results, we estimate that a database of 250 minutes of coded, spontaneous behavior would be sufﬁcient to train the system on the vast majority of facial actions. [sent-223, score-0.959]
</p><p>91 For example, the output of the SVM ﬁlter matched to the blink detector could be potentially used to measure the dynamics of eyelid closure, even though the system was not designed to explicitly detect the contours of the eyelid and measure the closure. [sent-225, score-0.326]
</p><p>92 ) The results presented here employed hand-labeled feature points for the head pose tracking step. [sent-227, score-0.274]
</p><p>93 We are presently developing a fully automated head pose tracker that integrates particle ﬁltering with a system developed by Matthew Brand for automatic real-time 3D tracking based on optic ﬂow [3]. [sent-228, score-0.479]
</p><p>94 All of the pieces of the puzzle are ready for the development of automated systems that recognize spontaneous facial actions at the level of detail required by FACS. [sent-229, score-0.996]
</p><p>95 Automatic analysis of of spontaneous facial behavior: A ﬁnal project report. [sent-246, score-0.881]
</p><p>96 3-D head pose estimation from video by nonlinear stochastic particle ﬁltering. [sent-256, score-0.408]
</p><p>97 A comparison of gabor ﬁlter methods for automatic detection of facial landmarks. [sent-305, score-0.808]
</p><p>98 ﬁnal report and panel recommendations for automatic facial action coding. [sent-314, score-0.811]
</p><p>99 The neuropsychology of facial expression: A review of the neurological and psychological mechanisms for producing facial expressions. [sent-346, score-1.319]
</p><p>100 Recognizing human facial expressions from long image sequences using optical ﬂow. [sent-351, score-0.955]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facial', 0.647), ('brow', 0.31), ('facs', 0.266), ('spontaneous', 0.234), ('svm', 0.174), ('expressions', 0.167), ('head', 0.138), ('face', 0.126), ('blink', 0.103), ('video', 0.095), ('action', 0.091), ('actions', 0.09), ('warped', 0.089), ('gabor', 0.088), ('pose', 0.087), ('raise', 0.082), ('subjects', 0.08), ('hmm', 0.078), ('frame', 0.076), ('au', 0.075), ('coding', 0.075), ('automatic', 0.073), ('matched', 0.067), ('behavioral', 0.065), ('blinks', 0.064), ('emotion', 0.059), ('recognition', 0.059), ('posed', 0.059), ('muscle', 0.059), ('particle', 0.056), ('canonical', 0.055), ('images', 0.054), ('sequences', 0.053), ('eyes', 0.052), ('image', 0.051), ('movement', 0.051), ('movements', 0.05), ('frontal', 0.047), ('trained', 0.044), ('braathen', 0.044), ('channeled', 0.044), ('emotional', 0.044), ('gabors', 0.044), ('nonrigid', 0.044), ('standards', 0.044), ('bank', 0.043), ('trajectories', 0.042), ('rotations', 0.042), ('outputs', 0.042), ('system', 0.041), ('peak', 0.041), ('rigid', 0.039), ('human', 0.037), ('database', 0.037), ('animation', 0.035), ('presently', 0.035), ('fear', 0.033), ('nonlinear', 0.032), ('motor', 0.032), ('bartlett', 0.03), ('contractions', 0.03), ('depressor', 0.03), ('ekman', 0.03), ('emergent', 0.03), ('eyelid', 0.03), ('fasel', 0.03), ('frontalis', 0.03), ('nod', 0.03), ('raiser', 0.03), ('smiles', 0.03), ('yacoob', 0.03), ('lter', 0.028), ('output', 0.028), ('examples', 0.027), ('discriminate', 0.027), ('dynamics', 0.027), ('geometry', 0.026), ('versus', 0.026), ('coded', 0.026), ('essa', 0.026), ('prototyped', 0.026), ('employed', 0.025), ('automated', 0.025), ('mechanisms', 0.025), ('tracking', 0.024), ('recognizing', 0.024), ('camera', 0.024), ('rotation', 0.024), ('warping', 0.023), ('rutgers', 0.023), ('comparative', 0.023), ('deliberately', 0.023), ('psychologists', 0.023), ('plane', 0.023), ('expression', 0.023), ('pixels', 0.023), ('emerged', 0.022), ('frank', 0.022), ('freely', 0.022), ('media', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="16-tfidf-1" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>2 0.18237764 <a title="16-tfidf-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.12539044 <a title="16-tfidf-3" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>Author: Anitha Kannan, Nebojsa Jojic, Brendan J. Frey</p><p>Abstract: Dimensionality reduction techniques such as principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data samples and points in a lower dimensional subspace. In [6], Jojic and Frey introduced mixture of transformation-invariant component analyzers (MTCA) that can account for global transformations such as translations and rotations, perform clustering and learn local appearance deformations by dimensionality reduction. However, due to enormous computational requirements of the EM algorithm for learning the model, O( ) where is the dimensionality of a data sample, MTCA was not practical for most applications. In this paper, we demonstrate how fast Fourier transforms can reduce the computation to the order of log . With this speedup, we show the effectiveness of MTCA in various applications - tracking, video textures, clustering video sequences, object recognition, and object detection in images. ¡ ¤ ¤ ¤ ¤</p><p>4 0.082290895 <a title="16-tfidf-4" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>Author: Samy Bengio</p><p>Abstract: This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same event. It is based on two other Markovian models, namely Asynchronous Input/ Output Hidden Markov Models and Pair Hidden Markov Models. An EM algorithm to train the model is presented, as well as a Viterbi decoder that can be used to obtain the optimal state sequence as well as the alignment between the two sequences. The model has been tested on an audio-visual speech recognition task using the M2VTS database and yielded robust performances under various noise conditions. 1</p><p>5 0.074540839 <a title="16-tfidf-5" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>Author: Patrick J. Wolfe, Simon J. Godsill</p><p>Abstract: The Bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music—something which has often been overlooked in traditional audio signal processing approaches. Here, after constructing a Bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms, we apply Markov chain Monte Carlo methods in order to sample from the resultant posterior distribution of interest. We present speech enhancement results which compare favourably in objective terms with standard time-varying ﬁltering techniques (and in several cases yield superior performance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledge of the noise power.</p><p>6 0.067071214 <a title="16-tfidf-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.064073585 <a title="16-tfidf-7" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>8 0.06400115 <a title="16-tfidf-8" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>9 0.062525839 <a title="16-tfidf-9" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>10 0.061097819 <a title="16-tfidf-10" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>11 0.060769424 <a title="16-tfidf-11" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>12 0.058965553 <a title="16-tfidf-12" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>13 0.057449397 <a title="16-tfidf-13" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>14 0.055205755 <a title="16-tfidf-14" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>15 0.054398801 <a title="16-tfidf-15" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>16 0.052109633 <a title="16-tfidf-16" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>17 0.051477615 <a title="16-tfidf-17" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>18 0.049862403 <a title="16-tfidf-18" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>19 0.049831539 <a title="16-tfidf-19" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>20 0.048339389 <a title="16-tfidf-20" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.157), (1, 0.006), (2, -0.017), (3, 0.093), (4, 0.017), (5, -0.012), (6, 0.051), (7, -0.035), (8, 0.131), (9, 0.075), (10, -0.012), (11, 0.12), (12, 0.04), (13, -0.007), (14, -0.036), (15, -0.043), (16, -0.046), (17, 0.084), (18, 0.025), (19, 0.011), (20, -0.003), (21, -0.071), (22, 0.116), (23, -0.071), (24, -0.136), (25, 0.107), (26, 0.024), (27, -0.169), (28, 0.08), (29, 0.001), (30, 0.126), (31, -0.103), (32, 0.05), (33, 0.065), (34, -0.032), (35, 0.147), (36, -0.038), (37, -0.111), (38, -0.031), (39, -0.009), (40, -0.213), (41, -0.134), (42, 0.162), (43, -0.008), (44, 0.009), (45, -0.078), (46, 0.176), (47, 0.088), (48, -0.013), (49, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97049356 <a title="16-lsi-1" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>2 0.50281739 <a title="16-lsi-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.46708447 <a title="16-lsi-3" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>Author: Anitha Kannan, Nebojsa Jojic, Brendan J. Frey</p><p>Abstract: Dimensionality reduction techniques such as principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data samples and points in a lower dimensional subspace. In [6], Jojic and Frey introduced mixture of transformation-invariant component analyzers (MTCA) that can account for global transformations such as translations and rotations, perform clustering and learn local appearance deformations by dimensionality reduction. However, due to enormous computational requirements of the EM algorithm for learning the model, O( ) where is the dimensionality of a data sample, MTCA was not practical for most applications. In this paper, we demonstrate how fast Fourier transforms can reduce the computation to the order of log . With this speedup, we show the effectiveness of MTCA in various applications - tracking, video textures, clustering video sequences, object recognition, and object detection in images. ¡ ¤ ¤ ¤ ¤</p><p>4 0.43372753 <a title="16-lsi-4" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>5 0.39449352 <a title="16-lsi-5" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>Author: Samy Bengio</p><p>Abstract: This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same event. It is based on two other Markovian models, namely Asynchronous Input/ Output Hidden Markov Models and Pair Hidden Markov Models. An EM algorithm to train the model is presented, as well as a Viterbi decoder that can be used to obtain the optimal state sequence as well as the alignment between the two sequences. The model has been tested on an audio-visual speech recognition task using the M2VTS database and yielded robust performances under various noise conditions. 1</p><p>6 0.33508492 <a title="16-lsi-6" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>7 0.32514188 <a title="16-lsi-7" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>8 0.32468399 <a title="16-lsi-8" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>9 0.31724173 <a title="16-lsi-9" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>10 0.29092026 <a title="16-lsi-10" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>11 0.27750871 <a title="16-lsi-11" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>12 0.27519074 <a title="16-lsi-12" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>13 0.27076346 <a title="16-lsi-13" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>14 0.26936254 <a title="16-lsi-14" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>15 0.26384398 <a title="16-lsi-15" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>16 0.25622585 <a title="16-lsi-16" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>17 0.25173363 <a title="16-lsi-17" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>18 0.24409358 <a title="16-lsi-18" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>19 0.24084219 <a title="16-lsi-19" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>20 0.24020468 <a title="16-lsi-20" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.031), (5, 0.272), (11, 0.028), (14, 0.013), (23, 0.037), (41, 0.026), (42, 0.047), (54, 0.105), (55, 0.034), (57, 0.019), (68, 0.033), (74, 0.103), (83, 0.012), (87, 0.016), (92, 0.017), (98, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82459462 <a title="16-lda-1" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>same-paper 2 0.8143124 <a title="16-lda-2" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>3 0.57653481 <a title="16-lda-3" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>4 0.56622565 <a title="16-lda-4" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>5 0.56326479 <a title="16-lda-5" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>Author: Rong Jin, Zoubin Ghahramani</p><p>Abstract: In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naIve methods degrade rapidly as more ambiguity is introduced into the labels. 1</p><p>6 0.56249619 <a title="16-lda-6" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>7 0.56074369 <a title="16-lda-7" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>8 0.56072307 <a title="16-lda-8" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>9 0.5593279 <a title="16-lda-9" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>10 0.55847621 <a title="16-lda-10" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>11 0.55846596 <a title="16-lda-11" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>12 0.55837005 <a title="16-lda-12" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>13 0.55811405 <a title="16-lda-13" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>14 0.55733007 <a title="16-lda-14" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>15 0.55671954 <a title="16-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.55572122 <a title="16-lda-16" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>17 0.55570036 <a title="16-lda-17" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>18 0.55505866 <a title="16-lda-18" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>19 0.55494165 <a title="16-lda-19" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>20 0.55478185 <a title="16-lda-20" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
