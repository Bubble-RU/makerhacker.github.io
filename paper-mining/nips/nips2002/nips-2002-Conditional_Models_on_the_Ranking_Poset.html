<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 nips-2002-Conditional Models on the Ranking Poset</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-58" href="#">nips2002-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 nips-2002-Conditional Models on the Ranking Poset</h1>
<br/><p>Source: <a title="nips-2002-58-pdf" href="http://papers.nips.cc/paper/2146-conditional-models-on-the-ranking-poset.pdf">pdf</a></p><p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A distance-based conditional model on the ranking poset is presented for use in classiﬁcation and ranking. The model is an extension of the Mallows model, and generalizes the classiﬁer combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.</p><p>Reference: <a title="nips-2002-58-reference" href="../nips2002_reference/nips-2002-Conditional_Models_on_the_Ranking_Poset_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract A distance-based conditional model on the ranking poset is presented for use in classiﬁcation and ranking. [sent-5, score-0.977]
</p><p>2 The model is an extension of the Mallows model, and generalizes the classiﬁer combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. [sent-6, score-0.318]
</p><p>3 The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. [sent-7, score-1.157]
</p><p>4 In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme. [sent-8, score-0.364]
</p><p>5 A generalization of this problem is conditional ranking, the task of assigning to a full or partial ranking of the items in . [sent-10, score-0.853]
</p><p>6 This paper studies the algebraic structure of this problem, and proposes a combinatorial structure called the ranking poset for building probability models for conditional ranking. [sent-11, score-1.083]
</p><p>7 ¦  ¤ ¢ ¥£¡  ¦  ¤  In ensemble approaches to classiﬁcation and ranking, several base models are combined to produce a single ranker or classiﬁer. [sent-12, score-0.176]
</p><p>8 An important distinction between different ensemble methods is whether they use discrete inputs, ranked inputs, or conﬁdence-rated predictions. [sent-13, score-0.223]
</p><p>9 In the case of discrete inputs, the base models provide a single item in , and no preference for a second or third choice is given. [sent-14, score-0.161]
</p><p>10 In the case of ranked input, the base classiﬁers output a full or partial ranking over . [sent-15, score-0.817]
</p><p>11 Of course, discrete input is a special case of ranked input, where the partial ranking consists of the single topmost item. [sent-16, score-0.856]
</p><p>12 In the case of conﬁdence-rated predictions, the base models again output full or partial rankings, but in addition provide a conﬁdence score, indicating how much one class should be preferred to another. [sent-17, score-0.313]
</p><p>13 While conﬁdence-rated predictions are sometimes preferable as input to an ensemble method, such conﬁdence scores are often not available (as is typically the case in metasearch), and even when they are available, the scores may not be well calibrated. [sent-18, score-0.111]
</p><p>14 ¤  ¤  This paper investigates a unifying algebraic framework for ensemble methods for classiﬁcation and conditional ranking, focusing on the cases of discrete and ranked inputs. [sent-19, score-0.383]
</p><p>15 , which consists of Our approach is based on the ranking poset on items, denoted the collection of all full and partial rankings equipped with the partial order given by re-  © ¨  §  ﬁnement of rankings. [sent-20, score-1.51]
</p><p>16 The structure of the poset of partial ranking over gives rise to natural invariant distance functions that generalize Kendall’s Tau and the Hamming distance. [sent-21, score-1.189]
</p><p>17 Using these distance functions we deﬁne a conditional model where . [sent-22, score-0.115]
</p><p>18 This conditional model generalizes several existing models for classiﬁcation and ranking, and includes as a special case the Mallows model [11]. [sent-23, score-0.166]
</p><p>19 In addition, the model represents algebraically the way in which input classiﬁers are combined in certain ensemble methods, including error correcting output codes [4], several versions of AdaBoost [7, 1], and cranking [10]. [sent-24, score-0.427]
</p><p>20 &%$¥ ©  ©  In Section 2 we review some basic algebraic concepts and in Section 3 we deﬁne the ranking poset. [sent-27, score-0.502]
</p><p>21 The new model and its Bayesian interpretation are described in Section 4. [sent-28, score-0.055]
</p><p>22 Identifying the items to be ranked with the numbers , if denotes a , then denotes the rank given to item and denotes permutation of the item assigned to rank . [sent-31, score-0.489]
</p><p>23 The collection of all permutations of -items forms the nonabelian symmetric group of order , denoted . [sent-32, score-0.176]
</p><p>24 The set of all such partial rankings forms the quotient space . [sent-35, score-0.441]
</p><p>25 C  5  © #X© 7 W  7  of positive integers that sum An ordered partition of is a sequence to . [sent-36, score-0.103]
</p><p>26 Such an ordered partition corresponds to a partial ranking of type with items in the ﬁrst position, items in the second position and so on. [sent-37, score-1.032]
</p><p>27 Then the subgroup contains all permutations for which the set equality holds for each ; that is, all permutations that only permute within each . [sent-41, score-0.22]
</p><p>28 A partial ranking of type is equivalent to a coset and the set of such partial rankings forms the quotient space . [sent-42, score-1.092]
</p><p>29 In the following, we list items separated by vertical lines, indicating that the items on the left side of the line are preferred to (ranked higher than) the items on the right side of the line. [sent-44, score-0.514]
</p><p>30 A partial ranking where the top 3 items are is denoted by . [sent-46, score-0.827]
</p><p>31 A partial ranking where with items ranked in the ﬁrst position is denoted by . [sent-48, score-0.935]
</p><p>32 In addition, since the indexing of the items is arbitrary, it is appropriate to require invariance to relabeling of . [sent-56, score-0.25]
</p><p>33 , for all Formally, this amounts to right invariance   7 © 9¢   © %$¥   © %$¥  " © £ f "  ¥£ ¢%¤Po 'X¦$¦Po   ¦  "¢©%$¦" ¥£ "  © ¥£ o " © ¥£ X! [sent-57, score-0.12]
</p><p>34 o  © ¡ 'V¡    is Kendall’s Tau  ¤  © F¢ 7  ' ( #&  ©   5   A popular right invariant distance on  (3)  7 #    $ %  © @  " © ¥£ ¢$"  where for and otherwise [8]. [sent-59, score-0.148]
</p><p>35 Kendall’s Tau can be interpreted as the number of discordant pairs of items between and , or the minimum number of adjacent transpositions needed to bring to . [sent-60, score-0.279]
</p><p>36 An adjacent transposition ﬂips a pair of items that have adjacent ranks. [sent-61, score-0.233]
</p><p>37 Critchlow [2] derives extensions of Kendall’s Tau and other distances on to distances on partial rankings. [sent-62, score-0.25]
</p><p>38 " © ¥£ ¢%$¦"  ©  5 h© ¥   v @ " RiP¦ £  5  6¥  v x eD¦  )  ©  (  @ " iP¦ £  )  7  3 The Ranking Poset We ﬁrst deﬁne partially ordered sets and then proceed to deﬁne the ranking poset. [sent-63, score-0.508]
</p><p>39 , where is a set and is a binary relation A partially ordered set or poset is a pair that satisﬁes (1) , (2) if and then , and (3) if and then for all . [sent-65, score-0.578]
</p><p>40 A ﬁnite poset is completely described by the covering relation. [sent-68, score-0.507]
</p><p>41 The planar Hasse diagram of is the graph for which the elements of are the nodes and the edges are given by the covering relation. [sent-69, score-0.171]
</p><p>42 The partial order of is deﬁned by reﬁnement; that is, if we can get from to by adding vertical lines. [sent-72, score-0.18]
</p><p>43 Note that is different from the poset of all set partitions of ordered by partition reﬁnement since in the order of the partition elements matters. [sent-73, score-0.645]
</p><p>44 Figure 1 shows the Hasse diagram of and a portion of the Hasse diagram of . [sent-74, score-0.196]
</p><p>45 of  ¡  U  © E l8¥  h  ¨  A subposet of is deﬁned by and if and only if A chain is a poset in which every two elements are comparable. [sent-76, score-0.585]
</p><p>46 A saturated chain  ¦  e Y 5 Xd  ¡  ¦  W 5 X8  3 a cbV  " Y 5 3 6`Qd4£  " W 5 V 3X6i2£  length is a sequence of elements that satisfy . [sent-77, score-0.114]
</p><p>47 A chain of is a maximal chain if there is no other saturated chain of that contains it. [sent-78, score-0.173]
</p><p>48 A graded poset of rank is a poset in which every maximal chain has length . [sent-79, score-1.081]
</p><p>49 In a graded poset, there is a rank or grade function such that if is a minimal element and if . [sent-80, score-0.164]
</p><p>50 In particular, the elements in the th grade, all of which are incomparable, are denoted by . [sent-83, score-0.09]
</p><p>51 That is, for begin, suppose that is a right invariant function on all and . [sent-89, score-0.118]
</p><p>52 Here right invariance is deﬁned with respect to the natural action of on , given by  © ¨  "  © ¥£ X! [sent-90, score-0.157]
</p><p>53 o  We will examine several distances that are based on the covering relation of . [sent-94, score-0.077]
</p><p>54 Down and up moves on the Hasse diagram will be denoted by and respectively. [sent-95, score-0.236]
</p><p>55   ¡  o  We are now ready to give the general form of a conditional model on . [sent-97, score-0.085]
</p><p>56 The model takes as input rankings in some subset of the ranking poset. [sent-99, score-0.652]
</p><p>57 © ¨  Thus, conditional on  will typically be selected by maxi, a marginal likelihood will be convex and have a unique global  I " # £ " ¢ 6§ ¢ ¦¤P  § I @ 8 # 5 ¥£ ¡    " £ ¤ ¥ ¤ ¥ S ¢  4. [sent-108, score-0.085]
</p><p>58 1 A Bayesian interpretation We now derive a Bayesian interpretation for the model given by (6). [sent-109, score-0.11]
</p><p>59 Our result parallels the interpretation of multistage ranking models given by Fligner and Verducci [6]. [sent-110, score-0.513]
</p><p>60 The key fact is that, under appropriate assumptions, the normalizing term does not depend on the partial ordering in the one-dimensional case. [sent-111, score-0.213]
</p><p>61 #  t  ¢  and  is invariant under the action  ¢# % ( $¡ ¤  0)`  o  W  W  I  ¢  ©  7  1 ¥ 2U$¥  7  for all  2  Proposition 4. [sent-115, score-0.124]
</p><p>62 First, note that since is invariant under the action of , it follows that for each . [sent-119, score-0.124]
</p><p>63 Indeed, by the invariance assumption, and since for we have such that . [sent-120, score-0.089]
</p><p>64 (9) (10)  (by right invariance of )  8  (11) (12)  (by invariance of )  @  ¢ ¥  © ¨ 2    2  a  ¢  2  1  3 ¥  © 7 © @ 1 G¨ 4© ¡ ) Q8 V  ¡ `l© §88 § 1 ) @ © 7 ¢ 2 @     2   %  W  ¢# % $¡ ¤  &`  #  ! [sent-122, score-0.209]
</p><p>65 there is    acts transitively on , for all Now, since We thus have that  does not in  ¥  The underlying generative model is given as follows. [sent-127, score-0.103]
</p><p>66 If is right invariant, is invariant under the action of , and acts transitively on , then the model deﬁned in equation (6) is the posterior under independent sampling of generalized Mallows models, , with prior . [sent-136, score-0.258]
</p><p>67 ©  ©  7    4¤  ¥  and  @ 2  S  7  o  " 5§8 w6£ ¡   2  7qWX© 7B@IW " §8 A ¥ £ B¡    P© 3  W  The conditions of this proposition are satisﬁed, for example, when as is assumed in the special cases of the next section. [sent-137, score-0.095]
</p><p>68 ¡ ¢S    7 W qX© 7     5 Special Cases This section derives several special cases of model (6), corresponding to existing ensemble methods. [sent-138, score-0.135]
</p><p>69 Following [9], the unnormalized versions of all the models may be easily derived, corresponding to the exponential loss used in boosting. [sent-141, score-0.088]
</p><p>70 1 Cranking and Mallows  model  Let , and let be the minimum number of downup ( ) moves on the Hasse diagram of needed to bring to . [sent-143, score-0.215]
</p><p>71 Since adjacent transpositions of permutations may be identiﬁed with a down move followed by an up move over the Hasse diagram, is equal to Kendall’s Tau . [sent-144, score-0.168]
</p><p>72  P¤ X 0U" " (§ §   § § ( £  In this case model (6) becomes the cranking model [10]  (16)  ¢  t  8   £ A % ¢ ¦A ¡  ¤A E ¥ ¤  &`   ! [sent-154, score-0.111]
</p><p>73 Other special cases that fall into this category are the models of Feigin [5] and Critchlow and Verducci [3]. [sent-157, score-0.081]
</p><p>74 2 Logistic models , , and let ) moves in the Hasse diagram. [sent-159, score-0.11]
</p><p>75 o  ©   5  © #W © 7  7 HX  5  ©  7 @ 2 BT@ W  t S @  Let (  if otherwise  (17)  " #  7 PG@ ¢%$¦Po £ o " © ¥£  In this case model (6) becomes equivalent to the multiclass generalization of logistic regression. [sent-161, score-0.124]
</p><p>76 M2; that is, becomes the (discrete) multiclass weak learner in the usual boosting notation. [sent-163, score-0.12]
</p><p>77 See [9] for details on the correspondence between exponential models and the unnormalized models that correspond to AdaBoost. [sent-164, score-0.119]
</p><p>78 3 Error correcting output codes A more interesting special case of the algebraic structure described in Sections 3 and 4 is where the ensemble method is error correcting output coding (ECOC) [4]. [sent-166, score-0.644]
</p><p>79 Here we set  , and take the parameter space to be  I   " #  I  @ 888 @ qBqb  I @  §  I  (18)  ) moves in the Hasse diagram  7 D@ W  5 © 7 @ ¥   ¥ " © ¥£ ¢%$¦Po  ©  ¢ ¤ ¢ ¤  ¥£¡  © qX© 7 W  ty¢ 8)E@SS @ l2  5  is the minimal number of up-down ( to . [sent-167, score-0.177]
</p><p>80 On input , the base rankers output , which corresponds to one of the binary classiﬁers in ECOC for the appropriate column of the binary coding matrix. [sent-169, score-0.308]
</p><p>81 For example, consider a binary classiﬁer trained on the coding column . [sent-170, score-0.104]
</p><p>82 On an input , the classiﬁer outputs 0 or 1, corresponding to the partial rankings and , respectively. [sent-171, score-0.405]
</p><p>83 Un @ © " v v v ( v ( qXX#¤%#£  ¦  For example, if from the sequence of moves  ¦  © ¨ ¢ P¦ q¨© " £ 3  W § c  ¨  and  ¦  Since  (22)  Since , the exponent of the model becomes . [sent-186, score-0.079]
</p><p>84 At test time, the model thus selects the label corresponding to the partial ranking arg . [sent-187, score-0.607]
</p><p>85 since is strictly negative, Equivalence with the ECOC decision rule thus follows from the fact that is the Hamming distance between the appropriate row of the coding matrix and the concatenation of the bits returned from the binary classiﬁers. [sent-189, score-0.134]
</p><p>86 Thus, with the appropriate deﬁnitions of and , the conditional model on the ranking poset is a probabilistic formulation of ECOC that yields the same classiﬁcation decisions. [sent-190, score-0.977]
</p><p>87 First, relaxing the constraint results in a more general model that corresponds to ECOC with a weighted Hamming distance, or index sensitive “channel,” where the learned weights may adapt to the precision of the various base classiﬁers. [sent-192, score-0.06]
</p><p>88 o  2 W   @ 888 VB@ b  I  I  @   " ¥ ¦£   ¤  I  A further generalization is achieved by considering that for a given coding matrix, the trained classiﬁer for a given column outputs either or depending on the input . [sent-194, score-0.098]
</p><p>89 Allowing the output of the classiﬁer instead to belong to other grades of results in a model that corresponds to error correcting output codes with nonbinary codes. [sent-195, score-0.247]
</p><p>90 While this is somewhat antithetic to the original spirit of ECOC—reducing multiclass to binary—the base classiﬁers in ECOC are often multiclass classiﬁers such as decision trees in [4]. [sent-196, score-0.216]
</p><p>91 For such classiﬁers, the task instead can be viewed as reducing multiclass to partial ranking. [sent-197, score-0.258]
</p><p>92 Moreover, there need not be an explicit coding matrix. [sent-198, score-0.072]
</p><p>93 Instead, the input rankers may output different partial rankings for different inputs, which are then combined according to model (6). [sent-199, score-0.491]
</p><p>94 In this way, a different coding matrix is built for each example in a dynamic manner. [sent-200, score-0.072]
</p><p>95 Such a scheme may be attractive in bypassing the problem of designing the coding matrix. [sent-201, score-0.072]
</p><p>96  a 1    ¡  §) a  1 ©    ¡)  © a  1  ¡ i)§  a  1  ¡ )  ¦  © ¨  6 Summary An algebraic framework has been presented for classiﬁcation and ranking, leading to conditional models on the ranking poset that are deﬁned in terms of an invariant distance or dissimilarity function. [sent-202, score-1.227]
</p><p>97 Using the invariance properties of the distances, we derived a generative interpretation of the probabilistic model, which may prove to be useful in model selection and validation. [sent-203, score-0.144]
</p><p>98 Through different choices of the components and , the family of models was shown to include as special cases the Mallows model, and the classiﬁer combination methods used by logistic models, boosting, cranking, and error correcting output codes. [sent-204, score-0.284]
</p><p>99 In the case of ECOC, the poset framework shows how probabilities may be assigned to partial rankings in a way that is consistent with the usual deﬁnitions of ECOC , and suggests several natural extensions. [sent-205, score-0.844]
</p><p>100 Cranking: Combining rankings using conditional probability models on permutations. [sent-272, score-0.315]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('poset', 0.465), ('ranking', 0.427), ('po', 0.235), ('hasse', 0.221), ('rankings', 0.199), ('partial', 0.18), ('ecoc', 0.176), ('items', 0.161), ('mallows', 0.155), ('correcting', 0.115), ('cranking', 0.111), ('critchlow', 0.111), ('kendall', 0.111), ('classi', 0.109), ('ranked', 0.108), ('diagram', 0.098), ('tau', 0.096), ('invariance', 0.089), ('verducci', 0.089), ('permutations', 0.088), ('invariant', 0.087), ('conditional', 0.085), ('ensemble', 0.085), ('moves', 0.079), ('multiclass', 0.078), ('algebraic', 0.075), ('coding', 0.072), ('lebanon', 0.07), ('fligner', 0.066), ('qip', 0.066), ('transitively', 0.066), ('base', 0.06), ('denoted', 0.059), ('grade', 0.058), ('rank', 0.057), ('ordered', 0.057), ('va', 0.056), ('interpretation', 0.055), ('special', 0.05), ('graded', 0.049), ('codes', 0.048), ('logistic', 0.046), ('partition', 0.046), ('chain', 0.045), ('proposition', 0.045), ('coset', 0.044), ('cosets', 0.044), ('rankers', 0.044), ('subgroup', 0.044), ('subposet', 0.044), ('transpositions', 0.044), ('xq', 0.044), ('nitions', 0.044), ('covering', 0.042), ('boosting', 0.042), ('un', 0.042), ('output', 0.042), ('nement', 0.04), ('item', 0.04), ('ers', 0.04), ('hamming', 0.039), ('orderings', 0.038), ('saturated', 0.038), ('bring', 0.038), ('action', 0.037), ('acts', 0.037), ('adjacent', 0.036), ('adaboost', 0.035), ('psfrag', 0.035), ('biometrika', 0.035), ('topmost', 0.035), ('pg', 0.035), ('qx', 0.035), ('distances', 0.035), ('er', 0.034), ('carrier', 0.033), ('quotient', 0.033), ('unnormalized', 0.033), ('normalizing', 0.033), ('binary', 0.032), ('elements', 0.031), ('right', 0.031), ('replacements', 0.031), ('cc', 0.031), ('lafferty', 0.031), ('models', 0.031), ('distance', 0.03), ('discrete', 0.03), ('yf', 0.029), ('forms', 0.029), ('cations', 0.027), ('qr', 0.027), ('gh', 0.027), ('dissimilarity', 0.027), ('permutation', 0.026), ('input', 0.026), ('metric', 0.025), ('partially', 0.024), ('paired', 0.024), ('exponential', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="58-tfidf-1" href="./nips-2002-Conditional_Models_on_the_Ranking_Poset.html">58 nips-2002-Conditional Models on the Ranking Poset</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A distance-based conditional model on the ranking poset is presented for use in classiﬁcation and ranking. The model is an extension of the Mallows model, and generalizes the classiﬁer combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.</p><p>2 0.1778371 <a title="58-tfidf-2" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>3 0.16781457 <a title="58-tfidf-3" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>4 0.15393497 <a title="58-tfidf-4" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>5 0.14202766 <a title="58-tfidf-5" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: We describe a new algorithmic framework for learning multiclass categorization problems. In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space. In this space each instance is assigned the label it is nearest to. We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data. A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC). Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching. We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems.</p><p>6 0.12579711 <a title="58-tfidf-6" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>7 0.081937715 <a title="58-tfidf-7" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>8 0.070761658 <a title="58-tfidf-8" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>9 0.064657293 <a title="58-tfidf-9" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>10 0.05911449 <a title="58-tfidf-10" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>11 0.057157412 <a title="58-tfidf-11" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>12 0.056645907 <a title="58-tfidf-12" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>13 0.056352802 <a title="58-tfidf-13" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>14 0.052359328 <a title="58-tfidf-14" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>15 0.05163547 <a title="58-tfidf-15" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>16 0.050928675 <a title="58-tfidf-16" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>17 0.046723131 <a title="58-tfidf-17" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>18 0.046358954 <a title="58-tfidf-18" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>19 0.045617331 <a title="58-tfidf-19" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>20 0.043416813 <a title="58-tfidf-20" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.162), (1, -0.08), (2, 0.036), (3, -0.024), (4, 0.184), (5, -0.006), (6, -0.041), (7, -0.115), (8, -0.032), (9, -0.062), (10, -0.057), (11, -0.026), (12, -0.053), (13, -0.069), (14, -0.071), (15, 0.129), (16, 0.091), (17, 0.057), (18, -0.021), (19, -0.076), (20, 0.017), (21, -0.08), (22, -0.081), (23, 0.123), (24, 0.043), (25, -0.019), (26, 0.048), (27, 0.182), (28, -0.016), (29, 0.06), (30, -0.005), (31, -0.079), (32, 0.08), (33, 0.009), (34, 0.026), (35, -0.021), (36, -0.026), (37, -0.099), (38, 0.047), (39, 0.096), (40, 0.151), (41, -0.02), (42, 0.102), (43, 0.044), (44, -0.121), (45, -0.083), (46, 0.044), (47, -0.058), (48, -0.035), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92736697 <a title="58-lsi-1" href="./nips-2002-Conditional_Models_on_the_Ranking_Poset.html">58 nips-2002-Conditional Models on the Ranking Poset</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A distance-based conditional model on the ranking poset is presented for use in classiﬁcation and ranking. The model is an extension of the Mallows model, and generalizes the classiﬁer combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.</p><p>2 0.66993147 <a title="58-lsi-2" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>3 0.63897032 <a title="58-lsi-3" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: We describe a new algorithmic framework for learning multiclass categorization problems. In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space. In this space each instance is assigned the label it is nearest to. We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data. A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC). Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching. We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems.</p><p>4 0.54119062 <a title="58-lsi-4" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>5 0.50381881 <a title="58-lsi-5" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>6 0.38833547 <a title="58-lsi-6" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>7 0.34423238 <a title="58-lsi-7" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>8 0.34003979 <a title="58-lsi-8" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>9 0.32505861 <a title="58-lsi-9" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>10 0.32033107 <a title="58-lsi-10" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>11 0.31771058 <a title="58-lsi-11" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>12 0.29689965 <a title="58-lsi-12" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>13 0.28396654 <a title="58-lsi-13" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>14 0.28188422 <a title="58-lsi-14" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>15 0.27879384 <a title="58-lsi-15" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>16 0.27212638 <a title="58-lsi-16" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>17 0.26170534 <a title="58-lsi-17" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>18 0.26109087 <a title="58-lsi-18" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>19 0.25684777 <a title="58-lsi-19" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>20 0.24327952 <a title="58-lsi-20" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.011), (23, 0.014), (42, 0.063), (52, 0.385), (54, 0.089), (55, 0.055), (67, 0.01), (68, 0.021), (74, 0.083), (92, 0.042), (98, 0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76901579 <a title="58-lda-1" href="./nips-2002-Conditional_Models_on_the_Ranking_Poset.html">58 nips-2002-Conditional Models on the Ranking Poset</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A distance-based conditional model on the ranking poset is presented for use in classiﬁcation and ranking. The model is an extension of the Mallows model, and generalizes the classiﬁer combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.</p><p>2 0.6135298 <a title="58-lda-2" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: We describe a new algorithmic framework for learning multiclass categorization problems. In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space. In this space each instance is assigned the label it is nearest to. We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data. A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC). Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching. We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems.</p><p>3 0.58833444 <a title="58-lda-3" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>Author: Kenji Fukumizu, Shotaro Akaho, Shun-ichi Amari</p><p>Abstract: We show the existence of critical points as lines for the likelihood function of mixture-type models. They are given by embedding of a critical point for models with less components. A sufﬁcient condition that the critical line gives local maxima or saddle points is also derived. Based on this fact, a component-split method is proposed for a mixture of Gaussian components, and its effectiveness is veriﬁed through experiments. 1</p><p>4 0.54178238 <a title="58-lda-4" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>5 0.44074735 <a title="58-lda-5" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>6 0.44008201 <a title="58-lda-6" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>7 0.43391812 <a title="58-lda-7" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>8 0.43336123 <a title="58-lda-8" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>9 0.43214202 <a title="58-lda-9" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>10 0.43088686 <a title="58-lda-10" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>11 0.43003774 <a title="58-lda-11" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>12 0.42997342 <a title="58-lda-12" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>13 0.42975697 <a title="58-lda-13" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>14 0.42872012 <a title="58-lda-14" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>15 0.42821971 <a title="58-lda-15" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>16 0.42784402 <a title="58-lda-16" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>17 0.42774522 <a title="58-lda-17" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>18 0.42773569 <a title="58-lda-18" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>19 0.42763272 <a title="58-lda-19" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>20 0.42759329 <a title="58-lda-20" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
