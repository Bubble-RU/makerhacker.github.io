<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-57" href="#">nips2002-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</h1>
<br/><p>Source: <a title="nips-2002-57-pdf" href="http://papers.nips.cc/paper/2338-concurrent-object-recognition-and-segmentation-by-graph-partitioning.pdf">pdf</a></p><p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>Reference: <a title="nips-2002-57-reference" href="../nips2002_reference/nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. [sent-7, score-0.655]
</p><p>2 The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. [sent-8, score-1.833]
</p><p>3 We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection. [sent-11, score-1.173]
</p><p>4 1 Introduction A good image segmentation must single out meaningful structures such as objects from a cluttered scene. [sent-12, score-0.571]
</p><p>5 Most current segmentation techniques take a bottom-up approach [5] , where image properties such as feature similarity (brightness, texture, motion etc), boundary smoothness and continuity are used to detect perceptually coherent units. [sent-13, score-0.682]
</p><p>6 Segmentation can also be performed in a top-down manner from object models, where object templates are projected onto an image and matching errors are used to determine the existence of the object [1] . [sent-14, score-1.347]
</p><p>7 Without utilizing any knowledge about the scene, image segmentation gets lost in poor data conditions: weak edges, shadows, occlusions and noise. [sent-16, score-0.62]
</p><p>8 Missed object boundaries can then hardly be recovered in subsequent object recognition. [sent-17, score-0.816]
</p><p>9 Gestaltists have long recognized this issue, circumventing it by adding a grouping factor called familiarity [6]. [sent-18, score-0.175]
</p><p>10 Without being subject to perceptual constraints imposed by low level grouping, an object detection process can produce many false positives in a cluttered scene [3]. [sent-19, score-0.651]
</p><p>11 Another approach, which we adopt in  this paper, is based on the observation that the falsely detected parts are not perceptually salient (Fig. [sent-21, score-0.249]
</p><p>12 Right arm: 7  Right leg: 3  Head: 4  Left arm: 4  Left leg: 9  Figure 1: Human body part detection. [sent-23, score-0.166]
</p><p>13 A total of 27 parts are detected, each labeled by one of the five part detectors for arms, legs and head. [sent-24, score-0.222]
</p><p>14 the patch on the floor that is labeled left leg has the same features as its surroundings. [sent-28, score-0.534]
</p><p>15 the patch on the treadmill that is labeled head has no other patches in the image to make up a whole human body. [sent-31, score-0.98]
</p><p>16 These two conditions, low-level image feature saliency and high-level part labeling consistency, are essential for the segmentation of objects from background. [sent-32, score-0.669]
</p><p>17 Both cues are encoded in our pixel and patch grouping respectively. [sent-33, score-0.84]
</p><p>18 We propose a segmentation mechanism that is coupled with the object recognition process (Fig. [sent-34, score-0.886]
</p><p>19 It learns classifiers from training images to detect parts along with the segmentation patterns and their relative spatial configurations. [sent-38, score-0.594]
</p><p>20 Recent work on object segmentation [1] uses image patches and their figure-ground labeling as building blocks for segmentation. [sent-40, score-1.231]
</p><p>21 3)Interactions: coupling object recognition with segmentation by linking patches with their corresponding pixels. [sent-44, score-1.111]
</p><p>22 With such a representation, we concurrently carry out object recognition and image segmentation processes. [sent-45, score-0.976]
</p><p>23 The final output is an object segmentation where the object group consists of pixels with coherent low-level features and patches with compatible part configurations. [sent-46, score-1.772]
</p><p>24 We formulate our object segmentation task in a graph partitioning framework. [sent-47, score-0.939]
</p><p>25 We represent low-level grouping cues with a graph where each pixel is a node and edges between the nodes encode the affinity of pixels based on their feature similarity [4]. [sent-48, score-1.168]
</p><p>26 We represent highlevel grouping cues with a graph where each detected patch is a node and edges between the nodes encode the labeling consistency based on prior knowledge of object part configurations. [sent-49, score-1.666]
</p><p>27 There are also edges connecting patch nodes with their supporting pixel nodes. [sent-50, score-0.709]
</p><p>28 We seek the optimal graph cut in this joint graph, which separates the desired patch and pixel nodes from the rest nodes. [sent-51, score-0.728]
</p><p>29 We build upon the computational framework of spectral graph partitioning [7], and achieve patch competition using the subspace constraint method proposed in [10]. [sent-52, score-0.616]
</p><p>30 Furthermore, we assume that some object recognition system has labeled a set of patches as object parts. [sent-57, score-1.165]
</p><p>31 Every patch has a local segmentation according to its part label. [sent-58, score-0.894]
</p><p>32 The recognition system has also learned the  • ')  (  Figure 2: Model of object segmentation. [sent-59, score-0.465]
</p><p>33 Given an image, we detect edges using a set of oriented filter banks. [sent-60, score-0.217]
</p><p>34 The edge responses provide low-level grouping cues, and a graph can be constructed with one node for each pixel. [sent-61, score-0.401]
</p><p>35 Shown on the middle right are affinity patterns of five center pixels within a square neighbourhood, overlaid on the edge map. [sent-62, score-0.573]
</p><p>36 We detect a set of candidate body parts using learned classifiers. [sent-64, score-0.212]
</p><p>37 Body part labeling provides high-level grouping cues, and a consistency graph can be constructed with one node for each patch. [sent-65, score-0.468]
</p><p>38 Edges are noisy, while patches contain ambiguity in local segmentation and part labeling. [sent-68, score-0.733]
</p><p>39 Patches and pixels interact by expected local segmentation based on object knowledge, as shown in the middle image. [sent-69, score-1.033]
</p><p>40 A global partitioning on the coupled graph outputs an object segmentation that has both pixel-level saliency and patch-level consistency. [sent-70, score-1.012]
</p><p>41 statistical distribution of the spatial configurations of object parts. [sent-71, score-0.489]
</p><p>42 how to evaluate low-level pixel cues, high-level patch cues and their segmentation correspondence. [sent-75, score-1.053]
</p><p>43 how to fuse partial and imprecise object knowledge with somewhat unreliable low-level cues to segment out the object of interest. [sent-78, score-0.97]
</p><p>44 patches  I[WJcrDJ [0-  , -  -  pixel-patch rebtio",  . [sent-79, score-0.258]
</p><p>45 ;; e_ _---,/ im g_  ~  edges  object  segmentation  o Figure 3: Given the image on the left, we want to detect the object on the ri ght). [sent-83, score-1.471]
</p><p>46 11 patches of various sizes are detected (middle top). [sent-84, score-0.336]
</p><p>47 Each patch has a partial local segmentation as shown in the center image. [sent-86, score-0.861]
</p><p>48 Object pixels are marked black, background white and others gray. [sent-87, score-0.244]
</p><p>49 pixels across a strong edge (middle bottom) are likely to be in different regions. [sent-90, score-0.25]
</p><p>50 Our goal is to find the best patchpixel combinations that conform to the object knowledge and data coherence. [sent-91, score-0.489]
</p><p>51 Let N be the number of pixels and M the number of patches. [sent-95, score-0.167]
</p><p>52 Let A be the pixel-pixel affinity matrix, B be the patch-patch affinity matrix, and C be the patch-pixel affinity matrix. [sent-96, score-0.723]
</p><p>53 Then the node set and the weight matrix for the pairwise edge set E are:  V  {I,··· , N,  }V+1, . [sent-99, score-0.168]
</p><p>54 ,N+M),  '"--v--'  pixels  W(A , B , C ; f3B, f3c)  [  A N xN  f3c· C M x N  patches  f3c . [sent-102, score-0.425]
</p><p>55 (1)  Object segmentation corresponds to a node bipartitioning problem, where V = VI U V2 and VI n V2 = 0. [sent-105, score-0.448]
</p><p>56 We assume VI contains a set of pixel and patch nodes that correspond to the object, and V 2 is the rest of the background pixels and patches that correspond to false positives and alternative labelings. [sent-106, score-1.285]
</p><p>57 We only need to process the image region enclosing all the detected patches. [sent-109, score-0.201]
</p><p>58 The rest pixels are associated with a virtual background patch, which we denote as patch N + M, in addition to M - 1 detected object patches. [sent-110, score-1.226]
</p><p>59 Restriction of segmentation to this region of interest (ROI) helps binding irrelavent background elements into one group [10]. [sent-111, score-0.502]
</p><p>60 2  Computing pixel-pixel similarity A  The pixel affinity matrix A measures low-level image feature similarity. [sent-113, score-0.561]
</p><p>61 In this paper, we choose intensity as our feature and calcuate A based on edge detection results. [sent-114, score-0.158]
</p><p>62 We first convolve the image with quadrature pairs of oriented filters to extract the magnitude of edge responses OE [4]. [sent-115, score-0.242]
</p><p>63 Pixel affinity A is inversely correlated with the maximum magnitude of edges crossing the line connecting two pixels. [sent-117, score-0.33]
</p><p>64 (2)  A(1 , 3) ;:::: 1 A(1 , 2) ;:::: 0  o D image oriented filter pairs edge magnitudes Figure 4: Pixel-pixel similarity matrix A is computed based on intensity edge magnitudes. [sent-125, score-0.443]
</p><p>65 3  Computing patch-patch compatibility B and competition  For object patches, we evaluate their position compatibility according to learned statistical distributions. [sent-127, score-0.632]
</p><p>66 For object part labels a and b, we can model their spatial distribution by a Gaussian, with mean /L a b and variance ~ab estimated from training data. [sent-128, score-0.498]
</p><p>67 For patches p and q, B(p, q) is low if p, q form rare configurations for their part labels p and q (Fig. [sent-131, score-0.377]
</p><p>68 As to the virtual background patch node, it only has affinity of 1 to itself. [sent-136, score-0.789]
</p><p>69 Patch compatibility measures alone do not prevent the desired pixel and patch group from including falsely detected patches and their pixels, nor does it favor the true object pixels to be away from unlabeled background pixels. [sent-137, score-1.799]
</p><p>70 Sb, there are four pairs of patches with the same object part labels. [sent-141, score-0.73]
</p><p>71 To encode mutual exclusion between patches, we enforce one winner among patch nodes in competition. [sent-142, score-0.574]
</p><p>72 For example, only one of the patches 7 and 8 can be validated to the object group: Xl (N + 7) + Xl (N + 8) = 1. [sent-143, score-0.714]
</p><p>73 We also set an exclusion constraint between a reliable patch and the virtual background patch so that the desired object group stands out alone without these unlabeled background pixels, e. [sent-144, score-1.613]
</p><p>74 We have:  L  Xl(k) = 1, m = 1 :  lSI·  (4)  7 and 8 cannot both be part of the object  a) compatibility patches b) competition Figure 5: a) Patch-patch compatibility matrix B is evaluated based on statistical configuration plausibility. [sent-148, score-1.011]
</p><p>75 b) Patches of the same object part label compete to enter the object group. [sent-150, score-0.906]
</p><p>76 Only one winner from each linked pair of patches can be validated as part of the object. [sent-151, score-0.404]
</p><p>77 4  Computing pixel-patch association C  Every object part label also projects an expected pixel segmentation within the patch window (Fig. [sent-153, score-1.493]
</p><p>78 The pixel-patch association matrix C has one column for each patch:  C(i,p)  =  {  I 0 :  if i is an object pixel of patch p, otherwise. [sent-155, score-1.04]
</p><p>79 (5)  For the virtual background patch, its member pixels are those outside the ROI. [sent-156, score-0.296]
</p><p>80 Head detector -> Patch 1  I  • 1  19  Leg detector -> Patch 11 expected local segmentation  12  110  Arm detector -> Patch 2  1  6 1  3  l_  i I" 15 71 s  patches association Figure 6: Pixel-patch association C for object patches. [sent-157, score-1.308]
</p><p>81 Object pixels are marked black, background white and others gray. [sent-158, score-0.244]
</p><p>82 A patch is associated with its object pixels in the given partial segmentation. [sent-159, score-1.025]
</p><p>83 Finally, we desire (38 to balance the total weights between pixel and patch grouping so that M « N does not render patch grouping insignificant, and we want (3c to be large enough so that the results of patch grouping can bring along their associated pixels: ITAI (3B (3B = 0·01 1TB1 , (3c = maxC. [sent-160, score-1.928]
</p><p>84 (8) (9) (10)  Once we get the optimal eigenvector, we compare 10 thresholds uniformly distributed within its range and choose the discrete segmentation that yields the best criterion E. [sent-172, score-0.388]
</p><p>85 1: Compute edge response OE and calculate pixel affinity A, Eq. [sent-174, score-0.5]
</p><p>86 2: Detect parts and calculate patch affinity B , Eq. [sent-176, score-0.745]
</p><p>87 Image segmentation alone gets lost in a cluttered scene. [sent-191, score-0.56]
</p><p>88 With concurrent segmentation and recognition, regions forming the object of interest pop out, with unwanted edges (caused by occlusion) and weak edges (illusory contours) corrected in the final segmentation. [sent-192, score-1.055]
</p><p>89 It is also faster to compute the pixel-patch grouping since the size of the solution space is greatly reduced. [sent-193, score-0.175]
</p><p>90 I segmentation alone  concurrent segmentation and recognition  I 44 seconds 17 seconds Figure 7: Eigenvectors (row 1) and their segmentations (row 2) for Fig. [sent-194, score-1.103]
</p><p>91 On the right, we show the optimal eigenvector on both pixels and patches, the horizontal dotted line indicating the threshold. [sent-196, score-0.2]
</p><p>92 We apply our method to human body detection in a single image. [sent-199, score-0.184]
</p><p>93 We manually label five body parts (both arms, both legs and the head) of a person walking on a treadmill in all  32 images of a complete gait cycle. [sent-200, score-0.364]
</p><p>94 Using the magnitude thresholded edge orientations in the hand-labeled boxes as features, we train linear Fisher classifiers [2] for each body part. [sent-201, score-0.255]
</p><p>95 Each individual classifier is trained to discriminate between the body part and a random image patch. [sent-203, score-0.314]
</p><p>96 We iteratively re-train the classifiers using false positives until the optimal performance is reached over the training set. [sent-204, score-0.208]
</p><p>97 In addition, we train linear colorbased classifiers for each body part to perform figure-ground discrimination at the pixel level. [sent-205, score-0.382]
</p><p>98 Though the pixelpatch affinity matrix C, derived from the color classifier, is neither precise nor complete, and the edges are weak at many object boundaries, the two processes complement each other in our pixel-patch grouping system and output a reasonably good object segmentation. [sent-210, score-1.379]
</p><p>99 segmentation alone: 68 seconds segmentation-recognition: 58 seconds Figure 8: Eigenvectors and their segmentations for the 261 x 183 human body image in Fig. [sent-211, score-0.813]
</p><p>100 Rapid object detection using a boosted cascade of simple features. [sent-260, score-0.453]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('patch', 0.419), ('object', 0.408), ('segmentation', 0.388), ('patches', 0.258), ('affinity', 0.241), ('grouping', 0.175), ('pixels', 0.167), ('pixel', 0.146), ('image', 0.123), ('xl', 0.108), ('body', 0.102), ('cues', 0.1), ('edges', 0.089), ('compatibility', 0.085), ('graph', 0.083), ('edge', 0.083), ('leg', 0.081), ('detected', 0.078), ('background', 0.077), ('positives', 0.076), ('classifiers', 0.07), ('lt', 0.064), ('part', 0.064), ('head', 0.063), ('false', 0.062), ('arm', 0.061), ('node', 0.06), ('cluttered', 0.06), ('partitioning', 0.06), ('alone', 0.059), ('recognition', 0.057), ('configurations', 0.055), ('segmentations', 0.055), ('detect', 0.055), ('parts', 0.055), ('nodes', 0.055), ('seconds', 0.054), ('competition', 0.054), ('labeling', 0.054), ('virtual', 0.052), ('oe', 0.051), ('detector', 0.049), ('concurrent', 0.048), ('validated', 0.048), ('perceptually', 0.048), ('middle', 0.047), ('mahamud', 0.046), ('treadmill', 0.046), ('vision', 0.046), ('detection', 0.045), ('association', 0.042), ('coherent', 0.042), ('thicker', 0.04), ('exclusion', 0.04), ('falsely', 0.04), ('gait', 0.04), ('saliency', 0.04), ('filter', 0.037), ('arms', 0.037), ('group', 0.037), ('human', 0.037), ('oriented', 0.036), ('five', 0.035), ('winner', 0.034), ('legs', 0.034), ('occlusion', 0.034), ('vi', 0.034), ('labeled', 0.034), ('weak', 0.033), ('coupled', 0.033), ('eigenvector', 0.033), ('configuration', 0.032), ('cuts', 0.032), ('lsi', 0.032), ('consistency', 0.032), ('partial', 0.031), ('conform', 0.031), ('calculate', 0.03), ('intensity', 0.03), ('lost', 0.029), ('contour', 0.028), ('salient', 0.028), ('eigenvalue', 0.027), ('ax', 0.027), ('find', 0.027), ('spatial', 0.026), ('manually', 0.026), ('similarity', 0.026), ('encode', 0.026), ('label', 0.026), ('rest', 0.025), ('let', 0.025), ('contours', 0.025), ('unlabeled', 0.025), ('classifier', 0.025), ('matrix', 0.025), ('gets', 0.024), ('local', 0.023), ('knowledge', 0.023), ('appearance', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="57-tfidf-1" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>2 0.21353358 <a title="57-tfidf-2" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>3 0.20810527 <a title="57-tfidf-3" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>4 0.1809126 <a title="57-tfidf-4" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>5 0.17023678 <a title="57-tfidf-5" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image. This might be attempted by using simple ﬁlters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images. In this paper we describe an approach which lies between the two extremes. It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves. We use a dynamic tree-like architecture to model the high resolution data. Approximate conditioning on the low resolution image is achieved through a mean ﬁeld approach. 1</p><p>6 0.16072565 <a title="57-tfidf-6" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>7 0.13779213 <a title="57-tfidf-7" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>8 0.1269398 <a title="57-tfidf-8" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>9 0.10843387 <a title="57-tfidf-9" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>10 0.10168468 <a title="57-tfidf-10" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>11 0.10045505 <a title="57-tfidf-11" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>12 0.093969837 <a title="57-tfidf-12" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>13 0.093328863 <a title="57-tfidf-13" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>14 0.071899064 <a title="57-tfidf-14" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>15 0.070033446 <a title="57-tfidf-15" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>16 0.068572938 <a title="57-tfidf-16" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>17 0.067500077 <a title="57-tfidf-17" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>18 0.065646462 <a title="57-tfidf-18" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>19 0.062272355 <a title="57-tfidf-19" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>20 0.061113738 <a title="57-tfidf-20" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, 0.002), (2, 0.001), (3, 0.308), (4, -0.03), (5, -0.03), (6, 0.214), (7, -0.068), (8, -0.025), (9, -0.03), (10, 0.032), (11, 0.016), (12, -0.052), (13, 0.044), (14, -0.077), (15, -0.061), (16, 0.067), (17, -0.015), (18, 0.09), (19, 0.003), (20, -0.155), (21, 0.097), (22, 0.012), (23, 0.02), (24, 0.035), (25, -0.061), (26, 0.092), (27, -0.094), (28, -0.108), (29, 0.037), (30, 0.046), (31, -0.074), (32, -0.115), (33, 0.03), (34, -0.127), (35, -0.127), (36, 0.089), (37, 0.032), (38, 0.107), (39, -0.015), (40, 0.072), (41, 0.022), (42, -0.216), (43, -0.032), (44, -0.043), (45, -0.019), (46, 0.095), (47, -0.043), (48, 0.031), (49, -0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98741287 <a title="57-lsi-1" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>2 0.63358009 <a title="57-lsi-2" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>3 0.62249202 <a title="57-lsi-3" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>4 0.58583063 <a title="57-lsi-4" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>5 0.49771339 <a title="57-lsi-5" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>6 0.44872802 <a title="57-lsi-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.38790694 <a title="57-lsi-7" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>8 0.38218105 <a title="57-lsi-8" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>9 0.36928761 <a title="57-lsi-9" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>10 0.36545861 <a title="57-lsi-10" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>11 0.36442313 <a title="57-lsi-11" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>12 0.35539669 <a title="57-lsi-12" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>13 0.34528306 <a title="57-lsi-13" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>14 0.33140635 <a title="57-lsi-14" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>15 0.32933849 <a title="57-lsi-15" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>16 0.32156384 <a title="57-lsi-16" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>17 0.31263444 <a title="57-lsi-17" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>18 0.31088361 <a title="57-lsi-18" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>19 0.30243999 <a title="57-lsi-19" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>20 0.29370958 <a title="57-lsi-20" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.014), (23, 0.013), (42, 0.034), (54, 0.066), (55, 0.026), (68, 0.023), (74, 0.597), (87, 0.019), (92, 0.021), (98, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98428857 <a title="57-lda-1" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>2 0.96048659 <a title="57-lda-2" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>Author: Martin Szummer, Tommi S. Jaakkola</p><p>Abstract: Classiﬁcation with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P (x)), to further constrain the conditional P (y|x) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P (x). We develop algorithms for solving the regularization problem for ﬁnite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases.</p><p>3 0.95630175 <a title="57-lda-3" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>4 0.95554644 <a title="57-lda-4" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>5 0.94751841 <a title="57-lda-5" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>6 0.89213043 <a title="57-lda-6" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>7 0.74011606 <a title="57-lda-7" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>8 0.70560354 <a title="57-lda-8" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>9 0.68191308 <a title="57-lda-9" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>10 0.67437804 <a title="57-lda-10" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>11 0.67352206 <a title="57-lda-11" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>12 0.67128086 <a title="57-lda-12" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>13 0.66750783 <a title="57-lda-13" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>14 0.65925682 <a title="57-lda-14" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>15 0.6504004 <a title="57-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.6370579 <a title="57-lda-16" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>17 0.62034267 <a title="57-lda-17" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>18 0.61733311 <a title="57-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.61306518 <a title="57-lda-19" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>20 0.60977668 <a title="57-lda-20" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
