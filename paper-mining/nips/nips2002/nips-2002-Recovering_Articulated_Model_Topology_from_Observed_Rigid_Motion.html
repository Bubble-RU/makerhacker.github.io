<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-172" href="#">nips2002-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</h1>
<br/><p>Source: <a title="nips-2002-172-pdf" href="http://papers.nips.cc/paper/2182-recovering-articulated-model-topology-from-observed-rigid-motion.pdf">pdf</a></p><p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>Reference: <a title="nips-2002-172-reference" href="../nips2002_reference/nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Accurate representation of articulated motion is a challenging problem for machine perception. [sent-4, score-0.759]
</p><p>2 Several successful tracking algorithms have been developed that model human body as an articulated tree. [sent-5, score-0.78]
</p><p>3 We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. [sent-6, score-0.828]
</p><p>4 This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. [sent-7, score-1.543]
</p><p>5 Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. [sent-8, score-0.706]
</p><p>6 The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. [sent-9, score-1.027]
</p><p>7 We demonstrate the performance of our algorithm on both synthetic and real motion capture data. [sent-10, score-0.369]
</p><p>8 1 Introduction Tracking human motion is an integral part of many proposed human-computer interfaces, surveillance and identiﬁcation systems, as well as animation and virtual reality systems. [sent-11, score-0.316]
</p><p>9 A common approach to this task is to model the body as a kinematic tree, and reformulate the problem as articulated body tracking[6]. [sent-12, score-0.966]
</p><p>10 Most of the state-of-the-art systems rely on predeﬁned kinematic models [16]. [sent-13, score-0.098]
</p><p>11 We are interested in a principled way to recover articulated models from observations. [sent-15, score-0.514]
</p><p>12 The recovered models may then be used for further tracking and/or recognition. [sent-16, score-0.131]
</p><p>13 In the ﬁrst stage the rigidly moving segments are tracked independently; at the second stage, the topology of the body (the connectivity between the segments) is recovered. [sent-18, score-0.651]
</p><p>14 After the topology is determined, the joint parameters may be determined. [sent-19, score-0.252]
</p><p>15 In this paper we concentrate on the second stage of this task, estimating the underlying topology of the observed articulated body, when the motion of the constituent rigid bodies is known. [sent-20, score-1.531]
</p><p>16 If we assume that the body may be modeled as a kinematic tree, and motion of a particular rigid segment is known, then the motions of the rigid segments that are connected through that segment are independent of each other. [sent-22, score-1.773]
</p><p>17 That is, we can model a probability distribution of the full body-  pose as a tree-structured graphical model, where each node corresponds to pose of a rigid segment. [sent-23, score-0.56]
</p><p>18 This observation allows us to formulate the problem of recovering topology of an articulated body as ﬁnding the tree-shaped graphical model that best (in the Maximum Likelihood sense) describes the observations. [sent-24, score-0.982]
</p><p>19 2 Prior Work While state-of-the-art tracking algorithms [16] do not address either model creation or model initialization, the necessity of automating these two steps has been long recognized. [sent-25, score-0.072]
</p><p>20 The approach in [10] required a subject to follow a set of predeﬁned movements, and recovered the descriptions of body parts and body topology from deformations of apparent contours. [sent-26, score-0.671]
</p><p>21 Various heuristics were used in [12] to adapt an articulated model of known topology to 3D observations. [sent-27, score-0.7]
</p><p>22 Analysis of magnetic motion capture data was used by [14] to recover limb lengths and joint locations for known topology, it also suggested similar analysis for topology extraction. [sent-28, score-0.643]
</p><p>23 A learning based approach for decomposing a set of observed marker positions and velocities into sets corresponding to various body parts was described in [17]. [sent-29, score-0.273]
</p><p>24 Our work builds on the latter two approaches in estimating the topology of the articulated tree model underlying the observed motion. [sent-30, score-0.893]
</p><p>25 Several methods have been used to recover multiple rigid motions from video, such as factorization [3, 18], RANSAC [7], and learning based methods [9]. [sent-31, score-0.59]
</p><p>26 In this work we assume that the 3-D rigid motions has been recovered and are represented using a 2-D Scaled Prismatic Model (SPM). [sent-32, score-0.572]
</p><p>27 3 Representing Pose and Motion A 2-D Scaled Prismatic Model (SPM) was proposed by [15] and is useful for representing image motion of projections of elongated 3-D objects. [sent-33, score-0.301]
</p><p>28 It is obtained by orthographically “projecting” the major axis of the object to the image plane. [sent-34, score-0.042]
</p><p>29 3-D rigid motion of an object, may be simulated by SPM transformations, using in-plane translation for rigid translation, and rotation and uniform scaling for plane-parallel and out-of-plane rotations respectively. [sent-36, score-1.203]
</p><p>30 SPM motion (or pose) may be expressed as a linear transformation in projective space as M=  a −b e b a f 0 0 1  (1)  Following [13] we have chosen to use exponential coordinates, derived from constant velocity equations, to parameterize motion. [sent-37, score-0.451]
</p><p>31 An SPM transformation may be represented as an exponential map ˆ  M = eξ  c ˆ ξ=θ ω 0  −ω c 0  vx vy 0    vx  vy  ξ = θ  ω c  (2)  In this representation vx is a horizontal velocity, vy – vertical velocity, ω – angular velocity, and c is a rate of scale change. [sent-38, score-0.697]
</p><p>32 Note that there is an inherent scale ambiguity, since θ and (vx , vy , ω, c)T may be chosen arbitrarily, as long as ˆ eξ = M. [sent-40, score-0.133]
</p><p>33 It can be shown ([13]) that if the SPM transformation is a combination of scaling and rotation, it may be expressed by the sum of two twists, with coincident centers (u x , uy )T of rotation and expansion. [sent-41, score-0.398]
</p><p>34     uy −ux −c −ux  −uy  −ω ξ = ω +c = 1  0   0 1  ω −c     ux   uy   ω  1 c   1  (3)  While “pure” translation, rotation or scale have intuitive representation with twists, the combination or rotation and scale does not. [sent-42, score-0.756]
</p><p>35 We propose a scaled twist representation, that preserves the intuitiveness of representation for all possible SPM motions. [sent-43, score-0.082]
</p><p>36 We want to separate the “direction” of motion (the direction of translation or the relative amounts of rotation and scale) from the amount of motion. [sent-44, score-0.527]
</p><p>37 If the transformation involves rotation and/or scale, then we choose θ so that ||(ω, c)|| 2 = 1, and then use eq. [sent-45, score-0.197]
</p><p>38 The computation may be expressed as a linear transformation: √  θ  ux      τ =  uy  =  ω    c       ω 2 + c2 ˜ ˜ c ˜ − ω2 +˜2 ˜ c ω ˜ ω 2 +˜2 ˜ c  ˜ − ω 2 ω c2 ˜ +˜ c ˜ − ω2 +˜2 ˜ c  √  1 ω 2 +˜2 ˜ c  √  1 ω 2 +˜2 ˜ c  where ξ = (˜x , vy , ω, c)T . [sent-47, score-0.288]
</p><p>39 v ˜ ˜ ˜     1   vx   ˜    vy   ˜   ω ˜  c ˜  (4)  The the pure translational motion (ω = c = 0) may be regarded as an inﬁnitely small rotation about a point at inﬁnity, e. [sent-48, score-0.716]
</p><p>40 4 Learning Articulated Topology We wish to infer the underlying topology of an articulated body from noisy observations of a set of rigid body motions. [sent-52, score-1.498]
</p><p>41 As a practical matter, one must make choices regarding density models; we discuss one such choice although other choices are also suitable. [sent-54, score-0.092]
</p><p>42 We denote the set of observed motions of N rigid bodies at time t, 1 ≤ t ≤ F as a set {Mt |1 ≤ s ≤ N }. [sent-55, score-0.579]
</p><p>43 Variables M i with observations  {Mt |1 ≤ t ≤ F } are assigned to the vertices of a graph, while edges between nodes indii cate dependency. [sent-58, score-0.072]
</p><p>44 We shall denote presence or absence of an edge between two variables, Mi and Mj by an index variable Eij , equal to one if an edge is present and zero otherwise. [sent-59, score-0.078]
</p><p>45 Furthermore, if the corresponding graphical model is a spanning tree, it can be expressed as a product of conditional densities (e. [sent-60, score-0.12]
</p><p>46 , MN ) =  PMs |pa(Ms ) (Ms |pa (Ms ))  (7)  Ms  where pa(Ms ) is the parent of Ms . [sent-65, score-0.035]
</p><p>47 While multiple nodes may have the same parent, each individual node has only one parent node. [sent-66, score-0.1]
</p><p>48 Furthermore, in any decomposition one node (the root node) has no parent. [sent-67, score-0.043]
</p><p>49 Any node (variable) in the model can serve as the root node [8]. [sent-68, score-0.086]
</p><p>50 Of the possible tree models (choices of E), we wish to choose the maximum likelihood tree which is equivalent to the minimum entropy tree [4]. [sent-70, score-0.495]
</p><p>51 The entropy of a tree model can be written H(M ) =  H(Ms ) − s  I(Mi ; Mj )  (8)  Eij =1  where H(Ms ) is the marginal entropy of each variable and I(Mi ; Mj ) is the mutual information between nodes Mi and Mj and quantiﬁes their statistical dependence. [sent-71, score-0.352]
</p><p>52 Consequently, the minimum entropy tree corresponds to the choice of E which minimizes the sum of the pairwise mutual informations [1]. [sent-72, score-0.349]
</p><p>53 The tree denoted by E can be found via the maximum spanning tree algorithm [2] using I(Mi ; Mj ) for all i, j as the edge weights. [sent-73, score-0.371]
</p><p>54 Our conjecture is that if our data are sampled from a variety of motions the topology of the estimated density model is likely to be the same as the topology of the articulated body model. [sent-74, score-1.348]
</p><p>55 It follows from the intuition that when considering only pairwise relationships, the relative motions of physically connected bodies will be most strongly related. [sent-75, score-0.278]
</p><p>56 1 Estimation of Mutual Information Computing the minimum entropy spanning tree requires estimating the pairwise mutual informations between rigid motions Mi and Mj for all i, j pairs. [sent-77, score-0.946]
</p><p>57 In order to do so we must make a choice regarding the parameterization of motion and a probability density over that parameterization; to estimate articulated topology it is sufﬁcient to use the the Scaled Prismatic Model with twist parameterization described in Section 3). [sent-78, score-1.132]
</p><p>58 2 Estimating Motion Entropy t We parameterize rigid motion, Mt , by the vector of quantities ξi (cf. [sent-80, score-0.377]
</p><p>59 t Mit Mj|i )  is  (11)  We wish to use scaled twists (Section 3) to compute the entropies involved. [sent-84, score-0.182]
</p><p>60 4 and 5), the entropies are related, H(ξ) = H(τ ) − E[log det(A)], (12) where E[log det(A)] may be estimated using Equation 6. [sent-86, score-0.06]
</p><p>61 3 Estimating the Motion Kernel In order to estimate the entropy of motion, we need to estimate the probability density based on the available samples. [sent-88, score-0.096]
</p><p>62 Since the functional form of the underlying density is not known we have chosen to use kernel-based density estimator, p(τ ) = α ˆ  K(τ ; τi ). [sent-89, score-0.119]
</p><p>63 If τ1 and τ2 do not represent pure translational motions, then they should be considered to be close if their centers of rotation are close. [sent-92, score-0.254]
</p><p>64 If τ1 and τ2 are pure translations, then they should be considered close if their directions are close. [sent-94, score-0.056]
</p><p>65 If τ1 and τ2 represent different types of motion (i. [sent-96, score-0.281]
</p><p>66 5 Implementation The input to our algorithm is a set of SPM poses (Section 3) {Pt |1 ≤ s ≤ S, 1 ≤ t ≤ T }, s where S is the number of tracked rigid segments and F is the number of frames. [sent-103, score-0.578]
</p><p>67 Pt1 )−1 s s s1 s2 s2 |s  (16)  t1 t t1 The parameter vectors τs2 t and τs2 |s1 are then extracted from the transformation matrices Ms2 and Ms2 |s1 (cf. [sent-105, score-0.089]
</p><p>68 Section 3), and the mutual information is estimated as described in Section 4. [sent-106, score-0.114]
</p><p>69 6 Results We have tested our algorithm both on synthetic and motion capture data. [sent-108, score-0.369]
</p><p>70 Two synthetic sequences were generated with the following steps. [sent-109, score-0.049]
</p><p>71 First, the rigid segments were positioned by randomly perturbing parameters of the corresponding kinematic tree structure. [sent-110, score-0.72]
</p><p>72 At each time step point positions were computed based on the corresponding segment pose, and perturbed with Gaussian noise with zero mean and standard deviation of 1 pixel. [sent-112, score-0.127]
</p><p>73 The inputs to the algorithm were the segment poses re-estimated from the feature point coordinates. [sent-113, score-0.143]
</p><p>74 In the motion capture-based experiment, the segment poses were estimated from the marker positions. [sent-114, score-0.498]
</p><p>75 The ﬁrst experiment involved a simple kinematic chain with 3 segments in order to demonstrate the operation of the algorithm. [sent-119, score-0.232]
</p><p>76 The system has a rotational joint between S 1 and S2 and prismatic joint between S2 and S3 . [sent-120, score-0.205]
</p><p>77 The sample conﬁgurations of the articulated body are shown in the ﬁrst row of the Figures 6. [sent-121, score-0.673]
</p><p>78 2 and the corresponding maximum spanning tree are in Figures 6. [sent-124, score-0.194]
</p><p>79 The second experiment involved a humanoid torso-like synthetic model containing 5 rigid segments. [sent-126, score-0.437]
</p><p>80 For the human motion experiment, we have used motion capture data of a dance sequence (Figure 6. [sent-130, score-0.671]
</p><p>81 The rigid segment motion was extracted from the positions of the markers tracked across 220 frames (the marker correspondence to the body locations was known). [sent-132, score-1.131]
</p><p>82 The algorithm was able to correctly recover the articulated body topology (Compare Figures 6. [sent-133, score-0.931]
</p><p>83 The dance is a highly structured activity, so not all degrees of freedom were explored in this sequence, and mutual information between some unconnected segments (e. [sent-136, score-0.284]
</p><p>84 7 Conclusions We have presented a novel general technique for recovering the underlying articulated structure from information about rigid segment motion. [sent-139, score-1.006]
</p><p>85 Our method relies on only a very weak assumption, that this structure may be represented by a tree with unknown topology. [sent-140, score-0.138]
</p><p>86 While the results presented in this paper were obtained using the Scaled Prismatic Model and non-parametric density estimator, our methodology does not rely on either modeling assumption. [sent-141, score-0.046]
</p><p>87 The ﬁrst row shows 3 sample frames from a 100 frame synthetic sequence. [sent-171, score-0.143]
</p><p>88 The adjacency matrix of the mutual information graph is shown in (d), with intensities corresponding to edge weights. [sent-172, score-0.274]
</p><p>89 The vertices in the graph correspond to the rigid segments labeled in (a). [sent-173, score-0.583]
</p><p>90 The sample frames from a randomly generated 150 frame sequence are shown in (a), (b), and (c). [sent-177, score-0.094]
</p><p>91 The adjacency matrix of the mutual information graph is shown in (d), with intensities corresponding to edge weights. [sent-178, score-0.274]
</p><p>92 The vertices in the graph correspond to the rigid segments labeled in (a). [sent-179, score-0.583]
</p><p>93 (a), (b), and (c) are the sample frames from a 220 frame sequence. [sent-183, score-0.094]
</p><p>94 The adjacency matrix of the mutual information graph is shown in (d), with intensities corresponding to edge weights. [sent-184, score-0.274]
</p><p>95 The vertices in the graph correspond to the rigid segments labeled in (a). [sent-185, score-0.583]
</p><p>96 A 3d featurebased tracker for tracking multiple moving objects with a controlled binocular head. [sent-192, score-0.094]
</p><p>97 Automatic joint parameter estimation from magnetic motion capture data. [sent-226, score-0.385]
</p><p>98 Singularities in articulated object tracking with 2-d and 3-d models. [sent-231, score-0.572]
</p><p>99 Stochastic tracking of 3d human ﬁgures using 2d image motion. [sent-236, score-0.127]
</p><p>100 Monocular perception of biological motion - detection and labeling. [sent-240, score-0.281]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('articulated', 0.478), ('rigid', 0.35), ('motion', 0.281), ('topology', 0.222), ('mj', 0.212), ('spm', 0.198), ('body', 0.195), ('motions', 0.163), ('uy', 0.154), ('ux', 0.138), ('tree', 0.138), ('mi', 0.137), ('segments', 0.134), ('rotation', 0.129), ('prismatic', 0.11), ('vy', 0.107), ('segment', 0.101), ('kinematic', 0.098), ('vx', 0.094), ('translation', 0.093), ('mutual', 0.092), ('tracking', 0.072), ('mt', 0.07), ('ms', 0.07), ('transformation', 0.068), ('bodies', 0.066), ('twists', 0.066), ('pt', 0.065), ('pose', 0.065), ('recovered', 0.059), ('vision', 0.057), ('spanning', 0.056), ('pure', 0.056), ('frames', 0.053), ('marker', 0.052), ('adjacency', 0.052), ('tracked', 0.052), ('recovering', 0.05), ('vertices', 0.05), ('entropy', 0.05), ('translational', 0.049), ('graph', 0.049), ('synthetic', 0.049), ('velocity', 0.048), ('scaled', 0.047), ('density', 0.046), ('informations', 0.044), ('node', 0.043), ('det', 0.042), ('intensities', 0.042), ('poses', 0.042), ('factorization', 0.041), ('frame', 0.041), ('capture', 0.039), ('edge', 0.039), ('prede', 0.039), ('eij', 0.038), ('entropies', 0.038), ('humanoid', 0.038), ('multibody', 0.038), ('graphical', 0.037), ('figures', 0.036), ('recover', 0.036), ('parameterization', 0.035), ('parent', 0.035), ('dance', 0.035), ('twist', 0.035), ('trevor', 0.035), ('rotational', 0.035), ('magnetic', 0.035), ('human', 0.035), ('kr', 0.033), ('wish', 0.031), ('joint', 0.03), ('kt', 0.029), ('estimating', 0.028), ('constituent', 0.028), ('expressed', 0.027), ('underlying', 0.027), ('parameterize', 0.027), ('positions', 0.026), ('stage', 0.026), ('scale', 0.026), ('concentrate', 0.025), ('pairwise', 0.025), ('condition', 0.024), ('relative', 0.024), ('pa', 0.024), ('freedom', 0.023), ('video', 0.023), ('choices', 0.023), ('nodes', 0.022), ('moving', 0.022), ('estimated', 0.022), ('object', 0.022), ('john', 0.022), ('extracted', 0.021), ('image', 0.02), ('centers', 0.02), ('identity', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="172-tfidf-1" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>2 0.15231745 <a title="172-tfidf-2" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>3 0.11200577 <a title="172-tfidf-3" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>4 0.10564883 <a title="172-tfidf-4" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>5 0.090890348 <a title="172-tfidf-5" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We focus on the problem of efﬁcient learning of dependency trees. It is well-known that given the pairwise mutual information coefﬁcients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construction of the correlation matrix that dominates the running time. We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights. The partial knowledge we maintain is a probabilistic conﬁdence interval on the coefﬁcients, which we derive by examining just a small sample of the data. The algorithm is able to ﬂag the need to shrink an interval, which translates to inspection of more data for the particular attribute pair. Experimental results show running time that is near-constant in the number of records, without signiﬁcant loss in accuracy of the generated trees. Interestingly, our spanning-tree algorithm is based solely on Tarjan’s red-edge rule, which is generally considered a guaranteed recipe for bad performance. 1</p><p>6 0.078479946 <a title="172-tfidf-6" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>7 0.073465154 <a title="172-tfidf-7" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>8 0.070805848 <a title="172-tfidf-8" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>9 0.068651564 <a title="172-tfidf-9" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>10 0.065646462 <a title="172-tfidf-10" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>11 0.060253695 <a title="172-tfidf-11" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>12 0.056950256 <a title="172-tfidf-12" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>13 0.05672301 <a title="172-tfidf-13" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>14 0.052109633 <a title="172-tfidf-14" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>15 0.048343517 <a title="172-tfidf-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.048047088 <a title="172-tfidf-16" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>17 0.04613604 <a title="172-tfidf-17" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>18 0.041781612 <a title="172-tfidf-18" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>19 0.040486284 <a title="172-tfidf-19" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>20 0.04015772 <a title="172-tfidf-20" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.012), (2, -0.016), (3, 0.086), (4, -0.027), (5, 0.044), (6, 0.022), (7, 0.006), (8, 0.077), (9, 0.066), (10, 0.023), (11, 0.119), (12, -0.001), (13, -0.037), (14, -0.125), (15, -0.023), (16, 0.131), (17, 0.035), (18, -0.046), (19, -0.108), (20, 0.0), (21, 0.252), (22, 0.042), (23, -0.019), (24, -0.002), (25, 0.066), (26, -0.126), (27, 0.04), (28, -0.009), (29, 0.066), (30, 0.063), (31, -0.184), (32, -0.002), (33, -0.033), (34, -0.07), (35, -0.048), (36, -0.071), (37, -0.059), (38, -0.04), (39, -0.109), (40, 0.023), (41, 0.216), (42, 0.01), (43, -0.197), (44, -0.03), (45, -0.109), (46, -0.012), (47, -0.035), (48, -0.078), (49, 0.154)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96842289 <a title="172-lsi-1" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>2 0.60458738 <a title="172-lsi-2" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>3 0.52039212 <a title="172-lsi-3" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>4 0.41475108 <a title="172-lsi-4" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>Author: Chakra Chennubhotla, Allan D. Jepson</p><p>Abstract: Using a Markov chain perspective of spectral clustering we present an algorithm to automatically ﬁnd the number of stable clusters in a dataset. The Markov chain’s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenﬂows along with their halﬂives. An eigenﬂow describes the ﬂow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halﬂife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenﬂow and inﬁnite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identiﬁed by computing the sensitivity of the eigenﬂow’s halﬂife to variations in the edge weights. We propose a novel E IGEN C UTS algorithm to perform clustering that removes these identiﬁed bottlenecks in an iterative fashion.</p><p>5 0.40449211 <a title="172-lsi-5" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>6 0.34978613 <a title="172-lsi-6" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>7 0.31210992 <a title="172-lsi-7" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>8 0.30769235 <a title="172-lsi-8" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>9 0.29910669 <a title="172-lsi-9" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>10 0.27739385 <a title="172-lsi-10" href="./nips-2002-Analysis_of_Information_in_Speech_Based_on_MANOVA.html">29 nips-2002-Analysis of Information in Speech Based on MANOVA</a></p>
<p>11 0.27238467 <a title="172-lsi-11" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>12 0.26323685 <a title="172-lsi-12" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>13 0.25933301 <a title="172-lsi-13" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>14 0.24533863 <a title="172-lsi-14" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>15 0.24335305 <a title="172-lsi-15" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>16 0.24255019 <a title="172-lsi-16" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>17 0.23826459 <a title="172-lsi-17" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>18 0.2374045 <a title="172-lsi-18" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>19 0.23475005 <a title="172-lsi-19" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>20 0.22728521 <a title="172-lsi-20" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.046), (11, 0.018), (23, 0.035), (41, 0.344), (42, 0.048), (54, 0.116), (55, 0.037), (57, 0.017), (68, 0.018), (74, 0.113), (92, 0.025), (98, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89358515 <a title="172-lda-1" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><p>2 0.88781559 <a title="172-lda-2" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>same-paper 3 0.8158865 <a title="172-lda-3" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>4 0.80967289 <a title="172-lda-4" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>5 0.7458629 <a title="172-lda-5" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>Author: Patrick J. Wolfe, Simon J. Godsill</p><p>Abstract: The Bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music—something which has often been overlooked in traditional audio signal processing approaches. Here, after constructing a Bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms, we apply Markov chain Monte Carlo methods in order to sample from the resultant posterior distribution of interest. We present speech enhancement results which compare favourably in objective terms with standard time-varying ﬁltering techniques (and in several cases yield superior performance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledge of the noise power.</p><p>6 0.51327378 <a title="172-lda-6" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>7 0.50580549 <a title="172-lda-7" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>8 0.49931493 <a title="172-lda-8" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>9 0.49034673 <a title="172-lda-9" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>10 0.48524457 <a title="172-lda-10" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>11 0.47942391 <a title="172-lda-11" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>12 0.47766492 <a title="172-lda-12" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>13 0.47747514 <a title="172-lda-13" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>14 0.47605324 <a title="172-lda-14" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>15 0.47552323 <a title="172-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.47512445 <a title="172-lda-16" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>17 0.47467104 <a title="172-lda-17" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>18 0.47456282 <a title="172-lda-18" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>19 0.4725205 <a title="172-lda-19" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>20 0.47251242 <a title="172-lda-20" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
