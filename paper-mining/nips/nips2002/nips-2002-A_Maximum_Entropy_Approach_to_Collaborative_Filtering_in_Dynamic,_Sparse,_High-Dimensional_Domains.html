<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-8" href="#">nips2002-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</h1>
<br/><p>Source: <a title="nips-2002-8-pdf" href="http://papers.nips.cc/paper/2278-a-maximum-entropy-approach-to-collaborative-filtering-in-dynamic-sparse-high-dimensional-domains.pdf">pdf</a></p><p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>Reference: <a title="nips-2002-8-reference" href="../nips2002_reference/nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. [sent-9, score-0.383]
</p><p>2 We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. [sent-10, score-0.732]
</p><p>3 We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. [sent-11, score-0.657]
</p><p>4 We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users. [sent-13, score-0.996]
</p><p>5 1 Introduction Recommender systems attempt to automate the process of “word of mouth” recommendations within a community. [sent-14, score-0.141]
</p><p>6 Typical application environments are dynamic in many respects: users come and go, users preferences and goals change, items are added and removed, and user navigation itself is a dynamic process. [sent-15, score-0.588]
</p><p>7 Consider, for instance, the problem of generating recommendations within ResearchIndex (a. [sent-17, score-0.141]
</p><p>8 , CiteSeer),1 an online digital library of computer science papers, receiving thousands of user accesses per hour. [sent-20, score-0.312]
</p><p>9 The site automatically locates computer science papers found on the Web, indexes their full text, allows browsing via the literature citation graph, and isolates the text around citations, among other services [8]. [sent-21, score-0.162]
</p><p>10 com  documents including the full text of each document, citation links between documents, and a wealth of user access data. [sent-24, score-0.517]
</p><p>11 With so many documents, and only seven accesses per user on average, the user-document data matrix is exceedingly sparse and thus challenging to model. [sent-25, score-0.343]
</p><p>12 In this paper, we work with the ResearchIndex data, since it is an interesting application domain, and is typical of many recommendation application areas [14]. [sent-26, score-0.123]
</p><p>13 A content ﬁltering approach is to recommend solely based on the features of a document (e. [sent-28, score-0.206]
</p><p>14 , showing documents written by the same author(s), or textually similar documents to ). [sent-30, score-0.446]
</p><p>15 Another possibility is to perform collaborative ﬁltering [13] by assessing the similarities between the documents requested by the current user and the users who interacted with ResearchIndex in the past. [sent-32, score-0.772]
</p><p>16 Once the users with browsing histories similar to that of a given user are identiﬁed, an assumption is made that the future browsing patterns will be similar as well, and the prediction is made accordingly. [sent-33, score-0.504]
</p><p>17 Common measures of similarity between users include Pearson correlation coefﬁcient [13], mean squared error [16], and vector similarity [1]. [sent-34, score-0.098]
</p><p>18 Most of these recommendation algorithms are context and order independent: that is, the rank of recommendations does not depend on the context of the user’s current navigation or on recency effects (past viewed items receive as much weight as recently viewed items). [sent-36, score-0.433]
</p><p>19 To overcome the sparsity and high dimensionality of the data, we cluster the documents with an objective of maximizing the likelihood that recommendable items co-occur in the same cluster. [sent-39, score-0.479]
</p><p>20 By marrying the clustering technique with the end goal of recommendation, our approach appears to do a good job at maintaining high recall (sensitivity). [sent-40, score-0.095]
</p><p>21 Similar ideas in the context of maxent were proposed recently by Goodman in [5]. [sent-41, score-0.649]
</p><p>22 We explicitly model time: each user is associated with a set of sessions, and each session is modeled as a time sequence of document accesses. [sent-42, score-0.486]
</p><p>23 We present a maxent model that effectively estimates the probability of the next visited document ID (DID) given the most recently visited DID (“bigrams”) and past indicative DIDs (“triggers”). [sent-43, score-0.826]
</p><p>24 To our knowledge, this is the ﬁrst application of maxent for collaborative ﬁltering, and one of the few published formulations that makes accurate recommendations in the context of a dynamic user session [3, 15]. [sent-44, score-1.261]
</p><p>25 We perform ofﬂine empirical tests of our recommender and compare it to competing models. [sent-45, score-0.116]
</p><p>26 In Section 2, we describe the log data from ResearchIndex and how we preprocessed it. [sent-48, score-0.026]
</p><p>27 Section 3 presents the greedy algorithm for clustering the documents and discusses how the clustering helps to decompose the original prediction task. [sent-49, score-0.464]
</p><p>28 In Section 4, we give a high-level description of our maxent model and the features we used for its learning. [sent-50, score-0.649]
</p><p>29 2 Preprocessing the ResearchIndex data Each document indexed in ResearchIndex is assigned a unique document ID (DID). [sent-53, score-0.335]
</p><p>30 Whenever a user accesses the site with a cookie-enabled browser, (s)he is identiﬁed as a new or returning user and all activity is recorded on the server side with a unique user ID (UID) and a time stamp (TID). [sent-54, score-0.792]
</p><p>31 We obtained a log ﬁle that recorded approximately 3 month worth of ResearchIndex data that can roughly be viewed as a series of requests . [sent-55, score-0.168]
</p><p>32 © ¥ §¥ £ ¡  £ ¦ £ ¨¦ ¤¢                 £§  In the ﬁrst processing step, we aggregated the requests by the and broke them into sessions. [sent-56, score-0.149]
</p><p>33 For a ﬁxed UID, a session is deﬁned as a sequence of document requests, with no two consecutive requests more than seconds apart. [sent-57, score-0.386]
</p><p>34 In our experiments we chose , so that if a user was inactive for more than 300 seconds, his next request was considered to mark a start of a new session. [sent-58, score-0.24]
</p><p>35 3 Dimensionality Reduction Via Clustering Even after the log is processed, the data still remains high-dimensional (62,240 documents), and sparse, and hence still hard to model. [sent-60, score-0.026]
</p><p>36 Since our objective was to predict the instantaneous user interests, among many possibilities of performing the clustering we chose to cluster based on user navigation patterns. [sent-62, score-0.814]
</p><p>37  "     We scanned the processed log once and for each document accumulated the number was requested immediately after ; in other words, we of times the document computed the ﬁrst-order Markov statistics or bigrams. [sent-64, score-0.443]
</p><p>38 Based on the user navigation patterns encoded in bigrams, the greedy clustering is done as shown in the following pseudocode:  ( & )%'"         ;  9  9 @ 7 2 865¥ 4 31  ; Number of Clusters Clusters. [sent-65, score-0.414]
</p><p>39 eee f i (QQf i (QQf ¦ ¦(© hf i i 6H) i 4 ¢ 4R 4¤Hi 4¤Hi  ¦ H f 4 ¦ eee ¥ P 6A4  hi  ¥ f  I%p) ¥ P  h¦  'i ¥ G   i3G H ¥ W ¡¨G %H  p i 4 A     4! [sent-85, score-0.29]
</p><p>40 A i A 4 f    4 i "   i P A¥   i eee h§¦¥ %4  i'  p h§¦ )Q%H¡6A4 ¥ §&2 hH   ¥ Qf  f ¡Q¥A gG h§¦ %4  i'p eee f gG 6R4 ¥ h§¦)hf f hH  h¦  i¥G H ¥ P ¥ 6Ap ¦ # ¥ C¥I £¥ ¥ hh1(G %H  iP A 4 i R   4 i A A G R H ¦ ! [sent-87, score-0.29]
</p><p>41 f A eee ¥   G ¥ i i f f i p p    ¦  f   G   ¥ f 'i  eee" W f ¥ S4P W¥  " hfH 0h§)46i¦ Q'i H C A    hh H SG 'Q¥H i P  6A%4hf (H QTSG ! [sent-88, score-0.145]
</p><p>42 ¥¥¦ p 'i $  i G i¥$ i &   ¡A i ¥ W R ¥ " i % ¥ H    4 h¢ eee R ¤ i A ¥ H A ¥ )f h¢ 4%¥  ¥ 4 ¥ If ¥ ChQi #¥ eee h§¦%4f ¥hG  Wh§! [sent-89, score-0.29]
</p><p>43 Return S  y7 5 86YAd  The algorithm starts with empty clusters and then cycles through all documents picking the pairs of documents that have the current highest joint visitation frequency as prompted by a bigram frequency (lines 1 and 2). [sent-92, score-0.538]
</p><p>44 If both documents in the selected pair are unassigned, a new cluster is allocated for them (lines 3 through 7). [sent-93, score-0.383]
</p><p>45 If one of the documents in the selected pair has been assigned to one of the previous clusters, the second document is assigned to the same cluster (lines 8 through 14). [sent-94, score-0.591]
</p><p>46 A  4  A  7 5 86  After the clustering, we can assume that if the user requests a document from the -th cluster , he is considerably more likely to prefer a next document from rather than , , i. [sent-96, score-0.824]
</p><p>47 This from assumption is reasonable because by construction clusters represent densely connected (in terms of trafﬁc) components, and the trafﬁc across the clusters is small compared to the trafﬁc within each cluster. [sent-99, score-0.24]
</p><p>48 In view of this observation, we broke individual user sessions down into subsessions, where each subsession consisted of documents belonging to the same cluster. [sent-100, score-0.558]
</p><p>49 The problem was thus reduced to a series of prediction problems for each cluster. [sent-101, score-0.051]
</p><p>50  "  C F     2 7 4 @  ( )%'" & d  D E  C 4       A B  4  7 5 5 2 @ @ 7 24 9 @  A  We studied the clusters by trying to ﬁnd out if the documents within a cluster are topically related. [sent-103, score-0.5]
</p><p>51 We ran code previously developed at NEC Labs [4] that uses information gain to ﬁnd the top features that distinguish each cluster from the rest. [sent-104, score-0.21]
</p><p>52 Table 1 shows the top features for some of the created clusters. [sent-105, score-0.05]
</p><p>53 The top features are quite consistent descriptors, suggesting that in one session a ResearchIndex user is typically interested in searching among topically-related documents. [sent-106, score-0.382]
</p><p>54 '" &  ( )%'" & G3G   y   gG¥G ¥ y § d P )%'" & d D(           In this paper, we model as a maxent distribution, where is the identity of the document that will be next requested by the user , given the history and the available for all other users. [sent-108, score-1.156]
</p><p>55 This choice of the maxent model is natural since our intuition is that all of the previously requested documents in the user session inﬂuence the identity of . [sent-109, score-1.288]
</p><p>56 It is also clear that we cannot afford to build a high-order model, because of the sparsity and high-dimensional data, so we need to restrict ourselves to models that can be reliably estimated from the low-order statistics. [sent-110, score-0.031]
</p><p>57 In order to introduce long term dependence of on the documents that occurred in the history of the session, we deﬁne a trigger as a pair of documents in a given cluster such that is substantially different from . [sent-112, score-0.676]
</p><p>58 To measure the quality of triggers and in order to rank them  y  G  )%'" & d (  P SC R¢ D     A  y  ¢ y )'%" d ( G& d ¥ ¢ Q     ( ! [sent-113, score-0.04]
</p><p>59 '" &  A      P     Table 2: Average number of hits and height of predictions across the clusters for different ranges of heights and using various models. [sent-114, score-0.402]
</p><p>60 The boxed numbers are the best values across all models. [sent-115, score-0.031]
</p><p>61 The set of features, together with maxent as an objective function, can be shown to lead to the following form of the conditional maxent model   6¥7 y Q¥ ! [sent-199, score-1.248]
</p><p>62 DP  y  A     d    § ¥  y Q¦ d ) P¥     Under fairly general assumptions, the maxent model can also be shown to be a maximum likelihood model [11]. [sent-209, score-0.624]
</p><p>63 Employing a Gaussian prior with a zero mean on parameters yields a maximum aposteriori solution that has been shown to be more accurate than the related maximum likelihood solution and other smoothing techniques for maxent models [2]. [sent-210, score-0.66]
</p><p>64 We use Gaussian smoothing in our experiments with a maxent model. [sent-211, score-0.66]
</p><p>65 The maxent model came in two ﬂavors: unsmoothed and smoothed with a Gaussian prior, with 0 mean and ﬁxed variance 2. [sent-234, score-0.65]
</p><p>66 We did not optimize the adjustable parameters of the models (such as the number of components for the mixture or the variance of the prior for maxent models) or the number of clusters (1000). [sent-235, score-0.741]
</p><p>67 We chronologically partitioned the log into roughly 8 million training requests (covering 82 days) and 2 million test requests (covering 17 days). [sent-236, score-0.35]
</p><p>68 We used the average height of predictions on the test data as a main evaluation criteria. [sent-237, score-0.215]
</p><p>69 The height of a prediction is are available from deﬁned as follows. [sent-238, score-0.181]
</p><p>70 Assuming that the probability estimates a model for a ﬁxed history and all possible values of , we ﬁrst sort them in the descending order of and then ﬁnd the distance in terms of the number of documents to the actually requested (which we know from the test data) from the top of this sorted list. [sent-239, score-0.386]
</p><p>71 The height tells us how deep into the list the user must go in order to see the document that actually interests him. [sent-240, score-0.574]
</p><p>72 The height of a perfect prediction is 0, the maximum (worst) height for a given cluster equals the number of documents in this cluster. [sent-241, score-0.694]
</p><p>73 Since heights greater than 20 are of little practical interest, we binned the heights of predictions for each cluster. [sent-242, score-0.144]
</p><p>74 Within each bin we also computed the average height of predictions. [sent-244, score-0.163]
</p><p>75 Thus, the best performing model would place most of the predictions inside the bin(s) with low value(s) of and within those bins the averages would be as low as possible. [sent-245, score-0.12]
</p><p>76 )e ¥    ¢ £        A  A  y QF   d ¡ ¥ ¡¡ 2 yt    Table 2 reports the average number of hits each model makes on average in each of the bins, as well as the average height of predictions within the bin. [sent-247, score-0.385]
</p><p>77 The smoothed maxent model has the best average height of predictions across the bins and scores roughly the same number of hits in each of the bins as the correlation method. [sent-248, score-1.088]
</p><p>78 The mixture of multinomials is quite close in quality to, but still not as good as, the maxent model with respect to both the number of hits and the height predictions in each of the bins. [sent-250, score-0.915]
</p><p>79 In Table 3, we present comparison of various models with respect to the average time taken and memory required to make a prediction. [sent-251, score-0.033]
</p><p>80 The table clearly illustrates that the maxent model (i. [sent-252, score-0.652]
</p><p>81 , the model-based approach) is substantially more time efﬁcient than the correlation (i. [sent-254, score-0.029]
</p><p>82 , the memory-based approach), even despite the fact that the model takes on average more memory. [sent-256, score-0.033]
</p><p>83 In particular, our maxent approach is roughly two orders of magnitude faster than the correlation. [sent-257, score-0.65]
</p><p>84 6 Conclusions and Future Work We have described a maxent approach to generating document recommendations in ResearchIndex. [sent-258, score-0.894]
</p><p>85 We addressed the problem of sparse, high-dimensional data by introducing a clustering of the documents based on the user navigation patterns. [sent-259, score-0.637]
</p><p>86 A particular advantage of our clustering is that by its deﬁnition the trafﬁc across the clusters is small compared to the trafﬁc within the cluster. [sent-260, score-0.243]
</p><p>87 This advantage allowed us to decompose the original prediction problem into a set of problems corresponding to the clusters. [sent-261, score-0.051]
</p><p>88 We also demonstrated that our clustering produces highly interpretable clusters: each cluster can be assigned a topical name based on the top-extracted features. [sent-262, score-0.282]
</p><p>89 We presented a number of models that can be used to solve a document prediction problem within cluster. [sent-263, score-0.23]
</p><p>90 We showed that the maxent model that combines zero and ﬁrst order Markov terms as well as the triggers with high information content provides the best average outof-sample performance. [sent-264, score-0.724]
</p><p>91 First, we plan to perform “live” testing of the clustering approach and various models in ResearchIndex. [sent-267, score-0.095]
</p><p>92 Secondly, our recent work [10] suggests that for difﬁcult prediction problems improvement beyond the plain maxent models can be sought by employing the mixtures of maxent models. [sent-268, score-1.299]
</p><p>93 We also plan to look at different clustering methods for documents (e. [sent-269, score-0.318]
</p><p>94 , based on the content or the link structure) and try to combine prediction results for different clusterings. [sent-271, score-0.078]
</p><p>95 Finally, one could think of a (quite involved) EM algorithm that performs the clustering of the documents in a manner that would make prediction within resulting clusters easier. [sent-273, score-0.486]
</p><p>96 Acknowledgements We would like to thank Steve Lawrence for making available the ResearchIndex log data, Eric Glover for running his naming code on our clusters, Kostas Tsioutsiouliklis and Darya Chudova for many useful discussions, and the anonymous reviewers for helpful suggestions. [sent-274, score-0.026]
</p><p>97 An open framework for practical testing of recommender systems using ResearchIindex. [sent-291, score-0.09]
</p><p>98 Dependency networks for density estimation, collaborative ﬁltering, and data visualization. [sent-310, score-0.131]
</p><p>99 A maximum entropy approach to collaborative ﬁltering in dynamic, sparse, high-dimensional domains. [sent-327, score-0.171]
</p><p>100 Probabilistic models for uniﬁed collaborative and content-based recommendation in sparse-data environments. [sent-349, score-0.254]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maxent', 0.624), ('researchindex', 0.25), ('user', 0.24), ('documents', 0.223), ('cluster', 0.16), ('document', 0.154), ('eee', 0.145), ('collaborative', 0.131), ('height', 0.13), ('recommendation', 0.123), ('recommendations', 0.116), ('requests', 0.116), ('requested', 0.109), ('traf', 0.104), ('clustering', 0.095), ('session', 0.092), ('clusters', 0.092), ('recommender', 0.09), ('hgg', 0.083), ('nec', 0.083), ('navigation', 0.079), ('pi', 0.077), ('browsing', 0.072), ('accesses', 0.072), ('bigrams', 0.072), ('pennock', 0.072), ('eg', 0.07), ('users', 0.069), ('items', 0.065), ('hciap', 0.062), ('neci', 0.062), ('pavlov', 0.062), ('sessions', 0.062), ('citation', 0.054), ('predictions', 0.052), ('prediction', 0.051), ('hits', 0.051), ('interests', 0.05), ('ltering', 0.049), ('heights', 0.046), ('hg', 0.045), ('lawrence', 0.044), ('bins', 0.043), ('dids', 0.042), ('glover', 0.042), ('hcs', 0.042), ('popescul', 0.042), ('qgg', 0.042), ('qqf', 0.042), ('uid', 0.042), ('uqcsap', 0.042), ('trigger', 0.041), ('entropy', 0.04), ('dp', 0.04), ('triggers', 0.04), ('hh', 0.037), ('discarding', 0.036), ('docs', 0.036), ('services', 0.036), ('smoothing', 0.036), ('id', 0.035), ('tr', 0.034), ('dynamic', 0.033), ('average', 0.033), ('multinomials', 0.033), ('broke', 0.033), ('days', 0.033), ('million', 0.033), ('seventeenth', 0.033), ('sparse', 0.031), ('across', 0.031), ('mouth', 0.031), ('pasadena', 0.031), ('sparsity', 0.031), ('proceedings', 0.03), ('hi', 0.03), ('correlation', 0.029), ('history', 0.029), ('table', 0.028), ('hs', 0.028), ('yt', 0.028), ('pietra', 0.028), ('content', 0.027), ('acm', 0.027), ('assigned', 0.027), ('markov', 0.027), ('heckerman', 0.026), ('competing', 0.026), ('smoothed', 0.026), ('roughly', 0.026), ('log', 0.026), ('ine', 0.025), ('qf', 0.025), ('context', 0.025), ('within', 0.025), ('features', 0.025), ('mixture', 0.025), ('top', 0.025), ('seconds', 0.024), ('visited', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="8-tfidf-1" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>2 0.15013053 <a title="8-tfidf-2" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>3 0.11667128 <a title="8-tfidf-3" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>4 0.11034642 <a title="8-tfidf-4" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>5 0.10828818 <a title="8-tfidf-5" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>6 0.1080265 <a title="8-tfidf-6" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>7 0.10282881 <a title="8-tfidf-7" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>8 0.093560033 <a title="8-tfidf-8" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>9 0.086572357 <a title="8-tfidf-9" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>10 0.083863825 <a title="8-tfidf-10" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>11 0.074359149 <a title="8-tfidf-11" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>12 0.07196857 <a title="8-tfidf-12" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>13 0.068159774 <a title="8-tfidf-13" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>14 0.068148911 <a title="8-tfidf-14" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>15 0.067735076 <a title="8-tfidf-15" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>16 0.062017057 <a title="8-tfidf-16" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>17 0.05727604 <a title="8-tfidf-17" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>18 0.056536689 <a title="8-tfidf-18" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>19 0.053580232 <a title="8-tfidf-19" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>20 0.053493492 <a title="8-tfidf-20" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.155), (1, -0.048), (2, 0.002), (3, 0.003), (4, -0.192), (5, 0.097), (6, -0.099), (7, -0.205), (8, -0.031), (9, 0.014), (10, -0.11), (11, -0.007), (12, -0.038), (13, 0.01), (14, 0.009), (15, -0.03), (16, 0.022), (17, -0.105), (18, 0.019), (19, 0.13), (20, 0.029), (21, 0.008), (22, 0.026), (23, 0.073), (24, 0.018), (25, 0.049), (26, 0.018), (27, 0.06), (28, -0.062), (29, -0.06), (30, -0.043), (31, 0.041), (32, 0.067), (33, 0.064), (34, 0.02), (35, 0.072), (36, -0.119), (37, -0.023), (38, 0.008), (39, 0.019), (40, 0.032), (41, 0.02), (42, 0.076), (43, -0.058), (44, -0.081), (45, -0.04), (46, 0.034), (47, 0.03), (48, -0.018), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94527161 <a title="8-lsi-1" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>2 0.76079792 <a title="8-lsi-2" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>3 0.71048617 <a title="8-lsi-3" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>4 0.60242522 <a title="8-lsi-4" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>5 0.58979201 <a title="8-lsi-5" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>6 0.57414091 <a title="8-lsi-6" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>7 0.56273943 <a title="8-lsi-7" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>8 0.52267921 <a title="8-lsi-8" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>9 0.47737363 <a title="8-lsi-9" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>10 0.46981764 <a title="8-lsi-10" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>11 0.45272085 <a title="8-lsi-11" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>12 0.42276087 <a title="8-lsi-12" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>13 0.39910617 <a title="8-lsi-13" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>14 0.38548312 <a title="8-lsi-14" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>15 0.37696961 <a title="8-lsi-15" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>16 0.37539425 <a title="8-lsi-16" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>17 0.33926484 <a title="8-lsi-17" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>18 0.33553255 <a title="8-lsi-18" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>19 0.32024086 <a title="8-lsi-19" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>20 0.31596303 <a title="8-lsi-20" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.025), (14, 0.025), (23, 0.03), (42, 0.085), (49, 0.319), (54, 0.084), (55, 0.036), (57, 0.018), (67, 0.019), (68, 0.029), (74, 0.098), (92, 0.018), (98, 0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80498385 <a title="8-lda-1" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>2 0.75667 <a title="8-lda-2" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>Author: Guoning Hu, Deliang Wang</p><p>Abstract: Monaural speech separation has been studied in previous systems that incorporate auditory scene analysis principles. A major problem for these systems is their inability to deal with speech in the highfrequency range. Psychoacoustic evidence suggests that different perceptual mechanisms are involved in handling resolved and unresolved harmonics. Motivated by this, we propose a model for monaural separation that deals with low-frequency and highfrequency signals differently. For resolved harmonics, our model generates segments based on temporal continuity and cross-channel correlation, and groups them according to periodicity. For unresolved harmonics, the model generates segments based on amplitude modulation (AM) in addition to temporal continuity and groups them according to AM repetition rates derived from sinusoidal modeling. Underlying the separation process is a pitch contour obtained according to psychoacoustic constraints. Our model is systematically evaluated, and it yields substantially better performance than previous systems, especially in the high-frequency range. 1 In t rod u ct i on In a natural environment, speech usually occurs simultaneously with acoustic interference. An effective system for attenuating acoustic interference would greatly facilitate many applications, including automatic speech recognition (ASR) and speaker identification. Blind source separation using independent component analysis [10] or sensor arrays for spatial filtering require multiple sensors. In many situations, such as telecommunication and audio retrieval, a monaural (one microphone) solution is required, in which intrinsic properties of speech or interference must be considered. Various algorithms have been proposed for monaural speech enhancement [14]. These methods assume certain properties of interference and have difficulty in dealing with general acoustic interference. Monaural separation has also been studied using phasebased decomposition [3] and statistical learning [17], but with only limited evaluation. While speech enhancement remains a challenge, the auditory system shows a remarkable capacity for monaural speech separation. According to Bregman [1], the auditory system separates the acoustic signal into streams, corresponding to different sources, based on auditory scene analysis (ASA) principles. Research in ASA has inspired considerable work to build computational auditory scene analysis (CASA) systems for sound separation [19] [4] [7] [18]. Such systems generally approach speech separation in two main stages: segmentation (analysis) and grouping (synthesis). In segmentation, the acoustic input is decomposed into sensory segments, each of which is likely to originate from a single source. In grouping, those segments that likely come from the same source are grouped together, based mostly on periodicity. In a recent CASA model by Wang and Brown [18], segments are formed on the basis of similarity between adjacent filter responses (cross-channel correlation) and temporal continuity, while grouping among segments is performed according to the global pitch extracted within each time frame. In most situations, the model is able to remove intrusions and recover low-frequency (below 1 kHz) energy of target speech. However, this model cannot handle high-frequency (above 1 kHz) signals well, and it loses much of target speech in the high-frequency range. In fact, the inability to deal with speech in the high-frequency range is a common problem for CASA systems. We study monaural speech separation with particular emphasis on the high-frequency problem in CASA. For voiced speech, we note that the auditory system can resolve the first few harmonics in the low-frequency range [16]. It has been suggested that different perceptual mechanisms are used to handle resolved and unresolved harmonics [2]. Consequently, our model employs different methods to segregate resolved and unresolved harmonics of target speech. More specifically, our model generates segments for resolved harmonics based on temporal continuity and cross-channel correlation, and these segments are grouped according to common periodicity. For unresolved harmonics, it is well known that the corresponding filter responses are strongly amplitude-modulated and the response envelopes fluctuate at the fundamental frequency (F0) of target speech [8]. Therefore, our model generates segments for unresolved harmonics based on common AM in addition to temporal continuity. The segments are grouped according to AM repetition rates. We calculate AM repetition rates via sinusoidal modeling, which is guided by target pitch estimated according to characteristics of natural speech. Section 2 describes the overall system. In section 3, systematic results and a comparison with the Wang-Brown system are given. Section 4 concludes the paper. 2 M od el d escri p t i on Our model is a multistage system, as shown in Fig. 1. Description for each stage is given below. 2.1 I n i t i a l p r oc e s s i n g First, an acoustic input is analyzed by a standard cochlear filtering model with a bank of 128 gammatone filters [15] and subsequent hair cell transduction [12]. This peripheral processing is done in time frames of 20 ms long with 10 ms overlap between consecutive frames. As a result, the input signal is decomposed into a group of timefrequency (T-F) units. Each T-F unit contains the response from a certain channel at a certain frame. The envelope of the response is obtained by a lowpass filter with Segregated Speech Mixture Peripheral and Initial Pitch mid-level segregation tracking processing Unit Final Resynthesis labeling segregation Figure 1. Schematic diagram of the proposed multistage system. passband [0, 1 kHz] and a Kaiser window of 18.25 ms. Mid-level processing is performed by computing a correlogram (autocorrelation function) of the individual responses and their envelopes. These autocorrelation functions reveal response periodicities as well as AM repetition rates. The global pitch is obtained from the summary correlogram. For clean speech, the autocorrelations generally have peaks consistent with the pitch and their summation shows a dominant peak corresponding to the pitch period. With acoustic interference, a global pitch may not be an accurate description of the target pitch, but it is reasonably close. Because a harmonic extends for a period of time and its frequency changes smoothly, target speech likely activates contiguous T-F units. This is an instance of the temporal continuity principle. In addition, since the passbands of adjacent channels overlap, a resolved harmonic usually activates adjacent channels, which leads to high crosschannel correlations. Hence, in initial segregation, the model first forms segments by merging T-F units based on temporal continuity and cross-channel correlation. Then the segments are grouped into a foreground stream and a background stream by comparing the periodicities of unit responses with global pitch. A similar process is described in [18]. Fig. 2(a) and Fig. 2(b) illustrate the segments and the foreground stream. The input is a mixture of a voiced utterance and a cocktail party noise (see Sect. 3). Since the intrusion is not strongly structured, most segments correspond to target speech. In addition, most segments are in the low-frequency range. The initial foreground stream successfully groups most of the major segments. 2.2 P i t c h tr a c k i n g In the presence of acoustic interference, the global pitch estimated in mid-level processing is generally not an accurate description of target pitch. To obtain accurate pitch information, target pitch is first estimated from the foreground stream. At each frame, the autocorrelation functions of T-F units in the foreground stream are summated. The pitch period is the lag corresponding to the maximum of the summation in the plausible pitch range: [2 ms, 12.5 ms]. Then we employ the following two constraints to check its reliability. First, an accurate pitch period at a frame should be consistent with the periodicity of the T-F units at this frame in the foreground stream. At frame j, let τ ( j) represent the estimated pitch period, and A(i, j,τ ) the autocorrelation function of uij, the unit in channel i. uij agrees with τ ( j) if A(i , j , τ ( j )) / A(i, j ,τ m ) > θ d (1) (a) (b) Frequency (Hz) 5000 5000 2335 2335 1028 1028 387 387 80 0 0.5 1 Time (Sec) 1.5 80 0 0.5 1 Time (Sec) 1.5 Figure 2. Results of initial segregation for a speech and cocktail-party mixture. (a) Segments formed. Each segment corresponds to a contiguous black region. (b) Foreground stream. Here, θd = 0.95, the same threshold used in [18], and τ m is the lag corresponding to the maximum of A(i, j,τ ) within [2 ms, 12.5 ms]. τ ( j) is considered reliable if more than half of the units in the foreground stream at frame j agree with it. Second, pitch periods in natural speech vary smoothly in time [11]. We stipulate the difference between reliable pitch periods at consecutive frames be smaller than 20% of the pitch period, justified from pitch statistics. Unreliable pitch periods are replaced by new values extrapolated from reliable pitch points using temporal continuity. As an example, suppose at two consecutive frames j and j+1 that τ ( j) is reliable while τ ( j+1) is not. All the channels corresponding to the T-F units agreeing with τ ( j) are selected. τ ( j+1) is then obtained from the summation of the autocorrelations for the units at frame j+1 in those selected channels. Then the re-estimated pitch is further verified with the second constraint. For more details, see [9]. Fig. 3 illustrates the estimated pitch periods from the speech and cocktail-party mixture, which match the pitch periods obtained from clean speech very well. 2.3 U n i t l a be l i n g With estimated pitch periods, (1) provides a criterion to label T-F units according to whether target speech dominates the unit responses or not. This criterion compares an estimated pitch period with the periodicity of the unit response. It is referred as the periodicity criterion. It works well for resolved harmonics, and is used to label the units of the segments generated in initial segregation. However, the periodicity criterion is not suitable for units responding to multiple harmonics because unit responses are amplitude-modulated. As shown in Fig. 4, for a filter response that is strongly amplitude-modulated (Fig. 4(a)), the target pitch corresponds to a local maximum, indicated by the vertical line, in the autocorrelation instead of the global maximum (Fig. 4(b)). Observe that for a filter responding to multiple harmonics of a harmonic source, the response envelope fluctuates at the rate of F0 [8]. Hence, we propose a new criterion for labeling the T-F units corresponding to unresolved harmonics by comparing AM repetition rates with estimated pitch. This criterion is referred as the AM criterion. To obtain an AM repetition rate, the entire response of a gammatone filter is half-wave rectified and then band-pass filtered to remove the DC component and other possible 14 Pitch Period (ms) 12 (a) 10 180 185 190 195 200 Time (ms) 2 4 6 8 Lag (ms) 205 210 8 6 4 0 (b) 0.5 1 Time (Sec) Figure 3. Estimated target pitch for the speech and cocktail-party mixture, marked by “x”. The solid line indicates the pitch contour obtained from clean speech. 0 10 12 Figure 4. AM effects. (a) Response of a filter with center frequency 2.6 kHz. (b) Corresponding autocorrelation. The vertical line marks the position corresponding to the pitch period of target speech. harmonics except for the F0 component. The rectified and filtered signal is then normalized by its envelope to remove the intensity fluctuations of the original signal, where the envelope is obtained via the Hilbert Transform. Because the pitch of natural speech does not change noticeably within a single frame, we model the corresponding normalized signal within a T-F unit by a single sinusoid to obtain the AM repetition rate. Specifically, f ,φ   f ij , φ ij = arg min M ˆ [r (i, jT − k ) − sin(2π k f / f S + φ )]2 , for f ∈[80 Hz, 500 Hz], (2) k =1 ˆ where a square error measure is used. r (i , t ) is the normalized filter response, fS is the sampling frequency, M spans a frame, and T= 10 ms is the progressing period from one frame to the next. In the above equation, fij gives the AM repetition rate for unit uij. Note that in the discrete case, a single sinusoid with a sufficiently high frequency can always match these samples perfectly. However, we are interested in finding a frequency within the plausible pitch range. Hence, the solution does not reduce to a degenerate case. With appropriately chosen initial values, this optimization problem can be solved effectively using iterative gradient descent (see [9]). The AM criterion is used to label T-F units that do not belong to any segments generated in initial segregation; such segments, as discussed earlier, tend to miss unresolved harmonics. Specifically, unit uij is labeled as target speech if the final square error is less than half of the total energy of the corresponding signal and the AM repetition rate is close to the estimated target pitch: | f ijτ ( j ) − 1 | < θ f . (3) Psychoacoustic evidence suggests that to separate sounds with overlapping spectra requires 6-12% difference in F0 [6]. Accordingly, we choose θf to be 0.12. 2.4 F i n a l s e gr e g a t i on a n d r e s y n t he s i s For adjacent channels responding to unresolved harmonics, although their responses may be quite different, they exhibit similar AM patterns and their response envelopes are highly correlated. Therefore, for T-F units labeled as target speech, segments are generated based on cross-channel envelope correlation in addition to temporal continuity. The spectra of target speech and intrusion often overlap and, as a result, some segments generated in initial segregation contain both units where target speech dominates and those where intrusion dominates. Given unit labels generated in the last stage, we further divide the segments in the foreground stream, SF, so that all the units in a segment have the same label. Then the streams are adjusted as follows. First, since segments for speech usually are at least 50 ms long, segments with the target label are retained in SF only if they are no shorter than 50 ms. Second, segments with the intrusion label are added to the background stream, SB, if they are no shorter than 50 ms. The remaining segments are removed from SF, becoming undecided. Finally, other units are grouped into the two streams by temporal and spectral continuity. First, SB expands iteratively to include undecided segments in its neighborhood. Then, all the remaining undecided segments are added back to SF. For individual units that do not belong to either stream, they are grouped into SF iteratively if the units are labeled as target speech as well as in the neighborhood of SF. The resulting SF is the final segregated stream of target speech. Fig. 5(a) shows the new segments generated in this process for the speech and cocktailparty mixture. Fig. 5(b) illustrates the segregated stream from the same mixture. Fig. 5(c) shows all the units where target speech is stronger than intrusion. The foreground stream generated by our algorithm contains most of the units where target speech is stronger. In addition, only a small number of units where intrusion is stronger are incorrectly grouped into it. A speech waveform is resynthesized from the final foreground stream. Here, the foreground stream works as a binary mask. It is used to retain the acoustic energy from the mixture that corresponds to 1’s and reject the mixture energy corresponding to 0’s. For more details, see [19]. 3 Evalu at i on an d comp ari son Our model is evaluated with a corpus of 100 mixtures composed of 10 voiced utterances mixed with 10 intrusions collected by Cooke [4]. The intrusions have a considerable variety. Specifically, they are: N0 - 1 kHz pure tone, N1 - white noise, N2 - noise bursts, N3 - “cocktail party” noise, N4 - rock music, N5 - siren, N6 - trill telephone, N7 - female speech, N8 - male speech, and N9 - female speech. Given our decomposition of an input signal into T-F units, we suggest the use of an ideal binary mask as the ground truth for target speech. The ideal binary mask is constructed as follows: a T-F unit is assigned one if the target energy in the corresponding unit is greater than the intrusion energy and zero otherwise. Theoretically speaking, an ideal binary mask gives a performance ceiling for all binary masks. Figure 5(c) illustrates the ideal mask for the speech and cocktail-party mixture. Ideal masks also suit well the situations where more than one target need to be segregated or the target changes dynamically. The use of ideal masks is supported by the auditory masking phenomenon: within a critical band, a weaker signal is masked by a stronger one [13]. In addition, an ideal mask gives excellent resynthesis for a variety of sounds and is similar to a prior mask used in a recent ASR study that yields excellent recognition performance [5]. The speech waveform resynthesized from the final foreground stream is used for evaluation, and it is denoted by S(t). The speech waveform resynthesized from the ideal binary mask is denoted by I(t). Furthermore, let e1(t) denote the signal present in I(t) but missing from S(t), and e2(t) the signal present in S(t) but missing from I(t). Then, the relative energy loss, REL, and the relative noise residue, RNR, are calculated as follows:     R EL = e12 (t ) t I 2 (t ) , S 2 (t ) . (4b) ¡ ¡ R NR = (4a) t 2 e 2 (t ) t t (a) (b) (c) Frequency (Hz) 5000 2355 1054 387 80 0 0.5 1 Time (Sec) 0 0.5 1 Time (Sec) 0 0.5 1 Time (Sec) Figure 5. Results of final segregation for the speech and cocktail-party mixture. (a) New segments formed in the final segregation. (b) Final foreground stream. (c) Units where target speech is stronger than the intrusion. Table 1: REL and RNR Proposed model Wang-Brown model REL (%) RNR (%) N0 2.12 0.02 N1 4.66 3.55 N2 1.38 1.30 N3 3.83 2.72 N4 4.00 2.27 N5 2.83 0.10 N6 1.61 0.30 N7 3.21 2.18 N8 1.82 1.48 N9 8.57 19.33 3.32 Average 3.40 REL (%) RNR (%) 6.99 0 28.96 1.61 5.77 0.71 21.92 1.92 10.22 1.41 7.47 0 5.99 0.48 8.61 4.23 7.27 0.48 15.81 33.03 11.91 4.39 15 SNR (dB) Intrusion 20 10 5 0 −5 N0 N1 N2 N3 N4 N5 N6 N7 N8 N9 Intrusion Type Figure 6. SNR results for segregated speech. White bars show the results from the proposed model, gray bars those from the Wang-Brown system, and black bars those of the mixtures. The results from our model are shown in Table 1. Each value represents the average of one intrusion with 10 voiced utterances. A further average across all intrusions is also shown in the table. On average, our system retains 96.60% of target speech energy, and the relative residual noise is kept at 3.32%. As a comparison, Table 1 also shows the results from the Wang-Brown model [18], whose performance is representative of current CASA systems. As shown in the table, our model reduces REL significantly. In addition, REL and RNR are balanced in our system. Finally, to compare waveforms directly we measure a form of signal-to-noise ratio (SNR) in decibels using the resynthesized signal from the ideal binary mask as ground truth: ( I (t ) − S (t )) 2 ] . I 2 (t )     SNR = 10 log10 [ t (5) t The SNR for each intrusion averaged across 10 target utterances is shown in Fig. 6, together with the results from the Wang-Brown system and the SNR of the original mixtures. Our model achieves an average SNR gain of around 12 dB and 5 dB improvement over the Wang-Brown model. 4 Di scu ssi on The main feature of our model lies in using different mechanisms to deal with resolved and unresolved harmonics. As a result, our model is able to recover target speech and reduce noise interference in the high-frequency range where harmonics of target speech are unresolved. The proposed system considers the pitch contour of the target source only. However, it is possible to track the pitch contour of the intrusion if it has a harmonic structure. With two pitch contours, one could label a T-F unit more accurately by comparing whether its periodicity is more consistent with one or the other. Such a method is expected to lead to better performance for the two-speaker situation, e.g. N7 through N9. As indicated in Fig. 6, the performance gain of our system for such intrusions is relatively limited. Our model is limited to separation of voiced speech. In our view, unvoiced speech poses the biggest challenge for monaural speech separation. Other grouping cues, such as onset, offset, and timbre, have been demonstrated to be effective for human ASA [1], and may play a role in grouping unvoiced speech. In addition, one should consider the acoustic and phonetic characteristics of individual unvoiced consonants. We plan to investigate these issues in future study. A c k n ow l e d g me n t s We thank G. J. Brown and M. Wu for helpful comments. Preliminary versions of this work were presented in 2001 IEEE WASPAA and 2002 IEEE ICASSP. This research was supported in part by an NSF grant (IIS-0081058) and an AFOSR grant (F4962001-1-0027). References [1] A. S. Bregman, Auditory scene analysis, Cambridge MA: MIT Press, 1990. [2] R. P. Carlyon and T. M. Shackleton, “Comparing the fundamental frequencies of resolved and unresolved harmonics: evidence for two pitch mechanisms?” J. Acoust. Soc. Am., Vol. 95, pp. 3541-3554, 1994. [3] G. Cauwenberghs, “Monaural separation of independent acoustical components,” In Proc. of IEEE Symp. Circuit & Systems, 1999. [4] M. Cooke, Modeling auditory processing and organization, Cambridge U.K.: Cambridge University Press, 1993. [5] M. Cooke, P. Green, L. Josifovski, and A. Vizinho, “Robust automatic speech recognition with missing and unreliable acoustic data,” Speech Comm., Vol. 34, pp. 267-285, 2001. [6] C. J. Darwin and R. P. Carlyon, “Auditory grouping,” in Hearing, B. C. J. Moore, Ed., San Diego CA: Academic Press, 1995. [7] D. P. W. Ellis, Prediction-driven computational auditory scene analysis, Ph.D. Dissertation, MIT Department of Electrical Engineering and Computer Science, 1996. [8] H. Helmholtz, On the sensations of tone, Braunschweig: Vieweg & Son, 1863. (A. J. Ellis, English Trans., Dover, 1954.) [9] G. Hu and D. L. Wang, “Monaural speech segregation based on pitch tracking and amplitude modulation,” Technical Report TR6, Ohio State University Department of Computer and Information Science, 2002. (available at www.cis.ohio-state.edu/~hu) [10] A. Hyvärinen, J. Karhunen, and E. Oja, Independent component analysis, New York: Wiley, 2001. [11] W. J. M. Levelt, Speaking: From intention to articulation, Cambridge MA: MIT Press, 1989. [12] R. Meddis, “Simulation of auditory-neural transduction: further studies,” J. Acoust. Soc. Am., Vol. 83, pp. 1056-1063, 1988. [13] B. C. J. Moore, An Introduction to the psychology of hearing, 4th Ed., San Diego CA: Academic Press, 1997. [14] D. O’Shaughnessy, Speech communications: human and machine, 2nd Ed., New York: IEEE Press, 2000. [15] R. D. Patterson, I. Nimmo-Smith, J. Holdsworth, and P. Rice, “An efficient auditory filterbank based on the gammatone function,” APU Report 2341, MRC, Applied Psychology Unit, Cambridge U.K., 1988. [16] R. Plomp and A. M. Mimpen, “The ear as a frequency analyzer II,” J. Acoust. Soc. Am., Vol. 43, pp. 764-767, 1968. [17] S. Roweis, “One microphone source separation,” In Advances in Neural Information Processing Systems 13 (NIPS’00), 2001. [18] D. L. Wang and G. J. Brown, “Separation of speech from interfering sounds based on oscillatory correlation,” IEEE Trans. Neural Networks, Vol. 10, pp. 684-697, 1999. [19] M. Weintraub, A theory and computational model of auditory monaural sound separation, Ph.D. Dissertation, Stanford University Department of Electrical Engineering, 1985.</p><p>3 0.51843846 <a title="8-lda-3" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>4 0.51693344 <a title="8-lda-4" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>Author: Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We ﬁnd that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.</p><p>5 0.51394784 <a title="8-lda-5" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>Author: Rong Jin, Zoubin Ghahramani</p><p>Abstract: In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naIve methods degrade rapidly as more ambiguity is introduced into the labels. 1</p><p>6 0.51285064 <a title="8-lda-6" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>7 0.51106322 <a title="8-lda-7" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>8 0.51042837 <a title="8-lda-8" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>9 0.51035172 <a title="8-lda-9" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>10 0.50911897 <a title="8-lda-10" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>11 0.50783801 <a title="8-lda-11" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>12 0.50677013 <a title="8-lda-12" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>13 0.50597256 <a title="8-lda-13" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>14 0.50553489 <a title="8-lda-14" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>15 0.50524867 <a title="8-lda-15" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>16 0.50492644 <a title="8-lda-16" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>17 0.50472116 <a title="8-lda-17" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>18 0.50438511 <a title="8-lda-18" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>19 0.50435442 <a title="8-lda-19" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>20 0.50371397 <a title="8-lda-20" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
