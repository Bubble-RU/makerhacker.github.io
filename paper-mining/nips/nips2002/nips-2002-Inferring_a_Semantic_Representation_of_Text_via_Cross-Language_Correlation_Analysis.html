<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-112" href="#">nips2002-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</h1>
<br/><p>Source: <a title="nips-2002-112-pdf" href="http://papers.nips.cc/paper/2324-inferring-a-semantic-representation-of-text-via-cross-language-correlation-analysis.pdf">pdf</a></p><p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>Reference: <a title="nips-2002-112-reference" href="../nips2002_reference/nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 net  Abstract The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. [sent-10, score-1.278]
</p><p>2 This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. [sent-11, score-0.05]
</p><p>3 By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. [sent-12, score-0.289]
</p><p>4 The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. [sent-13, score-0.198]
</p><p>5 Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. [sent-15, score-0.584]
</p><p>6 Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. [sent-16, score-0.215]
</p><p>7 Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data. [sent-18, score-0.254]
</p><p>8 1 Introduction Most text retrieval or categorization methods depend on exact matches between words. [sent-19, score-0.319]
</p><p>9 Such methods will, however, fail to recognize relevant documents that do not share words with a users’ queries. [sent-20, score-0.378]
</p><p>10 One reason for this is that the standard representation models (e. [sent-21, score-0.05]
</p><p>11 boolean, standard vector, probabilistic) treat words as if they are independent, although it is clear that they are not. [sent-23, score-0.092]
</p><p>12 A central problem in this ﬁeld is to automatically model term-  term semantic interrelationships, in a way to improve retrieval, and possibly to do so in an unsupervised way or with a minimal amount of supervision. [sent-24, score-0.261]
</p><p>13 For example latent semantic indexing (LSI) has been used to extract information about co-occurrence of terms in the same documents, an indicator of semantic relations, and this is achieved by singular value decomposition (SVD) of the term-document matrix. [sent-25, score-0.557]
</p><p>14 The LSI method has also been adapted to deal with the important problem of cross-language retrieval, where a query in a language is used to retrieve documents in a different language. [sent-26, score-0.53]
</p><p>15 In this framework, a common vector-space, including words from both languages, is created and then the training set is analysed in this space using SVD. [sent-28, score-0.136]
</p><p>16 More generally, many other statistical and linear algebra methods have been used to obtain an improved semantic representation of text data over LSI [6]. [sent-30, score-0.426]
</p><p>17 In this study we address the problem of learning a semantic representation of text from a paired bilingual corpus, a problem that is important both for mono-lingual and cross-lingual applications. [sent-31, score-0.623]
</p><p>18 This problem can be regarded either as an unsupervised problem with paired documents, or as a supervised monolingual problem with very complex labels (i. [sent-32, score-0.153]
</p><p>19 the label of an english document could be its french counterpart). [sent-34, score-0.696]
</p><p>20 In either way, the data can be readily obtained without an explicit labeling effort, and furthermore there is not the loss of information due to compressing the meaning of a document into a discrete label. [sent-35, score-0.167]
</p><p>21 We employ kernel Canonical Correlation Analysis (KCCA) [1] to learn a representation of text that captures aspects of its meaning. [sent-36, score-0.235]
</p><p>22 Given a paired bilingual corpus, this method deﬁnes two embedding spaces for the documents of the corpus, one for each language, and an obvious one-to-one correspondence between points in the two spaces. [sent-37, score-0.525]
</p><p>23 KCCA then ﬁnds projections in the two embedding spaces for which the resulting projected values are highly correlated. [sent-38, score-0.089]
</p><p>24 In other words, it looks for particular combinations of words that appear to have the same co-occurrence patterns in the two languages. [sent-39, score-0.092]
</p><p>25 Our hypothesis is that ﬁnding such correlations across a paired crosslingual corpus will locate the underlying semantics, since we assume that the two languages are ’conditionally independent’, or that the only thing they have in common is their meaning. [sent-40, score-0.524]
</p><p>26 The directions would carry information about the concepts that stood behind the process of generation of the text and, although expressed differently in different languages, are, nevertheless, semantically equivalent. [sent-41, score-0.115]
</p><p>27 pension plan cpp canadians beneﬁts retirement fund tax investment income ﬁnance young years rate superannuation disability taxes mounted future premiums seniors country rates jobs pay  AGRICULTURE? [sent-43, score-0.076]
</p><p>28 bl commission agriculteurs producteurs canadienne grain parti conseil commercialisat neuve ministre administration modiﬁcation qubec terre formistes partis grains op nationale lus bloc nations chambre administration  FISHING INDUSTRY? [sent-45, score-0.387]
</p><p>29 Such directions are then used to calculate the coordinates of the  documents in a ’language independent’ way. [sent-47, score-0.286]
</p><p>30 We show that the correlations we ﬁnd are not the effect of chance, and that the resulting representation signiﬁcantly improves performance of retrieval systems. [sent-49, score-0.34]
</p><p>31 We ﬁnd that the correlation existing between certain sets of words in English and French documents cannot be explained as a random correlation. [sent-50, score-0.474]
</p><p>32 Hence we need to explain it by means of relations between the generative processes of the two versions of the documents, that we assume to be conditionally independent given the topic or content. [sent-51, score-0.068]
</p><p>33 Under such assumptions, hence, such correlations detect similarities in content between the two documents, and can be exploited to derive a semantic representation of the text. [sent-52, score-0.428]
</p><p>34 This representation is then used for retrieval tasks, providing better performance than existing techniques. [sent-53, score-0.288]
</p><p>35 We ﬁrst apply the method to crosslingual information retrieval, comparing performance with a related approach based on latent semantic indexing (LSI) described below [5]. [sent-54, score-0.305]
</p><p>36 Secondly, we treat the second language as a complex label for the ﬁrst language document and view the projection obtained by CL-KCCA as a semantic map for use in a multilingual classiﬁcation task with very encouraging results. [sent-55, score-0.689]
</p><p>37 From the computational point of view, we detect such correlations by solving an eigenproblem, that is avoiding problems like local minima, and we do so by using kernels. [sent-56, score-0.086]
</p><p>38 The KCCA machinery will be given in Section 3 and in Section 4 we will show how to apply KCCA to cross-lingual retrieval while Section 4 describes the monolingual applications. [sent-57, score-0.248]
</p><p>39 2 Previous work The use of LSI for cross-language retrieval was proposed by [5]. [sent-59, score-0.204]
</p><p>40 An initial sample of documents is translated by human or, perhaps, by machine, to create a set of dual-language training documents and . [sent-61, score-0.572]
</p><p>41 After preprocessing documents a common vector-space, including words from both languages, is created and then the training set is analysed in this space using SVD:  © §£ ¥ £ " &%$#¢       © §£ ¥ £ ¡ ¨¦¤¢    ! [sent-62, score-0.422]
</p><p>42 3  (1)  0 1     (  )'   where the -th column of corresponds to document with its ﬁrst set of coordinates giving the ﬁrst language features and the second set the second language features. [sent-64, score-0.428]
</p><p>43 To translate a new document (query) to a language-independent representation one projects (folds-in) its expanded (ﬁlled up with zero components related to another language) vector representation into the space spanned by the ﬁrst eigenvectors : . [sent-65, score-0.236]
</p><p>44 The similarity between two documents is measured as the inner product between their projections. [sent-66, score-0.32]
</p><p>45 The documents that are the most similar to the query are considered to be relevant. [sent-67, score-0.384]
</p><p>46 Suppose as for cross-lingual LSI (CL-LSI) we are given aligned texts in, for simplicity, two languages, i. [sent-69, score-0.053]
</p><p>47 , every text in one language is a translation of text in another language or vice versa. [sent-71, score-0.522]
</p><p>48 Our hypothesis is that having the corpus mapped to a highdimensional feature space as and corpus to as (with and being respectively the kernels of the two mappings, i. [sent-72, score-0.37]
</p><p>49 matrices of the inner products between images of all the data points, [2]), we can learn (semantic) directions and in those spaces so that the projections and of input data images from the different languages would be maximally correlated. [sent-74, score-0.223]
</p><p>50 The  ¡   F HF A  H¥HF 7  GF E D FF F F © § ¦ ¨6 e e A   7 7  (4)  e  e  Once we have moved to a kernel deﬁned feature space the extra ﬂexibility introduced means that there is a danger of overﬁtting. [sent-80, score-0.07]
</p><p>51 By this we mean that we can ﬁnd spurious correlations by using large weight vectors to project the data so that the two projections are completely aligned. [sent-81, score-0.173]
</p><p>52 It is now possible to ﬁnd perfect correlations between the two representations. [sent-83, score-0.086]
</p><p>53 Using kernel functions will frequently result in linear independence of the training set, for example, when using Gaussian kernels. [sent-84, score-0.07]
</p><p>54 Similar operations for yield analogous equations that together with (8) can be written in a matrix form: (9)  where is the average per point correlation between projections , and  and  :  (10)  Canadian Parliament proceedings corpus. [sent-88, score-0.109]
</p><p>55 Equation (9) is known as a generalised eigenvalue problem. [sent-90, score-0.066]
</p><p>56 The standard approach to the solution of (9) in the case of a symmetric is to perform incomplete Cholesky decomposition of the matrix : and deﬁne which allows us, after simple transformations, to rewrite it as a standard eigenvalue problem . [sent-91, score-0.07]
</p><p>57 Thus, the spectrum of the problem (9) has paired positive and negative values between and . [sent-94, score-0.178]
</p><p>58 V     V W  4 Applications of KCCA Cross-linguistic retrieval with KCCA. [sent-95, score-0.204]
</p><p>59 The kernel CCA procedure identiﬁes a set of projections from both languages into a common semantic space. [sent-96, score-0.478]
</p><p>60 We ﬁrst select a number of semantic dimensions, , with largest correlation values . [sent-98, score-0.323]
</p><p>61 Here we assumed that is simply where is the training corpus in the given language: or . [sent-100, score-0.185]
</p><p>62  B aY ` ¡  `  aq`  `¡ Y  Using the semantic space in text categorisation. [sent-103, score-0.376]
</p><p>63 The semantic vectors in the given language can be exported and used in some other application, for example, Support Vector Machine classiﬁcation. [sent-104, score-0.407]
</p><p>64 Following [5] we conducted a series of experiments with the Hansard collection [3] to measure the ability of CL-LSI and CL-KCCA for any document from a test collection in one language to ﬁnd its mate in another language. [sent-107, score-0.448]
</p><p>65 3 million pairs of aligned text chunks (sentences or smaller fragments) from the 36 Canadian Parliament proceedings. [sent-109, score-0.226]
</p><p>66 As a testing collection we used only ’testing 1’. [sent-111, score-0.05]
</p><p>67 The raw text was split into sentences with Adwait Ratnaparkhi’s MXTERMINATOR and the sentences were aligned with I. [sent-112, score-0.282]
</p><p>68 After removing stop-words in both French and English parts and rare words (i. [sent-115, score-0.092]
</p><p>69 appearing less than three times) we obtained term-by-document ’English’ matrix and ’French’ matrix (we also removed a few documents that appeared to be problematic when split into paragraphs). [sent-117, score-0.33]
</p><p>70 As these matrices were still too large to perform SVD and KCCA on them, we split the whole collection into 14 chunks of about 910 documents each and conducted experiments separately with them, measuring the performance of the methods each time on a 917-document test collection. [sent-118, score-0.438]
</p><p>71 We have also trained the CL-KCCA method on randomly reassociated French-English document pairs and observed accuracy of about on test data which is far lower than results on the non-random original data. [sent-120, score-0.171]
</p><p>72 The Matlab implementation of KCCA using the same function, ’svd’, which solves the generalised eigenvalue problem through Cholesky incomplete decomposition, took about 8 minutes to compute on the same data. [sent-125, score-0.066]
</p><p>73 Only one - mate document in French was considered as relevant to each of the test English documents which were treated as queries and the relative number of correctly retrieved documents was computed (Table 2) , , , . [sent-128, score-0.892]
</p><p>74 Very similar results along with average precision over ﬁxed recalls: (omitted here) were obtained when French documents were treated as queries and English as test documents. [sent-129, score-0.342]
</p><p>75 As one can see from Table 2 CL-KCCA seems to capture most of the semantics in the ﬁrst few components achieving accuracy with as little as 100 components when CL-LSI needs all components for a similar ﬁgure. [sent-130, score-0.113]
</p><p>76 ¡  $ y '$%$$ t $ y V $ y $  ¡  P  Selecting the regularization parameter. [sent-131, score-0.052]
</p><p>77 The regularization parameter (6) not only makes the problem (9) well-posed numerically, but also provides control over capacity of the function space where the solution is being sought. [sent-132, score-0.052]
</p><p>78 The ”random” data is constructed by random reassociations of the data pairs, for example, denotes English-French parallel corpus which is obtained from the original English-French aligned collection by reshufﬂing the French (equivalently, English) documents. [sent-136, score-0.288]
</p><p>79 Suppose, denotes the (positive part of) spectrum of the KCCA solution on the paired dataset . [sent-137, score-0.178]
</p><p>80 If the method is overﬁtting the data it will be able to ﬁnd perfect correlations and hence where is the all-one vec-  P  c ic  0 (' &` )"§ & 9 &` %  4  9  y  5 F 6HFYc  1 1 i9  1  `   2  ¤¤  e  W  4 GFF  c  1  `  9   `  c 1 i9    2  ¤¤  e    2 31  1. [sent-138, score-0.121]
</p><p>81 5  4  FF HYc  1  Figure 1: Quantities (left), (middle) and (right) as functions of the regularization parameter . [sent-143, score-0.052]
</p><p>82 (Graphs were obtained for the regularization schema discussed in [1]). [sent-144, score-0.052]
</p><p>83 Three graphs in Figure 1 show the quantities , , and as functions of the regularization parameter . [sent-147, score-0.052]
</p><p>84 For small values of the spectrum of all the tests is close to the all-one spectrum (the spectrum ). [sent-148, score-0.207]
</p><p>85 This indicates overﬁtting since the method is able to ﬁnd correlations even in randomly associated pairs. [sent-149, score-0.086]
</p><p>86 As increases the spectrum of the randomly associated data becomes far from all-one, while that of the paired documents remains correlated. [sent-150, score-0.464]
</p><p>87 FF Gfc  0 9 % `P 2  ¤¤  e  4 GFF  W  FF c GfWc  % ` )#§ & 9 &` 2 (' %  ¤¤  e  W  4 HFF  FF c HYic  0 ' &` ( #§ & 9 % ` 2 c  P  P ¤ ¤  %9 %` 2  e  4 HFF  W  ¤¤  e  P  t  V    $V  Pseudo query test. [sent-154, score-0.098]
</p><p>88 To perform a more realistic test we generated short queries, which are most likely to occur in search engines, that consisted of the 5 most probable words from each test document. [sent-155, score-0.092]
</p><p>89 The relevant documents were the test documents themselves in monolinguistic retrieval (English query - English document) and their mates in the crosslinguistic (English query - French document) test. [sent-156, score-1.016]
</p><p>90 Table 3 shows the relative number of correctly retrieved as top-ranked English documents for English queries (left) and the relative number of correctly retrieved documents in the top ten ranked (right). [sent-157, score-0.752]
</p><p>91 The semantics (300 vectors) extracted from the Canadian Parliament corpus (Hansard) was used in Support Vector Machine (SVM) text classiﬁcation [2] of Reuters-21578 corpus (Table 5). [sent-160, score-0.563]
</p><p>92 In this experimental setting the intersection of vector spaces of the Hansards,  5159 English words from the ﬁrst 1000-French-English-document training chunk, and Reuters ModApt split, 9962 words from the 9602 training and 3299 test documents had 1473 words. [sent-161, score-0.512]
</p><p>93 The extracted KCCA vectors from English and French parts (raw ’KCCA’ of Table 5) and 300 eigenvectors from the same data (raw ’CL-LSI’) were used in the SVM [4] with the kernel (11) to classify the Reuters-21578 data. [sent-162, score-0.07]
</p><p>94 The experiments were averaged over 10 runs with 5% each time randomly chosen fraction of training data as the difference between bag-of-words and semantic methods is more contrasting on smaller samples. [sent-163, score-0.261]
</p><p>95   y 'y        ¡   £  5    Table 5: value, %, averaged over 10 subsequent runs of SVM classiﬁer with original Reuters-21578 data (’bag-of-words’) and preprocessed using semantics (300 vectors) extracted from the Canadian Parliament corpus by various methods. [sent-166, score-0.263]
</p><p>96 Our main ﬁndings are that: the correlation existing between certain sets of words in english and french documents cannot be explained as random correlations. [sent-168, score-1.034]
</p><p>97 Hence we need to explain it by means of relations between the generative processes of the two versions of the documents. [sent-169, score-0.068]
</p><p>98 The correlations detect similarities in content between the two documents, and can be exploited to derive a semantic representation of the text. [sent-170, score-0.428]
</p><p>99 The representation is then used for retrieval tasks, providing better performance than existing techniques. [sent-171, score-0.288]
</p><p>100 A probabilistic framework for the hierarchic organisation and classiﬁcation of document collections. [sent-205, score-0.136]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('english', 0.313), ('documents', 0.286), ('kcca', 0.265), ('semantic', 0.261), ('french', 0.247), ('ay', 0.231), ('retrieval', 0.204), ('corpus', 0.185), ('hf', 0.165), ('ff', 0.154), ('language', 0.146), ('lsi', 0.139), ('document', 0.136), ('parliament', 0.132), ('text', 0.115), ('paired', 0.109), ('canadian', 0.103), ('languages', 0.1), ('query', 0.098), ('words', 0.092), ('bilingual', 0.088), ('gff', 0.088), ('correlations', 0.086), ('svd', 0.081), ('semantics', 0.078), ('canonical', 0.071), ('gf', 0.071), ('kernel', 0.07), ('spectrum', 0.069), ('gfc', 0.066), ('hansards', 0.066), ('hff', 0.066), ('mate', 0.066), ('ministre', 0.066), ('nations', 0.066), ('paragraphs', 0.066), ('correlation', 0.062), ('retrieved', 0.062), ('chunks', 0.058), ('alexei', 0.058), ('grain', 0.058), ('queries', 0.056), ('table', 0.056), ('aligned', 0.053), ('regularization', 0.052), ('collection', 0.05), ('representation', 0.05), ('wc', 0.049), ('projections', 0.047), ('analysed', 0.044), ('atlantique', 0.044), ('ches', 0.044), ('cheurs', 0.044), ('crosslingual', 0.044), ('debates', 0.044), ('minister', 0.044), ('monolingual', 0.044), ('monolinguistic', 0.044), ('neuve', 0.044), ('newfoundland', 0.044), ('ouest', 0.044), ('pension', 0.044), ('pensions', 0.044), ('pls', 0.044), ('stocks', 0.044), ('terre', 0.044), ('yukon', 0.044), ('split', 0.044), ('nello', 0.042), ('spaces', 0.042), ('spurious', 0.04), ('hansard', 0.038), ('recalls', 0.038), ('industry', 0.038), ('administration', 0.038), ('fhf', 0.038), ('ans', 0.038), ('atlantic', 0.038), ('board', 0.038), ('wheat', 0.038), ('cp', 0.036), ('svm', 0.036), ('sentences', 0.035), ('ic', 0.035), ('decomposition', 0.035), ('eigenvalue', 0.035), ('relations', 0.035), ('accuracy', 0.035), ('existing', 0.034), ('inner', 0.034), ('house', 0.033), ('vinokourov', 0.033), ('commission', 0.033), ('versions', 0.033), ('plan', 0.032), ('exploited', 0.031), ('meaning', 0.031), ('cl', 0.031), ('generalised', 0.031), ('cholesky', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="112-tfidf-1" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>2 0.30703244 <a title="112-tfidf-2" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>Author: Jaz Kandola, Nello Cristianini, John S. Shawe-taylor</p><p>Abstract: The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such similarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain positive results. 1</p><p>3 0.29350895 <a title="112-tfidf-3" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>4 0.20169136 <a title="112-tfidf-4" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers</p><p>Abstract: We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 1</p><p>5 0.19481613 <a title="112-tfidf-5" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>6 0.14309068 <a title="112-tfidf-6" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>7 0.1366125 <a title="112-tfidf-7" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>8 0.11168272 <a title="112-tfidf-8" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>9 0.11034642 <a title="112-tfidf-9" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>10 0.10369771 <a title="112-tfidf-10" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>11 0.10119895 <a title="112-tfidf-11" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>12 0.085658394 <a title="112-tfidf-12" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>13 0.079917267 <a title="112-tfidf-13" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>14 0.078295745 <a title="112-tfidf-14" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>15 0.07538528 <a title="112-tfidf-15" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>16 0.074506283 <a title="112-tfidf-16" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>17 0.073616534 <a title="112-tfidf-17" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>18 0.071471691 <a title="112-tfidf-18" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>19 0.066866241 <a title="112-tfidf-19" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>20 0.065779001 <a title="112-tfidf-20" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.201), (1, -0.101), (2, 0.074), (3, -0.061), (4, -0.334), (5, 0.086), (6, -0.022), (7, -0.212), (8, 0.098), (9, -0.207), (10, -0.339), (11, -0.074), (12, 0.032), (13, -0.1), (14, 0.088), (15, 0.076), (16, -0.008), (17, -0.018), (18, -0.002), (19, 0.019), (20, -0.029), (21, 0.105), (22, 0.033), (23, -0.005), (24, -0.077), (25, 0.047), (26, 0.039), (27, 0.048), (28, 0.066), (29, 0.021), (30, -0.007), (31, -0.011), (32, -0.024), (33, -0.028), (34, 0.021), (35, 0.01), (36, -0.016), (37, -0.002), (38, -0.014), (39, -0.013), (40, 0.017), (41, -0.081), (42, 0.03), (43, -0.009), (44, 0.002), (45, -0.005), (46, -0.004), (47, -0.055), (48, 0.071), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96381032 <a title="112-lsi-1" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>2 0.83430815 <a title="112-lsi-2" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>3 0.82883453 <a title="112-lsi-3" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers</p><p>Abstract: We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 1</p><p>4 0.69444817 <a title="112-lsi-4" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>Author: Jaz Kandola, Nello Cristianini, John S. Shawe-taylor</p><p>Abstract: The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such similarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain positive results. 1</p><p>5 0.65864789 <a title="112-lsi-5" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><p>6 0.65723372 <a title="112-lsi-6" href="./nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">1 nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>7 0.5553925 <a title="112-lsi-7" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>8 0.47983778 <a title="112-lsi-8" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>9 0.4785825 <a title="112-lsi-9" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>10 0.41366467 <a title="112-lsi-10" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>11 0.40017974 <a title="112-lsi-11" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>12 0.38485345 <a title="112-lsi-12" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>13 0.35804266 <a title="112-lsi-13" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>14 0.35533839 <a title="112-lsi-14" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>15 0.35253266 <a title="112-lsi-15" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>16 0.29172754 <a title="112-lsi-16" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>17 0.28231773 <a title="112-lsi-17" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>18 0.2750755 <a title="112-lsi-18" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>19 0.27234614 <a title="112-lsi-19" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>20 0.26669908 <a title="112-lsi-20" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.059), (14, 0.021), (23, 0.015), (42, 0.101), (54, 0.134), (55, 0.031), (57, 0.01), (66, 0.321), (67, 0.022), (68, 0.025), (74, 0.084), (92, 0.022), (98, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80101568 <a title="112-lda-1" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>2 0.53576142 <a title="112-lda-2" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>Author: Cody Kwok, Dieter Fox, Marina Meila</p><p>Abstract: Particle ﬁlters estimate the state of dynamical systems from sensor information. In many real time applications of particle ﬁlters, however, sensor information arrives at a signiﬁcantly higher rate than the update rate of the ﬁlter. The prevalent approach to dealing with such situations is to update the particle ﬁlter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle ﬁlters, which make use of all sensor information even when the ﬁlter update rate is below the update rate of the sensors. This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a ﬁlter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.</p><p>3 0.53488576 <a title="112-lda-3" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>4 0.53419197 <a title="112-lda-4" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>Author: Jaz Kandola, Nello Cristianini, John S. Shawe-taylor</p><p>Abstract: The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such similarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain positive results. 1</p><p>5 0.53361177 <a title="112-lda-5" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Bernhard SchĂślkopf</p><p>Abstract: We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. 1</p><p>6 0.53054845 <a title="112-lda-6" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>7 0.5274055 <a title="112-lda-7" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>8 0.52478123 <a title="112-lda-8" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>9 0.52324045 <a title="112-lda-9" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>10 0.52157068 <a title="112-lda-10" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>11 0.52149749 <a title="112-lda-11" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>12 0.519337 <a title="112-lda-12" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>13 0.51899689 <a title="112-lda-13" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>14 0.51767159 <a title="112-lda-14" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>15 0.51751685 <a title="112-lda-15" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>16 0.51647091 <a title="112-lda-16" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>17 0.51625097 <a title="112-lda-17" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>18 0.51539075 <a title="112-lda-18" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>19 0.51440787 <a title="112-lda-19" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>20 0.51401079 <a title="112-lda-20" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
