<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-118" href="#">nips2002-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</h1>
<br/><p>Source: <a title="nips-2002-118-pdf" href="http://papers.nips.cc/paper/2209-kernel-based-extraction-of-slow-features-complex-cells-learn-disparity-and-translation-invariance-from-natural-images.pdf">pdf</a></p><p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>Reference: <a title="nips-2002-118-reference" href="../nips2002_reference/nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. [sent-4, score-0.127]
</p><p>2 This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. [sent-5, score-0.456]
</p><p>3 Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. [sent-6, score-1.957]
</p><p>4 The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. [sent-7, score-0.788]
</p><p>5 On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. [sent-9, score-0.341]
</p><p>6 Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. [sent-10, score-0.905]
</p><p>7 Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. [sent-11, score-0.175]
</p><p>8 That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www. [sent-12, score-0.586]
</p><p>9 fr/equipes/cortex/  space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. [sent-14, score-0.214]
</p><p>10 However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. [sent-15, score-0.49]
</p><p>11 In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. [sent-17, score-0.303]
</p><p>12 The use of the "kernel trick" allows projection of inputs into a nonlinear kernel induced feature space of very high (possibly infinite) dimension which is never explicitly represented or accessed. [sent-19, score-0.304]
</p><p>13 This leads to an efficient method that maps to an architecture that could be biologically implemented either by Sigma-Pi neurons, or fixed REF networks (as described for SFA [1]). [sent-20, score-0.145]
</p><p>14 We demonstrate that using this method to extract features that vary slowly in natural images leads to the development of both the complex-cell properties of translation invariance and disparity coding simultaneously. [sent-21, score-1.125]
</p><p>15 1  Finding Slow Features with kernels  Given I time-series vectors X i (x) of a new input x onto w which is equivalent to y = 2:! [sent-22, score-0.033]
</p><p>16 Finding a sparse solution If the eigen problem is solved on the entire training set then this algorithm also suffers from the curse of dimensionality, since the matrices (lxl) easily become computationally intractable. [sent-24, score-0.19]
</p><p>17 A sparse solution using a small subset p of the training data in the expansion is therefore essential: this is called the basis set BS. [sent-25, score-0.095]
</p><p>18 The output is now y = 2: iE BS Qik(Xi' x), and the solution must lie in the subspace spanned by BS. [sent-26, score-0.09]
</p><p>19 The kernel elements Kij are computed between the p basis vectors X i and the 1 training data Xj. [sent-27, score-0.156]
</p><p>20 This approach can effectively solve very large problems, provided p < < l. [sent-29, score-0.038]
</p><p>21 The question of course is how to choose the basis vectors: it is both necessary and sufficient that they span the space of the solution in the kernel induced feature space. [sent-30, score-0.185]
</p><p>22 In a recent version of the algorithm [12] we use the sparse greedy method of [13] as a preprocessing step. [sent-31, score-0.049]
</p><p>23 This efficiently finds a small basis set that minimises the least-squares error between data points in feature space and those reconstructed in the feature space defined by the basis set. [sent-32, score-0.322]
</p><p>24 In the simulations below we used a less efficient greedy algorithm that performed equally well here, but requires a considerably larger basis setl. [sent-33, score-0.081]
</p><p>25 The complete online algorithm requires minimal memory, making it ideal for very large data sets. [sent-34, score-0.039]
</p><p>26 The implementation estimates the long- and short-term kernel means online using exponential time averages parameterised using half-lives As, At (as in [9]). [sent-35, score-0.116]
</p><p>27 Likewise, the covariance matrices KK T , i(i(T are updated online at --T --T -T each time step e. [sent-36, score-0.039]
</p><p>28 KK is updated to KK + KK where K is the column vector of kernel values centred using the long term mean and computed for the current time step; there is therefore no need to explicitly compute or store kernel matrices. [sent-38, score-0.154]
</p><p>29 2  Simulation Results  The simulation was performed using a grey-level stereo pair of resolution 128x128, shown in Figure 1 [a]. [sent-39, score-0.325]
</p><p>30 A new 2D direction 0째 < 360째 was selected at every 64 time steps, and the image was translated by one pixel per time step in this direction (with toroidal wrap-around). [sent-40, score-0.088]
</p><p>31 e : :;  A set of 20 monocular simple cells was learnt using the algorithm described in [11] that maximises a nonlinear measure of temporal correlation (TRS) between the lVectors x are added to BS if, for y E BS, Ik(x,y) 1 ~ annealed from TO = 1, and the size of BS is set at 400. [sent-41, score-0.802]
</p><p>32 T  where threshold  T  is slowly  Figure 1: Training on natural images. [sent-42, score-0.104]
</p><p>33 [d] Output of nonlinear complex cells in binocular simulation. [sent-46, score-0.54]
</p><p>34 We chose this algorithm since it is based on a nonlinear measure of temporal correlation and yet provides a linear sparse-distributed coding, very similar to that of lCA for describing simple cells [14] . [sent-49, score-0.559]
</p><p>35 We did not use the objective function described above since in the linear case it yields filters similar to the local Fourier series 2 . [sent-50, score-0.284]
</p><p>36 The filters were optimised for this particular stereo pair; simulations using a greater variation of more natural images resulted in more spatially localised filters very similar to those in [14, 11]. [sent-51, score-0.703]
</p><p>37 We used only the 20 most predictable filters since results did not improve through use of the full set. [sent-52, score-0.411]
</p><p>38 The simple cell receptive field was 8x8, and during learning data was provided by both eyes at one position in the image 3 . [sent-53, score-0.441]
</p><p>39 The oriented Gabor-like weight vectors for the 20 cells contributing most to the TRS objective function are shown in Figure l[b], and the result of processing the left image with these linear filters is shown in Figure l[c]. [sent-54, score-0.667]
</p><p>40 The complex cells received input from these 20 types of simple cells when processing both the left and right eye images. [sent-55, score-0.624]
</p><p>41 t = 1, 'f] = 10- 3,0 = 10- 1 ; 10 5 input vectors were used. [sent-58, score-0.033]
</p><p>42 [a]  [b]  Figure 2: Testing on simulated pair used in [9] . [sent-59, score-0.043]
</p><p>43 [c] Output of most predictable complex cell trained on Figure I[a]. [sent-62, score-0.467]
</p><p>44 each cell therefore received 320 simple cell inputs (2x4x4x20); these were normalised to have unit variance and zero mean. [sent-63, score-0.324]
</p><p>45 The most predictable features were extracted for this input vector over 105 time-steps, using the kernel-based method described above, using data at just one position in the image. [sent-64, score-0.302]
</p><p>46 The basis set was made up of 400 input vectors, and a polynomial kernel of degree 2 was used. [sent-65, score-0.123]
</p><p>47 The temporal half-lifes for estimating the short- and long-term means in U and V were As = 2, Al = 200. [sent-66, score-0.129]
</p><p>48 The algorithm therefore extracts 400 outputs; we display the outputs for the 8 most predictable (determined by highest eigenvalues) in Figure I[d]; further values were hard to interpret. [sent-67, score-0.332]
</p><p>49 Below this, in Figure I[e], we show the complex outputs obtained if we substitute the right image with the left one in the stereo pair, so making the simulation monocular. [sent-68, score-0.577]
</p><p>50 It is visually apparent how the most predictable units are strongly selective for regions of iso-orientation (looking quite different to any simple cell response in [c]). [sent-70, score-0.367]
</p><p>51 In this particular image, it results in different "T" -shaped parts of the Pentagon of considerable size being distinctly isolated. [sent-71, score-0.033]
</p><p>52 That is, its response is invariant to the phase that determines the profile of the simple cell response. [sent-73, score-0.196]
</p><p>53 This is most striking in the output provided by the first feature; that is, this parameter is the most predictable in the image (providing an eigenvalue A = VjU = 7. [sent-78, score-0.441]
</p><p>54 This parameter is binocular disparity, generated by the variation in depth of the Pentagon roof compared to the ground. [sent-80, score-0.053]
</p><p>55 Here we have taken the artificial stereo pair used in [9], shown in Figure 2[a] , that has been generated using the known eggshell disparity function shown in Figure 2[b]. [sent-82, score-0.653]
</p><p>56 We presented this to the network trained wholly on the Pentagon stereo pair; it can be seen that the most predictable component, shown in Figure 2[c], replicates the disparity function of [b] 4. [sent-83, score-0.756]
</p><p>57 4The output is somewhat noisy, partly because the image has few linear features like those in Figure l[b] ; if we train the simple and complex cells on this image we get a much cleaner result . [sent-84, score-0.742]
</p><p>58 Although these properties have been dealt with in others' work discussed above, they have been considered either in isolation or through theoretical simulation. [sent-86, score-0.05]
</p><p>59 It is only because the kernel-based method we present allows us to work efficiently with large amounts of data in a nonlinear feature space derived from high dimensional input that we have been able to extract both complex cell properties together from realistic image data. [sent-87, score-0.657]
</p><p>60 It is also biologically plausible in as much as [a] it uses a reasonable objective function based on temporal coherence of output, and [b] the final computation required to extract these most predictable outputs could be performed either by Sigma-Pi neurons, or fixed RBF networks (as in SFA [1]) . [sent-89, score-0.879]
</p><p>61 However, we do not claim either that the precise formulation of the objective function is biologically exact, or that a biological system would use the same means to arrive at the final architecture that computes the optimal solution: the learning algorithm is certainly different. [sent-90, score-0.245]
</p><p>62 Our approach is therefore focussed on the constraints provided by [a] and [b]. [sent-91, score-0.038]
</p><p>63 The method also exploits a distributed representation for maximising the objective function that results from the generalised eigenvector solution. [sent-92, score-0.362]
</p><p>64 Is this plausible given the emphasis that has been laid on sparse-coding early in the visual system [15]? [sent-93, score-0.128]
</p><p>65 Sparse representations are often the result of constraining different outputs to be uncorrelated, or stronger, independent. [sent-94, score-0.146]
</p><p>66 However, as one ascends the perceptual pathway generating more reduced nonlinear representations, even the constraint of uncorrelated output may be too strong, or unnecessary, to create the highly robust representations exploited by the brain. [sent-95, score-0.326]
</p><p>67 For example, Rolls reports and defends a highly distributed coding of faces in infero-temporal cortical areas with cells responding to a large proportion of stimuli to some degree ([16], chapter 5). [sent-96, score-0.442]
</p><p>68 Our method enforces the constraint that successive eigenvectors are orthogonal in the metrics C and C and can result in the partly correlated output expected in the robust distributed coding Rolls proposes. [sent-97, score-0.311]
</p><p>69 However, this would not be the case if the long-term means used for C are estimated with a temporal half-life sufficiently large that these means do not differ from the true expected values. [sent-98, score-0.129]
</p><p>70 However, it can be seen in Figure l[d]) that there is a clear separation of orientation, and some mixing of disparity and orientation-sensitivity. [sent-100, score-0.393]
</p><p>71 It is a feature of our method that different outputs must have different measures of predictability (i. [sent-101, score-0.235]
</p><p>72 There is certainly no practical or biological reason why these parameters should be carried separately in the visual system (see [1] for discussion). [sent-105, score-0.114]
</p><p>73 In conclusion, this work provides further support for the fruitful approach of extracting non-trivial parameters through maximisation of objective functions based on temporal properties of perceptual input. [sent-106, score-0.435]
</p><p>74 One of the challenges here is to extend current linear models into the nonlinear domain whilst limiting the extra complexity they bring, which can lead to excess degrees of freedom and computational problems. [sent-107, score-0.19]
</p><p>75 We have described here a kernel-based method that goes some way towards this, extracting disparity and translation simultaneously for complex cells trained on natural images. [sent-108, score-1.003]
</p><p>76 A model of adaptive development of complex cortical cells. [sent-135, score-0.148]
</p><p>77 Learning viewpoint invariant face representations from visual experience in an attractor network. [sent-148, score-0.172]
</p><p>78 A model of invariant object recognition in the visual system: Learning rules, activation functions , lateral inhibition, and information-based performance measures. [sent-154, score-0.133]
</p><p>79 Learning perceptually salient visual parameters using spatiotemporal smoothness constraints. [sent-159, score-0.079]
</p><p>80 Extracting slow subspaces from natural videos leads to complex cells. [sent-167, score-0.247]
</p><p>81 Simple-cell-like receptive fields maximise temporal coherence in natural video. [sent-173, score-0.435]
</p><p>82 The independent components of natural scenes are edge filters . [sent-196, score-0.244]
</p><p>83 Emergence of simple-cell receptive field properties by learning a sparse code for natural images. [sent-203, score-0.33]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disparity', 0.338), ('cells', 0.262), ('predictable', 0.225), ('stereo', 0.193), ('filters', 0.186), ('maximising', 0.181), ('invariance', 0.173), ('pentagon', 0.152), ('rolls', 0.152), ('translation', 0.143), ('cell', 0.142), ('coding', 0.138), ('kk', 0.132), ('temporal', 0.129), ('nonlinear', 0.125), ('sfa', 0.114), ('trs', 0.114), ('outputs', 0.107), ('extracting', 0.102), ('complex', 0.1), ('stone', 0.099), ('objective', 0.098), ('receptive', 0.098), ('bs', 0.097), ('curse', 0.093), ('maximises', 0.091), ('output', 0.09), ('slow', 0.089), ('simulation', 0.089), ('image', 0.088), ('monocular', 0.08), ('coherence', 0.08), ('artificial', 0.079), ('visual', 0.079), ('kernel', 0.077), ('bray', 0.076), ('martinez', 0.076), ('qik', 0.076), ('field', 0.075), ('biologically', 0.075), ('learnt', 0.072), ('maximise', 0.07), ('blind', 0.067), ('predictability', 0.066), ('sphering', 0.066), ('whilst', 0.065), ('feature', 0.062), ('eural', 0.06), ('minimises', 0.06), ('natural', 0.058), ('maximisation', 0.056), ('separation', 0.055), ('invariant', 0.054), ('invariances', 0.053), ('binocular', 0.053), ('properties', 0.05), ('plausible', 0.049), ('sparse', 0.049), ('suffers', 0.048), ('development', 0.048), ('images', 0.047), ('efficiently', 0.046), ('slowly', 0.046), ('basis', 0.046), ('trick', 0.045), ('hebbian', 0.045), ('modelling', 0.044), ('orientation', 0.044), ('extract', 0.044), ('projecting', 0.043), ('correlation', 0.043), ('pair', 0.043), ('distributed', 0.042), ('exploits', 0.041), ('filter', 0.041), ('partly', 0.041), ('inputs', 0.04), ('features', 0.04), ('uncorrelated', 0.039), ('online', 0.039), ('representations', 0.039), ('provided', 0.038), ('final', 0.037), ('extracted', 0.037), ('networks', 0.035), ('certainly', 0.035), ('efficient', 0.035), ('dimensionality', 0.034), ('vectors', 0.033), ('optimised', 0.033), ('ijcnn', 0.033), ('cleaner', 0.033), ('lca', 0.033), ('ascends', 0.033), ('barrow', 0.033), ('distinctly', 0.033), ('ems', 0.033), ('etworks', 0.033), ('kayser', 0.033), ('minimising', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="118-tfidf-1" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>2 0.23954251 <a title="118-tfidf-2" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>3 0.23085339 <a title="118-tfidf-3" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>4 0.2107358 <a title="118-tfidf-4" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>5 0.17768115 <a title="118-tfidf-5" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>6 0.17489196 <a title="118-tfidf-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.11669565 <a title="118-tfidf-7" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>8 0.11510459 <a title="118-tfidf-8" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>9 0.11270737 <a title="118-tfidf-9" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>10 0.10972817 <a title="118-tfidf-10" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>11 0.10510087 <a title="118-tfidf-11" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>12 0.10468507 <a title="118-tfidf-12" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>13 0.079467028 <a title="118-tfidf-13" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>14 0.078143485 <a title="118-tfidf-14" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>15 0.073826 <a title="118-tfidf-15" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>16 0.073442213 <a title="118-tfidf-16" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>17 0.073223971 <a title="118-tfidf-17" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>18 0.069954015 <a title="118-tfidf-18" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>19 0.069407284 <a title="118-tfidf-19" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>20 0.069113478 <a title="118-tfidf-20" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.256), (1, 0.115), (2, 0.055), (3, 0.143), (4, -0.037), (5, -0.103), (6, 0.143), (7, 0.004), (8, -0.0), (9, 0.027), (10, 0.005), (11, -0.006), (12, 0.03), (13, 0.105), (14, 0.016), (15, 0.171), (16, 0.305), (17, -0.037), (18, -0.159), (19, 0.012), (20, 0.089), (21, -0.023), (22, 0.194), (23, -0.143), (24, -0.094), (25, -0.098), (26, -0.094), (27, 0.059), (28, -0.078), (29, 0.015), (30, -0.014), (31, 0.092), (32, 0.147), (33, -0.001), (34, 0.038), (35, 0.021), (36, -0.005), (37, 0.037), (38, 0.02), (39, 0.084), (40, 0.086), (41, -0.045), (42, -0.021), (43, 0.109), (44, -0.04), (45, 0.033), (46, 0.006), (47, 0.044), (48, 0.035), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96117842 <a title="118-lsi-1" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>2 0.86503261 <a title="118-lsi-2" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>3 0.80552453 <a title="118-lsi-3" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>4 0.72731644 <a title="118-lsi-4" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>5 0.69644743 <a title="118-lsi-5" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>6 0.48646063 <a title="118-lsi-6" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>7 0.40133286 <a title="118-lsi-7" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>8 0.39974493 <a title="118-lsi-8" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>9 0.39634115 <a title="118-lsi-9" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>10 0.38858572 <a title="118-lsi-10" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>11 0.37500283 <a title="118-lsi-11" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>12 0.36257201 <a title="118-lsi-12" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>13 0.34911391 <a title="118-lsi-13" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>14 0.34674791 <a title="118-lsi-14" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>15 0.3352223 <a title="118-lsi-15" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>16 0.33139747 <a title="118-lsi-16" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>17 0.32538152 <a title="118-lsi-17" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>18 0.29857808 <a title="118-lsi-18" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>19 0.29543829 <a title="118-lsi-19" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>20 0.2939302 <a title="118-lsi-20" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.015), (42, 0.052), (54, 0.108), (55, 0.517), (64, 0.022), (68, 0.03), (74, 0.087), (92, 0.012), (98, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94309878 <a title="118-lda-1" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>2 0.84506989 <a title="118-lda-2" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>3 0.83601922 <a title="118-lda-3" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>4 0.81829625 <a title="118-lda-4" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>Author: Agathe Girard, Carl Edward Rasmussen, Joaquin Quiñonero Candela, Roderick Murray-Smith</p><p>Abstract: We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. -step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form , the prediction of at time is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.   ¡ % # ¢ ¡     ¢ ¡¨ ¦ ¤ ¢ $</p><p>5 0.64789987 <a title="118-lda-5" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>6 0.59971368 <a title="118-lda-6" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>7 0.59702885 <a title="118-lda-7" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>8 0.59535861 <a title="118-lda-8" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>9 0.52469468 <a title="118-lda-9" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>10 0.52235174 <a title="118-lda-10" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>11 0.51114154 <a title="118-lda-11" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>12 0.50968134 <a title="118-lda-12" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>13 0.50707257 <a title="118-lda-13" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>14 0.50449258 <a title="118-lda-14" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>15 0.50280565 <a title="118-lda-15" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>16 0.50023198 <a title="118-lda-16" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>17 0.4954685 <a title="118-lda-17" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>18 0.49399582 <a title="118-lda-18" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>19 0.49396786 <a title="118-lda-19" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>20 0.49124214 <a title="118-lda-20" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
