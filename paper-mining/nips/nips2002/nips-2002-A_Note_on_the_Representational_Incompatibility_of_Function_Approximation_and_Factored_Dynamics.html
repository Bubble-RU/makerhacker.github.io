<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-13" href="#">nips2002-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</h1>
<br/><p>Source: <a title="nips-2002-13-pdf" href="http://papers.nips.cc/paper/2328-a-note-on-the-representational-incompatibility-of-function-approximation-and-factored-dynamics.pdf">pdf</a></p><p>Author: Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, Alexander Russell</p><p>Abstract: We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games.</p><p>Reference: <a title="nips-2002-13-reference" href="../nips2002_reference/nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. [sent-10, score-0.506]
</p><p>2 More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. [sent-11, score-0.675]
</p><p>3 Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. [sent-12, score-0.675]
</p><p>4 Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games. [sent-13, score-0.403]
</p><p>5 1 Introduction While a number of different representational approaches to large Markov decision processes (MDPs) have been proposed and studied over recent years, relatively little is known about the relationships between them. [sent-14, score-0.102]
</p><p>6 For example, in function approximation, a parametric form is proposed for the value functions of policies. [sent-15, score-0.069]
</p><p>7 Presumably, for any assumed parametric form (for instance, linear value functions), rather strong constraints on the underlying stochastic dynamics and rewards may be required to meet the assumption. [sent-16, score-0.2]
</p><p>8 Similarly, there has been recent interest in making parametric assumptions on the dynamics and rewards directly, as in the recent work on factored MDPs. [sent-18, score-0.389]
</p><p>9 Here it is known that the problem of computing an optimal policy from the MDP parameters is intractable (see [7] and the references therein), but exactly what the representational constraints on such policies are has remained largely unexplored. [sent-19, score-0.562]
</p><p>10 In this note, we give a new intractability result for planning in factored MDPs that exposes a noteworthy conceptual point missing from previous hardness results. [sent-20, score-0.468]
</p><p>11 Prior intractability results for planning in factored MDPs established that the problem of computing optimal policies from MDP parameters is hard, but left open the possibility that for any ﬁxed factored MDP, there might exist a compact, parametric representation of its optimal policy. [sent-21, score-0.983]
</p><p>12 This would be roughly analogous to standard NP-complete problems such as graph coloring — any 3-colorable graph has a “compact” description of its 3-coloring, but it is hard to compute it from the graph. [sent-22, score-0.179]
</p><p>13 Under a standard and widely believed complexitytheoretic assumption (that is even weaker than the assumption that NP does not have polynomial size Boolean circuits), we prove that a speciﬁc family of factored MDPs does not even possess “succinct” policies. [sent-24, score-0.564]
</p><p>14 By this we mean something extremely general — namely, that for each MDP in the family, it cannot have an optimal policy represented by an arbitrary Boolean circuit whose size is bounded by a polynomial in the size of the MDP description. [sent-25, score-0.575]
</p><p>15 Since such circuits can represent essentially any standard parametric functional form, we are showing that there exists no “reasonable” representation of good policies in factored MDPs, even if we ignore the problem of how to compute them from the MDP description. [sent-26, score-0.521]
</p><p>16 This result holds even if we ask only for policies whose expected return approximates the optimal within a polynomial factor. [sent-27, score-0.467]
</p><p>17 (With a slightly stronger complexity-theoretic assumption, it follows that obtaining an approximation even within an exponential factor is impossible. [sent-28, score-0.097]
</p><p>18 ) Thus, while previous results established that there was at least a computational barrier to going from factored MDP parameters to good policies, here we show that the barrier is actually representational, a considerably worse situation. [sent-29, score-0.283]
</p><p>19 The result highlights the fact that even when making strong and reasonable assumptions about one representational aspect of MDPs (such as value functions or dynamics), there is no reason in general for this to lead to any nontrivial restrictions on the others. [sent-30, score-0.156]
</p><p>20 The construction in our result is ultimately rather simple, and relies on powerful results developed in complexity theory over the last decade. [sent-31, score-0.075]
</p><p>21 The primary differences between our work and Liberatore’s is that our results prove intractability of approximation and rely on different proof techniques. [sent-34, score-0.167]
</p><p>22 2 DBN-Markov Decision Processes A Markov decision process is a tuple where is a set of states, is a set of actions, is a family of probability distributions on , one for each , and is a reward function. [sent-35, score-0.227]
</p><p>23 We will denote by the probability that action in state results in state . [sent-36, score-0.269]
</p><p>24 When started in a state , and provided with a sequence of actions the MDP traverses a sequence of states , where each is a random sample from the distribution . [sent-37, score-0.305]
</p><p>25 The -discounted return associated with such a path is . [sent-39, score-0.087]
</p><p>26 When the action sequence is generated according to this policy, we denote by the state sequence produced as above. [sent-41, score-0.258]
</p><p>27 A policy is optimal if for all policies and all , we have We consider MDPs where the transition law is represented as a dynamic Bayes net, or DBN-MDPs. [sent-42, score-0.49]
</p><p>28 Namely, if the state space has size , then is represented by a -layer Bayes net. [sent-43, score-0.131]
</p><p>29 There are variables in the ﬁrst layer, representing the state variables at any given time , along with the action chosen at time . [sent-44, score-0.172]
</p><p>30 There are variables in the second layer, representing the state variables at time . [sent-45, score-0.097]
</p><p>31 All directed edges in the Bayes net go from variables in the ﬁrst layer to variables in the second layer; for our result, it sufﬁces to consider Bayes nets in which the indegree of every second-layer node is bounded by some constant. [sent-46, score-0.233]
</p><p>32 Each second layer node has a conditional probability table (CPT) describing its conditional distribution for every possible setting of its parents in the Bayes net. [sent-47, score-0.102]
</p><p>33 Thus the stochastic dynamics of the DBN-MDP are entirely described by the Bayes net in the standard way; the next-state distribution for any state is given by simply ﬁxing the ﬁrst layer nodes to the settings given by the state. [sent-48, score-0.303]
</p><p>34 Any given action choice then yields the nextstate distribution according to standard Bayes net semantics. [sent-49, score-0.126]
</p><p>35 We shall assume throughout that the rewards are a linear function of state. [sent-50, score-0.074]
</p><p>36 3 Arthur-Merlin Games The complexity class AM is a probabilistic extension of the familiar class NP, and is typically described in terms of Arthur–Merlin games (see [2]). [sent-51, score-0.107]
</p><p>37 For instance, might be some standard encoding of an undirected graph , and might be interested in proving to that is 3-colorable. [sent-54, score-0.151]
</p><p>38 At each step of the conversation, ﬂips a fair coin, perhaps several times, and reports the resulting bits to ; this is interpreted as a “question” or “challenge” to . [sent-56, score-0.125]
</p><p>39 Thus responds with some number of bits, and the protocol proceeds to the next round. [sent-58, score-0.183]
</p><p>40 After poly steps, decides, based upon the conversation, whether to accept that or reject. [sent-59, score-0.386]
</p><p>41 We say that the language rithm such that:  is in the class AM poly if there is a (polynomial-time) algo-  When , there is always a strategy for to generate the responses to the random challenges that causes to accept. [sent-60, score-0.457]
</p><p>42 When , regardless of how responds to the random challenges, with probability at least , rejects. [sent-61, score-0.063]
</p><p>43 From the deﬁnition, it should be clear that every language in NP has an (easy) AM poly protocol in which , the prover, ignores  the random challenges, and simply presents with the standard NP witness to (e. [sent-65, score-0.631]
</p><p>44 More surprisingly, every language in the class PSPACE (the class of all languages that can be recognized in deterministic polynomial space, conjectured to be much larger than NP) also has an AM poly protocol, a beautiful and important result due to [6, 9]. [sent-68, score-0.506]
</p><p>45 ) If a language has an Arthur-Merlin game where Arthur asks only a constant number of questions, we say that AM . [sent-70, score-0.145]
</p><p>46 NP corresponds to Arthur-Merlin games where Arthur says nothing, and thus clearly NP AM . [sent-71, score-0.065]
</p><p>47 Let be a language in PSPACE, and let and be the Turing machines for the AM protocol for . [sent-75, score-0.251]
</p><p>48 Since is simply a Turing machine, it has some internal conﬁguration (sufﬁcient to completely describe the tape contents, read/write head position, abstract computational state, and so on) at any given moment in the protocol with . [sent-76, score-0.25]
</p><p>49 Since we assume is allpowerful (computationally unbounded), we can assume that has complete knowledge of this internal state of at all times. [sent-77, score-0.133]
</p><p>50 The protocol at round can thus be viewed: is in some state/conﬁguration ; a random bit sequence (the challenge) is generated; based on and , computes some response or action ; and based on and , enters its next conﬁguration . [sent-78, score-0.444]
</p><p>51 From this description, several observations can be made: ’s internal conﬁguration constitutes state in the Markovian sense — combined with the action , it entirely determines the next-state distribution. [sent-79, score-0.208]
</p><p>52 The dynamics are probabilistic due to the inﬂuence of the random bit sequence . [sent-80, score-0.236]
</p><p>53 We can thus view as implementing a policy in the MDP determined by (the internal conﬁguration of) — ’s actions, together with the stochastic , determine the evolution of the . [sent-81, score-0.28]
</p><p>54 At a high level, then, if every MDP so deﬁned by a language in AM poly had an “efﬁcient” policy , then something remarkable would occur: the arbitrary power allowed to in the deﬁnition of the class would have been unnecessary. [sent-84, score-0.657]
</p><p>55 For the moment, let us simply sketch the reﬁnements to this line of thought that will allow us to make the connection to factored MDPs: we will show that the MDPs deﬁned above can actually be represented by DBN-MDPs with only constant indegree and a linear reward function. [sent-86, score-0.433]
</p><p>56 As suggested, this will allow us to assert rather strong negative results about even the existence of efﬁcient policies, even when we ask for rather weak approximation to the optimal return. [sent-87, score-0.194]
</p><p>57 We now turn to the problem of planning in a DBN-MDP. [sent-88, score-0.135]
</p><p>58 Typically, one might like to have a “general-purpose” planning procedure — a procedure that takes as input a description of a DBN-MDP , and returns a description of the optimal policy . [sent-89, score-0.511]
</p><p>59 This is what is typically meant by the term planning, and we note that it demands a certain kind of uniformity — a single planning algorithm that can efﬁciently compute a succinct representation of the optimal policy for any DBN-MDP. [sent-90, score-0.691]
</p><p>60 Note that the existence of such a planning algorithm would certainly imply that every DBN-MDP has a succinct representation of its optimal policy — but the converse does not hold. [sent-91, score-0.668]
</p><p>61 It could be that the difﬁculty of planning in DBN-MDPs arises from the demand of uniformity — that is, that every DBNMDP possesses a succinct optimal policy, but the problem of computing it from the MDP parameters is intractable. [sent-92, score-0.483]
</p><p>62 This would be analogous to problems in NP — for example, every 3-colorable graph obviously has a succinct description of a 3-coloring, but it is difﬁcult to compute it from the graph. [sent-93, score-0.307]
</p><p>63 As mentioned in the introduction, it has been known for some time that planning in this uniform sense is computationally intractable. [sent-94, score-0.135]
</p><p>64 Here we establish the stronger and conceptually important result that it is not the uniformity giving rise to the difﬁculty, but rather that there simply exist DBN-MDPs in which the optimal policy does not possess a succinct representation in any natural parameterization. [sent-95, score-0.686]
</p><p>65 We will present a speciﬁc family of DBNMDPs (where has states with components), and show that, under a standard complexity-theoretic assumption, the corresponding family of optimal policies cannot be represented by arbitrary Boolean circuits of size polynomial in . [sent-96, score-0.591]
</p><p>66 We note that such circuits constitute a universal representation of efﬁciently computable functions, and all of the standard parametric forms in wide use in AI and statistics can be computed by such circuits. [sent-97, score-0.118]
</p><p>67 Let be any language in PSPACE, and let be a polynomial-time Turing machine running in time on inputs of length , implementing the algorithm of “Arthur” in the AM protocol for . [sent-99, score-0.251]
</p><p>68 Let be the maximum number of bits needed to write down a complete conﬁguration of that may arise during computation on an input of length (so , since no computation taking time can consume more than space). [sent-100, score-0.125]
</p><p>69 Each state of our DBN-MDP will have components, each corresponding to one bit of the encoding of a conﬁguration. [sent-101, score-0.276]
</p><p>70 No states will have rewards, except for the accepting states, which have reward . [sent-102, score-0.256]
</p><p>71 (Without loss of generality, we may assume that never enters an accepting state other than at time time . [sent-103, score-0.219]
</p><p>72 ) Note that we can encode conﬁgurations so that there is one bit position (say, the ﬁrst bit of the state vector) that records if the current state of is accepting or not. [sent-104, score-0.485]
</p><p>73 reward function is obviously linear (it is simply There are two actions: . [sent-106, score-0.164]
</p><p>74 Each action advances the simulation of the AM game by one time step. [sent-107, score-0.122]
</p><p>75 Steps where is choosing a bit to send to choosing to send a “ ” to . [sent-109, score-0.165]
</p><p>76 Steps where is doing deterministic computation; each action computation ahead one step. [sent-113, score-0.075]
</p><p>77 Note that each bit of the next move relation of a Turing machine depends on only bits of the preceding conﬁguration (i. [sent-115, score-0.228]
</p><p>78 , on the bits encoding the contents of the neighboring cells, the bits encoding the presence or absence of the input head in one of those cells, and the bits encoding the ﬁnite state information of the Turing machine). [sent-117, score-0.766]
</p><p>79 Thus the DBN-MDP describing on inputs of length has constant indegree; each bit is connected to the bits on which it depends. [sent-118, score-0.228]
</p><p>80 Note that a path in this MDP corresponding to an accepting computation of on an input of length has total reward ; a rejecting path has reward . [sent-119, score-0.409]
</p><p>81 A routine calculation shows  that the expected reward of the optimal policy is equal to the fraction of coin ﬂip sequences that cause to accept when communicating with an optimal . [sent-120, score-0.747]
</p><p>82 That is, Prob  accepts  Optimal expected reward  With the construction above, we can now describe our result: Theorem 1. [sent-121, score-0.272]
</p><p>83 If PSPACE is not contained in P/ POLY , then there is a family of DBN-MDPs , , such that for any two polynomials, and , there exist inﬁnitely many such that no circuit of size can compute a policy having expected reward greater than times the optimum. [sent-122, score-0.612]
</p><p>84 Before giving the formal proof, we remark that the assumption that PSPACE is not contained in P/ POLY is standard and widely believed, and informally asserts that not everything that can be computed in polynomial space can be computed by a non-uniform family of small circuits. [sent-123, score-0.222]
</p><p>85 Let be any language in PSPACE that is not in P/ POLY, and let be as described above. [sent-125, score-0.098]
</p><p>86 Suppose, contrary to the statement of Theorem, that for large enough there is indeed a circuit of size computing a policy for whose return is within a factor of optimal. [sent-126, score-0.477]
</p><p>87 We now consider the probabilistic circuit that operates as follows. [sent-127, score-0.108]
</p><p>88 takes a string as input, and estimates the expected return of the policy given by (which is the same as the probability that the prover associated with is able to convince that ). [sent-128, score-0.425]
</p><p>89 Speciﬁcally, builds the state corresponding to the start state of protocol on input , and then repeats the following procedure times: Given state , if is a state encoding a conﬁguration in which it is ’s turn, use to compute the message sent by and set to the new state of the AM protocol. [sent-129, score-0.714]
</p><p>90 Otherwise, if is a state encoding a conﬁguration in which it is ’s turn, ﬂip a coin at random and set to the new state of the AM protocol. [sent-130, score-0.443]
</p><p>91 Repeat until an accept or reject state is encountered. [sent-131, score-0.204]
</p><p>92 rejects is no more than  since in this case we are guaranteed that each iteration will accept with probability at least . [sent-133, score-0.107]
</p><p>93 On the other hand, if , then accepts with probability no more than , since each iteration accepts with probability at most . [sent-134, score-0.214]
</p><p>94 As has polynomial size and a probabilistic circuit can be simulated by a deterministic one of essentially the same size, it follows that is in P/ POLY , a contradiction. [sent-135, score-0.235]
</p><p>95 It is worth mentioning that, by the worst-case-to-average-case reduction of [1], if PSPACE is not in P/ POLY then we can select such a language so that the circuit will perform badly on a non-negligible fraction of the states of . [sent-136, score-0.285]
</p><p>96 That is, not only is it hard to ﬁnd an optimal policy, it will be the case that every policy that can be expressed as a polynomial size circuit will perform very badly on very many inputs. [sent-137, score-0.617]
</p><p>97 Finally, we remark that by coupling the above construction with the approximate lower bound protocol of [3], one can prove (under a stronger assumption) that there are no succinct policies for the DBN-MDPs which even approximate the optimum return to within an exponential factor. [sent-138, score-0.751]
</p><p>98 If PSPACE is not contained in AM , then there is a family of DBN-MDPs , , such that for any polynomial there exist inﬁnitely many such that no circuit of size can compute a policy having expected reward greater than times the optimum. [sent-140, score-0.705]
</p><p>99 Arthur-merlin games: a randomized proof system, and a hierarchy of complexity classes. [sent-151, score-0.077]
</p><p>100 Private coins versus public coins in interactive proof systems. [sent-156, score-0.162]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pspace', 0.294), ('poly', 0.279), ('policy', 0.244), ('mdp', 0.243), ('np', 0.233), ('mdps', 0.205), ('succinct', 0.191), ('factored', 0.189), ('policies', 0.184), ('arthur', 0.161), ('protocol', 0.153), ('coin', 0.14), ('turing', 0.139), ('planning', 0.135), ('guration', 0.132), ('reward', 0.132), ('bits', 0.125), ('circuit', 0.108), ('accepts', 0.107), ('accept', 0.107), ('bit', 0.103), ('language', 0.098), ('state', 0.097), ('polynomial', 0.093), ('accepting', 0.085), ('hardness', 0.08), ('indegree', 0.08), ('merlin', 0.08), ('encoding', 0.076), ('action', 0.075), ('rewards', 0.074), ('representational', 0.072), ('prover', 0.07), ('parametric', 0.069), ('layer', 0.066), ('games', 0.065), ('family', 0.065), ('intractability', 0.064), ('optimal', 0.062), ('challenge', 0.061), ('uniformity', 0.059), ('bayes', 0.058), ('return', 0.057), ('dynamics', 0.057), ('allender', 0.054), ('arora', 0.054), ('babai', 0.054), ('coloring', 0.054), ('convince', 0.054), ('fortnow', 0.054), ('highlights', 0.054), ('liberatore', 0.054), ('believed', 0.053), ('con', 0.052), ('net', 0.051), ('actions', 0.05), ('circuits', 0.049), ('challenges', 0.047), ('boolean', 0.047), ('game', 0.047), ('barrier', 0.047), ('coins', 0.047), ('conversation', 0.047), ('graph', 0.045), ('sequence', 0.043), ('complexity', 0.042), ('ask', 0.041), ('badly', 0.04), ('states', 0.039), ('contents', 0.037), ('enters', 0.037), ('prove', 0.037), ('ip', 0.037), ('every', 0.036), ('internal', 0.036), ('stronger', 0.036), ('acm', 0.035), ('proof', 0.035), ('description', 0.035), ('culty', 0.034), ('informally', 0.034), ('statement', 0.034), ('size', 0.034), ('construction', 0.033), ('interactive', 0.033), ('possess', 0.033), ('random', 0.033), ('simply', 0.032), ('send', 0.031), ('moore', 0.031), ('steps', 0.031), ('approximation', 0.031), ('responds', 0.03), ('proving', 0.03), ('remark', 0.03), ('decision', 0.03), ('even', 0.03), ('path', 0.03), ('head', 0.029), ('exist', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="13-tfidf-1" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>Author: Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, Alexander Russell</p><p>Abstract: We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games.</p><p>2 0.23582631 <a title="13-tfidf-2" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>3 0.16420564 <a title="13-tfidf-3" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>Author: Christopher G. Atkeson, Jun Morimoto</p><p>Abstract: A longstanding goal of reinforcement learning is to develop nonparametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of dimensionality. We have developed a trajectory-based approach, in which policies and value functions are represented nonparametrically along trajectories. These trajectories, policies, and value functions are updated as the value function becomes more accurate or as a model of the task is updated. We have applied this approach to periodic tasks such as hopping and walking, which required handling discount factors and discontinuities in the task dynamics, and using function approximation to represent value functions at discontinuities. We also describe extensions of the approach to make the policies more robust to modeling error and sensor noise.</p><p>4 0.14964163 <a title="13-tfidf-4" href="./nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><p>5 0.1451124 <a title="13-tfidf-5" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>6 0.13970491 <a title="13-tfidf-6" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>7 0.13936865 <a title="13-tfidf-7" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>8 0.12648216 <a title="13-tfidf-8" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>9 0.12473553 <a title="13-tfidf-9" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>10 0.11400066 <a title="13-tfidf-10" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>11 0.10349343 <a title="13-tfidf-11" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>12 0.10324309 <a title="13-tfidf-12" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>13 0.10137776 <a title="13-tfidf-13" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>14 0.088919856 <a title="13-tfidf-14" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>15 0.082627848 <a title="13-tfidf-15" href="./nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">61 nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>16 0.070674933 <a title="13-tfidf-16" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>17 0.068698421 <a title="13-tfidf-17" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>18 0.06574028 <a title="13-tfidf-18" href="./nips-2002-A_Differential_Semantics_for_Jointree_Algorithms.html">4 nips-2002-A Differential Semantics for Jointree Algorithms</a></p>
<p>19 0.058669809 <a title="13-tfidf-19" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>20 0.055173501 <a title="13-tfidf-20" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, 0.006), (2, -0.349), (3, -0.122), (4, -0.006), (5, -0.043), (6, 0.076), (7, -0.083), (8, -0.022), (9, -0.042), (10, 0.029), (11, 0.011), (12, -0.008), (13, 0.042), (14, 0.001), (15, -0.029), (16, 0.06), (17, -0.048), (18, 0.116), (19, -0.068), (20, 0.008), (21, -0.008), (22, -0.168), (23, -0.043), (24, -0.118), (25, -0.016), (26, -0.017), (27, -0.039), (28, -0.004), (29, 0.037), (30, -0.003), (31, -0.012), (32, 0.06), (33, 0.046), (34, 0.106), (35, -0.074), (36, -0.06), (37, -0.013), (38, -0.155), (39, 0.041), (40, -0.006), (41, -0.007), (42, -0.028), (43, 0.064), (44, -0.017), (45, -0.089), (46, 0.035), (47, -0.002), (48, 0.039), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9558562 <a title="13-lsi-1" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>Author: Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, Alexander Russell</p><p>Abstract: We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games.</p><p>2 0.77542883 <a title="13-lsi-2" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><p>3 0.77443737 <a title="13-lsi-3" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>4 0.75001544 <a title="13-lsi-4" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>Author: Robert B. Gramacy, Manfred K. Warmuth, Scott A. Brandt, Ismail Ari</p><p>Abstract: We are constructing caching policies that have 13-20% lower miss rates than the best of twelve baseline policies over a large variety of request streams. This represents an improvement of 49–63% over Least Recently Used, the most commonly implemented policy. We achieve this not by designing a speciﬁc new policy but by using on-line Machine Learning algorithms to dynamically shift between the standard policies based on their observed miss rates. A thorough experimental evaluation of our techniques is given, as well as a discussion of what makes caching an interesting on-line learning problem.</p><p>5 0.67718953 <a title="13-lsi-5" href="./nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><p>6 0.65190536 <a title="13-lsi-6" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>7 0.64888787 <a title="13-lsi-7" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>8 0.50334734 <a title="13-lsi-8" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>9 0.47890201 <a title="13-lsi-9" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>10 0.44197398 <a title="13-lsi-10" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>11 0.38937569 <a title="13-lsi-11" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>12 0.37135065 <a title="13-lsi-12" href="./nips-2002-A_Differential_Semantics_for_Jointree_Algorithms.html">4 nips-2002-A Differential Semantics for Jointree Algorithms</a></p>
<p>13 0.37041277 <a title="13-lsi-13" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>14 0.36036289 <a title="13-lsi-14" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>15 0.35767052 <a title="13-lsi-15" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>16 0.35423595 <a title="13-lsi-16" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>17 0.34905759 <a title="13-lsi-17" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>18 0.33623216 <a title="13-lsi-18" href="./nips-2002-Field-Programmable_Learning_Arrays.html">91 nips-2002-Field-Programmable Learning Arrays</a></p>
<p>19 0.32405046 <a title="13-lsi-19" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>20 0.31967351 <a title="13-lsi-20" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.02), (23, 0.041), (42, 0.055), (54, 0.11), (55, 0.034), (67, 0.017), (68, 0.019), (73, 0.34), (74, 0.108), (83, 0.013), (92, 0.046), (98, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80012482 <a title="13-lda-1" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>Author: Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, Alexander Russell</p><p>Abstract: We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games.</p><p>2 0.72870404 <a title="13-lda-2" href="./nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp</p><p>Abstract: Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves signiﬁcantly better accuracy, yet at a higher computational cost.</p><p>3 0.71101069 <a title="13-lda-3" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image. This might be attempted by using simple ﬁlters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images. In this paper we describe an approach which lies between the two extremes. It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves. We use a dynamic tree-like architecture to model the high resolution data. Approximate conditioning on the low resolution image is achieved through a mean ﬁeld approach. 1</p><p>4 0.51585013 <a title="13-lda-4" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>Author: Nicholas Roy, Geoffrey J. Gordon</p><p>Abstract: Standard value function approaches to ﬁnding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models. The intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space. However, in real POMDP problems most belief states are unlikely, and there is a structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space. We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity. We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state. We then plan directly on the low-dimensional belief features. By planning in a low-dimensional space, we can ﬁnd policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task.</p><p>5 0.50770569 <a title="13-lda-5" href="./nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><p>6 0.50477242 <a title="13-lda-6" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>7 0.50463223 <a title="13-lda-7" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>8 0.50339085 <a title="13-lda-8" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>9 0.50317973 <a title="13-lda-9" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>10 0.50269037 <a title="13-lda-10" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>11 0.50248057 <a title="13-lda-11" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>12 0.50187308 <a title="13-lda-12" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>13 0.50160813 <a title="13-lda-13" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>14 0.50157183 <a title="13-lda-14" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>15 0.49974051 <a title="13-lda-15" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>16 0.4989388 <a title="13-lda-16" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>17 0.49884331 <a title="13-lda-17" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>18 0.49883354 <a title="13-lda-18" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>19 0.49816182 <a title="13-lda-19" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>20 0.49788129 <a title="13-lda-20" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
