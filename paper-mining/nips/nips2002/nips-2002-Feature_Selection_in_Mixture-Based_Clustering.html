<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2002-Feature Selection in Mixture-Based Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-90" href="#">nips2002-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2002-Feature Selection in Mixture-Based Clustering</h1>
<br/><p>Source: <a title="nips-2002-90-pdf" href="http://papers.nips.cc/paper/2308-feature-selection-in-mixture-based-clustering.pdf">pdf</a></p><p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>Reference: <a title="nips-2002-90-reference" href="../nips2002_reference/nips-2002-Feature_Selection_in_Mixture-Based_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Figueiredo a Instituto de Telecomunicacoes, ¸˜ Instituto Superior T´ cnico e 1049-001 Lisboa Portugal  Abstract There exist many approaches to clustering, but the important issue of feature selection, i. [sent-10, score-0.345]
</p><p>2 , selecting the data attributes that are relevant for clustering, is rarely addressed. [sent-12, score-0.144]
</p><p>3 Feature selection for clustering is difﬁcult due to the absence of class labels. [sent-13, score-0.453]
</p><p>4 We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. [sent-14, score-0.494]
</p><p>5 In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. [sent-15, score-0.345]
</p><p>6 The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. [sent-17, score-0.584]
</p><p>7 Feature selection is then carried out by a backward search scheme. [sent-18, score-0.228]
</p><p>8 This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. [sent-19, score-0.516]
</p><p>9 However, not all the features are useful in constructing the partitions: some features may be just noise, thus not contributing to (or even degrading) the clustering process. [sent-22, score-0.688]
</p><p>10 The task of selecting the “best” feature subset, known as feature selection (FS), is therefore an important task. [sent-23, score-0.885]
</p><p>11 In addition, FS may lead to more economical clustering algorithms (both in storage and computation) and, in many cases, it may contribute to the interpretability of the models. [sent-24, score-0.236]
</p><p>12 FS is particularly relevant for data sets with large numbers of features; e. [sent-25, score-0.06]
</p><p>13 , on the order of thousands as seen in some molecular biology [22] and text clustering applications [21]. [sent-27, score-0.236]
</p><p>14 In supervised learning, FS has been widely studied, with most methods falling into two classes: ﬁlters, which work independently of the subsequent learning algorithm; wrappers, which use the learning algorithm to evaluate feature subsets [12]. [sent-28, score-0.49]
</p><p>15 In contrast, FS has received little attention in clustering, mainly because, without class labels, it is unclear how to assess feature relevance. [sent-29, score-0.424]
</p><p>16 The problem is even more difﬁcult when the number of clusters is unknown, since the number of clusters and the best feature subset are inter-related [6]. [sent-30, score-0.544]
</p><p>17 Some approaches to FS in clustering have been proposed. [sent-31, score-0.236]
</p><p>18 Dy and Brodley [6] suggested a heuristic to compare feature subsets, using cluster separability. [sent-45, score-0.345]
</p><p>19 A Bayesian approach for multinomial mixtures was proposed in [21]; another Bayesian approach using a shrinkage prior was considered in [8]. [sent-46, score-0.102]
</p><p>20 Dash and Liu [4] assess the clustering tendency of each feature by an entropy index. [sent-47, score-0.788]
</p><p>21 Finally, Devaney and Ram [5] use a notion of “category utility” for FS in conceptual clustering, and Modha and Scott-Spangler [17] assign weights to feature groups with a score similar to Fisher discrimination. [sent-50, score-0.407]
</p><p>22 In this paper, we introduce two new FS approaches for mixture-based clustering [10, 15]. [sent-51, score-0.236]
</p><p>23 The ﬁrst is based on a feature saliency measure which is obtained by an EM algorithm; unlike most FS methods, this does not involve any explicit search. [sent-52, score-0.704]
</p><p>24 The second approach extends the mutual-information based criterion of [13] to the unsupervised context; it is a wrapper, since FS is wrapped around a basic mixture estimation algorithm. [sent-53, score-0.38]
</p><p>25 Each is a -dimensional feature and all components have the same form (e. [sent-59, score-0.387]
</p><p>26 In the sequel, we will use the indices , and to run through data points, mixture components, and features, respectively. [sent-72, score-0.106]
</p><p>27 Assume now that some features are irrelevant, in the following sense: if feature is irrelevant, then , for , where is the common (i. [sent-73, score-0.571]
</p><p>28 Let be a set of binary parameters, such that if feature is relevant and otherwise; then,  W     (5)     2  ¤)  e '   %$#2  ¤ )  ¢    £ 2 ¥ ¤¥  ¥ ) ' 'e #2  "! [sent-76, score-0.405]
</p><p>29 © §¦a' %U qW 2  ¤ )  e ' ££ 2 D )  e (&  £ ¢ '  Our approach consists of: (i) treating the ’s as missing variables rather than as parameters; (ii) estimating from the data; is the probability that the -th feature is useful, which we call its saliency. [sent-78, score-0.437]
</p><p>30 The resulting mixture model (see proof in [14]) is (6)  © 8 © ED F 8  2 )w '  F  ¢  The form of reﬂects our prior knowledge about the distribution of the non-salient features. [sent-79, score-0.106]
</p><p>31 As in a standard ﬁnite mixture, we ﬁrst select the component label by sampling from a multinomial distribution with parameters . [sent-84, score-0.096]
</p><p>32 Then, for each feature , we ﬂip a biased coin whose probability of getting a head is ; if we get a head, we use the mixture component to generate the -th feature; otherwise, the common component is used. [sent-85, score-0.639]
</p><p>33 1 Model Selection Standard EM for mixtures exhibits some weaknesses which also affect the EM algorithm just mentioned: it requires knowledge of , and a good initialization is essential for reaching a good local optimum. [sent-89, score-0.166]
</p><p>34 To overcome these difﬁculties, we adopt the approach in [9], which is based on the MML criterion [23, 24]. [sent-90, score-0.074]
</p><p>35 The MML criterion for the proposed model (see details in [14]) consists of minimizing, with respect to , the following cost function  ! [sent-91, score-0.114]
</p><p>36 From a parameter estimation viewpoint, this is equivalent to a MAP estimate with conjugate (improper) Dirichlet-type priors on the ’s and ’s (see details in [14]); thus, the EM algorithm undergoes a minor modiﬁcation in the M-step, which still has a closed form. [sent-97, score-0.115]
</p><p>37 For the -th feature in the -th component, the  F 2 D F  $   D ¡ ' %#D "     “effective” number of data points for estimating is . [sent-100, score-0.381]
</p><p>38 Similarly, for the -th feature in the common component, the number of effective data points for estimation is . [sent-102, score-0.416]
</p><p>39 When goes to zero, the -th feature is no longer salient and and are removed. [sent-107, score-0.345]
</p><p>40                  ¤  D    F  B  Ea¨aa  ©   Finally, since the model selection algorithm determines the number of components, it can be initialized with a large value of , thus alleviating the need for a good initialization [9]. [sent-109, score-0.308]
</p><p>41 2 Experiments and Results The ﬁrst data set considered consists of 800 points from a mixture of 4 equiprobable Gaussians with mean vectors , , , , and identity covariance matrices. [sent-113, score-0.175]
</p><p>42 Eight “noisy” features (sampled from a density) were appended to this data, yielding a set of 800 10-D patterns. [sent-114, score-0.277]
</p><p>43 The proposed algorithm was run 10 times, each initialized with ; the common component is initialized to cover all data, and the feature saliencies are initialized at 0. [sent-115, score-0.796]
</p><p>44 The saliencies of all the ten features, together with their standard deviations (error bars), are shown in Fig. [sent-118, score-0.103]
</p><p>45 We conclude that, in this case, the algorithm successfully locates the clusters and correctly assigns the feature saliencies. [sent-120, score-0.478]
</p><p>46 2  Figure 1: Feature saliency for 10-D 4-component  5  10 Feature no  15  20  Figure 2: Feature saliency for the Trunk  Gaussian mixture. [sent-135, score-0.718]
</p><p>47 The smaller the feature number, the more important is the feature. [sent-139, score-0.345]
</p><p>48 Note that these features have a descending order of relevance. [sent-142, score-0.268]
</p><p>49 The values of the feature saliencies are shown in Fig. [sent-145, score-0.448]
</p><p>50 We see the general trend that as the feature number increases, the saliency decreases, following the true characteristics of the data. [sent-147, score-0.746]
</p><p>51 Feature saliency values were also computed for the “wine” data set (available at the UCI repository at www. [sent-150, score-0.39]
</p><p>52 After standardizing all features to zero mean and unit variance, we applied the LNKnet supervised feature selection algorithm (available at www. [sent-155, score-0.83]
</p><p>53 The nine features selected by LNKnet are 7, 13, 1, 5, 10, 2, 12, 6, 9. [sent-159, score-0.226]
</p><p>54 Our feature saliency algorithm (with no class labels) yielded the values  Table 1: Feature saliency of wine data 1 0. [sent-160, score-1.217]
</p><p>55 Ranking the features in descending order of saliency, we get the ordering: 7, 12, 6, 1, 9, 11, 10, 13, 2, 8, 4, 5, 3. [sent-174, score-0.268]
</p><p>56 The top 5 features (7, 12, 6, 1, 9) are all in the subset selected by LNKnet. [sent-175, score-0.299]
</p><p>57 If we skip the sixth feature (11), the following three features (10, 13, 2) were also selected by LNKnet. [sent-176, score-0.571]
</p><p>58 Thus we can see that for this data set, our algorithm, though totally unsupervised, performs comparably with a supervised feature selection algorithm. [sent-177, score-0.564]
</p><p>59 4 A Feature Selection Wrapper Our second approach is more traditional in the sense that it selects a feature subset, instead of estimating feature saliency. [sent-178, score-0.69]
</p><p>60 The number of mixture components is assumed known a priori, though no restriction on the covariance of the Gaussian components is imposed. [sent-179, score-0.223]
</p><p>61 1 Irrelevant Features and Conditional Independence Assume that the class labels, , and the full feature vector, , follow some joint probability . [sent-181, score-0.383]
</p><p>62 Recall that the (see (3)) are posterior class probabilities, Prob class . [sent-184, score-0.076]
</p><p>63 If is a completely irrelevant equals exactly, because of the conditional independence in (9), feature subset, then applied to (3). [sent-186, score-0.427]
</p><p>64 In practice, such features rarely exist, though they do exhibit different degrees of irrelevance. [sent-187, score-0.264]
</p><p>65 As both and are probabilities, a natural criterion for assessing their closeness is the expected value of the Kullback-Leibler divergence (KLD, [3]). [sent-189, score-0.074]
</p><p>66 This criterion is computed as a sample mean  2 '  gf7      2 ' D hf7   D gf7 t    (11)   g7 f  2 ' © 8 ED © 87 @2 2 ' D gf7  £ ' $ D gf7 t 5#" D hf7 t B A  A      t        §    in our case. [sent-190, score-0.074]
</p><p>67 A low value of indicates that the features in are “almost” conditionally independent from the expected class labels, given the features in . [sent-191, score-0.566]
</p><p>68 At each stage, we ﬁnd the feature such that is smallest and add it to . [sent-193, score-0.345]
</p><p>69 EM is then run again, using the features not in , to update the posterior probabilities . [sent-194, score-0.226]
</p><p>70 The process is then repeated until only one feature remains, in what can be considered as a backward search algorithm that yields a sorting of the features by decreasing order of irrelevance. [sent-195, score-0.723]
</p><p>71 2 The assignment entropy Given a method to sort the features in the order of relevance, we now require a method to measure how good each subset is. [sent-197, score-0.528]
</p><p>72 Unlike in supervised learning, we can not resort to classiﬁcation accuracy. [sent-198, score-0.07]
</p><p>73 We adopt the criterion that a clustering is good if the clusters are “crisp”, i. [sent-199, score-0.404]
</p><p>74 A natural way to formalize this is to consider the mean entropy of the ; that is, the clustering is considered to be good if is small. [sent-202, score-0.466]
</p><p>75 In the sequel, we call “the entropy of the assignment”. [sent-203, score-0.166]
</p><p>76 An important characteristic of the entropy is that it cannot increase when more features are used (because, for any random variables , , and , , a fundamental inequality of information theory [3]; note that is a conditional entropy ). [sent-204, score-0.558]
</p><p>77 Moreover, exhibits a diminishing returns behavior (decreasing abruptly as the most relevant features are included, but changing little when less relevant features are used). [sent-205, score-0.633]
</p><p>78 Of course, during the backward search, one can also consider picking the next feature whose removal least increases , rather than the one yielding the smallest KLD; both options are explored in the experiments. [sent-207, score-0.482]
</p><p>79 Finally, we mention that other minimum-entropy-type criteria have been recently used for clustering [7], [18], but not for feature selection. [sent-208, score-0.581]
</p><p>80 3 Experiments We have conducted experiments on data sets commonly used for supervised learning tasks. [sent-210, score-0.07]
</p><p>81 Since we are doing unsupervised learning, the class labels are, of course, withheld and only used for evaluation. [sent-211, score-0.234]
</p><p>82 The two heuristics for selecting the next feature to be removed (based on minimum KLD and minimum entropy) are considered in different runs. [sent-212, score-0.539]
</p><p>83 To assess clustering quality, we assign each data point to the Gaussian component that most likely generated it and then compare this labelling with the ground-truth. [sent-213, score-0.367]
</p><p>84 3 reveal that the general trend of the error rate agrees well with . [sent-216, score-0.088]
</p><p>85 The error rates either have a minimum close to the “knee” of the H curve, or the curve becomes ﬂat. [sent-217, score-0.086]
</p><p>86 The two heuristics for selecting the feature to be removed perform comparably. [sent-218, score-0.426]
</p><p>87 For the cover type data set, the DKL heuristic yields lower error rates than the one based on , while the contrary happens for image segmentation and WBC datasets. [sent-219, score-0.169]
</p><p>88 ¢  ¢  5 Concluding Remarks and Future Work The two approaches for unsupervised feature selection herein proposed have different advantages and drawbacks. [sent-220, score-0.629]
</p><p>89 The ﬁrst approach avoids explicit feature search and does not require a pre-speciﬁed number of clusters; however, it assumes that the features are conditionally independent, given the components. [sent-221, score-0.68]
</p><p>90 Several issues require further work: weakly relevant features (in the sense of [12]) are not removed by the ﬁrst algorithm while the second approach relies on a good initial clustering. [sent-224, score-0.392]
</p><p>91 of classes  cover type 2000 10 4  image segmentation 1000 18 7  4000  55  3500  50  3000  45 1500 40 1000  % Erorr  2000  65 60 55  2500 2000  50  1500  35  45  1000  500  30  500  0  25  0  70  1200  60  1000  55  2  4 6 No. [sent-231, score-0.123]
</p><p>92 of features  15  30 5  16  250 14 200 12  Entropy  300  150 100  10  50  8  0  5  10 15 20 No. [sent-234, score-0.226]
</p><p>93 of features  (e)  6 30  25  (f)  35  70  35  70  30  60  30  25  50  25  40  20  30  15  10  20  10  5  10  0  0  50 20 40 15  % Error  Entropy  60  Entropy  80  % Error  Entropy  25  15  (d)  % Error  (c)  10 No. [sent-236, score-0.226]
</p><p>94 of features  % Error  0  30 20 10 0  2  4  6 8 No. [sent-237, score-0.226]
</p><p>95 of features  0  12  (h)  Figure 3: (a) and (b): cover type; (c) and (d): image segmentation; (e) and (f): WBC; (g) and (h): wine. [sent-239, score-0.3]
</p><p>96 Feature removal by minimum KLD (left column) and minimum (right column). [sent-240, score-0.12]
</p><p>97 Feature subset selection and order identiﬁcation for unsupervised learning. [sent-282, score-0.357]
</p><p>98 Bayesian feature weighting for unsupervised learning, with application to object recognition. [sent-297, score-0.48]
</p><p>99 Generalized model selection for unsupervised learning in high dimensions. [sent-388, score-0.284]
</p><p>100 MML clustering of multi-state, Poisson, von Mises circular and Gaussian distributions. [sent-413, score-0.236]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.359), ('feature', 0.345), ('fs', 0.331), ('clustering', 0.236), ('features', 0.226), ('em', 0.167), ('entropy', 0.166), ('selection', 0.149), ('kld', 0.138), ('wbc', 0.138), ('unsupervised', 0.135), ('mixture', 0.106), ('pami', 0.105), ('mml', 0.103), ('saliencies', 0.103), ('figueiredo', 0.09), ('wrapper', 0.09), ('irrelevant', 0.082), ('wine', 0.076), ('conditionally', 0.076), ('criterion', 0.074), ('cover', 0.074), ('subset', 0.073), ('gi', 0.073), ('supervised', 0.07), ('devaney', 0.069), ('lnknet', 0.069), ('michigan', 0.069), ('modha', 0.069), ('jain', 0.068), ('hi', 0.066), ('clusters', 0.063), ('icml', 0.063), ('labels', 0.061), ('component', 0.06), ('trunk', 0.06), ('dash', 0.06), ('instituto', 0.06), ('wallace', 0.06), ('wrappers', 0.06), ('relevant', 0.06), ('initialized', 0.058), ('aa', 0.054), ('missing', 0.052), ('yielding', 0.051), ('segmentation', 0.049), ('backward', 0.046), ('error', 0.046), ('selecting', 0.046), ('symbolic', 0.046), ('dy', 0.046), ('bars', 0.045), ('pdf', 0.044), ('descending', 0.042), ('trend', 0.042), ('sequel', 0.042), ('components', 0.042), ('gaussians', 0.041), ('assess', 0.041), ('removal', 0.04), ('treating', 0.04), ('algorithm', 0.04), ('details', 0.04), ('minimum', 0.04), ('koller', 0.039), ('gaussian', 0.039), ('class', 0.038), ('course', 0.038), ('rarely', 0.038), ('head', 0.038), ('law', 0.037), ('points', 0.036), ('multinomial', 0.036), ('subsets', 0.035), ('estimation', 0.035), ('removed', 0.035), ('posteriori', 0.034), ('mixtures', 0.033), ('search', 0.033), ('ml', 0.033), ('restriction', 0.033), ('considered', 0.033), ('assignment', 0.032), ('conceptual', 0.032), ('covariances', 0.032), ('repository', 0.031), ('good', 0.031), ('exhibits', 0.031), ('uci', 0.031), ('extends', 0.03), ('assign', 0.03), ('absence', 0.03), ('carbonetto', 0.03), ('locates', 0.03), ('coin', 0.03), ('wraps', 0.03), ('alleviating', 0.03), ('genomic', 0.03), ('abruptly', 0.03), ('mises', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="90-tfidf-1" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>2 0.15868884 <a title="90-tfidf-2" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>3 0.15657136 <a title="90-tfidf-3" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>Author: Nuno Vasconcelos</p><p>Abstract: We address the question of feature selection in the context of visual recognition. It is shown that, besides efﬁcient from a computational standpoint, the infomax principle is nearly optimal in the minimum Bayes error sense. The concept of marginal diversity is introduced, leading to a generic principle for feature selection (the principle of maximum marginal diversity) of extreme computational simplicity. The relationships between infomax and the maximization of marginal diversity are identiﬁed, uncovering the existence of a family of classiﬁcation procedures for which near optimal (in the Bayes error sense) feature selection does not require combinatorial search. Examination of this family in light of recent studies on the statistics of natural images suggests that visual recognition problems are a subset of it.</p><p>4 0.14910606 <a title="90-tfidf-4" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>5 0.14559725 <a title="90-tfidf-5" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classiﬁcation problems. The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model. This paper gives a theoretical analysis about how class information is preserved in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions. When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions. So we will develop a novel but simple clustering algorithm specialized for the Fisher score, which can exploit important dimensions. This algorithm is successfully tested in experiments with artiﬁcial data and real data (amino acid sequences).</p><p>6 0.13609096 <a title="90-tfidf-6" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>7 0.13447809 <a title="90-tfidf-7" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>8 0.12904529 <a title="90-tfidf-8" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>9 0.12527156 <a title="90-tfidf-9" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>10 0.11802655 <a title="90-tfidf-10" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>11 0.10576769 <a title="90-tfidf-11" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>12 0.10535555 <a title="90-tfidf-12" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>13 0.10262294 <a title="90-tfidf-13" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>14 0.10206522 <a title="90-tfidf-14" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>15 0.095531434 <a title="90-tfidf-15" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>16 0.094803818 <a title="90-tfidf-16" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>17 0.087737866 <a title="90-tfidf-17" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>18 0.087442607 <a title="90-tfidf-18" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>19 0.084655315 <a title="90-tfidf-19" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>20 0.081118658 <a title="90-tfidf-20" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.255), (1, -0.101), (2, 0.031), (3, 0.056), (4, -0.031), (5, 0.072), (6, -0.112), (7, -0.176), (8, -0.107), (9, 0.102), (10, 0.064), (11, 0.028), (12, -0.048), (13, 0.169), (14, -0.062), (15, -0.072), (16, -0.038), (17, -0.149), (18, -0.001), (19, 0.169), (20, -0.012), (21, -0.011), (22, 0.04), (23, 0.031), (24, 0.005), (25, -0.073), (26, -0.033), (27, -0.147), (28, 0.138), (29, 0.029), (30, -0.017), (31, -0.04), (32, -0.003), (33, -0.239), (34, -0.071), (35, 0.013), (36, 0.063), (37, 0.023), (38, -0.045), (39, -0.002), (40, 0.083), (41, -0.049), (42, -0.095), (43, 0.019), (44, -0.107), (45, -0.062), (46, -0.07), (47, -0.035), (48, 0.015), (49, 0.155)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97886336 <a title="90-lsi-1" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>2 0.78078616 <a title="90-lsi-2" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>Author: Nuno Vasconcelos</p><p>Abstract: We address the question of feature selection in the context of visual recognition. It is shown that, besides efﬁcient from a computational standpoint, the infomax principle is nearly optimal in the minimum Bayes error sense. The concept of marginal diversity is introduced, leading to a generic principle for feature selection (the principle of maximum marginal diversity) of extreme computational simplicity. The relationships between infomax and the maximization of marginal diversity are identiﬁed, uncovering the existence of a family of classiﬁcation procedures for which near optimal (in the Bayes error sense) feature selection does not require combinatorial search. Examination of this family in light of recent studies on the statistics of natural images suggests that visual recognition problems are a subset of it.</p><p>3 0.67678022 <a title="90-lsi-3" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>Author: Tilman Lange, Mikio L. Braun, Volker Roth, Joachim M. Buhmann</p><p>Abstract: Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a speciﬁc learning task. For supervised learning, the standard practical technique is crossvalidation, which is not applicable for semi-supervised and unsupervised settings. In this paper, a new model assessment scheme is introduced which is based on a notion of stability. The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performance of the stability measure is studied for model order selection in comparison to standard techniques in this area.</p><p>4 0.64758009 <a title="90-lsi-4" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classiﬁcation problems. The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model. This paper gives a theoretical analysis about how class information is preserved in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions. When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions. So we will develop a novel but simple clustering algorithm specialized for the Fisher score, which can exploit important dimensions. This algorithm is successfully tested in experiments with artiﬁcial data and real data (amino acid sequences).</p><p>5 0.6211524 <a title="90-lsi-5" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>Author: Noam Slonim, Yair Weiss</p><p>Abstract: The information bottleneck (IB) method is an information-theoretic formulation for clustering problems. Given a joint distribution , this method constructs a new variable that deﬁnes partitions over the values of that are informative about . Maximum likelihood (ML) of mixture models is a standard statistical approach to clustering problems. In this paper, we ask: how are the two methods related ? We deﬁne a simple mapping between the IB problem and the ML problem for the multinomial mixture model. We show that under this mapping the or problems are strongly related. In fact, for uniform input distribution over for large sample size, the problems are mathematically equivalent. Speciﬁcally, in these cases, every ﬁxed point of the IB-functional deﬁnes a ﬁxed point of the (log) likelihood and vice versa. Moreover, the values of the functionals at the ﬁxed points are equal under simple transformations. As a result, in these cases, every algorithm that solves one of the problems, induces a solution for the other.   ©§ ¥£ ¨¦¤¢   </p><p>6 0.60088348 <a title="90-lsi-6" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>7 0.52896971 <a title="90-lsi-7" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>8 0.50803894 <a title="90-lsi-8" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>9 0.47286332 <a title="90-lsi-9" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>10 0.46611232 <a title="90-lsi-10" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>11 0.46287125 <a title="90-lsi-11" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>12 0.45166814 <a title="90-lsi-12" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>13 0.43929982 <a title="90-lsi-13" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>14 0.43584061 <a title="90-lsi-14" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>15 0.43570304 <a title="90-lsi-15" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>16 0.43259379 <a title="90-lsi-16" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>17 0.42765275 <a title="90-lsi-17" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>18 0.42719311 <a title="90-lsi-18" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>19 0.42397353 <a title="90-lsi-19" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>20 0.39285153 <a title="90-lsi-20" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.01), (23, 0.014), (42, 0.041), (54, 0.085), (55, 0.02), (67, 0.013), (68, 0.017), (74, 0.615), (92, 0.022), (98, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98862475 <a title="90-lda-1" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>Author: Stella X. Yu, Ralph Gross, Jianbo Shi</p><p>Abstract: Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-patch competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optimal partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection.</p><p>2 0.96926701 <a title="90-lda-2" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>Author: Martin Szummer, Tommi S. Jaakkola</p><p>Abstract: Classiﬁcation with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P (x)), to further constrain the conditional P (y|x) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P (x). We develop algorithms for solving the regularization problem for ﬁnite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases.</p><p>3 0.96681219 <a title="90-lda-3" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>4 0.9656955 <a title="90-lda-4" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>same-paper 5 0.9593401 <a title="90-lda-5" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>6 0.90872896 <a title="90-lda-6" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>7 0.76291108 <a title="90-lda-7" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>8 0.72980034 <a title="90-lda-8" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>9 0.70078641 <a title="90-lda-9" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>10 0.69761866 <a title="90-lda-10" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>11 0.69527614 <a title="90-lda-11" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>12 0.69197947 <a title="90-lda-12" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>13 0.69120705 <a title="90-lda-13" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>14 0.68651569 <a title="90-lda-14" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>15 0.67746133 <a title="90-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.66236877 <a title="90-lda-16" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>17 0.64947718 <a title="90-lda-17" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>18 0.6434809 <a title="90-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.63709509 <a title="90-lda-19" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>20 0.63591862 <a title="90-lda-20" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
