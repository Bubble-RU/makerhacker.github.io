<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-182" href="#">nips2002-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</h1>
<br/><p>Source: <a title="nips-2002-182-pdf" href="http://papers.nips.cc/paper/2268-shape-recipes-scene-representations-that-refer-to-the-image.pdf">pdf</a></p><p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><p>Reference: <a title="nips-2002-182-reference" href="../nips2002_reference/nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. [sent-6, score-0.377]
</p><p>2 Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. [sent-7, score-1.312]
</p><p>3 Strong priors [14] are often needed, which can give unrealistic shape reconstructions. [sent-14, score-0.547]
</p><p>4 We assume that the image is always available and we describe the underlying scene in reference to the image. [sent-17, score-0.296]
</p><p>5 The scene representation is a set of rules for transforming from the local image information to the desired scene quantities. [sent-18, score-0.449]
</p><p>6 We call this representation a scene recipe: a simple function for transforming local image data to local scene data. [sent-19, score-0.475]
</p><p>7 The computer doesn’t have to represent every curve of an intricate shape; the image does that for us, the computer just stores the rules for transforming from image to scene. [sent-20, score-0.46]
</p><p>8 In this paper, we focus on reconstructing the shapes that created the observed image, deriving shape recipes. [sent-21, score-0.592]
</p><p>9 The particular recipes we study here are regression coefﬁcients for transforming  (a)  (b)  (c)  (c)  Figure 1: 1-d example: The image (a) is rendered from the shape (b). [sent-22, score-1.304]
</p><p>10 The shape depends on the image in a non-local way. [sent-23, score-0.762]
</p><p>11 Bandpass ﬁltering both signals allows for a local shape recipe. [sent-24, score-0.573]
</p><p>12 The dotted line (which agrees closely with true solid line) in (d) shows shape reconstruction from 9-parameter linear regression (9-tap convolution) from bandpassed image, (c). [sent-25, score-0.661]
</p><p>13 2 Shape Recipes The shape representation consists in describing, for a particular image, the functional relationship between image and shape. [sent-27, score-0.812]
</p><p>14 To simplify the computation to obtain shape from image data, we require that the scene recipes be local: the scene structure in a region should only depend on a local neighborhood of the image. [sent-30, score-1.425]
</p><p>15 1 (a) shows the intensity proﬁle of a 1-d image arising from the shape proﬁle shown in Fig. [sent-33, score-0.792]
</p><p>16 Note that the function to recover the shape from the image cannot be local because the identical local images on the left and right sides of the surface edge correspond to different shape heights. [sent-35, score-1.452]
</p><p>17 In order to obtain locality in the shape-image relationship, we need to preprocess the shape and image signals. [sent-36, score-0.762]
</p><p>18 When shape and image are represented in a bandpass pyramid, within a subband, under generic rendering conditions [4], local shape changes lead to local image changes. [sent-37, score-1.68]
</p><p>19 (Representing the image in a Gaussian pyramid also gives a local relationship between image and bandpassed shape, effectively subsuming the image bandpass operation into the shape recipe. [sent-38, score-1.489]
</p><p>20 In this example, (d) relates to (c) by a simple shape recipe: convolution with a 9-tap ﬁlter, learned by linear regression from rendered random shape data. [sent-41, score-1.197]
</p><p>21 For 2-d images, we break the image and shape into subbands using a steerable pyramid [13], an oriented multi-scale decomposition with non-aliased subbands (Fig. [sent-44, score-1.056]
</p><p>22 A shape subband can be related to an image intensity subband by a function Zk = fk (Ik )  (1)  where fk is a local function and Zk and Ik are the kth subbands of the steerable pyramid representation of the shape and image, respectively. [sent-46, score-2.296]
</p><p>23 The simplest functional relationship between shape and image intensity is via a linear ﬁlter with a ﬁnite size impulse response: Zk ≈ rk Ik , where is convolution. [sent-47, score-0.878]
</p><p>24 The convolution kernel rk (speciﬁc to each scale and orientation) transforms the image subband Ik into the shape subband Zk . [sent-48, score-1.402]
</p><p>25 The recipe rk at each subband is learned by minimizing x |Zk − Ik rk |2 , regularizing rk as needed to avoid overﬁtting. [sent-49, score-0.562]
</p><p>26 (a) Image  (b) Stereo shape  (c) Stereo shape (surface plot)  (d) Re-rendered stereo shape  Figure 2: Shape estimate from stereo. [sent-52, score-2.011]
</p><p>27 (a) is one image of the stereo pair; the stereo reconstruction is depicted as (b) a range map and (c) a surface plot and (d) a re-rendering of the stereo shape. [sent-53, score-1.351]
</p><p>28 The stereo shape is noisy and misses ﬁne details. [sent-54, score-0.916]
</p><p>29 We conjecture that multiscale shape recipes have various desirable properties for estimation. [sent-55, score-1.023]
</p><p>30 First, they allow for a compact encoding of shape information, as much of the complexity of the shape is encoded in the image itself. [sent-56, score-1.309]
</p><p>31 The recipes need only specify how to translate image into shape. [sent-57, score-0.689]
</p><p>32 Secondly, regularities in how the shape recipes f k vary across scale and space provide a powerful mechanism for regularizing shape estimates. [sent-58, score-1.633]
</p><p>33 Instead of regularizing shape estimates by assuming a prior of smoothness of the surface, we can assume a slow spatial variation of the functional relationship between image and shape, which should make estimating shape recipes easier. [sent-59, score-1.879]
</p><p>34 Third, shape recipes implicitly encode lighting and material information, which can be used for material-based segmentation. [sent-60, score-1.223]
</p><p>35 In the next two sections we discuss the properties of smoothness across scale and space and we show potential applications in improving shape estimates from stereo and in image segmentation based on material properties. [sent-61, score-1.358]
</p><p>36 2 shows one image of a stereo pair and the associated shape estimated from a stereo algorithm1. [sent-63, score-1.45]
</p><p>37 The shape estimate is noisy in the high frequencies (see surface plot and rerendered shape), but we assume it is accurate in the low spatial frequencies. [sent-64, score-0.686]
</p><p>38 3 shows the steerable pyramid representations of the image (a) and shape (b) and the learned shape recipes (c) for each subband (linear convolution kernels that give the shape subband from the image subband). [sent-66, score-3.326]
</p><p>39 4 (a) and (b) show the image and the implicit shape representation: the pyramid’s lowresolution shape and the shape recipes used over the top four scales. [sent-69, score-2.317]
</p><p>40 4 (c) and (d) show explicitly the reconstructed shape implied by (a) and (b): note the high resolution details, including the ﬁne structure visible in the bottom left corner of (d). [sent-71, score-0.597]
</p><p>41 Compare with the stereo 1 We took our stereo photographs using a 3. [sent-72, score-0.688]
</p><p>42 3 Megapixel Olympus Camedia C-3040 camera, with a Pentax stereo adapter. [sent-73, score-0.344]
</p><p>43 We calibrated the stereo images using the point matching algorithm of Zhang [18], and rectiﬁed the stereo pair (so that epipoles are along scan lines) using the algorithm of [8], estimating disparity with the Zitnick–Kanade stereo algorithm [19]. [sent-74, score-1.064]
</p><p>44 (c) Shape recipes for each subband (a) Image pyramid  (b) Shape pyramid  Figure 3: Learning shape recipes at each subband. [sent-77, score-1.997]
</p><p>45 (a) and (b) are the steerable pyramid representations [13] of image and stereo shape. [sent-78, score-0.769]
</p><p>46 The steerable pyramid isolates information according to scale (the smaller subband images represent larger spatial scales) and orientation (clockwise among subbands of one size: vertical, diagonal, horizontal, other diagonal). [sent-80, score-0.611]
</p><p>47 (a) image  (b) low-res shape (center, top row) and recipes (for each subband orientation)  (c) recipes shape (surface plot)  (d) re-rendered recipes shape  Figure 4: Reconstruction from shape recipes. [sent-81, score-4.052]
</p><p>48 The shape is represented by the information contained in the image (a), the low-res shape pyramid residual and the shape recipes (b) estimated at the lowest resolution. [sent-82, score-2.448]
</p><p>49 The shape can be regenerated by applying the shape recipes (b) at the 4 highest resolution scales, then reconstructing from the shape pyramid. [sent-83, score-2.147]
</p><p>50 (d) shows the image re-rendered under different lighting conditions than (a). [sent-84, score-0.297]
</p><p>51 The reconstruction is not noisy and shows more detail than the stereo shape, Fig. [sent-85, score-0.369]
</p><p>52 2, including the ﬁne textures visible at the bottom left of the image (a) but not detected by the stereo algorithm. [sent-86, score-0.595]
</p><p>53 4 Segmenting shape recipes Segmenting an image into regions of uniform color or texture is often an approximation to an underlying goal of segmenting the image into regions of uniform material. [sent-89, score-1.54]
</p><p>54 Shape recipes, by describing how to transform from image to shape, implicitly encode both lighting and material properties. [sent-90, score-0.445]
</p><p>55 Across unchanging lighting conditions, segmenting by shape recipes allows us to segment according to a material’s rendering properties, even overcoming changes of intensities or texture of the rendered image. [sent-91, score-1.251]
</p><p>56 ) We expect shape recipes to vary smoothly over space except for abrupt boundaries at changes in material or illumination. [sent-93, score-1.176]
</p><p>57 Within each subband, we can write the shape Z k  (a) Shape  (b) Image  (c) Image-based segmentation  (d) Recipe-based segmentation  Figure 5: Segmentation example. [sent-94, score-0.659]
</p><p>58 Based on image information alone, it is difﬁcult to ﬁnd a good segmentation into 2 groups, (c). [sent-96, score-0.271]
</p><p>59 A segmentation into 2 different shape recipes naturally falls along the vertical material boundary, (d). [sent-97, score-1.207]
</p><p>60 as a mixture of recipes: N  p(Zk |Ik ) =  p(Zk − fk,n (Ik ))pn  (2)  n=1  where N speciﬁes the number of recipes needed to explain the underlying shape Z k . [sent-98, score-1.008]
</p><p>61 To estimate the parameters of the mixture (shape recipes and weights), given known shape and the associated image, we use the EM algorithm [17]. [sent-100, score-1.034]
</p><p>62 ) The shape recipes encode the relationship between image and shape when segmenting into 2 groups, and ﬁnds the vertical material boundary, (d). [sent-109, score-2.007]
</p><p>63 These cases need to be treated specially with shape recipes. [sent-112, score-0.547]
</p><p>64 6 (c) the occluding boundary in the shape only produces a smooth change in the image, Fig. [sent-114, score-0.583]
</p><p>65 In that region, a shape recipe will produce an incorrect shape estimate, however, the stereo algorithm will often succeed at ﬁnding those occlusion edges. [sent-116, score-1.585]
</p><p>66 On the other hand, stereo often fails to provide the shape of image regions with complex shape details, where the shape recipes succeed. [sent-117, score-2.675]
</p><p>67 For the special case of revising the stereo algorithm’s output using shape recipes, we propose a statistical framework to combine both sources of information. [sent-118, score-0.891]
</p><p>68 Image in full-res (a) and one steerable pyramid subband (b); stereo depth, full-res (c) and subband (d). [sent-120, score-1.068]
</p><p>69 (e) shows subband of shape reconstruction using learned shape recipe. [sent-121, score-1.399]
</p><p>70 Direct application of shape recipe across occlusion boundary misses the shape discontinuity. [sent-122, score-1.305]
</p><p>71 Stereo algorithm catches that discontinuity, but misses other shape details. [sent-123, score-0.572]
</p><p>72 Probabilistic combination of the two shape estimates (f, subband, g, surface), assuming Laplacian shape statistics, captures the desirable details of both, comparing favorably with laser scanner ground truth, (h, subband, i, surface, at slight misalignment from photos). [sent-124, score-1.136]
</p><p>73 via shape recipes: p(Z|S, I) = p(S, I|Z)p(Z)/p(S, I) (3) (For notational simplicity, we omit the spatial dependency from I, S and Z. [sent-125, score-0.581]
</p><p>74 ) As both stereo S and image intensity I provide strong constraints for the possible underlying shape Z, the factor p(Z) can be considered constant in the region of support of p(S, I|Z). [sent-126, score-1.15]
</p><p>75 (3) can be simpliﬁed by assuming that the shapes from stereo and from shape recipes are independent. [sent-129, score-1.383]
</p><p>76 Furthermore, we also assume independence between the pixels in the image and across subbands: p(S, I|Z) =  p(Sk |Zk )p(Ik |Zk )  (4)  k x,y  Sk , Zk and Ik refer to the outputs of the subband k. [sent-130, score-0.504]
</p><p>77 The terms p(Sk |Zk ) and p(Ik |Zk ) will depend on the noise models for the depth from stereo and for the shape recipes. [sent-132, score-0.915]
</p><p>78 For the shape estimate from stereo we assume a Gaussian distribution for the noise. [sent-133, score-0.917]
</p><p>79 At each subband and spatial location we have: 2  p(Sk |Zk ) = ps (Zk − Sk ) =  2  e−|Zk −Sk | /σs (2π)1/2 σs  (5)  In the case of the shape recipes, a Gaussian noise model is not adequate. [sent-134, score-0.847]
</p><p>80 The distribution of the error Zk − fk (Ik ) will depend on image noise, but more importantly, on all shape and image variations that are not functionally related with each other through the recipes. [sent-135, score-1.065]
</p><p>81 When trying to estimate shape using the shape recipe f k (Ik ), it fails to capture the discontinuity although it captures correctly other texture variations, Fig. [sent-140, score-1.287]
</p><p>82 Therefore, Zk − fk (Ik ) will describe the distribution of occluding edges that do not produce image variations and paint edges that do not translate into shape variations. [sent-142, score-0.931]
</p><p>83 The least square estimate for the shape subband Zk given both stereo and image data, is: Zk p(Sk |Zk )p(Ik |Zk )dZk (7) p(Sk |Zk )p(Ik |Zk )dZk This integral can be evaluated numerically independently at each pixel. [sent-149, score-1.398]
</p><p>84 When p = 2, then the LSE estimation is a weighted linear combination of the shape from stereo and shape recipes. [sent-150, score-1.438]
</p><p>85 However, with p 1 this problem is similar to the one of image denosing from wavelet decompositions [12] providing a non-linear combination of stereo and shape recipes. [sent-151, score-1.121]
</p><p>86 (7) is to take from the stereo everything that cannot be explained by the recipes, and to take from the recipes the rest. [sent-153, score-0.805]
</p><p>87 Whenever both stereo and shape recipes give similar estimates, we prefer the recipes because they are more accurate than the stereo information. [sent-154, score-2.157]
</p><p>88 Where stereo and shape recipes differ greatly, such as at occlusions, then the shape estimate follows the stereo shape. [sent-155, score-2.269]
</p><p>89 ˆ Zk =  Zk p(Zk |Sk , Ik )dZk =  6 Discussion and Summary Unlike shape-from-shading algorithms [5], shape recipes are fast, local procedures for computing shape from image. [sent-156, score-1.581]
</p><p>90 The approximation of linear shading [7] also assumes a local linear relationship between image and shape subbands. [sent-157, score-0.877]
</p><p>91 We have proposed shape recipes as a representation that leaves the burden of describing shape details to the image. [sent-159, score-1.622]
</p><p>92 Unlike many other shape representations, these are lowdimensional, and should change slowly over time, distance, and spatial scale. [sent-160, score-0.581]
</p><p>93 We developed a shape estimate improver that relies on an initial estimate being accurate at low resolutions. [sent-163, score-0.599]
</p><p>94 Assuming that a shape recipes change slowly over 4 octaves of spatial scale, we learned the shape recipes at low resolution and applied them at high resolution to ﬁnd shape from image details not exploited by the stereo algorithm. [sent-164, score-3.275]
</p><p>95 Shape recipes fold in information about both lighting and material properties and can also be used to estimate material boundaries over regions where the lighting is assumed to be constant. [sent-166, score-0.913]
</p><p>96 Gilchrist and Adelson describe “atmospheres”, which are local formulas for converting image intensities to perceived lightness values [3, 1]. [sent-167, score-0.293]
</p><p>97 A full description of an image in terms of a scene recipe would require both shape recipes and reﬂectance recipes (for computing reﬂectance values from  image data), which also requires labelling parts of the image as being caused by shading or reﬂectance changes, such as [15]. [sent-169, score-2.353]
</p><p>98 Using shape recipes, we ﬁnd simple transformation rules that let us convert from image to shape whenever we need to, by examining the image. [sent-171, score-1.309]
</p><p>99 We thank Ray Jones and Leonard McMillan for providing Cyberware scans, and Hao Zhang for code for rectiﬁcation of stereo images. [sent-172, score-0.344]
</p><p>100 A cooperative algorithm for stereo matching and occlusion detection. [sent-311, score-0.388]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shape', 0.547), ('recipes', 0.461), ('stereo', 0.344), ('zk', 0.28), ('subband', 0.266), ('image', 0.215), ('ik', 0.152), ('pyramid', 0.131), ('material', 0.122), ('recipe', 0.103), ('scene', 0.081), ('bandpassed', 0.071), ('fk', 0.07), ('sk', 0.069), ('lighting', 0.067), ('steerable', 0.061), ('surface', 0.059), ('segmentation', 0.056), ('shading', 0.055), ('rk', 0.052), ('subbands', 0.051), ('segmenting', 0.047), ('occlusion', 0.044), ('rendering', 0.04), ('convolution', 0.038), ('discontinuity', 0.037), ('lightness', 0.037), ('dzk', 0.035), ('torralba', 0.035), ('bandpass', 0.035), ('spatial', 0.034), ('relationship', 0.034), ('rendered', 0.033), ('images', 0.032), ('resolution', 0.031), ('shapes', 0.031), ('intensity', 0.03), ('transforming', 0.03), ('texture', 0.027), ('estimate', 0.026), ('ectance', 0.026), ('local', 0.026), ('reconstruction', 0.025), ('misses', 0.025), ('depth', 0.024), ('atmospheres', 0.024), ('gilchrist', 0.024), ('materials', 0.024), ('octaves', 0.024), ('paint', 0.024), ('phong', 0.024), ('zitnick', 0.024), ('regularizing', 0.023), ('recti', 0.023), ('laser', 0.023), ('across', 0.023), ('pn', 0.022), ('vertical', 0.021), ('occlusions', 0.02), ('occluding', 0.02), ('quadrants', 0.02), ('lambertian', 0.02), ('plot', 0.02), ('visible', 0.019), ('details', 0.019), ('groups', 0.019), ('boundaries', 0.019), ('scale', 0.018), ('smoothness', 0.018), ('orientation', 0.018), ('representations', 0.018), ('regression', 0.018), ('variations', 0.018), ('textures', 0.017), ('burden', 0.017), ('eg', 0.017), ('cuts', 0.016), ('boundary', 0.016), ('representation', 0.016), ('laplacian', 0.016), ('conditions', 0.015), ('properties', 0.015), ('wavelet', 0.015), ('intensities', 0.015), ('describing', 0.015), ('region', 0.014), ('regularities', 0.014), ('regions', 0.014), ('learned', 0.014), ('reconstructing', 0.014), ('changes', 0.014), ('expect', 0.013), ('translate', 0.013), ('zhang', 0.013), ('encode', 0.013), ('diagonal', 0.013), ('freeman', 0.013), ('truth', 0.013), ('implicitly', 0.013), ('edges', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="182-tfidf-1" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><p>2 0.15152723 <a title="182-tfidf-2" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>Author: B. Caputo, Gy. Dorkó</p><p>Abstract: This paper presents a kernel method that allows to combine color and shape information for appearance-based object recognition. It doesn't require to define a new common representation, but use the power of kernels to combine different representations together in an effective manner. These results are achieved using results of statistical mechanics of spin glasses combined with Markov random fields via kernel functions. Experiments show an increase in recognition rate up to 5.92% with respect to conventional strategies. 1</p><p>3 0.14145054 <a title="182-tfidf-3" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>4 0.13838667 <a title="182-tfidf-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.13804743 <a title="182-tfidf-5" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>Author: Michael E. Tipping, Christopher M. Bishop</p><p>Abstract: The extraction of a single high-quality image from a set of lowresolution images is an important problem which arises in fields such as remote sensing, surveillance, medical imaging and the extraction of still images from video. Typical approaches are based on the use of cross-correlation to register the images followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution images, using regularization to resolve the ill-posed nature of the inversion process. In this paper we develop a Bayesian treatment of the super-resolution problem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown high-resolution image. This approach allows us to estimate the unknown point spread function, and is rendered tractable through the introduction of a Gaussian process prior over images. Results indicate a significant improvement over techniques based on MAP (maximum a-posteriori) point optimization of the high resolution image and associated registration parameters. 1</p><p>6 0.11669565 <a title="182-tfidf-6" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>7 0.11526042 <a title="182-tfidf-7" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>8 0.099478476 <a title="182-tfidf-8" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>9 0.098810531 <a title="182-tfidf-9" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>10 0.085661247 <a title="182-tfidf-10" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>11 0.084629677 <a title="182-tfidf-11" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>12 0.068572938 <a title="182-tfidf-12" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>13 0.067114413 <a title="182-tfidf-13" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>14 0.066709384 <a title="182-tfidf-14" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>15 0.057967518 <a title="182-tfidf-15" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>16 0.052024487 <a title="182-tfidf-16" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>17 0.05195836 <a title="182-tfidf-17" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>18 0.050507363 <a title="182-tfidf-18" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>19 0.049947731 <a title="182-tfidf-19" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>20 0.045874216 <a title="182-tfidf-20" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, 0.017), (2, 0.001), (3, 0.253), (4, 0.004), (5, -0.056), (6, 0.174), (7, 0.005), (8, 0.011), (9, -0.051), (10, -0.011), (11, -0.04), (12, -0.01), (13, -0.027), (14, -0.016), (15, -0.024), (16, -0.053), (17, -0.043), (18, -0.037), (19, 0.004), (20, 0.051), (21, -0.012), (22, -0.052), (23, -0.029), (24, 0.075), (25, -0.019), (26, -0.021), (27, 0.017), (28, 0.028), (29, -0.076), (30, -0.014), (31, -0.078), (32, 0.093), (33, -0.042), (34, 0.077), (35, 0.053), (36, 0.01), (37, -0.001), (38, 0.029), (39, 0.106), (40, 0.122), (41, -0.056), (42, 0.018), (43, 0.162), (44, 0.104), (45, 0.015), (46, -0.092), (47, 0.186), (48, 0.161), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97548443 <a title="182-lsi-1" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><p>2 0.63440013 <a title="182-lsi-2" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>Author: Marshall F. Tappen, William T. Freeman, Edward H. Adelson</p><p>Abstract: We present an algorithm that uses multiple cues to recover shading and reﬂectance intrinsic images from a single image. Using both color information and a classiﬁer trained to recognize gray-scale patterns, each image derivative is classiﬁed as being caused by shading or a change in the surface’s reﬂectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classiﬁcation is clear to areas where it is ambiguous. We also show results on real images.</p><p>3 0.58124435 <a title="182-lsi-3" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>Author: Kinh Tieu, Erik G. Miller</p><p>Abstract: In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the ﬂexibility of this color ﬂow model by allowing ﬂow coefﬁcients to vary according to a low order polynomial over the image. This allows us to better ﬁt smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.</p><p>4 0.56328225 <a title="182-lsi-4" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>Author: Michael E. Tipping, Christopher M. Bishop</p><p>Abstract: The extraction of a single high-quality image from a set of lowresolution images is an important problem which arises in fields such as remote sensing, surveillance, medical imaging and the extraction of still images from video. Typical approaches are based on the use of cross-correlation to register the images followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution images, using regularization to resolve the ill-posed nature of the inversion process. In this paper we develop a Bayesian treatment of the super-resolution problem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown high-resolution image. This approach allows us to estimate the unknown point spread function, and is rendered tractable through the introduction of a Gaussian process prior over images. Results indicate a significant improvement over techniques based on MAP (maximum a-posteriori) point optimization of the high resolution image and associated registration parameters. 1</p><p>5 0.50540179 <a title="182-lsi-5" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>Author: B. Caputo, Gy. Dorkó</p><p>Abstract: This paper presents a kernel method that allows to combine color and shape information for appearance-based object recognition. It doesn't require to define a new common representation, but use the power of kernels to combine different representations together in an effective manner. These results are achieved using results of statistical mechanics of spin glasses combined with Markov random fields via kernel functions. Experiments show an increase in recognition rate up to 5.92% with respect to conventional strategies. 1</p><p>6 0.4742097 <a title="182-lsi-6" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>7 0.37970009 <a title="182-lsi-7" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>8 0.36434782 <a title="182-lsi-8" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>9 0.36325306 <a title="182-lsi-9" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>10 0.35471871 <a title="182-lsi-10" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>11 0.3544704 <a title="182-lsi-11" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>12 0.3305907 <a title="182-lsi-12" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>13 0.3190504 <a title="182-lsi-13" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>14 0.31876951 <a title="182-lsi-14" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>15 0.31626007 <a title="182-lsi-15" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>16 0.28119403 <a title="182-lsi-16" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>17 0.27052647 <a title="182-lsi-17" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>18 0.26094267 <a title="182-lsi-18" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>19 0.23802872 <a title="182-lsi-19" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>20 0.23446552 <a title="182-lsi-20" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.013), (3, 0.013), (11, 0.019), (23, 0.017), (42, 0.04), (45, 0.017), (54, 0.088), (55, 0.021), (61, 0.382), (64, 0.011), (67, 0.014), (68, 0.018), (74, 0.141), (92, 0.025), (98, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83330411 <a title="182-lda-1" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><p>2 0.6242153 <a title="182-lda-2" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>Author: J. L. Shapiro</p><p>Abstract: Population-based Incremental Learning is shown require very sensitive scaling of its learning rate. The learning rate must scale with the system size in a problem-dependent way. This is shown in two problems: the needle-in-a haystack, in which the learning rate must vanish exponentially in the system size, and in a smooth function in which the learning rate must vanish like the square root of the system size. Two methods are proposed for removing this sensitivity. A learning dynamics which obeys detailed balance is shown to give consistent performance over the entire range of learning rates. An analog of mutation is shown to require a learning rate which scales as the inverse system size, but is problem independent. 1</p><p>3 0.43686074 <a title="182-lda-3" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><p>4 0.42615581 <a title="182-lda-4" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>5 0.42545739 <a title="182-lda-5" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>6 0.42374471 <a title="182-lda-6" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>7 0.42338401 <a title="182-lda-7" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>8 0.42102876 <a title="182-lda-8" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>9 0.41833931 <a title="182-lda-9" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>10 0.41501191 <a title="182-lda-10" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>11 0.41499087 <a title="182-lda-11" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>12 0.4136802 <a title="182-lda-12" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>13 0.41356325 <a title="182-lda-13" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>14 0.41136461 <a title="182-lda-14" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>15 0.41109329 <a title="182-lda-15" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>16 0.40903169 <a title="182-lda-16" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>17 0.40897834 <a title="182-lda-17" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>18 0.4076021 <a title="182-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.40312684 <a title="182-lda-19" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>20 0.40258858 <a title="182-lda-20" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
