<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-81" href="#">nips2002-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</h1>
<br/><p>Source: <a title="nips-2002-81-pdf" href="http://papers.nips.cc/paper/2246-expected-and-unexpected-uncertainty-ach-and-ne-in-the-neocortex.pdf">pdf</a></p><p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of speciﬁc sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we reﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.</p><p>Reference: <a title="nips-2002-81-reference" href="../nips2002_reference/nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of speciﬁc sorts of variability. [sent-8, score-0.045]
</p><p>2 Here, we reﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. [sent-10, score-0.324]
</p><p>3 We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. [sent-11, score-0.218]
</p><p>4 We illustrate this proposal using an adaptive factor analysis model. [sent-12, score-0.028]
</p><p>5 1 Introduction Animals negotiating rich environments are faced with a set of hugely complex inference and learning problems, involving many forms of variability. [sent-13, score-0.085]
</p><p>6 They can be unsure which context presently pertains, cues can be systematically more or less reliable, and relationships amongst cues can change smoothly or abruptly. [sent-14, score-0.087]
</p><p>7 There is ample behavioral evidence that can be interpreted as suggesting that animals do make and respect these distinctions,5 and there is even some anatomical, physiological and pharmacological evidence as to which neural systems are engaged. [sent-16, score-0.131]
</p><p>8 29 Perhaps best delineated is the involvement of neocortical acetylcholine (ACh) in uncertainty. [sent-17, score-0.054]
</p><p>9 Following seminal earlier work,11, 14 we suggested6, 35 that ACh reports on the uncertainty associated with a top-down model, and thus controls the integration of bottom-up and top-down information during inference. [sent-18, score-0.218]
</p><p>10 Intuitively, this cholinergic signal reports on expected uncertainty, such that ACh levels are high when top-down information is not expected to support good predictions about bottom-up data and should be modiﬁed according to the incoming data. [sent-20, score-0.244]
</p><p>11 We6, 35 formally demonstrated the inference aspects of this idea using a hidden Markov model (HMM), in which top-down uncertainty derives from slow contextual changes. [sent-21, score-0.308]
</p><p>12 That is, in the HMM model, greater uncertainty in the topdown model (ie a lower posterior responsibility for the predominant context), reported by higher ACh levels, leads to comparatively slower learning about that context. [sent-23, score-0.176]
</p><p>13 By contrast, we had expected that higher ACh should lead to faster learning, since it would indicate  that the top-down model is potentially inadequate. [sent-24, score-0.046]
</p><p>14 As a further consequence, by thinking more generally about contextual change, we also realized the formal need for a signal reporting on unexpected uncertainty, that is, on strong violation of top-down predictions that are expected to be correct. [sent-26, score-0.208]
</p><p>15 There is suggestive empirical evidence that one of many roles for neocortical norepinephrine (NE) is reporting this;29 it is also consonant with various existing theories associated with NE. [sent-27, score-0.183]
</p><p>16 In sum, we suggest that expected and unexpected uncertainty play complementary but distinct roles in representational inference and learning. [sent-28, score-0.457]
</p><p>17 Both forms of uncertainties are postulated to decrease the inﬂuence of top-down information on representational inference and increase the rate of learning. [sent-29, score-0.111]
</p><p>18 However, unexpected uncertainty rises whenever there is a global change in the world, such as a context change, while expected uncertainty is a more subtle quantity dependent on internal representations of properties of the world. [sent-30, score-0.491]
</p><p>19 Here, we start by outlining some of the evidence for the individual and joint roles of ACh and NE in uncertainty. [sent-31, score-0.074]
</p><p>20 In section 3, we describe a simple, adaptive, factor analysis model that clariﬁes the uncertainty notions. [sent-32, score-0.154]
</p><p>21 Differential effects induced by disrupting ACh and NE are discussed in Section 4, accompanied by a comparison to impairments found in animals. [sent-33, score-0.165]
</p><p>22 Cortical innervations of these modulators are extensive, targeting all cortical regions and layers. [sent-35, score-0.115]
</p><p>23 9, 30 As is typical for neuromodulators, physiological studies indicate that the effects of direct application of ACh or NE are confusingly diverse. [sent-36, score-0.093]
</p><p>24 Within a small cortical area, iontophoresis or perfusion of ACh or NE (or their agonists) may cause synatic facilitation or suppression, depending on the cell and depending on whether the ﬁring is spontaneous or stimulusevoked; it may also induce direct hyperpolarization or depolarization. [sent-37, score-0.071]
</p><p>25 8, 12, 13, 15, 17, 18, 20 Based on these roughly similar anatomical and physiological properties, cholinergic and noradrenergic systems have been attributed correspondingly similar general computational roles, such as modulating the signal-to-noise ratio in sensory processing. [sent-40, score-0.188]
</p><p>26 9, 10 However, the effects of ACh and NE depletion in animal behavioral studies, as well as microdialysis of the neuromodulators during different conditions, point to more speciﬁc and distinct computational roles for ACh and NE. [sent-41, score-0.316]
</p><p>27 In our previous work on ACh, 6, 35 we suggested that it reports on expected uncertainty, ie uncertainty associated with estimated parameters in an internal model of the external world. [sent-42, score-0.308]
</p><p>28 This is consistent with results from animal conditioning experiments, in which animals learn faster about stimuli with variable predictive consequences. [sent-43, score-0.142]
</p><p>29 24 A series of lesion studies indicates cortical ACh innervation is essential for this sort of faster learning. [sent-44, score-0.12]
</p><p>30 19, 28 Moreover, this activation of NE neurons habituates rapidly when there is no predictive value or contingent response associated with the stimuli, and also disappears when conditioning is expressed at a behavioral level. [sent-47, score-0.109]
</p><p>31 28 There are few sophisticated behavioral studies into the interactions between ACh and NE. [sent-48, score-0.081]
</p><p>32 However, it is known that NE and ACh both rise when contingencies in an operant conditioning task are changed, but while NE level rapidly habituates, ACh level is elevated in a more sustained fashion. [sent-49, score-0.069]
</p><p>33 3, 28 In a task designed to tax sustained attention, lesions of the basal forebrain cholinergic neurons induced persistent impairments, 22 while deafferentation of cortical adrenergic inputs did not result in signiﬁcant impairment compared to controls. [sent-50, score-0.27]
</p><p>34 However, its focus on well-learned tasks, means that other drives of NE activity (particularly novelty) and effects (particularly plasticity) are downplayed, and a link to ACh is only a secondary concern. [sent-54, score-0.034]
</p><p>35 We focus on these latter aspects, proposing that NE reports unexpected uncertainty, ie uncertainty induced by a mismatch between prediction and observation, such as when there is a dramatic change in the external environment. [sent-55, score-0.436]
</p><p>36 We suggested that ACh reports the uncertainty in the top-down context, , where is the most likely value of the context and namely indicates the use of an approximation. [sent-60, score-0.26]
</p><p>37 ACh thereby reports expected uncertainty, as in the qualitative picture above, and appropriately controls cortical inference. [sent-61, score-0.207]
</p><p>38 However, if one also considers learning, for instance if is unknown, then the less certain the animal is that is the true contextual state, the less learning accorded to . [sent-62, score-0.075]
</p><p>39 In fact, this way of viewing ACh is also not consistent with a more systematic reading 5, 16 of Holland & Gallagher’s cholinergic results,14 which imply that ACh is better seen as a report of uncertainty in parameters rather than uncertainty in states. [sent-64, score-0.41]
</p><p>40 In order to model this more ﬁtting picture of ACh, we need an explicit model of parameter uncertainty. [sent-65, score-0.028]
</p><p>41 We constrain the problem to a single, implicit, context . [sent-66, score-0.042]
</p><p>42 It is easiest (and perhaps more realistic) to develop the new picture in a continuous space, in which the parameter governing the relationship between and is (scalar for convenience), which is imperfectly known (hence the parameter uncertainty, reported by ACh), and indeed can change. [sent-67, score-0.028]
</p><p>43 In particular, novelty plays a critical role in model evolution. [sent-70, score-0.092]
</p><p>44 (a) 2-layer adaptive factor analysis model, as speciﬁed by Eq. [sent-75, score-0.028]
</p><p>45 : major shifts in , : , : , : optimally projected into space, ie , where is the mean of the posterior distribution of given only the observation and ﬂat priors. [sent-82, score-0.068]
</p><p>46 Larger corresponds to greater reliance on rather than for inferring , while the intermediate value of exactly balances top-down uncertainty with bottom-up uncertainty in the inference of . [sent-86, score-0.393]
</p><p>47 However, in order to allow for the possibility of macroscopic changes implied by substantial novelty (as reported by NE), which are of evident importance in many experiments, we must add a speciﬁc component to the model. [sent-88, score-0.138]
</p><p>48 The interaction between microscopic and macroscopic novelty is essentially the interaction between ACh and NE. [sent-89, score-0.094]
</p><p>49 We will see later that the binary is ) the key to the model of NE; it comes from an assumption that there can occasionally ( be dramatic changes in a model that force its radical revision. [sent-92, score-0.094]
</p><p>50 We ﬁrst consider how the ACh term inﬂuences inference about ; then go on to study learning. [sent-97, score-0.085]
</p><p>51 The more uncertainty (ie the larger ), the smaller the role of the top-down expectation in determining . [sent-99, score-0.181]
</p><p>52 Examples of just such effects can be found in Figure 1 (d). [sent-100, score-0.034]
</p><p>53 Importantly, in this simple model, the uncertainty in does not depend on the prediction errors , but rather changes as a function only of time. [sent-104, score-0.226]
</p><p>54 However, if one takes into account the possibility that , then the posterior distribution for is the two-component mixture `  ¡ 0  G ¨   C  q {  C r  q z  r   o9 y ~  ¥ 2  |  r Q  q  Cz  ` ¡ qmm m H ¡ 0   u  ~ z C q r H9 y ¥ 2  G ¨  u  0 ! [sent-105, score-0.029]
</p><p>55  ` 9  ¥  G ¨  u ¥ ¦¤    |     H G  r U y  ¨    6   g mm m  u     G mm m  9  Wm mm V 0 ` 9  X ¥  0   m m m #  V ¨  u ¦¤ ¨ ¥  y        2 ¢ mmm ¥  0 ! [sent-106, score-0.302]
</p><p>56 Thus, just as for switching state-space models,7 exact inference is impractical. [sent-108, score-0.085]
</p><p>57 is our best after observing , and , corresponding to the ACh level, is the estimate of uncertainty in our estimate . [sent-112, score-0.154]
</p><p>58 In general, we might consider the NE level as reporting component of the equivalent mixture of equation 4. [sent-113, score-0.065]
</p><p>59 the posterior responsibility of the Even more straightforwardly, we can measure a Z-score, namely prediction error scaled by uncertainty in our estimates: , where and , assuming that . [sent-114, score-0.204]
</p><p>60 Whenever exceeds a threshold is unlikely to have come from an unmodiﬁed version of the current comvalue , ie ponent, we assume . [sent-115, score-0.068]
</p><p>61 In the learning algorithm, large uncertainty about the mean estimate, , results in large Kalman gain, , which causes a large shift in . [sent-120, score-0.154]
</p><p>62 Large also weakens the inﬂuence of top-down information in inference as in equation 3. [sent-121, score-0.085]
</p><p>63 High NE levels also leads to faster learning: large means , which causes (rather than had been ), ultimately resulting in a large Kalman gain and thus fast shifting of . [sent-122, score-0.058]
</p><p>64 High NE levels also enhances the dominance of bottom-up information in inference via its interactions with ACh: large promotes large . [sent-123, score-0.119]
</p><p>65 Note that this system predicts interesting reciprocal relationships between ACh and NE: higher ACh leads to smaller normalized prediction errors and therefore less active NE signalling, whereas greater NE would generally increase estimator uncertainty and thus ACh level. [sent-124, score-0.182]
</p><p>66 ACh level rises whenver detected to be (NE level exceeds ) and then smoothly falls. [sent-131, score-0.045]
</p><p>67       l   se       th   e  2 k     C       e       se    l ( 0§  l }2   e     s 0  e          ) #)      ) ) 07    l C F6l  ¡ `  more subtle changes in can miss detection, such as the third large shift in . [sent-137, score-0.104]
</p><p>68 Figure 2(b) shows higher ACh ( ) and NE ( levels both correspond to fast learning, ie fast shifting of . [sent-138, score-0.102]
</p><p>69 However, whereas NE is a constant monitor of prediction errors and ﬂuctuates accordingly with every data point, ACh falls smoothly and predictably, and only depends on the observations when global changes in the environment have been detected. [sent-139, score-0.095]
</p><p>70 `    ¡ ©    ¡    ¡ `   ` H `    4 Differential Effects of Disrupting ACh and NE Signalling   ¡  ¡ ©  The different roles of the NE ( ) and ACh ( ) can be teased apart by disrupting each and observing the subsequent effects on learning in our model. [sent-142, score-0.181]
</p><p>71 We will examine several different manipulations of and that disrupt normal learning, and relate the results to impairments observed in experimental manipulation of ACh or NE levels in animals. [sent-143, score-0.12]
</p><p>72 ¡  y  ¨ ¡ ©    ¡ ©  First, we simulate depletion of cortical NE by setting . [sent-145, score-0.129]
</p><p>73 By ruling out the possibility of , the system is unable to cope with abrupt, global changes in the world, ie when shifts. [sent-147, score-0.112]
</p><p>74 This is consistent with the large errors of similar magnitude in Figure 2(c) for very large , which effectively blocks the NE system from reporting global changes. [sent-149, score-0.065]
</p><p>75 However, as long as the underlying parameters remain the same, ie does not change greatly, the inference process functions normally, as we can see in the ﬁrst steps in Figure 3(a). [sent-150, score-0.175]
</p><p>76 These results are consistent with experimental observations: NE-lesioned animals are impaired in learning changes in reinforcement contingencies,26, 28 but have little difﬁculty doing previously learned discrimination tasks. [sent-151, score-0.114]
</p><p>77 We can also simulate depletion of cortical ACh by setting Figure 3(b) shows severe damage is caused the learning algorithm, but the inference symptoms are distinct from NE depletion. [sent-154, score-0.235]
</p><p>78 However, because the NE system is still intact, the system is able to detect when dramatically differs from the prediction (which is often, since is slow to adapt and leaves little room for variance), and thus to base inference of directly on the bottom-up information . [sent-156, score-0.136]
</p><p>79 Thus, inference is less impaired than learning, which has also been observed in   ¡  ¡  )    ¡ `  `  ¡ )  ¡ 0 ¡ )¥    ¤ ¦ ¨§¡  10 0 1  35  70  1  35  70  ¤ ¢ ¥£¡  10 0    Figure 3: Disrupting NE and ACh signals. [sent-157, score-0.114]
</p><p>80 Learning of is poor in both manipulations, but inference in ACh-depletion is less impaired. [sent-161, score-0.085]
</p><p>81 31 Moreover, the system exhibits a peculiar hesitancy in inference, ie constantly switching back and forth between relying on top-down estimate of , based on and bottom-up estimate, based on . [sent-163, score-0.068]
</p><p>82 Interestingly, hippocampal cholinergic deafferentation in animals also bring about a stronger susceptibility to interference compared with controls. [sent-165, score-0.172]
</p><p>83 10 ¡ )  mmm  ¡ ©  ¡   V     mm m   ¡ 0   6     mm m  ¡ `  ¡ `    and very high all the Saturation of ACh and NE are also easy to model, by setting time. [sent-166, score-0.211]
</p><p>84 The effect of these two manipulations are similar, both cause the estimation of and inference of to base strongly on the observation (data not shown). [sent-167, score-0.113]
</p><p>85 The performance decrements in the estimation of and inference about are functions of the output in our model, and do not worsen when there are global changes in continnoise, gencies. [sent-168, score-0.129]
</p><p>86 Administration of cholinergic agonists in the cortex has failed to induce impairments in tasks with changing contingencies, consistent with our predictions. [sent-170, score-0.198]
</p><p>87 However, to our knowledge, cholinergic and noradrenergic agonists have not yet been administered in combination with systematic manipulation of variability in the predictive consequences of stimuli and so the validity of our predictions remains to be tested. [sent-171, score-0.193]
</p><p>88 ¡ `  ¡ )  ¡ 0  ¡ `  ¡ )  C5  q H9 xn p  5 Discussion We have suggested that ACh and NE report expected and unexpected uncertainty in representational learning and inference. [sent-172, score-0.277]
</p><p>89 As such, high levels of ACh and NE should both correspond to faster learning about the environment and enhancement of bottom-up processing in inference. [sent-173, score-0.058]
</p><p>90 However, whereas NE reports on dramatic changes, ACh has the subtler role of reporting on uncertainties in internal estimates. [sent-174, score-0.181]
</p><p>91 We formalized these ideas in an adaptive factor analysis model. [sent-175, score-0.028]
</p><p>92 The model is adaptive in that the mean of the hidden variable is allowed to alter greatly from time to time, capturing the idea of a generally stable context which occasionally undergoes large changes, leading to substantial novelty in inputs. [sent-176, score-0.135]
</p><p>93 As exact learning is intractable, we proposed an approximate learning algorithm in which the roles for ACh and NE are clear, and demonstrated that it performs learning and inference competently. [sent-177, score-0.159]
</p><p>94 Moreover, by disrupting one or both of ACh and NE signalling systems, we showed that the two systems have interacting but distinct patterns of malfunctioning that qualitatively resemble experimental results in animal studies. [sent-178, score-0.161]
</p><p>95 There is no single collection of deﬁnitive experimental studies, and teasing apart the effects of NE and ACh is tricky, since they appear to share many properties. [sent-179, score-0.034]
</p><p>96 In particular, it only considers one particular context; and so refers all the uncertainty to the parameters of that context. [sent-182, score-0.154]
</p><p>97 This is exactly the complement of our previous model, 6, 35 which referred all the uncertainty to the choice of context rather than the parameters within each context. [sent-183, score-0.196]
</p><p>98 The main conceptual difference is that the idea that ACh reports on the latter form of contextual uncertainty sits ill with the data on how uncertainty boosts learning; this ﬁts better within the present model. [sent-184, score-0.418]
</p><p>99 Given multiple contexts, which could formally be handled within the framework of a mixture model, the tricky issue is to decide whether the parameters of the current context have changed, or a new (or pre-existing) context has taken over. [sent-185, score-0.113]
</p><p>100 More generally, a thoroughly hierarchical and non-linear model is clearly required as at a minimum as a way of addressing some of the complexities of cortical inference. [sent-187, score-0.071]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ach', 0.858), ('uncertainty', 0.154), ('ne', 0.117), ('cholinergic', 0.102), ('vu', 0.096), ('mm', 0.091), ('inference', 0.085), ('unexpected', 0.075), ('roles', 0.074), ('disrupting', 0.073), ('cortical', 0.071), ('ie', 0.068), ('reporting', 0.065), ('novelty', 0.065), ('reports', 0.064), ('depletion', 0.058), ('impairments', 0.058), ('behavioral', 0.056), ('dayan', 0.054), ('kalman', 0.047), ('hasselmo', 0.046), ('contextual', 0.046), ('changes', 0.044), ('behav', 0.044), ('coeruleus', 0.044), ('locus', 0.044), ('neuromodulators', 0.044), ('norepinephrine', 0.044), ('robbins', 0.044), ('sarter', 0.044), ('targeting', 0.044), ('context', 0.042), ('animals', 0.041), ('signalling', 0.038), ('tw', 0.038), ('agonists', 0.038), ('sara', 0.038), ('kakade', 0.035), ('levels', 0.034), ('physiological', 0.034), ('effects', 0.034), ('qh', 0.032), ('brain', 0.032), ('neurosci', 0.032), ('sj', 0.031), ('yu', 0.03), ('se', 0.03), ('animal', 0.029), ('acetylcholine', 0.029), ('bull', 0.029), ('deafferentation', 0.029), ('gallagher', 0.029), ('habituates', 0.029), ('holland', 0.029), ('impaired', 0.029), ('macroscopic', 0.029), ('mcgaughy', 0.029), ('mmm', 0.029), ('noradrenergic', 0.029), ('qmm', 0.029), ('rajkowski', 0.029), ('tricky', 0.029), ('bj', 0.029), ('adaptive', 0.028), ('prediction', 0.028), ('hmm', 0.028), ('manipulations', 0.028), ('picture', 0.028), ('role', 0.027), ('representational', 0.026), ('radical', 0.025), ('vankov', 0.025), ('tonic', 0.025), ('forebrain', 0.025), ('involvement', 0.025), ('nuclei', 0.025), ('studies', 0.025), ('dramatic', 0.025), ('faster', 0.024), ('conditioning', 0.024), ('stimuli', 0.024), ('smoothly', 0.023), ('everitt', 0.023), ('neocortex', 0.023), ('phasic', 0.023), ('sustained', 0.023), ('sensory', 0.023), ('res', 0.023), ('slow', 0.023), ('change', 0.022), ('qualitative', 0.022), ('expected', 0.022), ('sorts', 0.022), ('responsibility', 0.022), ('rises', 0.022), ('contingencies', 0.022), ('hippocampus', 0.022), ('psychological', 0.021), ('distinct', 0.021), ('basal', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="81-tfidf-1" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of speciﬁc sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we reﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.</p><p>2 0.072559066 <a title="81-tfidf-2" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>Author: Patrik O. Hoyer, Aapo Hyvärinen</p><p>Abstract: The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signiﬁcantly from trial to trial. This variability is most often interpreted as ‘noise’, purely detrimental to the sensory system. In this paper, we propose an alternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speciﬁcally, the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representational scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.</p><p>3 0.05981623 <a title="81-tfidf-3" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>Author: Nathaniel D. Daw, Aaron C. Courville, David S. Touretzky</p><p>Abstract: According to a series of inﬂuential models, dopamine (DA) neurons signal reward prediction error using a temporal-difference (TD) algorithm. We address a problem not convincingly solved in these accounts: how to maintain a representation of cues that predict delayed consequences. Our new model uses a TD rule grounded in partially observable semi-Markov processes, a formalism that captures two largely neglected features of DA experiments: hidden state and temporal variability. Previous models predicted rewards using a tapped delay line representation of sensory inputs; we replace this with a more active process of inference about the underlying state of the world. The DA system can then learn to map these inferred states to reward predictions using TD. The new model can explain previously vexing data on the responses of DA neurons in the face of temporal variability. By combining statistical model-based learning with a physiologically grounded TD theory, it also brings into contact with physiology some insights about behavior that had previously been conﬁned to more abstract psychological models.</p><p>4 0.059813201 <a title="81-tfidf-4" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>Author: Peter Dayan, Maneesh Sahani, Gregoire Deback</p><p>Abstract: Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety. In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning. We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination.</p><p>5 0.058581088 <a title="81-tfidf-5" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classiﬁcation that combines the computational advantage of a parametric solution with the ﬂexibility of sequential sampling techniques. We regard the parameters of the classiﬁer as latent states in a ﬁrst order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman ﬁltering. The variational Kalman ﬁlter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classiﬁers both in stationary and non-stationary environments. Although we focus on classiﬁcation, the algorithm is easily extended to other generalized nonlinear models.</p><p>6 0.053425916 <a title="81-tfidf-6" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>7 0.051476955 <a title="81-tfidf-7" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>8 0.04991854 <a title="81-tfidf-8" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>9 0.048495427 <a title="81-tfidf-9" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>10 0.047274657 <a title="81-tfidf-10" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>11 0.04192327 <a title="81-tfidf-11" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>12 0.040906463 <a title="81-tfidf-12" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>13 0.040873196 <a title="81-tfidf-13" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>14 0.03683814 <a title="81-tfidf-14" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>15 0.036066368 <a title="81-tfidf-15" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>16 0.034783967 <a title="81-tfidf-16" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>17 0.033922605 <a title="81-tfidf-17" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>18 0.03357327 <a title="81-tfidf-18" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>19 0.032400101 <a title="81-tfidf-19" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>20 0.032268248 <a title="81-tfidf-20" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.111), (1, 0.055), (2, -0.015), (3, 0.001), (4, -0.023), (5, 0.007), (6, -0.058), (7, 0.011), (8, 0.029), (9, 0.017), (10, -0.011), (11, -0.004), (12, 0.079), (13, 0.02), (14, -0.027), (15, -0.034), (16, -0.035), (17, 0.029), (18, 0.011), (19, -0.053), (20, 0.021), (21, -0.048), (22, 0.028), (23, 0.057), (24, -0.026), (25, -0.025), (26, 0.116), (27, -0.062), (28, 0.007), (29, 0.01), (30, -0.011), (31, 0.065), (32, 0.042), (33, -0.028), (34, 0.051), (35, -0.096), (36, 0.032), (37, -0.072), (38, 0.012), (39, -0.086), (40, 0.046), (41, 0.147), (42, 0.103), (43, 0.049), (44, 0.051), (45, 0.04), (46, 0.094), (47, 0.019), (48, 0.098), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91369456 <a title="81-lsi-1" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of speciﬁc sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we reﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.</p><p>2 0.60202253 <a title="81-lsi-2" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>Author: Peter Dayan, Maneesh Sahani, Gregoire Deback</p><p>Abstract: Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety. In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning. We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination.</p><p>3 0.46842092 <a title="81-lsi-3" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>4 0.46585003 <a title="81-lsi-4" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>Author: E. Solak, R. Murray-smith, W. E. Leithead, D. J. Leith, Carl E. Rasmussen</p><p>Abstract: Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.</p><p>5 0.45604041 <a title="81-lsi-5" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<p>Author: Aaron J. Gruber, Sara A. Solla, James C. Houk</p><p>Abstract: Single unit activity in the striatum of awake monkeys shows a marked dependence on the expected reward that a behavior will elicit. We present a computational model of spiny neurons, the principal neurons of the striatum, to assess the hypothesis that direct neuromodulatory effects of dopamine through the activation of D 1 receptors mediate the reward dependency of spiny neuron activity. Dopamine release results in the amplification of key ion currents, leading to the emergence of bistability, which not only modulates the peak firing rate but also introduces a temporal and state dependence of the model's response, thus improving the detectability of temporally correlated inputs. 1</p><p>6 0.45204136 <a title="81-lsi-6" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>7 0.45033371 <a title="81-lsi-7" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>8 0.43878368 <a title="81-lsi-8" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>9 0.43268305 <a title="81-lsi-9" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>10 0.39997306 <a title="81-lsi-10" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>11 0.38768497 <a title="81-lsi-11" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>12 0.38666496 <a title="81-lsi-12" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>13 0.34154961 <a title="81-lsi-13" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>14 0.33077005 <a title="81-lsi-14" href="./nips-2002-Prediction_of_Protein_Topologies_Using_Generalized_IOHMMs_and_RNNs.html">164 nips-2002-Prediction of Protein Topologies Using Generalized IOHMMs and RNNs</a></p>
<p>15 0.32635289 <a title="81-lsi-15" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>16 0.31753466 <a title="81-lsi-16" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>17 0.31582221 <a title="81-lsi-17" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>18 0.31422123 <a title="81-lsi-18" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>19 0.30988747 <a title="81-lsi-19" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>20 0.30621088 <a title="81-lsi-20" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.011), (23, 0.039), (37, 0.278), (42, 0.05), (54, 0.083), (55, 0.054), (57, 0.026), (64, 0.017), (67, 0.071), (68, 0.042), (74, 0.072), (79, 0.011), (87, 0.014), (92, 0.025), (98, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78889197 <a title="81-lda-1" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of speciﬁc sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we reﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.</p><p>2 0.77969331 <a title="81-lda-2" href="./nips-2002-Dynamical_Causal_Learning.html">75 nips-2002-Dynamical Causal Learning</a></p>
<p>Author: David Danks, Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning. This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 1</p><p>3 0.62353933 <a title="81-lda-3" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>Author: E. Solak, R. Murray-smith, W. E. Leithead, D. J. Leith, Carl E. Rasmussen</p><p>Abstract: Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.</p><p>4 0.54251546 <a title="81-lda-4" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>Author: Peter Dayan, Maneesh Sahani, Gregoire Deback</p><p>Abstract: Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety. In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning. We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination.</p><p>5 0.5299952 <a title="81-lda-5" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>6 0.52544832 <a title="81-lda-6" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>7 0.52512783 <a title="81-lda-7" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>8 0.52373123 <a title="81-lda-8" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>9 0.52285659 <a title="81-lda-9" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>10 0.52284467 <a title="81-lda-10" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>11 0.52275717 <a title="81-lda-11" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>12 0.52194721 <a title="81-lda-12" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>13 0.52135241 <a title="81-lda-13" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>14 0.52134061 <a title="81-lda-14" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>15 0.51833969 <a title="81-lda-15" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>16 0.518053 <a title="81-lda-16" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>17 0.5178467 <a title="81-lda-17" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>18 0.51569045 <a title="81-lda-18" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>19 0.51443559 <a title="81-lda-19" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>20 0.51432437 <a title="81-lda-20" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
