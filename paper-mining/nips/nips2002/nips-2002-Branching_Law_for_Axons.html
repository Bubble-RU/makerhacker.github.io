<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 nips-2002-Branching Law for Axons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-47" href="#">nips2002-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 nips-2002-Branching Law for Axons</h1>
<br/><p>Source: <a title="nips-2002-47-pdf" href="http://papers.nips.cc/paper/2265-branching-law-for-axons.pdf">pdf</a></p><p>Author: Dmitri B. Chklovskii, Armen Stepanyants</p><p>Abstract: What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d</p><p>Reference: <a title="nips-2002-47-reference" href="../nips2002_reference/nips-2002-Branching_Law_for_Axons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract What determines the caliber of axonal branches? [sent-5, score-0.576]
</p><p>2 We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. [sent-6, score-0.859]
</p><p>3 We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d " a b ranc hmg 1aw: d0 + 2 = ]v + 2 + d 2 + 2 . [sent-7, score-0.572]
</p><p>4 The denvatIOn re l' on th e les fact that the conduction speed scales with the axon diameter to the power V (v = 1 for myelinated axons and V = 0. [sent-8, score-1.028]
</p><p>5 We test the branching law on the available experimental data and find a reasonable agreement. [sent-10, score-0.628]
</p><p>6 1 Introduction Multi-cellular organisms have solved the problem of efficient transport of nutrients and communication between their body parts by evolving spectacular networks: trees, blood vessels, bronchs, and neuronal arbors. [sent-11, score-0.123]
</p><p>7 These networks consist of segments bifurcating into thinner and thinner branches. [sent-12, score-0.151]
</p><p>8 Understanding of branching in transport networks has been advanced through the application of the optimization theory ([1], [2] and references therein) . [sent-13, score-0.39]
</p><p>9 Here we apply the optimization theory to explain the caliber of branching segments in communication networks , i. [sent-14, score-0.632]
</p><p>10 Axons in different organisms vary in caliber from O. [sent-17, score-0.262]
</p><p>11 ll1m (terminal segments in neocortex) to lOOOl1m (squid giant axon) [3]. [sent-18, score-0.049]
</p><p>12 What factors could be responsible for such variation in axon caliber? [sent-19, score-0.116]
</p><p>13 According to the experimental data [4] and cable theory [5], thicker axons conduct action potential faster, leading to shorter reaction times and, perhaps, quicker thinking. [sent-20, score-0.292]
</p><p>14 This increases evolutionary fitness or, equivalently, reduces costs associated with conduction delays. [sent-21, score-0.428]
</p><p>15 It is likely that thick axons are evolutionary costly because they require large amount of cytoplasm and occupy valuable space [6], [7]. [sent-23, score-0.324]
</p><p>16 Then, is there an optimal axon caliber, which minimizes the combined cost of conduction delays and volume? [sent-24, score-0.654]
</p><p>17 In this paper we derive an expression for the optimal axon diameter, which minimizes the combined cost of conduction delay and volume. [sent-25, score-0.749]
</p><p>18 Although the relative cost of del ay and volume is unknown, we use this expression to derive a law describing segment caliber of branching axons with no free parameters. [sent-26, score-1.54]
</p><p>19 We test this law on the published anatomical data and find a satisfactory agreement. [sent-27, score-0.248]
</p><p>20 2 Derivation of the branching law Although our theory holds for a rather general class of cost functions (see Methods), we start, for the sake of simplicity, by deriving the branching law in a special case of a linear cost function. [sent-28, score-1.398]
</p><p>21 Detrimental contribution to fitness , It , of an axonal segment of length , L , can be represented as the sum of two terms , one proportional to the conduction delay along the segment, T, and the other - to the segment volume, V:  It =aT+ jJV. [sent-29, score-1.445]
</p><p>22 (1)  Here, a and f3 are unknown but constant coefficients which reflect the rel ative contribution to the fitness cost of the signal propagation delay and the axonal volume. [sent-30, score-0.904]
</p><p>23 5  4  diameter, d Figure 1: Fitness cost of a myelinated axonal segment as a function of its diameter. [sent-44, score-0.936]
</p><p>24 The lines show the volume cost, the delay cost, and the total cost. [sent-45, score-0.225]
</p><p>25 Diameter and cost values are normalized to their respective optimal values. [sent-47, score-0.145]
</p><p>26 We look for the axon caliber d that minimizes the cost function It. [sent-48, score-0.509]
</p><p>27 To do this, we rewrite It as a function of d by noticing the following relations: i) Volume,  V=! [sent-49, score-0.018]
</p><p>28 ; s  iii) Conduction velocity  s=kd for  myelinated axons (for non-myelinated axons, see Methods): (2)  This cost function contains two terms, which have opposite dependence on d, and has a minimum, Fig. [sent-61, score-0.635]
</p><p>29 a~  Next, by setting -  ad  =0  we find that the cost is minimized by the following axonal  caliber:  ( d=~) lrkfJ  1/3 (3)  The utility of this result may seem rather limited because the relative cost of time fJ ' is unknown. [sent-63, score-0.736]
</p><p>30 volume,  a/  Figure 2: A simple axonal arbor with a single branch point and three axonal segments. [sent-65, score-0.975]
</p><p>31 Time delays along each segment are " to, t" and t2. [sent-67, score-0.373]
</p><p>32 The total time delay down the first branch is T , =to +f" and the second T z=to +f2路  However, we can apply this result to axonal branching and arrive at a testable prediction about the relationship among branch diameters without knowing the relative cost. [sent-68, score-1.534]
</p><p>33 To do this we write the cost function for a bifurcation consisting of three segments, Fig. [sent-69, score-0.203]
</p><p>34 2: (4)  where to is a conduction delay along segment 0, t1 - conduction delay along segment 1,  t2 -  conduction delay along segment 2. [sent-70, score-2.214]
</p><p>35 Coefficients  a1  and  a2  represent relative costs of conduction delays for synapses located on the two daughter branches and may be different. [sent-71, score-0.474]
</p><p>36 We group the terms corresponding to the same segment together: (5)  We look for segment diameters , which minimize this cost function. [sent-72, score-0.948]
</p><p>37 To do this we make the dependence on the diameters explicit and differentiate in respect to them. [sent-73, score-0.31]
</p><p>38 (5) depends on the diameter of only one segment the variables separate and we arrive at expressions analogous to Eq. [sent-75, score-0.447]
</p><p>39 (3):  ( 2a J kfJn  I/3  d = I  l  '  ( Jif3  d = 2a2 2 k {In  (6)  It is easy to see that these diameters satisfy the following branching law:  dg = d? [sent-76, score-0.662]
</p><p>40 (7)  Similar expression can be derived for non-myelinated axons (see Methods) . [sent-78, score-0.269]
</p><p>41 In this case, the conduction velocity scales with the square root of segment diameter, resulting in a branching exponent of 2. [sent-79, score-1.194]
</p><p>42 (7) have been derived for blood vessels, tree branching and bronchs by balancing metabolic cost of pumping viscous fluid and volume cost [8], [9]. [sent-82, score-0.922]
</p><p>43 Application of viscous flow to dendrites has been discussed in [10]. [sent-83, score-0.108]
</p><p>44 However, it is hard to see how dendrites could be conduits to viscous fluid if their ends are sealed. [sent-84, score-0.15]
</p><p>45 Rail [11] has derived a similar law for branching dendrites by postulating impedance matching: (8)  However, the main purpose of Rail's law was to simplify calculations of dendritic conduction rather than to explain the actual branch caliber measurements. [sent-85, score-1.544]
</p><p>46 3  Comparison with experiment  We test our branching law, Eq. [sent-86, score-0.354]
</p><p>47 (7), by comparing it with the data obtained from myelinated motor fibers of the cat [12] , Fig. [sent-87, score-0.223]
</p><p>48 Data points represent 63 branch points for which all three axonal calibers were available. [sent-89, score-0.519]
</p><p>49 Despite the large spread in the data it is consistent with our predictions. [sent-92, score-0.069]
</p><p>50 57 , is closer to our prediction than to Rail ' s law, TJ = 1. [sent-94, score-0.019]
</p><p>51 where exponent TJ  We also show the histogram of the exponents TJ obtained for each of  63 branch  points from the same data set, Fig. [sent-96, score-0.421]
</p><p>52 67 , is much  closer to our predicted value for myelinated axons, '7  = 3,  than to RaIl's law,  '7 = 1. [sent-99, score-0.206]
</p><p>53 9  Figure 3: Comparison of the experimental data (asterisks) [12] with theoretical predictions. [sent-122, score-0.026]
</p><p>54 Each axonal bifurcation (with d, =F- d 2 ) is represented in the plot twice. [sent-123, score-0.405]
</p><p>55 57 , and our prediction for myelinated axons,  '7 = 3. [sent-127, score-0.187]
</p><p>56 Analysis of the experimental data reveals a large spread in the values of the exponent, '7. [sent-128, score-0.095]
</p><p>57 This spread may arise from the biological variability in the axon diameters, other factors influencing axon diameters, or measurement errors due to the finite resolution of light microscopy. [sent-129, score-0.379]
</p><p>58 Although we cannot distinguish between these causes, we performed a simulation showing that a reasonable measurement error is sufficient to account for the spread. [sent-130, score-0.06]
</p><p>59 First, based on the experimental data [12], we generate a set of diameters do, d, and d 2 at branch points, which satisfy Eq. [sent-131, score-0.506]
</p><p>60 We do this by taking all diameter pairs at branch point from the experimental data and calculating the value of the third diameter according to Eq. [sent-133, score-0.488]
</p><p>61 Next we simulate the experimental data by adding Gaussian noise to all branch diameters, and calculate the probability distribution for the exponent '7 resulting from this procedure. [sent-135, score-0.418]
</p><p>62 4 shows that the spread in the histogram of branching exponent could be explained by Gaussian measurement error with standard deviation of O. [sent-137, score-0.732]
</p><p>63 um precision with which diameter measurements are reported in [12]. [sent-142, score-0.145]
</p><p>64 14  12  RaIl's  10  average exponent  8  6  predicted exponent 2  0  0  2  3  6  Figure 4: Experimentally observed spread in the branching exponent may arise from the measurement errors. [sent-143, score-1.161]
</p><p>65 The histogram shows the distribution of the exponent '7, Eq. [sent-144, score-0.249]
</p><p>66 The line shows the simulated distribution of the exponent obtained in the presence of measurement errors. [sent-148, score-0.28]
</p><p>67 4  Conclusion  Starting with the hypotheses that axonal arbors had been optimized in the course of evolution for fast signal conduction while keeping arbor volume to a minimum we derived a branching law that relates segment diameters at a branch point. [sent-149, score-2.15]
</p><p>68 The derivation was done for the cost function of a general form , and relies only on the known scaling of signal propagation velocity with the axonal caliber. [sent-150, score-0.632]
</p><p>69 This law is consistent with the available experimental data on myelinated axons. [sent-151, score-0.413]
</p><p>70 The observed spread in the branching exponent may be accounted for by the measurement error. [sent-152, score-0.703]
</p><p>71 There, similar to non-myelinated axons, time delay or attenuation of passively propagating signals scales as one over the square root of diameter. [sent-155, score-0.195]
</p><p>72 This leads to a branching law with exponent of 5/2. [sent-156, score-0.774]
</p><p>73 However, the presence of reflections from branch points and active conductances is likely to complicate the picture. [sent-157, score-0.208]
</p><p>74 5  Methods  The detrimental contribution of an axonal arbor to the evolutionary fitness can be quantified by the cost, Q:. [sent-158, score-0.644]
</p><p>75 We postulate that the cost function , Q:, is a monotonically increasing function of the total axonal volume per neuron, V , and all signal propagation delays, Tj , from soma to  j -th synapse, where j = 1,2,3, . [sent-159, score-0.666]
</p><p>76 : (10)  Below we show that this rather general cost function (along with biophysical properties ofaxons) is minimized when axonal caliber satisfies the following branching law : ( 11)  with branching exponent '7 axons . [sent-162, score-2.124]
</p><p>77 (11) for a single branch point, our theory can be trivially extended to more complex arbor topologies. [sent-165, score-0.281]
</p><p>78 We rewrite the cost function, ([, in terms of volume contributions, ~, of i -th axonal segment to the total volume of the axonal arbor, V , and signal propagation delay, t i , occurred along i -th axonal segment. [sent-166, score-1.736]
</p><p>79 The cost function reduces to: (12)  Next, we express volume and signal propagation delay of each segment as a function of segment diameter. [sent-167, score-0.99]
</p><p>80 The volume of each cylindrical segment is given by:  1r  2  V =-Ld, 4 I  where  I  (13)  I  Li and d i are segment length and diameter, correspondingly. [sent-168, score-0.6]
</p><p>81 Signal  propagation delay, t i , is given by the ratio of segment length, L i , and signal speed, Si'  Signal speed along axonal segment, in turn, depends on its diameter as : (14)  where V = 1 for myelinated [4] and V = 0. [sent-169, score-1.095]
</p><p>82 As a result propagation delay along segment i is: (15)  Substituting Eqs. [sent-171, score-0.508]
</p><p>83 (12) , we find the dependence of the cost function on segment diameters,  t1'(1r Lod 2 +- ~d2 +- ~d2 - +~ v - +~v J 1r 1r Lo - Lov v ~  4  0  4  I  4  2 '  kd o  kd I  '  kd 0  kd 2  (16)  . [sent-173, score-0.839]
</p><p>84 To find the diameters of all segments, which minimize the cost function ([, we calculate its partial derivatives with respect to all segment diameters and set them to zero:  (17)  ~=Q:'! [sent-174, score-1.028]
</p><p>85 ' ad  v  2  2 '--2  d -Q:' 2  T2  v~  kd v +1  =0  2  By solving these equations we find the optimal segment diameters: dv +2  o  = 2v(Q:~  I  +Q:;. [sent-179, score-0.421]
</p><p>86 (18)  These equations imply that the cost function is minimized when the segment diameters at a branch point satisfy the following expression (independent of the particular form of the cost function, which enters Eq. [sent-186, score-1.075]
</p><p>87 The vascular system and the cost of blood volume. [sent-231, score-0.199]
</p><p>88 (1927) A relationship between circumference and weight in trees and its bearing on branching angles. [sent-235, score-0.415]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('branching', 0.354), ('axonal', 0.347), ('conduction', 0.291), ('diameters', 0.289), ('segment', 0.257), ('axons', 0.248), ('caliber', 0.229), ('exponent', 0.22), ('law', 0.2), ('myelinated', 0.187), ('branch', 0.172), ('delay', 0.157), ('diameter', 0.145), ('cost', 0.145), ('rail', 0.127), ('tj', 0.123), ('axon', 0.116), ('arbor', 0.109), ('kd', 0.092), ('delays', 0.083), ('fitness', 0.083), ('spread', 0.069), ('volume', 0.068), ('propagation', 0.061), ('measurement', 0.06), ('bifurcation', 0.058), ('blood', 0.054), ('dendrites', 0.054), ('viscous', 0.054), ('segments', 0.049), ('find', 0.048), ('signal', 0.045), ('bronchs', 0.042), ('chklovskii', 0.042), ('daughter', 0.042), ('fluid', 0.042), ('physiol', 0.042), ('thinner', 0.042), ('vessels', 0.042), ('branches', 0.038), ('fibers', 0.036), ('transport', 0.036), ('velocity', 0.034), ('evolutionary', 0.034), ('detrimental', 0.033), ('cold', 0.033), ('harbor', 0.033), ('spring', 0.033), ('organisms', 0.033), ('along', 0.033), ('histogram', 0.029), ('coefficients', 0.028), ('murray', 0.028), ('minimized', 0.027), ('dendritic', 0.026), ('experimental', 0.026), ('arrive', 0.025), ('trees', 0.025), ('ad', 0.024), ('thick', 0.024), ('santa', 0.024), ('expression', 0.021), ('scales', 0.021), ('dependence', 0.021), ('neuron', 0.021), ('expressions', 0.02), ('speed', 0.02), ('costs', 0.02), ('contribution', 0.02), ('minimizes', 0.019), ('satisfy', 0.019), ('closer', 0.019), ('fe', 0.018), ('testable', 0.018), ('postulating', 0.018), ('cajal', 0.018), ('bifurcating', 0.018), ('quantified', 0.018), ('stevens', 0.018), ('arbors', 0.018), ('asterisks', 0.018), ('bearing', 0.018), ('bungtown', 0.018), ('circumference', 0.018), ('conductances', 0.018), ('cylindrical', 0.018), ('cytoplasm', 0.018), ('del', 0.018), ('harvard', 0.018), ('metabolic', 0.018), ('reflections', 0.018), ('rel', 0.018), ('sinauer', 0.018), ('sunderland', 0.018), ('thicker', 0.018), ('vertebrates', 0.018), ('wiring', 0.018), ('arise', 0.018), ('rewrite', 0.018), ('root', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="47-tfidf-1" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>Author: Dmitri B. Chklovskii, Armen Stepanyants</p><p>Abstract: What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d</p><p>2 0.098233879 <a title="47-tfidf-2" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>Author: Brian Taba, Kwabena A. Boahen</p><p>Abstract: We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2] . This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modeling largely unelucidated and exceedingly plastic higher processing centers in cortex. Instead of an explicit blueprint for every cortical area, what is needed is a developmental rule that can wire complex circuits from minimal specifications. One candidate is the famous</p><p>3 0.08761131 <a title="47-tfidf-3" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>4 0.070438132 <a title="47-tfidf-4" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>Author: Duane Q. Nykamp</p><p>Abstract: We present a method to distinguish direct connections between two neurons from common input originating from other, unmeasured neurons. The distinction is computed from the spike times of the two neurons in response to a white noise stimulus. Although the method is based on a highly idealized linear-nonlinear approximation of neural response, we demonstrate via simulation that the approach can work with a more realistic, integrate-and-ﬁre neuron model. We propose that the approach exempliﬁed by this analysis may yield viable tools for reconstructing stimulus-driven neural networks from data gathered in neurophysiology experiments.</p><p>5 0.046838764 <a title="47-tfidf-5" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>Author: Ehud Ben-reuven, Yoram Singer</p><p>Abstract: Time difference of arrival (TDOA) is commonly used to estimate the azimuth of a source in a microphone array. The most common methods to estimate TDOA are based on ﬁnding extrema in generalized crosscorrelation waveforms. In this paper we apply microphone array techniques to a manikin head. By considering the entire cross-correlation waveform we achieve azimuth prediction accuracy that exceeds extrema locating methods. We do so by quantizing the azimuthal angle and treating the prediction problem as a multiclass categorization task. We demonstrate the merits of our approach by evaluating the various approaches on Sony’s AIBO robot.</p><p>6 0.045061372 <a title="47-tfidf-6" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>7 0.044434186 <a title="47-tfidf-7" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>8 0.042781819 <a title="47-tfidf-8" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>9 0.042767674 <a title="47-tfidf-9" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>10 0.041781612 <a title="47-tfidf-10" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>11 0.041582428 <a title="47-tfidf-11" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>12 0.038803998 <a title="47-tfidf-12" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>13 0.034094609 <a title="47-tfidf-13" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>14 0.033340823 <a title="47-tfidf-14" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>15 0.032254584 <a title="47-tfidf-15" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>16 0.03047977 <a title="47-tfidf-16" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>17 0.028431423 <a title="47-tfidf-17" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>18 0.028395373 <a title="47-tfidf-18" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>19 0.027772509 <a title="47-tfidf-19" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>20 0.027283525 <a title="47-tfidf-20" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.079), (1, 0.04), (2, -0.025), (3, -0.006), (4, 0.005), (5, 0.021), (6, -0.007), (7, 0.014), (8, 0.028), (9, 0.042), (10, -0.009), (11, 0.028), (12, -0.002), (13, 0.002), (14, 0.005), (15, -0.004), (16, -0.007), (17, -0.011), (18, -0.021), (19, -0.064), (20, -0.023), (21, 0.06), (22, 0.007), (23, -0.037), (24, 0.075), (25, -0.002), (26, 0.04), (27, -0.015), (28, 0.054), (29, -0.06), (30, 0.044), (31, -0.027), (32, -0.058), (33, -0.089), (34, -0.032), (35, 0.064), (36, -0.123), (37, -0.126), (38, 0.129), (39, 0.017), (40, 0.011), (41, 0.146), (42, -0.049), (43, 0.067), (44, 0.029), (45, 0.052), (46, -0.246), (47, 0.057), (48, 0.135), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96962482 <a title="47-lsi-1" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>Author: Dmitri B. Chklovskii, Armen Stepanyants</p><p>Abstract: What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d</p><p>2 0.47624698 <a title="47-lsi-2" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>Author: Brian Taba, Kwabena A. Boahen</p><p>Abstract: We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2] . This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modeling largely unelucidated and exceedingly plastic higher processing centers in cortex. Instead of an explicit blueprint for every cortical area, what is needed is a developmental rule that can wire complex circuits from minimal specifications. One candidate is the famous</p><p>3 0.4155024 <a title="47-lsi-3" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>Author: Ehud Ben-reuven, Yoram Singer</p><p>Abstract: Time difference of arrival (TDOA) is commonly used to estimate the azimuth of a source in a microphone array. The most common methods to estimate TDOA are based on ﬁnding extrema in generalized crosscorrelation waveforms. In this paper we apply microphone array techniques to a manikin head. By considering the entire cross-correlation waveform we achieve azimuth prediction accuracy that exceeds extrema locating methods. We do so by quantizing the azimuthal angle and treating the prediction problem as a multiclass categorization task. We demonstrate the merits of our approach by evaluating the various approaches on Sony’s AIBO robot.</p><p>4 0.3749972 <a title="47-lsi-4" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>5 0.33189064 <a title="47-lsi-5" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>Author: J. L. Shapiro</p><p>Abstract: Population-based Incremental Learning is shown require very sensitive scaling of its learning rate. The learning rate must scale with the system size in a problem-dependent way. This is shown in two problems: the needle-in-a haystack, in which the learning rate must vanish exponentially in the system size, and in a smooth function in which the learning rate must vanish like the square root of the system size. Two methods are proposed for removing this sensitivity. A learning dynamics which obeys detailed balance is shown to give consistent performance over the entire range of learning rates. An analog of mutation is shown to require a learning rate which scales as the inverse system size, but is problem independent. 1</p><p>6 0.32668906 <a title="47-lsi-6" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>7 0.30448294 <a title="47-lsi-7" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>8 0.27720863 <a title="47-lsi-8" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>9 0.24773954 <a title="47-lsi-9" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>10 0.23018618 <a title="47-lsi-10" href="./nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">95 nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>11 0.22272238 <a title="47-lsi-11" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>12 0.22211066 <a title="47-lsi-12" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>13 0.21800889 <a title="47-lsi-13" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>14 0.20914154 <a title="47-lsi-14" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>15 0.20636675 <a title="47-lsi-15" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>16 0.1982587 <a title="47-lsi-16" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>17 0.19541612 <a title="47-lsi-17" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>18 0.19500013 <a title="47-lsi-18" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>19 0.1947792 <a title="47-lsi-19" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>20 0.1913484 <a title="47-lsi-20" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (23, 0.027), (42, 0.032), (54, 0.105), (55, 0.056), (57, 0.018), (68, 0.045), (74, 0.041), (92, 0.028), (96, 0.466), (98, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80266464 <a title="47-lda-1" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>Author: Dmitri B. Chklovskii, Armen Stepanyants</p><p>Abstract: What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2 ) branches at a bifurcation obey v v 路 d</p><p>2 0.29518887 <a title="47-lda-2" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>3 0.29300478 <a title="47-lda-3" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>Author: Jason Weston, Olivier Chapelle, Vladimir Vapnik, André Elisseeff, Bernhard Schölkopf</p><p>Abstract: We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 1</p><p>4 0.29241449 <a title="47-lda-4" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>Author: Cheng S. Ong, Robert C. Williamson, Alex J. Smola</p><p>Abstract: We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves deﬁning a Reproducing Kernel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.</p><p>5 0.29219007 <a title="47-lda-5" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>6 0.29150307 <a title="47-lda-6" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>7 0.29086849 <a title="47-lda-7" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>8 0.29010981 <a title="47-lda-8" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>9 0.2895889 <a title="47-lda-9" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>10 0.28916875 <a title="47-lda-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.28896207 <a title="47-lda-11" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>12 0.28889358 <a title="47-lda-12" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>13 0.28874674 <a title="47-lda-13" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>14 0.28856504 <a title="47-lda-14" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>15 0.28847167 <a title="47-lda-15" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>16 0.28822249 <a title="47-lda-16" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>17 0.28776449 <a title="47-lda-17" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>18 0.28765669 <a title="47-lda-18" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>19 0.28751802 <a title="47-lda-19" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>20 0.28735772 <a title="47-lda-20" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
