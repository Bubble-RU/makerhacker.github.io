<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-166" href="#">nips2002-166</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</h1>
<br/><p>Source: <a title="nips-2002-166-pdf" href="http://papers.nips.cc/paper/2168-rate-distortion-function-in-the-spin-glass-state-a-toy-model.pdf">pdf</a></p><p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>Reference: <a title="nips-2002-166-reference" href="../nips2002_reference/nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. [sent-4, score-0.395]
</p><p>2 We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. [sent-5, score-0.375]
</p><p>3 The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. [sent-6, score-0.956]
</p><p>4 Numerical study shows that sparse constructions of the model provide suboptimal compressions. [sent-7, score-0.061]
</p><p>5 1  Introduction  Many information-science studies are very similar to those of statistical physics. [sent-8, score-0.046]
</p><p>6 Statistical physics and information science may have been expected to be directed towards common objectives since Shannon formulated an information theory based on the concept of entropy. [sent-9, score-0.222]
</p><p>7 However, envisaging how this actually happened would have been difﬁcult; that the physics of disordered systems, and spin glass theory in particular, at its maturity naturally includes some important aspects of information sciences, thus reuniting the two disciplines. [sent-10, score-0.629]
</p><p>8 This cross-disciplinary ﬁeld can thus be expected to develop much further beyond current perspectives in the future [1]. [sent-11, score-0.027]
</p><p>9 The areas where these relations are particularly strong are Shannon’s coding theory [2] and classical spin systems with quenched disorder, which is the replica theory of disordered statistical systems [3]. [sent-12, score-0.837]
</p><p>10 Triggered by the work of Sourlas [4], these links have recently been examined in the area of matrix-based error corrections [5, 6], network-based compressions [7], and turbo decoding [8]. [sent-13, score-0.14]
</p><p>11 Recent results of these topics are mostly based on the replica technique. [sent-14, score-0.276]
</p><p>12 Without exception, their basic characteristics (such as channel capacity, entropy rate, or achievable rate region) are only captured by the concept of a phase transition with a ﬁrst-order jump between the optimal and the other solutions arising in the scheme. [sent-15, score-0.402]
</p><p>13 However, the research in the cross-disciplinary ﬁeld so far can be categorized as a so-called ‘zero-distortion’ decoding scheme in terms of information theory: the system requires perfect reproduction of the input alphabets [2]. [sent-16, score-0.2]
</p><p>14 Here, the same spin glass techniques should be useful to describe the physics of systems with a ﬁdelity criterion; i. [sent-17, score-0.389]
</p><p>15 , a certain degree of information distortion is assumed when reproducing the alphabets. [sent-19, score-0.712]
</p><p>16 This framework is  called the rate distortion theory [9, 10]. [sent-20, score-0.889]
</p><p>17 Though processing information requires regarding the concept of distortions practically, where input alphabets are mostly represented by continuous variables, statistical physics only employs a few approaches [11, 12]. [sent-21, score-0.33]
</p><p>18 We analyze how information distortion can be described by the concepts of statistical physics. [sent-23, score-0.787]
</p><p>19 More speciﬁcally, we study the inverse problem of a Sourlas-type decoding problem by using the framework of replica symmetry breaking (RSB) of diluted disordered systems [13]. [sent-24, score-0.582]
</p><p>20 According to our analysis, this simple model provides an optimal compression scheme for an arbitrary ﬁdelity-criterion degree, though the encoding procedure remains an NPcomplete problem without any practical encoders. [sent-25, score-0.13]
</p><p>21 In Section 2, we brieﬂy review the concept of the rate distortion theory as well as the main results related to our purpose. [sent-27, score-0.959]
</p><p>22 2  Review: Rate Distortion Theory  We brieﬂy recall the deﬁnitions of the concepts of the rate distortion theory and state the simplest version of the main result at the end of this section. [sent-32, score-0.918]
</p><p>23 Let J be a discrete random variable with alphabet J . [sent-33, score-0.095]
</p><p>24 Assume that we have a source that produces a sequence J1 , J2 , · · · , JM , where each symbol is randomly drawn from a distribution. [sent-34, score-0.201]
</p><p>25 Throughout this paper we use vector notation to represent sequences for convenience of explanation: J = (J1 , J2 , · · · , JM )T ∈ J M . [sent-36, score-0.031]
</p><p>26 Here, the encoder describes the source sequence J ∈ J M by a codeword ξ = f (J ) ∈ X N . [sent-37, score-0.197]
</p><p>27 The ˆ ˆ decoder represents J by an estimate J = g(ξ) ∈ J M , as illustrated in Figure 1. [sent-38, score-0.026]
</p><p>28 Note that M represents the length of a source sequence, while N represents the length of a codeword. [sent-39, score-0.161]
</p><p>29 Note that the relation N < M always holds when a compression is considered; therefore, R < 1 also holds. [sent-41, score-0.133]
</p><p>30 1 A distortion function is a mapping ˆ d : J × J → R+  (1)  from the set of source alphabet-reproduction alphabet pairs into the set of non-negative real numbers. [sent-43, score-0.916]
</p><p>31 ˆ Intuitively, the distortion d(J, J) is a measure of the cost of representing the symbol J by ˆ This deﬁnition is quite general. [sent-44, score-0.773]
</p><p>32 In most cases, however, the reproduction the symbol J. [sent-45, score-0.116]
</p><p>33 ˆ ˆ alphabet J is the same as the source alphabet J . [sent-46, score-0.299]
</p><p>34 Hereafter, we set J = J and the following distortion measure is adopted as the ﬁdelity criterion: Deﬁnition 2. [sent-47, score-0.712]
</p><p>35 2 The Hamming distortion is given by ˆ d(J, J) =  0 1  ˆ if J = J ˆ , if J = J  (2)  , ˆ ˆ which results in a probable error distortion, since the relation E[d(J, J)] = P[J = J] holds, where E[·] represents the expectation and P[·] the probability of its argument. [sent-48, score-0.769]
</p><p>36 The distortion measure is so far deﬁned on a symbol-by-symbol basis. [sent-49, score-0.712]
</p><p>37 3 The distortion between sequences J , J ∈ J M is deﬁned by 1 ˆ d(J , J ) = M  M  ˆ d(Jj , Jj ) . [sent-51, score-0.743]
</p><p>38 (3)  j=1  Therefore, the distortion for a sequence is the average distortion per symbol of the elements of the sequence. [sent-52, score-1.516]
</p><p>39 4 The distortion associated with the code is deﬁned as ˆ D = E[d(J , J )] ,  (4)  where the expectation is with respect to the probability distribution on J . [sent-54, score-0.712]
</p><p>40 A rate distortion pair (R, D) should be achiebable if a sequence of rate distortion codes ˆ (f, g) exist with E[d(J , J )] ≤ D in the limit M → ∞. [sent-55, score-1.741]
</p><p>41 Moreover, the closure of the set of achievable rate distortion pairs is called the rate distortion region for a source. [sent-56, score-1.742]
</p><p>42 5 The rate distortion function R(D) is the inﬁmum of rates R, so that (R, D) is in the rate distortion region of the source for a given distortion D. [sent-58, score-2.505]
</p><p>43 As in [7], we restrict ourselves to a binary source J with a Hamming distortion measure for simplicity. [sent-59, score-0.821]
</p><p>44 We assume that binary alphabets are drawn randomly, i. [sent-60, score-0.09]
</p><p>45 , the source is not biased to rule out the possiblity of compression due to redundancy. [sent-62, score-0.28]
</p><p>46 We now ﬁnd the description rate R(D) required to describe the source with an expected proportion of errors less than or equal to D. [sent-63, score-0.223]
</p><p>47 In this simpliﬁed case, according to Shannon, the boundary can be written as follows. [sent-64, score-0.035]
</p><p>48 βg βg  (18)  Since the target bit of the estimation in this model is J i1 ,··· ,iK and its estimator the product Si1 · · · SiK , a performance measure for the information corruption could be the per-bond energy e. [sent-67, score-0.092]
</p><p>49 According to the one-step RSB framework, the lowest free energy can be calculated from the probability distributions π1RSB (x) and π1RSB (ˆ) satisfying the saddle point ˆ x equation (15) at the characteristic inverse temperature βg , when the replica symmetric entropy sRS disappears. [sent-68, score-0.542]
</p><p>50 The distortion D associated with this code is given by the fraction of the free energies that arise in the spin glass phase: D=  f1RSB − fRS 1 − tanh βg = . [sent-71, score-1.095]
</p><p>51 2|fRS | 2  (19)  Here, we substitute the spin glass solutions into the expression, making use of the fact that the replica symmetric entropy sRS disappears at a consistent βg , which is determined by (17). [sent-72, score-0.667]
</p><p>52 Using (17) and (19), simple algebra gives the relation between the rate R = N/M and the distortion D in the form R = 1 − H(D) , which coincides with the rate distortion function retrieving Theorem 2. [sent-73, score-1.735]
</p><p>53 Surprisingly, we do not observe any ﬁrst-order jumps between analytical solutions. [sent-75, score-0.062]
</p><p>54 Recently, we have seen that many approaches to the family of codes, characterized by the linear encoding operations, result in a quite different picture: the optimal boundary is constructed in the random energy model limit and is well captured by the concept of a ﬁrst-order jump. [sent-76, score-0.255]
</p><p>55 Our analysis of this model, viewed as a kind of inverse problem, provides an exception. [sent-77, score-0.05]
</p><p>56 Many optimal conditions in textbook information theory may be well described without the concept of a ﬁrst-order phase transitions from a view point of statistical physics. [sent-78, score-0.244]
</p><p>57 We will now investigate the possiblity of the other solutions satisfying (15) in the case of ﬁnite K and C. [sent-79, score-0.114]
</p><p>58 Since the saddle point equations (15) appear difﬁcult for analytical arguments, we resort to numerical evaluations representing the probability distributions π1RSB (x) and π1RSB (ˆ) by up to 105 bin models and carrying out the integrations by ˆ x using Monte Carlo methods. [sent-80, score-0.221]
</p><p>59 Note that the characteristic inverse temperature β g is also evaluated numerically by using (17). [sent-81, score-0.088]
</p><p>60 We set K = 2 and selected various values of C to demonstrate the performance of stable solutions. [sent-82, score-0.057]
</p><p>61 The numerical results obtained by the one-step RSB senario show suboptimal properties [Figure 2]. [sent-83, score-0.097]
</p><p>62 This strongly implies that the analytical solution is not the only stable solution. [sent-84, score-0.119]
</p><p>63 This conjecture might be veriﬁed elsewhere, carrying out large scale simulations. [sent-85, score-0.046]
</p><p>64 Firstly, we found that the consistency between the rate distortion theory and the Parisi one-step RSB scheme. [sent-87, score-0.889]
</p><p>65 Secondly, we conjectured that the analytical solution, which is consistent with the Shannon’s result, is not the only stable solution for some situations. [sent-88, score-0.146]
</p><p>66 5  D  Figure 2: Numerically-constructed stable solutions: Stable solutions of (15) for the ﬁnite values of K and L are calculated by using Monte Carlo methods. [sent-137, score-0.102]
</p><p>67 We use 105 bin models to approximate the probability distributions π1RSB (x) and π1RSB (ˆ), starting from various ˆ x initial conditions. [sent-138, score-0.035]
</p><p>68 The distributions converge to the continuous ones, giving suboptimal performance. [sent-139, score-0.061]
</p><p>69 (◦) K = 2 and L = 3, 4, · · · , 12 ; Solid line indicates the rate distortion function R(D). [sent-140, score-0.826]
</p><p>70 Coding theorems for a discrete source with a ﬁdelity criterion. [sent-160, score-0.109]
</p><p>71 Statistical mechanics of lossy data compression using a non-monotonic perceptron. [sent-170, score-0.312]
</p><p>72 A coding theorem for lossy data compression by LDPC codes. [sent-175, score-0.25]
</p><p>73 Graph bipartitioning and spin glasses on a random network of ﬁxed ﬁnite valence. [sent-188, score-0.29]
</p><p>74 The random energy model, an exactly solvable model of disordered systems. [sent-194, score-0.213]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distortion', 0.712), ('replica', 0.241), ('spin', 0.205), ('disordered', 0.151), ('delity', 0.151), ('kabashima', 0.138), ('rsb', 0.138), ('mechanics', 0.128), ('shannon', 0.121), ('rate', 0.114), ('source', 0.109), ('compression', 0.102), ('alphabet', 0.095), ('glass', 0.095), ('alphabets', 0.09), ('physics', 0.089), ('srs', 0.082), ('lossy', 0.082), ('hamming', 0.081), ('concept', 0.07), ('frs', 0.069), ('murayama', 0.069), ('possiblity', 0.069), ('ln', 0.066), ('theory', 0.063), ('analytical', 0.062), ('nition', 0.062), ('energy', 0.062), ('suboptimal', 0.061), ('symbol', 0.061), ('stable', 0.057), ('decoding', 0.055), ('reproduction', 0.055), ('glasses', 0.055), ('parisi', 0.055), ('tanh', 0.055), ('turbo', 0.055), ('breaking', 0.051), ('jm', 0.051), ('inverse', 0.05), ('toy', 0.049), ('carrying', 0.046), ('cosh', 0.046), ('statistical', 0.046), ('solutions', 0.045), ('entropy', 0.045), ('saddle', 0.042), ('jj', 0.041), ('phase', 0.038), ('temperature', 0.038), ('coding', 0.038), ('numerical', 0.036), ('symmetric', 0.036), ('boundary', 0.035), ('bin', 0.035), ('mostly', 0.035), ('symmetry', 0.034), ('veri', 0.033), ('achievable', 0.032), ('region', 0.032), ('captured', 0.032), ('sequence', 0.031), ('sequences', 0.031), ('relation', 0.031), ('codes', 0.03), ('mum', 0.03), ('disorder', 0.03), ('quenched', 0.03), ('bipartitioning', 0.03), ('corruption', 0.03), ('snapshots', 0.03), ('wong', 0.03), ('compressions', 0.03), ('encoder', 0.03), ('saitama', 0.03), ('concepts', 0.029), ('brie', 0.028), ('limit', 0.028), ('theorem', 0.028), ('criterion', 0.028), ('encoding', 0.028), ('carlo', 0.028), ('free', 0.028), ('conjectured', 0.027), ('perspectives', 0.027), ('textbook', 0.027), ('inset', 0.027), ('codeword', 0.027), ('ire', 0.027), ('de', 0.026), ('monte', 0.026), ('represents', 0.026), ('retrieving', 0.026), ('enforcing', 0.026), ('closure', 0.026), ('riken', 0.026), ('coincides', 0.026), ('happened', 0.026), ('jump', 0.026), ('sun', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="166-tfidf-1" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>2 0.25839064 <a title="166-tfidf-2" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>3 0.20337978 <a title="166-tfidf-3" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The problem of extracting the relevant aspects of data, in face of multiple conﬂicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.</p><p>4 0.10845212 <a title="166-tfidf-4" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>5 0.067111365 <a title="166-tfidf-5" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We examine the problem of generating state-space compressions of POMDPs in a way that minimally impacts decision quality. We analyze the impact of compressions on decision quality, observing that compressions that allow accurate policy evaluation (prediction of expected future reward) will not affect decision quality. We derive a set of sufﬁcient conditions that ensure accurate prediction in this respect, illustrate interesting mathematical properties these confer on lossless linear compressions, and use these to derive an iterative procedure for ﬁnding good linear lossy compressions. We also elaborate on how structured representations of a POMDP can be used to ﬁnd such compressions.</p><p>6 0.061946698 <a title="166-tfidf-6" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>7 0.058800559 <a title="166-tfidf-7" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>8 0.058555827 <a title="166-tfidf-8" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>9 0.050670173 <a title="166-tfidf-9" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>10 0.047946516 <a title="166-tfidf-10" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>11 0.04284763 <a title="166-tfidf-11" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>12 0.039308518 <a title="166-tfidf-12" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>13 0.039218716 <a title="166-tfidf-13" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>14 0.039158732 <a title="166-tfidf-14" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>15 0.037856322 <a title="166-tfidf-15" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>16 0.035749651 <a title="166-tfidf-16" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>17 0.035489108 <a title="166-tfidf-17" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>18 0.034245059 <a title="166-tfidf-18" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>19 0.033762649 <a title="166-tfidf-19" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>20 0.033663463 <a title="166-tfidf-20" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, -0.009), (2, -0.021), (3, 0.008), (4, -0.028), (5, 0.031), (6, -0.095), (7, -0.049), (8, -0.069), (9, -0.01), (10, 0.002), (11, -0.068), (12, -0.013), (13, -0.046), (14, -0.039), (15, -0.029), (16, -0.036), (17, -0.165), (18, -0.279), (19, -0.094), (20, 0.147), (21, -0.119), (22, -0.142), (23, -0.366), (24, 0.153), (25, 0.108), (26, 0.144), (27, -0.112), (28, -0.118), (29, 0.039), (30, 0.307), (31, 0.005), (32, -0.084), (33, 0.042), (34, -0.065), (35, -0.15), (36, -0.034), (37, 0.016), (38, 0.034), (39, 0.023), (40, 0.007), (41, 0.004), (42, -0.053), (43, -0.016), (44, 0.076), (45, 0.036), (46, 0.004), (47, -0.021), (48, -0.012), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97548568 <a title="166-lsi-1" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>2 0.91307855 <a title="166-lsi-2" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>Author: Albert E. Parker, Tomá\v S. Gedeon, Alexander G. Dimitrov</p><p>Abstract: In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Deterministic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to ﬁnd optima at a bifurcation point. 1</p><p>3 0.60827625 <a title="166-lsi-3" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The problem of extracting the relevant aspects of data, in face of multiple conﬂicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.</p><p>4 0.35718402 <a title="166-lsi-4" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>Author: Matthias Bethge, David Rotermund, Klaus Pawelzik</p><p>Abstract: Here we derive optimal gain functions for minimum mean square reconstruction from neural rate responses subjected to Poisson noise. The shape of these functions strongly depends on the length T of the time window within which spikes are counted in order to estimate the underlying firing rate. A phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero. For a particular function class, we were able to prove the existence of a second-order phase transition analytically. The critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical results. We conclude that under most circumstances relevant to information processing in the brain, rate coding can be better ascribed to a binary (low-entropy) code than to the other extreme of rich analog coding. 1 Optimal neuronal gain functions for short decoding time windows The use of action potentials (spikes) as a means of communication is the striking feature of neurons in the central nervous system. Since the discovery by Adrian [1] that action potentials are generated by sensory neurons with a frequency that is substantially determined by the stimulus, the idea of rate coding has become a prevalent paradigm in neuroscience [2]. In particular, today the coding properties of many neurons from various areas in the cortex have been characterized by tuning curves, which describe the average firing rate response as a function of certain stimulus parameters. This way of description is closely related to the idea of analog coding, which constitutes the basis for many neural network models. Reliabl v inference from the observed number of spikes about the underlying firing rate of a neuronal response, however, requires a sufficiently long time interval, while integration times of neurons in vivo [3] as well as reaction times of humans or animals when performing classification tasks [4, 5] are known to be rather short. Therefore, it is important to understand, how neural rate coding is affected by a limited time window available for decoding. While rate codes are usually characterized by tuning functions relating the intensity of the ,f * http://www.neuro.urn-bremen.dermbethge neuronal response to a particular stimulus parameter, the question, how relevant the idea of analog coding actually is does not depend on the particular entity represented by a neuron. Instead it suffices to determine the shape of the gain function, which displays the mean firing rate as a function of the actual analog signal to be sent to subsequent neurons. Here we seek for optimal gain functions that minimize the minimum average squared reconstruction error for a uniform source signal transmitted through a Poisson channel as a function of the maximum mean number of spikes. In formal terms, the issue is to optimally encode a real random variable x in the number of pulses emitted by a neuron within a certain time window. Thereby, x stands for the intended analog output of the neuron that shall be signaled to subsequent neurons. The latter, however, can only observe a number of spikes k integrated within a time interval of length T. The statistical dependency between x and k is specified by the assumption of Poisson noise p(kIJL(x)) = (JL~))k exp{ -JL(X)} , (1) and the choice of the gain function f(x), which together with T determines the mean spike count J.L(x) == T f(x) . An important additional constraint is the limited output range of the neuronal firing rate, which can be included by the requirement of a bounded gain function (fmin :::; f (x) :::; f max, VX). Since inhibition can reliably prevent a neuron from firing, we will here consider the case f min == 0 only. Instead of specifying f max, we impose a bound directly on the mean spike count (i.e. J.L(x) :::; /l), because f max constitutes a meaningful constraint only in conjunction with a fixed time window length T. As objective function we consider the minimum mean squared error (MMSE) with respect to Lebesgue measure for x E [0, 1], ~ 2 X _ E x2 _ E (i2 _ _ [jt( )] - [] [] - 3 X ~ (Xl (J01 xp(kIJL(x)) dx r J01p(kIJL(x)) dx' (2) where x(k) == E[xlk] denotes the mean square estimator, which is the conditional expectation (see e.g. [6]). 1.1 Tunings and errors As derived in [7] on the basis of Fisher information the optimal gain function for a single neuron in the asymptotic limit T -+ 00 has a parabolic shape: fasymp(x) == fmaxx2 . (3) For any finite /l, however, this gain function is not necessarily optimal, and in the limit T -+ 0, it is straight forward to show that the optimal tuning curve is a step function f step (xl'19) == fmax 8 (x - {)) , (4) where 8(z) denotes the Heaviside function that equals one, if z > 0 and zero if z < O. The optimal threshold 'l9(p,) of the step tuning curve depends on /l and can be determined analytically 11(-) =1_ It 3 - V8e-J.' +1 4(1 - e- il ) (5) as well as the corresponding MMSE [8]: 2 2[fste p] _ 1 ( 3'19 (p,) ) X - 12 1 - [(1 -11(p))(l - e-iL)]-1 - 1 . (6) 1 S +1 0.5 CJ;) o ........ '------'-----'---'---'--'~----'----'-- ~---'---'---'--'~ 10-1 ~---,.---,---.,...............---.----.---.---.-.......-.-.--.-~ ...............~ Figure 1: The upper panel shows a bifurcation plot for {}(Jt) - wand {}(Jt) + w of the optimal gain function in 51 as a function of {t illustrating the phase transition from binary to continuous encoding. The dotted line separates the regions before and after the phase transition in all three panels. Left from this line (i.e. for Jt < Jt C) the step function given by Eq. 4+5 is optimal. The middle panel shows the MMSE of this step function (dashed) and of the optimal gain function in 52 (solid), which becomes smaller than the first one after the phase transition. The relative deviation between the minimal errors of 51 and 52 (i.e. (X~l - X~2)/X~2) is displayed in the lower panel and has a maximum below 0.035. The binary shape for small {t and the continuous parabolic shape for large {t implies that there has to be a transition from discrete to analog encoding with increasing {to Unfortunately it is not possible to determine the optimal gain function within the set of all bounded functions B :== {fli : [0, 1] -+ [0, fmax]} and hence, one has to choose a certain parameterized function space 5 c B in advance that is feasible for the optimization. In [8], we investigated various such function-'spaces and for {t < 2.9, we did not find any gain function with an error smaller than the MMSE of the step function. Furthermore, we always observed a phase transition from binary to analog encoding at a critical {t C that depends only slightly on the function space. As one can see in Fig. 1 (upper) pc is approximately three. In this paper, we consider two function classes 51, 52, which both contain the binary gain function as well as the asymptotic optimal parabolic function as special cases. Furthermore 51 is a proper subset of 52. Our interest in 51 results from the fact that we can analyze the phase transition in this subset analytically, while 52 is the most general parameterization for which we have. determined the optimal encoding numerically. The latter has six free parameters a :::; b :::; c E [0, 1], fmid E (0, fmax), a, f3 E [0,00) and the parameterization of the gain functions is given by o fS2 (xla, b, c, fmid, a, (3) fmid ( ~=: == , O</p><p>5 0.31440726 <a title="166-lsi-5" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>6 0.22641665 <a title="166-lsi-6" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>7 0.21715109 <a title="166-lsi-7" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>8 0.19471198 <a title="166-lsi-8" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>9 0.19046381 <a title="166-lsi-9" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>10 0.18580827 <a title="166-lsi-10" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>11 0.18271747 <a title="166-lsi-11" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>12 0.17038712 <a title="166-lsi-12" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>13 0.16717944 <a title="166-lsi-13" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>14 0.16712077 <a title="166-lsi-14" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>15 0.16669872 <a title="166-lsi-15" href="./nips-2002-Branching_Law_for_Axons.html">47 nips-2002-Branching Law for Axons</a></p>
<p>16 0.15952124 <a title="166-lsi-16" href="./nips-2002-Analysis_of_Information_in_Speech_Based_on_MANOVA.html">29 nips-2002-Analysis of Information in Speech Based on MANOVA</a></p>
<p>17 0.15008721 <a title="166-lsi-17" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>18 0.14458472 <a title="166-lsi-18" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>19 0.14161491 <a title="166-lsi-19" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>20 0.13771728 <a title="166-lsi-20" href="./nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">105 nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.025), (23, 0.03), (42, 0.041), (54, 0.097), (55, 0.039), (65, 0.33), (67, 0.028), (68, 0.03), (74, 0.082), (92, 0.086), (98, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79193848 <a title="166-lda-1" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>2 0.79024345 <a title="166-lda-2" href="./nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">12 nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<p>Author: Alon Fishbach, Bradford J. May</p><p>Abstract: Psychophysical data suggest that temporal modulations of stimulus amplitude envelopes play a prominent role in the perceptual segregation of concurrent sounds. In particular, the detection of an unmodulated signal can be significantly improved by adding amplitude modulation to the spectral envelope of a competing masking noise. This perceptual phenomenon is known as “Comodulation Masking Release” (CMR). Despite the obvious influence of temporal structure on the perception of complex auditory scenes, the physiological mechanisms that contribute to CMR and auditory streaming are not well known. A recent physiological study by Nelken and colleagues has demonstrated an enhanced cortical representation of auditory signals in modulated noise. Our study evaluates these CMR-like response patterns from the perspective of a hypothetical auditory edge-detection neuron. It is shown that this simple neural model for the detection of amplitude transients can reproduce not only the physiological data of Nelken et al., but also, in light of previous results, a variety of physiological and psychoacoustical phenomena that are related to the perceptual segregation of concurrent sounds. 1 In t rod u ct i on The temporal structure of a complex sound exerts strong influences on auditory physiology (e.g. [10, 16]) and perception (e.g. [9, 19, 20]). In particular, studies of auditory scene analysis have demonstrated the importance of the temporal structure of amplitude envelopes in the perceptual segregation of concurrent sounds [2, 7]. Common amplitude transitions across frequency serve as salient cues for grouping sound energy into unified perceptual objects. Conversely, asynchronous amplitude transitions enhance the separation of competing acoustic events [3, 4]. These general principles are manifested in perceptual phenomena as diverse as comodulation masking release (CMR) [13], modulation detection interference [22] and synchronous onset grouping [8]. Despite the obvious importance of timing information in psychoacoustic studies of auditory masking, the way in which the CNS represents the temporal structure of an amplitude envelope is not well understood. Certainly many physiological studies have demonstrated neural sensitivities to envelope transitions, but this sensitivity is only beginning to be related to the variety of perceptual experiences that are evoked by signals in noise. Nelken et al. [15] have suggested a correspondence between neural responses to time-varying amplitude envelopes and psychoacoustic masking phenomena. In their study of neurons in primary auditory cortex (A1), adding temporal modulation to background noise lowered the detection thresholds of unmodulated tones. This enhanced signal detection is similar to the perceptual phenomenon that is known as comodulation masking release [13]. Fishbach et al. [11] have recently proposed a neural model for the detection of “auditory edges” (i.e., amplitude transients) that can account for numerous physiological [14, 17, 18] and psychoacoustical [3, 21] phenomena. The encompassing utility of this edge-detection model suggests a common mechanism that may link the auditory processing and perception of auditory signals in a complex auditory scene. Here, it is shown that the auditory edge detection model can accurately reproduce the cortical CMR-like responses previously described by Nelken and colleagues. 2 Th e M od el The model is described in detail elsewhere [11]. In short, the basic operation of the model is the calculation of the first-order time derivative of the log-compressed envelope of the stimulus. A computational model [23] is used to convert the acoustic waveform to a physiologically plausible auditory nerve representation (Fig 1a). The simulated neural response has a medium spontaneous rate and a characteristic frequency that is set to the frequency of the target tone. To allow computation of the time derivative of the stimulus envelope, we hypothesize the existence of a temporal delay dimension, along which the stimulus is progressively delayed. The intermediate delay layer (Fig 1b) is constructed from an array of neurons with ascending membrane time constants (τ); each neuron is modeled by a conventional integrate-and-fire model (I&F;, [12]). Higher membrane time constant induces greater delay in the neuron’s response [1]. The output of the delay layer converges to a single output neuron (Fig. 1c) via a set of connection with various efficacies that reflect a receptive field of a gaussian derivative. This combination of excitatory and inhibitory connections carries out the time-derivative computation. Implementation details and parameters are given in [11]. The model has 2 adjustable and 6 fixed parameters, the former were used to fit the responses of the model to single unit responses to variety of stimuli [11]. The results reported here are not sensitive to these parameters. (a) AN model (b) delay-layer (c) edge-detector neuron τ=6 ms I&F; Neuron τ=4 ms τ=3 ms bandpass log d dt RMS Figure 1: Schematic diagram of the model and a block diagram of the basic operation of each model component (shaded area). The stimulus is converted to a neural representation (a) that approximates the average firing rate of a medium spontaneous-rate AN fiber [23]. The operation of this stage can be roughly described as the log-compressed rms output of a bandpass filter. The neural representation is fed to a series of neurons with ascending membrane time constant (b). The kernel functions that are used to simulate these neurons are plotted for a few neurons along with the time constants used. The output of the delay-layer neurons converge to a single I&F; neuron (c) using a set of connections with weights that reflect a shape of a gaussian derivative. Solid arrows represent excitatory connections and white arrows represent inhibitory connections. The absolute efficacy is represented by the width of the arrows. 3 Resu lt s Nelken et al. [15] report that amplitude modulation can substantially modify the noise-driven discharge rates of A1 neurons in Halothane-anesthetized cats. Many cortical neurons show only a transient onset response to unmodulated noise but fire in synchrony (“lock”) to the envelope of modulated noise. A significant reduction in envelope-locked discharge rates is observed if an unmodulated tone is added to modulated noise. As summarized in Fig. 2, this suppression of envelope locking can reveal the presence of an auditory signal at sound pressure levels that are not detectable in unmodulated noise. It has been suggested that this pattern of neural responding may represent a physiological equivalent of CMR. Reproduction of CMR-like cortical activity can be illustrated by a simplified case in which the analytical amplitude envelope of the stimulus is used as the input to the edge-detector model. In keeping with the actual physiological approach of Nelken et al., the noise envelope is shaped by a trapezoid modulator for these simulations. Each cycle of modulation, E N(t), is given by: t 0≤t  < 3D E N (t ) = P P − D (t − 3 D ) 3 D ≤ t < 4 D 0 4 D ≤ t < 8D £ P D     ¢     ¡ where P is the peak pressure level and D is set to 12.5 ms. (b) Modulated noise 76 Spikes/sec Tone level (dB SPL) (a) Unmodulated noise 26 0 150 300 0 150 300 Time (ms) Figure 2: Responses of an A1 unit to a combination of noise and tone at many tone levels, replotted from Nelken et al. [15]. (a) Unmodulated noise and (b) modulated noise. The noise envelope is illustrated by the thick line above each figure. Each row shows the response of the neuron to the noise plus the tone at the level specified on the ordinate. The dashed line in (b) indicates the detection threshold level for the tone. The detection threshold (as defined and calculated by Nelken et al.) in the unmodulated noise was not reached. Since the basic operation of the model is the calculation of the rectified timederivative of the log-compressed envelope of the stimulus, the expected noisedriven rate of the model can be approximated by: ( ) ¢ E (t ) P0   d A ln 1 + dt ¡ M N ( t ) = max 0, ¥ ¤ £ where A=20/ln(10) and P0 =2e-5 Pa. The expected firing rate in response to the noise plus an unmodulated signal (tone) can be similarly approximated by: ) ¨ E ( t ) + PS P0 ¦ ( d A ln 1 + dt § M N + S ( t ) = max 0,   © where PS is the peak pressure level of the tone. Clearly, both MN (t) and MN+S (t) are identically zero outside the interval [0 D]. Within this interval it holds that: M N (t ) = AP D P0 + P D t 0≤t < D Clearly, M N + S < M N for the interval [0 D] of each modulation cycle. That is, the addition of a tone reduces the responses of the model to the rising part of the modulated envelope. Higher tone levels (Ps ) cause greater reduction in the model’s firing rate. (c) (b) Level derivative (dB SPL/ms) Level (dB SPL) (a) (d) Time (ms) Figure 3: An illustration of the basic operation of the model on various amplitude envelopes. The simplified operation of the model includes log compression of the amplitude envelope (a and c) and rectified time-derivative of the log-compressed envelope (b and d). (a) A 30 dB SPL tone is added to a modulated envelope (peak level of 70 dB SPL) 300 ms after the beginning of the stimulus (as indicated by the horizontal line). The addition of the tone causes a great reduction in the time derivative of the log-compressed envelope (b). When the envelope of the noise is unmodulated (c), the time-derivative of the log-compressed envelope (d) shows a tiny spike when the tone is added (marked by the arrow). Fig. 3 demonstrates the effect of a low-level tone on the time-derivative of the logcompressed envelope of a noise. When the envelope is modulated (Fig. 3a) the addition of the tone greatly reduces the derivative of the rising part of the modulation (Fig. 3b). In the absence of modulations (Fig. 3c), the tone presentation produces a negligible effect on the level derivative (Fig. 3d). Model simulations of neural responses to the stimuli used by Nelken et al. are plotted in Fig. 4. As illustrated schematically in Fig 3 (d), the presence of the tone does not cause any significant change in the responses of the model to the unmodulated noise (Fig. 4a). In the modulated noise, however, tones of relatively low levels reduce the responses of the model to the rising part of the envelope modulations. (b) Modulated noise 76 Spikes/sec Tone level (dB SPL) (a) Unmodulated noise 26 0 150 300 0 Time (ms) 150 300 Figure 4: Simulated responses of the model to a combination of a tone and Unmodulated noise (a) and modulated noise (b). All conventions are as in Fig. 2. 4 Di scu ssi on This report uses an auditory edge-detection model to simulate the actual physiological consequences of amplitude modulation on neural sensitivity in cortical area A1. The basic computational operation of the model is the calculation of the smoothed time-derivative of the log-compressed stimulus envelope. The ability of the model to reproduce cortical response patterns in detail across a variety of stimulus conditions suggests similar time-sensitive mechanisms may contribute to the physiological correlates of CMR. These findings augment our previous observations that the simple edge-detection model can successfully predict a wide range of physiological and perceptual phenomena [11]. Former applications of the model to perceptual phenomena have been mainly related to auditory scene analysis, or more specifically the ability of the auditory system to distinguish multiple sound sources. In these cases, a sharp amplitude transition at stimulus onset (“auditory edge”) was critical for sound segregation. Here, it is shown that the detection of acoustic signals also may be enhanced through the suppression of ongoing responses to the concurrent modulations of competing background sounds. Interestingly, these temporal fluctuations appear to be a common property of natural soundscapes [15]. The model provides testable predictions regarding how signal detection may be influenced by the temporal shape of amplitude modulation. Carlyon et al. [6] measured CMR in human listeners using three types of noise modulation: squarewave, sine wave and multiplied noise. From the perspective of the edge-detection model, these psychoacoustic results are intriguing because the different modulator types represent manipulations of the time derivative of masker envelopes. Squarewave modulation had the most sharply edged time derivative and produced the greatest masking release. Fig. 5 plots the responses of the model to a pure-tone signal in square-wave and sine-wave modulated noise. As in the psychoacoustical data of Carlyon et al., the simulated detection threshold was lower in the context of square-wave modulation. Our modeling results suggest that the sharply edged square wave evoked higher levels of noise-driven activity and therefore created a sensitive background for the suppressing effects of the unmodulated tone. (b) 60 Spikes/sec Tone level (dB SPL) (a) 10 0 200 400 600 0 Time (ms) 200 400 600 Figure 5: Simulated responses of the model to a combination of a tone at various levels and a sine-wave modulated noise (a) or a square-wave modulated noise (b). Each row shows the response of the model to the noise plus the tone at the level specified on the abscissa. The shape of the noise modulator is illustrated above each figure. The 100 ms tone starts 250 ms after the noise onset. Note that the tone detection threshold (marked by the dashed line) is 10 dB lower for the square-wave modulator than for the sine-wave modulator, in accordance with the psychoacoustical data of Carlyon et al. [6]. Although the physiological basis of our model was derived from studies of neural responses in the cat auditory system, the key psychoacoustical observations of Carlyon et al. have been replicated in recent behavioral studies of cats (Budelis et al. [5]). These data support the generalization of human perceptual processing to other species and enhance the possible correspondence between the neuronal CMR-like effect and the psychoacoustical masking phenomena. Clearly, the auditory system relies on information other than the time derivative of the stimulus envelope for the detection of auditory signals in background noise. Further physiological and psychoacoustic assessments of CMR-like masking effects are needed not only to refine the predictive abilities of the edge-detection model but also to reveal the additional sources of acoustic information that influence signal detection in constantly changing natural environments. Ackn ow led g men t s This work was supported in part by a NIDCD grant R01 DC004841. Refe ren ces [1] Agmon-Snir H., Segev I. (1993). “Signal delay and input synchronization in passive dendritic structure”, J. Neurophysiol. 70, 2066-2085. [2] Bregman A.S. (1990). “Auditory scene analysis: The perceptual organization of sound”, MIT Press, Cambridge, MA. [3] Bregman A.S., Ahad P.A., Kim J., Melnerich L. (1994) “Resetting the pitch-analysis system. 1. Effects of rise times of tones in noise backgrounds or of harmonics in a complex tone”, Percept. Psychophys. 56 (2), 155-162. [4] Bregman A.S., Ahad P.A., Kim J. (1994) “Resetting the pitch-analysis system. 2. Role of sudden onsets and offsets in the perception of individual components in a cluster of overlapping tones”, J. Acoust. Soc. Am. 96 (5), 2694-2703. [5] Budelis J., Fishbach A., May B.J. (2002) “Behavioral assessments of comodulation masking release in cats”, Abst. Assoc. for Res. in Otolaryngol. 25. [6] Carlyon R.P., Buus S., Florentine M. (1989) “Comodulation masking release for three types of modulator as a function of modulation rate”, Hear. Res. 42, 37-46. [7] Darwin C.J. (1997) “Auditory grouping”, Trends in Cog. Sci. 1(9), 327-333. [8] Darwin C.J., Ciocca V. (1992) “Grouping in pitch perception: Effects of onset asynchrony and ear of presentation of a mistuned component”, J. Acoust. Soc. Am. 91 , 33813390. [9] Drullman R., Festen H.M., Plomp R. (1994) “Effect of temporal envelope smearing on speech reception”, J. Acoust. Soc. Am. 95 (2), 1053-1064. [10] Eggermont J J. (1994). “Temporal modulation transfer functions for AM and FM stimuli in cat auditory cortex. Effects of carrier type, modulating waveform and intensity”, Hear. Res. 74, 51-66. [11] Fishbach A., Nelken I., Yeshurun Y. (2001) “Auditory edge detection: a neural model for physiological and psychoacoustical responses to amplitude transients”, J. Neurophysiol. 85, 2303–2323. [12] Gerstner W. (1999) “Spiking neurons”, in Pulsed Neural Networks , edited by W. Maass, C. M. Bishop, (MIT Press, Cambridge, MA). [13] Hall J.W., Haggard M.P., Fernandes M.A. (1984) “Detection in noise by spectrotemporal pattern analysis”, J. Acoust. Soc. Am. 76, 50-56. [14] Heil P. (1997) “Auditory onset responses revisited. II. Response strength”, J. Neurophysiol. 77, 2642-2660. [15] Nelken I., Rotman Y., Bar-Yosef O. (1999) “Responses of auditory cortex neurons to structural features of natural sounds”, Nature 397, 154-157. [16] Phillips D.P. (1988). “Effect of Tone-Pulse Rise Time on Rate-Level Functions of Cat Auditory Cortex Neurons: Excitatory and Inhibitory Processes Shaping Responses to Tone Onset”, J. Neurophysiol. 59, 1524-1539. [17] Phillips D.P., Burkard R. (1999). “Response magnitude and timing of auditory response initiation in the inferior colliculus of the awake chinchilla”, J. Acoust. Soc. Am. 105, 27312737. [18] Phillips D.P., Semple M.N., Kitzes L.M. (1995). “Factors shaping the tone level sensitivity of single neurons in posterior field of cat auditory cortex”, J. Neurophysiol. 73, 674-686. [19] Rosen S. (1992) “Temporal information in speech: acoustic, auditory and linguistic aspects”, Phil. Trans. R. Soc. Lond. B 336, 367-373. [20] Shannon R.V., Zeng F.G., Kamath V., Wygonski J, Ekelid M. (1995) “Speech recognition with primarily temporal cues”, Science 270, 303-304. [21] Turner C.W., Relkin E.M., Doucet J. (1994). “Psychophysical and physiological forward masking studies: probe duration and rise-time effects”, J. Acoust. Soc. Am. 96 (2), 795-800. [22] Yost W.A., Sheft S. (1994) “Modulation detection interference – across-frequency processing and auditory grouping”, Hear. Res. 79, 48-58. [23] Zhang X., Heinz M.G., Bruce I.C., Carney L.H. (2001). “A phenomenological model for the responses of auditory-nerve fibers: I. Nonlinear tuning with compression and suppression”, J. Acoust. Soc. Am. 109 (2), 648-670.</p><p>3 0.70675546 <a title="166-lda-3" href="./nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">111 nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>Author: Trevor Hastie, Rob Tibshirani</p><p>Abstract: We present a simple direct approach for solving the ICA problem, using density estimation and maximum likelihood. Given a candidate orthogonal frame, we model each of the coordinates using a semi-parametric density estimate based on cubic splines. Since our estimates have two continuous derivatives , we can easily run a second order search for the frame parameters. Our method performs very favorably when compared to state-of-the-art techniques. 1</p><p>4 0.50530189 <a title="166-lda-4" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>Author: Bernd Fischer, Johann Schumann, Wray Buntine, Alexander G. Gray</p><p>Abstract: Machine learning has reached a point where many probabilistic methods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e.g., as different instances of the EM algorithm. This enables the systematic derivation of algorithms customized for different models. Here, we describe the AUTO BAYES system which takes a high-level statistical model speciﬁcation, uses powerful symbolic techniques based on schema-based program synthesis and computer algebra to derive an efﬁcient specialized algorithm for learning that model, and generates executable code implementing that algorithm. This capability is far beyond that of code collections such as Matlab toolboxes or even tools for model-independent optimization such as BUGS for Gibbs sampling: complex new algorithms can be generated without new programming, algorithms can be highly specialized and tightly crafted for the exact structure of the model and data, and efﬁcient and commented code can be generated for different languages or systems. We present automatically-derived algorithms ranging from closed-form solutions of Bayesian textbook problems to recently-proposed EM algorithms for clustering, regression, and a multinomial form of PCA. 1 Automatic Derivation of Statistical Algorithms Overview. We describe a symbolic program synthesis system which works as a “statistical algorithm compiler:” it compiles a statistical model speciﬁcation into a custom algorithm design and from that further down into a working program implementing the algorithm design. This system, AUTO BAYES, can be loosely thought of as “part theorem prover, part Mathematica, part learning textbook, and part Numerical Recipes.” It provides much more ﬂexibility than a ﬁxed code repository such as a Matlab toolbox, and allows the creation of efﬁcient algorithms which have never before been implemented, or even written down. AUTO BAYES is intended to automate the more routine application of complex methods in novel contexts. For example, recent multinomial extensions to PCA [2, 4] can be derived in this way. The algorithm design problem. Given a dataset and a task, creating a learning method can be characterized by two main questions: 1. What is the model? 2. What algorithm will optimize the model parameters? The statistical algorithm (i.e., a parameter optimization algorithm for the statistical model) can then be implemented manually. The system in this paper answers the algorithm question given that the user has chosen a model for the data,and continues through to implementation. Performing this task at the state-of-the-art level requires an intertwined meld of probability theory, computational mathematics, and software engineering. However, a number of factors unite to allow us to solve the algorithm design problem computationally: 1. The existence of fundamental building blocks (e.g., standardized probability distributions, standard optimization procedures, and generic data structures). 2. The existence of common representations (i.e., graphical models [3, 13] and program schemas). 3. The formalization of schema applicability constraints as guards. 1 The challenges of algorithm design. The design problem has an inherently combinatorial nature, since subparts of a function may be optimized recursively and in different ways. It also involves the use of new data structures or approximations to gain performance. As the research in statistical algorithms advances, its creative focus should move beyond the ultimately mechanical aspects and towards extending the abstract applicability of already existing schemas (algorithmic principles like EM), improving schemas in ways that generalize across anything they can be applied to, and inventing radically new schemas. 2 Combining Schema-based Synthesis and Bayesian Networks Statistical Models. Externally, AUTO BAYES has the look and feel of 2 const int n_points as ’nr. of data points’ a compiler. Users specify their model 3 with 0 < n_points; 4 const int n_classes := 3 as ’nr. classes’ of interest in a high-level speciﬁcation 5 with 0 < n_classes language (as opposed to a program6 with n_classes << n_points; ming language). The ﬁgure shows the 7 double phi(1..n_classes) as ’weights’ speciﬁcation of the mixture of Gaus8 with 1 = sum(I := 1..n_classes, phi(I)); 9 double mu(1..n_classes); sians example used throughout this 9 double sigma(1..n_classes); paper.2 Note the constraint that the 10 int c(1..n_points) as ’class labels’; sum of the class probabilities must 11 c ˜ disc(vec(I := 1..n_classes, phi(I))); equal one (line 8) along with others 12 data double x(1..n_points) as ’data’; (lines 3 and 5) that make optimization 13 x(I) ˜ gauss(mu(c(I)), sigma(c(I))); of the model well-deﬁned. Also note 14 max pr(x| phi,mu,sigma ) wrt phi,mu,sigma ; the ability to specify assumptions of the kind in line 6, which may be used by some algorithms. The last line speciﬁes the goal inference task: maximize the conditional probability pr with respect to the parameters , , and . Note that moving the parameters across to the left of the conditioning bar converts this from a maximum likelihood to a maximum a posteriori problem. 1 model mog as ’Mixture of Gaussians’; ¡   £  £  £ §¤¢ £ © ¨ ¦ ¥ ©   ¡     ¡ £ £ £ ¨ Computational logic and theorem proving. Internally, AUTO BAYES uses a class of techniques known as computational logic which has its roots in automated theorem proving. AUTO BAYES begins with an initial goal and a set of initial assertions, or axioms, and adds new assertions, or theorems, by repeated application of the axioms, until the goal is proven. In our context, the goal is given by the input model; the derived algorithms are side effects of constructive theorems proving the existence of algorithms for the goal. 1 Schema guards vary widely; for example, compare Nead-Melder simplex or simulated annealing (which require only function evaluation), conjugate gradient (which require both Jacobian and Hessian), EM and its variational extension [6] (which require a latent-variable structure model). 2 Here, keywords have been underlined and line numbers have been added for reference in the text. The as-keyword allows annotations to variables which end up in the generated code’s comments. Also, n classes has been set to three (line 4), while n points is left unspeciﬁed. The class variable and single data variable are vectors, which deﬁnes them as i.i.d. Computer algebra. The ﬁrst core element which makes automatic algorithm derivation feasible is the fact that we can mechanize the required symbol manipulation, using computer algebra methods. General symbolic differentiation and expression simpliﬁcation are capabilities fundamental to our approach. AUTO BAYES contains a computer algebra engine using term rewrite rules which are an efﬁcient mechanism for substitution of equal quantities or expressions and thus well-suited for this task.3 Schema-based synthesis. The computational cost of full-blown theorem proving grinds simple tasks to a halt while elementary and intermediate facts are reinvented from scratch. To achieve the scale of deduction required by algorithm derivation, we thus follow a schema-based synthesis technique which breaks away from strict theorem proving. Instead, we formalize high-level domain knowledge, such as the general EM strategy, as schemas. A schema combines a generic code fragment with explicitly speciﬁed preconditions which describe the applicability of the code fragment. The second core element which makes automatic algorithm derivation feasible is the fact that we can use Bayesian networks to efﬁciently encode the preconditions of complex algorithms such as EM. First-order logic representation of Bayesian netNclasses works. A ﬁrst-order logic representation of Bayesian µ σ networks was developed by Haddawy [7]. In this framework, random variables are represented by functor symbols and indexes (i.e., speciﬁc instances φ x c of i.i.d. vectors) are represented as functor arguments. discrete gauss Nclasses Since unknown index values can be represented by Npoints implicitly universally quantiﬁed Prolog variables, this approach allows a compact encoding of networks involving i.i.d. variables or plates [3]; the ﬁgure shows the initial network for our running example. Moreover, such networks correspond to backtrack-free datalog programs, allowing the dependencies to be efﬁciently computed. We have extended the framework to work with non-ground probability queries since we seek to determine probabilities over entire i.i.d. vectors and matrices. Tests for independence on these indexed Bayesian networks are easily developed in Lauritzen’s framework which uses ancestral sets and set separation [9] and is more amenable to a theorem prover than the double negatives of the more widely known d-separation criteria. Given a Bayesian network, some probabilities can easily be extracted by enumerating the component probabilities at each node: § ¥ ¨¦¡ ¡ ¢© Lemma 1. Let be sets of variables over a Bayesian network with . Then descendents and parents hold 4 in the corresponding dependency graph iff the following probability statement holds: £ ¤  ¡ parents B % % 9 C0A@ ! 9  @8 § ¥   ¢   2 ' % % 310  parents    ©¢   £ ¡ !    ' % #!  </p><p>5 0.4977212 <a title="166-lda-5" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>Author: Christopher M. Bishop, David Spiegelhalter, John Winn</p><p>Abstract: In recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of probabilistic models. For each new application, however, it is currently necessary ﬁrst to derive the variational update equations, and then to implement them in application-speciﬁc code. Each of these steps is both time consuming and error prone. In this paper we describe a general purpose inference engine called VIBES (‘Variational Inference for Bayesian Networks’) which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding. New models are speciﬁed either through a simple script or via a graphical interface analogous to a drawing package. VIBES then automatically generates and solves the variational equations. We illustrate the power and ﬂexibility of VIBES using examples from Bayesian mixture modelling. 1</p><p>6 0.48953637 <a title="166-lda-6" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>7 0.48741391 <a title="166-lda-7" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>8 0.48701012 <a title="166-lda-8" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>9 0.48534462 <a title="166-lda-9" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>10 0.48313329 <a title="166-lda-10" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>11 0.47965291 <a title="166-lda-11" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>12 0.47877234 <a title="166-lda-12" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>13 0.47827059 <a title="166-lda-13" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>14 0.47699273 <a title="166-lda-14" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>15 0.4768545 <a title="166-lda-15" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>16 0.47684389 <a title="166-lda-16" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>17 0.47603536 <a title="166-lda-17" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>18 0.47567087 <a title="166-lda-18" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>19 0.47476467 <a title="166-lda-19" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>20 0.47272044 <a title="166-lda-20" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
