<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-62" href="#">nips2002-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</h1>
<br/><p>Source: <a title="nips-2002-62-pdf" href="http://papers.nips.cc/paper/2148-coulomb-classifiers-generalizing-support-vector-machines-via-an-analogy-to-electrostatic-systems.pdf">pdf</a></p><p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>Reference: <a title="nips-2002-62-reference" href="../nips2002_reference/nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. [sent-6, score-0.694]
</p><p>2 In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. [sent-8, score-1.068]
</p><p>3 Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. [sent-10, score-0.084]
</p><p>4 In this paper, we present a new derivation of SVMs by analogy to an electrostatic system of charged conductors. [sent-14, score-0.652]
</p><p>5 The electrostatic framework not only provides a physical interpretation of SVMs, but it also gives insight into some of the seemingly arbitrary aspects of SVMs (e. [sent-15, score-0.569]
</p><p>6 , the diagonal of the quadratic form), and it allows us to derive novel SVM approaches. [sent-17, score-0.084]
</p><p>7 Although we  are the ﬁrst to make the analogy between SVMs and electrostatic systems, previous researchers have used electrostatic nonlinearities in pattern recognition [1] and a mechanical interpretation of SVMs was introduced in [9]. [sent-18, score-1.078]
</p><p>8 We assume a supervised learning paradigm in which N training examples are available, each example i consisting of an input xi and a label yi ∈ {−1, +1}. [sent-20, score-0.152]
</p><p>9 We will introduce three electrostatic models that are directly analogous to existing machine-learning (ML) classiﬁers, each of which builds on and generalizes the previous. [sent-21, score-0.53]
</p><p>10 For each model, we describe the physical system upon which it is based and show its correspondence to an ML classiﬁer. [sent-22, score-0.081]
</p><p>11 1 Electrostatic model 1: Uncoupled point charges Consider an electrostatic system of point charges populating a space X homologous to X . [sent-24, score-1.067]
</p><p>12 Each point charge corresponds to a particular training example; point charge i is ﬁxed at location xi in X , and has a charge of sign yi . [sent-25, score-1.33]
</p><p>13 We deﬁne two sets of ﬁxed charges: S + = xi | yi = +1 and S − = xi | yi = −1 . [sent-26, score-0.304]
</p><p>14 The charge of point i is Qi ≡ yi αi , where αi ≥ 0 is the amount of charge, to be discussed below. [sent-27, score-0.458]
</p><p>15 If a unit positive charge is at x in X , it will be attracted to all charges in S − and repelled by all charges in S + . [sent-29, score-0.912]
</p><p>16 To ˜ move the charge from x to some other location x, the attractive and repelling forces must be overcome at every point along the trajectory; the path integral of the force along the trajectory is called the work and does not depend on the trajectory. [sent-30, score-0.4]
</p><p>17 The potential at x is the work that must be done to move a unit positive charge from a reference point (usually inﬁnity) to x. [sent-31, score-0.546]
</p><p>18 N  j The potential at x is ϕ (x) = j=1 Qj G x , x , where G is a function of the distance. [sent-32, score-0.133]
</p><p>19 In electrostatic systems with point charges, G (a, b) = 1/ a − b 2 . [sent-33, score-0.514]
</p><p>20 From this deﬁnition, one can see that the potential at x is negative (positive) if x is in a neighborhood of many negative (positive) charges. [sent-34, score-0.133]
</p><p>21 Thus, the potential indicates the sign and amount of charge in the local neighborhood. [sent-35, score-0.511]
</p><p>22 Abstracting from the electrostatic system, if αi = 1 and G is a function that decreases suﬃciently steeply with distance, we obtain a nearest-neighbor classiﬁer. [sent-37, score-0.494]
</p><p>23 This potential classiﬁer can be also interpreted as Parzen windows classiﬁer [9]. [sent-38, score-0.133]
</p><p>24 2 Electrostatic model 2: Coupled point charges Consider now an electrostatic model that extends the previous model in two respects. [sent-40, score-0.761]
</p><p>25 First, the point charges are replaced by conductors, e. [sent-41, score-0.267]
</p><p>26 Each conductor i has a self–potential coeﬃcient, denoted Pii , which is a measure of how much charge it can easily hold; for a metal sphere, Pii is related to sphere’s diameter. [sent-44, score-0.658]
</p><p>27 Second, the conductors in S + are coupled, as are the conductors in S − . [sent-45, score-0.432]
</p><p>28 “Coupling” means that charge is free to ﬂow between the conductors. [sent-46, score-0.356]
</p><p>29 In this model, we initially place the same charge ν/N on each conductor, and allow charges within S + and S − to ﬂow freely (we assume no resistance in the coupling and no polarization of the conductors). [sent-48, score-0.623]
</p><p>30 After the charges redistribute, charge will tend to end up on the periphery of a homogeneous neighborhood of conductors, because like charges repel. [sent-49, score-0.85]
</p><p>31 Charge will also tend to end up along the S + –S − boundary because opposite charges attract. [sent-50, score-0.287]
</p><p>32 Figure 1 depicts the redistribution of charges, where the shading is proportional to the magnitude αi . [sent-51, score-0.115]
</p><p>33 In this model, however, the αi are not uniform; the conductors with large αi will have the greatest inﬂuence on the potential function. [sent-53, score-0.349]
</p><p>34 -  + +  +  +  +  +  +  +  + +  + +  +  + +  -  + +  +  +  +  +  +  -  -  -  - - - - - - - -  Figure 1: Coupled conductor system following charge redistribution. [sent-56, score-0.65]
</p><p>35 Shading reﬂects the charge magnitude, and the contour indicates a zero potential. [sent-57, score-0.356]
</p><p>36 The redistribution of charges in the electrostatic system is achieved via minimization of the Coulomb energy. [sent-58, score-0.839]
</p><p>37 Imagine placing the same total charge magnitude, m, on S + and S − by dividing it uniformly among the conductors, i. [sent-59, score-0.356]
</p><p>38 The free charge ﬂow in S + and S − yields a distribution of charges, the αi , such that Coulomb energy is minimized. [sent-62, score-0.44]
</p><p>39 The potential at conductor i, ϕ(xi ), which we will denote more compactly as ϕi , can be described N in terms of the coeﬃcients of potential Pij [10]: ϕi = j=1 Pij Qj , where Pij is the potential induced on conductor i by charge Qj on conductor j; Pii ≥ Pij ≥ 0 and Pij = Pji . [sent-64, score-1.52]
</p><p>40 If each conductor i is a metal sphere centered at xi and has radius ri (radii are enforced to be small enough so that the spheres do not touch each other), the system can be modeled by a point charge Qi at xi , and Pij = G xi , xj as in the previous section [10]. [sent-65, score-1.094]
</p><p>41 The Coulomb energy is deﬁned in terms of the potential on the conductors, ϕi : E =  1 2  N  N  ϕi Q i = i=1  1 1 T Q P Q = Pij yi yj αi αj . [sent-67, score-0.324]
</p><p>42 2 2 i,j=1  When the energy minimum is reached, the potential ϕi will be the same for all connected i ∈ S + (i ∈ S − ); we denote this potential ϕS + (ϕS − ). [sent-68, score-0.35]
</p><p>43 Two additional constraints on the system of coupled conductors are necessary in order to interpret the system in terms of existing machine learning models. [sent-69, score-0.377]
</p><p>44 First, the positive and negative potentials must be balanced, i. [sent-70, score-0.09]
</p><p>45 This constraint is achieved by setting the reference point of the potentials through b, N b = −0. [sent-73, score-0.073]
</p><p>46 5 (ϕS + + ϕS − ), into the potential function: ϕ (x) = i=1 Qi G xi , x + b. [sent-74, score-0.203]
</p><p>47 Second, the conductors must be prevented from reversing the sign of their charge, i. [sent-75, score-0.238]
</p><p>48 These  requirements can be satisﬁed in the electrostatic model by disconnecting a conductor i from the charge ﬂow in S + or S − when αi reaches a bound, which will subsequently freeze its charge. [sent-80, score-1.126]
</p><p>49 Mathematically, the requirements are satisﬁed by treating energy minimization as a constrained optimization problem with 0 ≤ αi ≤ C. [sent-81, score-0.13]
</p><p>50 The electrostatic system corresponds to a ν–support vector machine (ν–SVM) [9] with kernel G if we set C = 1/N . [sent-82, score-0.592]
</p><p>51 The electrostatic system assures that i∈S + αi = i∈S − αi = 0. [sent-83, score-0.533]
</p><p>52 The identity holds because the Coulomb energy is exactly the ν–SVM quadratic objective function, and the thresholded electrostatic potential evaluated at a location is exactly the SVM decision rule. [sent-85, score-0.763]
</p><p>53 The minimization of potentials diﬀerences in the systems S + and S − corresponds to the minimization of slack variables in the SVM (slack variables express missing potential due to the upper bound on αi ). [sent-86, score-0.287]
</p><p>54 Mercer’s condition [6], the essence of the nonlinear SVM theory, is equivalent to the fact that continuous electrostatic energy is positive, i. [sent-87, score-0.578]
</p><p>55 The self-potentials of the electrostatic system provide an interpretation to the diagonal elements in the quadratic objective function of the SVM. [sent-90, score-0.594]
</p><p>56 This interpretation of the diagonal elements allows us to introduce novel kernels and novel SVM methods, as we discuss later. [sent-91, score-0.273]
</p><p>57 3 Electrostatic model 3: Coupled point charges with battery In electrostatic model 2, we control the magnitude of charge applied to S + and S − . [sent-93, score-1.302]
</p><p>58 Although we apply the same charge magnitude to each, we do not have to control the resulting potentials ϕS + and ϕS − , which may be imbalanced. [sent-94, score-0.44]
</p><p>59 We compensate for this imbalance via the potential oﬀset b. [sent-95, score-0.133]
</p><p>60 In electrostatic model 3, we control the potentials ϕS + and ϕS + directly by adding a battery to the system. [sent-96, score-0.701]
</p><p>61 We connect S + to the positive pole of the battery with potential +1 and S − to the negative pole with potential −1. [sent-97, score-0.515]
</p><p>62 The battery ensures that ϕS + = +1 and ϕS − = −1 because charges ﬂow from the battery into or out of the system until the systems take on the potential of the battery poles. [sent-98, score-0.881]
</p><p>63 The potential ϕi = yi is forced by the battery on conductor i. [sent-100, score-0.624]
</p><p>64 The total Coulomb energy is the energy from model 2 minus the work done by the battery. [sent-101, score-0.168]
</p><p>65 The work done by the battery is i≤N yi Qi = i≤N αi . [sent-102, score-0.236]
</p><p>66 The Coulomb energy is 1 T Q P Q − 2  N  N  αi = i=1  1 Pij yi yj αi αj − 2 i,j=1  N  αi . [sent-103, score-0.191]
</p><p>67 i=1  This physical system corresponds to a C–support vector machine (C–SVM) [2, 11]. [sent-104, score-0.105]
</p><p>68 The C–SVM requires that i yi αi = 0; although this constraint may not be fulﬁlled in the system described here, it can be enforced by a slightly diﬀerent system [4]. [sent-105, score-0.185]
</p><p>69 1 Novel Kernels The electrostatic perspective makes it easy to understand why SVM algorithms can break down in high-dimensional spaces: Kernels with rapid fall-oﬀ induce small potentials and consequently, almost every conductor retains charge. [sent-108, score-0.802]
</p><p>70 Because a charged conductor corresponds to a support vector, the number of support vectors is large, which leads to two disadvantages: (1) the classiﬁcation procedure is slow, and (2) the expected generalization error increases with the number of support vectors [11]. [sent-109, score-0.47]
</p><p>71 We therefore should use kernels that do not drop oﬀ exponentially. [sent-110, score-0.072]
</p><p>72 The self–potential  permits the use of kernels that would otherwise be invalid, such as a generalization −l −l of the electric ﬁeld: G xi , xj := xi − xj 2 and G xi , xi := ri = Pii , where ri the radius of the ith sphere. [sent-111, score-0.558]
</p><p>73 The classiﬁcation boundary was constructed using 2  −l/2  , which is an a C-SVM with a Plummer kernel G xi , xj := xi − xj 2 + 2 approximation to our novel Coulomb kernel but lacks its weak singularities. [sent-120, score-0.426]
</p><p>74 Boundary curves are given for the novel kernel (solid), best RBF-kernel SVM which overﬁts at high density regions where the resulting boundary goes through a dark circle (dashed), and optimal boundary (dotted). [sent-123, score-0.228]
</p><p>75 2 Novel SVM models Our electrostatic framework can be used to derive novel SVM approaches [4], two representative examples of which we illustrate here. [sent-125, score-0.578]
</p><p>76 1 κ–Support Vector Machine (κ–SVM): We can exploit the physical interpretation of Pii as conductor i’s self–potential. [sent-128, score-0.33]
</p><p>77 The Pii ’s determine the smoothness of the charge distribution at the energy minimum. [sent-129, score-0.44]
</p><p>78 new old We can introduce a parameter κ to rescale the self potential – Pii = κ Pii . [sent-130, score-0.185]
</p><p>79 With this modiﬁcation, and with C = ∞, electrostatic model 3 becomes what we call the κ–SVM. [sent-132, score-0.494]
</p><p>80 2 p–Support Vector Machine (p–SVM): At the Coulomb energy minimum the electrostatic potentials equalize: ϕ i − yi = 0, ∀i (y is the label vector). [sent-135, score-0.713]
</p><p>81 This motivates the introduction of potential diﬀerence, 2 1 1 T T 1 T T T 2 P Q + y 2 = 2 Q P P Q + Q P y + 2 y y as the objective. [sent-136, score-0.133]
</p><p>82 The constraint 1T P Y α = 0 ensures that the average potential for each class is equal. [sent-142, score-0.133]
</p><p>83 By construction, P T P is positive deﬁnite; consequently, this formulation does not require positive deﬁnite kernels. [sent-143, score-0.074]
</p><p>84 We compared two standard architectures, the C–SVM and the ν–SVM, to our novel architectures: to the κ–SVM, to the p–SVM, and to a combination of them, the κ–p–SVM. [sent-152, score-0.084]
</p><p>85 Our two novel architectures, the κ–SVM and the p–SVM, performed well against the two existing architectures (note that the diﬀerences between the C– and the ν–SVM are due to model selection). [sent-158, score-0.16]
</p><p>86 Additionally, the Plummer kernel appears to be more robust against hyperparameter and SVM choices than the RBF or polynomial kernels. [sent-160, score-0.081]
</p><p>87 The comparison is among ﬁve SVMs (the table columns) using three kernel functions (the table rows). [sent-238, score-0.089]
</p><p>88 The ﬁrst data set, the “cat cortex” data, is a matrix of connection strengths between 65 cat cortical areas and was provided by [8], where the available anatomical literature was used to determine proximity values between cortical areas. [sent-242, score-0.123]
</p><p>89 The goal was to classify a protein as belonging to a given globin class or not. [sent-248, score-0.074]
</p><p>90 As Table 2 shows, our novel architecture, the p–SVM, beats out an existing architecture in the literature, the G–SVM, on 5 of 8 classiﬁcation tasks, and ties the G–SVM on 2 of 8; it loses out on only 1 of 8. [sent-249, score-0.12]
</p><p>91 3  Table 2: Mean % misclassiﬁcations for the cat-cortex and protein data sets using the p–SVM and the G–SVM and a range of regularization parameters (indicated in the column labeled “Reg. [sent-309, score-0.075]
</p><p>92 The result obtained for the cat-cortex data is via leaveone-out cross validation, and for the protein data is via ten-fold cross validation. [sent-311, score-0.098]
</p><p>93 4  Conclusion  The electrostatic framework and its analogy to SVMs has led to several important ideas. [sent-313, score-0.573]
</p><p>94 First, it suggests SVM methods for kernels that are not positive deﬁnite. [sent-314, score-0.109]
</p><p>95 Second, it suggests novel approaches and kernels that perform as well as standard methods (will undoubtably perform better on some problems). [sent-315, score-0.156]
</p><p>96 The novel approach treats the proximity matrix as an SVM Gram matrix which lead to excellent experimental results. [sent-317, score-0.125]
</p><p>97 We argued that the electrostatic framework not only characterizes a family of support-vector machines, but it also characterizes other techniques such as nearest neighbor classiﬁcation. [sent-318, score-0.538]
</p><p>98 Perhaps the most important contribution of the electrostatic framework is that, by interrelating and encompassing a variety of methods, it lays out a broad space of possible algorithms. [sent-319, score-0.494]
</p><p>99 But by making the dimensions of this space explicit, the electrostatic framework allows one to easily explore the space and discover novel algorithms. [sent-321, score-0.578]
</p><p>100 Theoretical foundations e of the potential function method in pattern recognition learning. [sent-333, score-0.133]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('electrostatic', 0.494), ('charge', 0.356), ('coulomb', 0.295), ('svm', 0.27), ('conductor', 0.255), ('charges', 0.247), ('conductors', 0.216), ('pii', 0.177), ('battery', 0.154), ('potential', 0.133), ('pij', 0.116), ('classi', 0.101), ('svms', 0.089), ('novel', 0.084), ('energy', 0.084), ('yi', 0.082), ('plu', 0.079), ('kernels', 0.072), ('xi', 0.07), ('pol', 0.062), ('charged', 0.062), ('plummer', 0.059), ('analogy', 0.057), ('ri', 0.057), ('potentials', 0.053), ('protein', 0.052), ('self', 0.052), ('minkowski', 0.051), ('rbf', 0.048), ('coupled', 0.047), ('ow', 0.047), ('metal', 0.047), ('xj', 0.046), ('qj', 0.044), ('support', 0.043), ('physical', 0.042), ('proximity', 0.041), ('boundary', 0.04), ('cat', 0.04), ('architectures', 0.04), ('banana', 0.039), ('globins', 0.039), ('sphere', 0.039), ('system', 0.039), ('qi', 0.039), ('ml', 0.038), ('positive', 0.037), ('cation', 0.037), ('pairwise', 0.037), ('existing', 0.036), ('kernel', 0.035), ('uci', 0.035), ('erences', 0.034), ('hochreiter', 0.034), ('redistribution', 0.034), ('interpretation', 0.033), ('boulder', 0.031), ('magnitude', 0.031), ('di', 0.031), ('er', 0.03), ('ers', 0.029), ('regions', 0.029), ('pole', 0.029), ('consequently', 0.029), ('objective', 0.028), ('slack', 0.027), ('machines', 0.027), ('table', 0.027), ('colorado', 0.026), ('shading', 0.026), ('enforced', 0.025), ('attracted', 0.025), ('hyperparameter', 0.025), ('yj', 0.025), ('fl', 0.025), ('validation', 0.025), ('minimization', 0.025), ('corresponds', 0.024), ('depicts', 0.024), ('gh', 0.024), ('obermayer', 0.024), ('location', 0.024), ('cross', 0.023), ('regularization', 0.023), ('mozer', 0.023), ('ss', 0.023), ('coe', 0.023), ('sign', 0.022), ('classify', 0.022), ('bold', 0.022), ('characterizes', 0.022), ('led', 0.022), ('misclassi', 0.022), ('proteins', 0.021), ('requirements', 0.021), ('cortical', 0.021), ('polynomial', 0.021), ('coupling', 0.02), ('benchmark', 0.02), ('point', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="62-tfidf-1" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>2 0.15786323 <a title="62-tfidf-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.13176131 <a title="62-tfidf-3" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>4 0.13051184 <a title="62-tfidf-4" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>Author: Baback Moghaddam, Gregory Shakhnarovich</p><p>Abstract: We introduce a novel learning algorithm for binary classiﬁcation with hyperplane discriminants based on pairs of training points from opposite classes (dyadic hypercuts). This algorithm is further extended to nonlinear discriminants using kernel functions satisfying Mercer’s conditions. An ensemble of simple dyadic hypercuts is learned incrementally by means of a conﬁdence-rated version of AdaBoost, which provides a sound strategy for searching through the ﬁnite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classiﬁers was found to be comparable to that of SVMs and k-NN classiﬁers. Furthermore, the computational cost of classiﬁcation (at run time) was found to be similar to, or better than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we oﬀer an on-line and incremental learning machine for building kernel discriminants whose complexity (number of kernel evaluations) can be directly controlled (traded oﬀ for accuracy). 1</p><p>5 0.1281094 <a title="62-tfidf-5" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>6 0.10677835 <a title="62-tfidf-6" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>7 0.098556533 <a title="62-tfidf-7" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>8 0.092736721 <a title="62-tfidf-8" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>9 0.088278212 <a title="62-tfidf-9" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>10 0.087061994 <a title="62-tfidf-10" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>11 0.080883294 <a title="62-tfidf-11" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>12 0.077570312 <a title="62-tfidf-12" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>13 0.076970741 <a title="62-tfidf-13" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>14 0.076844193 <a title="62-tfidf-14" href="./nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">200 nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>15 0.073225446 <a title="62-tfidf-15" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>16 0.072440006 <a title="62-tfidf-16" href="./nips-2002-Self_Supervised_Boosting.html">181 nips-2002-Self Supervised Boosting</a></p>
<p>17 0.067234837 <a title="62-tfidf-17" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>18 0.066518061 <a title="62-tfidf-18" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>19 0.065532699 <a title="62-tfidf-19" href="./nips-2002-Approximate_Inference_and_Protein-Folding.html">32 nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>20 0.062650926 <a title="62-tfidf-20" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.193), (1, -0.097), (2, 0.097), (3, -0.055), (4, 0.125), (5, 0.003), (6, 0.051), (7, 0.026), (8, 0.018), (9, 0.009), (10, -0.052), (11, 0.094), (12, 0.011), (13, 0.029), (14, 0.078), (15, -0.128), (16, -0.031), (17, 0.1), (18, 0.06), (19, 0.075), (20, 0.016), (21, 0.014), (22, 0.11), (23, -0.164), (24, 0.001), (25, -0.07), (26, 0.032), (27, -0.014), (28, -0.032), (29, -0.101), (30, 0.074), (31, -0.068), (32, -0.015), (33, 0.032), (34, -0.002), (35, 0.023), (36, 0.022), (37, -0.038), (38, -0.008), (39, 0.046), (40, -0.207), (41, -0.008), (42, 0.129), (43, -0.049), (44, 0.025), (45, -0.086), (46, 0.02), (47, -0.047), (48, 0.101), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93832499 <a title="62-lsi-1" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>2 0.58622372 <a title="62-lsi-2" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>3 0.57434988 <a title="62-lsi-3" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>Author: Ralf Herbrich, Neil D. Lawrence, Matthias Seeger</p><p>Abstract: We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d–sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n · d2 ), and in large real-world classiﬁcation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signiﬁcantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation. 1</p><p>4 0.56829977 <a title="62-lsi-4" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>5 0.56586254 <a title="62-lsi-5" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>6 0.54574275 <a title="62-lsi-6" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>7 0.51534635 <a title="62-lsi-7" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>8 0.51415271 <a title="62-lsi-8" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>9 0.50896102 <a title="62-lsi-9" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>10 0.50412852 <a title="62-lsi-10" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>11 0.45734555 <a title="62-lsi-11" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>12 0.44570985 <a title="62-lsi-12" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>13 0.42620835 <a title="62-lsi-13" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>14 0.41624853 <a title="62-lsi-14" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>15 0.40898031 <a title="62-lsi-15" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>16 0.40809968 <a title="62-lsi-16" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>17 0.39127272 <a title="62-lsi-17" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>18 0.39080173 <a title="62-lsi-18" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>19 0.36507753 <a title="62-lsi-19" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>20 0.35691774 <a title="62-lsi-20" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.016), (23, 0.028), (42, 0.06), (54, 0.093), (55, 0.031), (67, 0.016), (68, 0.383), (74, 0.063), (79, 0.013), (86, 0.021), (92, 0.067), (98, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97739953 <a title="62-lda-1" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>Author: Duane Q. Nykamp</p><p>Abstract: We present a method to distinguish direct connections between two neurons from common input originating from other, unmeasured neurons. The distinction is computed from the spike times of the two neurons in response to a white noise stimulus. Although the method is based on a highly idealized linear-nonlinear approximation of neural response, we demonstrate via simulation that the approach can work with a more realistic, integrate-and-ﬁre neuron model. We propose that the approach exempliﬁed by this analysis may yield viable tools for reconstructing stimulus-driven neural networks from data gathered in neurophysiology experiments.</p><p>2 0.87317789 <a title="62-lda-2" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>Author: David Barber</p><p>Abstract: The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the beneﬁts of a tractable probabilistic model. 1</p><p>same-paper 3 0.84774631 <a title="62-lda-3" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>4 0.77831489 <a title="62-lda-4" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>Author: Arunava Banerjee, Alexandre Pouget</p><p>Abstract: If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.</p><p>5 0.75914967 <a title="62-lda-5" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<p>Author: Matthew Brand</p><p>Abstract: We construct a nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. The mapping preserves local geometric relations in the manifold and is pseudo-invertible. We show how to estimate the intrinsic dimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single lowdimensional coordinate system, and compute forward and reverse mappings between the sample and coordinate spaces. The objective functions are convex and their solutions are given in closed form. 1 Nonlinear dimensionality reduction (NLDR) by charting Charting is the problem of assigning a low-dimensional coordinate system to data points in a high-dimensional sample space. It is presumed that the data lies on or near a lowdimensional manifold embedded in the sample space, and that there exists a 1-to-1 smooth nonlinear transform between the manifold and a low-dimensional vector space. The datamodeler’s goal is to estimate smooth continuous mappings between the sample and coordinate spaces. Often this analysis will shed light on the intrinsic variables of the datagenerating phenomenon, for example, revealing perceptual or conﬁguration spaces. Our goal is to ﬁnd a mapping—expressed as a kernel-based mixture of linear projections— that minimizes information loss about the density and relative locations of sample points. This constraint is expressed in a posterior that combines a standard gaussian mixture model (GMM) likelihood function with a prior that penalizes uncertainty due to inconsistent projections in the mixture. Section 3 develops a special case where this posterior is unimodal and maximizable in closed form, yielding a GMM whose covariances reveal a patchwork of overlapping locally linear subspaces that cover the manifold. Section 4 shows that for this (or any) GMM and a choice of reduced dimension d, there is a unique, closed-form solution for a minimally distorting merger of the subspaces into a d-dimensional coordinate space, as well as an reverse mapping deﬁning the surface of the manifold in the sample space. The intrinsic dimensionality d of the data manifold can be estimated from the growth process of point-to-point distances. In analogy to differential geometry, we call the subspaces “charts” and their merger the “connection.” Section 5 considers example problems where these methods are used to untie knots, unroll and untwist sheets, and visualize video data. 1.1 Background Topology-neutral NLDR algorithms can be divided into those that compute mappings, and those that directly compute low-dimensional embeddings. The ﬁ has its roots in mapeld ping algorithms: DeMers and Cottrell [3] proposed using auto-encoding neural networks with a hidden layer “ bottleneck,” effectively casting dimensionality reduction as a compression problem. Hastie deﬁ principal curves [5] as nonparametric 1 D curves that pass ned through the center of “ nearby” data points. A rich literature has grown up around properly regularizing this approach and extending it to surfaces. Smola and colleagues [10] analyzed the NLDR problem in the broader framework of regularized quantization methods. More recent advances aim for embeddings: Gomes and Mojsilovic [4] treat manifold completion as an anisotropic diffusion problem, iteratively expanding points until they connect to their neighbors. The I SO M AP algorithm [12] represents remote distances as sums of a trusted set of distances between immediate neighbors, then uses multidimensional scaling to compute a low-dimensional embedding that minimally distorts all distances. The locally linear embedding algorithm (LLE) [9] represents each point as a weighted combination of a trusted set of nearest neighbors, then computes a minimally distorting low-dimensional barycentric embedding. They have complementary strengths: I SO M AP handles holes well but can fail if the data hull is nonconvex [12]; and vice versa for LLE [9]. Both offer embeddings without mappings. It has been noted that trusted-set methods are vulnerable to noise because they consider the subset of point-to-point relationships that has the lowest signal-to-noise ratio; small changes to the trusted set can induce large changes in the set of constraints on the embedding, making solutions unstable [1]. In a return to mapping, Roweis and colleagues [8] proposed global coordination— learning a mixture of locally linear projections from sample to coordinate space. They constructed a posterior that penalizes distortions in the mapping, and gave a expectation-maximization (EM) training rule. Innovative use of variational methods highlighted the difﬁ culty of even hill-climbing their multimodal posterior. Like [2, 7, 6, 8], the method we develop below is a decomposition of the manifold into locally linear neighborhoods. It bears closest relation to global coordination [8], although by a different construction of the problem, we avoid hill-climbing a spiky posterior and instead develop a closed-form solution. 2 Estimating locally linear scale and intrinsic dimensionality . We begin with matrix of sample points Y = [y1 , · · · , yN ], yn ∈ RD populating a Ddimensional sample space, and a conjecture that these points are samples from a manifold M of intrinsic dimensionality d < D. We seek a mapping onto a vector space . G(Y) → X = [x1 , · · · , xN ], xn ∈ Rd and 1-to-1 reverse mapping G−1 (X) → Y such that local relations between nearby points are preserved (this will be formalized below). The map G should be non-catastrophic, that is, without folds: Parallel lines on the manifold in RD should map to continuous smooth non-intersecting curves in Rd . This guarantees that linear operations on X such as interpolation will have reasonable analogues on Y. Smoothness means that at some scale r the mapping from a neighborhood on M to Rd is effectively linear. Consider a ball of radius r centered on a data point and containing n(r) data points. The count n(r) grows as rd , but only at the locally linear scale; the grow rate is inﬂated by isotropic noise at smaller scales and by embedding curvature at larger scales. . To estimate r, we look at how the r-ball grows as points are added to it, tracking c(r) = d d log n(r) log r. At noise scales, c(r) ≈ 1/D < 1/d, because noise has distributed points in all directions with equal probability. At the scale at which curvature becomes signiﬁ cant, c(r) < 1/d, because the manifold is no longer perpendicular to the surface of the ball, so the ball does not have to grow as fast to accommodate new points. At the locally linear scale, the process peaks at c(r) = 1/d, because points are distributed only in the directions of the manifold’s local tangent space. The maximum of c(r) therefore gives an estimate of both the scale and the local dimensionality of the manifold (see ﬁ gure 1), provided that the ball hasn’t expanded to a manifold boundary— boundaries have lower dimension than Scale behavior of a 1D manifold in 2-space Point−count growth process on a 2D manifold in 3−space 1 10 radial growth process 1D hypothesis 2D hypothesis 3D hypothesis radius (log scale) samples noise scale locally linear scale curvature scale 0 10 2 1 10 2 10 #points (log scale) 3 10 Figure 1: Point growth processes. L EFT: At the locally linear scale, the number of points in an r-ball grows as rd ; at noise and curvature scales it grows faster. R IGHT: Using the point-count growth process to ﬁ the intrinsic dimensionality of a 2D manifold nonlinearly nd embedded in 3-space (see ﬁ gure 2). Lines of slope 1/3 , 1/2 , and 1 are ﬁ tted to sections of the log r/ log nr curve. For neighborhoods of radius r ≈ 1 with roughly n ≈ 10 points, the slope peaks at 1/2 indicating a dimensionality of d = 2. Below that, the data appears 3 D because it is dominated by noise (except for n ≤ D points); above, the data appears >2 D because of manifold curvature. As the r-ball expands to cover the entire data-set the dimensionality appears to drop to 1 as the process begins to track the 1D edges of the 2D sheet. the manifold. For low-dimensional manifolds such as sheets, the boundary submanifolds (edges and corners) are very small relative to the full manifold, so the boundary effect is typically limited to a small rise in c(r) as r approaches the scale of the entire data set. In practice, our code simply expands an r-ball at every point and looks for the ﬁ peak in rst c(r), averaged over many nearby r-balls. One can estimate d and r globally or per-point. 3 Charting the data In the charting step we ﬁ a soft partitioning of the data into locally linear low-dimensional nd neighborhoods, as a prelude to computing the connection that gives the global lowdimensional embedding. To minimize information loss in the connection, we require that the data points project into a subspace associated with each neighborhood with (1) minimal loss of local variance and (2) maximal agreement of the projections of nearby points into nearby neighborhoods. Criterion (1) is served by maximizing the likelihood function of a Gaussian mixture model (GMM) density ﬁ tted to the data: . p(yi |µ, Σ) = ∑ j p(yi |µ j , Σ j ) p j = ∑ j N (yi ; µ j , Σ j ) p j . (1) Each gaussian component deﬁ a local neighborhood centered around µ j with axes denes ﬁ ned by the eigenvectors of Σ j . The amount of data variance along each axis is indicated by the eigenvalues of Σ j ; if the data manifold is locally linear in the vicinity of the µ j , all but the d dominant eigenvalues will be near-zero, implying that the associated eigenvectors constitute the optimal variance-preserving local coordinate system. To some degree likelihood maximization will naturally realize this property: It requires that the GMM components shrink in volume to ﬁ the data as tightly as possible, which is best achieved by t positioning the components so that they “ pancake” onto locally ﬂat collections of datapoints. However, this state of affairs is easily violated by degenerate (zero-variance) GMM components or components ﬁ tted to overly small enough locales where the data density off the manifold is comparable to density on the manifold (e.g., at the noise scale). Consequently a prior is needed. Criterion (2) implies that neighboring partitions should have dominant axes that span similar subspaces, since disagreement (large subspace angles) would lead to inconsistent projections of a point and therefore uncertainty about its location in a low-dimensional coordinate space. The principal insight is that criterion (2) is exactly the cost of coding the location of a point in one neighborhood when it is generated by another neighborhood— the cross-entropy between the gaussian models deﬁ ning the two neighborhoods: D(N1 N2 ) = = dy N (y; µ1 ,Σ1 ) log N (y; µ1 ,Σ1 ) N (y; µ2 ,Σ2 ) (log |Σ−1 Σ2 | + trace(Σ−1 Σ1 ) + (µ2 −µ1 ) Σ−1 (µ2 −µ1 ) − D)/2. (2) 1 2 2 Roughly speaking, the terms in (2) measure differences in size, orientation, and position, respectively, of two coordinate frames located at the means µ1 , µ2 with axes speciﬁ by ed the eigenvectors of Σ1 , Σ2 . All three terms decline to zero as the overlap between the two frames is maximized. To maximize consistency between adjacent neighborhoods, we form . the prior p(µ, Σ) = exp[− ∑i= j mi (µ j )D(Ni N j )], where mi (µ j ) is a measure of co-locality. Unlike global coordination [8], we are not asking that the dominant axes in neighboring charts are aligned— only that they span nearly the same subspace. This is a much easier objective to satisfy, and it contains a useful special case where the posterior p(µ, Σ|Y) ∝ ∑i p(yi |µ, Σ)p(µ, Σ) is unimodal and can be maximized in closed form: Let us associate a gaussian neighborhood with each data-point, setting µi = yi ; take all neighborhoods to be a priori equally probable, setting pi = 1/N; and let the co-locality measure be determined from some local kernel. For example, in this paper we use mi (µ j ) ∝ N (µ j ; µi , σ2 ), with the scale parameter σ specifying the expected size of a neighborhood on the manifold in sample space. A reasonable choice is σ = r/2, so that 2erf(2) > 99.5% of the density of mi (µ j ) is contained in the area around yi where the manifold is expected to be locally linear. With uniform pi and µi , mi (µ j ) and ﬁ xed, the MAP estimates of the GMM covariances are Σi = ∑ mi (µ j ) (y j − µi )(y j − µi ) + (µ j − µi )(µ j − µi ) + Σ j j ∑ mi (µ j ) (3) . j Note that each covariance Σi is dependent on all other Σ j . The MAP estimators for all covariances can be arranged into a set of fully constrained linear equations and solved exactly for their mutually optimal values. This key step brings nonlocal information about the manifold’s shape into the local description of each neighborhood, ensuring that adjoining neighborhoods have similar covariances and small angles between their respective subspaces. Even if a local subset of data points are dense in a direction perpendicular to the manifold, the prior encourages the local chart to orient parallel to the manifold as part of a globally optimal solution, protecting against a pathology noted in [8]. Equation (3) is easily adapted to give a reduced number of charts and/or charts centered on local centroids. 4 Connecting the charts We now build a connection for set of charts speciﬁ as an arbitrary nondegenerate GMM. A ed GMM gives a soft partitioning of the dataset into neighborhoods of mean µk and covariance Σk . The optimal variance-preserving low-dimensional coordinate system for each neighborhood derives from its weighted principal component analysis, which is exactly speciﬁ ed by the eigenvectors of its covariance matrix: Eigendecompose Vk Λk Vk ← Σk with eigen. values in descending order on the diagonal of Λk and let Wk = [Id , 0]Vk be the operator . th projecting points into the k local chart, such that local chart coordinate uki = Wk (yi − µk ) . and Uk = [uk1 , · · · , ukN ] holds the local coordinates of all points. Our goal is to sew together all charts into a globally consistent low-dimensional coordinate system. For each chart there will be a low-dimensional afﬁ transform Gk ∈ R(d+1)×d ne that projects Uk into the global coordinate space. Summing over all charts, the weighted average of the projections of point yi into the low-dimensional vector space is W j (y − µ j ) 1 . x|y = ∑ G j j p j|y (y) . xi |yi = ∑ G j ⇒ u ji 1 j p j|y (yi ), (4) where pk|y (y) ∝ pk N (y; µk , Σk ), ∑k pk|y (y) = 1 is the probability that chart k generates point y. As pointed out in [8], if a point has nonzero probabilities in two charts, then there should be afﬁ transforms of those two charts that map the point to the same place in a ne global coordinate space. We set this up as a weighted least-squares problem: . G = [G1 , · · · , GK ] = arg min uki 1 ∑ pk|y (yi )p j|y (yi ) Gk Gk ,G j i −Gj u ji 1 2 . (5) F Equation (5) generates a homogeneous set of equations that determines a solution up to an afﬁ transform of G. There are two solution methods. First, let us temporarily anchor one ne neighborhood at the origin to ﬁ this indeterminacy. This adds the constraint G1 = [I, 0] . x . To solve, deﬁ indicator matrix Fk = [0, · · · , 0, I, 0, · · · , 0] with the identity mane . trix occupying the kth block, such that Gk = GFk . Let the diagonal of Pk = diag([pk|y (y1 ), · · · , pk|y (yN )]) record the per-point posteriors of chart k. The squared error of the connection is then a sum of of all patch-to-anchor and patch-to-patch inconsistencies: . E =∑ (GUk − k U1 0 2 )Pk P1 F + ∑ (GU j − GUk )P j Pk j=k 2 F ; . Uk = Fk Uk 1 . (6) Setting dE /dG = 0 and solving to minimize convex E gives −1 G = ∑ Uk P2 k k ∑ j=k P2 j Uk − ∑ ∑ Uk P2 P2 k 1 Uk P2 P2 U j k j k j=k U1 0 . (7) We now remove the dependence on a reference neighborhood G1 by rewriting equation 5, G = arg min ∑ j=k (GU j − GUk )P j Pk G 2 F = GQ 2 F = trace(GQQ G ) , (8) . where Q = ∑ j=k U j − Uk P j Pk . If we require that GG = I to prevent degenerate solutions, then equation (8) is solved (up to rotation in coordinate space) by setting G to the eigenvectors associated with the smallest eigenvalues of QQ . The eigenvectors can be computed efﬁ ciently without explicitly forming QQ ; other numerical efﬁ ciencies obtain by zeroing any vanishingly small probabilities in each Pk , yielding a sparse eigenproblem. A more interesting strategy is to numerically condition the problem by calculating the trailing eigenvectors of QQ + 1. It can be shown that this maximizes the posterior 2 p(G|Q) ∝ p(Q|G)p(G) ∝ e− GQ F e− G1 , where the prior p(G) favors a mapping G whose unit-norm rows are also zero-mean. This maximizes variance in each row of G and thereby spreads the projected points broadly and evenly over coordinate space. The solutions for MAP charts (equation (5)) and connection (equation (8)) can be applied to any well-ﬁ tted mixture of gaussians/factors1 /PCAs density model; thus large eigenproblems can be avoided by connecting just a small number of charts that cover the data. 1 We thank reviewers for calling our attention to Teh & Roweis ([11]— in this volume), which shows how to connect a set of given local dimensionality reducers in a generalized eigenvalue problem that is related to equation (8). LLE, n=5 charting (projection onto coordinate space) charting best Isomap LLE, n=6 LLE, n=7 LLE, n=8 random subset of local charts XYZ view LLE, n=9 LLE, n=10 XZ view data (linked) embedding, XY view XY view original data reconstruction (back−projected coordinate grid) best LLE (regularized) Figure 2: The twisted curl problem. L EFT: Comparison of charting, I SO M AP, & LLE. 400 points are randomly sampled from the manifold with noise. Charting is the only method that recovers the original space without catastrophes (folding), albeit with some shear. R IGHT: The manifold is regularly sampled (with noise) to illustrate the forward and backward projections. Samples are shown linked into lines to help visualize the manifold structure. Coordinate axes of a random selection of charts are shown as bold lines. Connecting subsets of charts such as this will also give good mappings. The upper right quadrant shows various LLE results. At bottom we show the charting solution and the reconstructed (back-projected) manifold, which smooths out the noise. Once the connection is solved, equation (4) gives the forward projection of any point y down into coordinate space. There are several numerically distinct candidates for the backprojection: posterior mean, mode, or exact inverse. In general, there may not be a unique posterior mode and the exact inverse is not solvable in closed form (this is also true of [8]). Note that chart-wise projection deﬁ a complementary density in coordinate space nes px|k (x) = N (x; Gk 0 1 , Gk [Id , 0]Λk [Id , 0] 0 0 0 Gk ). (9) Let p(y|x, k), used to map x into subspace k on the surface of the manifold, be a Dirac delta function whose mean is a linear function of x. Then the posterior mean back-projection is obtained by integrating out uncertainty over which chart generates x: y|x = ∑ pk|x (x) k µk + Wk Gk I 0 + x − Gk 0 1 , (10) where (·)+ denotes pseudo-inverse. In general, a back-projecting map should not reconstruct the original points. Instead, equation (10) generates a surface that passes through the weighted average of the µi of all the neighborhoods in which yi has nonzero probability, much like a principal curve passes through the center of each local group of points. 5 Experiments Synthetic examples: 400 2 D points were randomly sampled from a 2 D square and embedded in 3 D via a curl and twist, then contaminated with gaussian noise. Even if noiselessly sampled, this manifold cannot be “ unrolled” without distortion. In addition, the outer curl is sampled much less densely than the inner curl. With an order of magnitude fewer points, higher noise levels, no possibility of an isometric mapping, and uneven sampling, this is arguably a much more challenging problem than the “ swiss roll” and “ s-curve” problems featured in [12, 9, 8, 1]. Figure 2LEFT contrasts the (unique) output of charting and the best outputs obtained from I SO M AP and LLE (considering all neighborhood sizes between 2 and 20 points). I SO M AP and LLE show catastrophic folding; we had to change LLE’s b. data, yz view c. local charts d. 2D embedding e. 1D embedding 1D ordinate a. data, xy view true manifold arc length Figure 3: Untying a trefoil knot ( ) by charting. 900 noisy samples from a 3 D-embedded 1 D manifold are shown as connected dots in front (a) and side (b) views. A subset of charts is shown in (c). Solving for the 2 D connection gives the “ unknot” in (d). After removing some points to cut the knot, charting gives a 1 D embedding which we plot against true manifold arc length in (e); monotonicity (modulo noise) indicates correctness. Three principal degrees of freedom recovered from raw jittered images pose scale expression images synthesized via backprojection of straight lines in coordinate space Figure 4: Modeling the manifold of facial images from raw video. Each row contains images synthesized by back-projecting an axis-parallel straight line in coordinate space onto the manifold in image space. Blurry images correspond to points on the manifold whose neighborhoods contain few if any nearby data points. regularization in order to coax out nondegenerate (>1 D) solutions. Although charting is not designed for isometry, after afﬁ transform the forward-projected points disagree with ne the original points with an RMS error of only 1.0429, lower than the best LLE (3.1423) or best I SO M AP (1.1424, not shown). Figure 2RIGHT shows the same problem where points are sampled regularly from a grid, with noise added before and after embedding. Figure 3 shows a similar treatment of a 1 D line that was threaded into a 3 D trefoil knot, contaminated with gaussian noise, and then “ untied” via charting. Video: We obtained a 1965-frame video sequence (courtesy S. Roweis and B. Frey) of 20 × 28-pixel images in which B.F. strikes a variety of poses and expressions. The video is heavily contaminated with synthetic camera jitters. We used raw images, though image processing could have removed this and other uninteresting sources of variation. We took a 500-frame subsequence and left-right mirrored it to obtain 1000 points in 20 × 28 = 560D image space. The point-growth process peaked just above d = 3 dimensions. We solved for 25 charts, each centered on a random point, and a 3D connection. The recovered degrees of freedom— recognizable as pose, scale, and expression— are visualized in ﬁ gure 4. original data stereographic map to 3D fishbowl charting Figure 5: Flattening a ﬁ shbowl. From the left: Original 2000×2D points; their stereographic mapping to a 3D ﬁ shbowl; its 2D embedding recovered using 500 charts; and the stereographic map. Fewer charts lead to isometric mappings that fold the bowl (not shown). Conformality: Some manifolds can be ﬂattened conformally (preserving local angles) but not isometrically. Figure 5 shows that if the data is ﬁ nely charted, the connection behaves more conformally than isometrically. This problem was suggested by J. Tenenbaum. 6 Discussion Charting breaks kernel-based NLDR into two subproblems: (1) Finding a set of datacovering locally linear neighborhoods (“ charts” ) such that adjoining neighborhoods span maximally similar subspaces, and (2) computing a minimal-distortion merger (“ connection” ) of all charts. The solution to (1) is optimal w.r.t. the estimated scale of local linearity r; the solution to (2) is optimal w.r.t. the solution to (1) and the desired dimensionality d. Both problems have Bayesian settings. By ofﬂoading the nonlinearity onto the kernels, we obtain least-squares problems and closed form solutions. This scheme is also attractive because large eigenproblems can be avoided by using a reduced set of charts. The dependence on r, like trusted-set methods, is a potential source of solution instability. In practice the point-growth estimate seems fairly robust to data perturbations (to be expected if the data density changes slowly over a manifold of integral Hausdorff dimension), while the use of a soft neighborhood partitioning appears to make charting solutions reasonably stable to variations in r. Eigenvalue stability analyses may prove useful here. Ultimately, we would prefer to integrate r out. In contrast, use of d appears to be a virtue: Unlike other eigenvector-based methods, the best d-dimensional embedding is not merely a linear projection of the best d + 1-dimensional embedding; a unique distortion is found for each value of d that maximizes the information content of its embedding. Why does charting performs well on datasets where the signal-to-noise ratio confounds recent state-of-the-art methods? Two reasons may be adduced: (1) Nonlocal information is used to construct both the system of local charts and their global connection. (2) The mapping only preserves the component of local point-to-point distances that project onto the manifold; relationships perpendicular to the manifold are discarded. Thus charting uses global shape information to suppress noise in the constraints that determine the mapping. Acknowledgments Thanks to J. Buhmann, S. Makar, S. Roweis, J. Tenenbaum, and anonymous reviewers for insightful comments and suggested “ challenge” problems. References [1] M. Balasubramanian and E. L. Schwartz. The IsoMap algorithm and topological stability. Science, 295(5552):7, January 2002. [2] C. Bregler and S. Omohundro. Nonlinear image interpolation using manifold learning. In NIPS–7, 1995. [3] D. DeMers and G. Cottrell. Nonlinear dimensionality reduction. In NIPS–5, 1993. [4] J. Gomes and A. Mojsilovic. A variational approach to recovering a manifold from sample points. In ECCV, 2002. [5] T. Hastie and W. Stuetzle. Principal curves. J. Am. Statistical Assoc, 84(406):502–516, 1989. [6] G. Hinton, P. Dayan, and M. Revow. Modeling the manifolds of handwritten digits. IEEE Trans. Neural Networks, 8, 1997. [7] N. Kambhatla and T. Leen. Dimensionality reduction by local principal component analysis. Neural Computation, 9, 1997. [8] S. Roweis, L. Saul, and G. Hinton. Global coordination of linear models. In NIPS–13, 2002. [9] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, December 22 2000. [10] A. Smola, S. Mika, B. Schölkopf, and R. Williamson. Regularized principal manifolds. Machine Learning, 1999. [11] Y. W. Teh and S. T. Roweis. Automatic alignment of hidden representations. In NIPS–15, 2003. [12] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, December 22 2000.</p><p>6 0.68946934 <a title="62-lda-6" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>7 0.67453659 <a title="62-lda-7" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>8 0.63245475 <a title="62-lda-8" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>9 0.61835718 <a title="62-lda-9" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>10 0.59869426 <a title="62-lda-10" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>11 0.5985257 <a title="62-lda-11" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>12 0.58381271 <a title="62-lda-12" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>13 0.57434696 <a title="62-lda-13" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>14 0.57335383 <a title="62-lda-14" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>15 0.56994784 <a title="62-lda-15" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>16 0.56909913 <a title="62-lda-16" href="./nips-2002-Prediction_of_Protein_Topologies_Using_Generalized_IOHMMs_and_RNNs.html">164 nips-2002-Prediction of Protein Topologies Using Generalized IOHMMs and RNNs</a></p>
<p>17 0.56579745 <a title="62-lda-17" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>18 0.56509519 <a title="62-lda-18" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>19 0.56006783 <a title="62-lda-19" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>20 0.55918097 <a title="62-lda-20" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
