<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-54" href="#">nips2002-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</h1>
<br/><p>Source: <a title="nips-2002-54-pdf" href="http://papers.nips.cc/paper/2249-combining-dimensions-and-features-in-similarity-based-representations.pdf">pdf</a></p><p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="nips-2002-54-reference" href="../nips2002_reference/nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bt', 0.499), ('sti', 0.471), ('btr', 0.399), ('bp', 0.304), ('ys', 0.199), ('js', 0.197), ('btsj', 0.153), ('sj', 0.142), ('st', 0.129), ('tr', 0.124), ('zr', 0.113), ('qy', 0.107), ('pb', 0.102), ('sr', 0.093), ('jk', 0.09), ('jb', 0.071), ('bs', 0.065), ('yst', 0.061), ('br', 0.061), ('sjz', 0.056), ('srz', 0.056), ('bb', 0.056), ('ak', 0.054), ('tb', 0.049), ('tzbzr', 0.046), ('bz', 0.041), ('saj', 0.041), ('zjs', 0.041), ('jr', 0.041), ('sii', 0.04), ('baj', 0.036), ('brr', 0.036), ('szr', 0.036), ('ybbr', 0.036), ('rz', 0.036), ('sjj', 0.031), ('sjjk', 0.031), ('srr', 0.031), ('ybbi', 0.031), ('ybz', 0.031), ('yt', 0.031), ('jj', 0.027), ('yp', 0.027), ('wd', 0.027), ('sjrb', 0.026), ('ysr', 0.026), ('zsj', 0.026), ('rr', 0.024), ('ti', 0.024), ('az', 0.022), ('bjj', 0.02), ('pbz', 0.02), ('ybr', 0.02), ('rb', 0.02), ('ts', 0.019), ('sk', 0.018), ('jz', 0.018), ('basa', 0.015), ('bsk', 0.015), ('bti', 0.015), ('btjk', 0.015), ('dwad', 0.015), ('hti', 0.015), ('jzr', 0.015), ('pbjjbb', 0.015), ('pszrr', 0.015), ('pzj', 0.015), ('pzt', 0.015), ('sth', 0.015), ('tpb', 0.015), ('ybb', 0.015), ('ybir', 0.015), ('zti', 0.015), ('ib', 0.015), ('wi', 0.015), ('bsr', 0.013), ('wr', 0.013), ('pz', 0.013), ('zi', 0.012), ('bi', 0.012), ('vj', 0.012), ('zj', 0.011), ('kd', 0.011), ('ab', 0.011), ('dd', 0.011), ('bbi', 0.01), ('bjs', 0.01), ('bskr', 0.01), ('btir', 0.01), ('bzr', 0.01), ('jbr', 0.01), ('jsrr', 0.01), ('jzi', 0.01), ('jzsj', 0.01), ('jzsjjk', 0.01), ('ktasr', 0.01), ('rbj', 0.01), ('rbz', 0.01), ('rys', 0.01), ('sabz', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="54-tfidf-1" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>2 0.083507553 <a title="54-tfidf-2" href="./nips-2002-Approximate_Inference_and_Protein-Folding.html">32 nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>Author: Chen Yanover, Yair Weiss</p><p>Abstract: Side-chain prediction is an important subtask in the protein-folding problem. We show that finding a minimal energy side-chain configuration is equivalent to performing inference in an undirected graphical model. The graphical model is relatively sparse yet has many cycles. We used this equivalence to assess the performance of approximate inference algorithms in a real-world setting. Specifically we compared belief propagation (BP), generalized BP (GBP) and naive mean field (MF). In cases where exact inference was possible, max-product BP always found the global minimum of the energy (except in few cases where it failed to converge), while other approximation algorithms of similar complexity did not. In the full protein data set, maxproduct BP always found a lower energy configuration than the other algorithms, including a widely used protein-folding software (SCWRL). 1</p><p>3 0.060608972 <a title="54-tfidf-3" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>Author: Shinji Watanabe, Yasuhiro Minami, Atsushi Nakamura, Naonori Ueda</p><p>Abstract: In this paper, we propose a Bayesian framework, which constructs shared-state triphone HMMs based on a variational Bayesian approach, and recognizes speech based on the Bayesian prediction classiﬁcation; variational Bayesian estimation and clustering for speech recognition (VBEC). An appropriate model structure with high recognition performance can be found within a VBEC framework. Unlike conventional methods, including BIC or MDL criterion based on the maximum likelihood approach, the proposed model selection is valid in principle, even when there are insufﬁcient amounts of data, because it does not use an asymptotic assumption. In isolated word recognition experiments, we show the advantage of VBEC over conventional methods, especially when dealing with small amounts of data.</p><p>4 0.054490771 <a title="54-tfidf-4" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>5 0.04963962 <a title="54-tfidf-5" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>Author: Marina Sokolova, Mario Marchand, Nathalie Japkowicz, John S. Shawe-taylor</p><p>Abstract: We introduce a new learning algorithm for decision lists to allow features that are constructed from the data and to allow a tradeoﬀ between accuracy and complexity. We bound its generalization error in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. 1</p><p>6 0.046374641 <a title="54-tfidf-6" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>7 0.037188973 <a title="54-tfidf-7" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>8 0.03660674 <a title="54-tfidf-8" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>9 0.025413372 <a title="54-tfidf-9" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>10 0.023850352 <a title="54-tfidf-10" href="./nips-2002-Handling_Missing_Data_with_Variational_Bayesian_Learning_of_ICA.html">101 nips-2002-Handling Missing Data with Variational Bayesian Learning of ICA</a></p>
<p>11 0.0225979 <a title="54-tfidf-11" href="./nips-2002-Fractional_Belief_Propagation.html">94 nips-2002-Fractional Belief Propagation</a></p>
<p>12 0.019967595 <a title="54-tfidf-12" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>13 0.017086189 <a title="54-tfidf-13" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>14 0.015804356 <a title="54-tfidf-14" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>15 0.012982072 <a title="54-tfidf-15" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>16 0.011464055 <a title="54-tfidf-16" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>17 0.011405021 <a title="54-tfidf-17" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>18 0.011378929 <a title="54-tfidf-18" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>19 0.01048651 <a title="54-tfidf-19" href="./nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">111 nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>20 0.0096952599 <a title="54-tfidf-20" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.023), (1, -0.009), (2, -0.026), (3, 0.007), (4, -0.002), (5, 0.017), (6, -0.027), (7, 0.03), (8, 0.007), (9, 0.015), (10, 0.014), (11, -0.005), (12, -0.051), (13, -0.001), (14, -0.046), (15, -0.043), (16, 0.001), (17, 0.058), (18, -0.026), (19, 0.02), (20, 0.017), (21, -0.04), (22, 0.021), (23, -0.037), (24, -0.075), (25, 0.061), (26, 0.01), (27, 0.026), (28, 0.036), (29, -0.057), (30, 0.002), (31, -0.025), (32, 0.029), (33, 0.022), (34, -0.162), (35, 0.056), (36, 0.145), (37, -0.115), (38, 0.06), (39, 0.071), (40, 0.07), (41, -0.015), (42, 0.117), (43, 0.319), (44, -0.067), (45, -0.142), (46, -0.074), (47, -0.289), (48, -0.063), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99944168 <a title="54-lsi-1" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>2 0.43946931 <a title="54-lsi-2" href="./nips-2002-Handling_Missing_Data_with_Variational_Bayesian_Learning_of_ICA.html">101 nips-2002-Handling Missing Data with Variational Bayesian Learning of ICA</a></p>
<p>Author: Kwokleung Chan, Te-Won Lee, Terrence J. Sejnowski</p><p>Abstract: Missing data is common in real-world datasets and is a problem for many estimation techniques. We have developed a variational Bayesian method to perform Independent Component Analysis (ICA) on high-dimensional data containing missing entries. Missing data are handled naturally in the Bayesian framework by integrating the generative density model. Modeling the distributions of the independent sources with mixture of Gaussians allows sources to be estimated with different kurtosis and skewness. The variational Bayesian method automatically determines the dimensionality of the data and yields an accurate density model for the observed data without overﬁtting problems. This allows direct probability estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data.</p><p>3 0.37427223 <a title="54-lsi-3" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>Author: Shinji Watanabe, Yasuhiro Minami, Atsushi Nakamura, Naonori Ueda</p><p>Abstract: In this paper, we propose a Bayesian framework, which constructs shared-state triphone HMMs based on a variational Bayesian approach, and recognizes speech based on the Bayesian prediction classiﬁcation; variational Bayesian estimation and clustering for speech recognition (VBEC). An appropriate model structure with high recognition performance can be found within a VBEC framework. Unlike conventional methods, including BIC or MDL criterion based on the maximum likelihood approach, the proposed model selection is valid in principle, even when there are insufﬁcient amounts of data, because it does not use an asymptotic assumption. In isolated word recognition experiments, we show the advantage of VBEC over conventional methods, especially when dealing with small amounts of data.</p><p>4 0.37083176 <a title="54-lsi-4" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><p>5 0.34143943 <a title="54-lsi-5" href="./nips-2002-Approximate_Inference_and_Protein-Folding.html">32 nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>Author: Chen Yanover, Yair Weiss</p><p>Abstract: Side-chain prediction is an important subtask in the protein-folding problem. We show that finding a minimal energy side-chain configuration is equivalent to performing inference in an undirected graphical model. The graphical model is relatively sparse yet has many cycles. We used this equivalence to assess the performance of approximate inference algorithms in a real-world setting. Specifically we compared belief propagation (BP), generalized BP (GBP) and naive mean field (MF). In cases where exact inference was possible, max-product BP always found the global minimum of the energy (except in few cases where it failed to converge), while other approximation algorithms of similar complexity did not. In the full protein data set, maxproduct BP always found a lower energy configuration than the other algorithms, including a widely used protein-folding software (SCWRL). 1</p><p>6 0.32317832 <a title="54-lsi-6" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>7 0.29887992 <a title="54-lsi-7" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>8 0.23537175 <a title="54-lsi-8" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>9 0.19937541 <a title="54-lsi-9" href="./nips-2002-Conditional_Models_on_the_Ranking_Poset.html">58 nips-2002-Conditional Models on the Ranking Poset</a></p>
<p>10 0.19734675 <a title="54-lsi-10" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>11 0.19417043 <a title="54-lsi-11" href="./nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">25 nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>12 0.1732318 <a title="54-lsi-12" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>13 0.16336386 <a title="54-lsi-13" href="./nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">111 nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>14 0.14887981 <a title="54-lsi-14" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>15 0.1424679 <a title="54-lsi-15" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>16 0.13673298 <a title="54-lsi-16" href="./nips-2002-Rational_Kernels.html">167 nips-2002-Rational Kernels</a></p>
<p>17 0.12225191 <a title="54-lsi-17" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>18 0.12156018 <a title="54-lsi-18" href="./nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">131 nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>19 0.10715136 <a title="54-lsi-19" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>20 0.10230595 <a title="54-lsi-20" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(41, 0.763), (67, 0.029), (79, 0.011), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99679387 <a title="54-lda-1" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>2 0.59557003 <a title="54-lda-2" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><p>3 0.45410204 <a title="54-lda-3" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>4 0.43229717 <a title="54-lda-4" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>5 0.38625145 <a title="54-lda-5" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>Author: Patrick J. Wolfe, Simon J. Godsill</p><p>Abstract: The Bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music—something which has often been overlooked in traditional audio signal processing approaches. Here, after constructing a Bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms, we apply Markov chain Monte Carlo methods in order to sample from the resultant posterior distribution of interest. We present speech enhancement results which compare favourably in objective terms with standard time-varying ﬁltering techniques (and in several cases yield superior performance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledge of the noise power.</p><p>6 0.077515088 <a title="54-lda-6" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>7 0.058863997 <a title="54-lda-7" href="./nips-2002-Handling_Missing_Data_with_Variational_Bayesian_Learning_of_ICA.html">101 nips-2002-Handling Missing Data with Variational Bayesian Learning of ICA</a></p>
<p>8 0.058622397 <a title="54-lda-8" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>9 0.057764895 <a title="54-lda-9" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>10 0.051256478 <a title="54-lda-10" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>11 0.040182129 <a title="54-lda-11" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>12 0.040150501 <a title="54-lda-12" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>13 0.039179966 <a title="54-lda-13" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>14 0.034258977 <a title="54-lda-14" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<p>15 0.02921685 <a title="54-lda-15" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>16 0.026823292 <a title="54-lda-16" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>17 0.021601584 <a title="54-lda-17" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>18 0.015605174 <a title="54-lda-18" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>19 0.015307419 <a title="54-lda-19" href="./nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">12 nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<p>20 0.015242104 <a title="54-lda-20" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
