<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2002-Robust Novelty Detection with Single-Class MPM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-178" href="#">nips2002-178</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>178 nips-2002-Robust Novelty Detection with Single-Class MPM</h1>
<br/><p>Source: <a title="nips-2002-178-pdf" href="http://papers.nips.cc/paper/2187-robust-novelty-detection-with-single-class-mpm.pdf">pdf</a></p><p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>Reference: <a title="nips-2002-178-reference" href="../nips2002_reference/nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pin', 0.454), ('mpm', 0.389), ('aty', 0.244), ('jrn', 0.244), ('jat', 0.203), ('quantil', 0.203), ('lanckriet', 0.189), ('yo', 0.177), ('pr', 0.174), ('atx', 0.142), ('robust', 0.139), ('cov', 0.13), ('ey', 0.127), ('misclass', 0.127), ('znew', 0.122), ('uncertainty', 0.111), ('ya', 0.106), ('minimax', 0.106), ('hyperpl', 0.105), ('max', 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="178-tfidf-1" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>2 0.089800648 <a title="178-tfidf-2" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>Author: E. Solak, R. Murray-smith, W. E. Leithead, D. J. Leith, Carl E. Rasmussen</p><p>Abstract: Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.</p><p>3 0.088460751 <a title="178-tfidf-3" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>Author: Olivier Bousquet, Daniel Herrmann</p><p>Abstract: We investigate data based procedures for selecting the kernel when learning with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors ﬁx. This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to overﬁtting. We thus propose a suitable way of constraining the class. We use an efﬁcient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach.</p><p>4 0.0825525 <a title="178-tfidf-4" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>5 0.070194945 <a title="178-tfidf-5" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>6 0.06614431 <a title="178-tfidf-6" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>7 0.065864556 <a title="178-tfidf-7" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>8 0.064659983 <a title="178-tfidf-8" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>9 0.063117974 <a title="178-tfidf-9" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>10 0.062179148 <a title="178-tfidf-10" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>11 0.06014042 <a title="178-tfidf-11" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>12 0.059133962 <a title="178-tfidf-12" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>13 0.050790068 <a title="178-tfidf-13" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>14 0.050036211 <a title="178-tfidf-14" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>15 0.049281519 <a title="178-tfidf-15" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>16 0.048993029 <a title="178-tfidf-16" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>17 0.048764467 <a title="178-tfidf-17" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>18 0.048642255 <a title="178-tfidf-18" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>19 0.046152957 <a title="178-tfidf-19" href="./nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">197 nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>20 0.04493776 <a title="178-tfidf-20" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, 0.066), (2, -0.009), (3, 0.023), (4, 0.017), (5, -0.029), (6, -0.073), (7, 0.011), (8, -0.0), (9, -0.031), (10, 0.029), (11, -0.021), (12, -0.095), (13, -0.027), (14, -0.034), (15, 0.042), (16, 0.052), (17, 0.007), (18, -0.043), (19, 0.06), (20, -0.03), (21, -0.008), (22, -0.079), (23, -0.034), (24, 0.0), (25, -0.006), (26, -0.043), (27, -0.012), (28, 0.004), (29, -0.023), (30, -0.024), (31, -0.069), (32, 0.041), (33, -0.022), (34, 0.025), (35, -0.07), (36, -0.074), (37, -0.033), (38, 0.024), (39, 0.079), (40, -0.052), (41, 0.111), (42, -0.145), (43, -0.147), (44, 0.048), (45, -0.017), (46, -0.059), (47, -0.152), (48, -0.059), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87616432 <a title="178-lsi-1" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>2 0.5400995 <a title="178-lsi-2" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>Author: Thomas Strohmann, Gregory Z. Grudic</p><p>Abstract: We formulate the regression problem as one of maximizing the minimum probability, symbolized by Ω, that future predicted outputs of the regression model will be within some ±ε bound of the true regression function. Our formulation is unique in that we obtain a direct estimate of this lower probability bound Ω. The proposed framework, minimax probability machine regression (MPMR), is based on the recently described minimax probability machine classiﬁcation algorithm [Lanckriet et al.] and uses Mercer Kernels to obtain nonlinear regression models. MPMR is tested on both toy and real world data, verifying the accuracy of the Ω bound, and the efﬁcacy of the regression models. 1</p><p>3 0.53371543 <a title="178-lsi-3" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>4 0.50243521 <a title="178-lsi-4" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>Author: Elzbieta Pekalska, David Tax, Robert Duin</p><p>Abstract: Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identiﬁcation or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classiﬁer for dissimilarity representations. By the use of linear programming an efﬁcient one-class description can be found, based on a small number of prototype objects. This classiﬁer can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classiﬁer by Campbell and Bennett is given.</p><p>5 0.49436337 <a title="178-lsi-5" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>6 0.48911282 <a title="178-lsi-6" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>7 0.45561725 <a title="178-lsi-7" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>8 0.45058191 <a title="178-lsi-8" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>9 0.43459171 <a title="178-lsi-9" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>10 0.4281435 <a title="178-lsi-10" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>11 0.42680135 <a title="178-lsi-11" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>12 0.42031246 <a title="178-lsi-12" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>13 0.41621733 <a title="178-lsi-13" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>14 0.40990707 <a title="178-lsi-14" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>15 0.40454501 <a title="178-lsi-15" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>16 0.39910451 <a title="178-lsi-16" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>17 0.3981922 <a title="178-lsi-17" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>18 0.39641646 <a title="178-lsi-18" href="./nips-2002-Manifold_Parzen_Windows.html">138 nips-2002-Manifold Parzen Windows</a></p>
<p>19 0.38624889 <a title="178-lsi-19" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>20 0.37933695 <a title="178-lsi-20" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(22, 0.572), (26, 0.018), (39, 0.04), (47, 0.057), (48, 0.092), (66, 0.036), (72, 0.029), (93, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95415723 <a title="178-lda-1" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: Although the study of clustering is centered around an intuitively compelling goal, it has been very diﬃcult to develop a uniﬁed framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the diﬃculty in ﬁnding such a uniﬁcation, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-oﬀs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median. 1</p><p>2 0.88170958 <a title="178-lda-2" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>Author: Luk Chong Yeung, Brian S. Blais, Leon N. Cooper, Harel Z. Shouval</p><p>Abstract: A uniﬁed, biophysically motivated Calcium-Dependent Learning model has been shown to account for various rate-based and spike time-dependent paradigms for inducing synaptic plasticity. Here, we investigate the properties of this model for a multi-synapse neuron that receives inputs with diﬀerent spike-train statistics. In addition, we present a physiological form of metaplasticity, an activity-driven regulation mechanism, that is essential for the robustness of the model. A neuron thus implemented develops stable and selective receptive ﬁelds, given various input statistics 1</p><p>same-paper 3 0.8447367 <a title="178-lda-3" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>Author: Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: In this paper we consider the problem of novelty detection, presenting an algorithm that aims to find a minimal region in input space containing a fraction 0: of the probability mass underlying a data set. This algorithm- the</p><p>4 0.81275755 <a title="178-lda-4" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>Author: Tatsuto Murayama, Masato Okada</p><p>Abstract: We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions. 1</p><p>5 0.71203959 <a title="178-lda-5" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>Author: Christopher Williams, Michalis K. Titsias</p><p>Abstract: We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.</p><p>6 0.5344134 <a title="178-lda-6" href="./nips-2002-Analysis_of_Information_in_Speech_Based_on_MANOVA.html">29 nips-2002-Analysis of Information in Speech Based on MANOVA</a></p>
<p>7 0.53176951 <a title="178-lda-7" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>8 0.51624399 <a title="178-lda-8" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>9 0.5130266 <a title="178-lda-9" href="./nips-2002-A_Differential_Semantics_for_Jointree_Algorithms.html">4 nips-2002-A Differential Semantics for Jointree Algorithms</a></p>
<p>10 0.49661064 <a title="178-lda-10" href="./nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">98 nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>11 0.48708794 <a title="178-lda-11" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>12 0.47830832 <a title="178-lda-12" href="./nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">142 nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>13 0.47784662 <a title="178-lda-13" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>14 0.47687849 <a title="178-lda-14" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>15 0.47394827 <a title="178-lda-15" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>16 0.47139144 <a title="178-lda-16" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>17 0.4574118 <a title="178-lda-17" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>18 0.4567464 <a title="178-lda-18" href="./nips-2002-Regularized_Greedy_Importance_Sampling.html">174 nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>19 0.45539528 <a title="178-lda-19" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>20 0.4552533 <a title="178-lda-20" href="./nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">30 nips-2002-Annealing and the Rate Distortion Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
