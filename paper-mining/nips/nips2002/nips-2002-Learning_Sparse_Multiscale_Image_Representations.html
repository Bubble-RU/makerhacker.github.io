<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2002-Learning Sparse Multiscale Image Representations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-126" href="#">nips2002-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2002-Learning Sparse Multiscale Image Representations</h1>
<br/><p>Source: <a title="nips-2002-126-pdf" href="http://papers.nips.cc/paper/2145-learning-sparse-multiscale-image-representations.pdf">pdf</a></p><p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>Reference: <a title="nips-2002-126-reference" href="../nips2002_reference/nips-2002-Learning_Sparse_Multiscale_Image_Representations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. [sent-7, score-0.806]
</p><p>2 The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. [sent-8, score-0.248]
</p><p>3 Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. [sent-9, score-0.303]
</p><p>4 The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. [sent-10, score-0.2]
</p><p>5 Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. [sent-11, score-0.416]
</p><p>6 1  Introduction  Increasing interest has been given to the use of overcomplete representations for natural scenes, where the number of basis functions exceeds the number of image pixels. [sent-12, score-0.845]
</p><p>7 This may translate into gains in coding eﬃciency for image compression, and improved accuracy for tasks such as denoising. [sent-14, score-0.358]
</p><p>8 Overcomplete representations have been shown to reduce Gibbs-like artifacts common to thresholding methods employing critically sampled wavelets [4, 3, 9]. [sent-15, score-0.56]
</p><p>9 Common wavelet denoising approaches generally apply either a hard or softthresholding function to coeﬃcients which have been obtained by ﬁltering an image with a the basis functions. [sent-16, score-0.834]
</p><p>10 One can view these thresholding methods as a means of selecting coeﬃcients for an image based on an assumed sparse prior on the coeﬃcients [1, 2]. [sent-17, score-0.652]
</p><p>11 This statistical framework provides a principled means of selecting an appropriate thresholding function. [sent-18, score-0.275]
</p><p>12 When such thresholding methods are applied to overcomplete representations, however, problems arise due to the dependencies between coeﬃcients. [sent-19, score-0.479]
</p><p>13 Choosing optimal thresholds for a non-orthogonal basis is still  an unsolved problem. [sent-20, score-0.232]
</p><p>14 In one approach, orthogonal subgroups of an overcomplete shift-invariant expansion are thresholded separately and then the results are combined by averaging [4, 3]. [sent-21, score-0.32]
</p><p>15 Here we address two major issues regarding the use of overcomplete representations for images. [sent-23, score-0.413]
</p><p>16 First, current methods make use of various overcomplete wavelet bases. [sent-24, score-0.566]
</p><p>17 What is the optimal basis to use for a speciﬁc class of data? [sent-25, score-0.157]
</p><p>18 To help answer this question, we describe how to adapt an overcomplete wavelet basis to the statistics of natural images. [sent-26, score-0.723]
</p><p>19 Secondly, we address the problem of properly inferring the coeﬃcients for an image when the basis is overcomplete. [sent-27, score-0.49]
</p><p>20 We avoid problems associated with thresholding by using the wavelet basis as part of a generative model, rather than a simple ﬁltering mechanism. [sent-28, score-0.718]
</p><p>21 We then sample the coeﬃcients from the resulting posterior distribution by simulating a Markov process known as a Gibbs-sampler. [sent-29, score-0.058]
</p><p>22 Our previous work in this area made use of a prior distribution peaked at zero and tapering away smoothly to obtain sparse coeﬃcients [7]. [sent-30, score-0.216]
</p><p>23 However, we encountered a number of signiﬁcant limitations with this method. [sent-31, score-0.032]
</p><p>24 First, the smooth priors do not force inactive coeﬃcients to have values exactly equal to zero, resulting in decreased coding eﬃciency. [sent-32, score-0.231]
</p><p>25 Eﬃciency may be partially regained by thresholding the near-zero coeﬃcients, but due to the non-orthogonality of the representation this will produce sub-optimal results as previously mentioned. [sent-33, score-0.279]
</p><p>26 The maximum a posteriori (MAP) estimate also introduced biases in the learning process. [sent-34, score-0.097]
</p><p>27 These eﬀects can be partially compensated for by renormalizing the basis functions, but other parameters of the model such as those of the prior could not be learned. [sent-35, score-0.292]
</p><p>28 Finally, the gradient ascent method has convergence problems due to the power spectrum of natural images and the overcompleteness of the representation. [sent-36, score-0.199]
</p><p>29 Here we resolve these problems by using a prior distribution which is composed of a mixture of a Gaussian and a Dirac delta function, so that inactive coeﬃcients are encouraged to have exact zero values. [sent-37, score-0.469]
</p><p>30 Similar models employing a mixture of two Gaussians have been used for classifying wavelet coeﬃcients into active (high variance) and inactive (low variance) states [2, 5]. [sent-38, score-0.599]
</p><p>31 Such a classiﬁcation should be even more advantageous if the basis is overcomplete. [sent-39, score-0.19]
</p><p>32 A method for performing Gibbs-sampling for the Delta-plus-Gaussian prior in the context of an image pyramid is derived, and demonstrated to be eﬀective at obtaining very sparse representations which match the form of the imposed prior. [sent-40, score-0.642]
</p><p>33 Biases in the learning are overcome by sampling instead of using a MAP estimate. [sent-41, score-0.054]
</p><p>34 Unlike typical wavelet transforms which use a single 1-D mother wavelet function to generate 2-D functions by inner product, we do not constrain the functions ψi (x, y) to be 1-D separable. [sent-44, score-0.901]
</p><p>35 The functions ψi (x, y) provide an eﬃcient way to perform computations involving W by means of convolutions. [sent-45, score-0.079]
</p><p>36 Basis functions of coarser scales are produced by upsampling the ψi (x, y) functions and blurring with a low-pass ﬁlter φ(x, y), also known as the scaling function. [sent-46, score-0.251]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coe', 0.599), ('wavelet', 0.324), ('image', 0.249), ('overcomplete', 0.242), ('cients', 0.24), ('thresholding', 0.237), ('basis', 0.157), ('davis', 0.15), ('inactive', 0.135), ('representations', 0.115), ('overcompleteness', 0.113), ('denoising', 0.104), ('sallee', 0.099), ('mother', 0.09), ('pyramid', 0.09), ('multiscale', 0.084), ('sparse', 0.073), ('dirac', 0.072), ('biases', 0.069), ('ltering', 0.067), ('newton', 0.064), ('delta', 0.06), ('employing', 0.057), ('prior', 0.055), ('uc', 0.054), ('functions', 0.054), ('coarser', 0.049), ('wavelets', 0.049), ('upsampling', 0.049), ('phil', 0.049), ('seamlessly', 0.049), ('composed', 0.047), ('unsolved', 0.045), ('blurring', 0.045), ('ective', 0.045), ('active', 0.043), ('ciency', 0.043), ('rescaled', 0.042), ('encouraged', 0.042), ('subgroups', 0.042), ('bruno', 0.042), ('steerable', 0.042), ('partially', 0.042), ('coding', 0.041), ('mixture', 0.04), ('selecting', 0.038), ('arguably', 0.038), ('compensated', 0.038), ('critically', 0.038), ('neuroscience', 0.037), ('encourages', 0.036), ('olshausen', 0.036), ('snr', 0.036), ('rotated', 0.036), ('gains', 0.036), ('thresholded', 0.036), ('artifacts', 0.035), ('ascent', 0.033), ('simulating', 0.033), ('advantageous', 0.033), ('resolve', 0.033), ('demonstrated', 0.032), ('localized', 0.032), ('translate', 0.032), ('shifted', 0.032), ('encountered', 0.032), ('favorably', 0.031), ('spatially', 0.031), ('superposition', 0.031), ('address', 0.031), ('thresholds', 0.03), ('peaked', 0.03), ('smoothly', 0.03), ('exact', 0.029), ('sampling', 0.029), ('common', 0.029), ('decreased', 0.029), ('zero', 0.028), ('posteriori', 0.028), ('imposed', 0.028), ('exceeds', 0.028), ('transforms', 0.028), ('properly', 0.028), ('images', 0.028), ('constrain', 0.027), ('center', 0.027), ('regardless', 0.027), ('scenes', 0.027), ('located', 0.027), ('psychology', 0.027), ('gibbs', 0.026), ('meaningful', 0.026), ('secondly', 0.026), ('force', 0.026), ('overcome', 0.025), ('inferring', 0.025), ('spectrum', 0.025), ('posterior', 0.025), ('issues', 0.025), ('computations', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="126-tfidf-1" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>2 0.24878679 <a title="126-tfidf-2" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>3 0.15751739 <a title="126-tfidf-3" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>4 0.12886171 <a title="126-tfidf-4" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>Author: Michael E. Tipping, Christopher M. Bishop</p><p>Abstract: The extraction of a single high-quality image from a set of lowresolution images is an important problem which arises in fields such as remote sensing, surveillance, medical imaging and the extraction of still images from video. Typical approaches are based on the use of cross-correlation to register the images followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution images, using regularization to resolve the ill-posed nature of the inversion process. In this paper we develop a Bayesian treatment of the super-resolution problem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown high-resolution image. This approach allows us to estimate the unknown point spread function, and is rendered tractable through the introduction of a Gaussian process prior over images. Results indicate a significant improvement over techniques based on MAP (maximum a-posteriori) point optimization of the high resolution image and associated registration parameters. 1</p><p>5 0.12028084 <a title="126-tfidf-5" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>Author: Sumio Watanabe, Shun-ichi Amari</p><p>Abstract: A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coeﬃcient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general. 1</p><p>6 0.11587991 <a title="126-tfidf-6" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>7 0.10213223 <a title="126-tfidf-7" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>8 0.089706905 <a title="126-tfidf-8" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>9 0.085661247 <a title="126-tfidf-9" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>10 0.085629068 <a title="126-tfidf-10" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>11 0.085278019 <a title="126-tfidf-11" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>12 0.084414773 <a title="126-tfidf-12" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>13 0.077628583 <a title="126-tfidf-13" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>14 0.077538811 <a title="126-tfidf-14" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>15 0.06740997 <a title="126-tfidf-15" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>16 0.066787988 <a title="126-tfidf-16" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>17 0.063236266 <a title="126-tfidf-17" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>18 0.057361279 <a title="126-tfidf-18" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>19 0.057279751 <a title="126-tfidf-19" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>20 0.056955654 <a title="126-tfidf-20" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.15), (1, 0.01), (2, -0.013), (3, 0.267), (4, 0.013), (5, -0.03), (6, 0.091), (7, 0.03), (8, 0.046), (9, -0.071), (10, 0.046), (11, -0.117), (12, 0.079), (13, 0.045), (14, 0.121), (15, 0.022), (16, 0.01), (17, -0.015), (18, -0.041), (19, 0.126), (20, -0.058), (21, -0.114), (22, -0.098), (23, 0.016), (24, 0.14), (25, 0.002), (26, 0.028), (27, 0.058), (28, 0.227), (29, -0.053), (30, -0.026), (31, 0.021), (32, -0.054), (33, 0.05), (34, 0.062), (35, -0.011), (36, 0.007), (37, 0.041), (38, -0.183), (39, 0.097), (40, -0.14), (41, -0.03), (42, 0.024), (43, -0.079), (44, -0.09), (45, 0.051), (46, -0.063), (47, 0.002), (48, -0.153), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96727735 <a title="126-lsi-1" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>2 0.75229007 <a title="126-lsi-2" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>3 0.62952095 <a title="126-lsi-3" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>4 0.60528785 <a title="126-lsi-4" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>5 0.54216987 <a title="126-lsi-5" href="./nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">133 nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>Author: Anat Levin, Assaf Zomet, Yair Weiss</p><p>Abstract: Certain simple images are known to trigger a percept of transparency: the input image I is perceived as the sum of two images I(x, y) = I1 (x, y) + I2 (x, y). This percept is puzzling. First, why do we choose the “more complicated” description with two images rather than the “simpler” explanation I(x, y) = I1 (x, y) + 0 ? Second, given the inﬁnite number of ways to express I as a sum of two images, how do we compute the “best” decomposition ? Here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative ﬁlters and “corner detectors” in natural scenes and use this model to ﬁnd the most probable decomposition of a novel image. The optimization is performed using loopy belief propagation. We show that our model computes perceptually “correct” decompositions on synthetic images and discuss its application to real images. 1</p><p>6 0.52197534 <a title="126-lsi-6" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>7 0.45341432 <a title="126-lsi-7" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>8 0.4292044 <a title="126-lsi-8" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>9 0.42854422 <a title="126-lsi-9" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>10 0.3825486 <a title="126-lsi-10" href="./nips-2002-Unsupervised_Color_Constancy.html">202 nips-2002-Unsupervised Color Constancy</a></p>
<p>11 0.36767456 <a title="126-lsi-11" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>12 0.35647398 <a title="126-lsi-12" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>13 0.34909332 <a title="126-lsi-13" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>14 0.34338486 <a title="126-lsi-14" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>15 0.33381414 <a title="126-lsi-15" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>16 0.31462464 <a title="126-lsi-16" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>17 0.31143963 <a title="126-lsi-17" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>18 0.28821737 <a title="126-lsi-18" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>19 0.24523498 <a title="126-lsi-19" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>20 0.24232908 <a title="126-lsi-20" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.015), (14, 0.015), (23, 0.036), (33, 0.337), (41, 0.012), (42, 0.033), (54, 0.109), (55, 0.048), (64, 0.018), (68, 0.032), (74, 0.121), (87, 0.012), (92, 0.014), (98, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78242546 <a title="126-lda-1" href="./nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">126 nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>Author: Phil Sallee, Bruno A. Olshausen</p><p>Abstract: We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coeﬃcients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coeﬃcients to have exact zero values. Coeﬃcients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coeﬃcients. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 1</p><p>2 0.53022808 <a title="126-lda-2" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>Author: Jason Weston, Olivier Chapelle, Vladimir Vapnik, André Elisseeff, Bernhard Schölkopf</p><p>Abstract: We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 1</p><p>3 0.50614285 <a title="126-lda-3" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>4 0.50171423 <a title="126-lda-4" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>Author: David R. Martin, Charless C. Fowlkes, Jitendra Malik</p><p>Abstract: The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classiﬁer is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.</p><p>5 0.49891171 <a title="126-lda-5" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>6 0.49598289 <a title="126-lda-6" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>7 0.49578881 <a title="126-lda-7" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>8 0.49359408 <a title="126-lda-8" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>9 0.49175733 <a title="126-lda-9" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>10 0.49159789 <a title="126-lda-10" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>11 0.4911198 <a title="126-lda-11" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>12 0.49080655 <a title="126-lda-12" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>13 0.48967308 <a title="126-lda-13" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>14 0.48959127 <a title="126-lda-14" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>15 0.48846143 <a title="126-lda-15" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>16 0.48783398 <a title="126-lda-16" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>17 0.48734123 <a title="126-lda-17" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>18 0.4869813 <a title="126-lda-18" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>19 0.48696756 <a title="126-lda-19" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>20 0.48685932 <a title="126-lda-20" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
