<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-108" href="#">nips2002-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</h1>
<br/><p>Source: <a title="nips-2002-108-pdf" href="http://papers.nips.cc/paper/2256-improving-transfer-rates-in-brain-computer-interfacing-a-case-study.pdf">pdf</a></p><p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>Reference: <a title="nips-2002-108-reference" href="../nips2002_reference/nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. [sent-5, score-0.498]
</p><p>2 The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. [sent-6, score-0.272]
</p><p>3 In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50. [sent-8, score-0.464]
</p><p>4 Besides the clinical application, developing such a brain-computer interface (BCI) is in itself an exciting goal as indicated by a growing research interest in this ﬁeld. [sent-14, score-0.096]
</p><p>5 Several EEG-based techniques have been proposed for realization of BCIs (see [6, 12], for an overview). [sent-15, score-0.083]
</p><p>6 In the ﬁrst approach, participants are trained to control their EEG frequency pattern for binary decisions. [sent-17, score-0.114]
</p><p>7 Imaginations of movements, resulting in the “Bereitschaftspotential” over sensorimotor cortex areas, are used to transmit information in the device of Pfurtscheller    ¡  Figure 1: Stimulusmatrix with one column highlighted. [sent-22, score-0.06]
</p><p>8 [2] applied sophisticated methods for data-analysis to this approach and reached fast transfer rates of 23 bits/min when classifying brain signals preceding overt muscle activity. [sent-26, score-0.552]
</p><p>9 It is rather slow (<6 bits/min) and requires intensively trained participants but is in practical use. [sent-33, score-0.146]
</p><p>10 Farwell & Donchin [4, 3, 10] developed a BCI-System by utilizing speciﬁc positive deﬂections (P300) in EEG-signals accompanying rare events (as discussed in detail below). [sent-35, score-0.085]
</p><p>11 For BCIs, it is very desirable to have fast transfer rates. [sent-37, score-0.276]
</p><p>12 In our own studies, we therefore tried to accelerate the fourth approach by using state-of-the-art machine learning techniques and fusing data from different electrodes for data-analysis. [sent-38, score-0.308]
</p><p>13 For that purpose we utilized the basic setup of Farwell & Donchin (referred to as F&D;) [4] who used the well-studied P300-Component to create a BCI-system. [sent-39, score-0.109]
</p><p>14 People were instructed to focus on one symbol in the matrix, and mentally count its highlightings. [sent-42, score-0.267]
</p><p>15 From EEG-research it is known, that counting a rare speciﬁc event (oddballstimulus) in a series of background stimuli evokes a P300 for the oddball stimulus. [sent-43, score-0.061]
</p><p>16 Hence, highlighting the attended symbol in the 6 6-matrix should result in a P300, a characteristic positive deﬂection with a latency of around 300ms in the EEG-signal. [sent-44, score-0.213]
</p><p>17 It is therefore possible to infer the selected symbol by detecting the P300 in EEG-signals. [sent-45, score-0.196]
</p><p>18 For identiﬁcation of the right column and row associated with a P300, Farwell & Donchin used the model-based techniques Area and Peak picking (both described in section 2) to detect the P300. [sent-48, score-0.162]
</p><p>19 Using SWDA in a later study [3] resulted in transfer rates between 4. [sent-50, score-0.464]
</p><p>20 8 symbols per minute at an accuracy of 80% with a temporal distance of 125ms between two highlightings. [sent-52, score-0.075]
</p><p>21 In our work reported here we could improve several aspects of the F&D-approach; by utilizing very recent machine learning techniques and a larger number of EEG-electrodes. [sent-53, score-0.102]
</p><p>22 First of all, we could increase the transfer rate by using Support Vector Machines (SVM) [11] for classiﬁcation. [sent-54, score-0.248]
</p><p>23 Inspired by a recent approach to learning of discriminative densities [7] we utilized the values of the SVM classiﬁcation function as a measure of conﬁdence which we accumulate over certain classiﬁcations in order to speed up the transfer rate. [sent-55, score-0.396]
</p><p>24 In addition, we enhanced classiﬁcation rates by augmenting the data-space. [sent-56, score-0.184]
</p><p>25 While Farwell & Donchin employed only data from a single electrode for classiﬁcation, we used the data from 10 electrodes simultaneously. [sent-57, score-0.323]
</p><p>26 2 Methods In the following we describe the techniques used for acquisition, preprocessing and analysis of the EEG-data. [sent-58, score-0.05]
</p><p>27 The experimental setup was the following: participants were seated in front of a computer screen presenting the matrix (see Fig. [sent-61, score-0.147]
</p><p>28 EEG-data were recorded with 10 Ag/AgCl electrodes at positions of the extended international 10-20 system (Fz, Cz, Pz, C3, C4, P3, P4, Oz, OL, OR 1 ) sampled at 200Hz and low-pass ﬁltered at 30Hz. [sent-63, score-0.222]
</p><p>29 The participants had to perform a certain number of trials. [sent-64, score-0.114]
</p><p>30 For the duration of a trial, they were instructed to focus their attention on a target symbol speciﬁed by the program, to mentally count the highlightings of the target symbol, and to avoid any body movement (especially eye moves and blinks). [sent-65, score-0.439]
</p><p>31 Each trial is subdivided into a certain number of subtrials. [sent-66, score-0.054]
</p><p>32 For different BCI-setups, the time between stimulus onsets, the interstimulus interval (ISI), was either 150, 300 or 500ms, while a highlighting always lasts 150ms. [sent-70, score-0.117]
</p><p>33 To each stimulus correspondes an epoch, a time frame of 600ms after stimulus onset 2 During this interval a P300 should be evoked if the stimulus contains the target symbol. [sent-71, score-0.296]
</p><p>34 There is no pause between subtrials, but between trials. [sent-72, score-0.047]
</p><p>35 During the pause, the participants had time to focus on the next target symbol, before they initiated the next trial. [sent-73, score-0.2]
</p><p>36 The target symbol was chosen randomly from the available set of symbols and was presented by the program in order to create a data set of labelled EEG-signals for the subsequent ofﬂine analysis. [sent-74, score-0.327]
</p><p>37 To compensate for slow drifts of the DC potential, in a ﬁrst step the linear trend of the raw data in each electrode over the duration of a trial was eliminated. [sent-76, score-0.219]
</p><p>38 This was separately done for each electrode taking the data of all trials into account. [sent-78, score-0.182]
</p><p>39 Test- and trainingsets were created by choosing the data according to one symbol as testset, and the data of the other symbols as trainingset in a crossvalidation scheme. [sent-80, score-0.321]
</p><p>40 The task of classifying a subtrial for the identiﬁcation of a target symbol has to be distinguished from the classiﬁcation of a single epoch for detection of a signal, correlated with oddball-stimuli, which we brieﬂy refer to as a “P300 component” in a simpliﬁed manner in the following. [sent-81, score-0.77]
</p><p>41 In case of using a subtrial to select a symbol, two P300 components have to be detected within epochs: one corresponding to a row-, another to a column-stimulus. [sent-82, score-0.299]
</p><p>42 The detection algorithm works on the data of an epoch and has to compute a score which reﬂects the presence of a P300 within that epoch. [sent-83, score-0.22]
</p><p>43 Therefore, 12 epochs have to be evaluated for the selection of one target symbol. [sent-84, score-0.258]
</p><p>44 For the P300-detection, we utilized two model-based methods which had been proposed by F&D;, and one completely data-driven method based on Support Vector Machines (SVMs) [11]. [sent-85, score-0.076]
</p><p>45 For training of the classiﬁers, we built up a sets of epochs containing an equal number of positive and negative examples, i. [sent-86, score-0.172]
</p><p>46 time course  model−based methods  trial subtrial 1  subtrial 2  subtrial 3  stimulus onsets  epoch of 600ms  Figure 2: Trials, subtrials and epochs in the course of time (left). [sent-91, score-1.761]
</p><p>47 Area calculates surface in the P300-window, Peak picking calculates differences between peaks. [sent-93, score-0.158]
</p><p>48 The ﬁrst model-based method uses as its score as shown in Fig. [sent-94, score-0.048]
</p><p>49 Hyperparameters of the model-based methods were the boundaries picking method”, of the P300-window. [sent-96, score-0.082]
</p><p>50 They were selected regarding the average of epochs containing the P300 by taking the boundaries of the largest area. [sent-97, score-0.202]
</p><p>51 When using SVMs, it is not clear what measure to take as the score of an epoch. [sent-101, score-0.048]
</p><p>52 However, a recent approach to learning of discriminative densities [7] suggests an interpretation of the usual discrimination function for SVMs with positive kernels in terms of scaled density differences. [sent-103, score-0.072]
</p><p>53 This ﬁnding provides us with a well-motivated score of an epoch: with as the data vector of an epoch and as the corresponding class label which is positive/negative for epochs with/without target stimulus the SVM-score is computed as  ¤    ¨ ¦ ©§¥  ¥ B @ CA0 ¤  ¤ ( 3 1 0 ( ' % # ! [sent-104, score-0.548]
</p><p>54 Because EEG-data possess a very poor signal-to-noise ratio (SNR), identiﬁcation of the target symbol from a single subtrial is usually not reliable enough to achieve a reasonable classiﬁcation rate. [sent-108, score-0.551]
</p><p>55 Therefore, several subtrials have to be combined for classiﬁcation, slowing down the transfer rate. [sent-109, score-0.63]
</p><p>56 Thus, an important goal is to decrease the amount of subtrials which have to be combined for a satisfactory classiﬁcation rate. [sent-110, score-0.382]
</p><p>57 Therefore, we tested a method for certain -combinations of subtrials in the following way: different series of successive subtrials were taken out of a test set and the corresponding single classiﬁcations were combined as explained below. [sent-112, score-0.763]
</p><p>58 Thereby, the test series contained only subtrials belonging to identical symbols and these were combined in their original temporal order3. [sent-113, score-0.485]
</p><p>59 In contrast, Farwell & Donchin randomly chose samples from a test set, built from subtrials taken from different trials and belonging to different symbols. [sent-114, score-0.402]
</p><p>60 Based on the data of subtrials, one has to choose a row and a column in order to identify the target symbol, i. [sent-118, score-0.116]
</p><p>61 Therefore, in a ﬁrst step, the single scores 4 of the epoch corresponding to the stimulus associated to the -th row of the -th subtrial were summed up to the total score . [sent-121, score-0.619]
</p><p>62 Equivalent steps were performed to choose the target column. [sent-123, score-0.086]
</p><p>63 Based on these decisions the target symbol was ﬁnally selected in accordance to the presented matrix. [sent-124, score-0.282]
</p><p>64 Second, further single electrodes were taken as input source. [sent-128, score-0.19]
</p><p>65 This revealed information about interesting scalp positions to record a P300 and on the other hand indicated which channels may contain a useful signal. [sent-129, score-0.143]
</p><p>66 Third, the SVM classiﬁcation rate with respect to epochs was improved by increasing the data-space. [sent-130, score-0.172]
</p><p>67 Therefore, the input vector for the classiﬁer was extended by combining data from the same epoch but from different electrodes. [sent-131, score-0.172]
</p><p>68 These tests indicated that the best classiﬁcation rates could be achieved using as detection method an SVM with all ten electrodes as input sources. [sent-132, score-0.52]
</p><p>69 Since the results of the ﬁrst three steps were established based on the data of one initial experiment with only one participant, we evaluated the generality of these techniques by testing different subjects and BCI parameters. [sent-133, score-0.05]
</p><p>70 Finally, the BCI performance in terms of attainable communication rates is estimated from these analyses. [sent-134, score-0.184]
</p><p>71 Method comparison using the Pz electrode as input source. [sent-135, score-0.133]
</p><p>72 All four methods were applied to the data of one initial experiment with an ISI of 500ms and 3 subtrials per trial. [sent-136, score-0.353]
</p><p>73 Figure 3 presents the classiﬁcation rates of up to 10 subtrials. [sent-137, score-0.184]
</p><p>74 The SVM method achieved best performance, its epoch classiﬁcation rate was 76. [sent-138, score-0.204]
</p><p>75 0) in a 10-fold crossvalidation with about 380 subtrials samples in the training sets, and about 40 in the test sets. [sent-140, score-0.433]
</p><p>76 Of each subtrial in the training set, 4 epochs (2 with, 2 without a P300) were taken as training samples, whereas all 12 epochs of the subtrials of the test set were classiﬁed. [sent-141, score-0.996]
</p><p>77 For each training set, hyperparameters were selected by another 3-fold crossvalidation on this set. [sent-142, score-0.137]
</p><p>78 3 For a higher number of subtrial combinations, subtrials from different trials had to be combined. [sent-143, score-0.701]
</p><p>79 However, real-world-application of this BCI don’t require such combinations with respect to the ﬁnally achieved transfer rates reported in section 3. [sent-144, score-0.464]
</p><p>80 Figure 3: (left) Method comparison on the Pz electrode: The three techniques were applied to the data of the initial experiment. [sent-146, score-0.05]
</p><p>81 The results of the Peak picking and SVM method are shown in Figure 3. [sent-151, score-0.082]
</p><p>82 The SVM is able to extract useful information from all ten electrodes, whereas the Peak picking performance varies for different scalp positions. [sent-152, score-0.18]
</p><p>83 Especially, the electrodes over the visual cortex areas OZ, OR and OL are useless for the model-based techniques, as the same characteristics are revealed by tests with the Area method. [sent-153, score-0.255]
</p><p>84 While Farwell & Donchin used only one electrode for data-analysis, we extended the data-space by using larger numbers of electrodes. [sent-155, score-0.133]
</p><p>85 We calculated classiﬁcation rates for Pz alone, three, seven, and ten electrodes. [sent-156, score-0.235]
</p><p>86 A signal correlated with oddball-stimuli was classiﬁed at rates of 76. [sent-157, score-0.184]
</p><p>87 These rates were calculated with 850 positive and 850 negative epoch samples and a 3-fold crossvalidation. [sent-162, score-0.356]
</p><p>88 Applying data-space augmentation for classiﬁcation to infer symbols in the matrix results in the classiﬁcation rates depicted in Figure 3 (right) for an ISI of 500ms. [sent-164, score-0.306]
</p><p>89 Using ten electrodes simultaneously, combined in one data vector, outperforms lower-dimensional data-spaces. [sent-165, score-0.27]
</p><p>90 Figure 5: Mean-classiﬁcation rates (left) and transfer rates (right) for different ISIs. [sent-166, score-0.616]
</p><p>91 Note that a subtrial takes a speciﬁc amount of time. [sent-168, score-0.299]
</p><p>92 Therefore, the time dependend transfer rates are decreasing with the number of subtrials. [sent-169, score-0.432]
</p><p>93 Means, best and worst classiﬁcation rates are presented in Figure 5, as well as average and best transfer rates. [sent-174, score-0.432]
</p><p>94 the probability for classiﬁcation, and the    Using an ISI of 300ms results in slower transfer rates than using an ISI of 150ms. [sent-176, score-0.432]
</p><p>95 The latter ISI results on the average in classifying a symbol after 5. [sent-177, score-0.213]
</p><p>96 The poorest performer needs 9s to reach this criterion, the best performer achieves an accuracy of 95. [sent-179, score-0.108]
</p><p>97 4 Conclusion With an application of the data-driven SVM-method to classiﬁcation of single-channel EEG-signals, we could improve transfer rates as compared with model-based techniques. [sent-185, score-0.432]
</p><p>98 Furthermore, by increasing the number of EEG-channels, even higher classiﬁcation and transfer rates could be achieved. [sent-186, score-0.432]
</p><p>99 This resulted in high transfer rates with a maximum of 84. [sent-188, score-0.464]
</p><p>100 Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials. [sent-227, score-0.194]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subtrials', 0.353), ('subtrial', 0.299), ('transfer', 0.248), ('farwell', 0.245), ('classi', 0.201), ('electrodes', 0.19), ('donchin', 0.189), ('rates', 0.184), ('isi', 0.181), ('epoch', 0.172), ('epochs', 0.172), ('symbol', 0.166), ('pz', 0.142), ('electrode', 0.133), ('participants', 0.114), ('bci', 0.108), ('cation', 0.105), ('svm', 0.089), ('target', 0.086), ('picking', 0.082), ('oz', 0.082), ('swda', 0.082), ('ol', 0.08), ('crossvalidation', 0.08), ('utilized', 0.076), ('symbols', 0.075), ('birbaumer', 0.071), ('bler', 0.071), ('kotchoubey', 0.071), ('pfurtscheller', 0.071), ('bielefeld', 0.071), ('stimulus', 0.07), ('clinical', 0.065), ('peak', 0.065), ('device', 0.06), ('svms', 0.057), ('fz', 0.054), ('mentally', 0.054), ('performer', 0.054), ('prosthesis', 0.054), ('rehabilitation', 0.054), ('trial', 0.054), ('utilizing', 0.052), ('ten', 0.051), ('ine', 0.05), ('techniques', 0.05), ('trials', 0.049), ('score', 0.048), ('bcis', 0.047), ('augmentation', 0.047), ('blankertz', 0.047), ('ghanayim', 0.047), ('hinterberger', 0.047), ('instructed', 0.047), ('perelmouter', 0.047), ('scalp', 0.047), ('taub', 0.047), ('wolpaw', 0.047), ('helge', 0.047), ('meinicke', 0.047), ('twellmann', 0.047), ('cz', 0.047), ('highlighting', 0.047), ('participant', 0.047), ('pause', 0.047), ('classifying', 0.047), ('brain', 0.045), ('cations', 0.045), ('onsets', 0.043), ('mental', 0.043), ('people', 0.041), ('eeg', 0.04), ('penalties', 0.04), ('accelerate', 0.04), ('area', 0.039), ('calculates', 0.038), ('discriminative', 0.037), ('nally', 0.036), ('dence', 0.036), ('densities', 0.035), ('realization', 0.033), ('revealed', 0.033), ('highlighted', 0.033), ('rare', 0.033), ('setup', 0.033), ('tests', 0.032), ('positions', 0.032), ('resulted', 0.032), ('achieved', 0.032), ('slow', 0.032), ('indicated', 0.031), ('neurophysiology', 0.031), ('selected', 0.03), ('row', 0.03), ('combined', 0.029), ('realize', 0.029), ('tried', 0.028), ('series', 0.028), ('fast', 0.028), ('hyperparameters', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="108-tfidf-1" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>2 0.18494698 <a title="108-tfidf-2" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Recently, interest is growing to develop an effective communication interface connecting the human brain to a computer, the ’Brain-Computer Interface’ (BCI). One motivation of BCI research is to provide a new communication channel substituting normal motor output in patients with severe neuromuscular disabilities. In the last decade, various neurophysiological cortical processes, such as slow potential shifts, movement related potentials (MRPs) or event-related desynchronization (ERD) of spontaneous EEG rhythms, were shown to be suitable for BCI, and, consequently, different independent approaches of extracting BCI-relevant EEG-features for single-trial analysis are under investigation. Here, we present and systematically compare several concepts for combining such EEG-features to improve the single-trial classiﬁcation. Feature combinations are evaluated on movement imagination experiments with 3 subjects where EEG-features are based on either MRPs or ERD, or both. Those combination methods that incorporate the assumption that the single EEG-features are physiologically mutually independent outperform the plain method of ’adding’ evidence where the single-feature vectors are simply concatenated. These results strengthen the hypothesis that MRP and ERD reﬂect at least partially independent aspects of cortical processes and open a new perspective to boost BCI effectiveness.</p><p>3 0.14211817 <a title="108-tfidf-3" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>Author: Peter Meinicke, Thorsten Twellmann, Helge Ritter</p><p>Abstract: We propose a framework for classiﬁer design based on discriminative densities for representation of the differences of the class-conditional distributions in a way that is optimal for classiﬁcation. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximization of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability density functions, the resulting maximum contrast classiﬁers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vector Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approximate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classiﬁcation performance.</p><p>4 0.12010867 <a title="108-tfidf-4" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><p>5 0.1179772 <a title="108-tfidf-5" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>Author: Sariel Har-Peled, Dan Roth, Dav Zimak</p><p>Abstract: The constraint classiﬁcation framework captures many ﬂavors of multiclass classiﬁcation including winner-take-all multiclass classiﬁcation, multilabel classiﬁcation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classiﬁer in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classiﬁcation beneﬁts over existing methods of multiclass classiﬁcation.</p><p>6 0.11606421 <a title="108-tfidf-6" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>7 0.10026047 <a title="108-tfidf-7" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>8 0.098140053 <a title="108-tfidf-8" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>9 0.09132643 <a title="108-tfidf-9" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>10 0.091211818 <a title="108-tfidf-10" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>11 0.087295197 <a title="108-tfidf-11" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>12 0.084304564 <a title="108-tfidf-12" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>13 0.078745782 <a title="108-tfidf-13" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>14 0.078416735 <a title="108-tfidf-14" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>15 0.075437225 <a title="108-tfidf-15" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>16 0.073225446 <a title="108-tfidf-16" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>17 0.073218293 <a title="108-tfidf-17" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>18 0.069718018 <a title="108-tfidf-18" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>19 0.067137122 <a title="108-tfidf-19" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>20 0.063914888 <a title="108-tfidf-20" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.202), (1, -0.031), (2, 0.096), (3, -0.053), (4, 0.158), (5, -0.052), (6, -0.034), (7, -0.092), (8, 0.097), (9, 0.037), (10, -0.1), (11, 0.132), (12, 0.048), (13, 0.046), (14, 0.107), (15, -0.124), (16, -0.011), (17, 0.061), (18, -0.022), (19, -0.037), (20, 0.023), (21, -0.027), (22, 0.051), (23, -0.032), (24, 0.006), (25, 0.001), (26, -0.01), (27, -0.107), (28, -0.053), (29, -0.03), (30, -0.047), (31, -0.029), (32, 0.014), (33, -0.054), (34, 0.067), (35, 0.031), (36, 0.016), (37, -0.007), (38, -0.037), (39, 0.008), (40, -0.075), (41, -0.001), (42, 0.061), (43, 0.048), (44, 0.033), (45, 0.056), (46, 0.105), (47, -0.008), (48, -0.062), (49, -0.2)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93294358 <a title="108-lsi-1" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>2 0.72725862 <a title="108-lsi-2" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Recently, interest is growing to develop an effective communication interface connecting the human brain to a computer, the ’Brain-Computer Interface’ (BCI). One motivation of BCI research is to provide a new communication channel substituting normal motor output in patients with severe neuromuscular disabilities. In the last decade, various neurophysiological cortical processes, such as slow potential shifts, movement related potentials (MRPs) or event-related desynchronization (ERD) of spontaneous EEG rhythms, were shown to be suitable for BCI, and, consequently, different independent approaches of extracting BCI-relevant EEG-features for single-trial analysis are under investigation. Here, we present and systematically compare several concepts for combining such EEG-features to improve the single-trial classiﬁcation. Feature combinations are evaluated on movement imagination experiments with 3 subjects where EEG-features are based on either MRPs or ERD, or both. Those combination methods that incorporate the assumption that the single EEG-features are physiologically mutually independent outperform the plain method of ’adding’ evidence where the single-feature vectors are simply concatenated. These results strengthen the hypothesis that MRP and ERD reﬂect at least partially independent aspects of cortical processes and open a new perspective to boost BCI effectiveness.</p><p>3 0.71664965 <a title="108-lsi-3" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>Author: Peter Meinicke, Thorsten Twellmann, Helge Ritter</p><p>Abstract: We propose a framework for classiﬁer design based on discriminative densities for representation of the differences of the class-conditional distributions in a way that is optimal for classiﬁcation. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximization of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability density functions, the resulting maximum contrast classiﬁers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vector Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approximate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classiﬁcation performance.</p><p>4 0.64429861 <a title="108-lsi-4" href="./nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">196 nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, Gerhard A. Müller</p><p>Abstract: We describe the RA scanner, a novel system for the examination of patients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various linear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably perform parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was concluded that the RA scanner permits a reliable classiﬁcation of pathological ﬁnger joints, thus paving the way for a further development from prototype to product stage.</p><p>5 0.63759291 <a title="108-lsi-5" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>6 0.62008893 <a title="108-lsi-6" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>7 0.5376094 <a title="108-lsi-7" href="./nips-2002-FloatBoost_Learning_for_Classification.html">92 nips-2002-FloatBoost Learning for Classification</a></p>
<p>8 0.53703082 <a title="108-lsi-8" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>9 0.52401537 <a title="108-lsi-9" href="./nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">109 nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>10 0.51700205 <a title="108-lsi-10" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>11 0.50551331 <a title="108-lsi-11" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>12 0.50000888 <a title="108-lsi-12" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>13 0.4963595 <a title="108-lsi-13" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>14 0.41289562 <a title="108-lsi-14" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>15 0.40058693 <a title="108-lsi-15" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>16 0.38716647 <a title="108-lsi-16" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>17 0.38193184 <a title="108-lsi-17" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>18 0.37677777 <a title="108-lsi-18" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>19 0.37294698 <a title="108-lsi-19" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>20 0.35392779 <a title="108-lsi-20" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.02), (11, 0.011), (23, 0.504), (42, 0.047), (54, 0.086), (55, 0.037), (68, 0.011), (74, 0.06), (87, 0.011), (92, 0.025), (98, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87522757 <a title="108-lda-1" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>Author: Christian W. Eurich</p><p>Abstract: A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. 1</p><p>same-paper 2 0.85983109 <a title="108-lda-2" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>3 0.85355592 <a title="108-lda-3" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>Author: W Wu, M. J. Black, Y. Gao, M. Serruya, A. Shaikhouni, J. P. Donoghue, Elie Bienenstock</p><p>Abstract: The direct neural control of external devices such as computer displays or prosthetic limbs requires the accurate decoding of neural activity representing continuous movement. We develop a real-time control system using the spiking activity of approximately 40 neurons recorded with an electrode array implanted in the arm area of primary motor cortex. In contrast to previous work, we develop a control-theoretic approach that explicitly models the motion of the hand and the probabilistic relationship between this motion and the mean ﬁring rates of the cells in 70 bins. We focus on a realistic cursor control task in which the subject must move a cursor to “hit” randomly placed targets on a computer monitor. Encoding and decoding of the neural data is achieved with a Kalman ﬁlter which has a number of advantages over previous linear ﬁltering techniques. In particular, the Kalman ﬁlter reconstructions of hand trajectories in off-line experiments are more accurate than previously reported results and the model provides insights into the nature of the neural coding of movement. ¨ ©§</p><p>4 0.67422551 <a title="108-lda-4" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>Author: Benjamin V. Roy, Daniela D. Farias</p><p>Abstract: This paper extends our earlier analysis on approximate linear programming as an approach to approximating the cost-to-go function in a discounted-cost dynamic program [6]. In this paper, we consider the average-cost criterion and a version of approximate linear programming that generates approximations to the optimal average cost and differential cost function. We demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well-aligned with the objective of deriving a policy with low average cost. For that, the algorithm should aim at producing a good approximation of the differential cost function. We propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portions of the state space via state-relevance weights. Performance bounds suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choices for state-relevance weights.</p><p>5 0.49625188 <a title="108-lda-5" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Recently, interest is growing to develop an effective communication interface connecting the human brain to a computer, the ’Brain-Computer Interface’ (BCI). One motivation of BCI research is to provide a new communication channel substituting normal motor output in patients with severe neuromuscular disabilities. In the last decade, various neurophysiological cortical processes, such as slow potential shifts, movement related potentials (MRPs) or event-related desynchronization (ERD) of spontaneous EEG rhythms, were shown to be suitable for BCI, and, consequently, different independent approaches of extracting BCI-relevant EEG-features for single-trial analysis are under investigation. Here, we present and systematically compare several concepts for combining such EEG-features to improve the single-trial classiﬁcation. Feature combinations are evaluated on movement imagination experiments with 3 subjects where EEG-features are based on either MRPs or ERD, or both. Those combination methods that incorporate the assumption that the single EEG-features are physiologically mutually independent outperform the plain method of ’adding’ evidence where the single-feature vectors are simply concatenated. These results strengthen the hypothesis that MRP and ERD reﬂect at least partially independent aspects of cortical processes and open a new perspective to boost BCI effectiveness.</p><p>6 0.48232734 <a title="108-lda-6" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>7 0.46209237 <a title="108-lda-7" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>8 0.45419288 <a title="108-lda-8" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>9 0.44857937 <a title="108-lda-9" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>10 0.44630063 <a title="108-lda-10" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>11 0.44140658 <a title="108-lda-11" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>12 0.43862343 <a title="108-lda-12" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>13 0.43086684 <a title="108-lda-13" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>14 0.429748 <a title="108-lda-14" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>15 0.42524371 <a title="108-lda-15" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>16 0.42351452 <a title="108-lda-16" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>17 0.42303547 <a title="108-lda-17" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>18 0.40846288 <a title="108-lda-18" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>19 0.4064185 <a title="108-lda-19" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>20 0.40576431 <a title="108-lda-20" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
