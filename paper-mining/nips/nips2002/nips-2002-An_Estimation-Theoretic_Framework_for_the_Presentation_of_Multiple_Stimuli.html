<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-26" href="#">nips2002-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</h1>
<br/><p>Source: <a title="nips-2002-26-pdf" href="http://papers.nips.cc/paper/2270-an-estimation-theoretic-framework-for-the-presentation-of-multiple-stimuli.pdf">pdf</a></p><p>Author: Christian W. Eurich</p><p>Abstract: A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. 1</p><p>Reference: <a title="nips-2002-26-reference" href="../nips2002_reference/nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. [sent-4, score-0.84]
</p><p>2 Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. [sent-5, score-0.547]
</p><p>3 Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. [sent-6, score-0.569]
</p><p>4 The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. [sent-7, score-0.267]
</p><p>5 1  Introduction  An important issue in the Neurosciences is the investigation of the encoding properties of neural populations from their electrophysiological properties such as tuning curves, background noise, and correlations in the ﬁring. [sent-8, score-0.825]
</p><p>6 Many theoretical studies have used estimation theory, in particular the measure of Fisher information, to account for the neural encoding accuracy with respect to the presentation of a single stimulus (e. [sent-9, score-0.809]
</p><p>7 Most modeling studies, however, neglect the fact that in a natural situation, neural activity results from multiple objects or even complex sensory scenes. [sent-12, score-0.138]
</p><p>8 In particular, attention experiments require the presentation of at least one distractor along with the attended stimulus. [sent-13, score-0.471]
</p><p>9 Electrophysiological data are now available demonstrating eﬀects of selective attention on neural ﬁring behavior in various cortical areas [6, 7, 8]. [sent-14, score-0.277]
</p><p>10 Such experiments require the development of theoretical tools which deviate from the usual practice of considering only single stimuli in the analysis. [sent-15, score-0.277]
</p><p>11 [9] employ an extended encoding scheme for stimulus distributions and use Bayesian decoding to account for the presentation of multiple objects. [sent-17, score-0.643]
</p><p>12 Similarly, Bayesian estimation has been used in the context of attentional phenomena [10]. [sent-18, score-0.187]
</p><p>13 de/˜eurich  In this paper, a new estimation-theoretic framework for the simultaneous presentation of multiple stimuli is introduced. [sent-22, score-0.585]
</p><p>14 Fisher information is employed to compute lower bounds for the encoding error and the discrimational ability of neural populations independent of a particular estimator. [sent-23, score-0.35]
</p><p>15 Here we focus on the simultaneous presentation of two objects in the context of attentional phenomena. [sent-24, score-0.436]
</p><p>16 Furthermore, we assume a linearity in the neural response for reasons of analytical tractability; however, the method can be extended to include neural nonlinearities. [sent-25, score-0.158]
</p><p>17 1  Estimation Theory for Multiple Stimuli Tuning Curves in Compound Space  The tuning curve f (X ) of a neuron is deﬁned to be the average neural response to repetitive presentations of stimulus conﬁgurations X . [sent-27, score-0.939]
</p><p>18 In most cases, the response is taken to be the number n(X ) of action potentials occurring within some time interval τ after stimulus presentation, or the neural ﬁring rate r(X ) = n(X )/τ : n(X ) . [sent-28, score-0.335]
</p><p>19 (1) τ Within an estimation-theoretic framework, the variability of the neural response is described by a probability distribution conditioned on the value of X , P (n; X ). [sent-29, score-0.109]
</p><p>20 The average · in (1) can be regarded either as an average over multiple presentations of the same stimulus conﬁguration (in an experimental setup), or as an average over n (in a theoretical description). [sent-30, score-0.343]
</p><p>21 f (X ) = r(X ) =  In most electrophysiological experiments, tuning curves are assessed through the presentation of a single stimulus, X = x, such as a bar or a grating characterized by a single orientation, or a dot of light at a speciﬁc position in the animal’s visual ﬁeld (e. [sent-31, score-0.882]
</p><p>22 Such tuning curves will be denoted by f1 (x), where the subscript refers to the single object. [sent-34, score-0.569]
</p><p>23 The behavior of a neuron upon presentation of multiple objects, however, cannot be inferred from tuning curves f1 (x). [sent-35, score-0.909]
</p><p>24 Instead, neurons may show nonlinearities such as the so-called non-classical receptive ﬁelds in the visual area V1 which have attracted much attention in the recent past (e. [sent-36, score-0.447]
</p><p>25 , xM , the neuronal tuning curve can be written as a function fM (x1 , . [sent-42, score-0.589]
</p><p>26 , xM ), where the subscript M is not necessarily a parameter of the function but an indicator of the number of stimuli it refers to. [sent-45, score-0.311]
</p><p>27 The domain of this function will be called the compound space of the stimuli. [sent-46, score-0.377]
</p><p>28 The resulting tuning function is therefore a function of two scalar variables x1 and x2 : f2 (x1 , x2 ) = r(x1 , x2 ) = n(x1 , x2 ) /τ . [sent-48, score-0.394]
</p><p>29 Figure 1 visualizes the concept of the compound space. [sent-49, score-0.377]
</p><p>30 In order to obtain an analytical access to the encoding properties of a neural population, we will furthermore assume that a neuron’s response f2 (x1 , x2 ) is a linear superposition of the single-stimulus responses f1 (x1 ) and f1 (x2 ), i. [sent-50, score-0.37]
</p><p>31 Such linear behavior has been observed in area 17 of the cat upon presentation of bi-vectorial transparent motion stimuli [15] and in areas MT and MST of the macaque monkey upon simultaneous presentation of two moving objects [16]. [sent-53, score-1.255]
</p><p>32 In  f2(x1,x2)  f1(x)  x'  x''  x  x''  x'  x2  x1  Figure 1: The concept of compound space. [sent-54, score-0.377]
</p><p>33 A single-stimulus tuning curve f 1 (x) (left) yields the average response to the presentation of either x or x ; the simultaneous presentation of x and x , however, can be formalized only through a tuning curve f2 (x1 , x2 ) (right). [sent-55, score-1.695]
</p><p>34 general, however, the compound space method is not restricted to linear neural responses. [sent-56, score-0.456]
</p><p>35 The consideration of a neural population in the compound space yields tuning properties and symmetries which are very diﬀerent from those in a D-dimensional single-stimulus space considered in the literature (e. [sent-57, score-1.096]
</p><p>36 Figure 2a shows a tuning curve f2 (x1 , x2 ) given by (2), where f1 (x) is a Gaussian, f1 (x) = F exp −  (x − c)2 2σ 2  ;  (3)  F is a gain factor which can be scaled to be the maximal ﬁring rate of the neuron. [sent-61, score-0.632]
</p><p>37 2  (c,c)  8 8  6  (a)  x2  6  4  4 2 2  x1  (b)  x1  Figure 2: (a) A tuning curve f2 (x1 , x2 ) in a 2-dimensional compound space given by (2) and (3) with k = 0. [sent-68, score-0.9]
</p><p>38 (b) Arrangement of tuning curves: The centers of the tuning curves are restricted to the diagonal x 1 = x2 . [sent-71, score-1.013]
</p><p>39 The cross is a schematic cross-section of the tuning curve in (a). [sent-72, score-0.523]
</p><p>40 single-stimulus tuning curve f1 (x) whose center is located at x = c yields a linear superposition whose center is given by the vector (c, c) in the compound space. [sent-73, score-1.008]
</p><p>41 This is due to the fact that both axes describe the same physical stimulus feature. [sent-74, score-0.254]
</p><p>42 Therefore, all tuning curve centers are restricted to the 1-dimensional subspace  x1 = x2 . [sent-75, score-0.604]
</p><p>43 The tuning curve centers are assumed to have a distribution in the compound space which can be written as 0 if c1 = c2 η (c1 , c2 ) = ˜ . [sent-76, score-0.982]
</p><p>44 (4) η(c) if c1 = c2 The geometrical features in the compound space suggest that an estimationtheoretic approach will yield encoding properties of neural populations which are diﬀerent from those obtained from the presentation of a single stimulus. [sent-77, score-0.899]
</p><p>45 2  Fisher Information  In order to assess the encoding accuracy of a neural population, the stochasticity of the neural response is taken into account. [sent-79, score-0.354]
</p><p>46 , N ) as a response to the stimulus conﬁguration X , P (n(1) , n(2) , . [sent-83, score-0.286]
</p><p>47 (5)  These parameter-dependent distributions are obtained either experimentally or through a noise model; a convenient choice for the latter is a Poisson distribution with a spike count average given by the tuning curve (1) of each neuron. [sent-91, score-0.566]
</p><p>48 In the 2-dimensional compound space discussed in the previous section, P (n; X ) ≡ P (n; x1 , x2 ). [sent-92, score-0.377]
</p><p>49 (6) ∂xi ∂xj The Cram´r-Rao inequality states that a lower bound on the expected square ese −1 timation error of the ith feature, 2 )ii provided that i,min (i=1,2), is given by (J the estimator is unbiased. [sent-94, score-0.12]
</p><p>50 In the following, this lower bound is studied in the 2-dimensional compound space. [sent-95, score-0.377]
</p><p>51 The single-neuron Fisher information in the compound space can be written down for an arbitrary noise model. [sent-97, score-0.408]
</p><p>52 whereby the tuning is assumed to be linear according to (2), and the single-stimulus tuning curve f1 (x) is a Gaussian given by (3). [sent-99, score-0.966]
</p><p>53 For independently spiking neurons (5), the population Fisher information is the sum of the single-neuron Fisher information values. [sent-102, score-0.202]
</p><p>54 Assuming some density η(c) of tuning curve centers on the diagonal x1 = x2 , the population Fisher information is therefore obtained by an integration of (8). [sent-103, score-0.726]
</p><p>55 ρ therefore quantiﬁes the similarity of the stimuli x1 and x2 . [sent-108, score-0.241]
</p><p>56 1  Example 1: Symmetrical Tuning  First we study the symmetrical case k = 1/2 the receptive ﬁelds of which are given in Fig. [sent-113, score-0.15]
</p><p>57 4 shows the minimal square estimation error for x1 , 2 1,min (ρ), as obtained from the ﬁrst diagonal element of the inverse Fisher information matrix. [sent-116, score-0.333]
</p><p>58 Due to the symmetry, it is identical to the minimal square error for x2 , 2 2,min (ρ). [sent-117, score-0.204]
</p><p>59 (11) 1 1 2 2 Correspondingly, the diagonal Fisher information matrix yields a lower bound for √ √ the estimation errors of (x1 + x2 )/ 2 and (x2 − x1 )/ 2, respectively. [sent-121, score-0.176]
</p><p>60 The estimation error for (x1 + x2 )/ 2 takes a ﬁnite value for  20  2 emin(r)  15  Figure 4: Minimal square estimation error for stimulus x1 or x2 . [sent-124, score-0.566]
</p><p>61 5  -4  -2  0  2  4  0  -4  r  -2  0  2  4  r  √ Figure 5: Minimal square estimation error for (a) (x1 + x2 )/ 2 and (b) (x2 − √ x1 )/ 2. [sent-131, score-0.21]
</p><p>62 However, the estimation error for (x2 − x1 )/ 2 diverges as ρ −→ 0. [sent-137, score-0.174]
</p><p>63 This error corresponds to an estimation of the diﬀerence of the two presented stimuli. [sent-138, score-0.13]
</p><p>64 As expected, a discrimination becomes impossible as the stimuli merge. [sent-139, score-0.29]
</p><p>65 The Fisher √ information for (x2 − x1 )/ 2 can be regarded as a discrimination measure which takes the simultaneous presentation of stimuli into account. [sent-140, score-0.587]
</p><p>66 2  Example 2: Attention on Both Stimuli  Electrophysiological studies in V1 and V4 [7] and MT [8] of macaque monkeys suggest that the gain but not the width of tuning curves is increased as stimuli in a cell’s receptive ﬁeld are attended. [sent-142, score-0.981]
</p><p>67 This can easily be incorporated in the current model: The gain corresponds to the factor F in the tuning curve (3). [sent-143, score-0.569]
</p><p>68 As expected, the minimal square errors are smaller for higher F in all cases (dotted lines); a higher ﬁring rate yields a better stimulus estimation. [sent-146, score-0.437]
</p><p>69 This suggests that attention increases localization accuracy of x1 and x2 as well as their discrimination if both stimuli are attended. [sent-147, score-0.466]
</p><p>70 The former is consistent with psychophysical results on attentional enhancement of spatial resolution in human subjects [17]. [sent-148, score-0.182]
</p><p>71 3  Example 3: Attending One Stimulus  The situation changes if only one of the two stimuli is attended. [sent-150, score-0.281]
</p><p>72 Electrophysiological recordings in monkey area V4 suggest that upon presentation of two stimuli inside a neuron’s receptive ﬁeld, the inﬂuence of the attended stimulus increases as compared to the unattended one [6]. [sent-151, score-1.127]
</p><p>73 In our framework, this situation can be considered by increasing the weight factor of the attended stimulus in the linear  superposition (2). [sent-152, score-0.448]
</p><p>74 The resulting tuning curve shows characteristic distortions as compared to the symmetrical case k = 0. [sent-155, score-0.589]
</p><p>75 2  5 8 8  6  x2  6  4  4 2 2  x1  0  -4  -2  0  2  4  r  Figure 6: Neural encoding for one attended stimulus. [sent-163, score-0.279]
</p><p>76 (b) as Minimal square estimation errors for the direction (x2 − x1 )/ 2 resulting from a rotated Fisher information matrix. [sent-170, score-0.199]
</p><p>77 reveals that the attended stimulus x1 yields a smaller minimal square estimation error than it does in the non-attention case k = 0. [sent-177, score-0.688]
</p><p>78 5 whereas the minimal square error for the unattended stimulus x2 is increased (data not shown). [sent-178, score-0.514]
</p><p>79 Figure 6b shows √ the minimal square error for the diﬀerence of the stimuli, (x2 − x1 )/ 2. [sent-179, score-0.204]
</p><p>80 The minimal estimation error becomes larger as compared to k = 0. [sent-180, score-0.214]
</p><p>81 This result can be interpreted as follows: Attending stimulus x1 yields a better encoding of x1 but a worse encoding of √2 . [sent-182, score-0.623]
</p><p>82 The latter results in the larger estimation error for the x diﬀerence (x2 − x1 )/ 2 of the stimulus values. [sent-183, score-0.356]
</p><p>83 4  Summary and Discussion  A method was introduced to account for the encoding of multiple stimuli by populations of neurons. [sent-186, score-0.549]
</p><p>84 Estimation theory was performed in a compound space whose axes are deﬁned by the features of each stimulus. [sent-187, score-0.405]
</p><p>85 Here we studied a speciﬁc example of linear neurons with Gaussian tuning and Poissonian spike statistics to gain insight into the symmetries in the compound space and the interpretation of the resulting estimation errors. [sent-188, score-1.128]
</p><p>86 The approach allows for a detailed consideration of attention eﬀects on the neural level [7, 8, 6]. [sent-189, score-0.214]
</p><p>87 The method can be extended to include nonlinear neural behavior as multiple stimuli are presented; see e. [sent-190, score-0.37]
</p><p>88 [13, 14], where the response of single neurons to two orientation stimuli cannot be easily inferred from the neural behavior in the case of only one stimulus. [sent-192, score-0.566]
</p><p>89 More experimental and theoretical work has to be done in order to account for the psychophysical performance under the inﬂuence of attention as it has been measured, for example, in [17]. [sent-193, score-0.232]
</p><p>90 From theoretical considerations in the case of a single stimulus [2, 3, 4, 5] it is well known that the encoding accuracy of a neural population may depend on various properties such as the number of encoded features, the noise model, and the correlations in the neural activity. [sent-195, score-0.669]
</p><p>91 Paradiso, A theory for the use of visual orientation information which exploits the columnar structure of striate cortex, Biol. [sent-202, score-0.152]
</p><p>92 Wilke, Multidimensional encoding strategy of spiking neurons, Neural Comp. [sent-215, score-0.158]
</p><p>93 Amari, Attention modulation of neural tuning through peak and base rate, Neural Comp. [sent-227, score-0.443]
</p><p>94 Desimone, Selective attention gates visual processing in the extrastriate cortex, Science 229 (1985) 782–784. [sent-231, score-0.188]
</p><p>95 Maunsell, Eﬀects of attention on orientation-tuning functions of single neurons in macaque cortical area V4, J. [sent-237, score-0.416]
</p><p>96 Mart´ ınetz Trujillo, Feature-based attention inﬂuences motion processing gain in macaque visual cortex, Nature 399 (1999) 575–579. [sent-243, score-0.379]
</p><p>97 Swindale (1998), Orientation tuning curves: empirical description and estimation of parameters, Biol. [sent-264, score-0.484]
</p><p>98 van Essen, Neuronal responses to static texture patterns in area V1 of the alert macaque monkey, J. [sent-270, score-0.231]
</p><p>99 van de Grind, Responses of complex cells in area 17 of the cat to bi-vectorial transparent motion, Vis. [sent-294, score-0.162]
</p><p>100 Schwarz, Responses of MT and MST neurons to one and two moving objects in the receptive ﬁeld, J. [sent-302, score-0.215]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tuning', 0.394), ('compound', 0.377), ('stimuli', 0.241), ('fisher', 0.237), ('stimulus', 0.226), ('presentation', 0.212), ('encoding', 0.158), ('jij', 0.146), ('eurich', 0.14), ('attention', 0.138), ('curve', 0.129), ('attended', 0.121), ('electrophysiological', 0.121), ('population', 0.113), ('attending', 0.112), ('emin', 0.112), ('macaque', 0.111), ('curves', 0.105), ('populations', 0.103), ('attentional', 0.097), ('estimation', 0.09), ('symmetries', 0.089), ('neurons', 0.089), ('simultaneous', 0.085), ('minimal', 0.084), ('unattended', 0.084), ('wilke', 0.084), ('receptive', 0.084), ('square', 0.08), ('und', 0.073), ('monkey', 0.068), ('symmetrical', 0.066), ('orientation', 0.065), ('exp', 0.063), ('superposition', 0.061), ('erent', 0.061), ('di', 0.061), ('response', 0.06), ('psychophysical', 0.058), ('discriminational', 0.056), ('nakahara', 0.056), ('poissonian', 0.056), ('transparent', 0.056), ('mt', 0.053), ('ring', 0.052), ('centers', 0.051), ('zemel', 0.051), ('visual', 0.05), ('neural', 0.049), ('discrimination', 0.049), ('mst', 0.049), ('whereby', 0.049), ('area', 0.049), ('neuron', 0.047), ('yields', 0.047), ('multiple', 0.047), ('dotted', 0.047), ('gain', 0.046), ('bremen', 0.044), ('diverges', 0.044), ('erence', 0.043), ('spike', 0.043), ('objects', 0.042), ('upon', 0.042), ('ects', 0.042), ('responses', 0.042), ('situation', 0.04), ('error', 0.04), ('cortex', 0.04), ('diagonal', 0.039), ('accuracy', 0.038), ('striate', 0.037), ('nonlinearities', 0.037), ('refers', 0.037), ('theoretical', 0.036), ('neuronal', 0.035), ('motion', 0.034), ('presentations', 0.034), ('xm', 0.034), ('worse', 0.034), ('formalized', 0.033), ('subscript', 0.033), ('amari', 0.033), ('behavior', 0.033), ('elds', 0.032), ('written', 0.031), ('uence', 0.031), ('guration', 0.031), ('dayan', 0.03), ('restricted', 0.03), ('cortical', 0.029), ('direction', 0.029), ('inferred', 0.029), ('van', 0.029), ('axes', 0.028), ('cat', 0.028), ('selective', 0.028), ('consideration', 0.027), ('subjects', 0.027), ('symmetry', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="26-tfidf-1" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>Author: Christian W. Eurich</p><p>Abstract: A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. 1</p><p>2 0.27425426 <a title="26-tfidf-2" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>Author: Javier R. Movellan, Thomas Wachtler, Thomas D. Albright, Terrence Sejnowski</p><p>Abstract: We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive ﬁeld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations. In this paper we introduce the notion of Morton-style factorial coding and illustrate how it may help analyze information integration and perceptual organization in the brain. In the neurosciences factorial codes are often studied in the context of mean tuning curves. A tuning curve is called separable if it can be expressed as the product of terms selectively inﬂuenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. Morton (1969) analyzed a wide variety of psychophysical experiments on word perception and showed that they could be explained using a model in which stimulus and context have separable effects on perception. More precisely, in Mortons’ model the joint effect of stimulus and context on a perceptual representation can be obtained by multiplying terms selectively controlled by stimulus and by context, i.e.,  £ © # #</p><p>3 0.22923563 <a title="26-tfidf-3" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>4 0.20221035 <a title="26-tfidf-4" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>Author: Christian K. Machens, Michael Wehr, Anthony M. Zador</p><p>Abstract: How do cortical neurons represent the acoustic environment? This question is often addressed by probing with simple stimuli such as clicks or tone pips. Such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to uncover complex or higher-order neuronal response properties. Here we adopt an alternative approach, probing neuronal responses with complex acoustic stimuli, including animal vocalizations and music. We have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential ﬂuctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit, instead of just its output—a sparse binary spike train—as in conventional single unit physiological recordings. Whole cell recording thus provides a much richer source of information about the neuron’s response. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component—the spectrotemporal receptive ﬁeld (STRF)—of the transformation from the sound (as represented by its time-varying spectrogram) to the neuron’s membrane potential. We ﬁnd that the STRF has a rich dynamical structure, including excitatory regions positioned in general accord with the prediction of the simple tuning curve. We also ﬁnd that in many cases, much of the neuron’s response, although deterministically related to the stimulus, cannot be predicted by the linear component, indicating the presence of as-yet-uncharacterized nonlinear response properties.</p><p>5 0.1768916 <a title="26-tfidf-5" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>Author: Patrik O. Hoyer, Aapo Hyvärinen</p><p>Abstract: The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signiﬁcantly from trial to trial. This variability is most often interpreted as ‘noise’, purely detrimental to the sensory system. In this paper, we propose an alternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speciﬁcally, the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representational scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.</p><p>6 0.16430497 <a title="26-tfidf-6" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>7 0.14721315 <a title="26-tfidf-7" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>8 0.14446956 <a title="26-tfidf-8" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>9 0.13794121 <a title="26-tfidf-9" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>10 0.12388664 <a title="26-tfidf-10" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>11 0.12083242 <a title="26-tfidf-11" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>12 0.11689156 <a title="26-tfidf-12" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>13 0.099620663 <a title="26-tfidf-13" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>14 0.092399292 <a title="26-tfidf-14" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>15 0.07548704 <a title="26-tfidf-15" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>16 0.074971974 <a title="26-tfidf-16" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>17 0.07403741 <a title="26-tfidf-17" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>18 0.073962145 <a title="26-tfidf-18" href="./nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">12 nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<p>19 0.073144436 <a title="26-tfidf-19" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>20 0.06704662 <a title="26-tfidf-20" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.21), (1, 0.237), (2, 0.096), (3, -0.033), (4, -0.044), (5, -0.199), (6, -0.063), (7, -0.029), (8, -0.122), (9, 0.037), (10, -0.048), (11, 0.01), (12, 0.018), (13, 0.03), (14, -0.085), (15, -0.044), (16, 0.044), (17, 0.03), (18, -0.024), (19, -0.019), (20, 0.087), (21, -0.058), (22, -0.109), (23, -0.002), (24, -0.076), (25, 0.083), (26, -0.117), (27, 0.001), (28, -0.037), (29, -0.048), (30, -0.117), (31, 0.064), (32, -0.273), (33, -0.149), (34, 0.078), (35, -0.037), (36, -0.053), (37, -0.139), (38, 0.063), (39, -0.145), (40, -0.075), (41, 0.048), (42, 0.021), (43, 0.028), (44, -0.001), (45, -0.035), (46, 0.101), (47, 0.047), (48, -0.079), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97977245 <a title="26-lsi-1" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>Author: Christian W. Eurich</p><p>Abstract: A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. 1</p><p>2 0.73435014 <a title="26-lsi-2" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>Author: Javier R. Movellan, Thomas Wachtler, Thomas D. Albright, Terrence Sejnowski</p><p>Abstract: We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive ﬁeld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations. In this paper we introduce the notion of Morton-style factorial coding and illustrate how it may help analyze information integration and perceptual organization in the brain. In the neurosciences factorial codes are often studied in the context of mean tuning curves. A tuning curve is called separable if it can be expressed as the product of terms selectively inﬂuenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. Morton (1969) analyzed a wide variety of psychophysical experiments on word perception and showed that they could be explained using a model in which stimulus and context have separable effects on perception. More precisely, in Mortons’ model the joint effect of stimulus and context on a perceptual representation can be obtained by multiplying terms selectively controlled by stimulus and by context, i.e.,  £ © # #</p><p>3 0.65289003 <a title="26-lsi-3" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>4 0.63296187 <a title="26-lsi-4" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>Author: Matthias Bethge, David Rotermund, Klaus Pawelzik</p><p>Abstract: Here we derive optimal gain functions for minimum mean square reconstruction from neural rate responses subjected to Poisson noise. The shape of these functions strongly depends on the length T of the time window within which spikes are counted in order to estimate the underlying firing rate. A phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero. For a particular function class, we were able to prove the existence of a second-order phase transition analytically. The critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical results. We conclude that under most circumstances relevant to information processing in the brain, rate coding can be better ascribed to a binary (low-entropy) code than to the other extreme of rich analog coding. 1 Optimal neuronal gain functions for short decoding time windows The use of action potentials (spikes) as a means of communication is the striking feature of neurons in the central nervous system. Since the discovery by Adrian [1] that action potentials are generated by sensory neurons with a frequency that is substantially determined by the stimulus, the idea of rate coding has become a prevalent paradigm in neuroscience [2]. In particular, today the coding properties of many neurons from various areas in the cortex have been characterized by tuning curves, which describe the average firing rate response as a function of certain stimulus parameters. This way of description is closely related to the idea of analog coding, which constitutes the basis for many neural network models. Reliabl v inference from the observed number of spikes about the underlying firing rate of a neuronal response, however, requires a sufficiently long time interval, while integration times of neurons in vivo [3] as well as reaction times of humans or animals when performing classification tasks [4, 5] are known to be rather short. Therefore, it is important to understand, how neural rate coding is affected by a limited time window available for decoding. While rate codes are usually characterized by tuning functions relating the intensity of the ,f * http://www.neuro.urn-bremen.dermbethge neuronal response to a particular stimulus parameter, the question, how relevant the idea of analog coding actually is does not depend on the particular entity represented by a neuron. Instead it suffices to determine the shape of the gain function, which displays the mean firing rate as a function of the actual analog signal to be sent to subsequent neurons. Here we seek for optimal gain functions that minimize the minimum average squared reconstruction error for a uniform source signal transmitted through a Poisson channel as a function of the maximum mean number of spikes. In formal terms, the issue is to optimally encode a real random variable x in the number of pulses emitted by a neuron within a certain time window. Thereby, x stands for the intended analog output of the neuron that shall be signaled to subsequent neurons. The latter, however, can only observe a number of spikes k integrated within a time interval of length T. The statistical dependency between x and k is specified by the assumption of Poisson noise p(kIJL(x)) = (JL~))k exp{ -JL(X)} , (1) and the choice of the gain function f(x), which together with T determines the mean spike count J.L(x) == T f(x) . An important additional constraint is the limited output range of the neuronal firing rate, which can be included by the requirement of a bounded gain function (fmin :::; f (x) :::; f max, VX). Since inhibition can reliably prevent a neuron from firing, we will here consider the case f min == 0 only. Instead of specifying f max, we impose a bound directly on the mean spike count (i.e. J.L(x) :::; /l), because f max constitutes a meaningful constraint only in conjunction with a fixed time window length T. As objective function we consider the minimum mean squared error (MMSE) with respect to Lebesgue measure for x E [0, 1], ~ 2 X _ E x2 _ E (i2 _ _ [jt( )] - [] [] - 3 X ~ (Xl (J01 xp(kIJL(x)) dx r J01p(kIJL(x)) dx' (2) where x(k) == E[xlk] denotes the mean square estimator, which is the conditional expectation (see e.g. [6]). 1.1 Tunings and errors As derived in [7] on the basis of Fisher information the optimal gain function for a single neuron in the asymptotic limit T -+ 00 has a parabolic shape: fasymp(x) == fmaxx2 . (3) For any finite /l, however, this gain function is not necessarily optimal, and in the limit T -+ 0, it is straight forward to show that the optimal tuning curve is a step function f step (xl'19) == fmax 8 (x - {)) , (4) where 8(z) denotes the Heaviside function that equals one, if z > 0 and zero if z < O. The optimal threshold 'l9(p,) of the step tuning curve depends on /l and can be determined analytically 11(-) =1_ It 3 - V8e-J.' +1 4(1 - e- il ) (5) as well as the corresponding MMSE [8]: 2 2[fste p] _ 1 ( 3'19 (p,) ) X - 12 1 - [(1 -11(p))(l - e-iL)]-1 - 1 . (6) 1 S +1 0.5 CJ;) o ........ '------'-----'---'---'--'~----'----'-- ~---'---'---'--'~ 10-1 ~---,.---,---.,...............---.----.---.---.-.......-.-.--.-~ ...............~ Figure 1: The upper panel shows a bifurcation plot for {}(Jt) - wand {}(Jt) + w of the optimal gain function in 51 as a function of {t illustrating the phase transition from binary to continuous encoding. The dotted line separates the regions before and after the phase transition in all three panels. Left from this line (i.e. for Jt < Jt C) the step function given by Eq. 4+5 is optimal. The middle panel shows the MMSE of this step function (dashed) and of the optimal gain function in 52 (solid), which becomes smaller than the first one after the phase transition. The relative deviation between the minimal errors of 51 and 52 (i.e. (X~l - X~2)/X~2) is displayed in the lower panel and has a maximum below 0.035. The binary shape for small {t and the continuous parabolic shape for large {t implies that there has to be a transition from discrete to analog encoding with increasing {to Unfortunately it is not possible to determine the optimal gain function within the set of all bounded functions B :== {fli : [0, 1] -+ [0, fmax]} and hence, one has to choose a certain parameterized function space 5 c B in advance that is feasible for the optimization. In [8], we investigated various such function-'spaces and for {t < 2.9, we did not find any gain function with an error smaller than the MMSE of the step function. Furthermore, we always observed a phase transition from binary to analog encoding at a critical {t C that depends only slightly on the function space. As one can see in Fig. 1 (upper) pc is approximately three. In this paper, we consider two function classes 51, 52, which both contain the binary gain function as well as the asymptotic optimal parabolic function as special cases. Furthermore 51 is a proper subset of 52. Our interest in 51 results from the fact that we can analyze the phase transition in this subset analytically, while 52 is the most general parameterization for which we have. determined the optimal encoding numerically. The latter has six free parameters a :::; b :::; c E [0, 1], fmid E (0, fmax), a, f3 E [0,00) and the parameterization of the gain functions is given by o fS2 (xla, b, c, fmid, a, (3) fmid ( ~=: == , O</p><p>5 0.55353981 <a title="26-lsi-5" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>Author: Christian K. Machens, Michael Wehr, Anthony M. Zador</p><p>Abstract: How do cortical neurons represent the acoustic environment? This question is often addressed by probing with simple stimuli such as clicks or tone pips. Such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to uncover complex or higher-order neuronal response properties. Here we adopt an alternative approach, probing neuronal responses with complex acoustic stimuli, including animal vocalizations and music. We have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential ﬂuctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit, instead of just its output—a sparse binary spike train—as in conventional single unit physiological recordings. Whole cell recording thus provides a much richer source of information about the neuron’s response. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component—the spectrotemporal receptive ﬁeld (STRF)—of the transformation from the sound (as represented by its time-varying spectrogram) to the neuron’s membrane potential. We ﬁnd that the STRF has a rich dynamical structure, including excitatory regions positioned in general accord with the prediction of the simple tuning curve. We also ﬁnd that in many cases, much of the neuron’s response, although deterministically related to the stimulus, cannot be predicted by the linear component, indicating the presence of as-yet-uncharacterized nonlinear response properties.</p><p>6 0.53873265 <a title="26-lsi-6" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>7 0.51129043 <a title="26-lsi-7" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>8 0.50425559 <a title="26-lsi-8" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>9 0.41404441 <a title="26-lsi-9" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>10 0.38309592 <a title="26-lsi-10" href="./nips-2002-Adaptation_and_Unsupervised_Learning.html">18 nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>11 0.37264794 <a title="26-lsi-11" href="./nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">12 nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<p>12 0.36287355 <a title="26-lsi-12" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>13 0.36150178 <a title="26-lsi-13" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>14 0.35265487 <a title="26-lsi-14" href="./nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">71 nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<p>15 0.33548626 <a title="26-lsi-15" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>16 0.31441268 <a title="26-lsi-16" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>17 0.30100051 <a title="26-lsi-17" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>18 0.29706445 <a title="26-lsi-18" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>19 0.29470167 <a title="26-lsi-19" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>20 0.28132665 <a title="26-lsi-20" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.012), (23, 0.475), (42, 0.026), (54, 0.084), (55, 0.025), (64, 0.025), (67, 0.012), (68, 0.032), (74, 0.059), (92, 0.034), (98, 0.137)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90117151 <a title="26-lda-1" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>Author: Christian W. Eurich</p><p>Abstract: A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli. Minimal square estimation errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very diﬀerent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional eﬀects and can be extended to include neural nonlinearities such as nonclassical receptive ﬁelds. 1</p><p>2 0.87232184 <a title="26-lda-2" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>Author: W Wu, M. J. Black, Y. Gao, M. Serruya, A. Shaikhouni, J. P. Donoghue, Elie Bienenstock</p><p>Abstract: The direct neural control of external devices such as computer displays or prosthetic limbs requires the accurate decoding of neural activity representing continuous movement. We develop a real-time control system using the spiking activity of approximately 40 neurons recorded with an electrode array implanted in the arm area of primary motor cortex. In contrast to previous work, we develop a control-theoretic approach that explicitly models the motion of the hand and the probabilistic relationship between this motion and the mean ﬁring rates of the cells in 70 bins. We focus on a realistic cursor control task in which the subject must move a cursor to “hit” randomly placed targets on a computer monitor. Encoding and decoding of the neural data is achieved with a Kalman ﬁlter which has a number of advantages over previous linear ﬁltering techniques. In particular, the Kalman ﬁlter reconstructions of hand trajectories in off-line experiments are more accurate than previously reported results and the model provides insights into the nature of the neural coding of movement. ¨ ©§</p><p>3 0.87205893 <a title="26-lda-3" href="./nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">108 nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>Author: Peter Meinicke, Matthias Kaper, Florian Hoppe, Manfred Heumann, Helge Ritter</p><p>Abstract: In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the transfer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The objective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as motivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combination with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.</p><p>4 0.68296689 <a title="26-lda-4" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>Author: Benjamin V. Roy, Daniela D. Farias</p><p>Abstract: This paper extends our earlier analysis on approximate linear programming as an approach to approximating the cost-to-go function in a discounted-cost dynamic program [6]. In this paper, we consider the average-cost criterion and a version of approximate linear programming that generates approximations to the optimal average cost and differential cost function. We demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well-aligned with the objective of deriving a policy with low average cost. For that, the algorithm should aim at producing a good approximation of the differential cost function. We propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portions of the state space via state-relevance weights. Performance bounds suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choices for state-relevance weights.</p><p>5 0.52917027 <a title="26-lda-5" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>Author: Javier R. Movellan, Thomas Wachtler, Thomas D. Albright, Terrence Sejnowski</p><p>Abstract: We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive ﬁeld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations. In this paper we introduce the notion of Morton-style factorial coding and illustrate how it may help analyze information integration and perceptual organization in the brain. In the neurosciences factorial codes are often studied in the context of mean tuning curves. A tuning curve is called separable if it can be expressed as the product of terms selectively inﬂuenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. Morton (1969) analyzed a wide variety of psychophysical experiments on word perception and showed that they could be explained using a model in which stimulus and context have separable effects on perception. More precisely, in Mortons’ model the joint effect of stimulus and context on a perceptual representation can be obtained by multiplying terms selectively controlled by stimulus and by context, i.e.,  £ © # #</p><p>6 0.51362175 <a title="26-lda-6" href="./nips-2002-Combining_Features_for_BCI.html">55 nips-2002-Combining Features for BCI</a></p>
<p>7 0.50273061 <a title="26-lda-7" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>8 0.4988704 <a title="26-lda-8" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>9 0.49011111 <a title="26-lda-9" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>10 0.48066452 <a title="26-lda-10" href="./nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">180 nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>11 0.48053265 <a title="26-lda-11" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>12 0.47984907 <a title="26-lda-12" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>13 0.47781602 <a title="26-lda-13" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>14 0.46256453 <a title="26-lda-14" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>15 0.46166652 <a title="26-lda-15" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>16 0.45961317 <a title="26-lda-16" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>17 0.44465011 <a title="26-lda-17" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>18 0.44173774 <a title="26-lda-18" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>19 0.43672511 <a title="26-lda-19" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>20 0.43644497 <a title="26-lda-20" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
