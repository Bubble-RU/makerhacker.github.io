<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-151" href="#">nips2002-151</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2002-151-pdf" href="http://papers.nips.cc/paper/2280-multiplicative-updates-for-nonnegative-quadratic-programming-in-support-vector-machines.pdf">pdf</a></p><p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>Reference: <a title="nips-2002-151-reference" href="../nips2002_reference/nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). [sent-7, score-1.472]
</p><p>2 The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. [sent-8, score-0.769]
</p><p>3 The updates optimize the traditionally proposed objective function for SVMs. [sent-9, score-0.448]
</p><p>4 They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. [sent-10, score-0.143]
</p><p>5 They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. [sent-11, score-0.331]
</p><p>6 We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. [sent-12, score-0.633]
</p><p>7 In practice, the updates converge very rapidly to good classiﬁers. [sent-13, score-0.421]
</p><p>8 In SVMs, kernel methods are used to map inputs into a higher, potentially inﬁnite, dimensional feature space; the decision boundary between classes is then identiﬁed as the maximum margin hyperplane in the feature space. [sent-16, score-0.445]
</p><p>9 While SVMs provide the ﬂexibility to implement highly nonlinear classiﬁers, the maximum margin criterion helps to control the capacity for overﬁtting. [sent-17, score-0.223]
</p><p>10 Computing the maximum margin hyperplane in SVMs gives rise to a problem in nonnegative quadratic programming. [sent-19, score-0.802]
</p><p>11 The resulting optimization is convex, but due to the nonnegativity constraints, it cannot be solved in closed form, and iterative solutions are required. [sent-20, score-0.272]
</p><p>12 There is a large literature on iterative algorithms for nonnegative quadratic programming in general and for SVMs as a special case[3, 17]. [sent-21, score-0.633]
</p><p>13 Gradient-based methods are the simplest possible approach, but their convergence depends on careful selection of the learning rate, as well as constant attention to the nonnegativity constraints which may not be naturally enforced. [sent-22, score-0.235]
</p><p>14 Multiplicative updates based on exponentiated gradients (EG)[5, 10] have been  investigated as an alternative to traditional gradient-based methods. [sent-23, score-0.492]
</p><p>15 Multiplicative updates are naturally suited to sparse nonnegative optimizations, but EG updates—like their additive counterparts—suffer the drawback of having to choose a learning rate. [sent-24, score-0.743]
</p><p>16 Subset selection methods constitute another approach to the problem of nonnegative quadratic programming in SVMs. [sent-25, score-0.557]
</p><p>17 Generally speaking, these methods split the variables at each iteration into two sets: a ﬁxed set in which the variables are held constant, and a working set in which the variables are optimized by an internal subroutine. [sent-26, score-0.152]
</p><p>18 At the end of each iteration, a heuristic is used to transfer variables between the two sets and improve the objective function. [sent-27, score-0.143]
</p><p>19 An extreme version of this approach is the method of Sequential Minimal Optimization (SMO)[15], which updates only two variables per iteration. [sent-28, score-0.42]
</p><p>20 In this case, there exists an analytical solution for the updates, so that one avoids the expense of a potentially iterative optimization within each iteration of the main loop. [sent-29, score-0.137]
</p><p>21 In general, despite the many proposed approaches for training SVMs, solving the quadratic programming problem remains a bottleneck in their implementation. [sent-30, score-0.247]
</p><p>22 The M3 updates have a simple closed form and converge monotonically to the solution of the maximum margin hyperplane. [sent-33, score-0.742]
</p><p>23 2 Nonnegative quadratic programming We begin by studying the general problem of nonnegative quadratic programming. [sent-37, score-0.672]
</p><p>24 Consider the minimization of the quadratic objective function 1 F (v) = vT Av + bT v, (1) 2 subject to the constraints that vi ≥ 0 ∀i. [sent-38, score-0.477]
</p><p>25 Due to the nonnegativity constraints, however, there does not exist an analytical solution for the global minimum (or minima), and an iterative solution is needed. [sent-40, score-0.264]
</p><p>26 1 Multiplicative updates Our iterative solution is expressed in terms of the positive and negative components of the matrix A in eq. [sent-42, score-0.455]
</p><p>27 In particular, let A+ and A− denote the nonnegative matrices: |Aij | if Aij < 0, Aij if Aij > 0, (2) A+ = and A− = ij ij 0 otherwise. [sent-44, score-0.61]
</p><p>28 In terms of these nonnegative matrices, our proposed updates (to be applied in parallel to all the elements of v) take the form: vi ←− vi  −bi +  b2 + 4(A+ v)i (A− v)i i . [sent-46, score-1.225]
</p><p>29 (3) prescribes a multiplicative update for the ith element of v in terms of the ith elements of the vectors b, A+ v, and A+ v. [sent-52, score-0.594]
</p><p>30 Second, since the elements of v, A+ , and A− are nonnegative, the overall factor multiplying vi on the right hand side of eq. [sent-53, score-0.281]
</p><p>31 Hence, these updates never violate the constraints of nonnegativity. [sent-55, score-0.417]
</p><p>32 2 Fixed points We can show further that these updates have ﬁxed points wherever the objective function, F (v) achieves its minimum value. [sent-57, score-0.584]
</p><p>33 At ∗ ∗ such a point, one of two conditions must hold for each element v i : either (i) vi > 0 and ∗ (∂F/∂vi )|v∗ = 0, or (ii), vi = 0 and (∂F/∂vi )|v∗ ≥ 0. [sent-59, score-0.555]
</p><p>34 Here, the corresponding terms of ∗ the gradient must be nonnegative, thus pinning vi to the boundary of the feasibility region. [sent-63, score-0.283]
</p><p>35 (3) have ﬁxed points wherever the conditions for global minima are satisﬁed. [sent-65, score-0.16]
</p><p>36 To see this, let γi =  −bi +  b2 + 4(A+ v∗ )i (A− v∗ )i i 2(A+ v∗ )i  (5)  denote the factor multiplying the ith element of v in eq. [sent-66, score-0.192]
</p><p>37 Fixed points of the multiplicative updates occur when one of two conditions holds for each element v i : ∗ ∗ either (i) vi > 0 and γi = 1, or (ii) vi = 0. [sent-68, score-1.376]
</p><p>38 Thus the conditions for global minima establish the conditions for ﬁxed points of the multiplicative updates. [sent-71, score-0.535]
</p><p>39 3 Monotonic convergence The updates not only have the correct ﬁxed points; they also lead to monotonic improvement in the objective function, F (v). [sent-73, score-0.632]
</p><p>40 (1) decreases monotonically to the value of its global minimum under the multiplicative updates in eq. [sent-75, score-0.911]
</p><p>41 The proof of this theorem (sketched in Appendix A) relies on the construction of an auxiliary function which provides an upper bound on F (v). [sent-77, score-0.288]
</p><p>42 3 Support vector machines We now consider the problem of computing the maximum margin hyperplane in SVMs[3, 17, 18]. [sent-79, score-0.423]
</p><p>43 Let {(xi , yi )}N denote labeled examples with binary class labels yi = ±1, and i=1 let K(xi , xj ) denote the kernel dot product between inputs. [sent-80, score-0.425]
</p><p>44 In this paper, we focus on the simple case where in the high dimensional feature space, the classes are linearly separable and the hyperplane is required to pass through the origin 1 . [sent-81, score-0.217]
</p><p>45 In this case, the maximum margin hyperplane is obtained by minimizing the loss function: 1 αi αj yi yj K(xi , xj ), (6) L(α) = − αi + 2 ij i subject to the nonnegativity constraints αi ≥ 0. [sent-82, score-0.823]
</p><p>46 The maximal margin hyperplane has normal vector w = i α∗ yi xi i and satisﬁes the margin constraints yi K(w, xi ) ≥ 1 for all examples in the training set. [sent-84, score-1.101]
</p><p>47 4%  Table 1: Misclassiﬁcation error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. [sent-100, score-0.821]
</p><p>48 Thus, the multiplicative updates for computing the maximal margin hyperplane in hard margin SVMs are given by: αi ←− αi  1+  1 + 4(A+ α)i (A− α)i 2(A+ α)i  (7)  where A± are deﬁned as in eq. [sent-105, score-1.329]
</p><p>49 We will refer to the learning algorithm for hard margin SVMs based on these updates as Multiplicative Margin Maximization (M3 ). [sent-107, score-0.568]
</p><p>50 It is worth comparing the properties of these updates to those of other approaches. [sent-108, score-0.379]
</p><p>51 Like multiplicative updates based on exponentiated gradients (EG)[5, 10], the M 3 updates are well suited to sparse nonnegative optimizations2; unlike EG updates, however, they do not involve a learning rate, and they come with a guarantee of monotonic improvement. [sent-109, score-1.78]
</p><p>52 Like the updates for Sequential Minimal Optimization (SMO)[15], the M 3 updates have a simple closed form; unlike SMO updates, however, they can be used to adjust all the quadratic programming variables in parallel (or any subset thereof), not just two at a time. [sent-110, score-1.111]
</p><p>53 Finally, we emphasize that the M3 updates optimize the traditional objective function for SVMs; they do not compromise the goal of computing the maximal margin hyperplane. [sent-111, score-0.697]
</p><p>54 2 Experimental results We tested the effectiveness of the multiplicative updates in eq. [sent-113, score-0.789]
</p><p>55 (7) on two real world problems: binary classiﬁcation of aspect-angle dependent sonar signals[9] and breast cancer data[14]. [sent-114, score-0.319]
</p><p>56 The sonar and breast cancer data sets consist of 208 and 683 labeled examples, respectively. [sent-116, score-0.352]
</p><p>57 Training and test sets for the breast cancer experiments were created by 80%/20% splits of the available data. [sent-117, score-0.236]
</p><p>58 Misclassiﬁcation rates on the test data sets after 512 iterations of the multiplicative updates are shown in Table 1. [sent-124, score-0.881]
</p><p>59 As expected, the results match previously published error rates on these data sets[5], showing that the M3 updates do in practice converge to the maximum margin hyperplane. [sent-125, score-0.677]
</p><p>60 Figure 1 shows the rapid convergence of the updates to good classiﬁers in just one or two iterations. [sent-126, score-0.515]
</p><p>61 2  In fact, the multiplicative updates by nature cannot directly set a variable to zero. [sent-127, score-0.789]
</p><p>62 4  04 08  64  support vectors  non-support vectors  t  coefficients  iteration  0 0  100  200 300 training examples  400  g  500  Figure 1: Rapid convergence of the multiplicative updates in eq. [sent-147, score-1.079]
</p><p>63 The plots show results after different numbers of iterations on the breast cancer data set with the radial basis function kernel (σ = 3). [sent-149, score-0.319]
</p><p>64 For ease of visualization, the training examples were ordered so that support vectors appear to the left and non-support vectors, to the right. [sent-151, score-0.14]
</p><p>65 3 Asymptotic convergence The rapid decay of non-support vector coefﬁcients in Fig. [sent-156, score-0.164]
</p><p>66 1 motivated us to analyze their rates of asymptotic convergence. [sent-157, score-0.126]
</p><p>67 If we hold all the variables but αi ﬁxed and apply its multiplicative update, then the new displacement δαi after the update is given asymptotically by (δαi ) ≈ (δαi )γi , where 1 + 4(A+ α∗ )i (A− α∗ )i , (8) 2(A+ α∗ )i and Aij = yi yj K(xi , xj ). [sent-160, score-0.712]
</p><p>68 ) We can thus bound the asymptotic rate of convergence—in this idealized but instructive setting— by computing an upper bound on γi , which determines how fast the perturbed coefﬁcient decays to zero. [sent-164, score-0.267]
</p><p>69 ) In general, the asymptotic rate of convergence is determined by the overall positioning of the data points and classiﬁcation hyperplane in the feature space. [sent-166, score-0.412]
</p><p>70 γi =  1+  Theorem 2 Let di = |K(xi , w)|/ K(w, w) denote the perpendicular distance in the feature space from xi to the maximum margin hyperplane, and let d = minj dj = 1/ K(w, w) denote the one-sided margin of the classiﬁer. [sent-168, score-0.707]
</p><p>71 Also, let i = K(xi , xi ) denote the distance of xi to the origin in the feature space, and let = maxj j denote the largest such distance. [sent-169, score-0.498]
</p><p>72 Then a bound on the asymptotic rate of convergence γ i is given by: γi ≤  1+  1 (di − d)d 2 i  −1  . [sent-170, score-0.274]
</p><p>73 (9)  + +  +  li di  _  + _ _  d classification hyperplane  _  Figure 2: Quantities used to bound the asymptotic rate of convergence in eq. [sent-171, score-0.402]
</p><p>74 Solid circles denote support vectors; empty circles denote non-support vectors. [sent-173, score-0.15]
</p><p>75 The proof of this theorem is sketched in Appendix B. [sent-174, score-0.151]
</p><p>76 This is a highly desirable property for large numerical calculations, suggesting that the multiplicative updates could be used to quickly prune away outliers and reduce the size of the quadratic programming problem. [sent-177, score-1.01]
</p><p>77 In this paper, we have derived simple, closed form multiplicative updates for solving the nonnegative quadratic programming problem in SVMs. [sent-180, score-1.403]
</p><p>78 The M3 updates are straightforward to implement and have a rigorous guarantee of monotonic convergence. [sent-181, score-0.537]
</p><p>79 It is intriguing that multiplicative updates derived from auxiliary functions appear in so many other areas of machine learning, especially those involving sparse, nonnegative optimizations. [sent-182, score-1.222]
</p><p>80 Examples include the Baum-Welch algorithm[1] for discrete hidden markov models, generalized iterative scaling[6] and adaBoost[4] for logistic regression, and nonnegative matrix factorization[11, 12] for dimensionality reduction and feature extraction. [sent-183, score-0.441]
</p><p>81 In these areas, simple multiplicative updates with guarantees of monotonic convergence have emerged over time as preferred methods of optimization. [sent-184, score-0.973]
</p><p>82 A tutorial on support vector machines for pattern recognition. [sent-200, score-0.126]
</p><p>83 Learning the parts of objects with nonnegative matrix factorization. [sent-250, score-0.336]
</p><p>84 Fast training of support vector machines using sequential minimal optimization. [sent-281, score-0.179]
</p><p>85 A  Proof of Theorem 1  The proof of monotonic convergence in the objective function F (v), eq. [sent-314, score-0.295]
</p><p>86 An auxiliary function G(˜ , v) has the two crucial v ˜ properties that F (˜ ) ≤ G(˜ , v) and F (v) = G(v, v) for all nonnegative v,v. [sent-317, score-0.433]
</p><p>87 From such v v an auxiliary function, we can derive the update rule v = arg minv G(˜ , v) which never v ˜ increases (and generally decreases) the objective function F (v): F (v ) ≤ G(v , v) ≤ G(v, v) = F (v). [sent-318, score-0.196]
</p><p>88 For nonnegative quadratic programming, we derive an auxiliary function G( v, v) by decomposing F (v) in eq. [sent-320, score-0.548]
</p><p>89 (1) into three terms and then bounding each term separately: F (v) G(˜ , v) v  = =  1 2 1 2  A+ vi vj − ij ij  i  1 2  A− vi vj + ij ij  (A+ v)i 2 1 vi − ˜ vi 2  bi vi ,  (11)  i  A− vi vj ij ij  1 + log  vi vj ˜˜ vi vj  +  bi vi . [sent-321, score-3.379]
</p><p>90 The minimization of G(˜ , v) is performed by v v v setting its derivative to zero, leading to the multiplicative updates in eq. [sent-323, score-0.789]
</p><p>91 The updates  move each element vi in the same direction as −∂F/∂vi , with ﬁxed points occurring only ∗ if vi = 0 or ∂F/∂vi = 0. [sent-325, score-0.966]
</p><p>92 Since the overall optimization is convex, all minima of F (v) are global minima. [sent-326, score-0.125]
</p><p>93 The updates converge to the unique global minimum if it exists. [sent-327, score-0.502]
</p><p>94 B  Proof of Theorem 2  The proof of the bound on the asymptotic rate of convergence relies on the repeated use of equalities and inequalities that hold at the ﬁxed point α∗ . [sent-328, score-0.371]
</p><p>95 As + − shorthand, let zi = (A+ α∗ )i and zi = (A− α∗ )i . [sent-330, score-0.392]
</p><p>96 Then we have the following result: + 1 2zi = (13) γi 1 + 1 + 4z + z − i  i  + 2zi  ≥ 1+  (14)  + − + − (zi − zi )2 + 4zi zi  − + + 2zi zi − z i − 1 + − = 1+ + − 1 + z i + zi zi + z i + 1  =  (15)  + − zi − z i − 1 . [sent-331, score-1.074]
</p><p>97 (16) as:  ≥ 1+  + − zi − z i =  Aij α∗ = j  yi yj K(xi , xj )α∗ = yi K(xi , w) = |K(xi , w)|, (17) j  j  j  where w = j α∗ xj yj is the normal vector to the maximum margin hyperplane. [sent-334, score-0.892]
</p><p>98 (16) by: + zi  A + α∗ ij j  =  (18)  j  α∗ j  ≤ max A+ ik k  (19)  j  α∗ j  ≤ max |K(xi , xk )| k  K(xi , xi ) max  ≤  α∗ j  K(xk , xk )  k  (21)  j  K(xi , xi ) max  =  (20)  j  K(xk , xk )K(w, w). [sent-336, score-1.025]
</p><p>99 (23) is obtained by recognizing that α∗ is nonzero only for the coefﬁj cients of support vectors, and that in this case the optimality condition (∂L/∂α j )|α∗ = 0 implies k Ajk α∗ = 1. [sent-341, score-0.173]
</p><p>100 (24) γi 2 K(xi , xi ) maxk K(xk , xk )K(w, w) This reduces in a straightforward way to the claim of the theorem. [sent-345, score-0.279]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multiplicative', 0.41), ('updates', 0.379), ('nonnegative', 0.336), ('vi', 0.255), ('margin', 0.189), ('zi', 0.179), ('aij', 0.148), ('svms', 0.14), ('xi', 0.136), ('hyperplane', 0.128), ('sonar', 0.116), ('quadratic', 0.115), ('xk', 0.114), ('nonnegativity', 0.107), ('programming', 0.106), ('cancer', 0.106), ('auxiliary', 0.097), ('breast', 0.097), ('ij', 0.096), ('monotonic', 0.094), ('asymptotic', 0.093), ('convergence', 0.09), ('coef', 0.086), ('yj', 0.085), ('yi', 0.084), ('ajk', 0.08), ('iterative', 0.076), ('vj', 0.076), ('theorem', 0.072), ('objective', 0.069), ('cients', 0.065), ('bi', 0.064), ('xj', 0.062), ('exponentiated', 0.059), ('smo', 0.059), ('closed', 0.057), ('support', 0.054), ('radial', 0.054), ('eg', 0.051), ('bound', 0.051), ('minima', 0.049), ('denote', 0.048), ('rapid', 0.046), ('classi', 0.045), ('element', 0.045), ('machines', 0.044), ('global', 0.044), ('mangasarian', 0.042), ('proof', 0.042), ('converge', 0.042), ('monotonically', 0.041), ('variables', 0.041), ('rate', 0.04), ('ith', 0.039), ('constraints', 0.038), ('sketched', 0.037), ('xed', 0.037), ('minimum', 0.037), ('kernel', 0.036), ('wherever', 0.035), ('guarantee', 0.035), ('let', 0.034), ('maximal', 0.034), ('max', 0.034), ('maximization', 0.034), ('adjust', 0.034), ('maximum', 0.034), ('rates', 0.033), ('sets', 0.033), ('origin', 0.033), ('decays', 0.032), ('involve', 0.032), ('optimization', 0.032), ('points', 0.032), ('vectors', 0.031), ('update', 0.03), ('inequalities', 0.029), ('misclassi', 0.029), ('iteration', 0.029), ('feature', 0.029), ('straightforward', 0.029), ('examples', 0.029), ('nonzero', 0.028), ('adaboost', 0.028), ('vector', 0.028), ('polynomial', 0.028), ('gradients', 0.028), ('suited', 0.028), ('gradient', 0.028), ('prove', 0.027), ('appendix', 0.027), ('pass', 0.027), ('axes', 0.027), ('sequential', 0.027), ('implies', 0.026), ('traditional', 0.026), ('iterations', 0.026), ('training', 0.026), ('relies', 0.026), ('multiplying', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="151-tfidf-1" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>2 0.13967466 <a title="151-tfidf-2" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>3 0.13135478 <a title="151-tfidf-3" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>4 0.12915973 <a title="151-tfidf-4" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>Author: Olivier Bousquet, Daniel Herrmann</p><p>Abstract: We investigate data based procedures for selecting the kernel when learning with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors ﬁx. This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to overﬁtting. We thus propose a suitable way of constraining the class. We use an efﬁcient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach.</p><p>5 0.12839383 <a title="151-tfidf-5" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>Author: Baback Moghaddam, Gregory Shakhnarovich</p><p>Abstract: We introduce a novel learning algorithm for binary classiﬁcation with hyperplane discriminants based on pairs of training points from opposite classes (dyadic hypercuts). This algorithm is further extended to nonlinear discriminants using kernel functions satisfying Mercer’s conditions. An ensemble of simple dyadic hypercuts is learned incrementally by means of a conﬁdence-rated version of AdaBoost, which provides a sound strategy for searching through the ﬁnite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classiﬁers was found to be comparable to that of SVMs and k-NN classiﬁers. Furthermore, the computational cost of classiﬁcation (at run time) was found to be similar to, or better than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we oﬀer an on-line and incremental learning machine for building kernel discriminants whose complexity (number of kernel evaluations) can be directly controlled (traded oﬀ for accuracy). 1</p><p>6 0.12825735 <a title="151-tfidf-6" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>7 0.12372448 <a title="151-tfidf-7" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>8 0.11403621 <a title="151-tfidf-8" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>9 0.11366791 <a title="151-tfidf-9" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>10 0.1123213 <a title="151-tfidf-10" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>11 0.10669032 <a title="151-tfidf-11" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>12 0.10469003 <a title="151-tfidf-12" href="./nips-2002-Learning_in_Spiking_Neural_Assemblies.html">129 nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>13 0.10359569 <a title="151-tfidf-13" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>14 0.10051455 <a title="151-tfidf-14" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>15 0.096666045 <a title="151-tfidf-15" href="./nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">96 nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>16 0.089077316 <a title="151-tfidf-16" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>17 0.088113859 <a title="151-tfidf-17" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>18 0.086400472 <a title="151-tfidf-18" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>19 0.083886638 <a title="151-tfidf-19" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>20 0.083643161 <a title="151-tfidf-20" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.241), (1, -0.139), (2, -0.002), (3, -0.071), (4, 0.137), (5, -0.015), (6, 0.021), (7, 0.067), (8, -0.013), (9, -0.037), (10, 0.058), (11, -0.175), (12, -0.033), (13, -0.108), (14, 0.018), (15, -0.027), (16, 0.079), (17, 0.053), (18, 0.01), (19, 0.073), (20, -0.041), (21, 0.023), (22, 0.129), (23, -0.033), (24, 0.068), (25, -0.019), (26, 0.082), (27, 0.022), (28, 0.024), (29, -0.163), (30, -0.048), (31, 0.041), (32, -0.114), (33, 0.119), (34, 0.056), (35, -0.072), (36, 0.016), (37, -0.009), (38, 0.11), (39, -0.087), (40, -0.056), (41, -0.069), (42, 0.064), (43, -0.076), (44, -0.109), (45, -0.051), (46, -0.043), (47, 0.034), (48, 0.023), (49, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96841967 <a title="151-lsi-1" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>2 0.68251723 <a title="151-lsi-2" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>Author: Thomas Strohmann, Gregory Z. Grudic</p><p>Abstract: We formulate the regression problem as one of maximizing the minimum probability, symbolized by Ω, that future predicted outputs of the regression model will be within some ±ε bound of the true regression function. Our formulation is unique in that we obtain a direct estimate of this lower probability bound Ω. The proposed framework, minimax probability machine regression (MPMR), is based on the recently described minimax probability machine classiﬁcation algorithm [Lanckriet et al.] and uses Mercer Kernels to obtain nonlinear regression models. MPMR is tested on both toy and real world data, verifying the accuracy of the Ω bound, and the efﬁcacy of the regression models. 1</p><p>3 0.64235246 <a title="151-lsi-3" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>4 0.62330586 <a title="151-lsi-4" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>5 0.59997469 <a title="151-lsi-5" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>6 0.5358361 <a title="151-lsi-6" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>7 0.5047549 <a title="151-lsi-7" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>8 0.48902655 <a title="151-lsi-8" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>9 0.47996578 <a title="151-lsi-9" href="./nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">158 nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>10 0.45671356 <a title="151-lsi-10" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>11 0.45586991 <a title="151-lsi-11" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>12 0.4242073 <a title="151-lsi-12" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>13 0.40846854 <a title="151-lsi-13" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>14 0.40607989 <a title="151-lsi-14" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>15 0.40215889 <a title="151-lsi-15" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>16 0.39121923 <a title="151-lsi-16" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>17 0.38678372 <a title="151-lsi-17" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>18 0.38616255 <a title="151-lsi-18" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>19 0.38044924 <a title="151-lsi-19" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>20 0.3786864 <a title="151-lsi-20" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.014), (11, 0.013), (23, 0.012), (42, 0.056), (54, 0.156), (55, 0.038), (68, 0.013), (74, 0.062), (92, 0.425), (98, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98461998 <a title="151-lda-1" href="./nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>Author: Sumio Watanabe, Shun-ichi Amari</p><p>Abstract: A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coeﬃcient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general. 1</p><p>2 0.93309826 <a title="151-lda-2" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Classiﬁcation trees are one of the most popular types of classiﬁers, with ease of implementation and interpretation being among their attractive features. Despite the widespread use of classiﬁcation trees, theoretical analysis of their performance is scarce. In this paper, we show that a new family of classiﬁcation trees, called dyadic classiﬁcation trees (DCTs), are near optimal (in a minimax sense) for a very broad range of classiﬁcation problems. This demonstrates that other schemes (e.g., neural networks, support vector machines) cannot perform signiﬁcantly better than DCTs in many cases. We also show that this near optimal performance is attained with linear (in the number of training data) complexity growing and pruning algorithms. Moreover, the performance of DCTs on benchmark datasets compares favorably to that of standard CART, which is generally more computationally intensive and which does not possess similar near optimality properties. Our analysis stems from theoretical results on structural risk minimization, on which the pruning rule for DCTs is based.</p><p>same-paper 3 0.93105423 <a title="151-lda-3" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>4 0.92961437 <a title="151-lda-4" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>5 0.73106921 <a title="151-lda-5" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>Author: Bernd Fischer, Johann Schumann, Wray Buntine, Alexander G. Gray</p><p>Abstract: Machine learning has reached a point where many probabilistic methods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e.g., as different instances of the EM algorithm. This enables the systematic derivation of algorithms customized for different models. Here, we describe the AUTO BAYES system which takes a high-level statistical model speciﬁcation, uses powerful symbolic techniques based on schema-based program synthesis and computer algebra to derive an efﬁcient specialized algorithm for learning that model, and generates executable code implementing that algorithm. This capability is far beyond that of code collections such as Matlab toolboxes or even tools for model-independent optimization such as BUGS for Gibbs sampling: complex new algorithms can be generated without new programming, algorithms can be highly specialized and tightly crafted for the exact structure of the model and data, and efﬁcient and commented code can be generated for different languages or systems. We present automatically-derived algorithms ranging from closed-form solutions of Bayesian textbook problems to recently-proposed EM algorithms for clustering, regression, and a multinomial form of PCA. 1 Automatic Derivation of Statistical Algorithms Overview. We describe a symbolic program synthesis system which works as a “statistical algorithm compiler:” it compiles a statistical model speciﬁcation into a custom algorithm design and from that further down into a working program implementing the algorithm design. This system, AUTO BAYES, can be loosely thought of as “part theorem prover, part Mathematica, part learning textbook, and part Numerical Recipes.” It provides much more ﬂexibility than a ﬁxed code repository such as a Matlab toolbox, and allows the creation of efﬁcient algorithms which have never before been implemented, or even written down. AUTO BAYES is intended to automate the more routine application of complex methods in novel contexts. For example, recent multinomial extensions to PCA [2, 4] can be derived in this way. The algorithm design problem. Given a dataset and a task, creating a learning method can be characterized by two main questions: 1. What is the model? 2. What algorithm will optimize the model parameters? The statistical algorithm (i.e., a parameter optimization algorithm for the statistical model) can then be implemented manually. The system in this paper answers the algorithm question given that the user has chosen a model for the data,and continues through to implementation. Performing this task at the state-of-the-art level requires an intertwined meld of probability theory, computational mathematics, and software engineering. However, a number of factors unite to allow us to solve the algorithm design problem computationally: 1. The existence of fundamental building blocks (e.g., standardized probability distributions, standard optimization procedures, and generic data structures). 2. The existence of common representations (i.e., graphical models [3, 13] and program schemas). 3. The formalization of schema applicability constraints as guards. 1 The challenges of algorithm design. The design problem has an inherently combinatorial nature, since subparts of a function may be optimized recursively and in different ways. It also involves the use of new data structures or approximations to gain performance. As the research in statistical algorithms advances, its creative focus should move beyond the ultimately mechanical aspects and towards extending the abstract applicability of already existing schemas (algorithmic principles like EM), improving schemas in ways that generalize across anything they can be applied to, and inventing radically new schemas. 2 Combining Schema-based Synthesis and Bayesian Networks Statistical Models. Externally, AUTO BAYES has the look and feel of 2 const int n_points as ’nr. of data points’ a compiler. Users specify their model 3 with 0 < n_points; 4 const int n_classes := 3 as ’nr. classes’ of interest in a high-level speciﬁcation 5 with 0 < n_classes language (as opposed to a program6 with n_classes << n_points; ming language). The ﬁgure shows the 7 double phi(1..n_classes) as ’weights’ speciﬁcation of the mixture of Gaus8 with 1 = sum(I := 1..n_classes, phi(I)); 9 double mu(1..n_classes); sians example used throughout this 9 double sigma(1..n_classes); paper.2 Note the constraint that the 10 int c(1..n_points) as ’class labels’; sum of the class probabilities must 11 c ˜ disc(vec(I := 1..n_classes, phi(I))); equal one (line 8) along with others 12 data double x(1..n_points) as ’data’; (lines 3 and 5) that make optimization 13 x(I) ˜ gauss(mu(c(I)), sigma(c(I))); of the model well-deﬁned. Also note 14 max pr(x| phi,mu,sigma ) wrt phi,mu,sigma ; the ability to specify assumptions of the kind in line 6, which may be used by some algorithms. The last line speciﬁes the goal inference task: maximize the conditional probability pr with respect to the parameters , , and . Note that moving the parameters across to the left of the conditioning bar converts this from a maximum likelihood to a maximum a posteriori problem. 1 model mog as ’Mixture of Gaussians’; ¡   £  £  £ §¤¢ £ © ¨ ¦ ¥ ©   ¡     ¡ £ £ £ ¨ Computational logic and theorem proving. Internally, AUTO BAYES uses a class of techniques known as computational logic which has its roots in automated theorem proving. AUTO BAYES begins with an initial goal and a set of initial assertions, or axioms, and adds new assertions, or theorems, by repeated application of the axioms, until the goal is proven. In our context, the goal is given by the input model; the derived algorithms are side effects of constructive theorems proving the existence of algorithms for the goal. 1 Schema guards vary widely; for example, compare Nead-Melder simplex or simulated annealing (which require only function evaluation), conjugate gradient (which require both Jacobian and Hessian), EM and its variational extension [6] (which require a latent-variable structure model). 2 Here, keywords have been underlined and line numbers have been added for reference in the text. The as-keyword allows annotations to variables which end up in the generated code’s comments. Also, n classes has been set to three (line 4), while n points is left unspeciﬁed. The class variable and single data variable are vectors, which deﬁnes them as i.i.d. Computer algebra. The ﬁrst core element which makes automatic algorithm derivation feasible is the fact that we can mechanize the required symbol manipulation, using computer algebra methods. General symbolic differentiation and expression simpliﬁcation are capabilities fundamental to our approach. AUTO BAYES contains a computer algebra engine using term rewrite rules which are an efﬁcient mechanism for substitution of equal quantities or expressions and thus well-suited for this task.3 Schema-based synthesis. The computational cost of full-blown theorem proving grinds simple tasks to a halt while elementary and intermediate facts are reinvented from scratch. To achieve the scale of deduction required by algorithm derivation, we thus follow a schema-based synthesis technique which breaks away from strict theorem proving. Instead, we formalize high-level domain knowledge, such as the general EM strategy, as schemas. A schema combines a generic code fragment with explicitly speciﬁed preconditions which describe the applicability of the code fragment. The second core element which makes automatic algorithm derivation feasible is the fact that we can use Bayesian networks to efﬁciently encode the preconditions of complex algorithms such as EM. First-order logic representation of Bayesian netNclasses works. A ﬁrst-order logic representation of Bayesian µ σ networks was developed by Haddawy [7]. In this framework, random variables are represented by functor symbols and indexes (i.e., speciﬁc instances φ x c of i.i.d. vectors) are represented as functor arguments. discrete gauss Nclasses Since unknown index values can be represented by Npoints implicitly universally quantiﬁed Prolog variables, this approach allows a compact encoding of networks involving i.i.d. variables or plates [3]; the ﬁgure shows the initial network for our running example. Moreover, such networks correspond to backtrack-free datalog programs, allowing the dependencies to be efﬁciently computed. We have extended the framework to work with non-ground probability queries since we seek to determine probabilities over entire i.i.d. vectors and matrices. Tests for independence on these indexed Bayesian networks are easily developed in Lauritzen’s framework which uses ancestral sets and set separation [9] and is more amenable to a theorem prover than the double negatives of the more widely known d-separation criteria. Given a Bayesian network, some probabilities can easily be extracted by enumerating the component probabilities at each node: § ¥ ¨¦¡ ¡ ¢© Lemma 1. Let be sets of variables over a Bayesian network with . Then descendents and parents hold 4 in the corresponding dependency graph iff the following probability statement holds: £ ¤  ¡ parents B % % 9 C0A@ ! 9  @8 § ¥   ¢   2 ' % % 310  parents    ©¢   £ ¡ !    ' % #!  </p><p>6 0.67979294 <a title="151-lda-6" href="./nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>7 0.66907722 <a title="151-lda-7" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>8 0.65344191 <a title="151-lda-8" href="./nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">166 nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>9 0.64176327 <a title="151-lda-9" href="./nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">189 nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>10 0.637954 <a title="151-lda-10" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>11 0.63387853 <a title="151-lda-11" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>12 0.62636244 <a title="151-lda-12" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>13 0.62349468 <a title="151-lda-13" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>14 0.62046683 <a title="151-lda-14" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>15 0.61807406 <a title="151-lda-15" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>16 0.61345327 <a title="151-lda-16" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>17 0.60879326 <a title="151-lda-17" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>18 0.60718751 <a title="151-lda-18" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>19 0.60218179 <a title="151-lda-19" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>20 0.60027409 <a title="151-lda-20" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
