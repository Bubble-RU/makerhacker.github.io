<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-84" href="#">nips2002-84</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</h1>
<br/><p>Source: <a title="nips-2002-84-pdf" href="http://papers.nips.cc/paper/2325-fast-exact-inference-with-a-factored-model-for-natural-language-parsing.pdf">pdf</a></p><p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>Reference: <a title="nips-2002-84-reference" href="../nips2002_reference/nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. [sent-6, score-0.294]
</p><p>2 Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference. [sent-8, score-0.709]
</p><p>3 Additionally, methods based only on key lexical dependencies have been shown to be very effective in choosing between valid syntactic forms [1]. [sent-12, score-0.225]
</p><p>4 Modern statistical parsers [2, 3] standardly use complex joint models of over both category labels and lexical items, where “everything is conditioned on everything” to the extent possible within the limits of data sparseness and ﬁnite computer memory. [sent-13, score-0.345]
</p><p>5 For example, the probability that a verb phrase will take a noun phrase object depends on the head word of the verb phrase. [sent-14, score-0.362]
</p><p>6 There are certainly statistical interactions between syntactic and semantic structure, and, if deeper underlying variables of communication are not modeled, everything tends to be dependent on everything else in language [4]. [sent-16, score-0.217]
</p><p>7 However, the above considerations suggest that there might be considerable value in a factored model, which provides separate models of syntactic conﬁgurations and lexical dependencies, and then combines them to determine optimal parses. [sent-17, score-0.275]
</p><p>8 For example, under this view, we may know that acquired takes right dependents headed by nouns such as company or division, while agreed takes no noun-headed right dependents at all. [sent-18, score-0.187]
</p><p>9 If so, there is no need to explicitly model the phrasal selection on top of the lexical selection. [sent-19, score-0.16]
</p><p>10 2 A Factored Model Generative models for parsing typically model one of the kinds of structures shown in ﬁgure 1. [sent-22, score-0.365]
</p><p>11 A lexicalized tree can be viewed as the pair L = (T, D) of a phrase structure tree T and a dependency tree D. [sent-24, score-0.716]
</p><p>12 In this view, generative models over lexicalized trees, of the sort standard in lexicalized PCFG parsing [2, 3], can be regarded as assigning mass P(T, D) to such pairs. [sent-25, score-0.78]
</p><p>13 To the extent that dependency and phrase structure need not be modeled jointly, we can factor our model as P(T, D) = P(T )P(D): this approach is the basis of our proposed models, and its use is, to our knowledge, new. [sent-26, score-0.342]
</p><p>14 We show that the combination of even quite simple “off the shelf” implementations of the two sub-models can provide decent parsing performance. [sent-33, score-0.317]
</p><p>15 It is now well known that such annotation improves the accuracy of PCFG parsing by weakening the PCFG independence assumptions. [sent-42, score-0.336]
</p><p>16 Since the counts were not fragmented by head word or head tag, we were able to directly use the MLE parameters, without smoothing. [sent-44, score-0.327]
</p><p>17 Models of P(D) were lexical dependency models, which deal with tagged words: pairs w, t . [sent-48, score-0.359]
</p><p>18 First the head wh , th of a constituent is generated, then successive right dependents wd , td until a STOP token is generated, then successive left dependents until is generated again. [sent-49, score-0.547]
</p><p>19 The dependency models required smoothing, as the word-word dependency data is very sparse. [sent-53, score-0.527]
</p><p>20 In our basic model, DEP - BASIC, we generate a dependent conditioned on the head and direction, using a mixture of two generation paths: a head can select a speciﬁc argument word, or a head can select only an argument tag. [sent-54, score-0.39]
</p><p>21 For head selection of words, there is a prior distribution over dependents taken by the head’s tag, for example, left dependents taken by past tense verbs: P(wd , td |th , dir ) = count(wd , td , th , dir )/count(th , dir ). [sent-55, score-0.847]
</p><p>22 Observations of bilexical pairs are taken against this prior, with some prior strength κ: P(wd , td |wh , th , dir ) =  count(wd , td , wh , th , dir ) + κ P(wd , td |th , dir ) count(wh , th , dir ) + κ  This model can capture bilexical selection, such as the afﬁnity between payrolls and fell. [sent-56, score-1.063]
</p><p>23 Alternately, the dependent can have only its tag selected, and then the word is generated independently: P(wd , td |wh , th , dir ) = P(wd |td )P(td |wh , th , dir ). [sent-57, score-0.542]
</p><p>24 The estimates for P(td |wh , th , dir ) are similar to the above. [sent-58, score-0.178]
</p><p>25 In the enhanced dependency model, DEP - VAL, we condition not only on direction, but also on distance and valence. [sent-60, score-0.251]
</p><p>26 By factoring the semantic and syntactic models, we have certainly simpliﬁed both (and fragmented the data less), but there are always simpler models, and researchers have adopted complex ones because of their parsing accuracy. [sent-66, score-0.415]
</p><p>27 In the remainder of the paper, we demonstrate the three primary beneﬁts of our model: a fast, exact parsing algorithm; parsing accuracy comparable to non-factored models; and useful modularity which permits easy extensibility. [sent-67, score-0.645]
</p><p>28 3 An A* Parser In this section, we outline an efﬁcient algorithm for ﬁnding the Viterbi, or most probable, parse for a given terminal sequence in our factored lexicalized model. [sent-71, score-0.409]
</p><p>29 The naive approach to lexicalized PCFG parsing is to act as if the lexicalized PCFG is simply a large nonlexical PCFG, with many more symbols than its nonlexicalized PCFG backbone. [sent-72, score-0.771]
</p><p>30 For example, while the original PCFG might have a symbol NP, the lexicalized one has a symbol NP-x for every possible head x in the vocabulary. [sent-73, score-0.391]
</p><p>31 3 Within a dynamic program, the core parse item in this case is the edge, shown in ﬁgure 2, which is speciﬁed by its start, end, root symbol, and head position. [sent-75, score-0.26]
</p><p>32 5 The core of our parsing algorithm is a tabular agenda-based parser, using the O(n 5 ) schema above. [sent-79, score-0.353]
</p><p>33 The novelty is in the choice of agenda priority, where we exploit the rapid parsing algorithms available for the sub-models to speed up the otherwise impractical combined parse. [sent-80, score-0.396]
</p><p>34 Our choice of priority also guarantees optimality, in the sense that when the goal edge is removed, its most probable parse is known exactly. [sent-81, score-0.226]
</p><p>35 Other lexicalized parsers accelerate parsing in ways that destroy this optimality guarantee. [sent-82, score-0.694]
</p><p>36 First, we parse exhaustively with the two sub-models, not to ﬁnd complete parses, but to ﬁnd best outside scores for each edge e. [sent-84, score-0.328]
</p><p>37 An outside score is the score of the best parse structure which starts at the goal and includes e, the words before it, and the words after it, as depicted in ﬁgure 3. [sent-85, score-0.43]
</p><p>38 For the syntactic model, P(T ), well-known cubic PCFG parsing algorithms are easily adapted to ﬁnd outside scores. [sent-87, score-0.46]
</p><p>39 For the semantic model, P(D), there are several presentations of cubic dependency parsing algorithms, including [9] and [12]. [sent-88, score-0.594]
</p><p>40 These can also be adapted to produce outside scores in cubic time, though since their basic data structures are not edges, there is some subtlety. [sent-89, score-0.225]
</p><p>41 An agenda-based parser tracks all edges that have been constructed at a given time. [sent-91, score-0.407]
</p><p>42 When an edge is ﬁrst constructed, it is put on an agenda, which is a priority queue indexed by some score for that node. [sent-92, score-0.19]
</p><p>43 The agenda is a holding area for edges which have been built in at least one way, but which have not yet been used in the construction of other edges. [sent-93, score-0.158]
</p><p>44 The core cycle of the parser is to remove the highest-priority edge from the agenda, and act on it according to the edge combination schema, combining it with any previously removed, compatible edges. [sent-94, score-0.545]
</p><p>45 This much is common to many parsers; agenda-based parsers primarily differ in their choice of edge priority. [sent-95, score-0.229]
</p><p>46 If the best known inside score for an edge is used as a priority, then the parser will be optimal. [sent-96, score-0.491]
</p><p>47 However, even the O(n 4 ) formulation is impractical for exhaustive parsing with broad-coverage, lexicalized treebank grammars. [sent-101, score-0.575]
</p><p>48 We did implement a version of this parser using the O(n 4 ) formulation of [9], but, because of the effectiveness of the A* estimate, it was only marginally faster; see section 4. [sent-103, score-0.346]
</p><p>49 Use the PCFG parser to ﬁnd outside scores αPCFG (e) for each edge. [sent-112, score-0.469]
</p><p>50 Extract the dependency sub-model and set up the dependency parser. [sent-113, score-0.502]
</p><p>51 Use the dependency parser to ﬁnd outside scores αDEP (e) for each edge. [sent-114, score-0.72]
</p><p>52 Combine PCFG and dependency sub-models into the lexicalized model. [sent-115, score-0.475]
</p><p>53 Form the combined outside estimate a(e) = αPCFG (e) + αDEP (e) Use the lexicalized A* parser, with a(e) as an A* estimate of α(e)  α e β words  Figure 3: The top-level algorithm and an illustration of inside and outside scores. [sent-116, score-0.458]
</p><p>54 However, removing edges by inside score is not practical (see section 4 for an empirical demonstration), because all small edges end up having better scores than any large edges. [sent-133, score-0.336]
</p><p>55 Luckily, the optimality of the algorithm remains if, rather than removing items from the agenda by their best inside scores, we add to those scores any optimistic (admissible) estimate of the cost to complete a parse using that item. [sent-134, score-0.303]
</p><p>56 To our knowledge, no way of generating effective, admissible A* estimates for lexicalized parsing has previously been proposed. [sent-136, score-0.51]
</p><p>57 6 However, because of the factored structure of our model, we can use the results of the sub-models’ parses to give us quite sharp A* estimates. [sent-137, score-0.159]
</p><p>58 Say we want to know the outside score of an edge e. [sent-138, score-0.222]
</p><p>59 That score will be the score α(Te , De ) (a logprobability) of a certain structure (Te , De ) outside of e, where Te and De are a compatible pair. [sent-139, score-0.291]
</p><p>60 From the initial phases, we know the exact scores of the overall best Te and the best De which can occur outside of e, though of course it may well be that Te and De are not compatible. [sent-140, score-0.193]
</p><p>61 Therefore, we can use the sum of the sub-models’ outside scores, a(e) = αPCFG (Te ) + αDEP (De ), as an upper bound on the outside score for the combined model. [sent-142, score-0.265]
</p><p>62 4 Empirical Performance In this section, we demonstrate that (i) the factored model’s parsing performance is comparable to non-factored models which use similar features, (ii) there is an advantage to exact inference, and (iii) the A* savings are substantial. [sent-145, score-0.428]
</p><p>63 First, we give parsing ﬁgures on the standard Penn treebank parsing task. [sent-146, score-0.637]
</p><p>64 The treebank only supplies node labels (like NP) and 6 The basic idea of changing edge priorities to more effectively guide parser work is standardly used, and other authors have made very effective use of inadmissible estimates. [sent-149, score-0.526]
</p><p>65 Absolute pruning can, and does, prevent the most likely parse from being returned at all. [sent-151, score-0.174]
</p><p>66 This, too, may result in the ﬁrst parse found not being the most likely parse, but it has another more subtle drawback: if we hold back an edge e for too long, we may use e to build another edge f in a new, better way. [sent-153, score-0.25]
</p><p>67 We effectively have three parsers: the PCFG (sub-)parser, which produces nonlexical phrase structures like ﬁgure 1a, the dependency (sub-)parser, which produces dependency structures like ﬁgure 1b, and the combination parser, which produces lexicalized phrase structures like ﬁgure 1c. [sent-196, score-1.096]
</p><p>68 The outputs of the combination parser can also be projected down to either nonlexical phrase structures or dependency structures. [sent-197, score-0.767]
</p><p>69 First, the phrase structure of the PCFG and combination parsers can be compared to the treebank parses. [sent-199, score-0.349]
</p><p>70 The parsing measures standardly used for this task are labeled precision and recall. [sent-200, score-0.336]
</p><p>71 Second, for the dependency and combination parsers, we can score the dependency structures. [sent-202, score-0.613]
</p><p>72 A dependency structure D is viewed as a set of head-dependent pairs h, d , with an extra dependency r oot, x where r oot is a special symbol and x is the head of the sentence. [sent-203, score-0.69]
</p><p>73 Although the dependency model generates part-of-speech tags as well, these are ignored for dependency accuracy. [sent-204, score-0.502]
</p><p>74 Since all dependency structures over n non-punctuation terminals contain n dependencies (n − 1 plus the root dependency), we report only accuracy, which is identical to both precision and recall. [sent-206, score-0.329]
</p><p>75 It should be stressed that the “correct” dependency structures, though generally correct, are generated from the PCFG structures by linguistically motivated, but automatic and only heuristic rules. [sent-207, score-0.305]
</p><p>76 Figure 4 shows the relevant scores for the various PCFG and dependency parsers alone. [sent-208, score-0.483]
</p><p>77 8 The valence model increases the dependency model’s accuracy from 76. [sent-209, score-0.276]
</p><p>78 Note, however, that even the pair of basic models has a combined dependency accuracy higher than the enhanced dependency model alone, and the top three have combined F1 better than the best PCFG model alone. [sent-222, score-0.649]
</p><p>79 For the top pair, ﬁgure 6c illustrates the relative F1 of the combination parser to the PCFG component alone, showing the unsurprising trend that the addition of the dependency model helps more for longer sentences, which, on average, contain more attachment ambiguity. [sent-223, score-0.658]
</p><p>80 7% is greater than that of the lexicalized parsers presented in [15, 16], but less than that of the newer, more complex, parsers presented in [3, 2], which reach as high as 90. [sent-225, score-0.548]
</p><p>81 8 The dependency model is sensitive to any preterminal annotation (tag splitting) done by the PCFG model. [sent-231, score-0.276]
</p><p>82 However, it is worth pointing out that these higher-accuracy parsers incorporate many ﬁnely wrought enhancements which could presumably be extracted and applied to beneﬁt our individual models. [sent-236, score-0.162]
</p><p>83 Given the impracticality of exact inference for standard parsers, a common strategy is to take a PCFG backbone, extract a set of top parses, either the top k or all parses within a score threshold of the top parse, and rerank them [3, 17]. [sent-238, score-0.302]
</p><p>84 Figure 5 shows the result of parsing with our combined model, using the best model pair, but with the A* estimates altered to block parses whose PCFG projection had a score further than a threshold δ = 2 in log-probability from the best PCFG-only parse. [sent-241, score-0.47]
</p><p>85 Figure 6a shows the average number of edges extracted from the agenda as sentence length increases. [sent-244, score-0.24]
</p><p>86 Clearly, the uniform-cost version of the parser is dramatically less efﬁcient; by sentence length 15 it extracts over 800K edges, while even at length 40 the A* heuristics are so effective that only around 2K edges are extracted. [sent-246, score-0.533]
</p><p>87 At length 10, the average number is less than 80, and the fraction of edges not suppressed is better than 1/10K (and improves as sentence length increases). [sent-247, score-0.189]
</p><p>88 To explain this effectiveness, we suggest that the combined parsing phase is really only ﬁguring out how to reconcile the two models’ preferences. [sent-248, score-0.348]
</p><p>89 11 The A* estimates were so effective that even with our object-heavy Java implementation of the combined parser, total parse time was dominated by the initial, array-based PCFG phase (see ﬁgure 6b). [sent-249, score-0.198]
</p><p>90 12 9 For example, the dependency distance function of [2] registers punctuation and verb counts, and both smooth the PCFG production probabilities, which could improve the PCFG grammar. [sent-250, score-0.314]
</p><p>91 As they note, their pruning metric seems to mimic Goodman’s maximum-constituents parsing [18], which maximizes the expected number of correct nodes rather than the likelihood of the entire parse. [sent-255, score-0.344]
</p><p>92 In any case, we see it as valuable to have an exact parser with which these types of questions can be investigated at all for lexicalized parsing. [sent-256, score-0.596]
</p><p>93 11 Note that the uniform-cost parser does enough work to exploit the shared structure of the dynamic program, and therefore edge counts appear to grow polynomially. [sent-257, score-0.435]
</p><p>94 However, the A* parser does so little work that there is minimal structure-sharing. [sent-258, score-0.324]
</p><p>95 12 The average time to parse a sentence with the best model on a 750MHz Pentium III with 2GB RAM was: for 20 words, PCFG 13 sec, dependencies 0. [sent-261, score-0.198]
</p><p>96 5 Conclusion The framework of factored models over lexicalized trees has several advantages. [sent-265, score-0.318]
</p><p>97 The concrete model presented performs comparably to other, more complex, non-exact models proposed, and can be easily extended in the ways that other parser models have been. [sent-267, score-0.374]
</p><p>98 Most importantly, it admits a novel A* parsing approach which allows fast, exact inference of the most probable parse. [sent-268, score-0.358]
</p><p>99 Efﬁcient parsing for bilexical context-free grammars and head-automaton grammars. [sent-314, score-0.36]
</p><p>100 A new statistical parser based on bigram lexical dependencies. [sent-363, score-0.432]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pcfg', 0.623), ('parser', 0.324), ('parsing', 0.286), ('dependency', 0.251), ('lexicalized', 0.224), ('parsers', 0.162), ('dir', 0.125), ('head', 0.123), ('parse', 0.116), ('dep', 0.108), ('lexical', 0.108), ('te', 0.091), ('np', 0.087), ('wd', 0.087), ('edges', 0.083), ('score', 0.08), ('outside', 0.075), ('agenda', 0.075), ('dependents', 0.075), ('td', 0.073), ('syntactic', 0.073), ('scores', 0.07), ('phrase', 0.07), ('parses', 0.069), ('factored', 0.069), ('edge', 0.067), ('treebank', 0.065), ('wh', 0.061), ('acl', 0.059), ('tag', 0.059), ('pruning', 0.058), ('sentence', 0.058), ('structures', 0.054), ('th', 0.053), ('gure', 0.052), ('tree', 0.05), ('payrolls', 0.05), ('standardly', 0.05), ('exact', 0.048), ('vp', 0.047), ('schema', 0.046), ('priority', 0.043), ('klein', 0.04), ('bilexical', 0.037), ('headed', 0.037), ('nonlexical', 0.037), ('grammars', 0.037), ('compatible', 0.035), ('combined', 0.035), ('sec', 0.035), ('word', 0.033), ('verb', 0.033), ('factory', 0.033), ('language', 0.032), ('combination', 0.031), ('semantic', 0.031), ('everything', 0.03), ('grammar', 0.03), ('acc', 0.03), ('punctuation', 0.03), ('words', 0.029), ('phase', 0.027), ('top', 0.027), ('september', 0.026), ('cubic', 0.026), ('accuracy', 0.025), ('annotation', 0.025), ('attachment', 0.025), ('eisner', 0.025), ('fragmented', 0.025), ('gcnu', 0.025), ('phrasal', 0.025), ('fell', 0.025), ('models', 0.025), ('inference', 0.024), ('length', 0.024), ('dependencies', 0.024), ('factorization', 0.023), ('counts', 0.023), ('stanford', 0.023), ('effectiveness', 0.022), ('optimality', 0.022), ('symbol', 0.022), ('manning', 0.022), ('penn', 0.022), ('charniak', 0.022), ('oot', 0.022), ('val', 0.022), ('structure', 0.021), ('rules', 0.021), ('dependent', 0.021), ('mass', 0.021), ('core', 0.021), ('effective', 0.02), ('sentences', 0.02), ('goodman', 0.02), ('heads', 0.02), ('constituents', 0.02), ('mle', 0.02), ('inside', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="84-tfidf-1" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>2 0.093061149 <a title="84-tfidf-2" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We focus on the problem of efﬁcient learning of dependency trees. It is well-known that given the pairwise mutual information coefﬁcients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construction of the correlation matrix that dominates the running time. We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights. The partial knowledge we maintain is a probabilistic conﬁdence interval on the coefﬁcients, which we derive by examining just a small sample of the data. The algorithm is able to ﬂag the need to shrink an interval, which translates to inspection of more data for the particular attribute pair. Experimental results show running time that is near-constant in the number of records, without signiﬁcant loss in accuracy of the generated trees. Interestingly, our spanning-tree algorithm is based solely on Tarjan’s red-edge rule, which is generally considered a guaranteed recipe for bad performance. 1</p><p>3 0.067070708 <a title="84-tfidf-3" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>Author: Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynarrtic programming with quadratic time complexity. Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified, regardless of the number of support vectors. This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner.</p><p>4 0.06561061 <a title="84-tfidf-4" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>Author: Zach Solan, Eytan Ruppin, David Horn, Shimon Edelman</p><p>Abstract: The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity. 1 Motivation Models dealing with the acquisition of syntactic knowledge are sharply divided into two classes, depending on whether they subscribe to some variant of the classical generative theory of syntax, or operate within the framework of “general-purpose” statistical or distributional learning. An example of the former is the model of [2], which attempts to learn syntactic structures such as Functional Category, as stipulated by the Government and Binding theory. An example of the latter model is Elman’s widely used Simple Recursive Network (SRN) [3]. We believe that polarization between statistical and classical (generative, rule-based) approaches to syntax is counterproductive, because it hampers the integration of the stronger aspects of each method into a common powerful framework. Indeed, on the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation, probabilistic learning, and “connectionist” modeling. Yet, generic connectionist architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difﬁcult to train. We present a scheme that acquires “raw” syntactic information construed in a distributional sense, yet also supports the distillation of rule-like regularities out of the accrued statistical knowledge. Our research is motivated by linguistic theories that postulate syntactic structures (and transformations) rooted in distributional data, as exempliﬁed by the work of Zellig Harris [1]. 2 The ADIOS model The ADIOS (Automatic DIstillation Of Structure) model constructs syntactic representations of a sample of language from unlabeled corpus data. The model consists of two elements: (1) a Representational Data Structure (RDS) graph, and (2) a Pattern Acquisition (PA) algorithm that learns the RDS in an unsupervised fashion. The PA algorithm aims to detect patterns — repetitive sequences of “signiﬁcant” strings of primitives occurring in the corpus (Figure 1). In that, it is related to prior work on alignment-based learning [4] and regular expression (“local grammar”) extraction [5] from corpora. We stress, however, that our algorithm requires no pre-judging either of the scope of the primitives or of their classiﬁcation, say, into syntactic categories: all the information needed for its operation is extracted from the corpus in an unsupervised fashion. In the initial phase of the PA algorithm the text is segmented down to the smallest possible morphological constituents (e.g., ed is split off both walked and bed; the algorithm later discovers that bed should be left whole, on statistical grounds).1 This initial set of unique constituents is the vertex set of the newly formed RDS (multi-)graph. A directed edge is inserted between two vertices whenever the corresponding transition exists in the corpus (Figure 2(a)); the edge is labeled by the sentence number and by its within-sentence index. Thus, corpus sentences initially correspond to paths in the graph, a path being a sequence of edges that share the same sentence number. (a) mh mi mk mj (b) ci{j,k}l ml mn mi ck ... cj ml cu cv . Figure 1: (a) Two sequences mi , mj , ml and mi , mk , ml form a pattern ci{j,k}l = mi , {mj , mk }, ml , which allows mj and mk to be attributed to the same equivalence class, following the principle of complementary distributions [1]. Both the length of the shared context and the cohesiveness of the equivalence class need to be taken into account in estimating the goodness of the candidate pattern (see eq. 1). (b) Patterns can serve as constituents in their own right; recursively abstracting patterns from a corpus allows us to capture the syntactic regularities concisely, yet expressively. Abstraction also supports generalization: in this schematic illustration, two new paths (dashed lines) emerge from the formation of equivalence classes associated with cu and cv . In the second phase, the PA algorithm repeatedly scans the RDS graph for Signiﬁcant P atterns (sequences of constituents) ( SP), which are then used to modify the graph (Algorithm 1). For each path pi , the algorithm constructs a list of candidate constituents, ci1 , . . . , cik . Each of these consists of a “preﬁx” (sequence of graph edges), an equivalence class of vertices, and a “sufﬁx” (another sequence of edges; cf. Figure 2(b)). The criterion I for judging pattern signiﬁcance combines a syntagmatic consideration (the pattern must be long enough) with a paradigmatic one (its constituents c1 , . . . , ck must have high mutual information): I (c1 , c2 , . . . , ck ) = 2 e−(L/k) P (c1 , c2 , . . . , ck ) log P (c1 , c2 , . . . , ck ) Πk P (cj ) j=1 (1) where L is the typical context length and k is the length of the candidate pattern; the probabilities associated with a cj are estimated from frequencies that are immediately available 1 We remark that the algorithm can work in any language, with any set of tokens, including individual characters – or phonemes, if applied to speech. Algorithm 1 PA (pattern acquisition), phase 2 1: while patterns exist do 2: for all path ∈ graph do {path=sentence; graph=corpus} 3: for all source node ∈ path do 4: for all sink node ∈ path do {source and sink can be equivalence classes} 5: degree of separation = path index(sink) − path index(source); 6: pattern table ⇐ detect patterns(source, sink, degree of separation, equivalence table); 7: end for 8: end for 9: winner ⇐ get most signiﬁcant pattern(pattern table); 10: equivalence table ⇐ detect equivalences(graph, winner); 11: graph ⇐ rewire graph(graph, winner); 12: end for 13: end while in the graph (e.g., the out-degree of a node is related to the marginal probability of the corresponding cj ). Equation 1 balances two opposing “forces” in pattern formation: (1) the length of the pattern, and (2) the number and the cohesiveness of the set of examples that support it. On the one hand, shorter patterns are likely to be supported by more examples; on the other hand, they are also more likely to lead to over-generalization, because shorter patterns mean less context. A pattern tagged as signiﬁcant is added as a new vertex to the RDS graph, replacing the constituents and edges it subsumes (Figure 2). Note that only those edges of the multigraph that belong to the detected pattern are rewired; edges that belong to sequences not subsumed by the pattern are untouched. This highly context-sensitive approach to pattern abstraction, which is unique to our model, allows ADIOS to achieve a high degree of representational parsimony without sacriﬁcing generalization power. During the pass over the corpus the list of equivalence sets is updated continuously; the identiﬁcation of new signiﬁcant patterns is done using thecurrent equivalence sets (Figure 3(d)). Thus, as the algorithm processes more and more text, it “bootstraps” itself and enriches the RDS graph structure with new SPs and their accompanying equivalence sets. The recursive nature of this process enables the algorithm to form more and more complex patterns, in a hierarchical manner. The relationships among these can be visualized recursively in a tree format, with tree depth corresponding to the level of recursion (e.g., Figure 3(c)). The PA algorithm halts if it processes a given amount of text without ﬁnding a new SP or equivalence set (in real-life language acquisition this process may never stop). Generalization. A collection of patterns distilled from a corpus can be seen as an empirical grammar of sorts; cf. [6], p.63: “the grammar of a language is simply an inventory of linguistic units.” The patterns can eventually become highly abstract, thus endowing the model with an ability to generalize to unseen inputs. Generalization is possible, for example, when two equivalence classes are placed next to each other in a pattern, creating new paths among the members of the equivalence classes (dashed lines in Figure 1(b)). Generalization can also ensue from partial activation of existing patterns by novel inputs. This function is supported by the input module, designed to process a novel sentence by forming its distributed representation in terms of activities of existing patterns (Figure 6). These are computed by propagating activation from bottom (the terminals) to top (the patterns) of the RDS. The initial activities wj of the terminals cj are calculated given the novel input s1 , . . . , sk as follows: wj = max {I(sk , cj )} m=1..k (2) 102: do you see the cat? 101: the cat is eating 103: are you sure? Sentence Number Within-Sentence Index 101_1 101_4 101_3 101_2 101_5 101_6 END her ing show eat play is cat Pam the BEGIN (a) 131_3 131_2 109_7 END ing 121_12 stay 121_10 play 121_8 101_6 109_6 cat the BEGIN 109_5 121_9 eat 109_4 (b) 109_9 101_5 109_8 101_4 101_3 101_2 is 131_1 101_1 121_13 121_11 131_1 131_3 101_1 109_4 PATTERN 230: the cat is {eat, play, stay} -ing 165_1 Equivalence Class 230: stay, eat, play 165_2 221_3 here stay play 171_3 165_3 eat 221_1 we 171_2 they BEGIN (d) END 101_2 109_5 121_9 121_8 171_1 ing stay 131_2 play eat is cat the BEGIN (c) PATTERN 231: BEGIN {they, we} {230} here 221_2 Figure 2: (a) A small portion of the RDS graph for a simple corpus, with sentence #101 (the cat is eat -ing) indicated by solid arcs. (b) This sentence joins a pattern the cat is {eat, play, stay} -ing, in which two others (#109,121) already participate. (c) The abstracted pattern, and the equivalence class associated with it (edges that belong to sequences not subsumed by this pattern, e.g., #131, are untouched). (d) The identiﬁcation of new signiﬁcant patterns is done using the acquired equivalence classes (e.g., #230). In this manner, the system “bootstraps” itself, recursively distilling more and more complex patterns. where I(sk , cj ) is the mutual information between sk and cj . For an equivalence class, the value propagated upwards is the strongest non-zero activation of its members; for a pattern, it is the average weight of the children nodes, on the condition that all the children were activated by adjacent inputs. Activity propagation continues until it reaches the top nodes of the pattern lattice. When the algorithm encounters a novel word, all the members of the terminal equivalence class contribute a value of , which is then propagated upwards as usual. This enables the model to make an educated guess as to the meaning of the unfamiliar word, by considering the patterns that become active (Figure 6(b)). 3 Results We now brieﬂy describe the results of several studies designed to evaluate the viability of the ADIOS model, in which it was exposed to corpora of varying size and complexity. (a) propnoun:</p><p>5 0.056065217 <a title="84-tfidf-5" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>Author: Willem H. Zuidema</p><p>Abstract: Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spontaneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed - somewhat unfortunately -</p><p>6 0.054327819 <a title="84-tfidf-6" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>7 0.049412623 <a title="84-tfidf-7" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>8 0.047769897 <a title="84-tfidf-8" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>9 0.047340177 <a title="84-tfidf-9" href="./nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">61 nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>10 0.046377458 <a title="84-tfidf-10" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>11 0.04309519 <a title="84-tfidf-11" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>12 0.039289847 <a title="84-tfidf-12" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>13 0.039227277 <a title="84-tfidf-13" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>14 0.038262554 <a title="84-tfidf-14" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>15 0.037151482 <a title="84-tfidf-15" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>16 0.036942203 <a title="84-tfidf-16" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>17 0.03474221 <a title="84-tfidf-17" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>18 0.034600124 <a title="84-tfidf-18" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>19 0.034265611 <a title="84-tfidf-19" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>20 0.033966705 <a title="84-tfidf-20" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.104), (1, -0.022), (2, -0.037), (3, 0.004), (4, -0.065), (5, 0.03), (6, -0.017), (7, -0.044), (8, -0.014), (9, -0.061), (10, -0.009), (11, -0.012), (12, -0.022), (13, 0.051), (14, -0.053), (15, -0.041), (16, 0.077), (17, 0.086), (18, 0.003), (19, -0.157), (20, -0.035), (21, 0.118), (22, -0.091), (23, 0.019), (24, 0.013), (25, -0.06), (26, -0.003), (27, -0.106), (28, 0.084), (29, -0.105), (30, 0.043), (31, -0.02), (32, 0.028), (33, 0.016), (34, -0.0), (35, 0.058), (36, 0.045), (37, 0.058), (38, -0.084), (39, -0.009), (40, -0.017), (41, -0.034), (42, 0.002), (43, 0.01), (44, -0.097), (45, 0.044), (46, 0.094), (47, 0.04), (48, -0.017), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93615538 <a title="84-lsi-1" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>2 0.68668127 <a title="84-lsi-2" href="./nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We focus on the problem of efﬁcient learning of dependency trees. It is well-known that given the pairwise mutual information coefﬁcients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construction of the correlation matrix that dominates the running time. We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights. The partial knowledge we maintain is a probabilistic conﬁdence interval on the coefﬁcients, which we derive by examining just a small sample of the data. The algorithm is able to ﬂag the need to shrink an interval, which translates to inspection of more data for the particular attribute pair. Experimental results show running time that is near-constant in the number of records, without signiﬁcant loss in accuracy of the generated trees. Interestingly, our spanning-tree algorithm is based solely on Tarjan’s red-edge rule, which is generally considered a guaranteed recipe for bad performance. 1</p><p>3 0.633735 <a title="84-lsi-3" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>Author: Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynarrtic programming with quadratic time complexity. Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified, regardless of the number of support vectors. This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner.</p><p>4 0.63164854 <a title="84-lsi-4" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>Author: Zach Solan, Eytan Ruppin, David Horn, Shimon Edelman</p><p>Abstract: The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity. 1 Motivation Models dealing with the acquisition of syntactic knowledge are sharply divided into two classes, depending on whether they subscribe to some variant of the classical generative theory of syntax, or operate within the framework of “general-purpose” statistical or distributional learning. An example of the former is the model of [2], which attempts to learn syntactic structures such as Functional Category, as stipulated by the Government and Binding theory. An example of the latter model is Elman’s widely used Simple Recursive Network (SRN) [3]. We believe that polarization between statistical and classical (generative, rule-based) approaches to syntax is counterproductive, because it hampers the integration of the stronger aspects of each method into a common powerful framework. Indeed, on the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation, probabilistic learning, and “connectionist” modeling. Yet, generic connectionist architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difﬁcult to train. We present a scheme that acquires “raw” syntactic information construed in a distributional sense, yet also supports the distillation of rule-like regularities out of the accrued statistical knowledge. Our research is motivated by linguistic theories that postulate syntactic structures (and transformations) rooted in distributional data, as exempliﬁed by the work of Zellig Harris [1]. 2 The ADIOS model The ADIOS (Automatic DIstillation Of Structure) model constructs syntactic representations of a sample of language from unlabeled corpus data. The model consists of two elements: (1) a Representational Data Structure (RDS) graph, and (2) a Pattern Acquisition (PA) algorithm that learns the RDS in an unsupervised fashion. The PA algorithm aims to detect patterns — repetitive sequences of “signiﬁcant” strings of primitives occurring in the corpus (Figure 1). In that, it is related to prior work on alignment-based learning [4] and regular expression (“local grammar”) extraction [5] from corpora. We stress, however, that our algorithm requires no pre-judging either of the scope of the primitives or of their classiﬁcation, say, into syntactic categories: all the information needed for its operation is extracted from the corpus in an unsupervised fashion. In the initial phase of the PA algorithm the text is segmented down to the smallest possible morphological constituents (e.g., ed is split off both walked and bed; the algorithm later discovers that bed should be left whole, on statistical grounds).1 This initial set of unique constituents is the vertex set of the newly formed RDS (multi-)graph. A directed edge is inserted between two vertices whenever the corresponding transition exists in the corpus (Figure 2(a)); the edge is labeled by the sentence number and by its within-sentence index. Thus, corpus sentences initially correspond to paths in the graph, a path being a sequence of edges that share the same sentence number. (a) mh mi mk mj (b) ci{j,k}l ml mn mi ck ... cj ml cu cv . Figure 1: (a) Two sequences mi , mj , ml and mi , mk , ml form a pattern ci{j,k}l = mi , {mj , mk }, ml , which allows mj and mk to be attributed to the same equivalence class, following the principle of complementary distributions [1]. Both the length of the shared context and the cohesiveness of the equivalence class need to be taken into account in estimating the goodness of the candidate pattern (see eq. 1). (b) Patterns can serve as constituents in their own right; recursively abstracting patterns from a corpus allows us to capture the syntactic regularities concisely, yet expressively. Abstraction also supports generalization: in this schematic illustration, two new paths (dashed lines) emerge from the formation of equivalence classes associated with cu and cv . In the second phase, the PA algorithm repeatedly scans the RDS graph for Signiﬁcant P atterns (sequences of constituents) ( SP), which are then used to modify the graph (Algorithm 1). For each path pi , the algorithm constructs a list of candidate constituents, ci1 , . . . , cik . Each of these consists of a “preﬁx” (sequence of graph edges), an equivalence class of vertices, and a “sufﬁx” (another sequence of edges; cf. Figure 2(b)). The criterion I for judging pattern signiﬁcance combines a syntagmatic consideration (the pattern must be long enough) with a paradigmatic one (its constituents c1 , . . . , ck must have high mutual information): I (c1 , c2 , . . . , ck ) = 2 e−(L/k) P (c1 , c2 , . . . , ck ) log P (c1 , c2 , . . . , ck ) Πk P (cj ) j=1 (1) where L is the typical context length and k is the length of the candidate pattern; the probabilities associated with a cj are estimated from frequencies that are immediately available 1 We remark that the algorithm can work in any language, with any set of tokens, including individual characters – or phonemes, if applied to speech. Algorithm 1 PA (pattern acquisition), phase 2 1: while patterns exist do 2: for all path ∈ graph do {path=sentence; graph=corpus} 3: for all source node ∈ path do 4: for all sink node ∈ path do {source and sink can be equivalence classes} 5: degree of separation = path index(sink) − path index(source); 6: pattern table ⇐ detect patterns(source, sink, degree of separation, equivalence table); 7: end for 8: end for 9: winner ⇐ get most signiﬁcant pattern(pattern table); 10: equivalence table ⇐ detect equivalences(graph, winner); 11: graph ⇐ rewire graph(graph, winner); 12: end for 13: end while in the graph (e.g., the out-degree of a node is related to the marginal probability of the corresponding cj ). Equation 1 balances two opposing “forces” in pattern formation: (1) the length of the pattern, and (2) the number and the cohesiveness of the set of examples that support it. On the one hand, shorter patterns are likely to be supported by more examples; on the other hand, they are also more likely to lead to over-generalization, because shorter patterns mean less context. A pattern tagged as signiﬁcant is added as a new vertex to the RDS graph, replacing the constituents and edges it subsumes (Figure 2). Note that only those edges of the multigraph that belong to the detected pattern are rewired; edges that belong to sequences not subsumed by the pattern are untouched. This highly context-sensitive approach to pattern abstraction, which is unique to our model, allows ADIOS to achieve a high degree of representational parsimony without sacriﬁcing generalization power. During the pass over the corpus the list of equivalence sets is updated continuously; the identiﬁcation of new signiﬁcant patterns is done using thecurrent equivalence sets (Figure 3(d)). Thus, as the algorithm processes more and more text, it “bootstraps” itself and enriches the RDS graph structure with new SPs and their accompanying equivalence sets. The recursive nature of this process enables the algorithm to form more and more complex patterns, in a hierarchical manner. The relationships among these can be visualized recursively in a tree format, with tree depth corresponding to the level of recursion (e.g., Figure 3(c)). The PA algorithm halts if it processes a given amount of text without ﬁnding a new SP or equivalence set (in real-life language acquisition this process may never stop). Generalization. A collection of patterns distilled from a corpus can be seen as an empirical grammar of sorts; cf. [6], p.63: “the grammar of a language is simply an inventory of linguistic units.” The patterns can eventually become highly abstract, thus endowing the model with an ability to generalize to unseen inputs. Generalization is possible, for example, when two equivalence classes are placed next to each other in a pattern, creating new paths among the members of the equivalence classes (dashed lines in Figure 1(b)). Generalization can also ensue from partial activation of existing patterns by novel inputs. This function is supported by the input module, designed to process a novel sentence by forming its distributed representation in terms of activities of existing patterns (Figure 6). These are computed by propagating activation from bottom (the terminals) to top (the patterns) of the RDS. The initial activities wj of the terminals cj are calculated given the novel input s1 , . . . , sk as follows: wj = max {I(sk , cj )} m=1..k (2) 102: do you see the cat? 101: the cat is eating 103: are you sure? Sentence Number Within-Sentence Index 101_1 101_4 101_3 101_2 101_5 101_6 END her ing show eat play is cat Pam the BEGIN (a) 131_3 131_2 109_7 END ing 121_12 stay 121_10 play 121_8 101_6 109_6 cat the BEGIN 109_5 121_9 eat 109_4 (b) 109_9 101_5 109_8 101_4 101_3 101_2 is 131_1 101_1 121_13 121_11 131_1 131_3 101_1 109_4 PATTERN 230: the cat is {eat, play, stay} -ing 165_1 Equivalence Class 230: stay, eat, play 165_2 221_3 here stay play 171_3 165_3 eat 221_1 we 171_2 they BEGIN (d) END 101_2 109_5 121_9 121_8 171_1 ing stay 131_2 play eat is cat the BEGIN (c) PATTERN 231: BEGIN {they, we} {230} here 221_2 Figure 2: (a) A small portion of the RDS graph for a simple corpus, with sentence #101 (the cat is eat -ing) indicated by solid arcs. (b) This sentence joins a pattern the cat is {eat, play, stay} -ing, in which two others (#109,121) already participate. (c) The abstracted pattern, and the equivalence class associated with it (edges that belong to sequences not subsumed by this pattern, e.g., #131, are untouched). (d) The identiﬁcation of new signiﬁcant patterns is done using the acquired equivalence classes (e.g., #230). In this manner, the system “bootstraps” itself, recursively distilling more and more complex patterns. where I(sk , cj ) is the mutual information between sk and cj . For an equivalence class, the value propagated upwards is the strongest non-zero activation of its members; for a pattern, it is the average weight of the children nodes, on the condition that all the children were activated by adjacent inputs. Activity propagation continues until it reaches the top nodes of the pattern lattice. When the algorithm encounters a novel word, all the members of the terminal equivalence class contribute a value of , which is then propagated upwards as usual. This enables the model to make an educated guess as to the meaning of the unfamiliar word, by considering the patterns that become active (Figure 6(b)). 3 Results We now brieﬂy describe the results of several studies designed to evaluate the viability of the ADIOS model, in which it was exposed to corpora of varying size and complexity. (a) propnoun:</p><p>5 0.53033471 <a title="84-lsi-5" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>Author: Willem H. Zuidema</p><p>Abstract: Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spontaneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed - somewhat unfortunately -</p><p>6 0.51826501 <a title="84-lsi-6" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>7 0.47567958 <a title="84-lsi-7" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>8 0.43418956 <a title="84-lsi-8" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>9 0.35420725 <a title="84-lsi-9" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>10 0.33861434 <a title="84-lsi-10" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>11 0.30480015 <a title="84-lsi-11" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>12 0.30333909 <a title="84-lsi-12" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>13 0.28935692 <a title="84-lsi-13" href="./nips-2002-A_Hierarchical_Bayesian_Markovian_Model_for_Motifs_in_Biopolymer_Sequences.html">7 nips-2002-A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences</a></p>
<p>14 0.27894953 <a title="84-lsi-14" href="./nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">61 nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>15 0.26527104 <a title="84-lsi-15" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>16 0.26383027 <a title="84-lsi-16" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>17 0.26374847 <a title="84-lsi-17" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>18 0.26373261 <a title="84-lsi-18" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>19 0.2564047 <a title="84-lsi-19" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>20 0.2549867 <a title="84-lsi-20" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.023), (23, 0.027), (41, 0.339), (42, 0.039), (54, 0.104), (55, 0.032), (56, 0.011), (57, 0.011), (58, 0.012), (67, 0.025), (68, 0.025), (73, 0.017), (74, 0.089), (79, 0.01), (87, 0.02), (92, 0.036), (98, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91158706 <a title="84-lda-1" href="./nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">54 nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>Author: Daniel J. Navarro, Michael D. Lee</p><p>Abstract: unkown-abstract</p><p>2 0.8809011 <a title="84-lda-2" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><p>same-paper 3 0.79729211 <a title="84-lda-3" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>Author: Dan Klein, Christopher D. Manning</p><p>Abstract: We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.</p><p>4 0.77893728 <a title="84-lda-4" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>5 0.7318368 <a title="84-lda-5" href="./nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">38 nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>Author: Patrick J. Wolfe, Simon J. Godsill</p><p>Abstract: The Bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music—something which has often been overlooked in traditional audio signal processing approaches. Here, after constructing a Bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms, we apply Markov chain Monte Carlo methods in order to sample from the resultant posterior distribution of interest. We present speech enhancement results which compare favourably in objective terms with standard time-varying ﬁltering techniques (and in several cases yield superior performance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledge of the noise power.</p><p>6 0.48443735 <a title="84-lda-6" href="./nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">183 nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>7 0.47088253 <a title="84-lda-7" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>8 0.4705143 <a title="84-lda-8" href="./nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">31 nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>9 0.46143448 <a title="84-lda-9" href="./nips-2002-Monaural_Speech_Separation.html">147 nips-2002-Monaural Speech Separation</a></p>
<p>10 0.45451573 <a title="84-lda-10" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>11 0.45220268 <a title="84-lda-11" href="./nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">14 nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>12 0.4460167 <a title="84-lda-12" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>13 0.44223571 <a title="84-lda-13" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>14 0.44153821 <a title="84-lda-14" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>15 0.44137976 <a title="84-lda-15" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>16 0.44129461 <a title="84-lda-16" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>17 0.44063801 <a title="84-lda-17" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>18 0.44046471 <a title="84-lda-18" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>19 0.44038326 <a title="84-lda-19" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>20 0.44023159 <a title="84-lda-20" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
