<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-51" href="#">nips2002-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</h1>
<br/><p>Source: <a title="nips-2002-51-pdf" href="http://papers.nips.cc/paper/2311-classifying-patterns-of-visual-motion-a-neuromorphic-approach.pdf">pdf</a></p><p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>Reference: <a title="nips-2002-51-reference" href="../nips2002_reference/nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch ¡  ¢  Abstract We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. [sent-5, score-0.779]
</p><p>2 The complete system is described by the dynamics of its physical network architectures. [sent-6, score-0.256]
</p><p>3 The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. [sent-7, score-1.204]
</p><p>4 Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. [sent-8, score-0.731]
</p><p>5 And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. [sent-9, score-0.52]
</p><p>6 We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. [sent-10, score-0.271]
</p><p>7 1 Introduction The classiﬁcation of continuous temporal patterns is possible using Hopﬁeld networks with asymmetric weights [2], but classiﬁcation is restricted to periodic trajectories with a wellknown start and end point. [sent-12, score-0.252]
</p><p>8 We simplify the task by ﬁrst mapping the continuous visual motion patterns to sequences of motion events. [sent-15, score-1.305]
</p><p>9 A motion event is characterized by the occurrence of visual motion in one out of a pre-deﬁned set of directions. [sent-16, score-1.19]
</p><p>10 Known approaches for sequence classiﬁcation can be divided into two major categories: The ﬁrst group typically applies standard Hopﬁeld networks with time-dependent weight matrices [4, 5]. [sent-17, score-0.152]
</p><p>11 These networks are relatively inefﬁcient in storage capacity, using many units per stored pattern. [sent-18, score-0.145]
</p><p>12 The second approach relies on time-delay elements and some form of coincidence detectors that respond dominantly to the correctly time-shifted events of a known sequence [6, 7]. [sent-19, score-0.309]
</p><p>13 The sequence classiﬁcation network of our proposed system is based on the work of Tank and Hopﬁeld [6], but extended to be time-continuous and to show increased robustness. [sent-25, score-0.261]
</p><p>14 Finally, we modify the network architecture to allow the system to learn arbitrary sequences of a particular length. [sent-26, score-0.318]
</p><p>15 2 System architecture  N  mx  N  my W  0  E  S  W  A  τ1 τ2 τ3 τ1 τ2 τ3 τ1 τ2 τ3 τ1 τ2 τ3  B  E  NWE  C  S  time  Optical flow chip  Direction selective network  Sequence classification network  System output  Figure 1: The complete classiﬁcation system. [sent-27, score-0.716]
</p><p>16 The input to the system is a real-world moving visual stimulus and the output is the activity of units representing particular trajectory classes. [sent-28, score-0.642]
</p><p>17 The system contains three major stages of processing as shown in Figure 1: the optical ﬂow chip estimates global visual motion, the direction selective network (DSN) maps the estimate to motion events and the sequence classiﬁcation network (SCN) ﬁnally classiﬁes the sequences of these events. [sent-29, score-1.911]
</p><p>18 The architecture reﬂects the separation of the task into the classiﬁcation in motion space (DSN) and, consecutively, the classiﬁcation in time (SCN). [sent-30, score-0.576]
</p><p>19 1 The optical ﬂow chip The front-end of the classiﬁcation system consists of the optical ﬂow chip [1, 8], that estimates 2D visual motion. [sent-34, score-1.005]
</p><p>20 Due to adaptive circuitry, the estimate of visual motion is fairly independent of illumination conditions. [sent-35, score-0.641]
</p><p>21 The estimation of visual motion requires the integration of visual information within the image space in order to solve for inherent visual ambiguities. [sent-36, score-0.969]
</p><p>22 For the purpose of the here presented classiﬁcation system, the integration of visual information is set to take place over the complete image space. [sent-37, score-0.222]
</p><p>23 Thus, the resulting estimate represents the global visual motion perceived. [sent-38, score-0.664]
</p><p>24 The output signals of the chip are and that represent at any instant the two components of the two analog voltages actual global motion vector. [sent-39, score-0.879]
</p><p>25 The output signals are linear to the perceived motion within a range of volts. [sent-40, score-0.592]
</p><p>26 The continuous-time voltage trajectory is the input to the direction selective network. [sent-42, score-0.319]
</p><p>27 2 The direction selective network (DSN) The second stage transforms the trajectory into a sequence of motion events, where an event means that the motion vector points into a particular region of motion space. [sent-45, score-1.991]
</p><p>28 Each direction selective unit (DSU) receives highest input when is within   #'       the corresponding region. [sent-47, score-0.225]
</p><p>29 In the following we choose four motion directions referred to as north (N), east (E), south (S) and west (W) and a central region for zero motion. [sent-48, score-0.54]
</p><p>30 98     ¡ ¢§   © ¨   ¦¤¢  ¥£ ¡ &    $ %# §  © ¨ ¡ ¢§  where and are the excitatory and inhibitory weights between the DSU [8]. [sent-53, score-0.161]
</p><p>31 Following gradient descent, the dynamics of the units are described by (2)  r  r  &  $ r    r  7 x 7  r 4 r 4 r xy& e r wut avUr r ¥ p      sq x y& e 4 x y& e   97 h b   i 12&  Yfg e @  © X @ &   aTX T ` § & ¡ ¨ 1  ¢§ ! [sent-55, score-0.148]
</p><p>32 The input to the DSU is if if  c $ db  where motion unit is  is the motion estimate in polar coordinates. [sent-58, score-1.094]
</p><p>33 In Figure 2b we compare the outputs of a DSU to thresh      activity  b a  mx  N  my E  0  c  S  1  0  mo 0. [sent-60, score-0.168]
</p><p>34 3 tion E-W s] mo  Figure 2: The direction selective network. [sent-68, score-0.223]
</p><p>35 Dotted lines show the regions in motion space where the different units win. [sent-71, score-0.63]
</p><p>36 b) The response of the N-DSU to constant input is shown as surface plot, while the responses of the same unit to dynamic motion trajectories (circles and straight lines) are plotted as lines. [sent-72, score-0.724]
</p><p>37 c) The output of the zero motion unit to constant input. [sent-74, score-0.606]
</p><p>38 3 The sequence classiﬁcation network (SCN) The classiﬁcation of the temporal structure of the DSN output is the task of the SCN. [sent-78, score-0.283]
</p><p>39 In equivalence with the regions in motion space these time-delays form ’regions’ in time. [sent-80, score-0.526]
</p><p>40 The number of units (SCU) of the SCN is equal to the number of trajectory classes the system is able to classify. [sent-82, score-0.312]
</p><p>41 We use time-delays, where is the number of events of the longest sequence to be classiﬁed. [sent-83, score-0.286]
</p><p>42 The time interval delay between two maxima of the time-delay functions is the characteristic time-scale of the sequence classiﬁcation. [sent-84, score-0.349]
</p><p>43 are the weights of the connections between the DSN and the SCN and is the delayed output of the DSU. [sent-91, score-0.199]
</p><p>44 For example, if the sequence NW-E has to be classiﬁed, the inputs from the E-DSU delayed by delay , from the W-DSU by delay and from the N-DSU by delay are excitatory, while all the others are inhibitory. [sent-96, score-0.803]
</p><p>45 All excitatory as well as all inhibitory weights are equal with excitation being twice as strong as inhibition. [sent-97, score-0.161]
</p><p>46 It prevents the ﬁrst motion event from overruling the rest of the sequence and is crucial for the exact classiﬁcation of short sequences. [sent-99, score-0.636]
</p><p>47 ¡  ¡  ¡ )(  a  N  E  S  W  b  WTA  N  3xTdelay  N W W Tdelay E E 2xTdelay  τ1 τ2 τ3 τ1 τ2 τ3 τ1 τ2 τ3 τ1 τ2 τ3  delayed motion events  motion simultaneous events input  NWE  time  Figure 3: The sequence classiﬁcation network. [sent-100, score-1.664]
</p><p>48 The time-delays between the DSU and the SRU are numbered in units of delay . [sent-102, score-0.323]
</p><p>49 b) A sequence is classiﬁed by delaying consecutive motion events such that they provide a simultaneous excitatory input. [sent-106, score-0.876]
</p><p>50 ¡  GFD B R R P 81 3 W G F D S P 81 `YH X0 4VUTA Q3 A IH3 8ECA delay  delay  3A  delay  , where  ie fe a phg dfdcbB H  97 3 1 @865420  1  3 Performance of the system We measure the performance of the system in two different ways. [sent-107, score-0.803]
</p><p>51 Knowing the response properties of the optical ﬂow chip [8] we simulate its output to analyze systematically the two other stages of the system. [sent-109, score-0.548]
</p><p>52 Secondly, we test the complete system including the optical ﬂow chip under real conditions. [sent-110, score-0.505]
</p><p>53 1 Robustness to time warping      T £     We simulate the visual motion trajectories as a sum of Gaussians in time, thus where . [sent-113, score-0.815]
</p><p>54 Time is always measured in units of the characteristic time-delay delay . [sent-118, score-0.323]
</p><p>55  #'     x  ¡ ¤  ¡  © § G ¨¦    1 $  % $ 1   ¥ ¤ $ ¥ ¤   T      ¡  ¡ ¦ x ¡ © ¦( h    c P  x  ¥  c 4 &  £ ¡ ¤¢      For ﬁxed can be decreased down to delay , delay for sequences of length two and down to delay for longer sequences. [sent-119, score-0.765]
</p><p>56 Fixing delay , classiﬁcation is still according to Figure 4a; e. [sent-120, score-0.219]
</p><p>57 for a sequence of length three and guaranteed for varying input strength volts, can maximally increase by . [sent-122, score-0.169]
</p><p>58 For three and four events (gray and white bars in Figure 4). [sent-123, score-0.265]
</p><p>59 +150%  time warp  time warp    ¡  h 1 ¡  ¡  +150%  x  a  +100% +50% 0%  no class. [sent-127, score-0.194]
</p><p>60 The results are shown for three different trajectory lengths (black: two motion events, gray: three events, white: four events) and three different input strengths (maximal output is changed. [sent-136, score-0.871]
</p><p>61 No is stretched linearly and therefore the duration of the events is proportional to classiﬁcation is possible for sequences of length four at very low input levels. [sent-139, score-0.409]
</p><p>62 ¡    ¡ ¤  ¡h1  x  The system cannot distinguish between the sequences e. [sent-140, score-0.181]
</p><p>63 In this case, the sum of the weighted integrals of the delay functions of both sequences leads to an equivalent input to the SCN. [sent-143, score-0.386]
</p><p>64 However, if two adjacent events are not allowed to be the same, this problem does not occur. [sent-144, score-0.199]
</p><p>65 For a sequence with ﬁve events and more, the time shift becomes larger than delay for some of the events, which leads to inhibition instead of excitation. [sent-146, score-0.548]
</p><p>66 2 Real world application - writing letters with patterns of hand movements The complete system was applied to classify visual motion patterns elicited by hand movements in front of the optical ﬂow chip. [sent-149, score-1.127]
</p><p>67 Using sequences of three events we are able to classify 36 valid sequences and therefore encode the alphabet. [sent-150, score-0.466]
</p><p>68 Figure 5 shows a typical visual motion pattern (assigned to the letter ’H’) and the corresponding signals at all stages of processing. [sent-151, score-0.704]
</p><p>69 5  0  1  2  3  time [Tdelay ]  4  3  4  5  1  SCU activity  DSU activity  1  0  2  time [Tdelay ]  0. [sent-159, score-0.204]
</p><p>70 a) The output of the optical ﬂow chip to a moving hand in a N-S vs. [sent-161, score-0.462]
</p><p>71 The marks on the trajectory show different time stamps. [sent-163, score-0.178]
</p><p>72 b) The same trajectory including the time stamps in a motion vs. [sent-164, score-0.675]
</p><p>73 c) The output of the DSN showing classiﬁcation in motion space. [sent-167, score-0.565]
</p><p>74 Here, the unit that recognizes the trajectory class ’H’ is shown by the solid line. [sent-170, score-0.238]
</p><p>75 The signal of the optical ﬂow chip is read into the computer using an AD-card. [sent-175, score-0.394]
</p><p>76 4 Learning motion trajectories We expanded the system to be able to learn visual motion patterns. [sent-177, score-1.315]
</p><p>77 We model each set of four synapses connecting the four DSU to a single SCU with the same time-delay by a competitive network of four synapse units (see Figure 6) with very slow time constants. [sent-178, score-0.555]
</p><p>78 We impose on the output of the four units that their sum equals . [sent-179, score-0.215]
</p><p>79 5 0  c  5  0  10  5  15  20  10  15  20  wexc  x  weights  x  x  x  0  -1  +  0  -winh  time [sec] Figure 6: Learning trajectory classes. [sent-181, score-0.221]
</p><p>80 a) Schematics of the competitive network of a set of synapses. [sent-182, score-0.14]
</p><p>81 The dashed line shows one synapse: the synaptic weight , the input to the synapse unit and its output . [sent-183, score-0.417]
</p><p>82 Multiplication by the output signal of the SCU is indicated by the “x” in the small square, the linear mapping by the bold line from the synapse output to the weight. [sent-184, score-0.303]
</p><p>83  ¥ ¥ §8 & ¢1    6        ¢ 1 4 £2&  ¥   ¡  e     FG    @ A  1    ¥    &  where the synapse units have an sigmoidal activation function are deﬁned as in (2) and (4). [sent-189, score-0.306]
</p><p>84        $ %# §   ¥    ¥  Since the activity of the synapse units is always between 0 and 1 a linear mapping to the actual synaptic weights is performed: . [sent-192, score-0.385]
</p><p>85 The input term in (6) is the product of: the input weight ( ), the delayed input to the synapse ( ) and the output of the SCU ( ) (see Figure 6a). [sent-196, score-0.444]
</p><p>86 The weight of a particular synapse is increased if both, the input to the synapse and the activity of the target SCU are high. [sent-198, score-0.374]
</p><p>87 The reduction of the other weights is due to the competitive network behavior. [sent-199, score-0.183]
</p><p>88 Under the restriction that trajectories must differ by more than one event the system is able to learn sequences of length three. [sent-201, score-0.337]
</p><p>89 Sequences that differ by only one event are learnt by the same SCU, thus subsequent sequences overwrite previous learned ones. [sent-202, score-0.16]
</p><p>90 In Figure 6b,c the learning process of one particular trajectory class of three events is shown. [sent-203, score-0.357]
</p><p>91 This trajectory is part of a set of          £ & 1  6    ¥ §    ¡ ¨ %# §  $       six trajectories that were learned during one simulation cycle, where each input trajectory was consecutively presented ﬁve times. [sent-204, score-0.475]
</p><p>92 5 Conclusions and outlook We have shown a strikingly simple3 network system that reliably classiﬁes distinct visual motion patterns. [sent-205, score-0.815]
</p><p>93 Clearly, the application of the optical ﬂow chip substantially reduces the remaining computational load and allows real-time processing. [sent-206, score-0.394]
</p><p>94 A remarkable feature of our system is that - with the exception of the visual motion frontend, but including the learning rule - all networks have competitive dynamics and are based on the classical Winner-Take-All architecture. [sent-207, score-0.863]
</p><p>95 Thus, given also the small network size, it seems very likely to allow a complete aVLSI system-on-chip integration, not considering the learning mechanism. [sent-209, score-0.139]
</p><p>96 Such a single chip system would represent a very efﬁcient computational device, requiring minimal space, weight and power. [sent-210, score-0.303]
</p><p>97 The ’quasi-discretization’ in visual motion space that emerges from the non-linear ampliﬁcation in the direction selective network could be reﬁned to include not only more directions but also different speed-levels. [sent-211, score-0.867]
</p><p>98 Computation of smooth optical ﬂow in a feedback connected analog network. [sent-222, score-0.21]
</p><p>99 Constraint optimization networks for visual motion perception - analysis and synthesis. [sent-278, score-0.682]
</p><p>100 the presented man-machine interface consists only of 31 units and 4x4 time-delays, not counting the network elements in the optical ﬂow chip. [sent-300, score-0.416]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('motion', 0.497), ('dsn', 0.241), ('delay', 0.219), ('chip', 0.206), ('events', 0.199), ('dsu', 0.193), ('scn', 0.193), ('scu', 0.192), ('optical', 0.188), ('wta', 0.173), ('volts', 0.169), ('ow', 0.162), ('visual', 0.144), ('trajectory', 0.135), ('classi', 0.133), ('synapse', 0.116), ('sequences', 0.108), ('hop', 0.107), ('trajectories', 0.104), ('units', 0.104), ('network', 0.101), ('tdelay', 0.097), ('sequence', 0.087), ('selective', 0.085), ('cation', 0.074), ('system', 0.073), ('mo', 0.071), ('excitatory', 0.069), ('output', 0.068), ('avlsi', 0.067), ('activity', 0.059), ('input', 0.059), ('delayed', 0.059), ('warp', 0.054), ('event', 0.052), ('inhibitory', 0.049), ('nwe', 0.048), ('olt', 0.048), ('stocker', 0.048), ('tio', 0.048), ('ampli', 0.046), ('activation', 0.044), ('dynamics', 0.044), ('four', 0.043), ('weights', 0.043), ('time', 0.043), ('collective', 0.042), ('sigmoidal', 0.042), ('filled', 0.042), ('consecutively', 0.042), ('networks', 0.041), ('unit', 0.041), ('synaptic', 0.041), ('direction', 0.04), ('integration', 0.04), ('competitive', 0.039), ('dashed', 0.039), ('fg', 0.038), ('alan', 0.038), ('recognizes', 0.038), ('eth', 0.038), ('mx', 0.038), ('complete', 0.038), ('patterns', 0.037), ('voltages', 0.036), ('tank', 0.036), ('stages', 0.036), ('architecture', 0.036), ('letters', 0.031), ('robustness', 0.03), ('eld', 0.03), ('firstly', 0.029), ('line', 0.029), ('regions', 0.029), ('connections', 0.029), ('nn', 0.028), ('classify', 0.028), ('electronic', 0.027), ('movements', 0.027), ('mobile', 0.027), ('temporal', 0.027), ('signals', 0.027), ('tion', 0.027), ('simulate', 0.027), ('exception', 0.025), ('simultaneous', 0.024), ('weight', 0.024), ('usa', 0.024), ('solid', 0.024), ('three', 0.023), ('maximal', 0.023), ('relies', 0.023), ('synapses', 0.023), ('response', 0.023), ('global', 0.023), ('interface', 0.023), ('mapping', 0.022), ('analog', 0.022), ('secondly', 0.022), ('outline', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="51-tfidf-1" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>2 0.19068567 <a title="51-tfidf-2" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>3 0.15503004 <a title="51-tfidf-3" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>Author: Alex Holub, Gilles Laurent, Pietro Perona</p><p>Abstract: Re-mapping patterns in order to equalize their distribution may greatly simplify both the structure and the training of classifiers. Here, the properties of one such map obtained by running a few steps of discrete-time dynamical system are explored. The system is called 'Digital Antennal Lobe' (DAL) because it is inspired by recent studies of the antennallobe, a structure in the olfactory system of the grasshopper. The pattern-spreading properties of the DAL as well as its average behavior as a function of its (few) design parameters are analyzed by extending previous results of Van Vreeswijk and Sompolinsky. Furthermore, a technique for adapting the parameters of the initial design in order to obtain opportune noise-rejection behavior is suggested. Our results are demonstrated with a number of simulations. 1</p><p>4 0.15379892 <a title="51-tfidf-4" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>5 0.15271498 <a title="51-tfidf-5" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>Author: Alexandre R. Romariz, Kelvin Wagner</p><p>Abstract: An optoelectronic implementation of a spiking neuron model based on the FitzHugh-Nagumo equations is presented. A tunable semiconductor laser source and a spectral ﬁlter provide a nonlinear mapping from driver voltage to detected signal. Linear electronic feedback completes the implementation, which allows either electronic or optical input signals. Experimental results for a single system and numeric results of model interaction conﬁrm that important features of spiking neural models can be implemented through this approach.</p><p>6 0.15231745 <a title="51-tfidf-6" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>7 0.12676288 <a title="51-tfidf-7" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>8 0.12650904 <a title="51-tfidf-8" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>9 0.12212349 <a title="51-tfidf-9" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>10 0.11050989 <a title="51-tfidf-10" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>11 0.10176206 <a title="51-tfidf-11" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>12 0.10038799 <a title="51-tfidf-12" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>13 0.090656325 <a title="51-tfidf-13" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>14 0.088807933 <a title="51-tfidf-14" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>15 0.087100744 <a title="51-tfidf-15" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>16 0.085372008 <a title="51-tfidf-16" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>17 0.079483755 <a title="51-tfidf-17" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>18 0.075663336 <a title="51-tfidf-18" href="./nips-2002-Neuromorphic_Bisable_VLSI_Synapses_with_Spike-Timing-Dependent_Plasticity.html">154 nips-2002-Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity</a></p>
<p>19 0.075534061 <a title="51-tfidf-19" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>20 0.074424066 <a title="51-tfidf-20" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.208), (1, 0.137), (2, -0.005), (3, 0.003), (4, 0.135), (5, 0.144), (6, 0.113), (7, -0.036), (8, 0.169), (9, 0.122), (10, -0.089), (11, 0.153), (12, 0.008), (13, -0.004), (14, -0.117), (15, -0.018), (16, 0.123), (17, 0.046), (18, -0.051), (19, -0.087), (20, 0.175), (21, 0.155), (22, 0.14), (23, 0.015), (24, 0.042), (25, 0.115), (26, -0.123), (27, 0.155), (28, -0.009), (29, 0.135), (30, 0.112), (31, -0.106), (32, -0.001), (33, -0.064), (34, -0.03), (35, -0.005), (36, -0.122), (37, 0.091), (38, -0.126), (39, -0.13), (40, -0.034), (41, 0.048), (42, 0.015), (43, -0.113), (44, -0.022), (45, -0.013), (46, -0.113), (47, 0.005), (48, -0.053), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96942508 <a title="51-lsi-1" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>2 0.67976958 <a title="51-lsi-2" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>Author: Leonid Taycher, John Iii, Trevor Darrell</p><p>Abstract: Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We propose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on ﬁnding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our algorithm on both synthetic and real motion capture data.</p><p>3 0.59024423 <a title="51-lsi-3" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>Author: Alex Holub, Gilles Laurent, Pietro Perona</p><p>Abstract: Re-mapping patterns in order to equalize their distribution may greatly simplify both the structure and the training of classifiers. Here, the properties of one such map obtained by running a few steps of discrete-time dynamical system are explored. The system is called 'Digital Antennal Lobe' (DAL) because it is inspired by recent studies of the antennallobe, a structure in the olfactory system of the grasshopper. The pattern-spreading properties of the DAL as well as its average behavior as a function of its (few) design parameters are analyzed by extending previous results of Van Vreeswijk and Sompolinsky. Furthermore, a technique for adapting the parameters of the initial design in order to obtain opportune noise-rejection behavior is suggested. Our results are demonstrated with a number of simulations. 1</p><p>4 0.58827609 <a title="51-lsi-4" href="./nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">136 nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>Author: Matthias O. Franz, Javaan S. Chahl</p><p>Abstract: The tangential neurons in the ﬂy brain are sensitive to the typical optic ﬂow patterns generated during self-motion. In this study, we examine whether a simpliﬁed linear model of these neurons can be used to estimate self-motion from the optic ﬂow. We present a theory for the construction of an estimator consisting of a linear combination of optic ﬂow vectors that incorporates prior knowledge both about the distance distribution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirectional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas translation estimates turn out to be less reliable. 1</p><p>5 0.57167357 <a title="51-lsi-5" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>Author: Robert A. Jacobs, Melissa Dominguez</p><p>Abstract: We consider the hypothesis that systems learning aspects of visual perception may beneﬁt from the use of suitably designed developmental progressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were “developmental models” in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale developmental progression (i.e. it received coarse-scale motion features early in training and ﬁner-scale features were added to its input as training progressed), another model used a ﬁne-to-multiscale progression, and the third model used a random progression. The ﬁnal model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model’s superior performance. We conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.</p><p>6 0.57113963 <a title="51-lsi-6" href="./nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">160 nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>7 0.51737976 <a title="51-lsi-7" href="./nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">22 nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>8 0.45062646 <a title="51-lsi-8" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>9 0.39740157 <a title="51-lsi-9" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>10 0.38076097 <a title="51-lsi-10" href="./nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">128 nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>11 0.37783149 <a title="51-lsi-11" href="./nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>12 0.36501908 <a title="51-lsi-12" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>13 0.34026599 <a title="51-lsi-13" href="./nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">186 nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>14 0.33761996 <a title="51-lsi-14" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>15 0.33035177 <a title="51-lsi-15" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>16 0.32460532 <a title="51-lsi-16" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>17 0.29786304 <a title="51-lsi-17" href="./nips-2002-Neuromorphic_Bisable_VLSI_Synapses_with_Spike-Timing-Dependent_Plasticity.html">154 nips-2002-Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity</a></p>
<p>18 0.29321688 <a title="51-lsi-18" href="./nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">153 nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>19 0.29277843 <a title="51-lsi-19" href="./nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">59 nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>20 0.28812909 <a title="51-lsi-20" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.309), (11, 0.015), (23, 0.04), (42, 0.047), (54, 0.072), (55, 0.042), (57, 0.015), (64, 0.023), (67, 0.018), (68, 0.108), (74, 0.069), (92, 0.027), (98, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82369983 <a title="51-lda-1" href="./nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">51 nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>Author: Jakob Heinzle, Alan Stocker</p><p>Abstract: We report a system that classiﬁes and can learn to classify patterns of visual motion on-line. The complete system is described by the dynamics of its physical network architectures. The combination of the following properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical ﬂow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classiﬁcation task is signiﬁcantly reduced by mapping the continuous motion trajectories to sequences of ’motion events’. And thirdly, all the network structures are simple and with the exception of the optical ﬂow chip based on a Winner-Take-All (WTA) architecture. We demonstrate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Regarding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.</p><p>2 0.71927416 <a title="51-lda-2" href="./nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">87 nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>Author: Anitha Kannan, Nebojsa Jojic, Brendan J. Frey</p><p>Abstract: Dimensionality reduction techniques such as principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data samples and points in a lower dimensional subspace. In [6], Jojic and Frey introduced mixture of transformation-invariant component analyzers (MTCA) that can account for global transformations such as translations and rotations, perform clustering and learn local appearance deformations by dimensionality reduction. However, due to enormous computational requirements of the EM algorithm for learning the model, O( ) where is the dimensionality of a data sample, MTCA was not practical for most applications. In this paper, we demonstrate how fast Fourier transforms can reduce the computation to the order of log . With this speedup, we show the effectiveness of MTCA in various applications - tracking, video textures, clustering video sequences, object recognition, and object detection in images. ¡ ¤ ¤ ¤ ¤</p><p>3 0.57044828 <a title="51-lda-3" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><p>4 0.52885216 <a title="51-lda-4" href="./nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>Author: David Barber</p><p>Abstract: The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the beneﬁts of a tractable probabilistic model. 1</p><p>5 0.52693856 <a title="51-lda-5" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>6 0.51973534 <a title="51-lda-6" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>7 0.51622909 <a title="51-lda-7" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>8 0.50385094 <a title="51-lda-8" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>9 0.50341415 <a title="51-lda-9" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>10 0.50192118 <a title="51-lda-10" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>11 0.49690297 <a title="51-lda-11" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>12 0.49465805 <a title="51-lda-12" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>13 0.48888212 <a title="51-lda-13" href="./nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">50 nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>14 0.48797256 <a title="51-lda-14" href="./nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">199 nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>15 0.4878625 <a title="51-lda-15" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>16 0.48613024 <a title="51-lda-16" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>17 0.48435652 <a title="51-lda-17" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>18 0.48076886 <a title="51-lda-18" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>19 0.47785228 <a title="51-lda-19" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>20 0.47767049 <a title="51-lda-20" href="./nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">172 nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
