<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-28" href="#">nips2002-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</h1>
<br/><p>Source: <a title="nips-2002-28-pdf" href="http://papers.nips.cc/paper/2231-an-information-theoretic-approach-to-the-functional-classification-of-neurons.pdf">pdf</a></p><p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>Reference: <a title="nips-2002-28-reference" href="../nips2002_reference/nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract A population of neurons typically exhibits a broad diversity of responses to sensory inputs. [sent-3, score-0.546]
</p><p>2 The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. [sent-4, score-0.966]
</p><p>3 We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. [sent-5, score-0.181]
</p><p>4 Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. [sent-6, score-1.09]
</p><p>5 Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. [sent-7, score-1.253]
</p><p>6 Already in his classical work, Cajal [1] recognized that the shapes of cells can be classiﬁed, and he identiﬁed many of the cell types that we recognize today. [sent-9, score-0.82]
</p><p>7 Morphological and molecular classiﬁcation are appealing because they deal with relatively ﬁxed properties, but ultimately the functional properties of neurons are the most important, and neurons that share the same morphology or molecular markers need not embody the same function. [sent-12, score-0.45]
</p><p>8 Functional classiﬁcation of retinal ganglion cells typically has consisted of ﬁnding qualitatively different responses to simple stimuli. [sent-16, score-1.153]
</p><p>9 Classes are deﬁned by whether ganglion cells ﬁre spikes at the onset or offset of a step of light or both (ON, OFF, ON/OFF cells in frog [5]) or whether they ﬁre once or twice per cycle of a drifting grating (X, Y cells in cat [6]). [sent-17, score-2.116]
</p><p>10 In the frog, the literature reports 1 class of ON-type ganglion cell and 4 or 5 classes of OFF-type [7]. [sent-19, score-0.653]
</p><p>11 The salamander has been reported to have only 3 of these OFF-type ganglion cells [8]. [sent-20, score-1.032]
</p><p>12 In some cases, there is very close agreement between anatomical and functional classes, such as the (α,β) and (Y,X) cells in the cat. [sent-23, score-0.629]
</p><p>13 Here we show how information theory allows us to deﬁne the problem of classiﬁcation without any a priori assumptions regarding which features of visual stimulus or neural response are most signiﬁcant, and without imposing a metric on these variables. [sent-25, score-0.357]
</p><p>14 All notions of similarity emerge from the joint statistics of neurons in a population as they respond to common stimuli. [sent-26, score-0.199]
</p><p>15 To the extent that we identify the function of retinal ganglion cells as providing the brain with information about the visual world, then our approach ﬁnds exactly the classiﬁcation which captures this functionality in a maximally efﬁcient manner. [sent-27, score-1.058]
</p><p>16 Applied to experiments on the tiger salamander retina, this method identiﬁes the major types of ganglion cells in agreement with traditional methods, but on a ﬁner level we ﬁnd clear structure within a group of 19 fast OFF cells that suggests at least 5 functional subclasses. [sent-28, score-1.885]
</p><p>17 More profoundly, even cells within a subclass are very different from one another, so that on average the ganglion cell responses to the simpliﬁed visual stimuli we have used provide ∼6 bits/sec of information about cell identity within our population of 21 cells. [sent-29, score-2.157]
</p><p>18 This is sufﬁcient to identify uniquely each neuron in an “elementary patch” of the retina within one second, and a typical pair of cells can be distinguished reliably by observing an average of just two or three spikes. [sent-30, score-0.973]
</p><p>19 2  Theory  Suppose that we could give a complete characterization, for each neuron i = 1, 2, · · · , N in a population, of the probability P (r|s, i) that a stimulus s will generate the response r. [sent-31, score-0.278]
</p><p>20 For visual neurons we might assume that responses are determined by the projection of the stimulus movie s onto a single template or receptive ﬁeld fi , P (r|s, i) = F (r; fi ·s); classifying neurons then amounts to clustering the receptive ﬁelds. [sent-33, score-1.0]
</p><p>21 It seems strange that classifying the responses of visual neurons requires us to say in advance what it means for images or movies to be similar. [sent-35, score-0.426]
</p><p>22 Imagine that we present a stimulus s and record the response r from a single neuron in the population, but we don’t know which one. [sent-37, score-0.278]
</p><p>23 But we still face the problem of deﬁning similarity: even if all the receptive ﬁelds in the retina can be summarized meaningfully by the diameters of the center and surround (for example), why should we believe that Euclidean distance in this two dimensional space is a sensible metric? [sent-39, score-0.176]
</p><p>24 as the mutual information between responses and identity (conditional on the stimulus), I(r; i|s) =  1 N  N  P (r|s, i) log2 r  i=1  P (r|s, i) bits, P (r|s)  (1)  N  where P (r|s) = (1/N ) i=1 P (r|s, i). [sent-40, score-0.358]
</p><p>25 It is natural to ask this question on average in an ensemble of stimuli P (s) (ideally the natural ensemble), I(r; i|s) I(r; i|s)  s  s  =  1 N  N  [ds]P (s)P (r|s, i) log2 i=1  P (r|s, i) ; P (r|s)  (2)  is invariant under all invertible transformations of r or s. [sent-42, score-0.176]
</p><p>26 Because information is mutual, we also can think of I(r; i|s) s as the information that cellular identity provides about the responses we will record. [sent-43, score-0.349]
</p><p>27 But now it is clear what we mean by classifying the cells: If there are clear classes, then we can predict the responses to a stimulus just by knowing the class to which a neuron belongs rather than knowing its unique identity. [sent-44, score-0.533]
</p><p>28 Thus we should be able to ﬁnd a mapping i → C of cells into classes C = 1, 2, · · · , K such that I(r; C|s) s is almost as large as I(r; i|s) s , despite the fact that the number of classes K is much less than the number of cells N . [sent-45, score-1.13]
</p><p>29 Here we conﬁne ourselves to hard classiﬁcations, and use a greedy agglomerative algorithm [12] which starts with K = N and makes mergers which at every step provide the smallest reduction in I(r; C|s). [sent-50, score-0.188]
</p><p>30 The matrix of “distances” ∆Iij characterizes the similarities among neurons in pairwise fashion. [sent-52, score-0.197]
</p><p>31 Finally, if cells belong to clear classes, then we ought to be able to replace each cell by a typical or average member of the class without sacriﬁcing function. [sent-53, score-0.897]
</p><p>32 In this case function is quantiﬁed by asking how much information cells provide about the visual scene. [sent-54, score-0.623]
</p><p>33 There is a strict complementarity of the information measures: information that the stimulus/response relation provides about the identity of the cell is exactly information about the visual scene which will be lost if we don’t know the identity of the cells [14]. [sent-55, score-1.143]
</p><p>34 Our information theoretic  approach to classiﬁcation of neurons thus produces classes such that replacing cells with average class members provides the smallest loss of information about the sensory inputs. [sent-56, score-0.837]
</p><p>35 3  The responses of retinal ganglion cells to identical stimuli  We recorded simultaneously 21 retinal ganglion cells from the salamander using a multielectrode array. [sent-57, score-2.387]
</p><p>36 2 The visual stimulus consisted of 100 repeats of a 20 s segment of spatially uniform ﬂicker (see ﬁg. [sent-58, score-0.298]
</p><p>37 Thus, the photoreceptors were presented with exactly the same visual stimulus, and the movie is many correlation times in duration, so we can replace averages over stimuli by averages over time (ergodicity). [sent-60, score-0.241]
</p><p>38 A 3 s sample of the ganglion cell’s responses to the visual stimulus is shown in Fig. [sent-61, score-0.76]
</p><p>39 There are times when many of the cells ﬁre together, while at other times only a subset of these cells is active. [sent-63, score-1.052]
</p><p>40 a  b  time  15 5  10 5 0 0  10 20 cell rank order (by rate)  0  mean contrast  10  20  firing rate (spikes/s)  Information rate (bits/s)  500 ms  d  c  0. [sent-65, score-0.403]
</p><p>41 2 -300  -200 -100 0 time relative to spike (ms)  Figure 1: Responses of salamander ganglion cells to modulated uniform ﬁeld intensity. [sent-69, score-1.1]
</p><p>42 a: The retina is presented with a series of uniform intensity “images”. [sent-70, score-0.176]
</p><p>43 b: A 3 sec segment of the (concurrent) responses of 21 ganglion cells to repeated presentation of the stimulus. [sent-72, score-1.121]
</p><p>44 The rasters are ordered from bottom to top according to the average ﬁring rate of the neurons (over the whole movie). [sent-73, score-0.281]
</p><p>45 c: Firing rate and Information rates of the different cells as a function of their rank, ordered by their ﬁring rate. [sent-74, score-0.623]
</p><p>46 d: The average stimulus pattern preceding a spike for each of the different cells. [sent-75, score-0.272]
</p><p>47 Traditionally, these would be classiﬁed as 1 ON cell, 1 slow-OFF cell and 19 fast-OFF cells. [sent-76, score-0.262]
</p><p>48 On a ﬁner time scale than shown here, the latency of the responses of the single neurons and their spiking patterns differ across time. [sent-77, score-0.348]
</p><p>49 To analyze the responses of the different 2  The retina is isolated from the eye of the larval tiger salamander (Ambystoma tigrinum) and perfused in Ringer’s medium. [sent-78, score-0.532]
</p><p>50 1b are ordered according to their average ﬁring rate, it is clear that there is no ‘simple’ grouping of the cells’ responses with respect to this response parameter; ﬁring rates range continuously from 1 to 7 spikes per second (Fig. [sent-84, score-0.523]
</p><p>51 Similarly, the rate of information (estimated according to [15]) that the cells encode about the same stimulus also ranges continuously from 3 to 20 bits/s. [sent-86, score-0.741]
</p><p>52 We estimate the average stimulus pattern preceding a spike for each of the cells, the spike triggered average (STA), shown in Fig. [sent-87, score-0.397]
</p><p>53 According to traditional classiﬁcation based on the STA, one of the cells is an ON cell, one is a slow OFF cells and 19 belong to the fast OFF class [16]. [sent-89, score-1.121]
</p><p>54 While it may be possible to separate the 19 waveforms of the fast OFF cells into subgroups, this requires assumptions about what stimulus features are important. [sent-90, score-0.713]
</p><p>55 4  Clustering of the ganglion cells responses into functional types  To classify these ganglion cells, we solved the information theoretic optimization problem described above. [sent-92, score-1.584]
</p><p>56 Figure 2a shows the pairwise distances D(i, j) among the 21 cells, ordered by their average ﬁring rates; again, ﬁring rate alone does not cluster the cells. [sent-93, score-0.294]
</p><p>57 The result of the greedy clustering of the cells is shown by a binary dendrogram in Fig. [sent-94, score-0.724]
</p><p>58 2 0  distance (bits/s)  4  2  15  1  20 0  5 10 15 20 number of clusters  0  5  10  15  20  Figure 2: Clustering ganglion cell responses. [sent-102, score-0.696]
</p><p>59 a: Average distances between the cells responses; cells are ordered by their average ﬁring rate. [sent-103, score-1.236]
</p><p>60 c: The information that the cells’ responses convey about the clusters in every stage of the clustering in (b), normalized to the total information that the responses convey about cell identity. [sent-107, score-0.996]
</p><p>61 Using different response segment parameters or clustering method (e. [sent-108, score-0.217]
</p><p>62 The greedy agglomerative approximation [12] starts from every cell as a single cluster. [sent-112, score-0.317]
</p><p>63 We pool their spike trains together as the responses of the new cell class. [sent-116, score-0.522]
</p><p>64 2c shows the compression in information achieved by each of the mergers: for each number of clusters, we plot the mutual information between the clusters and the responses, I(r; C|s) s , normalized by the information that the response conveys about the full set of cells, I(r; i|s) s . [sent-119, score-0.326]
</p><p>65 2c are robust (up to one cell difference in the ﬁnal dendrogram) to changes in the word size and bin size used; we even obtain the same results with a nearest neighbor clustering based on D(i, j). [sent-121, score-0.429]
</p><p>66 The remaining 5 clusters are subclasses of fast OFF cells. [sent-125, score-0.199]
</p><p>67 2d which shows the dissimilarity matrix from panel a, reordered by the result of the clustering, demonstrates that while there is clear structure within the cell population, the subclasses there are not sharply distinct. [sent-127, score-0.423]
</p><p>68 While one might be happy with classifying the fast OFF cells into 5 subclasses, we further asked whether the cells within a subclass are reliably distinguishable from one another; that is, are the bottom mergers in Fig. [sent-129, score-1.608]
</p><p>69 To this end we randomly split each of the 21 cells into 2 halves (of 50 repeats each), or ‘siblings’, and re-clustered. [sent-131, score-0.557]
</p><p>70 Figure 3b shows the very different cumulative probability distributions of pairwise distances among the parent cells and that of the distances between siblings. [sent-133, score-0.793]
</p><p>71 5 2 3 4 1 average distance between cells (bits/s)  Figure 3: Every cell is different from the others. [sent-145, score-0.845]
</p><p>72 a: Clustering of cell responses after randomly splitting every cell into 2 “siblings”. [sent-146, score-0.743]
</p><p>73 The nearest neighbor of each of the new cells is its sibling and (except for one case) so is the ﬁrst merge. [sent-147, score-0.695]
</p><p>74 The distances between siblings are easily discriminated from the continuous distribution of values of all the (real) cells. [sent-151, score-0.216]
</p><p>75 It might be that cells are distinguishable, but only after observing their responses for very long times. [sent-153, score-0.788]
</p><p>76 3b  shows that more than 90% of the pairs are reliably distinguishable within 2 seconds or less. [sent-155, score-0.264]
</p><p>77 This result is especially striking given the low mean spike rate of these cells; clearly, at times where none of the cells is spiking, it is impossible to distinguish between them. [sent-156, score-0.69]
</p><p>78 To place the information about identity on an absolute scale, we compare it to the entropy of the responses at each time, using 10 ms segments of the responses at each time during the stimulus (Fig. [sent-157, score-0.721]
</p><p>79 4 On average observing a single neural response gives about 6 bits/s about the identity of the cells within this population. [sent-160, score-0.837]
</p><p>80 We also computed the average number of spikes per cell which we need to observe to distinguish reliably between cells i and j, nd (i, j) =  1 ¯ 2 (ri  + rj ) ¯ . [sent-161, score-1.104]
</p><p>81 D(i, j)  (5)  where ri is the average spike rate of cell i in the experiment. [sent-162, score-0.427]
</p><p>82 Evidently, more than 80% of the pairs are reliably distinguishable after observing, on average, only 3 spikes from one of the neurons. [sent-164, score-0.318]
</p><p>83 Since ganglion cells ﬁre in bursts, this suggest that most cells are reliably distinguishable based on a single ﬁring ‘event’! [sent-165, score-1.636]
</p><p>84 We also show that for the 11 most similar cells (those in the left subtree in Fig. [sent-166, score-0.559]
</p><p>85 b  1  cumulative distribution  information about identity in 10 ms response segment (bits)  a  0. [sent-168, score-0.377]
</p><p>86 5  1  2  3  4  5  10  20  30  nd (spikes)  Figure 4: High diversity among cells. [sent-180, score-0.192]
</p><p>87 a: The average information that a response segment conveys about the identity of the cell as a function of the entropy of the responses. [sent-181, score-0.622]
</p><p>88 Results shown are for 2-letter words of 5 ms bins; similar behavior is observed for different word sizes and bins b: Cumulative distribution of the average number of spikes that are needed to distinguish between pair of cells. [sent-183, score-0.294]
</p><p>89 5  Discussion  We have identiﬁed a diversity of functional types of retinal ganglion cells by clustering them to preserve information about their identity. [sent-184, score-1.333]
</p><p>90 Beyond the easy classiﬁcation of the major types of salamander ganglion cells – fast OFF, slow OFF, and ON – in agreement with traditional methods, we have found clear structure within the fast OFF cells that suggests at least 5 more functional classes. [sent-185, score-1.886]
</p><p>91 Furthermore, we found evidence that each cell is functionally unique. [sent-186, score-0.293]
</p><p>92 Even under this relatively simple stimulus, the analysis revealed that the 4 Since the cells receive the same stimulus and often possess shared circuitry, an efﬁciency as high as 100% is very unlikely. [sent-187, score-0.673]
</p><p>93 cell responses convey ∼6 bits/s of information about cell identity within this population of 21 cells. [sent-188, score-0.998]
</p><p>94 Ganglion cells in the salamander interact with each other and collect information from a ∼250 µm radius; given the density of ganglion cells, the observed rate implies that a single ganglion cell can be discriminated from all the cells in this “elementary patch” within 1 s. [sent-189, score-2.307]
</p><p>95 This is a surprising degree of diversity, given that 19 cells in our sample would be traditionally viewed as nominally the same. [sent-190, score-0.565]
</p><p>96 However, we found that this stimulus was rich enough to distinguish every ganglion cell in our data set. [sent-192, score-0.844]
</p><p>97 Using a larger collection of cells will enable us to explore the possibility that there is a continuum of unique functional units in the retina. [sent-194, score-0.596]
</p><p>98 By comparing the spiking of closely related cells, it might be possible to achieve much ﬁner discrimination among stimuli that tend to activate both cells. [sent-197, score-0.213]
</p><p>99 Finally, great functional diversity opens up additional possibilities for learning strategies, in which downstream neurons select the most useful of its inputs rather than merely summing over identical inputs to reduce their noise. [sent-199, score-0.352]
</p><p>100 This suggests that the extreme diversity found here in the vertebrate retina may not be the result of some inevitable sloppiness of neural development but rather as evolutionary selection of a different strategy for representing the visual world. [sent-201, score-0.406]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cells', 0.526), ('ganglion', 0.352), ('cell', 0.262), ('responses', 0.192), ('diversity', 0.155), ('salamander', 0.154), ('retina', 0.147), ('stimulus', 0.147), ('mergers', 0.133), ('neurons', 0.127), ('stimuli', 0.119), ('reliably', 0.117), ('distinguishable', 0.115), ('dendrogram', 0.111), ('siblings', 0.111), ('identity', 0.101), ('sibling', 0.089), ('clustering', 0.087), ('spikes', 0.086), ('classi', 0.084), ('ring', 0.083), ('retinal', 0.083), ('clusters', 0.082), ('response', 0.079), ('subclasses', 0.077), ('population', 0.072), ('functional', 0.07), ('distances', 0.07), ('visual', 0.069), ('spike', 0.068), ('frog', 0.067), ('molecular', 0.063), ('ms', 0.061), ('arxiv', 0.058), ('cumulative', 0.057), ('average', 0.057), ('ordered', 0.057), ('distinguish', 0.056), ('movie', 0.053), ('subclass', 0.053), ('neuron', 0.052), ('clear', 0.052), ('segment', 0.051), ('convey', 0.049), ('ner', 0.046), ('shannon', 0.046), ('conveys', 0.044), ('icker', 0.044), ('bialek', 0.044), ('neighbor', 0.042), ('observing', 0.042), ('cation', 0.042), ('rate', 0.04), ('fast', 0.04), ('classes', 0.039), ('djs', 0.039), ('anatomy', 0.039), ('cajal', 0.039), ('nominally', 0.039), ('tiger', 0.039), ('classifying', 0.038), ('nearest', 0.038), ('mutual', 0.037), ('among', 0.037), ('fi', 0.037), ('tishby', 0.037), ('discriminated', 0.035), ('meister', 0.035), ('ruyter', 0.035), ('sta', 0.035), ('vertebrate', 0.035), ('bins', 0.034), ('bits', 0.034), ('metric', 0.034), ('light', 0.033), ('pairwise', 0.033), ('iij', 0.033), ('illinois', 0.033), ('subtree', 0.033), ('agreement', 0.033), ('types', 0.032), ('theoretic', 0.032), ('identi', 0.032), ('within', 0.032), ('repeats', 0.031), ('morphological', 0.031), ('functionally', 0.031), ('steveninck', 0.031), ('spiking', 0.029), ('traditional', 0.029), ('receptive', 0.029), ('intensity', 0.029), ('quanti', 0.028), ('agglomerative', 0.028), ('merge', 0.028), ('information', 0.028), ('might', 0.028), ('cations', 0.027), ('every', 0.027), ('merging', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="28-tfidf-1" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>2 0.26390243 <a title="28-tfidf-2" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>3 0.23072334 <a title="28-tfidf-3" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>Author: Christian K. Machens, Michael Wehr, Anthony M. Zador</p><p>Abstract: How do cortical neurons represent the acoustic environment? This question is often addressed by probing with simple stimuli such as clicks or tone pips. Such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to uncover complex or higher-order neuronal response properties. Here we adopt an alternative approach, probing neuronal responses with complex acoustic stimuli, including animal vocalizations and music. We have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential ﬂuctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit, instead of just its output—a sparse binary spike train—as in conventional single unit physiological recordings. Whole cell recording thus provides a much richer source of information about the neuron’s response. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component—the spectrotemporal receptive ﬁeld (STRF)—of the transformation from the sound (as represented by its time-varying spectrogram) to the neuron’s membrane potential. We ﬁnd that the STRF has a rich dynamical structure, including excitatory regions positioned in general accord with the prediction of the simple tuning curve. We also ﬁnd that in many cases, much of the neuron’s response, although deterministically related to the stimulus, cannot be predicted by the linear component, indicating the presence of as-yet-uncharacterized nonlinear response properties.</p><p>4 0.2107358 <a title="28-tfidf-4" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>5 0.20310596 <a title="28-tfidf-5" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>Author: Patrik O. Hoyer, Aapo Hyvärinen</p><p>Abstract: The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signiﬁcantly from trial to trial. This variability is most often interpreted as ‘noise’, purely detrimental to the sensory system. In this paper, we propose an alternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speciﬁcally, the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representational scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.</p><p>6 0.19236346 <a title="28-tfidf-6" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>7 0.18362488 <a title="28-tfidf-7" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>8 0.164923 <a title="28-tfidf-8" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>9 0.15597104 <a title="28-tfidf-9" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>10 0.1510922 <a title="28-tfidf-10" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>11 0.14946657 <a title="28-tfidf-11" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>12 0.13794121 <a title="28-tfidf-12" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>13 0.12392918 <a title="28-tfidf-13" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>14 0.12330201 <a title="28-tfidf-14" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>15 0.10597689 <a title="28-tfidf-15" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>16 0.099692047 <a title="28-tfidf-16" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>17 0.095894255 <a title="28-tfidf-17" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>18 0.094683632 <a title="28-tfidf-18" href="./nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">187 nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>19 0.092568867 <a title="28-tfidf-19" href="./nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">102 nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>20 0.092505254 <a title="28-tfidf-20" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.262), (1, 0.278), (2, 0.116), (3, -0.01), (4, 0.008), (5, -0.136), (6, -0.047), (7, -0.147), (8, -0.167), (9, 0.089), (10, -0.043), (11, 0.042), (12, 0.004), (13, 0.084), (14, -0.029), (15, 0.086), (16, 0.239), (17, -0.058), (18, -0.103), (19, 0.029), (20, 0.04), (21, 0.009), (22, 0.056), (23, -0.088), (24, -0.13), (25, -0.042), (26, -0.115), (27, 0.001), (28, -0.103), (29, -0.06), (30, -0.062), (31, 0.042), (32, 0.058), (33, 0.012), (34, 0.089), (35, 0.026), (36, -0.005), (37, -0.057), (38, 0.052), (39, 0.031), (40, 0.052), (41, -0.079), (42, -0.028), (43, -0.024), (44, 0.002), (45, 0.059), (46, -0.024), (47, -0.039), (48, -0.022), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98618114 <a title="28-lsi-1" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>2 0.78430837 <a title="28-lsi-2" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>3 0.68901622 <a title="28-lsi-3" href="./nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">118 nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>Author: Alistair Bray, Dominique Martinez</p><p>Abstract: In Slow Feature Analysis (SFA [1]), it has been demonstrated that high-order invariant properties can be extracted by projecting inputs into a nonlinear space and computing the slowest changing features in this space; this has been proposed as a simple general model for learning nonlinear invariances in the visual system. However, this method is highly constrained by the curse of dimensionality which limits it to simple theoretical simulations. This paper demonstrates that by using a different but closely-related objective function for extracting slowly varying features ([2, 3]), and then exploiting the kernel trick, this curse can be avoided. Using this new method we show that both the complex cell properties of translation invariance and disparity coding can be learnt simultaneously from natural images when complex cells are driven by simple cells also learnt from the image. The notion of maximising an objective function based upon the temporal predictability of output has been progressively applied in modelling the development of invariances in the visual system. F6ldiak used it indirectly via a Hebbian trace rule for modelling the development of translation invariance in complex cells [4] (closely related to many other models [5,6,7]); this rule has been used to maximise invariance as one component of a hierarchical system for object and face recognition [8]. On the other hand, similar functions have been maximised directly in networks for extracting linear [2] and nonlinear [9, 1] visual invariances. Direct maximisation of such functions have recently been used to model complex cells [10] and as an alternative to maximising sparseness/independence in modelling simple cells [11]. Slow Feature Analysis [1] combines many of the best properties of these methods to provide a good general nonlinear model. That is, it uses an objective function that minimises the first-order temporal derivative of the outputs; it provides a closedform solution which maximises this function by projecting inputs into a nonlinear http://www.loria.fr/equipes/cortex/ space; it exploits sphering (or PCA-whitening) of the data to ensure that all outputs have unit variance and are uncorrelated. However, the method suffers from the curse of dimensionality in that the nonlinear feature space soon becomes very large as the input dimension grows, and yet this feature space must be represented explicitly in order for the essential sphering to occur. The alternative that we propose here is to use the objective function of Stone [2, 9], that maximises output variance over a long period whilst minimising variance over a shorter period; in the linear case, this can be implemented by a biologically plausible mixture of Hebbian and anti-Hebbian learning on the same synapses [2]. In recent work, Stone has proposed a closed-form solution for maximising this function in the linear domain of blind source separation that does not involve data-sphering. This paper describes how this method can be kernelised. The use of the</p><p>4 0.67302352 <a title="28-lsi-4" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>Author: Jarmo Hurri, Aapo Hyvärinen</p><p>Abstract: We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive ﬁelds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the ﬁrst approach we extract receptive ﬁelds whose outputs are as temporally coherent as possible. This approach yields simple-cell-like receptive ﬁelds (oriented, localized, multiscale). Thus, temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive ﬁelds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell temporal dependencies. Estimation of this model from natural data yields both simple-cell-like receptive ﬁelds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both layers of the generative model are estimated simultaneously from scratch. This is a signiﬁcant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been ﬁxed a priori.</p><p>5 0.6670661 <a title="28-lsi-5" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>6 0.66016257 <a title="28-lsi-6" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>7 0.59193105 <a title="28-lsi-7" href="./nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">206 nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<p>8 0.56606483 <a title="28-lsi-8" href="./nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">26 nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>9 0.54395783 <a title="28-lsi-9" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>10 0.52999425 <a title="28-lsi-10" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>11 0.52867365 <a title="28-lsi-11" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>12 0.45206973 <a title="28-lsi-12" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>13 0.4267818 <a title="28-lsi-13" href="./nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">171 nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>14 0.39512083 <a title="28-lsi-14" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>15 0.37956288 <a title="28-lsi-15" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>16 0.36798012 <a title="28-lsi-16" href="./nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">103 nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>17 0.36459923 <a title="28-lsi-17" href="./nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">177 nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>18 0.34939322 <a title="28-lsi-18" href="./nips-2002-An_Impossibility_Theorem_for_Clustering.html">27 nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>19 0.34594458 <a title="28-lsi-19" href="./nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">76 nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>20 0.3428587 <a title="28-lsi-20" href="./nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">12 nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.013), (11, 0.015), (14, 0.035), (23, 0.037), (42, 0.048), (54, 0.107), (55, 0.065), (64, 0.02), (67, 0.016), (68, 0.05), (74, 0.093), (75, 0.025), (92, 0.026), (94, 0.231), (98, 0.134)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83326453 <a title="28-lda-1" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>2 0.8012709 <a title="28-lda-2" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>Author: Stuart Andrews, Ioannis Tsochantaridis, Thomas Hofmann</p><p>Abstract: This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristically. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization. 1</p><p>3 0.67696589 <a title="28-lda-3" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>4 0.66807646 <a title="28-lda-4" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>5 0.66201311 <a title="28-lda-5" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>Author: Wolfgang Maass, Thomas Natschläger, Henry Markram</p><p>Abstract: A key challenge for neural modeling is to explain how a continuous stream of multi-modal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-ﬁre neurons in real-time. We propose a new computational model that is based on principles of high dimensional dynamical systems in combination with statistical learning theory. It can be implemented on generic evolved or found recurrent circuitry.</p><p>6 0.65915519 <a title="28-lda-6" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>7 0.65703565 <a title="28-lda-7" href="./nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">193 nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>8 0.65380311 <a title="28-lda-8" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>9 0.65233684 <a title="28-lda-9" href="./nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">148 nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>10 0.64947939 <a title="28-lda-10" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>11 0.64747071 <a title="28-lda-11" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>12 0.64677119 <a title="28-lda-12" href="./nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">5 nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>13 0.64602357 <a title="28-lda-13" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>14 0.64521509 <a title="28-lda-14" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>15 0.64482361 <a title="28-lda-15" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>16 0.64439356 <a title="28-lda-16" href="./nips-2002-Binary_Coding_in_Auditory_Cortex.html">43 nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>17 0.64412606 <a title="28-lda-17" href="./nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">81 nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>18 0.64237422 <a title="28-lda-18" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>19 0.64101171 <a title="28-lda-19" href="./nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>20 0.64097881 <a title="28-lda-20" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
