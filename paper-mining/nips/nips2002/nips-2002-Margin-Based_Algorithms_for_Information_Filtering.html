<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2002-Margin-Based Algorithms for Information Filtering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-139" href="#">nips2002-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 nips-2002-Margin-Based Algorithms for Information Filtering</h1>
<br/><p>Source: <a title="nips-2002-139-pdf" href="http://papers.nips.cc/paper/2154-margin-based-algorithms-for-information-filtering.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>Reference: <a title="nips-2002-139-reference" href="../nips2002_reference/nips-2002-Margin-Based_Algorithms_for_Information_Filtering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it  Abstract In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. [sent-7, score-0.169]
</p><p>2 Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. [sent-8, score-0.581]
</p><p>3 While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. [sent-9, score-0.253]
</p><p>4 These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model. [sent-10, score-0.295]
</p><p>5 Consider a stream of discrete data that are individually labelled as “relevant” or “nonrelevant” according to some ﬁxed relevance criterion; for instance, news about a certain topic, emails that are not spam, or fraud cases from logged data of user behavior. [sent-12, score-0.163]
</p><p>6 In all of these cases, a ﬁlter can be used to drop uninteresting parts of the stream, forwarding to the user only those data which are likely to fulﬁl the relevance criterion. [sent-13, score-0.158]
</p><p>7 However, unlike standard on-line pattern classiﬁcation tasks, where the classiﬁer observes the correct label after each prediction, here the relevance of a data element is known only if the ﬁlter decides to forward that data element to the user. [sent-15, score-0.278]
</p><p>8 Each element of an arbitrary data sequence is characterized by a feature vector and an associated relevance label (say, for relevant and for nonrelevant). [sent-20, score-0.228]
</p><p>9 At each time , the ﬁltering system observes the -th feature vector and must decide whether or not to forward it. [sent-21, score-0.083]
</p><p>10 If the data is forwarded, then its relevance label is revealed to the system,  %$&$%$#"! [sent-22, score-0.167]
</p><p>11 If the data is not forwarded, its relevance label remains hidden. [sent-26, score-0.167]
</p><p>12 We call the -th instance of the data sequence and the pair the -th example. [sent-27, score-0.022]
</p><p>13 There are two kinds of errors the ﬁltering system can make in judging the relevance of a feature vector . [sent-29, score-0.124]
</p><p>14 We say that an example is a false positive if and is classiﬁed as relevant by the system; similarly, we say that is a false negative if and is classiﬁed as nonrelevant by the system. [sent-30, score-0.165]
</p><p>15 Although false negatives remain unknown, the ﬁltering system is scored according to the overall number of wrong relevance judgements it makes. [sent-31, score-0.29]
</p><p>16 That is, both false positives and false negatives are counted as mistakes. [sent-32, score-0.119]
</p><p>17 In this paper, we study the ﬁltering problem under the assumption that relevance judgements are generated using an unknown probabilistic linear function. [sent-33, score-0.254]
</p><p>18 We design ﬁltering rules that maintain a linear hypothesis and use the margin information to decide whether to forward the next instance. [sent-34, score-0.107]
</p><p>19 , the number of wrong judgements made by a ﬁltering rule over and above those made by the rule knowing the probabilistic function used to generate judgements. [sent-37, score-0.295]
</p><p>20 We show ﬁnite-time (nonasymptotical) bounds on the regret that hold for arbitrary sequences of instances. [sent-38, score-0.319]
</p><p>21 The only other results of this kind we are aware of are those proven in [9] for the apple tasting model. [sent-39, score-0.119]
</p><p>22 Since in the apple tasting model relevance judgements are chosen adversarially rather than probabilistically, we cannot compare their bounds with ours. [sent-40, score-0.374]
</p><p>23 We report some preliminary experimental results which might suggest the superiority of our methods as opposed to the general transformations developed within the apple tasting framework. [sent-41, score-0.119]
</p><p>24 As a matter of fact, these general transformations do not take margin information into account. [sent-42, score-0.049]
</p><p>25 ¥© ' '¨ ¡    ¥© ' ¨   © §¡ '  ¥ ¦  £ ¤   '¡  £  ¡  ' '¨ ¡  ¡     ' '¨ ¡       ' ¨ "' ¡  ¡ ¢     '¡  '¡  In Section 2, we introduce our probabilistic relevance model and make some preliminary observations. [sent-43, score-0.147]
</p><p>26 In Section 3, we consider a restricted version of the model within which we prove a regret bound for a simple ﬁltering rule called SIMPLE - FIL. [sent-44, score-0.436]
</p><p>27 In Section 4, we generalize this ﬁltering rule and show its good performance on the Reuters Corpus Volume 1. [sent-45, score-0.103]
</p><p>28 The algorithm employed, which we call RIDGE - FIL, is a linear least squares algorithm inspired by [2]. [sent-46, score-0.063]
</p><p>29 In that section we also prove, within the unrestricted probabilistic model, a regret bound for the randomized variant P - RIDGE - FIL of the general ﬁltering rule. [sent-47, score-0.463]
</p><p>30 Both RIDGE - FIL and its randomized variant can be run with kernels [13] and adapted to the case when the unknown linear function drifts with time. [sent-48, score-0.098]
</p><p>31   ©  %    '¡  The relevance of is given by a -valued random variable (where means “relevant”) such that there exists a ﬁxed and unknown vector , , for which for all . [sent-50, score-0.146]
</p><p>32 In this model, we want to perform almost as well as the algorithm that knows and forwards if and only if . [sent-53, score-0.111]
</p><p>33 For any ﬁxed sequence of instances, we use to denote the margin and to denote the margin . [sent-55, score-0.098]
</p><p>34 We deﬁne the expected regret of the linear-threshold ﬁltering algorithm at time as . [sent-56, score-0.273]
</p><p>35 3 A simpliﬁed model We start by analyzing a restricted model where each data element has the same unknown probability of being relevant and we want to perform almost as well as the ﬁltering rule that consistently does the optimal action (i. [sent-63, score-0.185]
</p><p>36 The analysis of this model is used in Section 4 to guide the design of good ﬁltering rules for the unrestricted model. [sent-66, score-0.059]
</p><p>37   ¤ ¦© © ¤ ¦  and let be the sample average of , where is Let the number of forwarded data elements in the ﬁrst time steps and is the fraction of elements that have been forwarded. [sent-70, score-0.243]
</p><p>38 Obviously, the optimal true positives among the rule forwards if and only if . [sent-71, score-0.192]
</p><p>39 Consider instead the empirical rule that forwards if and only if . [sent-72, score-0.173]
</p><p>40 Hence, data should be forwarded (irrespective to the sign of the estimate ) also when the conﬁdence gets too small with respect to . [sent-75, score-0.232]
</p><p>41 A problem in this argument is that large level for deviation bounds require for making small. [sent-76, score-0.046]
</p><p>42 An algorithm, which we call SIMPLE - FIL, implementing the above line of reasoning takes the form of the following simple rule: forward if and only if , where . [sent-80, score-0.062]
</p><p>43 The expected regret at time of SIMPLE - FIL is deﬁned as the probability that SIMPLE - FIL makes a mistake at time minus the probability that the optimal ﬁltering rule makes a mistake, that is . [sent-81, score-0.398]
</p><p>44 The next result shows a logarithmic bound on this regret. [sent-82, score-0.032]
</p><p>45 We can bound the actual regret after the deﬁnition of the ﬁltering rule we have  of time  G H&  $  D  ¡ S&    D  D  & ¥  D 3   ¦   ¤  ¦  ¡  ¤  ¦  $    D©    ¤  a  &    ¤  Theorem 1 The expected cumulative regret of SIMPLE - FIL after any number steps is at most . [sent-91, score-0.68]
</p><p>46  ¡  '   ` Y a ¦¥a3  6 ¤ ¤ b TY      $ ¨FC GFEBC EB © ' A6¢ D  '¢ '2C(0 1B    ¤  '  D  )  h gh    ¤  ¥ h bg  Dh  a  '  '  8  b ¥Y  )  2  D  ¥   '  D  )  2  ¡  ` Y a  '  © XY V $ @32   ! [sent-101, score-0.087]
</p><p>47 (    &  2  , which is a sum of  ¨ d $      '  '  32 4T1  ¢   ¦`    Applying Chernoff-Hoeffding [11] bounds to independent random variables, we obtain  We now bound  ¥  for  separately. [sent-111, score-0.078]
</p><p>48 We now bound and implies that , we have that Since for some . [sent-113, score-0.032]
</p><p>49 Hence we can write  ¥ F  D  '  '  )  D  '  '  h gh  ¥ h dg  )  a  Dh  '  '  Applying Chernoff-Hoeffding bounds again, we get Finally, one can easily verify that . [sent-114, score-0.147]
</p><p>50 g  a  '   a 1     &  Y  4 Linear least squares ﬁltering In order to generalize SIMPLE - FIL to the original (unrestricted) learning model described in Section 2, we need a low-variance estimate of the target vector . [sent-117, score-0.039]
</p><p>51 Let be the matrix whose columns are the forwarded feature vectors after the ﬁrst time steps and let be the vector of corresponding observed relevance labels (the index will be momentarily dropped). [sent-118, score-0.349]
</p><p>52 Consider the least squares estimator of , where is the pseudo-inverse of . [sent-120, score-0.055]
</p><p>53 For all belonging to the column space of , this is an unbiased estimator of , that is To remove the assumption on , we make full rank by adding the identity . [sent-121, score-0.035]
</p><p>54 The -measure is deﬁned by , where is precision (fraction of relevant documents among the forwarded ones) and is recall (fraction of forwarded documents among the relevant ones). [sent-127, score-0.61]
</p><p>55 In the plot, the ﬁltering rule RIDGE - FIL is compared with RIDGE - FULL which sees the correct label after each classiﬁcation. [sent-128, score-0.127]
</p><p>56 While precision and recall of RIDGE - FULL are balanced, RIDGE - FIL’s recall is higher than precision due to the need of forwarding more documents than believed relevant. [sent-129, score-0.123]
</p><p>57 This in order to make the conﬁdence of the estimator converge to 1 fast enough. [sent-130, score-0.035]
</p><p>58 ¢  ¢¨ ¦ ¤ ¢ ©§¥£¡     ¤      ¤     a h` '  6    obtaining , a “sparse” version of the ridge regression estimator [12] (the sparsity is due to the fact that we only store in the forwarded instances, i. [sent-132, score-0.67]
</p><p>59 To estimate directly the margin , rather than , we further modify, along the lines of the techniques analyzed in [3, 6, 15], the sparse ridge with the quantity , where regression estimator. [sent-135, score-0.479]
</p><p>60  u © ' ` '  '¡'  '¡  ¡   a  '  6 7  ¡   '       '  7  Using the Sherman-Morrison formula, we can then write out the expectation of  as  " " #    % $    ! [sent-138, score-0.022]
</p><p>61 In order to generalize to the estimator (3) the analysis of SIMPLE - FIL, we need to ﬁnd a large deviation bound of the form for all , where goes to zero “sufﬁciently fast” as . [sent-141, score-0.086]
</p><p>62 Though we have not been able to ﬁnd such bounds, we report some experimental results showing that algorithms based on (3) and inspired by the analysis of SIMPLE - FIL do exhibit a good empirical behavior on real-world data. [sent-142, score-0.021]
</p><p>63 2 we prove a bound (not based on the analysis of SIMPLE - FIL) on the expected regret of a randomized variant of the algorithm used in the experiments. [sent-144, score-0.41]
</p><p>64 For this variant we are able to prove a regret bound that scales essentially with the square root of (to be contrasted with the logarithmic regret of SIMPLE - FIL). [sent-145, score-0.64]
</p><p>65 and update as follows:  '  ¤ ¥  h a hQ  7h h  if    ; , where  and    '  &  ¡  '  7 `'  &   Ri ' & X  '  DC  7  & W¥     '  ¡ d  DC  7    7    '  , otherwise;    ¡ & Sf &e;   hh '  '  7h h   i '  7  that  ` u #)4  then forward  ,  '  2. [sent-152, score-0.08]
</p><p>66 If was forwarded then get label the same updates as in 2; otherwise, do not make any update. [sent-159, score-0.266]
</p><p>67 document ﬁltering problem based on the ﬁrst 70000 newswire stories from the Reuters Corpus Volume 1. [sent-162, score-0.042]
</p><p>68 We selected the 34 Reuters topics whose frequency in the set of 70000 documents was between 1% and 5% (a plausible range for ﬁltering applications). [sent-163, score-0.053]
</p><p>69 For each topic, we deﬁned a ﬁltering task whose relevance judgements were assigned based on whether the document was labelled with that topic or not. [sent-164, score-0.295]
</p><p>70 Finally, all document vectors were normalized to length 1. [sent-168, score-0.042]
</p><p>71 Finally, we also of tested the apple-tasting ﬁltering rule (see [9, STAP transformation]) based on the binary classiﬁer RIDGE - FULL. [sent-176, score-0.084]
</p><p>72 2 Probabilistic ridge ﬁltering In this section we introduce a probabilistic ﬁltering algorithm, derived from the (on-line) ridge regression algorithm, for the class of linear probabilistic relevance functions. [sent-180, score-1.004]
</p><p>73 If , then is forwarded and gets updated according to the following two-steps ridge regression-like rule. [sent-183, score-0.636]
</p><p>74 First, the intermediate vector is computed via the standard on-line ridge regression algorithm using the inverse of matrix . [sent-184, score-0.43]
</p><p>75 On the other hand, if then is forwarded (and consequently is updated) with some probability . [sent-191, score-0.205]
</p><p>76 The analysis of P - RIDGE - FIL is inspired by the analysis in [1] for a related but different problem, and is based on relating the expected regret in a given trial to a measure of the progress of towards . [sent-192, score-0.335]
</p><p>77 h a hG  7h h  ¡  7 @  ' '¡       Y©'  &  H   ¡  7 @  ' ¡ ¤©   ¡  7  7 f     Y©'     ¤¦ © § ¤¤ ¥ 7    7   ' '   '  7 7  Lemma 2 Using the notation of Figure 2, let be the trial when the -th update occurs. [sent-194, score-0.061]
</p><p>78 Then the following inequality holds: , where denotes the determinant of matrix and . [sent-195, score-0.02]
</p><p>79 '  '  '  '   £¦  ¤¥  7 f   R  '  §¤    ¨  ¡  7 f  ¡ 47  7  Y  '  '  '  7  2  a   Y   '   ¡  a  ¨  ¡    7  C   2 '  '   R  2  2  ¡  h h h h  ¡ '  7 " RP'     ¥ G¡  DC   2 '    7 f   Y  '  §  7  ¡    ¡  7 f  7 "    '  '   R  '  a hh  . [sent-216, score-0.04]
</p><p>80 For all , if algorithm P - RIDGE - FIL , then its expected cumulative regret ! [sent-217, score-0.291]
</p><p>81 If is the trial when the -th forward takes place, we deﬁne the random variables and . [sent-220, score-0.081]
</p><p>82 Let be the regret of P - RIDGE - FIL in trial and be the regret of the update rule in trial . [sent-222, score-0.712]
</p><p>83 If , then gets lower bounded via Lemma 2 only with probability , while for the regret we can only use the trivial bound . [sent-224, score-0.357]
</p><p>84 Thus, using Lemma C  8  3  DC   0              c     3c  Now, it is easy to verify that in the conditional space where and 2 and Eq. [sent-228, score-0.019]
</p><p>85 (4) we can write  (4)  C  DC      DC   2     ' 2   @   ¡  2  ¡ "    gI¥ ¨ &  DC § T   gIH ¨ &  DC Rh  DC h  C    ¡  2  D C      DC c§ T       C         DC h  2  ¡ B     3c    S  DC   2    1  '  1 This parametrization requires the knowledge of . [sent-229, score-0.022]
</p><p>86 G  This can be further upper bounded by  4    ¦ E  43$   %! [sent-231, score-0.025]
</p><p>87 In turn, this term has been bounded terms as by virtue of (2) with . [sent-237, score-0.025]
</p><p>88 & I¥  DC    hh  h h 2  a  '  )  DC  a    c '   )  thereby concluding the proof. [sent-250, score-0.04]
</p><p>89 Relative loss bounds for on-line density estimation with the exponential family of distributions, Machine Learning, 43:211–246. [sent-267, score-0.046]
</p><p>90 Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. [sent-291, score-0.046]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dc', 0.505), ('fil', 0.478), ('ridge', 0.404), ('regret', 0.273), ('ltering', 0.236), ('forwarded', 0.205), ('relevance', 0.124), ('forwards', 0.089), ('judgements', 0.085), ('rule', 0.084), ('apple', 0.068), ('gih', 0.068), ('unrestricted', 0.059), ('rh', 0.057), ('documents', 0.053), ('conconi', 0.051), ('gentile', 0.051), ('nonrelevant', 0.051), ('sih', 0.051), ('tasting', 0.051), ('bw', 0.05), ('margin', 0.049), ('dh', 0.047), ('bounds', 0.046), ('bg', 0.045), ('randomized', 0.043), ('label', 0.043), ('document', 0.042), ('gh', 0.042), ('ty', 0.042), ('ch', 0.042), ('italy', 0.041), ('pih', 0.041), ('mistake', 0.041), ('reuters', 0.041), ('trial', 0.041), ('forward', 0.04), ('hh', 0.04), ('lemma', 0.04), ('false', 0.038), ('relevant', 0.038), ('tf', 0.036), ('classi', 0.036), ('estimator', 0.035), ('bramante', 0.034), ('crema', 0.034), ('forwarding', 0.034), ('milan', 0.034), ('warmuth', 0.034), ('variant', 0.033), ('bound', 0.032), ('dence', 0.031), ('gx', 0.03), ('dti', 0.03), ('prove', 0.029), ('pw', 0.027), ('sgn', 0.027), ('gets', 0.027), ('regression', 0.026), ('topic', 0.026), ('observes', 0.025), ('corpus', 0.025), ('bounded', 0.025), ('uy', 0.024), ('colt', 0.024), ('negatives', 0.024), ('element', 0.023), ('theorem', 0.023), ('probabilistic', 0.023), ('unknown', 0.022), ('call', 0.022), ('knows', 0.022), ('write', 0.022), ('lter', 0.022), ('inspired', 0.021), ('stream', 0.021), ('perceptron', 0.021), ('instances', 0.02), ('let', 0.02), ('squares', 0.02), ('inequality', 0.02), ('df', 0.019), ('wrong', 0.019), ('dropped', 0.019), ('generalize', 0.019), ('positives', 0.019), ('bregman', 0.019), ('verify', 0.019), ('ih', 0.019), ('get', 0.018), ('ei', 0.018), ('gi', 0.018), ('restricted', 0.018), ('proof', 0.018), ('cumulative', 0.018), ('labelled', 0.018), ('decide', 0.018), ('precision', 0.018), ('fraction', 0.018), ('er', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="139-tfidf-1" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>2 0.18844821 <a title="139-tfidf-2" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>3 0.083285965 <a title="139-tfidf-3" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert function Spaces. We introduce a concept of scalesensitive effective data dimension, and show that it characterizes the convergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in ﬁnite dimensional spaces to non-parametric kernel learning methods. We derive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.</p><p>4 0.082178563 <a title="139-tfidf-4" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>Author: Jason Weston, Olivier Chapelle, Vladimir Vapnik, André Elisseeff, Bernhard Schölkopf</p><p>Abstract: We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 1</p><p>5 0.066646956 <a title="139-tfidf-5" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>Author: Luis E. Ortiz, David A. McAllester</p><p>Abstract: This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding’s inequality, the Angluin-Valiant bound, Bernstein’s inequality, Bennett’s inequality, or McDiarmid’s theorem.</p><p>6 0.056474924 <a title="139-tfidf-6" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>7 0.054635957 <a title="139-tfidf-7" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>8 0.054213125 <a title="139-tfidf-8" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>9 0.052387662 <a title="139-tfidf-9" href="./nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">184 nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>10 0.052074946 <a title="139-tfidf-10" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>11 0.051639449 <a title="139-tfidf-11" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>12 0.051570594 <a title="139-tfidf-12" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>13 0.050808076 <a title="139-tfidf-13" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>14 0.048633527 <a title="139-tfidf-14" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>15 0.045707297 <a title="139-tfidf-15" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>16 0.044286847 <a title="139-tfidf-16" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>17 0.044188786 <a title="139-tfidf-17" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>18 0.043718111 <a title="139-tfidf-18" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>19 0.042492624 <a title="139-tfidf-19" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>20 0.042278543 <a title="139-tfidf-20" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, -0.05), (2, 0.012), (3, -0.025), (4, 0.004), (5, 0.004), (6, -0.055), (7, -0.008), (8, 0.034), (9, -0.053), (10, -0.012), (11, -0.08), (12, 0.029), (13, -0.077), (14, 0.035), (15, -0.03), (16, 0.035), (17, -0.082), (18, -0.07), (19, 0.001), (20, 0.002), (21, 0.023), (22, -0.038), (23, 0.074), (24, -0.051), (25, 0.046), (26, 0.13), (27, -0.016), (28, -0.186), (29, -0.112), (30, 0.035), (31, 0.094), (32, 0.0), (33, -0.121), (34, -0.163), (35, 0.174), (36, -0.161), (37, 0.101), (38, -0.229), (39, -0.154), (40, 0.028), (41, -0.114), (42, -0.033), (43, -0.073), (44, 0.158), (45, -0.165), (46, 0.064), (47, -0.019), (48, -0.121), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96474463 <a title="139-lsi-1" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>2 0.82689548 <a title="139-lsi-2" href="./nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">17 nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>3 0.43027967 <a title="139-lsi-3" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>Author: Luis E. Ortiz, David A. McAllester</p><p>Abstract: This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding’s inequality, the Angluin-Valiant bound, Bernstein’s inequality, Bennett’s inequality, or McDiarmid’s theorem.</p><p>4 0.36098239 <a title="139-lsi-4" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert function Spaces. We introduce a concept of scalesensitive effective data dimension, and show that it characterizes the convergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in ﬁnite dimensional spaces to non-parametric kernel learning methods. We derive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.</p><p>5 0.29837856 <a title="139-lsi-5" href="./nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">143 nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>Author: Bin Wu, K. Wong, David Bodoff</p><p>Abstract: We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR). Mean-ﬁeld methods are applied to analyze the model and derive efﬁcient practical algorithms to estimate the parameters in the problem. The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.</p><p>6 0.27208853 <a title="139-lsi-6" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>7 0.27136406 <a title="139-lsi-7" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>8 0.25818157 <a title="139-lsi-8" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>9 0.25554577 <a title="139-lsi-9" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>10 0.25514957 <a title="139-lsi-10" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>11 0.25368986 <a title="139-lsi-11" href="./nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">8 nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>12 0.22206253 <a title="139-lsi-12" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>13 0.213191 <a title="139-lsi-13" href="./nips-2002-Multiple_Cause_Vector_Quantization.html">150 nips-2002-Multiple Cause Vector Quantization</a></p>
<p>14 0.21266404 <a title="139-lsi-14" href="./nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">149 nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>15 0.21248616 <a title="139-lsi-15" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>16 0.21211043 <a title="139-lsi-16" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>17 0.2116684 <a title="139-lsi-17" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>18 0.20901287 <a title="139-lsi-18" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>19 0.20057462 <a title="139-lsi-19" href="./nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>20 0.19766752 <a title="139-lsi-20" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.017), (23, 0.017), (35, 0.355), (42, 0.061), (54, 0.127), (55, 0.031), (67, 0.017), (68, 0.018), (74, 0.06), (92, 0.034), (98, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76655889 <a title="139-lda-1" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><p>2 0.70336986 <a title="139-lda-2" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>Author: Ali Rahimi, Trevor Darrell</p><p>Abstract: Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpliﬁes the state covariance matrix after incorporating each measurement.</p><p>3 0.64306456 <a title="139-lda-3" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>4 0.49963003 <a title="139-lda-4" href="./nips-2002-Boosting_Density_Estimation.html">46 nips-2002-Boosting Density Estimation</a></p>
<p>Author: Saharon Rosset, Eran Segal</p><p>Abstract: Several authors have suggested viewing boosting as a gradient descent search for a good ﬁt in function space. We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation. We show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well. We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models.</p><p>5 0.49922657 <a title="139-lda-5" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><p>6 0.49856949 <a title="139-lda-6" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>7 0.49709952 <a title="139-lda-7" href="./nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">21 nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>8 0.49618566 <a title="139-lda-8" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>9 0.49420989 <a title="139-lda-9" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>10 0.49373245 <a title="139-lda-10" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>11 0.49327254 <a title="139-lda-11" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>12 0.49284059 <a title="139-lda-12" href="./nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">79 nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>13 0.49228847 <a title="139-lda-13" href="./nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">45 nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>14 0.49194831 <a title="139-lda-14" href="./nips-2002-Bayesian_Monte_Carlo.html">41 nips-2002-Bayesian Monte Carlo</a></p>
<p>15 0.49167421 <a title="139-lda-15" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>16 0.49116218 <a title="139-lda-16" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>17 0.49084234 <a title="139-lda-17" href="./nips-2002-Incremental_Gaussian_Processes.html">110 nips-2002-Incremental Gaussian Processes</a></p>
<p>18 0.48975527 <a title="139-lda-18" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>19 0.48932043 <a title="139-lda-19" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>20 0.48925847 <a title="139-lda-20" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
