<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-15" href="#">nips2002-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</h1>
<br/><p>Source: <a title="nips-2002-15-pdf" href="http://papers.nips.cc/paper/2285-a-probabilistic-model-for-learning-concatenative-morphology.pdf">pdf</a></p><p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>Reference: <a title="nips-2002-15-reference" href="../nips2002_reference/nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. [sent-7, score-0.561]
</p><p>2 The system is composed of a generative probability model and hill-climbing and directed search algorithms. [sent-8, score-0.235]
</p><p>3 By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. [sent-9, score-0.32]
</p><p>4 Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. [sent-11, score-0.221]
</p><p>5 Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique. [sent-12, score-0.157]
</p><p>6 1 Introduction One of the fundamental problems in computational linguistics is adaptation of language processing systems to new languages with minimal reliance on human expertise. [sent-13, score-0.131]
</p><p>7 A ubiquitous component of language processing systems is the morphological analyzer, which determines the properties of morphologically complex words like watches and gladly by inferring their derivation as watch+s and glad+ly. [sent-14, score-0.459]
</p><p>8 The derivation reveals much about the word, such as the fact that glad+ly share syntactic properties with quick+ly and semantic properties with its stem glad. [sent-15, score-0.364]
</p><p>9 While morphological processes can take many forms, the most common are sufﬁxation and preﬁxation (collectively, concatenative morphology). [sent-16, score-0.247]
</p><p>10 In this paper, we present a system for unsupervised inference of morphological derivations of written words, with no prior knowledge of the language in question. [sent-17, score-0.323]
</p><p>11 Speciﬁcally, neither the stems nor the sufﬁxes of the language are given in advance. [sent-18, score-0.277]
</p><p>12 It is applicable to any language for written words lists are available. [sent-20, score-0.215]
</p><p>13 In languages that have been a focus of research in computational linguistics the practical applications are limited, but in languages like Polish, automated analysis of unannotated text corpora has potential applications for information retrieval and other language processing systems. [sent-21, score-0.205]
</p><p>14 In this paper, however, we focus on the problem of unsupervised morphological inference for its inherent interest. [sent-23, score-0.231]
</p><p>15 We are particularly interested in developing automated morphological analysis as a ﬁrst stage of a larger grammatical inference system, and hence we favor a conservative analysis that identiﬁes primarily productive morphological processes (those that can be applied to new words). [sent-31, score-0.434]
</p><p>16 In this paper, we present a probabilistic model and search algorithm for automated analysis of sufﬁxation, along with experiments comparing our system to that of Goldsmith [4]. [sent-32, score-0.163]
</p><p>17 This system, which extends the system of Snover and Brent [5], is designed to detect the ﬁnal stem and sufﬁx break of each word given a list of words. [sent-33, score-0.547]
</p><p>18 It does not distinguish between derivational and inﬂectional sufﬁxation or between the notion of a stem and a root. [sent-34, score-0.429]
</p><p>19 Further, it does not currently have a mechanism to deal with multiple interpretations of a word, or to deal with morphological ambiguity. [sent-35, score-0.182]
</p><p>20 2 Probability Model This section introduces a prior probability distribution over the space of all hypotheses, where a hypothesis is a set of words, each with morphological split separating the stem and sufﬁx. [sent-37, score-0.668]
</p><p>21 The hypothesis is generated by choosing the number of stems and sufﬁxes, the spellings of those stems and sufﬁxes and then the combination of the stems and sufﬁxes. [sent-39, score-0.745]
</p><p>22 The seven steps are presented below, along with their probability distributions and a running example of how a hypothesis could be generated by this process. [sent-40, score-0.122]
</p><p>23 For each stem , choose its length in letters , according to the inverse squared distribution. [sent-51, score-0.384]
</p><p>24 For each from 1 to , generate stem by choosing letters at random, according to the probabilities . [sent-57, score-0.364]
</p><p>25 The probability of any character, , being chosen is obtained from a maximum likelihood estimate: where is the count of among all the hypothesized stems and sufﬁxes and . [sent-60, score-0.288]
</p><p>26 The joint probability of the hypothesized stem and sufﬁx sets is deﬁned by the distribution:    &%   7  ¨ ¨ ¤ ¤ ¤  ©¢ §¦¥ £¡ ¢  %' $  #     7  ¨ ¨ ¤ ¤ ¥ ¥¢ §¦¤  ©  #  ¢ ¡  ' ()   ¢ $ " #! [sent-61, score-0.435]
</p><p>27 ©   %   "  4 6   &%  (3)  7 8'       2  $ 1" 0 % $ 0 0 0   £§ 9 ©    2 3  "  4  5  STEM SUFF 0  ¤¢ ¥£¡  The factorial terms reﬂect the fact that the stems and sufﬁxes could be generated in any order. [sent-62, score-0.217]
</p><p>28 A paradigm is a set of sufﬁxes and the stems that attach to those sufﬁxes and no others. [sent-67, score-0.318]
</p><p>29 Each stem is in exactly one paradigm, and each paradigm has at least one stem. [sent-68, score-0.465]
</p><p>30 The distribution for picking , sufﬁxes for paradigm is: C  #    © § "  '  E    © 7  "  A  @  C  "0 ' C ¤¢ ¥£¡  @ D  is therefore:     @  The joint probability over all paradigms,  (5)  C  7 5' 4 © § E F  "  "0  @ D  C  ¤¢ ¥£¡  Example: = 2, 1, 2 . [sent-73, score-0.129]
</p><p>31 For each paradigm , choose the set of sufﬁxes, PARA that the paradigm will represent. [sent-75, score-0.222]
</p><p>32 For each stem choose the paradigm that the stem will belong in, according to a distribution that favors paradigms with more stems. [sent-82, score-0.939]
</p><p>33 The probability of choosing a paradigm , for a stem is calculated using a maximum likelihood estimate: PARA   9 I¡  9 P  0 %'       9 H¡  79  #  0  where PARA is the set of stems in paradigm . [sent-83, score-0.811]
</p><p>34 PARA  0    ¡  7%  Example: PARA  Combining the results of stages 6 and 7, one can see that the running example would yield the hypothesis consisting of the set of words with sufﬁx breaks, walk+ , walk+s, walk+ed, look+ , look+s, look+ed, far+ , door+ , door+s, cat+ , cat+s . [sent-87, score-0.23]
</p><p>35 Removing the breaks in the words results in the set of input words. [sent-88, score-0.208]
</p><p>36 To ﬁnd the probability for this hypothesis just take of the product of the probabilities from equations (1) to (7). [sent-89, score-0.122]
</p><p>37 Typically one wishes to know the probability of the hypothesis given the data, however in our case such a distribution is not required. [sent-91, score-0.122]
</p><p>38 Equation (8) shows how the probability of the hypothesis given the data could be derived from Bayes law. [sent-92, score-0.122]
</p><p>39 Hyp Data Hyp Hyp Data (8) Data  §  0  §  ¤¢ ¥£¡ ¤ (¡ § ¢  ¤¢ © )(¡ £§  ¤¢ ¥£¡  0  Our search only considers hypotheses consistent with the data. [sent-93, score-0.159]
</p><p>40 The probability of the data Data Hyp , is always , since if you remove the breaks from any given the hypothesis, hypothesis, the input data is produced. [sent-94, score-0.128]
</p><p>41 The prior probability of the data is constant over all hypotheses, thus the probability of the hypothesis given the data reduces to Hyp . [sent-96, score-0.15]
</p><p>42 The prior probability of the hypothesis is given by the above generative process and, among all consistent hypotheses, the one with the greatest prior probability also has the greatest posterior probability. [sent-97, score-0.211]
</p><p>43   §  %  §  ¤¢ ¥(¡  0  ¤¢ )(¡  3 Search This section details a novel search algorithm which is used to ﬁnd a high probability segmentation of the all the words in the input lexicon, . [sent-98, score-0.307]
</p><p>44 The input lexicon is a list of words extracted from a corpus. [sent-99, score-0.331]
</p><p>45 The output of the search is a segmentation of each of the input words into a stem and sufﬁx. [sent-100, score-0.643]
</p><p>46 $  The search algorithm has two phases, which we call the directed search and the hillclimbing search. [sent-101, score-0.248]
</p><p>47 The directed search builds up a consistent hypothesis about the segmentation of all words in the input out of consistent hypothesis about subsets of the words. [sent-102, score-0.527]
</p><p>48 The hill-climbing search further tunes the result of the directed search by trying out nearby hypotheses over all the input words. [sent-103, score-0.339]
</p><p>49 1 Directed Search The directed search is accomplished in two steps. [sent-105, score-0.154]
</p><p>50 The remainder of the input lexicon is added to this sub-hypothesis at which point it becomes the ﬁnal hypothesis. [sent-108, score-0.195]
</p><p>51 ¡  We deﬁne the set of possible sufﬁxes to be the set of terminal substrings, including the empty string , of the words in . [sent-109, score-0.136]
</p><p>52 For each subset of the possible sufﬁxes , there is a maximal set of possible stems (initial substrings) , such that for each and each , is a word in . [sent-110, score-0.298]
</p><p>53 We deﬁne to be the sub-hypothesis in which each input word that can be analyzed as consisting of a stem in and a sufﬁx in is analyzed that way. [sent-111, score-0.471]
</p><p>54 This subhypothesis consists of all pairings of the stems in and the sufﬁxes in with the corresponding morphological breaks. [sent-112, score-0.399]
</p><p>55 We only consider sub-hypotheses which have at least two stems and two sufﬁxes. [sent-114, score-0.217]
</p><p>56 "  "  "  ¥ ¦¤  ¢ ©   ¨  ¢     ¢ £   " § ¤  ¨  $  9  ¤ ¤ § ¢ §  ¥ §  $  ¨  "  For each sub-hypothesis, , there is a corresponding null hypothesis, , which has the same set of words as , but in which all the words are hypothesized to consist of the  ¨  word as the stem and as the sufﬁx. [sent-115, score-0.791]
</p><p>57 By beginning at the node representing no sufﬁxes, one can apply standard graph search techniques, such as a beam search or a best ﬁrst search to ﬁnd the best scoring nodes without visiting all nodes. [sent-120, score-0.43]
</p><p>58 While one cannot guarantee that such approaches perform exactly the same as examining all sub-hypotheses, initial experiments using a beam search with a beam size equal to , with a of 100, show that the best sub-hypotheses are found with a signiﬁcant decrease in the number of nodes visited. [sent-121, score-0.22]
</p><p>59 '    '    ¡ £   '    ¡ ¢   ¡  ¡  ¡  ¡  ¡  The highest scoring sub-hypotheses are incrementally combined in order to create a hypothesis over the complete set of input words. [sent-123, score-0.191]
</p><p>60 We iteratively remove the highest scoring hypothesis from . [sent-127, score-0.174]
</p><p>61 The words in are added to each of the remaining sub-hypotheses in , and their null hypotheses, with their morphological breaks from . [sent-128, score-0.395]
</p><p>62 If a word in was already in the morphological break from overrides the one from . [sent-129, score-0.314]
</p><p>63 All of the sub-hypotheses are now rescored, as the words in them have changed. [sent-130, score-0.136]
</p><p>64 All words in that are not in are added to with sufﬁx . [sent-134, score-0.136]
</p><p>65 2 Hill Climbing Search The hill climbing search further optimizes the probability of the hypothesis by moving stems to new nodes. [sent-136, score-0.511]
</p><p>66 For each possible sufﬁx , and each node , the search attempts to add to . [sent-137, score-0.126]
</p><p>67 This means that all stems in that can take the sufﬁx are moved to a new node, , which represents all the sufﬁxes of and . [sent-138, score-0.239]
</p><p>68 This is analogous to pushing stems to adjacent nodes in a directed graph. [sent-139, score-0.305]
</p><p>69 A stem , can only be moved into a node with the sufﬁx , if the new word, is an observed word in the input lexicon. [sent-140, score-0.525]
</p><p>70 The hill climbing search continues to add and remove sufﬁxes to nodes until the probability of the hypothesis cannot be increased. [sent-143, score-0.35]
</p><p>71 1 Experiment We tested our unsupervised morphology learning system, which we refer to as Paramorph, and Goldsmith’s MDL system, otherwise known as Linguistica1 , on various sized word lists 1 A demo version available on the web, http://humanities. [sent-146, score-0.257]
</p><p>72 The results were evaluated by measuring the accuracy of the stem relations identiﬁed. [sent-154, score-0.403]
</p><p>73 We extracted input lexicons from each corpus, excluding words containing non-alphabetic characters. [sent-155, score-0.162]
</p><p>74 The 100 most common words in each corpus were also excluded, since these words tend to be function words and are not very informative for morphology. [sent-156, score-0.48]
</p><p>75 The experiments in English were also conducted on the 16,000 most common words from the Hansard corpus. [sent-158, score-0.136]
</p><p>76 1 Stem Relation Ideally, we would like to be able to specify the correct morphological break for each of the words in the input, however morphology is laced with ambiguity, and we believe this to be an inappropriate method for this task. [sent-161, score-0.477]
</p><p>77 It seems that the stem “locate” is combined with the sufﬁx “tion”, but in terms of simple concatenation it is unclear if the break should be placed before or after the “t”. [sent-163, score-0.453]
</p><p>78 In an attempt to solve this problem we have developed a new measure of performance, which does not specify the exact morphological split of a word. [sent-164, score-0.182]
</p><p>79 We measure the accuracy of the stems predicted by examining whether two words which are morphologically related are predicted as having the same stem. [sent-165, score-0.552]
</p><p>80 The actual break point for the stems is not evaluated, only whether the words are predicted as having the same stem. [sent-166, score-0.45]
</p><p>81 Two words are related if they share the same immediate stem. [sent-168, score-0.136]
</p><p>82 For example the words “building”, “build”, and “builds” are related since they all have “build” as a stem, just as “building” and “buildings” are related as they both have “building” as a stem. [sent-169, score-0.136]
</p><p>83 Irregular forms of words are also considered to be related even though such relations would be very difﬁcult to detect with a simple concatenation model. [sent-171, score-0.214]
</p><p>84 The stem relation precision measures how many of the relations predicted by the system were correct, while the recall measures how many of the relations present in the data were found. [sent-172, score-0.623]
</p><p>85 Stem relation fscore is an unbiased combination of precision and recall that favors equal scores. [sent-173, score-0.19]
</p><p>86 Due to software difﬁculties we were unable to get Linguistica to run on 500, 1000, and 2000 words in English. [sent-177, score-0.176]
</p><p>87 2  0  500  1000 2k 4k Lexicon Size  8k  Figure 2: Stem Relation Fscores sufﬁxes across lexicon sizes and Linguistica found an increasingly large number of sufﬁxes, predicting over 700 different sufﬁxes in the 16,000 word English lexicon. [sent-190, score-0.268]
</p><p>88 Figure 2 shows the fscores using the stem relation metric for various sizes of English and Polish input lexicons. [sent-191, score-0.491]
</p><p>89 Paramorph maintains a very high precision across lexicon sizes in both languages, whereas the precision of Linguistica decreases considerably at larger lexicon sizes. [sent-192, score-0.434]
</p><p>90 However Linguistica shows an increasing recall as the lexicon size increases, with Paramorph having a decreasing recall as lexicon size increases, though the recall of Linguistica in Polish is consistently lower than the Paramorph’s recall. [sent-193, score-0.428]
</p><p>91 Sufﬁxes -a -e -ego -ej -ie -o -y -a -ami -y -e ¸ -cie -li -m -´ c  Stems dziwn chmur siekier gada odda sprzeda  9 9  Table 1: Sample Paradigms in Polish Table 1 shows several of the larger paradigms found by Paramorph when run on 8000 words of Polish. [sent-195, score-0.204]
</p><p>92 The ﬁrst paradigm shown is for the single adjective stem meaning “strange” with numerous inﬂections for gender, number and case, as well as one derivational sufﬁx, “ie” which changes it into an adverb, “strangely”. [sent-196, score-0.53]
</p><p>93 The second paradigm is for the nouns, “cloud” and “ax”, with various case inﬂections and the third paradigm paradigm contains the verbs, “talk”, “return”, and “sell”. [sent-197, score-0.303]
</p><p>94 This is consistent with our goals to create a conservative system for morphological analysis, where the number of false positives is minimized. [sent-202, score-0.214]
</p><p>95 In addition phonology plays a much stronger role in Polish morphology, causing alterations in stems, which are difﬁcult to detect using a concatenative framework. [sent-205, score-0.114]
</p><p>96 5 Discussion Many of the stem relations predicted by Paramorph result from postulating stem and sufﬁx breaks in words that are actually morphologically simple. [sent-206, score-1.076]
</p><p>97 This occurs when the endings of these words resemble other, correct, sufﬁxes. [sent-207, score-0.136]
</p><p>98 In an attempt to deal with this problem we have investigated incorporating semantic information into the probability model since morphologically related words also tend to be semantically related. [sent-208, score-0.267]
</p><p>99 Paramorph performed better for the most part with respect to Fscore than Linguistica, but more importantly, the precision of Linguistica does not approach the precision of our algorithm, particularly on the larger corpus sizes. [sent-211, score-0.15]
</p><p>100 Unsupervised learning of the morphology of a natural language. [sent-234, score-0.108]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xes', 0.452), ('stem', 0.364), ('paramorph', 0.293), ('linguistica', 0.261), ('suf', 0.228), ('polish', 0.228), ('stems', 0.217), ('para', 0.212), ('morphological', 0.182), ('lexicon', 0.169), ('words', 0.136), ('english', 0.111), ('morphology', 0.108), ('paradigm', 0.101), ('brent', 0.098), ('hypothesis', 0.094), ('search', 0.094), ('xation', 0.085), ('morphologically', 0.081), ('word', 0.081), ('corpus', 0.072), ('hyp', 0.071), ('paradigms', 0.068), ('concatenative', 0.065), ('derivational', 0.065), ('ectional', 0.065), ('fscore', 0.065), ('hypotheses', 0.065), ('directed', 0.06), ('language', 0.06), ('scoring', 0.052), ('break', 0.051), ('walk', 0.05), ('unsupervised', 0.049), ('climbing', 0.049), ('fscores', 0.049), ('snover', 0.049), ('suff', 0.049), ('breaks', 0.046), ('predicted', 0.046), ('door', 0.043), ('goldsmith', 0.043), ('hypothesized', 0.043), ('precision', 0.039), ('acl', 0.039), ('relations', 0.039), ('automated', 0.037), ('languages', 0.037), ('beam', 0.036), ('linguistics', 0.034), ('mdl', 0.034), ('relation', 0.034), ('cat', 0.033), ('buildings', 0.033), ('glad', 0.033), ('productive', 0.033), ('subhypotheses', 0.033), ('system', 0.032), ('node', 0.032), ('null', 0.031), ('recall', 0.03), ('causing', 0.03), ('hill', 0.029), ('look', 0.028), ('ections', 0.028), ('hansard', 0.028), ('louis', 0.028), ('suffixes', 0.028), ('remove', 0.028), ('identi', 0.028), ('probability', 0.028), ('nodes', 0.028), ('culties', 0.027), ('examining', 0.026), ('input', 0.026), ('mo', 0.024), ('build', 0.024), ('segmentation', 0.023), ('ly', 0.023), ('matthew', 0.023), ('nal', 0.023), ('michael', 0.022), ('software', 0.022), ('favors', 0.022), ('moved', 0.022), ('semantically', 0.022), ('generative', 0.021), ('discovering', 0.02), ('greatest', 0.02), ('concatenation', 0.02), ('choose', 0.02), ('detect', 0.019), ('incrementally', 0.019), ('lists', 0.019), ('substrings', 0.019), ('building', 0.019), ('washington', 0.018), ('unclear', 0.018), ('sizes', 0.018), ('unable', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="15-tfidf-1" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>2 0.10119895 <a title="15-tfidf-2" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>Author: Alexei Vinokourov, Nello Cristianini, John Shawe-Taylor</p><p>Abstract: The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the ﬁrst and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reﬂect some semantic similarity. Certain patterns of English words that relate to a speciﬁc meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we ﬁrst demonstrate that the correlations detected between the two versions of the corpus are signiﬁcantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reﬂect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signiﬁcantly superior to LSI on the same data.</p><p>3 0.095261283 <a title="15-tfidf-3" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers</p><p>Abstract: We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 1</p><p>4 0.085818179 <a title="15-tfidf-4" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>Author: Jürgen Schmidhuber</p><p>Abstract: Given is a problem sequence and a probability distribution (the bias) on programs computing solution candidates. We present an optimally fast way of incrementally solving each task in the sequence. Bias shifts are computed by program preﬁxes that modify the distribution on their sufﬁxes by reusing successful code for previous tasks (stored in non-modiﬁable memory). No tested program gets more runtime than its probability times the total search time. In illustrative experiments, ours becomes the ﬁrst general system to learn a universal solver for arbitrary disk Towers of Hanoi tasks (minimal solution size ). It demonstrates the advantages of incremental learning by proﬁting from previously solved, simpler tasks involving samples of a simple context free language.   ¦ ¤ ¢ §¥£¡ 1 Brief Introduction to Optimal Universal Search Consider an asymptotically optimal method for tasks with quickly veriﬁable solutions: ¦ ¦  ©  £ £¨ © © ©  © ¦ ¦ ¦   Method 1.1 (L SEARCH ) View the -th binary string as a potential program for a universal Turing machine. Given some problem, for all do: every steps on average execute (if possible) one instruction of the -th program candidate, until one of the programs has computed a solution.   !     © © © ¢</p><p>5 0.056363419 <a title="15-tfidf-5" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>Author: Jaz Kandola, Nello Cristianini, John S. Shawe-taylor</p><p>Abstract: The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such similarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain positive results. 1</p><p>6 0.054762378 <a title="15-tfidf-6" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>7 0.049749337 <a title="15-tfidf-7" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>8 0.048716448 <a title="15-tfidf-8" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>9 0.038000025 <a title="15-tfidf-9" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>10 0.037725702 <a title="15-tfidf-10" href="./nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">191 nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>11 0.03663091 <a title="15-tfidf-11" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>12 0.035478242 <a title="15-tfidf-12" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>13 0.034408949 <a title="15-tfidf-13" href="./nips-2002-Theory-Based_Causal_Inference.html">198 nips-2002-Theory-Based Causal Inference</a></p>
<p>14 0.033948321 <a title="15-tfidf-14" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>15 0.033310432 <a title="15-tfidf-15" href="./nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">145 nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>16 0.031834129 <a title="15-tfidf-16" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>17 0.027769525 <a title="15-tfidf-17" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>18 0.027442768 <a title="15-tfidf-18" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>19 0.027393254 <a title="15-tfidf-19" href="./nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">69 nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>20 0.02704153 <a title="15-tfidf-20" href="./nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">83 nips-2002-Extracting Relevant Structures with Side Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.098), (1, -0.017), (2, 0.003), (3, 0.002), (4, -0.073), (5, 0.05), (6, -0.045), (7, -0.083), (8, 0.009), (9, -0.083), (10, -0.091), (11, 0.001), (12, 0.037), (13, -0.029), (14, -0.065), (15, -0.002), (16, 0.042), (17, -0.005), (18, 0.023), (19, -0.115), (20, -0.046), (21, 0.005), (22, -0.071), (23, 0.015), (24, -0.022), (25, -0.06), (26, -0.017), (27, -0.062), (28, 0.098), (29, 0.059), (30, 0.046), (31, 0.057), (32, 0.018), (33, -0.025), (34, 0.039), (35, 0.14), (36, 0.125), (37, -0.042), (38, -0.045), (39, -0.086), (40, 0.028), (41, -0.051), (42, -0.09), (43, -0.004), (44, 0.077), (45, -0.02), (46, -0.022), (47, -0.018), (48, -0.017), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95575082 <a title="15-lsi-1" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>2 0.63555825 <a title="15-lsi-2" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>Author: Zach Solan, Eytan Ruppin, David Horn, Shimon Edelman</p><p>Abstract: The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity. 1 Motivation Models dealing with the acquisition of syntactic knowledge are sharply divided into two classes, depending on whether they subscribe to some variant of the classical generative theory of syntax, or operate within the framework of “general-purpose” statistical or distributional learning. An example of the former is the model of [2], which attempts to learn syntactic structures such as Functional Category, as stipulated by the Government and Binding theory. An example of the latter model is Elman’s widely used Simple Recursive Network (SRN) [3]. We believe that polarization between statistical and classical (generative, rule-based) approaches to syntax is counterproductive, because it hampers the integration of the stronger aspects of each method into a common powerful framework. Indeed, on the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation, probabilistic learning, and “connectionist” modeling. Yet, generic connectionist architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difﬁcult to train. We present a scheme that acquires “raw” syntactic information construed in a distributional sense, yet also supports the distillation of rule-like regularities out of the accrued statistical knowledge. Our research is motivated by linguistic theories that postulate syntactic structures (and transformations) rooted in distributional data, as exempliﬁed by the work of Zellig Harris [1]. 2 The ADIOS model The ADIOS (Automatic DIstillation Of Structure) model constructs syntactic representations of a sample of language from unlabeled corpus data. The model consists of two elements: (1) a Representational Data Structure (RDS) graph, and (2) a Pattern Acquisition (PA) algorithm that learns the RDS in an unsupervised fashion. The PA algorithm aims to detect patterns — repetitive sequences of “signiﬁcant” strings of primitives occurring in the corpus (Figure 1). In that, it is related to prior work on alignment-based learning [4] and regular expression (“local grammar”) extraction [5] from corpora. We stress, however, that our algorithm requires no pre-judging either of the scope of the primitives or of their classiﬁcation, say, into syntactic categories: all the information needed for its operation is extracted from the corpus in an unsupervised fashion. In the initial phase of the PA algorithm the text is segmented down to the smallest possible morphological constituents (e.g., ed is split off both walked and bed; the algorithm later discovers that bed should be left whole, on statistical grounds).1 This initial set of unique constituents is the vertex set of the newly formed RDS (multi-)graph. A directed edge is inserted between two vertices whenever the corresponding transition exists in the corpus (Figure 2(a)); the edge is labeled by the sentence number and by its within-sentence index. Thus, corpus sentences initially correspond to paths in the graph, a path being a sequence of edges that share the same sentence number. (a) mh mi mk mj (b) ci{j,k}l ml mn mi ck ... cj ml cu cv . Figure 1: (a) Two sequences mi , mj , ml and mi , mk , ml form a pattern ci{j,k}l = mi , {mj , mk }, ml , which allows mj and mk to be attributed to the same equivalence class, following the principle of complementary distributions [1]. Both the length of the shared context and the cohesiveness of the equivalence class need to be taken into account in estimating the goodness of the candidate pattern (see eq. 1). (b) Patterns can serve as constituents in their own right; recursively abstracting patterns from a corpus allows us to capture the syntactic regularities concisely, yet expressively. Abstraction also supports generalization: in this schematic illustration, two new paths (dashed lines) emerge from the formation of equivalence classes associated with cu and cv . In the second phase, the PA algorithm repeatedly scans the RDS graph for Signiﬁcant P atterns (sequences of constituents) ( SP), which are then used to modify the graph (Algorithm 1). For each path pi , the algorithm constructs a list of candidate constituents, ci1 , . . . , cik . Each of these consists of a “preﬁx” (sequence of graph edges), an equivalence class of vertices, and a “sufﬁx” (another sequence of edges; cf. Figure 2(b)). The criterion I for judging pattern signiﬁcance combines a syntagmatic consideration (the pattern must be long enough) with a paradigmatic one (its constituents c1 , . . . , ck must have high mutual information): I (c1 , c2 , . . . , ck ) = 2 e−(L/k) P (c1 , c2 , . . . , ck ) log P (c1 , c2 , . . . , ck ) Πk P (cj ) j=1 (1) where L is the typical context length and k is the length of the candidate pattern; the probabilities associated with a cj are estimated from frequencies that are immediately available 1 We remark that the algorithm can work in any language, with any set of tokens, including individual characters – or phonemes, if applied to speech. Algorithm 1 PA (pattern acquisition), phase 2 1: while patterns exist do 2: for all path ∈ graph do {path=sentence; graph=corpus} 3: for all source node ∈ path do 4: for all sink node ∈ path do {source and sink can be equivalence classes} 5: degree of separation = path index(sink) − path index(source); 6: pattern table ⇐ detect patterns(source, sink, degree of separation, equivalence table); 7: end for 8: end for 9: winner ⇐ get most signiﬁcant pattern(pattern table); 10: equivalence table ⇐ detect equivalences(graph, winner); 11: graph ⇐ rewire graph(graph, winner); 12: end for 13: end while in the graph (e.g., the out-degree of a node is related to the marginal probability of the corresponding cj ). Equation 1 balances two opposing “forces” in pattern formation: (1) the length of the pattern, and (2) the number and the cohesiveness of the set of examples that support it. On the one hand, shorter patterns are likely to be supported by more examples; on the other hand, they are also more likely to lead to over-generalization, because shorter patterns mean less context. A pattern tagged as signiﬁcant is added as a new vertex to the RDS graph, replacing the constituents and edges it subsumes (Figure 2). Note that only those edges of the multigraph that belong to the detected pattern are rewired; edges that belong to sequences not subsumed by the pattern are untouched. This highly context-sensitive approach to pattern abstraction, which is unique to our model, allows ADIOS to achieve a high degree of representational parsimony without sacriﬁcing generalization power. During the pass over the corpus the list of equivalence sets is updated continuously; the identiﬁcation of new signiﬁcant patterns is done using thecurrent equivalence sets (Figure 3(d)). Thus, as the algorithm processes more and more text, it “bootstraps” itself and enriches the RDS graph structure with new SPs and their accompanying equivalence sets. The recursive nature of this process enables the algorithm to form more and more complex patterns, in a hierarchical manner. The relationships among these can be visualized recursively in a tree format, with tree depth corresponding to the level of recursion (e.g., Figure 3(c)). The PA algorithm halts if it processes a given amount of text without ﬁnding a new SP or equivalence set (in real-life language acquisition this process may never stop). Generalization. A collection of patterns distilled from a corpus can be seen as an empirical grammar of sorts; cf. [6], p.63: “the grammar of a language is simply an inventory of linguistic units.” The patterns can eventually become highly abstract, thus endowing the model with an ability to generalize to unseen inputs. Generalization is possible, for example, when two equivalence classes are placed next to each other in a pattern, creating new paths among the members of the equivalence classes (dashed lines in Figure 1(b)). Generalization can also ensue from partial activation of existing patterns by novel inputs. This function is supported by the input module, designed to process a novel sentence by forming its distributed representation in terms of activities of existing patterns (Figure 6). These are computed by propagating activation from bottom (the terminals) to top (the patterns) of the RDS. The initial activities wj of the terminals cj are calculated given the novel input s1 , . . . , sk as follows: wj = max {I(sk , cj )} m=1..k (2) 102: do you see the cat? 101: the cat is eating 103: are you sure? Sentence Number Within-Sentence Index 101_1 101_4 101_3 101_2 101_5 101_6 END her ing show eat play is cat Pam the BEGIN (a) 131_3 131_2 109_7 END ing 121_12 stay 121_10 play 121_8 101_6 109_6 cat the BEGIN 109_5 121_9 eat 109_4 (b) 109_9 101_5 109_8 101_4 101_3 101_2 is 131_1 101_1 121_13 121_11 131_1 131_3 101_1 109_4 PATTERN 230: the cat is {eat, play, stay} -ing 165_1 Equivalence Class 230: stay, eat, play 165_2 221_3 here stay play 171_3 165_3 eat 221_1 we 171_2 they BEGIN (d) END 101_2 109_5 121_9 121_8 171_1 ing stay 131_2 play eat is cat the BEGIN (c) PATTERN 231: BEGIN {they, we} {230} here 221_2 Figure 2: (a) A small portion of the RDS graph for a simple corpus, with sentence #101 (the cat is eat -ing) indicated by solid arcs. (b) This sentence joins a pattern the cat is {eat, play, stay} -ing, in which two others (#109,121) already participate. (c) The abstracted pattern, and the equivalence class associated with it (edges that belong to sequences not subsumed by this pattern, e.g., #131, are untouched). (d) The identiﬁcation of new signiﬁcant patterns is done using the acquired equivalence classes (e.g., #230). In this manner, the system “bootstraps” itself, recursively distilling more and more complex patterns. where I(sk , cj ) is the mutual information between sk and cj . For an equivalence class, the value propagated upwards is the strongest non-zero activation of its members; for a pattern, it is the average weight of the children nodes, on the condition that all the children were activated by adjacent inputs. Activity propagation continues until it reaches the top nodes of the pattern lattice. When the algorithm encounters a novel word, all the members of the terminal equivalence class contribute a value of , which is then propagated upwards as usual. This enables the model to make an educated guess as to the meaning of the unfamiliar word, by considering the patterns that become active (Figure 6(b)). 3 Results We now brieﬂy describe the results of several studies designed to evaluate the viability of the ADIOS model, in which it was exposed to corpora of varying size and complexity. (a) propnoun:</p><p>3 0.59850925 <a title="15-lsi-3" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>Author: Willem H. Zuidema</p><p>Abstract: Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spontaneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed - somewhat unfortunately -</p><p>4 0.58617049 <a title="15-lsi-4" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>Author: David Fass, Jacob Feldman</p><p>Abstract: We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1- The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e). (a) (b) (c) (d) (e) Figure 1: Categories similar to those previously studied. Lines represent contours of equal probability. All except (e) are unimodal. ~http://ruccs.rutgers.edu/~jacob/feldman.html Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey of performance across a range of interesting distributions. For example, Nosofsky has previously examined the</p><p>5 0.54737258 <a title="15-lsi-5" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers</p><p>Abstract: We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 1</p><p>6 0.52366453 <a title="15-lsi-6" href="./nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">42 nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>7 0.50137633 <a title="15-lsi-7" href="./nips-2002-Bayesian_Models_of_Inductive_Generalization.html">40 nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>8 0.50002933 <a title="15-lsi-8" href="./nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">146 nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>9 0.46392453 <a title="15-lsi-9" href="./nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">84 nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>10 0.42240825 <a title="15-lsi-10" href="./nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">112 nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>11 0.40347719 <a title="15-lsi-11" href="./nips-2002-Replay%2C_Repair_and_Consolidation.html">176 nips-2002-Replay, Repair and Consolidation</a></p>
<p>12 0.32228845 <a title="15-lsi-12" href="./nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">85 nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>13 0.29442948 <a title="15-lsi-13" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>14 0.29430544 <a title="15-lsi-14" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>15 0.28039443 <a title="15-lsi-15" href="./nips-2002-Stability-Based_Model_Selection.html">188 nips-2002-Stability-Based Model Selection</a></p>
<p>16 0.27110046 <a title="15-lsi-16" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>17 0.26984307 <a title="15-lsi-17" href="./nips-2002-Learning_Semantic_Similarity.html">125 nips-2002-Learning Semantic Similarity</a></p>
<p>18 0.26444003 <a title="15-lsi-18" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>19 0.25935382 <a title="15-lsi-19" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>20 0.25405699 <a title="15-lsi-20" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.036), (23, 0.012), (42, 0.048), (54, 0.108), (55, 0.023), (57, 0.023), (64, 0.012), (67, 0.011), (68, 0.031), (74, 0.072), (75, 0.39), (87, 0.019), (92, 0.02), (98, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78711134 <a title="15-lda-1" href="./nips-2002-Approximate_Inference_and_Protein-Folding.html">32 nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>Author: Chen Yanover, Yair Weiss</p><p>Abstract: Side-chain prediction is an important subtask in the protein-folding problem. We show that finding a minimal energy side-chain configuration is equivalent to performing inference in an undirected graphical model. The graphical model is relatively sparse yet has many cycles. We used this equivalence to assess the performance of approximate inference algorithms in a real-world setting. Specifically we compared belief propagation (BP), generalized BP (GBP) and naive mean field (MF). In cases where exact inference was possible, max-product BP always found the global minimum of the energy (except in few cases where it failed to converge), while other approximation algorithms of similar complexity did not. In the full protein data set, maxproduct BP always found a lower energy configuration than the other algorithms, including a widely used protein-folding software (SCWRL). 1</p><p>same-paper 2 0.7713818 <a title="15-lda-2" href="./nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">15 nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>3 0.55758989 <a title="15-lda-3" href="./nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">141 nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>Author: Tatyana Sharpee, Nicole C. Rust, William Bialek</p><p>Abstract: unkown-abstract</p><p>4 0.49338436 <a title="15-lda-4" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>Author: Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to deﬁne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “ﬁnance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.</p><p>5 0.39977455 <a title="15-lda-5" href="./nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">28 nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>Author: Elad Schneidman, William Bialek, Michael Ii</p><p>Abstract: A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classiﬁcation is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, without any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach recovers classical results, but also provides clear evidence for subclasses beyond those identiﬁed previously. Further, we ﬁnd that each of the ganglion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells. 1</p><p>6 0.39576209 <a title="15-lda-6" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>7 0.39116254 <a title="15-lda-7" href="./nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">124 nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>8 0.38881311 <a title="15-lda-8" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>9 0.38848874 <a title="15-lda-9" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>10 0.38845712 <a title="15-lda-10" href="./nips-2002-Clustering_with_the_Fisher_Score.html">53 nips-2002-Clustering with the Fisher Score</a></p>
<p>11 0.38838512 <a title="15-lda-11" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>12 0.38623279 <a title="15-lda-12" href="./nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">48 nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>13 0.38548413 <a title="15-lda-13" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>14 0.38538307 <a title="15-lda-14" href="./nips-2002-Real-Time_Particle_Filters.html">169 nips-2002-Real-Time Particle Filters</a></p>
<p>15 0.38492054 <a title="15-lda-15" href="./nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">10 nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>16 0.38467008 <a title="15-lda-16" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>17 0.38416311 <a title="15-lda-17" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>18 0.38416308 <a title="15-lda-18" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>19 0.38292146 <a title="15-lda-19" href="./nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">11 nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>20 0.38281307 <a title="15-lda-20" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
