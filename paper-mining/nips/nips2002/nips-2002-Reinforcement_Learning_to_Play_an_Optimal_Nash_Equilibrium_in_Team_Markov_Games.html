<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-175" href="#">nips2002-175</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</h1>
<br/><p>Source: <a title="nips-2002-175-pdf" href="http://papers.nips.cc/paper/2171-reinforcement-learning-to-play-an-optimal-nash-equilibrium-in-team-markov-games.pdf">pdf</a></p><p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>Reference: <a title="nips-2002-175-reference" href="../nips2002_reference/nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. [sent-6, score-0.391]
</p><p>2 The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. [sent-7, score-0.696]
</p><p>3 So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. [sent-8, score-0.559]
</p><p>4 In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. [sent-9, score-0.603]
</p><p>5 An approach called Nash-Q [9, 6, 8] has been proposed for learning the game structure and the agents’ strategies (to a ﬁxed point called Nash equilibrium where no agent can improve its expected payoff by deviating to a different strategy). [sent-15, score-1.011]
</p><p>6 Even team Markov games (where the agents have common interests) can have multiple Nash equilibria, only some of which are optimal (that is, maximize sum of the agents’ discounted payoffs). [sent-17, score-0.559]
</p><p>7 Claus and Boutilier introduced ﬁctitious play, an equilibrium selection technique in game theory, to RL. [sent-23, score-0.695]
</p><p>8 Their algorithm, joint action learner (JAL) [2], guarantees the convergence to a Nash equilibrium in a team stage game. [sent-24, score-0.642]
</p><p>9 The same problem prevails in other equilibrium-selection approaches in game theory such as adaptive play [18] and the evolutionary model proposed in [7]. [sent-26, score-0.545]
</p><p>10 In RL, learning to play an optimal Nash equilibrium in team Markov games has been posed as one of the important open problems [9]. [sent-30, score-0.638]
</p><p>11 While there have been heuristic approaches to this problm, no existing algorithm has been proposed that is guarenteed to converge to an optimal Nash equilibrium in this setting. [sent-31, score-0.362]
</p><p>12 In this paper, we present optimal adaptive learning (OAL), the ﬁrst algorithm that converge to an optimal Nash equilibrium with probability 1 in any team Markov game (Section 3). [sent-32, score-1.045]
</p><p>13 We denote by the action that policy prescribes in state . [sent-39, score-0.313]
</p><p>14 If the learning policy has the “Greedy in the Limit with Inﬁnite Exploration” (GLIE) property, then will converge to (with either a model-based or model-free approach) and the agent will converge in behavior to an optimal policy [14]. [sent-50, score-0.542]
</p><p>15 2 Multiagent RL in team Markov games when the game is unknown A natural extension of an MDP to multiagent environments is a Markov game (aka. [sent-56, score-1.209]
</p><p>16 In this paper we focus on team Markov games, that are Markov games where each agent receives the same expected payoff (in the presence of noise, different agent may still receive different payoffs at a particular moment. [sent-58, score-0.796]
</p><p>17 In other words, there are no conﬂicts between the agents, but learning the game structure and learning to coordinate are nevertheless highly nontrivial. [sent-60, score-0.487]
</p><p>18 Deﬁnition 1 A team Markov game (aka identical-interest stochastic game) is a tuple , where is a set of n agents; S is a ﬁnite state space; is a joint action space of n agents; is the common expected payoff function; and is a transition function. [sent-61, score-1.11]
</p><p>19 The objective of the agents is to ﬁnd a deterministic joint policy (aka. [sent-62, score-0.414]
</p><p>20 The Q-function, , is the expected sum of discounted payoffs given that the agents play joint action in state and follow policy thereafter. [sent-69, score-0.77]
</p><p>21 is a Nash equilibrium if each individual policy is a best response A joint policy to the others. [sent-74, score-0.565]
</p><p>22 That is, for all , and any individual policy , , where is the joint policy of all agents except agent . [sent-75, score-0.698]
</p><p>23 ) A Nash equilibrium is strict if the inequality above is strict. [sent-79, score-0.329]
</p><p>24 An optimal Nash equilibrium is a Nash equilibrium that gives the agents the maximal expected sum of discounted payoffs. [sent-80, score-0.778]
</p><p>25 In team games, each optimal Nash equilibrium is an optimal joint policy (and there are no other optimal joint policies). [sent-81, score-0.865]
</p><p>26 If we treat as the payoff of joint action in state , we obtain a team game in matrix form. [sent-83, score-1.042]
</p><p>27 An optimal joint action in is an optimal Nash equilibrium of that state game. [sent-85, score-0.648]
</p><p>28 Thus, the task of optimal coordination in a team Markov game boils down to having all the agents play an optimal Nash equilibrium in state games. [sent-86, score-1.382]
</p><p>29 Furthermore, if the payoffs are only expectations over each agent’s noisy payoffs and unknown to the agents before playing, even identiﬁcation of these sub-optimal Nash equilibria during learning is nontrivial. [sent-92, score-0.45]
</p><p>30 3 Optimal adaptive learning (OAL) algorithm We ﬁrst consider the case where agents know the game before playing. [sent-93, score-0.721]
</p><p>31 This enables the learning agents to construct a virtual game (VG) for each state of the team Markov game to eliminate all the strict suboptimal Nash equilibria in that state. [sent-94, score-1.628]
</p><p>32 Let be the payoff that the agents receive from the VG in state for a joint action . [sent-95, score-0.684]
</p><p>33 For example, the VG for the game in Table 1 gives payoff 1 for each optimal Nash equilibrium ( , , and ), and payoff 0 to every other joint action. [sent-97, score-1.148]
</p><p>34 Deﬁnition 2 (Weakly acyclic game [18]) Let be an n-player game in matrix form. [sent-99, score-0.932]
</p><p>35 The best-response graph of takes each joint action as a vertex and connects two vertices and with a directed edge if and only if 1) ; 2) there exists exactly one agent such that is a best response to and . [sent-100, score-0.448]
</p><p>36 We say the game is weakly acyclic if in its best-response graph, from any initial vertex , there exists a directed path to some vertex from which there is no outgoing edge. [sent-101, score-0.691]
</p><p>37 To tackle the equilibrium selection problem for weakly acyclic games, Young [18] proposed a learning algorithm called adaptive play (AP), which works as follows. [sent-102, score-0.501]
</p><p>38 Let be a joint action played at time over an n-player game in matrix form. [sent-103, score-0.665]
</p><p>39 When , each agent randomly chooses its , each agent looks back at the most recent plays and randomly (without replacement) selects samples from . [sent-111, score-0.39]
</p><p>40 Let be the number of times that a reduced joint action (a joint action without agent ’s individual action) appears in the samples at . [sent-112, score-0.589]
</p><p>41 Let be agent ’s payoff given that joint action has been played. [sent-113, score-0.527]
</p><p>42 t its individual action as , and then randomly chooses an action from a set of best responses: . [sent-116, score-0.311]
</p><p>43 Young showed that AP in a weakly acyclic game converges to a strict Nash equilibrium w. [sent-117, score-0.941]
</p><p>44 Thus, AP on the VG for the game in Table 1 leads to an equilibrium with payoff 1 which is actually an optimal Nash equilibrium for the original game. [sent-120, score-1.14]
</p><p>45 Unfortunately, this does not extend to all VGs because not all VGs are weakly acyclic: in a VG without any strict Nash equilibrium, AP may not converge to the strategy proﬁle with payoff 1. [sent-121, score-0.373]
</p><p>46 In order to address more general settings, we now modify the notion of weakly acyclic game and adaptive play to accommodate weak optimal Nash equilibria. [sent-122, score-0.728]
</p><p>47 t a biased set (WAGB)): Let be a set containing some of the Nash equilibria of a game (and no other joint policies). [sent-125, score-0.704]
</p><p>48 Game is a WAGB if, from any initial vertex , there exists a directed path to either a Nash equilibrium inside or a strict Nash equilibrium. [sent-126, score-0.427]
</p><p>49 We can convert any VG to a WAGB by setting the biased set to include all joint policies that give payoff 1 (and no other joint policies). [sent-127, score-0.429]
</p><p>50 It enables each agent to deterministically select a best-response action once any Nash equilibrium in the biased set is attained (even if there exist several best responses when the Nash equilibrium is not strict). [sent-129, score-0.85]
</p><p>51 Let be the biased set composed of some Nash equilibria of a game in matrix form. [sent-133, score-0.619]
</p><p>52 If (1) there exists a joint action , and , and (2) there exists at least one joint action such that and , then agent chooses its best-response action such that and . [sent-135, score-0.793]
</p><p>53 That is, is contained in the most recent play of a Nash equilibrium inside . [sent-136, score-0.329]
</p><p>54 On the other hand, if the two conditions above are not met, then agent chooses its best-response action in the same way as AP. [sent-137, score-0.311]
</p><p>55 1 to either a Nash equilibrium in or a strict Nash equilibrium. [sent-140, score-0.329]
</p><p>56 So far we tackled learning of coordination in team Markov games where the game structure is known. [sent-141, score-0.753]
</p><p>57 Our real interests are in learning when the game is unknown. [sent-142, score-0.478]
</p><p>58 All the joint actions belonging to the set are treated as optimal Nash equilibria in the virtual game which give agents payoff 1. [sent-158, score-1.217]
</p><p>59 As we will present thereafter, we craft carefully using an understanding of the convergence rate of a model-based RL algorithm that is used to learn the game structure. [sent-165, score-0.494]
</p><p>60 Learning of coordination policy If , randomly select an action, otherwise do (a) Update the virtual game at state : if and Set . [sent-167, score-0.807]
</p><p>61 Calculate expected payoff of individual action over the virtual game at current state  1 )   1    1 #)   ¢ £      . [sent-178, score-0.908]
</p><p>62 If conditions 1) and 2) of BAP are met, choose a best-response action with respect to the biased set Otherwise, randomly select a best-response action from . [sent-182, score-0.357]
</p><p>63 Off-policy learning of game structure (a) Observe state transition and payoff i. [sent-201, score-0.73]
</p><p>64   $       and time :    as follows:  response set at state  1 0 £ ' 2¦G¢%  is the number of times a joint action has been played in state by time . [sent-205, score-0.379]
</p><p>65 is the number of times that a joint action appears in agent ’s samples (at time ) from the most recent joint actions taken in state . [sent-207, score-0.596]
</p><p>66 1 we show that OAL agents learn optimal coordination if the game is known. [sent-212, score-0.79]
</p><p>67 Speciﬁcally, we show that BAP against a WAGB with known game structure converges to a Nash equilibrium under GLIE exploration. [sent-213, score-0.73]
</p><p>68 2 we show that OAL agents will learn the game structure. [sent-215, score-0.673]
</p><p>69 Speciﬁcally, any virtual game can be converted to a WAGB which will be learned surely. [sent-216, score-0.531]
</p><p>70 3 which shows that OAL agents will learn the game structure and optimal coordination. [sent-218, score-0.735]
</p><p>71 1 Learning to coordinate in a known game In this section, we ﬁrst model our biased adaptive play (BAP) algorithm with best-response action selection as a stationary Markov chain. [sent-226, score-0.819]
</p><p>72 The only exception is that all the states with being either a member of the biased set or a strict Nash equilibrium are grouped into a unique terminal state . [sent-233, score-0.552]
</p><p>73 If is only a weak Nash equilibrium (in this case, ), BAP biases each agent to choose its most recent action because conditions (1) and (2) of BAP are satisﬁed. [sent-243, score-0.532]
</p><p>74 Therefore, if agents come into , they will be stuck in a particular state forever instead of cycling around multiple states in . [sent-246, score-0.331]
</p><p>75 Let L(a) be the length of the shortest directed path in the best-response graph of G from a joint action a to either an absorbing vertex or a vertex in D, and let . [sent-251, score-0.4]
</p><p>76 1, biased adaptive play in G converges to either a strict Nash equilibrium or a Nash equilibrium in D. [sent-254, score-0.805]
</p><p>77 2 BAP with GLIE exploration as a nonstationary Markov chain Without knowing game structure, the learners need to use exploration to estimate their payoffs. [sent-258, score-0.679]
</p><p>78 Second, in Young’s model, it is possible to have several absorbing states while in our model, at most one absorbing state exists (for any team game, our model has exactly one absorbing state). [sent-269, score-0.475]
</p><p>79 1, BAP with GLIE exploration (and ) converges to either a strict Nash equilibrium or a Nash equilibrium in D. [sent-283, score-0.714]
</p><p>80 2 Learning the virtual game So far, we have shown that if the game structure is known in a WAGB, then BAP will converge to the terminal state. [sent-285, score-1.056]
</p><p>81 To prove optimal convergence of the OAL algorithm, we need to further demonstrate that 1) every virtual game is a WAGB, and 2) in OAL, the “temporary” virtual game will converge to the “correct” virtual game w. [sent-286, score-1.758]
</p><p>82 The ﬁrst of these two issues is handled by the following lemma: Lemma 4 The virtual game VG of any n-player team state game is a weakly acyclic game w. [sent-289, score-1.761]
</p><p>83 (By the deﬁnition of a virtual game, there are no strict Nash equilibria other than optimal ones. [sent-292, score-0.349]
</p><p>84 Lemma 4 implies that BAP in a known virtual game with GLIE exploration will converge to an optimal Nash equilibrium. [sent-294, score-0.727]
</p><p>85 This is because (by Theorem 3) BAP in a WAGB will converge to either a Nash equilibrium in a biased set or a strict Nash equilibrium, and (by Lemma 4) any virtual game is a WAGB with all such Nash equilibria being optimal. [sent-295, score-1.082]
</p><p>86 They show that OAL will cause agents to obtain the correct virtual game almost surely. [sent-297, score-0.747]
</p><p>87 Lemma 6 states that if the criterion for including a joint action among the -optimal joint actions in OAL is not made strict too quickly (quicker than the iterated logarithm), then agents will identify all optimal joint actions with probability one. [sent-306, score-0.885]
</p><p>88 3 Main convergence theorem Now we are ready to prove that OAL converges to an optimal Nash equilibrium in any team Markov game, even when the game structure is unknown. [sent-313, score-1.044]
</p><p>89 The idea is to show that the OAL agents learn the game structure (VGs) and the optimal coordination policy (over these VGs). [sent-314, score-0.903]
</p><p>90 OAL tackles these two learning problems simultaneously—speciﬁcally, it interleaves BAP (with GLIE exploration) with learning of game structure. [sent-315, score-0.47]
</p><p>91 X  w x1  X    ¡%  4  Theorem 7 (Optimal convergence) In any team Markov game among agents, if (1) , and (2) satisﬁes Lemma 6, then the OAL algorithm converges to an optimal Nash equilibrium w. [sent-321, score-0.949]
</p><p>92 According to [1], a team Markov game can be decomposed into a sequence of state games. [sent-325, score-0.671]
</p><p>93 The optimal equilibria of these state games form the optimal policy for the game. [sent-326, score-0.504]
</p><p>94 Thus, it is sufﬁcient to only prove that the OAL algorithm will converge to the optimal policy over individual state games w. [sent-330, score-0.419]
</p><p>95 If occurs and Condition (1) of the theorem is satisﬁed, by Theorem 3, OAL will converge to either a strict Nash equilibrium or a Nash equilibrium in the biased set w. [sent-336, score-0.719]
</p><p>96 Furthermore, by Lemma 4, we know that the biased set contains all of the optimal Nash equilibria (and nothing else), and there are no strict Nash equilibria outside the biased set. [sent-339, score-0.535]
</p><p>97 Therefore, if occurs, then OAL converges to an optimal Nash equilibrium w. [sent-340, score-0.354]
</p><p>98 Let be any positive constant, and let be the event that the agents play an optimal joint action at a given state for all . [sent-343, score-0.669]
</p><p>99 Therefore, OAL converges to an optimal Nash equilibrium w. [sent-347, score-0.354]
</p><p>100 In this paper, we present OAL, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. [sent-351, score-0.511]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nash', 0.506), ('game', 0.438), ('oal', 0.266), ('equilibrium', 0.239), ('bap', 0.222), ('agents', 0.216), ('glie', 0.163), ('payoff', 0.162), ('team', 0.157), ('agent', 0.156), ('wagb', 0.133), ('action', 0.124), ('policy', 0.113), ('equilibria', 0.104), ('exploration', 0.093), ('virtual', 0.093), ('strict', 0.09), ('markov', 0.089), ('multiagent', 0.089), ('games', 0.087), ('joint', 0.085), ('biased', 0.077), ('play', 0.077), ('state', 0.076), ('rl', 0.068), ('weakly', 0.065), ('lemma', 0.063), ('absorbing', 0.062), ('optimal', 0.062), ('actions', 0.057), ('payoffs', 0.057), ('acyclic', 0.056), ('coordination', 0.055), ('converges', 0.053), ('vgs', 0.051), ('vu', 0.051), ('vg', 0.048), ('terminal', 0.046), ('reinforcement', 0.043), ('converge', 0.041), ('ap', 0.041), ('transition', 0.038), ('stationary', 0.038), ('convergence', 0.037), ('vertex', 0.034), ('theorem', 0.033), ('exists', 0.032), ('chain', 0.031), ('chooses', 0.031), ('nnn', 0.031), ('sandholm', 0.031), ('adaptive', 0.03), ('let', 0.029), ('young', 0.029), ('prove', 0.025), ('interests', 0.024), ('successor', 0.024), ('nonstationary', 0.024), ('ik', 0.024), ('states', 0.024), ('discounted', 0.022), ('receive', 0.021), ('know', 0.021), ('ctitious', 0.02), ('guarenteed', 0.02), ('ppomy', 0.02), ('xiaofeng', 0.02), ('fu', 0.02), ('policies', 0.02), ('ex', 0.02), ('proof', 0.019), ('learn', 0.019), ('nition', 0.018), ('played', 0.018), ('icting', 0.018), ('econometrica', 0.018), ('selection', 0.018), ('coordinate', 0.017), ('randomly', 0.017), ('directed', 0.017), ('replacement', 0.016), ('sq', 0.016), ('learning', 0.016), ('tuple', 0.016), ('satis', 0.016), ('multiple', 0.015), ('select', 0.015), ('ww', 0.015), ('boutilier', 0.015), ('lexicographic', 0.015), ('path', 0.015), ('strategy', 0.015), ('individual', 0.015), ('meet', 0.014), ('icml', 0.014), ('stochastic', 0.014), ('cooperative', 0.014), ('wang', 0.014), ('itr', 0.014), ('recent', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="175-tfidf-1" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>2 0.52199703 <a title="175-tfidf-2" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>3 0.32843432 <a title="175-tfidf-3" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>Author: Luis E. Ortiz, Michael Kearns</p><p>Abstract: We introduce NashProp, an iterative and local message-passing algorithm for computing Nash equilibria in multi-player games represented by arbitrary undirected graphs. We provide a formal analysis and experimental evidence demonstrating that NashProp performs well on large graphical games with many loops, often converging in just a dozen iterations on graphs with hundreds of nodes. NashProp generalizes the tree algorithm of (Kearns et al. 2001), and can be viewed as similar in spirit to belief propagation in probabilistic inference, and thus complements the recent work of (Vickrey and Koller 2002), who explored a junction tree approach. Thus, as for probabilistic inference, we have at least two promising general-purpose approaches to equilibria computation in graphs.</p><p>4 0.1997003 <a title="175-tfidf-4" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>5 0.19167641 <a title="175-tfidf-5" href="./nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><p>6 0.10349343 <a title="175-tfidf-6" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>7 0.10010564 <a title="175-tfidf-7" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>8 0.089198217 <a title="175-tfidf-8" href="./nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">61 nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>9 0.074513875 <a title="175-tfidf-9" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>10 0.073695995 <a title="175-tfidf-10" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>11 0.071632631 <a title="175-tfidf-11" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>12 0.066604428 <a title="175-tfidf-12" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>13 0.065644473 <a title="175-tfidf-13" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>14 0.059770286 <a title="175-tfidf-14" href="./nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">82 nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>15 0.051323883 <a title="175-tfidf-15" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>16 0.045853589 <a title="175-tfidf-16" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>17 0.044771057 <a title="175-tfidf-17" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>18 0.039933555 <a title="175-tfidf-18" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>19 0.035129614 <a title="175-tfidf-19" href="./nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">9 nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>20 0.030709049 <a title="175-tfidf-20" href="./nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.114), (1, -0.021), (2, -0.365), (3, -0.119), (4, -0.028), (5, -0.138), (6, 0.079), (7, -0.173), (8, -0.131), (9, -0.307), (10, 0.255), (11, 0.333), (12, 0.259), (13, -0.099), (14, -0.012), (15, 0.074), (16, -0.088), (17, 0.029), (18, -0.079), (19, 0.077), (20, 0.09), (21, -0.011), (22, 0.122), (23, 0.073), (24, 0.07), (25, 0.037), (26, -0.007), (27, -0.03), (28, -0.016), (29, -0.005), (30, -0.01), (31, 0.015), (32, -0.033), (33, 0.002), (34, -0.051), (35, -0.019), (36, 0.008), (37, 0.009), (38, -0.001), (39, 0.026), (40, -0.013), (41, -0.007), (42, -0.002), (43, -0.037), (44, -0.004), (45, -0.001), (46, -0.022), (47, 0.021), (48, 0.026), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98141265 <a title="175-lsi-1" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>same-paper 2 0.97512275 <a title="175-lsi-2" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>3 0.81875199 <a title="175-lsi-3" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>Author: Luis E. Ortiz, Michael Kearns</p><p>Abstract: We introduce NashProp, an iterative and local message-passing algorithm for computing Nash equilibria in multi-player games represented by arbitrary undirected graphs. We provide a formal analysis and experimental evidence demonstrating that NashProp performs well on large graphical games with many loops, often converging in just a dozen iterations on graphs with hundreds of nodes. NashProp generalizes the tree algorithm of (Kearns et al. 2001), and can be viewed as similar in spirit to belief propagation in probabilistic inference, and thus complements the recent work of (Vickrey and Koller 2002), who explored a junction tree approach. Thus, as for probabilistic inference, we have at least two promising general-purpose approaches to equilibria computation in graphs.</p><p>4 0.59059739 <a title="175-lsi-4" href="./nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><p>5 0.36956775 <a title="175-lsi-5" href="./nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">3 nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>6 0.33866113 <a title="175-lsi-6" href="./nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">13 nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>7 0.3317546 <a title="175-lsi-7" href="./nips-2002-Learning_to_Take_Concurrent_Actions.html">134 nips-2002-Learning to Take Concurrent Actions</a></p>
<p>8 0.32975283 <a title="175-lsi-8" href="./nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">185 nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>9 0.17837474 <a title="175-lsi-9" href="./nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">33 nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>10 0.17496988 <a title="175-lsi-10" href="./nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">61 nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>11 0.15385234 <a title="175-lsi-11" href="./nips-2002-Adaptive_Caching_by_Refetching.html">20 nips-2002-Adaptive Caching by Refetching</a></p>
<p>12 0.15265381 <a title="175-lsi-12" href="./nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>13 0.15263973 <a title="175-lsi-13" href="./nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>14 0.14928265 <a title="175-lsi-14" href="./nips-2002-Value-Directed_Compression_of_POMDPs.html">205 nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>15 0.1482453 <a title="175-lsi-15" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>16 0.1426698 <a title="175-lsi-16" href="./nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">44 nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>17 0.13429162 <a title="175-lsi-17" href="./nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">65 nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>18 0.13248946 <a title="175-lsi-18" href="./nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">179 nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>19 0.11691833 <a title="175-lsi-19" href="./nips-2002-Prediction_and_Semantic_Association.html">163 nips-2002-Prediction and Semantic Association</a></p>
<p>20 0.11488705 <a title="175-lsi-20" href="./nips-2002-Identity_Uncertainty_and_Citation_Matching.html">107 nips-2002-Identity Uncertainty and Citation Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.019), (23, 0.026), (35, 0.198), (42, 0.065), (54, 0.094), (55, 0.021), (58, 0.013), (67, 0.012), (68, 0.015), (74, 0.257), (83, 0.031), (92, 0.041), (98, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87546253 <a title="175-lda-1" href="./nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><p>2 0.81567746 <a title="175-lda-2" href="./nips-2002-Efficient_Learning_Equilibrium.html">78 nips-2002-Efficient Learning Equilibrium</a></p>
<p>Author: Ronen I. Brafman, Moshe Tennenholtz</p><p>Abstract: We introduce efficient learning equilibrium (ELE), a normative approach to learning in non cooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become irrational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1</p><p>3 0.81421781 <a title="175-lda-3" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>Author: Martin Szummer, Tommi S. Jaakkola</p><p>Abstract: Classiﬁcation with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P (x)), to further constrain the conditional P (y|x) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P (x). We develop algorithms for solving the regularization problem for ﬁnite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases.</p><p>4 0.80983454 <a title="175-lda-4" href="./nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">90 nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>Author: Martin H. Law, Anil K. Jain, Mário Figueiredo</p><p>Abstract: There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difﬁcult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the ﬁrst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami’s mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classiﬁed as a “wrapper”, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.</p><p>5 0.80836922 <a title="175-lda-5" href="./nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">66 nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>Author: Terry Elliott, Jörg Kramer</p><p>Abstract: A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been proposed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate moving patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of reﬁned topography and ocular dominance columns, even in the presence of signiﬁcant amounts of spontaneous activity and ﬁxed-pattern noise in the sensors.</p><p>6 0.8081131 <a title="175-lda-6" href="./nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">57 nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>7 0.80336809 <a title="175-lda-7" href="./nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>8 0.7375952 <a title="175-lda-8" href="./nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">137 nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>9 0.71792316 <a title="175-lda-9" href="./nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">139 nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>10 0.71272117 <a title="175-lda-10" href="./nips-2002-Bayesian_Image_Super-Resolution.html">39 nips-2002-Bayesian Image Super-Resolution</a></p>
<p>11 0.6999312 <a title="175-lda-11" href="./nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">152 nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>12 0.69681829 <a title="175-lda-12" href="./nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">132 nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>13 0.69566596 <a title="175-lda-13" href="./nips-2002-Dynamic_Structure_Super-Resolution.html">74 nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>14 0.69304281 <a title="175-lda-14" href="./nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">89 nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>15 0.68199807 <a title="175-lda-15" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>16 0.68156451 <a title="175-lda-16" href="./nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">122 nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>17 0.66930467 <a title="175-lda-17" href="./nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">173 nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>18 0.66444999 <a title="175-lda-18" href="./nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">2 nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>19 0.66204768 <a title="175-lda-19" href="./nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">100 nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>20 0.65568745 <a title="175-lda-20" href="./nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
