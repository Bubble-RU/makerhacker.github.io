<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 nips-2009-Streaming Pointwise Mutual Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-233" href="#">nips2009-233</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>233 nips-2009-Streaming Pointwise Mutual Information</h1>
<br/><p>Source: <a title="nips-2009-233-pdf" href="http://papers.nips.cc/paper/3730-streaming-pointwise-mutual-information.pdf">pdf</a></p><p>Author: Benjamin V. Durme, Ashwin Lall</p><p>Abstract: Recent work has led to the ability to perform space efﬁcient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data. 1</p><p>Reference: <a title="nips-2009-233-reference" href="../nips2009_reference/nips-2009-Streaming_Pointwise_Mutual_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. [sent-2, score-0.163]
</p><p>2 We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. [sent-3, score-0.105]
</p><p>3 1  Introduction  Recent work has led to the ability to perform space efﬁcient counting over large vocabularies [Talbot, 2009; Van Durme and Lall, 2009]. [sent-5, score-0.15]
</p><p>4 We explore what a data structure of this type means for the computation of associativity scores, or pointwise mutual information, in a streaming context. [sent-8, score-0.163]
</p><p>5 We show that approximate k-best PMI rank lists may be maintained online, with high accuracy, both in theory and in practice. [sent-9, score-0.19]
</p><p>6 Further, we assume that the sets X and Y are so large that it is infeasible to explicitly maintain precise counts for every such pair on a single machine (e. [sent-12, score-0.058]
</p><p>7 We deﬁne the pointwise mutual information (PMI) of a pair x and y to be PMI(x, y) ≡ lg  P (x, y) P (x)P (y)  where these (empirical) probabilities are computed over a particular data set of interest. [sent-15, score-0.405]
</p><p>8 Our goal in this paper is to estimate these top-k sets in a streaming fashion, i. [sent-18, score-0.062]
</p><p>9 , the data is being accessed by crawling the web and it is infeasible to buffer all the crawled results. [sent-25, score-0.191]
</p><p>10 As mentioned earlier, there has been considerable work in keeping track of the counts of a large number of items succinctly. [sent-26, score-0.081]
</p><p>11 Trigger Models Rosenfeld [1994] was interested in collecting trigger pairs, A, B , such that the presence of A in a document is likely to “trigger” an occurrence of B. [sent-36, score-0.092]
</p><p>12 There the concern was in ﬁnding the most useful triggers overall, and thus pairs were favored based on high average mu¯¯ ¯ P (AB) P (AB) P (AB) ¯¯ ¯ tual information; I(A, B) = P (AB) lg P (A)P (B) + P (AB) lg P (A)P (B) + P (AB) lg P (A)P (B) + ¯ ¯ ¯ (¯ ¯ lg P¯AB) . [sent-37, score-1.418]
</p><p>13 = ¯ S If we take x to range over cues, and y to be a memory structure, then in our work here we are storing the identities of the top-k memory structures for a given cue x, as according to strength of associativity. [sent-44, score-0.094]
</p><p>14 An obvious ﬁrst attempt at an algorithm for this problem is to use approximate counters to estimate the PMI for each pair in 3 Rosenfeld: . [sent-46, score-0.09]
</p><p>15 unlike in a bigram model, where the number of different consecutive word pairs is much less than [the vocabulary] V 2 , the number of word pairs where both words occurred in the same document is a signiﬁcant fraction of V 2 . [sent-49, score-0.143]
</p><p>16 2  the stream and maintain the top-k for each x using a priority queue. [sent-52, score-0.238]
</p><p>17 Example 1 (probability of y changes): Consider the stream xy xy xy xz wz | wy wy wy wy wy which we have divided in half. [sent-54, score-0.631]
</p><p>18 After the ﬁrst half, y is best for x since PMI(x, y) = lg lg (5/4) and PMI(x, z) = lg  1/5 (4/5)(2/5)  3/5 (4/5)(3/5)  =  = lg (5/8). [sent-55, score-1.34]
</p><p>19 At the end of the second half of the stream,  3/10 1/10 z is best for x since PMI(x, y) = lg (4/10)(8/10) ≈ lg (0. [sent-56, score-0.67]
</p><p>20 However, during the second half of the stream we never encounter x and hence never update its value. [sent-59, score-0.197]
</p><p>21 Example 2 (probability of x changes): Consider the stream pd py py xy xd in which we are interested in only the top PMI tuples for x. [sent-63, score-0.265]
</p><p>22 When we see xy in the stream, 1/4 PMI(x, y) = lg (1/4)(3/4) ≈ lg (1. [sent-64, score-0.738]
</p><p>23 33), and when we see xd in the stream, PMI(x, d) =  lg  1/5 (2/5)(2/5)  = lg (1. [sent-65, score-0.67]
</p><p>24 However, xy’s PMI is now  1/5 PMI(x, y) = lg (2/5)(3/5) = lg (0. [sent-68, score-0.67]
</p><p>25 833) which means that we should replace xy with xd. [sent-69, score-0.068]
</p><p>26 Theorem 1: Any algorithm that explicitly maintains the top-k PMIs for all x ∈ X in a stream of length at most n (where |X| ∈ o(n)) in a single pass requires Ω(n|X|) time. [sent-75, score-0.221]
</p><p>27 We will prove this theorem using the following lemma: Lemma 1: Any algorithm that explicitly maintains the top-k PMIs of |X| = p + 1 items over a stream of length at most n = 2r + 2p + 1 in a single pass requires Ω(pr) time. [sent-76, score-0.244]
</p><p>28 Proof of Lemma 1: Let us take the length of the stream to be n, where we assume without loss of generality that n is odd. [sent-77, score-0.221]
</p><p>29 , xp y2 , xp+1 y1  xp+1 y2 , xp+1 y2 ,     xp+1 y1 , xp+1 y1 , xp+1 y2 , xp+1 y2 , r times   . [sent-87, score-0.141]
</p><p>30 After xp+1 y1 appears in the stream for the ﬁrst time, it should be evident that all the elements of Xp have a higher PMI with y2 than y1 . [sent-96, score-0.219]
</p><p>31 Now, the current PMI for each element of Xp must be correct at any point in the stream since the stream may terminate at any time. [sent-99, score-0.417]
</p><p>32 , xp will change at least r times in the course of this stream, for 3  a total of at least pr operations. [sent-103, score-0.165]
</p><p>33 5  Algorithm  The lower bound from the previous section shows that, when solving the PMI problem, the best one can do is effectively cross-check the PMI for every possible x ∈ X for each item in the stream. [sent-111, score-0.062]
</p><p>34 Our algorithm uses approximate counting to retain the counts of all pairs of items x, y in a data structure Cxy . [sent-116, score-0.278]
</p><p>35 We keep exact counts of all x and y since this takes considerably less space. [sent-117, score-0.058]
</p><p>36 We assume Cxy to be based on recent work in space efﬁcient counting methods for streamed text data [Talbot, 2009; Van Durme and Lall, 2009]. [sent-119, score-0.128]
</p><p>37 For our implementation we used TOMB counters [Van Durme and Lall, 2009] which approximate counts by storing values in log-scale. [sent-120, score-0.18]
</p><p>38 These log-scale counts are maintained in unary within layers of Bloom ﬁlters [Bloom, 1970] (Figure 1) that can be probabilistically updated using a small base (Figure 2); each occurrence of an item in the stream prompts a probabilistic update to its value, dependent on the base. [sent-121, score-0.31]
</p><p>39 By tuning this base, one can trade off between the accuracy of the counts and the space savings of approximate counting. [sent-122, score-0.093]
</p><p>40 , the issue in Example 2 in the previous section), we divide the stream up into ﬁxed-size buffers B and re-compute the PMIs for all pairs seen within each buffer (see Algorithm 1). [sent-130, score-0.422]
</p><p>41 Updating counts for x, y and x, y is constant time per element in the stream. [sent-131, score-0.081]
</p><p>42 Insertion into a k-best priority queue requires O(lg k) operations. [sent-132, score-0.063]
</p><p>43 Per interval, we perform in the worst case one insertion per new element observed, along with one insertion for each element stored in the previous rank lists. [sent-133, score-0.183]
</p><p>44 As long as |B| ≥ |X|k, updating rank lists costs O(|B|lg k) per interval. [sent-134, score-0.126]
</p><p>45 5 The algorithm therefore requires O(n + n lg k) = O(n lg k) time, where n is the length of the stream. [sent-135, score-0.694]
</p><p>46 A beneﬁt of our algorithm is that this can be kept signiﬁcantly smaller than |X| × |Y |,6 since in practice, |Y | lg k. [sent-139, score-0.335]
</p><p>47 , the extra cost for reinserting elements from the previous rank lists is amortized over the buffer length. [sent-142, score-0.317]
</p><p>48 In giving a bound on this error, we will make two assumptions: (i) the PMI for a given x follows a Zipﬁan distribution (something that we observed in our data), and (ii) the items in the stream are drawn independently from some underlying distribution (i. [sent-152, score-0.256]
</p><p>49 Both these assumptions together help us to sidestep the lower bound proved earlier and demonstrate that our single-pass algorithm will perform well on real language data sets. [sent-158, score-0.064]
</p><p>50 We ﬁrst make the observation that, for any y in the set of top-k PMIs for x, if x, y appears in the ﬁnal buffer then we are guaranteed that y is correctly placed in the top-k at the end. [sent-159, score-0.213]
</p><p>51 This is because we recompute PMIs for all the pairs in the last buffer at the end of the algorithm (line 13 of Algorithm 1). [sent-160, score-0.225]
</p><p>52 The probability that x, y does not appear in the last buffer can be bounded using the i. [sent-161, score-0.191]
</p><p>53 We do so by studying the last buffer in which x, y appears. [sent-168, score-0.191]
</p><p>54 The only way that y can displace y in the top-k for x in our algorithm is if at the end of this buffer the following holds true: ct (x, y ) ct (x, y) > , ct (y ) ct (y) where the t subscripts denotes the respective counts at the end of the buffer. [sent-169, score-0.594]
</p><p>55 , 1/2m ) will the last buffer containing x, y appear before the midpoint of the stream. [sent-174, score-0.213]
</p><p>56 So, let us assume that the buffer appears after the midpoint of the stream. [sent-175, score-0.235]
</p><p>57 Then, the probability that x, y appears more than (1 + δ)c(x, y )/2 times by this point can be bounded by the Chernoff bound to be at most 5  exp(−c(x, y )δ 2 /8). [sent-176, score-0.058]
</p><p>58 Putting all these together, we get that Pr  ct (x, y ) (1 + δ)c(x, y ) > ct (y ) (1 − δ)c(y )  < 1/2m + exp(−c(x, y )δ 2 /8) + exp(−c(y )δ 2 /4). [sent-178, score-0.152]
</p><p>59 Let us take the rank of the PMI of y to be i (and recall that the rank of the PMI of y is at most k). [sent-180, score-0.13]
</p><p>60 We can now put all these results c(y) c(y together to bound the probability of the event Pr  ct (x, y) ct (x, y ) > ct (y ) ct (y)  ≤ 1/2m + exp(−c(x, y )δ 2 /8) + exp(−c(y )δ 2 /4),  where we take δ = (((i/k)s − 1)2PMI(x,y ) − 1)/(((i/k)s − 1)2PMI(x,y ) + 1). [sent-183, score-0.34]
</p><p>61 Taking a union bound across all possible y ∈ Y gives a bound of 1/2m + |Y |(exp(−c(x, y )δ 2 /8) + exp(−c(y )δ 2 /4)). [sent-185, score-0.072]
</p><p>62 7  6  Experiments  We evaluated our algorithm for online, k-best PMI with a set of experiments on collecting verbal triggers in a document collection. [sent-186, score-0.105]
</p><p>63 For each unique verb x observed in the stream, our goal was to recover the top-k verbs y with the highest PMI given x. [sent-190, score-0.107]
</p><p>64 Tokens were tagged for part of speech (POS) using SVMTool [Gim´ nez and M` rquez, 2004], e a a POS tagger based on SVMlight [Joachims, 1999]. [sent-193, score-0.089]
</p><p>65 Our stream was constructed by considering all pairwise combinations of the roughly 82 (on average) verb tokens occurring in each document. [sent-194, score-0.288]
</p><p>66 10 We will refer to such non-approximate counting as perfect counting. [sent-198, score-0.201]
</p><p>67 We did not heavily tune our counting mechanism to this task, other than to experiment with a few different bases (settling on a base of 1. [sent-200, score-0.128]
</p><p>68 As such, empirical results for approximate counting 7  For streams composed such as described in our experiments, this bound becomes powerful as m approaches 100 or beyond (recalling that both c(x, y ), c(y ) > m). [sent-202, score-0.199]
</p><p>69 This can be viewed as a restricted version of the experiments of Chambers and Jurafsky [2008], where we consider all verb pairs, regardless of whether they are assumed to possess a co-referent argument. [sent-207, score-0.068]
</p><p>70 If fully enumerated as text, this stream would have required 12GB of uncompressed storage. [sent-209, score-0.197]
</p><p>71 05  q  0  10  20  30  40  standard instrumented  0. [sent-216, score-0.055]
</p><p>72 3(b) : Accuracy of top-5 ranklist using the standard measurement, and when using an instrumented counter that had oracle access to which x, y were above threshold. [sent-218, score-0.201]
</p><p>73 Table 1: When using a perfect counter and a buffer of 50, 500 and 5,000 documents, for k = 1, 5, 10: the accuracy of the resultant k-best lists when compared to the ﬁrst k, k + 1 and k + 2 true values. [sent-219, score-0.5]
</p><p>74 74 k=1 k=5 k = 10  should be taken as a lower bound, while the perfect counting results are the upper bound on what an approximate counter might achieve. [sent-247, score-0.418]
</p><p>75 We measured the accuracy of resultant k-best lists by ﬁrst collecting the true top-50 elements for each x, ofﬂine, to be used as a key. [sent-248, score-0.12]
</p><p>76 , rank 9, should really be of rank 12, rather than rank 50. [sent-253, score-0.195]
</p><p>77 1  Results  In Table 1 we see that when using a perfect counter, our algorithm succeeds in recovering almost all top-k elements. [sent-257, score-0.073]
</p><p>78 For example, when k = 5, reading 500 documents at a time, our rank lists are 97. [sent-258, score-0.126]
</p><p>79 As there appears to be minimal impact based on buffer size, we ﬁxed |B| = 500 documents for the remainder of our experiments. [sent-261, score-0.213]
</p><p>80 11 This result supports the intuition behind our misclassiﬁcation probability bound: while it is possible for an adversary to construct a stream that would mislead our online algorithm, this seems to rarely occur in practice. [sent-262, score-0.229]
</p><p>81 Shown in Figure 3(b) are the accuracy results when using an approximate counter and a buffer size of 500 documents, to collect top-5 rank lists. [sent-263, score-0.437]
</p><p>82 The standard result is based on comparing the rank lists to the key just as with the results when using a perfect counter. [sent-265, score-0.199]
</p><p>83 A problem with this evaluation is that the hard threshold used for both generating the key, and the results for perfect counting, cannot be guaranteed to hold when using approximate counts. [sent-266, score-0.108]
</p><p>84 It is possible that 11 Strictly speaking, |B| is no larger than the maximum length interval in the stream resulting from enumerating the contents of, e. [sent-267, score-0.221]
</p><p>85 Left columns are based on using a perfect counter, while right columns are based on an approximate counter. [sent-271, score-0.108]
</p><p>86 Numeral preﬁxes denote rank of element in true top-k lists. [sent-272, score-0.088]
</p><p>87 All results are with respect to a buffer of 500 documents. [sent-273, score-0.191]
</p><p>88 For purposes of comparison, we instrumented the approximate solution to use a perfect counter in parallel. [sent-276, score-0.309]
</p><p>89 All PMI values were computed as before, using approximate counts, but the perfect counter was used just in verifying whether a given pair exceeded the threshold. [sent-277, score-0.254]
</p><p>90 In this way the approximate counting solution saw just those elements of the stream as observed in the perfect counting case, allowing us to evaluate the ranking error introduced by the counter, irrespective of issues in “dipping below” the threshold. [sent-278, score-0.561]
</p><p>91 As seen in the instrumented curve, top-5 rank lists generated when using the approximate counter are composed primarily of elements truly ranked 10 or below. [sent-279, score-0.362]
</p><p>92 2  Examples  Figure 2 contains the top-5 most associated verbs as according to our algorithm, both when using a perfect and an approximate counter. [sent-281, score-0.147]
</p><p>93 As can be seen for the perfect counter, and as suggested by Table 1, in practice it is possible to track PMI scores over buffered intervals with a very high degree of accuracy. [sent-282, score-0.073]
</p><p>94 For the examples shown (and more generally throughout the results), the resultant k-best lists are near perfect matches to those computed ofﬂine. [sent-283, score-0.163]
</p><p>95 When using an approximate counter we continue to see reasonable results, with some error introduced due to the use of probabilistic counting. [sent-284, score-0.181]
</p><p>96 The rank 1 entry reported for x = laughed exempliﬁes the earlier referenced issue of the approximate counter being able to incorrectly dip below the threshold for terms that the gold standard would never see. [sent-285, score-0.317]
</p><p>97 We showed that while a precise solution comes at a high cost in the streaming model, there exists a simple algorithm that performs well on real data. [sent-287, score-0.062]
</p><p>98 An avenue of future work is to drop the assumption that each of the top-k PMI values is maintained explicitly and see whether there is an algorithm that is feasible for the streaming version of the problem or if a similar lower bound still applies. [sent-288, score-0.127]
</p><p>99 An experiment of Schooler and Anderson [1997] assumed words in NYTimes headlines operated as cues for the retrieval of memory structures associated with co-occurring terms. [sent-290, score-0.058]
</p><p>100 [Gim´ nez and M` rquez, 2004] Jes´ s Gim´ nez and Llu´s M` rquez. [sent-320, score-0.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pmi', 0.738), ('lg', 0.335), ('stream', 0.197), ('buffer', 0.191), ('counter', 0.146), ('xp', 0.141), ('counting', 0.128), ('pmis', 0.096), ('church', 0.084), ('durme', 0.082), ('ct', 0.076), ('perfect', 0.073), ('talbot', 0.072), ('lall', 0.068), ('xy', 0.068), ('verb', 0.068), ('rank', 0.065), ('streaming', 0.062), ('lists', 0.061), ('chambers', 0.06), ('counts', 0.058), ('rosenfeld', 0.055), ('zip', 0.055), ('bloom', 0.055), ('counters', 0.055), ('instrumented', 0.055), ('pmik', 0.055), ('ab', 0.051), ('rochester', 0.048), ('schooler', 0.048), ('cxy', 0.048), ('hanks', 0.048), ('wy', 0.046), ('triggers', 0.044), ('bomb', 0.041), ('chklovski', 0.041), ('displace', 0.041), ('gim', 0.041), ('laughed', 0.041), ('nez', 0.041), ('priority', 0.041), ('verbs', 0.039), ('pointwise', 0.038), ('jurafsky', 0.037), ('bound', 0.036), ('insertion', 0.036), ('approximate', 0.035), ('pairs', 0.034), ('mutual', 0.032), ('storing', 0.032), ('online', 0.032), ('document', 0.031), ('trigger', 0.031), ('associativity', 0.031), ('memory', 0.031), ('gold', 0.03), ('collecting', 0.03), ('resultant', 0.029), ('pos', 0.029), ('maintained', 0.029), ('language', 0.028), ('ashwin', 0.027), ('assassinate', 0.027), ('detonate', 0.027), ('latches', 0.027), ('narrative', 0.027), ('nytimes', 0.027), ('overridden', 0.027), ('override', 0.027), ('panang', 0.027), ('pantel', 0.027), ('rquez', 0.027), ('snickered', 0.027), ('svmtool', 0.027), ('tickle', 0.027), ('tickled', 0.027), ('tickling', 0.027), ('vetoed', 0.027), ('vetoing', 0.027), ('cues', 0.027), ('item', 0.026), ('linguistics', 0.024), ('length', 0.024), ('qq', 0.024), ('graff', 0.024), ('tagged', 0.024), ('hy', 0.024), ('tagger', 0.024), ('pr', 0.024), ('items', 0.023), ('element', 0.023), ('tokens', 0.023), ('appears', 0.022), ('list', 0.022), ('word', 0.022), ('queue', 0.022), ('vocabularies', 0.022), ('midpoint', 0.022), ('increment', 0.022), ('osborne', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="233-tfidf-1" href="./nips-2009-Streaming_Pointwise_Mutual_Information.html">233 nips-2009-Streaming Pointwise Mutual Information</a></p>
<p>Author: Benjamin V. Durme, Ashwin Lall</p><p>Abstract: Recent work has led to the ability to perform space efﬁcient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data. 1</p><p>2 0.065004788 <a title="233-tfidf-2" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>Author: Nir Ailon, Ragesh Jaiswal, Claire Monteleoni</p><p>Abstract: We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are: a derivation of an extremely simple pseudo-approximation batch algorithm for k-means (based on the recent k-means++), in which the algorithm is allowed to output more than k centers, and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (ﬁtting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data reveal the practical utility of our method. 1</p><p>3 0.047790773 <a title="233-tfidf-3" href="./nips-2009-Who%E2%80%99s_Doing_What%3A_Joint_Modeling_of_Names_and_Verbs_for_Simultaneous_Face_and_Pose_Annotation.html">259 nips-2009-Who’s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation</a></p>
<p>Author: Jie Luo, Barbara Caputo, Vittorio Ferrari</p><p>Abstract: Given a corpus of news items consisting of images accompanied by text captions, we want to ﬁnd out “who’s doing what”, i.e. associate names and action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint ‘face and pose’ model solves the correspondence problem better than earlier models covering only the face, and that it can perform recognition of new uncaptioned images. 1</p><p>4 0.047041953 <a title="233-tfidf-4" href="./nips-2009-Differential_Use_of_Implicit_Negative_Evidence_in_Generative_and_Discriminative_Language_Learning.html">66 nips-2009-Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</a></p>
<p>Author: Anne Hsu, Thomas L. Griffiths</p><p>Abstract: A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-speciﬁc knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented. 1</p><p>5 0.044787101 <a title="233-tfidf-5" href="./nips-2009-Ranking_Measures_and_Loss_Functions_in_Learning_to_Rank.html">199 nips-2009-Ranking Measures and Loss Functions in Learning to Rank</a></p>
<p>Author: Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, Hang Li</p><p>Abstract: Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learningto-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measurebased ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classiﬁcation tasks, and deﬁne a so-called essential loss for ranking as the weighted sum of the classiﬁcation errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modiﬁcations can lead to better ranking performances, demonstrating the correctness of our theoretical analysis. 1</p><p>6 0.043189321 <a title="233-tfidf-6" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>7 0.040744502 <a title="233-tfidf-7" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>8 0.040424109 <a title="233-tfidf-8" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>9 0.035035793 <a title="233-tfidf-9" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>10 0.03440946 <a title="233-tfidf-10" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<p>11 0.033773214 <a title="233-tfidf-11" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>12 0.031769656 <a title="233-tfidf-12" href="./nips-2009-Robust_Principal_Component_Analysis%3A_Exact_Recovery_of_Corrupted_Low-Rank_Matrices_via_Convex_Optimization.html">208 nips-2009-Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization</a></p>
<p>13 0.031163495 <a title="233-tfidf-13" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>14 0.030406464 <a title="233-tfidf-14" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>15 0.030214049 <a title="233-tfidf-15" href="./nips-2009-Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units.html">186 nips-2009-Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units</a></p>
<p>16 0.030101653 <a title="233-tfidf-16" href="./nips-2009-Learning_to_Rank_by_Optimizing_NDCG_Measure.html">136 nips-2009-Learning to Rank by Optimizing NDCG Measure</a></p>
<p>17 0.027749918 <a title="233-tfidf-17" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>18 0.027451647 <a title="233-tfidf-18" href="./nips-2009-A_Parameter-free_Hedging_Algorithm.html">14 nips-2009-A Parameter-free Hedging Algorithm</a></p>
<p>19 0.027154742 <a title="233-tfidf-19" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>20 0.026468864 <a title="233-tfidf-20" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.088), (1, 0.019), (2, -0.01), (3, -0.038), (4, 0.007), (5, -0.012), (6, -0.057), (7, -0.046), (8, 0.004), (9, 0.005), (10, 0.019), (11, -0.02), (12, 0.011), (13, -0.005), (14, 0.029), (15, 0.021), (16, 0.006), (17, -0.008), (18, -0.032), (19, 0.032), (20, -0.03), (21, 0.008), (22, 0.034), (23, 0.001), (24, -0.034), (25, 0.021), (26, 0.054), (27, -0.015), (28, 0.037), (29, -0.001), (30, -0.082), (31, -0.037), (32, -0.022), (33, -0.012), (34, 0.014), (35, 0.041), (36, 0.029), (37, -0.039), (38, -0.021), (39, 0.144), (40, 0.064), (41, 0.05), (42, -0.017), (43, 0.084), (44, 0.017), (45, -0.111), (46, -0.02), (47, 0.043), (48, 0.086), (49, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87762737 <a title="233-lsi-1" href="./nips-2009-Streaming_Pointwise_Mutual_Information.html">233 nips-2009-Streaming Pointwise Mutual Information</a></p>
<p>Author: Benjamin V. Durme, Ashwin Lall</p><p>Abstract: Recent work has led to the ability to perform space efﬁcient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data. 1</p><p>2 0.68402404 <a title="233-lsi-2" href="./nips-2009-Differential_Use_of_Implicit_Negative_Evidence_in_Generative_and_Discriminative_Language_Learning.html">66 nips-2009-Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</a></p>
<p>Author: Anne Hsu, Thomas L. Griffiths</p><p>Abstract: A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-speciﬁc knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented. 1</p><p>3 0.55948704 <a title="233-lsi-3" href="./nips-2009-Who%E2%80%99s_Doing_What%3A_Joint_Modeling_of_Names_and_Verbs_for_Simultaneous_Face_and_Pose_Annotation.html">259 nips-2009-Who’s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation</a></p>
<p>Author: Jie Luo, Barbara Caputo, Vittorio Ferrari</p><p>Abstract: Given a corpus of news items consisting of images accompanied by text captions, we want to ﬁnd out “who’s doing what”, i.e. associate names and action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint ‘face and pose’ model solves the correspondence problem better than earlier models covering only the face, and that it can perform recognition of new uncaptioned images. 1</p><p>4 0.45343366 <a title="233-lsi-4" href="./nips-2009-Sequential_effects_reflect_parallel_learning_of_multiple_environmental_regularities.html">216 nips-2009-Sequential effects reflect parallel learning of multiple environmental regularities</a></p>
<p>Author: Matthew Wilder, Matt Jones, Michael C. Mozer</p><p>Abstract: Across a wide range of cognitive tasks, recent experience inﬂuences behavior. For example, when individuals repeatedly perform a simple two-alternative forcedchoice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g., Jones and Sieck, 2003; Mozer, Kinoshita, and Shettel, 2007; Yu and Cohen, 2008). The Dynamic Belief Model (DBM) (Yu and Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that ﬁrst-order statistics (base rates) also inﬂuence sequential effects. We propose a model that learns both ﬁrst- and second-order sequence properties, each according to the basic principles of the DBM but under a uniﬁed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious ﬁts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Martello, Sahm, and Spillmann, 2005) and electrophysiological studies (Jentzsch and Sommer, 2002), supporting the psychological and neurobiological reality of its two components. 1</p><p>5 0.36811861 <a title="233-lsi-5" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>Author: Nir Ailon, Ragesh Jaiswal, Claire Monteleoni</p><p>Abstract: We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are: a derivation of an extremely simple pseudo-approximation batch algorithm for k-means (based on the recent k-means++), in which the algorithm is allowed to output more than k centers, and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (ﬁtting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data reveal the practical utility of our method. 1</p><p>6 0.3466565 <a title="233-lsi-6" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>7 0.34125727 <a title="233-lsi-7" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>8 0.33515731 <a title="233-lsi-8" href="./nips-2009-Quantification_and_the_language_of_thought.html">196 nips-2009-Quantification and the language of thought</a></p>
<p>9 0.33352888 <a title="233-lsi-9" href="./nips-2009-Modeling_Social_Annotation_Data_with_Content_Relevance_using_a_Topic_Model.html">153 nips-2009-Modeling Social Annotation Data with Content Relevance using a Topic Model</a></p>
<p>10 0.32612318 <a title="233-lsi-10" href="./nips-2009-Randomized_Pruning%3A_Efficiently_Calculating_Expectations_in_Large_Dynamic_Programs.html">197 nips-2009-Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs</a></p>
<p>11 0.32119346 <a title="233-lsi-11" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>12 0.30922654 <a title="233-lsi-12" href="./nips-2009-Optimal_Scoring_for_Unsupervised_Learning.html">182 nips-2009-Optimal Scoring for Unsupervised Learning</a></p>
<p>13 0.30613145 <a title="233-lsi-13" href="./nips-2009-The_Wisdom_of_Crowds_in_the_Recollection_of_Order_Information.html">244 nips-2009-The Wisdom of Crowds in the Recollection of Order Information</a></p>
<p>14 0.30355653 <a title="233-lsi-14" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>15 0.30191982 <a title="233-lsi-15" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>16 0.29697573 <a title="233-lsi-16" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<p>17 0.28599483 <a title="233-lsi-17" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>18 0.28080198 <a title="233-lsi-18" href="./nips-2009-Adapting_to_the_Shifting_Intent_of_Search_Queries.html">24 nips-2009-Adapting to the Shifting Intent of Search Queries</a></p>
<p>19 0.2775054 <a title="233-lsi-19" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>20 0.27693021 <a title="233-lsi-20" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (7, 0.01), (24, 0.06), (25, 0.057), (35, 0.032), (36, 0.083), (37, 0.011), (39, 0.032), (58, 0.045), (61, 0.024), (71, 0.067), (80, 0.377), (81, 0.022), (86, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75748932 <a title="233-lda-1" href="./nips-2009-Streaming_Pointwise_Mutual_Information.html">233 nips-2009-Streaming Pointwise Mutual Information</a></p>
<p>Author: Benjamin V. Durme, Ashwin Lall</p><p>Abstract: Recent work has led to the ability to perform space efﬁcient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, otherwise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI computation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data. 1</p><p>2 0.56750941 <a title="233-lda-2" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>Author: Jarno Vanhatalo, Pasi Jylänki, Aki Vehtari</p><p>Abstract: In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be signiﬁcantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the inﬂuence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution. 1</p><p>3 0.47459656 <a title="233-lda-3" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>4 0.39117068 <a title="233-lda-4" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>Author: Khashayar Rohanimanesh, Sameer Singh, Andrew McCallum, Michael J. Black</p><p>Abstract: Large, relational factor graphs with structure deﬁned by ﬁrst-order logic or other languages give rise to notoriously difﬁcult inference problems. Because unrolling the structure necessary to represent distributions over all hypotheses has exponential blow-up, solutions are often derived from MCMC. However, because of limitations in the design and parameterization of the jump function, these samplingbased methods suffer from local minima—the system must transition through lower-scoring conﬁgurations before arriving at a better MAP solution. This paper presents a new method of explicitly selecting fruitful downward jumps by leveraging reinforcement learning (RL). Rather than setting parameters to maximize the likelihood of the training data, parameters of the factor graph are treated as a log-linear function approximator and learned with methods of temporal difference (TD); MAP inference is performed by executing the resulting policy on held out test data. Our method allows efﬁcient gradient updates since only factors in the neighborhood of variables affected by an action need to be computed—we bypass the need to compute marginals entirely. Our method yields dramatic empirical success, producing new state-of-the-art results on a complex joint model of ontology alignment, with a 48% reduction in error over state-of-the-art in that domain. 1</p><p>5 0.3897 <a title="233-lda-5" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>Author: Ruslan Salakhutdinov</p><p>Abstract: Markov random ﬁelds (MRF’s), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF’s is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF’s. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.</p><p>6 0.38880119 <a title="233-lda-6" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>7 0.38668132 <a title="233-lda-7" href="./nips-2009-Distribution-Calibrated_Hierarchical_Classification.html">71 nips-2009-Distribution-Calibrated Hierarchical Classification</a></p>
<p>8 0.38544208 <a title="233-lda-8" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>9 0.38499263 <a title="233-lda-9" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>10 0.38377553 <a title="233-lda-10" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>11 0.38318139 <a title="233-lda-11" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>12 0.38294342 <a title="233-lda-12" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>13 0.38206273 <a title="233-lda-13" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>14 0.38201627 <a title="233-lda-14" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>15 0.38147762 <a title="233-lda-15" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>16 0.38146287 <a title="233-lda-16" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>17 0.38122496 <a title="233-lda-17" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>18 0.38080478 <a title="233-lda-18" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>19 0.38037565 <a title="233-lda-19" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>20 0.37933418 <a title="233-lda-20" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
