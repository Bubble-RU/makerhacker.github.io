<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-194" href="#">nips2009-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</h1>
<br/><p>Source: <a title="nips-2009-194-pdf" href="http://papers.nips.cc/paper/3731-predicting-the-optimal-spacing-of-study-a-multiscale-context-model-of-memory.pdf">pdf</a></p><p>Author: Harold Pashler, Nicholas Cepeda, Robert Lindsey, Ed Vul, Michael C. Mozer</p><p>Abstract: When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a signiﬁcant impact on memory retention. Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. Appropriate spacing of study can double retention on educationally relevant time scales. We introduce a Multiscale Context Model (MCM) that is able to predict the inﬂuence of a particular study schedule on retention for speciﬁc material. MCM’s prediction is based on empirical data characterizing forgetting of the material following a single study session. MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003). On the surface, these models are unrelated and incompatible, but we show they share a core feature that allows them to be integrated. MCM can determine study schedules that maximize the durability of learning, and has implications for education and training. MCM can be cast either as a neural network with inputs that ﬂuctuate over time, or as a cascade of leaky integrators. MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, & Shadmehr, 2007), yet MCM is better able to account for human declarative memory. 1</p><p>Reference: <a title="nips-2009-194-reference" href="../nips2009_reference/nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a signiﬁcant impact on memory retention. [sent-8, score-0.627]
</p><p>2 Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. [sent-9, score-0.449]
</p><p>3 Appropriate spacing of study can double retention on educationally relevant time scales. [sent-10, score-0.517]
</p><p>4 MCM’s prediction is based on empirical data characterizing forgetting of the material following a single study session. [sent-12, score-0.272]
</p><p>5 MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, & Shadmehr, 2007), yet MCM is better able to account for human declarative memory. [sent-17, score-0.263]
</p><p>6 This advice is based on a memory phenomenon known as the distributed practice or spacing effect (Cepeda, Pashler, Vul, Wixted, & Rohrer, 2006). [sent-20, score-0.497]
</p><p>7 The spacing effect is typically studied via a controlled experimental paradigm in which participants are asked to study unfamiliar paired associates (e. [sent-21, score-0.49]
</p><p>8 ” The lag between second session and the test is known as the retention interval or RI. [sent-28, score-0.18]
</p><p>9 The solid line of Figure 1a sketches this curve, which we will refer to as the spacing function. [sent-30, score-0.348]
</p><p>10 The left edge of the graph corresponds to massed practice, when session two immediately follows session one. [sent-31, score-0.18]
</p><p>11 (2006) sug1  forgetting function spacing function  (b) m2  % recall  (a)  m3  m4  . [sent-34, score-0.542]
</p><p>12 pool 1  pool 2  pool 3  pool 4  pool N  ISI  Figure 1: (a) The spacing function (solid line) depicts recall at test following two study sessions separated by a given ISI; the forgetting function (dashed line) depicts recall as a function of the lag between study and test. [sent-40, score-1.225]
</p><p>13 For educationally relevant RIs on the order of weeks and months, the effect of spacing can be tremendous: optimal spacing can double retention over massed practice (Cepeda et al. [sent-44, score-0.808]
</p><p>14 The spacing function is related to another observable measure of retention, the forgetting function, which characterizes recall accuracy following a single study session as a function of the lag between study and test. [sent-46, score-0.82]
</p><p>15 For example, suppose participants in the experiment described above learned material in study session 1, and were then tested on the material immediately prior to study session 2. [sent-47, score-0.41]
</p><p>16 Typical forgetting functions follow a generalized power-law decay, of the form P (recall) = A(1 + Bt)−C , where A, B, and C are constants, and t is the study-test lag (Wixted & Carpenter, 2007). [sent-50, score-0.208]
</p><p>17 Our goal is to develop a model of long-term memory that characterizes the memory-trace strength of items learned over two or more sessions. [sent-51, score-0.197]
</p><p>18 The model predicts recall accuracy as a function of the RI, taking into account the study schedule—the ISI or set of ISIs determining the spacing of study sessions. [sent-52, score-0.561]
</p><p>19 The spacing effect is among the best known phenomena in cognitive psychology, and many theoretical explanations have been suggested. [sent-54, score-0.351]
</p><p>20 Two well developed computational models of human memory have been elaborated to explain the spacing effect (Pavlik & Anderson, 2005; Raaijmakers, 2003). [sent-55, score-0.549]
</p><p>21 These models are necessarily complex: the brain contains multiple, interacting memory systems whose decay and interference characteristics depend on the speciﬁc content being stored and its relationship to other content. [sent-56, score-0.23]
</p><p>22 Consequently, these computational theories are fairly ﬂexible and can provide reasonable post-hoc ﬁts to spacing effect data, but we question their predictive value. [sent-57, score-0.412]
</p><p>23 Rather than developing a general theory of memory, we introduce a model that speciﬁcally predicts the shape of the spacing function. [sent-58, score-0.358]
</p><p>24 Because the spacing function depends not only on the RI, but also on the nature of the material being learned, and the manner and amount of study, the model requires empirical constraints. [sent-59, score-0.363]
</p><p>25 We propose a novel approach to obtaining a predictive model: we collect behavioral data to determine the forgetting function for the speciﬁc material being learned. [sent-60, score-0.216]
</p><p>26 We then use the forgetting function, which is based on a single study session, to predict the spacing function, which is based on two or more study sessions. [sent-61, score-0.643]
</p><p>27 2  Accounts of the spacing effect  We review two existing theories proposed to explain the spacing effect, and then propose a synthesis of these theories. [sent-65, score-0.793]
</p><p>28 However, after introducing our model and showing its predictive power, we discuss an intriguingly similar Bayesian theory of memory adaptation (Kording et al. [sent-68, score-0.186]
</p><p>29 1  Encoding-variability theories  One class of theories proposed to explain the spacing effect focuses on the notion of encoding variability. [sent-72, score-0.503]
</p><p>30 According to these theories, when an item is studied, a memory trace is formed that incorporates the current psychological context. [sent-73, score-0.378]
</p><p>31 Retrieval of a stored item depends at least in part on the similarity of the contexts at the study and test. [sent-75, score-0.276]
</p><p>32 If psychological context is assumed to ﬂuctuate randomly over time, two study sessions close together in time will have similar contexts. [sent-76, score-0.23]
</p><p>33 Consequently, at the time of a recall test, either both study contexts will match the test context or neither will. [sent-77, score-0.208]
</p><p>34 Increasing the ISI can thus prove advantageous because the test context will have higher likelihood of matching one study context or the other. [sent-78, score-0.19]
</p><p>35 Greater contextual variation enhances memory on this account by making for less redundancy in the underlying memory traces. [sent-79, score-0.349]
</p><p>36 Raaijmakers (2003) developed an encoding variability theory by incorporating time-varying contextual drift into the well-known Search of Associative Memory (SAM) model (Raaijmakers & Shiffrin, 1981), and explained a range of data from the spacing literature. [sent-82, score-0.408]
</p><p>37 The input layer to this neural net is a pool of binary valued neurons that represent the contextual state at the current time; the output layer consists of a set of memory elements, one per item to be stored. [sent-88, score-0.484]
</p><p>38 To simplify notation throughout this paper, we’ll describe this model and all others in terms of a single-item memory, allowing us to avoid an explicit index term for the item being stored or retrieved. [sent-89, score-0.2]
</p><p>39 The memory element for the item under consideration has an activation level, m, which is a linear function of the context unit activities: m = j wj cj , where cj is the binary activation level of context unit j and wj is the strength of connection from context j. [sent-90, score-0.725]
</p><p>40 The probability of retrieval of the item is assumed to be monotonically related to m. [sent-91, score-0.219]
</p><p>41 When an item is studied, its connection strengths are adjusted according to a Hebbian learning rule with an upper limit on the connection strength: ∆wj = min(1 − wj , cj m), ˆ (2) where m = 1 if the item was just presented for study, or 0 otherwise. [sent-92, score-0.392]
</p><p>42 When an item is studied, the ˆ weights for all contextual features present at the time of study will be strengthened. [sent-93, score-0.331]
</p><p>43 Later retrieval is more likely if the context at test matches the context at study: the memory element receives a contribution only when an input is active and its connection strength is nonzero. [sent-94, score-0.381]
</p><p>44 When an item has been studied twice, retrieval will be more robust if the two study opportunities strengthen different weights, which occurs when the ISI is large and the contextual states do not overlap signiﬁcantly. [sent-96, score-0.385]
</p><p>45 After an item has been studied at least once, SAM assumes that the memory trace resulting from further study is inﬂuenced by whether the item is accessible to retrieval at the time of study. [sent-98, score-0.701]
</p><p>46 Other memory models similarly claim that memory traces are weaker if an item is inaccessible to retrieval at the time of study (e. [sent-100, score-0.611]
</p><p>47 We have described the key components of SAM that explain the spacing effect, but the model has additional complexity, including a short-term memory store, inter-item interference, and additional 3  context based on associativity and explicit cues. [sent-103, score-0.562]
</p><p>48 , hours), but the same model cannot explain spacing effects on a different time scale (e. [sent-109, score-0.406]
</p><p>49 2  Predictive-utility theories  We now turn to another class of theories that has been proposed to explain the spacing effect. [sent-114, score-0.481]
</p><p>50 When an item is studied multiple times with a given ISI, the rational analysis suggests that the need probability drops off rapidly following the last study once an interval of time greater than the ISI has passed. [sent-118, score-0.307]
</p><p>51 In MTS, each item to be stored is represented by a dedicated cascade of N leaky integrators. [sent-123, score-0.284]
</p><p>52 The activation of integrator i, xi , decays over time according to: xi (t + ∆t) = xi (t) exp(−∆t/τi ), (3) where τi is the decay time constant. [sent-124, score-0.223]
</p><p>53 The probability of retrieving the item is related to the total k trace strength, sN , where sk = j=1 xj . [sent-125, score-0.203]
</p><p>54 When an item is repeatedly presented for study with short ISIs, the trace can successfully be represented by the integrators with short time constants, and consequently, the trace will decay rapidly. [sent-131, score-0.537]
</p><p>55 Increasing the spacing shifts the representation to integrators with slower decay rates. [sent-132, score-0.534]
</p><p>56 Essentially, we take from SAM the notion of contextual drift and retrieval-dependent update, and from MTS the multiscale representation and the cascaded error-correction memory update, and we obtain a new model which we call the Multiscale Context Model or MCM. [sent-138, score-0.31]
</p><p>57 MCM can also be described in terms of N leaky integrators, where integrator i has time constant τi and activity scaled by γi . [sent-143, score-0.197]
</p><p>58 As the reader might infer from our description of SAM and MTS, these parameters characterize memory decay, extending Equation 3 such that the total trace strength at time t is deﬁned as: N  sN (t) =  γi exp(− i=1  4  t )xi (0). [sent-146, score-0.25]
</p><p>59 τi  If xi (0) = 1 for all i—which is the integrator activity following the ﬁrst study in MTS—the trace strength as a function of time is a mixture of exponentials. [sent-147, score-0.297]
</p><p>60 To match the form of human forgetting (Figure 1), this mixture must approximate a power function. [sent-148, score-0.184]
</p><p>61 Given N and human forgetting data collected in an experiment, we can search for the parameters {µ, ν, ω, ξ} that obtain a least squares ﬁt to the data. [sent-157, score-0.184]
</p><p>62 Given the human forgetting function function, then, we can completely determine the {τi } and {γi }. [sent-158, score-0.184]
</p><p>63 1  Casting MCM as a cascade of leaky integrators  Assume that—as in MTS—a dedicated set of N leaky integrators hold the memory of each item to be learned. [sent-161, score-0.754]
</p><p>64 Let xi denote the activity of integrator i associated with the item, and let si be the average strength of the ﬁrst i integrators, weighted by the {γj } terms: si =  1 Γi  i  i  γj xj , where Γi = j=1  γj . [sent-162, score-0.228]
</p><p>65 When an item is studied, its integrators receive a boost in activity. [sent-164, score-0.34]
</p><p>66 Integrator i receives a boost that depends on how close the average strength of the ﬁrst i integrators is to full strength, i. [sent-165, score-0.217]
</p><p>67 We adopt the retrieval-dependent update assumption of SAM, and ﬁx = 1 for an item that is unsuccessfully recalled at the time of study, and = r > 1 for an item that is successfully recalled. [sent-168, score-0.401]
</p><p>68 (1) MTS weighs all integrators equally when combining the individual integrator activities. [sent-170, score-0.229]
</p><p>69 (2) MTS provides no guidance in setting the τ and γ constants; MCM constrains these parameters based on the human forgetting function. [sent-172, score-0.184]
</p><p>70 The context units are binary valued and units in pool i ﬂip with time constant τi . [sent-180, score-0.231]
</p><p>71 ) 5  As depicted in Figure 1b, the model also includes a set of N memory elements for each item to be learned. [sent-183, score-0.32]
</p><p>72 Activation of memory element i, denoted mi , indicates strength of retrieval for the item based on context pools 1. [sent-185, score-0.562]
</p><p>73 The activation function is cascaded such that memory element i receives input from context units in pool i as well as memory element i − 1: mi = mi−1 +  wij cij + b, j  where wij is the connection weight from context unit j to memory element i, m0 ≡ 0, and b = −β/(1 − β) is a bias weight. [sent-189, score-0.901]
</p><p>74 The bias simply serves to offset spurious activity reaching the memory elements, activity that is unrelated to the fact that the item was previously studied and stored. [sent-190, score-0.455]
</p><p>75 The probability of recalling the item is related to the activity of memory element N : P (recall) = min(1, mN ). [sent-192, score-0.38]
</p><p>76 When the item is studied, the weights from context units in pool i are adjusted according to an update rule that performs gradient descent in an error measure Ei = ei 2 , where ei = 1 − mi /Γi . [sent-193, score-0.472]
</p><p>77 This error is minimized when the memory element i reaches activation level Γi (deﬁned earlier as the proportion of units in the entire context pool that contributes to activity at stage i). [sent-194, score-0.412]
</p><p>78 (2) SAM’s memory update rule can be interpreted as Hebbian learning; MCM’s update can be interpreted as error-correction learning. [sent-203, score-0.204]
</p><p>79 3  Relating leaky integrator and neural net characterizations of MCM  To make contact with MTS, we have described MCM as a cascade of leaky integrators, and to make contact with SAM, we have described MCM as a neural net. [sent-205, score-0.251]
</p><p>80 , in press) have recently conducted well-controlled experimental manipulations of spacing involving RIs on educationally relevant time scales of days to months. [sent-209, score-0.46]
</p><p>81 Most research in the spacing literature involves brief RIs, on the scale of minutes to an hour, and methodological concerns have been raised with the few well-known studies involving longer RIs (Cepeda et al. [sent-210, score-0.349]
</p><p>82 Recall accuracy at the start of the second session provides the basic forgetting function, and recall accuracy at test provides the spacing function. [sent-216, score-0.622]
</p><p>83 In panel (e), the peaks of the model’s spacing functions are indicated by the triangle pointers. [sent-222, score-0.349]
</p><p>84 These four model parameters determine the time constants and weighting coefﬁcients of the mixture-of-exponentials approximation to the forgetting function (Equation 5). [sent-224, score-0.186]
</p><p>85 The model has only one other free parameter, r , the magnitude of update on a trial when an item is successfully recalled (see Equation 6). [sent-225, score-0.203]
</p><p>86 With r , MCM is fully constrained and can make strong predictions regarding the spacing function. [sent-227, score-0.351]
</p><p>87 For each experiment, MCM’s prediction of the peak of the spacing function is entirely consistent with the data, and for the most part, MCM’s quantiative predictions are excellent. [sent-231, score-0.394]
</p><p>88 (It would be extremely surprising to psychologists if the peak were in general independent of the material, as content effects pervade the memory literature. [sent-236, score-0.212]
</p><p>89 Each red circle represents a single spacing experiment in which the ISI was varied for a given RI. [sent-241, score-0.329]
</p><p>90 MCM predicts the spacing functions with absolutely spectacular precision, considering the predictions are fully constrained and parameter free. [sent-249, score-0.38]
</p><p>91 Moreover, MCM anticipates the peaks of the spacing functions, with the curvature of the peak decreasing with the RI, and the optimal ISI increasing with the RI. [sent-250, score-0.392]
</p><p>92 In addition to these results, MCM also predicts the probability of recall at test conditional on successful or unsuccessful recall during the test at the start of the second study session. [sent-251, score-0.207]
</p><p>93 Finally, MCM is able to post-hoc ﬁt classic studies from the spacing literature (for which forgetting functions are not available). [sent-254, score-0.491]
</p><p>94 5  Discussion  MCM’s blind prediction of 7 different spacing functions is remarkable considering that the domain’s complexity (the content, manner and amount of study) is reduced to four parameters, which are fully determined by the forgetting function. [sent-255, score-0.491]
</p><p>95 The state predicts the appearance of an item in the temporal stream of experience. [sent-264, score-0.203]
</p><p>96 (Two parameters determine the range of time scales; two specify internal and observation noise levels; and two perform an afﬁne transform from internal memory strength to recall probability. [sent-270, score-0.332]
</p><p>97 ) In terms of sum-squared error, the model shows a reasonable ﬁt, but the model clearly misses the peaks of the spacing functions, and in fact predicts a peak that is independent of RI. [sent-271, score-0.421]
</p><p>98 Notably, the KF model is a post-hoc ﬁt to the spacing functions, whereas MCM produces a true prediction of the spacing functions, i. [sent-272, score-0.658]
</p><p>99 , parameters of MCM are determined without peeking at the spacing function. [sent-274, score-0.329]
</p><p>100 Practice and forgetting effects on vocabulary memory: An activation-based model of the spacing effect. [sent-331, score-0.533]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mcm', 0.656), ('spacing', 0.329), ('isi', 0.272), ('cepeda', 0.215), ('mts', 0.192), ('item', 0.174), ('forgetting', 0.162), ('integrators', 0.147), ('memory', 0.146), ('sam', 0.142), ('ris', 0.09), ('integrator', 0.082), ('isis', 0.082), ('session', 0.08), ('pool', 0.078), ('study', 0.076), ('days', 0.073), ('ri', 0.071), ('wixted', 0.069), ('raaijmakers', 0.068), ('kf', 0.064), ('theories', 0.061), ('pashler', 0.059), ('decay', 0.058), ('context', 0.057), ('contextual', 0.057), ('rohrer', 0.057), ('leaky', 0.056), ('multiscale', 0.055), ('retention', 0.054), ('strength', 0.051), ('recall', 0.051), ('lag', 0.046), ('retrieval', 0.045), ('sessions', 0.044), ('peak', 0.043), ('vul', 0.042), ('kording', 0.042), ('pools', 0.04), ('ei', 0.037), ('units', 0.036), ('activity', 0.035), ('activation', 0.035), ('material', 0.034), ('educationally', 0.034), ('pavlik', 0.034), ('staddon', 0.034), ('studied', 0.033), ('unrelated', 0.032), ('explain', 0.03), ('internal', 0.03), ('participants', 0.03), ('si', 0.03), ('cascaded', 0.03), ('schedules', 0.03), ('update', 0.029), ('net', 0.029), ('anderson', 0.029), ('trace', 0.029), ('psychological', 0.029), ('predicts', 0.029), ('cascade', 0.028), ('cij', 0.027), ('stored', 0.026), ('element', 0.025), ('mi', 0.024), ('time', 0.024), ('wj', 0.024), ('effects', 0.023), ('cancelled', 0.023), ('carpenter', 0.023), ('chelaru', 0.023), ('higa', 0.023), ('marr', 0.023), ('milson', 0.023), ('effect', 0.022), ('human', 0.022), ('drift', 0.022), ('synthesis', 0.022), ('wij', 0.022), ('predictions', 0.022), ('consequently', 0.021), ('months', 0.021), ('behavioral', 0.02), ('peaks', 0.02), ('cj', 0.02), ('panels', 0.02), ('declarative', 0.02), ('shiffrin', 0.02), ('habituation', 0.02), ('massed', 0.02), ('shadmehr', 0.02), ('intriguingly', 0.02), ('uctuate', 0.02), ('et', 0.02), ('solid', 0.019), ('facts', 0.019), ('boost', 0.019), ('vocabulary', 0.019), ('shorter', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="194-tfidf-1" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>Author: Harold Pashler, Nicholas Cepeda, Robert Lindsey, Ed Vul, Michael C. Mozer</p><p>Abstract: When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a signiﬁcant impact on memory retention. Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. Appropriate spacing of study can double retention on educationally relevant time scales. We introduce a Multiscale Context Model (MCM) that is able to predict the inﬂuence of a particular study schedule on retention for speciﬁc material. MCM’s prediction is based on empirical data characterizing forgetting of the material following a single study session. MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003). On the surface, these models are unrelated and incompatible, but we show they share a core feature that allows them to be integrated. MCM can determine study schedules that maximize the durability of learning, and has implications for education and training. MCM can be cast either as a neural network with inputs that ﬂuctuate over time, or as a cascade of leaky integrators. MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, & Shadmehr, 2007), yet MCM is better able to account for human declarative memory. 1</p><p>2 0.099731222 <a title="194-tfidf-2" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>Author: Hongjing Lu, Matthew Weiden, Alan L. Yuille</p><p>Abstract: We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We deﬁne conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efﬁciently perform model selection to decide whether observations are from one or multiple categories.</p><p>3 0.087896876 <a title="194-tfidf-3" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>Author: Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</p><p>Abstract: We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject ﬁrst studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle ﬁlter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability. 1</p><p>4 0.062605232 <a title="194-tfidf-4" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>Author: Daniel Cavagnaro, Jay Myung, Mark A. Pitt</p><p>Abstract: In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efﬁciently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to ﬁnd designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1</p><p>5 0.052705847 <a title="194-tfidf-5" href="./nips-2009-Hierarchical_Learning_of_Dimensional_Biases_in_Human_Categorization.html">109 nips-2009-Hierarchical Learning of Dimensional Biases in Human Categorization</a></p>
<p>Author: Adam Sanborn, Nick Chater, Katherine A. Heller</p><p>Abstract: Existing models of categorization typically represent to-be-classiﬁed items as points in a multidimensional space. While from a mathematical point of view, an inﬁnite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions – and have a strong preference to generalize along the axes of these dimensions, but not “diagonally”. What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Speciﬁcally, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. The learning behaviour of the model captures the developmental shift from roughly “isotropic” for children to the axis-aligned generalization that adults show. 1</p><p>6 0.050895404 <a title="194-tfidf-6" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>7 0.042605687 <a title="194-tfidf-7" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>8 0.041848898 <a title="194-tfidf-8" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>9 0.036975052 <a title="194-tfidf-9" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>10 0.034236733 <a title="194-tfidf-10" href="./nips-2009-Quantification_and_the_language_of_thought.html">196 nips-2009-Quantification and the language of thought</a></p>
<p>11 0.031978715 <a title="194-tfidf-11" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>12 0.031843845 <a title="194-tfidf-12" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>13 0.030963682 <a title="194-tfidf-13" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>14 0.03023494 <a title="194-tfidf-14" href="./nips-2009-The_Wisdom_of_Crowds_in_the_Recollection_of_Order_Information.html">244 nips-2009-The Wisdom of Crowds in the Recollection of Order Information</a></p>
<p>15 0.030197242 <a title="194-tfidf-15" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>16 0.030151876 <a title="194-tfidf-16" href="./nips-2009-Sequential_effects_reflect_parallel_learning_of_multiple_environmental_regularities.html">216 nips-2009-Sequential effects reflect parallel learning of multiple environmental regularities</a></p>
<p>17 0.027749419 <a title="194-tfidf-17" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>18 0.026728114 <a title="194-tfidf-18" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>19 0.026667677 <a title="194-tfidf-19" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>20 0.02661187 <a title="194-tfidf-20" href="./nips-2009-Subject_independent_EEG-based_BCI_decoding.html">237 nips-2009-Subject independent EEG-based BCI decoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.096), (1, -0.044), (2, 0.023), (3, -0.024), (4, 0.026), (5, 0.016), (6, -0.011), (7, 0.001), (8, -0.05), (9, -0.006), (10, 0.015), (11, -0.052), (12, 0.01), (13, -0.056), (14, 0.079), (15, 0.013), (16, -0.03), (17, 0.062), (18, -0.126), (19, 0.058), (20, -0.009), (21, -0.021), (22, 0.065), (23, 0.007), (24, -0.052), (25, -0.014), (26, -0.003), (27, 0.018), (28, -0.049), (29, -0.017), (30, -0.014), (31, 0.003), (32, 0.029), (33, 0.036), (34, -0.018), (35, -0.028), (36, 0.081), (37, -0.003), (38, -0.062), (39, 0.001), (40, 0.012), (41, 0.02), (42, 0.042), (43, 0.061), (44, 0.038), (45, 0.063), (46, -0.01), (47, -0.024), (48, 0.038), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92312545 <a title="194-lsi-1" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>Author: Harold Pashler, Nicholas Cepeda, Robert Lindsey, Ed Vul, Michael C. Mozer</p><p>Abstract: When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a signiﬁcant impact on memory retention. Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. Appropriate spacing of study can double retention on educationally relevant time scales. We introduce a Multiscale Context Model (MCM) that is able to predict the inﬂuence of a particular study schedule on retention for speciﬁc material. MCM’s prediction is based on empirical data characterizing forgetting of the material following a single study session. MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003). On the surface, these models are unrelated and incompatible, but we show they share a core feature that allows them to be integrated. MCM can determine study schedules that maximize the durability of learning, and has implications for education and training. MCM can be cast either as a neural network with inputs that ﬂuctuate over time, or as a cascade of leaky integrators. MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, & Shadmehr, 2007), yet MCM is better able to account for human declarative memory. 1</p><p>2 0.70290351 <a title="194-lsi-2" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>Author: Daniel Cavagnaro, Jay Myung, Mark A. Pitt</p><p>Abstract: In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efﬁciently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to ﬁnd designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1</p><p>3 0.6586135 <a title="194-lsi-3" href="./nips-2009-Sequential_effects_reflect_parallel_learning_of_multiple_environmental_regularities.html">216 nips-2009-Sequential effects reflect parallel learning of multiple environmental regularities</a></p>
<p>Author: Matthew Wilder, Matt Jones, Michael C. Mozer</p><p>Abstract: Across a wide range of cognitive tasks, recent experience inﬂuences behavior. For example, when individuals repeatedly perform a simple two-alternative forcedchoice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g., Jones and Sieck, 2003; Mozer, Kinoshita, and Shettel, 2007; Yu and Cohen, 2008). The Dynamic Belief Model (DBM) (Yu and Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that ﬁrst-order statistics (base rates) also inﬂuence sequential effects. We propose a model that learns both ﬁrst- and second-order sequence properties, each according to the basic principles of the DBM but under a uniﬁed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious ﬁts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Martello, Sahm, and Spillmann, 2005) and electrophysiological studies (Jentzsch and Sommer, 2002), supporting the psychological and neurobiological reality of its two components. 1</p><p>4 0.65253818 <a title="194-lsi-4" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>Author: Xiaojin Zhu, Bryan R. Gibson, Timothy T. Rogers</p><p>Abstract: We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity. Rademacher complexity measures a learner’s ability to ﬁt random labels, and can be used to bound the learner’s true error based on the observed training sample error. We ﬁrst review the deﬁnition of Rademacher complexity and its generalization bound. We then describe a “learning the noise” procedure to experimentally measure human Rademacher complexities. The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overﬁtting in human learning. Finally, we discuss the potential applications of human Rademacher complexity in cognitive science. 1</p><p>5 0.65041685 <a title="194-lsi-5" href="./nips-2009-Hierarchical_Learning_of_Dimensional_Biases_in_Human_Categorization.html">109 nips-2009-Hierarchical Learning of Dimensional Biases in Human Categorization</a></p>
<p>Author: Adam Sanborn, Nick Chater, Katherine A. Heller</p><p>Abstract: Existing models of categorization typically represent to-be-classiﬁed items as points in a multidimensional space. While from a mathematical point of view, an inﬁnite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions – and have a strong preference to generalize along the axes of these dimensions, but not “diagonally”. What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Speciﬁcally, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. The learning behaviour of the model captures the developmental shift from roughly “isotropic” for children to the axis-aligned generalization that adults show. 1</p><p>6 0.64518052 <a title="194-lsi-6" href="./nips-2009-The_Wisdom_of_Crowds_in_the_Recollection_of_Order_Information.html">244 nips-2009-The Wisdom of Crowds in the Recollection of Order Information</a></p>
<p>7 0.63325334 <a title="194-lsi-7" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>8 0.5969916 <a title="194-lsi-8" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>9 0.57500333 <a title="194-lsi-9" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>10 0.49506569 <a title="194-lsi-10" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>11 0.49246913 <a title="194-lsi-11" href="./nips-2009-Quantification_and_the_language_of_thought.html">196 nips-2009-Quantification and the language of thought</a></p>
<p>12 0.48343349 <a title="194-lsi-12" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>13 0.47902459 <a title="194-lsi-13" href="./nips-2009-Abstraction_and_Relational_learning.html">21 nips-2009-Abstraction and Relational learning</a></p>
<p>14 0.44350019 <a title="194-lsi-14" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>15 0.41847187 <a title="194-lsi-15" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>16 0.41240051 <a title="194-lsi-16" href="./nips-2009-Bayesian_Belief_Polarization.html">39 nips-2009-Bayesian Belief Polarization</a></p>
<p>17 0.39909178 <a title="194-lsi-17" href="./nips-2009-Streaming_Pointwise_Mutual_Information.html">233 nips-2009-Streaming Pointwise Mutual Information</a></p>
<p>18 0.38790148 <a title="194-lsi-18" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>19 0.38646615 <a title="194-lsi-19" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>20 0.37075695 <a title="194-lsi-20" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.034), (25, 0.064), (35, 0.036), (36, 0.055), (39, 0.065), (58, 0.05), (61, 0.016), (71, 0.096), (81, 0.011), (86, 0.075), (91, 0.019), (93, 0.356)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78917062 <a title="194-lda-1" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>Author: Harold Pashler, Nicholas Cepeda, Robert Lindsey, Ed Vul, Michael C. Mozer</p><p>Abstract: When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a signiﬁcant impact on memory retention. Behavioral experiments have shown a nonmonotonic relationship between spacing and retention: short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals. Appropriate spacing of study can double retention on educationally relevant time scales. We introduce a Multiscale Context Model (MCM) that is able to predict the inﬂuence of a particular study schedule on retention for speciﬁc material. MCM’s prediction is based on empirical data characterizing forgetting of the material following a single study session. MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003). On the surface, these models are unrelated and incompatible, but we show they share a core feature that allows them to be integrated. MCM can determine study schedules that maximize the durability of learning, and has implications for education and training. MCM can be cast either as a neural network with inputs that ﬂuctuate over time, or as a cascade of leaky integrators. MCM is intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, & Shadmehr, 2007), yet MCM is better able to account for human declarative memory. 1</p><p>2 0.61779243 <a title="194-lda-2" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>Author: Vinayak Rao, Yee W. Teh</p><p>Abstract: Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally DP distributed. They are used in Bayesian nonparametric models when the usual exchangeability assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each associated with a point in a space such that neighbouring DPs are more dependent. We describe Markov chain Monte Carlo inference involving Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence on a synthetic dataset and demonstrate an application of the model to topic modeling through time. 1</p><p>3 0.42515507 <a title="194-lda-3" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We introduce a two-layer undirected graphical model, called a “Replicated Softmax”, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efﬁcient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.</p><p>4 0.4237344 <a title="194-lda-4" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><p>5 0.42253357 <a title="194-lda-5" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>Author: Hongjing Lu, Matthew Weiden, Alan L. Yuille</p><p>Abstract: We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We deﬁne conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efﬁciently perform model selection to decide whether observations are from one or multiple categories.</p><p>6 0.4203406 <a title="194-lda-6" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>7 0.41916379 <a title="194-lda-7" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>8 0.41740322 <a title="194-lda-8" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>9 0.41639265 <a title="194-lda-9" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>10 0.4146539 <a title="194-lda-10" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>11 0.41464069 <a title="194-lda-11" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>12 0.41342238 <a title="194-lda-12" href="./nips-2009-Learning_from_Multiple_Partially_Observed_Views_-_an_Application_to_Multilingual_Text_Categorization.html">130 nips-2009-Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization</a></p>
<p>13 0.4133442 <a title="194-lda-13" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>14 0.41143626 <a title="194-lda-14" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>15 0.41055763 <a title="194-lda-15" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>16 0.40957093 <a title="194-lda-16" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>17 0.40934145 <a title="194-lda-17" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>18 0.40722033 <a title="194-lda-18" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>19 0.4067826 <a title="194-lda-19" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>20 0.40676695 <a title="194-lda-20" href="./nips-2009-Hierarchical_Modeling_of_Local_Image_Features_through_%24L_p%24-Nested_Symmetric_Distributions.html">111 nips-2009-Hierarchical Modeling of Local Image Features through $L p$-Nested Symmetric Distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
