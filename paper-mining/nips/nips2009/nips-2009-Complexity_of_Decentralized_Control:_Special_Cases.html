<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 nips-2009-Complexity of Decentralized Control: Special Cases</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-53" href="#">nips2009-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 nips-2009-Complexity of Decentralized Control: Special Cases</h1>
<br/><p>Source: <a title="nips-2009-53-pdf" href="http://papers.nips.cc/paper/3857-complexity-of-decentralized-control-special-cases.pdf">pdf</a></p><p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>Reference: <a title="nips-2009-53-reference" href="../nips2009_reference/nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. [sent-5, score-0.656]
</p><p>2 1  Introduction  Decentralized and partially observable stochastic decision and planning problems are very common, comprising anything from strategic games of chance to robotic space exploration. [sent-8, score-0.274]
</p><p>3 In such domains, multiple agents act under uncertainty about both their environment and the plans and actions of others. [sent-9, score-0.447]
</p><p>4 These problems can be represented as decentralized partially observable Markov decision processes (Dec-POMDPs), or the equivalent, partially observable stochastic games (POSGs), allowing for precise formulation of solution concepts and success criteria. [sent-10, score-0.74]
</p><p>5 The complexity of ﬁnite-horizon Dec-POMDPs goes down—from NEXP to NP—when agents interact only via a joint reward structure, and are otherwise independent. [sent-16, score-0.561]
</p><p>6 Further, we consider two other Dec-POMDP sub-classes from the literature: (a) domains where local agent sub-problems are independent except for a (relatively small) number of event-based interactions, and (b) those where agents only interact inﬂuencing the set of currently available actions. [sent-19, score-0.936]
</p><p>7 ) These results provide further impetus to devise new tools for the analysis and classiﬁcation of problem difﬁculty in decentralized problem solving. [sent-22, score-0.407]
</p><p>8 2  Basic deﬁnitions  The cooperative, decentralized partially observable Markov decision process (Dec-POMDP) is a highly general and powerful framework, capable of representing a wide range of real-world problem 1  domains. [sent-23, score-0.583]
</p><p>9 , an , s ) is the probability of going from state s to state s , given joint action a1 , . [sent-31, score-0.269]
</p><p>10 , an ) is the reward obtained for performing joint action a1 , . [sent-53, score-0.255]
</p><p>11 The most important sub-instance of the Dec-POMDP model is the decentralized MDP (Dec-MDP), where the joint observation tells us everything we need to know about the system state. [sent-57, score-0.509]
</p><p>12 A decentralized Markov decision process (Dec-MDP) is a Dec-POMDP that is jointly fully observable. [sent-59, score-0.499]
</p><p>13 In a Dec-MDP, then, the sum total of the individual agent observations provides a complete picture of the state of the environment. [sent-70, score-0.536]
</p><p>14 It is important to note, however, that this does not mean that any individual agent actually possesses this information. [sent-71, score-0.459]
</p><p>15 Dec-MDPs are still fully decentralized in general, and individual agents cannot count on access to the global state when choosing actions. [sent-72, score-0.861]
</p><p>16 A local policy for an agent αi is a mapping from sequences of that agent’s observations, oi = o1 , . [sent-74, score-0.717]
</p><p>17 A joint policy for n agents is a i i collection of local policies, one per agent, π = π1 , . [sent-78, score-0.483]
</p><p>18 A solution method for a decentralized problem seeks to ﬁnd some joint policy that maximizes expected value given the starting state (or distribution over states) of the problem. [sent-82, score-0.596]
</p><p>19 For complexity purposes, the decision version of the Dec-(PO)MDP problem is to determine whether there exists some joint policy with value greater at least k. [sent-83, score-0.226]
</p><p>20 A tiling is a mapping of board locations to tile-types, t : {0, . [sent-95, score-0.251]
</p><p>21 , n − 1} → L; such a tiling is consistent just in case (i) the origin location of the board receives tile-type 0 (t(0, 0) = tile0 ); and (ii) all adjoint tile assignments are compatible: (∀x, y) t(x, y), t(x + 1, y) ∈ H & t(x, y), t(x, y + 1) ∈ V. [sent-101, score-0.282]
</p><p>22 The reduction transforms a given instance of TILING into a 2-agent Dec-MDP, where each agent is queried about some location in the grid, and must answer with a tile to be placed there. [sent-105, score-0.494]
</p><p>23 By careful design of the query and response mechanism, it is ensured that a policy with non-negative value exists only if the agents already have a consistent tiling, thus showing the Dec-MDP to be as hard as TILING. [sent-106, score-0.359]
</p><p>24 4  Factored Dec-POMDPs and independence  In general, the state transitions, observations, and rewards in a Dec-POMDP can involve probabilistic dependencies between agents. [sent-109, score-0.389]
</p><p>25 [6, 7] have thus studied problems in which the global statespace consists of the product of local states, so that each agent has its own individual state-space. [sent-112, score-0.632]
</p><p>26 A Dec-POMDP can then be transition independent, observation independent, or reward independent, as each the local effects given by each corresponding function are independent of one another. [sent-113, score-0.376]
</p><p>27 A factored, n-agent Dec-POMDP is a Dec-POMDP such that the system state can be factored into n + 1 distinct components, so that S = S0 × S1 × · · · × Sn , and no state-variable appears in any Si , Sj , i = j. [sent-115, score-0.26]
</p><p>28 As with the local (agent-speciﬁc) actions, ai , and observations, oi , in the general Dec-POMDP deﬁnition, we now refer to the local state, s ∈ Si × S0 , namely that portion of the overall stateˆ space that is either speciﬁc to agent αi (si ∈ Si ), or shared among all agents (so ∈ S0 ). [sent-116, score-1.14]
</p><p>29 We use the notation s−i for the sequence of all state-components except that for agent αi : s−i = (s0 , s1 , . [sent-117, score-0.431]
</p><p>30 A factored, n-agent DEC-POMDP is transition independent iff the state-transition function can be separated into n + 1 distinct transition functions P0 , . [sent-125, score-0.221]
</p><p>31 , Pn , where, for any next state si ∈ Si , P (si | (s0 , . [sent-128, score-0.236]
</p><p>32 , an ), s−i ) =  P0 (s0 | s0 ) if i = 0; Pi (si | si , ai , s0 ) else. [sent-134, score-0.275]
</p><p>33 ˆ  In other words, the next local state of each agent is independent of the local states of all others, given its previous local state and local action, and the external system features (S0 ). [sent-135, score-1.106]
</p><p>34 , On , where, for any local observation oi ∈ Ωi , O(oi | (a1 , . [sent-140, score-0.25]
</p><p>35 , sn ), o−i ) = Oi (oi | ai , si ) ˆ In such cases, the probability of an agent’s individual observations is a function of their own local states and actions alone, independent of the states of others, and of what those others do or observe. [sent-146, score-0.794]
</p><p>36 A factored, n-agent Dec-POMDP is reward independent iff the joint reward function can be represented by local reward functions R1 , . [sent-148, score-0.648]
</p><p>37 , Rn (ˆn , an )) s s and Ri (ˆi , ai ) ≥ Ri (ˆi , ai ) ⇔ f (R1 , . [sent-160, score-0.232]
</p><p>38 , Rn ) s s s s That is, joint reward is a function of local reward, constrained so that we maximize global reward if and only if we maximize local rewards. [sent-172, score-0.569]
</p><p>39 s s It is important to note that each deﬁnition applies equally to Dec-MDPs; in such cases, joint full observability of the overall state is often accompanied by full observability at the local level. [sent-180, score-0.577]
</p><p>40 A factored, n-agent Dec-MDP is locally fully observable iff an agent’s local observation uniquely determines its local state: ∀oi ∈ Ωi , ∃ˆi : P (ˆi | oi ) = 1. [sent-182, score-0.519]
</p><p>41 s s Local full observability is not equivalent to independence of observations. [sent-183, score-0.237]
</p><p>42 In particular, a problem may be locally fully observable without being observation independent (since agents may simply observe outcomes of non-independent joint actions). [sent-184, score-0.507]
</p><p>43 1  Shared rewards alone lead to reduced complexity  It is easy to see that if a Dec-MDP (or Dec-POMDP) has all three forms of independence given by Deﬁnitions 5–7, it can be decomposed into n separate problems, where each agent αi works solely within the local sub-environment Si × S0 . [sent-187, score-0.823]
</p><p>44 ) Thus we see that in some respects, transition and observation independence are fundamental to the reduction of worst-case complexity from NEXP to NP. [sent-209, score-0.269]
</p><p>45 When only the rewards depend upon the actions of both agents, the problems become easier; however, when the situation is reversed, 4  the general problem remains NEXP-hard. [sent-210, score-0.425]
</p><p>46 The structure of rewards, while obviously key to the nature of the optimal (or otherwise) solution, is not as vital—even if agents can separate their individual reward-functions, making them entirely independent, other dependencies can still make the problem extremely complex. [sent-212, score-0.381]
</p><p>47 We therefore turn to two other interesting special-case Dec-MDP frameworks, in which independent reward functions are accompanied by restricted degrees of transition- and observation-based interaction. [sent-213, score-0.232]
</p><p>48 Such events can be thought of as occasions upon which that agent takes the given action to generate the associated state transition. [sent-220, score-0.764]
</p><p>49 Dependencies are then introduced in the form of relationships between one agent’s possible actions in given states and another agent’s primitive events. [sent-221, score-0.419]
</p><p>50 While no precise worst-case complexity results have been previously proven, the authors do point out that the class of problems has an upper-bound deterministic complexity that is exponential in the size of the state space, |S|, and doubly exponential in the number of deﬁned interactions. [sent-222, score-0.234]
</p><p>51 A history for an agent αi in a factored, n-agent Dec-POMDP D is a sequence of possible local states and actions, beginning in the agent’s initial state: Φi = [ˆ0 , a0 , s1 , a1 , . [sent-228, score-0.675]
</p><p>52 A primitive event e = (ˆi , ai , si ) for an agent αi is a triple s ˆ representing a transition between two local states, given some action ai ∈ Ai . [sent-237, score-1.314]
</p><p>53 A primitive event e occurs in the history Φi , written Φi e, if and only if the triple e is a sub-sequence of the sequence Φi . [sent-242, score-0.367]
</p><p>54 An event E occurs in the history Φi , written Φi E, if and only if some component occurs in that history: ∃e ∈ E : Φi e. [sent-243, score-0.206]
</p><p>55 If the historical sequence of state-action transitions that the agent encounters contains any one of those particular transitions, then the history satisﬁes the overall event. [sent-246, score-0.577]
</p><p>56 Events can thus be used, for example, to represent such things as taking a particular action in any one of a number of states over time, or taking one of several actions at some particular state. [sent-247, score-0.299]
</p><p>57 A primitive event e is proper if it occurs at most once in any given history. [sent-250, score-0.33]
</p><p>58 An event E is proper if it consists of proper primitive events that are mutually i exclusive, in that no two of them both occur in any history: ∀Φi ¬∃x, y : (x = y) ∧ (ex ∈ E) ∧ (ey ∈ E) ∧ (Φi  ex ) ∧ (Φi  ey ). [sent-252, score-0.482]
</p><p>59 Proper primitive events can be used, for instance, to represent actions that take place at particular times (building the time into the local state si ∈ e). [sent-253, score-0.828]
</p><p>60 Since any given point in time can only occur ˆ once in any history, the events involving such time-steps will be proper by default. [sent-254, score-0.205]
</p><p>61 A proper event 5  E can then be formed by collecting all the primitive events involving some single time-step, or by taking all possible primitive events involving an unrepeatable action. [sent-255, score-0.799]
</p><p>62 Local full observability: each agent αi can determine its own portion of the state-space, si ∈ S0 × Si , exactly. [sent-261, score-0.624]
</p><p>63 s s Interactions between agents are given in terms of a set of dependencies between certain state-action transitions for one agent, and events featuring transitions involving the other agent. [sent-264, score-0.623]
</p><p>64 Thus, if a history contains one of the primitive events from the latter set, this can have some direct effect upon the transition-model for the ﬁrst agent, introducing probabilistic transition-dependencies. [sent-265, score-0.464]
</p><p>65 ij ij  Such a dependency is thus a collection of possible actions that agent αj can take in one of its local state, each of which depends upon whether the other agent αi has made one of the state-transitions in its own set of primitive events. [sent-268, score-1.437]
</p><p>66 Such structures can be used to model, for instance, cases where one agent cannot successfully complete some task until the other agent has completed an enabling sub-task, or where the precise outcome depends upon the groundwork laid by the other agent. [sent-269, score-0.966]
</p><p>67 A dependency dk = Ei , Dj is satisﬁed when the ij k current history for enabling agent αi contains the relevant event: Φi Ei . [sent-271, score-0.654]
</p><p>68 For any state-action pair sj , aj , we deﬁne a Boolean indicator variable bsj aj , which is true if and only if some dependency ˆ ˆ that contains the pair is satisﬁed: bsj aj = ˆ  1 0  k k k if (∃ dk = Ei , Dj ) sj , aj ∈ Dj & Φi ˆ ij otherwise. [sent-272, score-0.691]
</p><p>69 The transition function for our Dec-MDP is factored into two functions, P1 and P2 , each deﬁning the distribution over next possible local states: Pi (ˆi | si , ai , bsi ai ). [sent-275, score-0.756]
</p><p>70 We can thus write Pi (ˆi , ai , bsi ai , si ) for this transition probability. [sent-276, score-0.498]
</p><p>71 s ˆ s ˆ ˆ ˆ When agents take some action in a state for which dependencies exist, they observe whether or not the related events have occurred; that is, after taking any action aj in state sj , they can observe the state of indicator variable bsj aj . [sent-277, score-1.144]
</p><p>72 Factored, ﬁnite-horizon, n-agent Dec-MDPs with local full observability, independent rewards, and event-driven interactions are NEXP-complete. [sent-280, score-0.225]
</p><p>73 Local reward independence, which was not present in the original problem, is ensured by using event dependencies to affect future rewards of the other agent. [sent-284, score-0.442]
</p><p>74 Thus, local immediate rewards remain dependent only upon the actions of the individual agent, but the state in which that agent ﬁnds itself (and so the options available to its reward function) can depend upon events involving the other agent. [sent-285, score-1.411]
</p><p>75 ) 1 The model can be extended to n agents with little real difﬁculty. [sent-288, score-0.27]
</p><p>76 A factored, ﬁnite-horizon, n-agent Dec-MDP with local full observability, independent rewards, and event-driven interactions are solvable in nondeterministic polynomial time (NP) if the number of dependencies is O(log |S|), where S is the state-set of the problem. [sent-297, score-0.339]
</p><p>77 First, NEXP-completeness of the event-based case, even with independent rewards and local full observability (Theorem 5), means that many interesting problems are potentially intractable. [sent-307, score-0.484]
</p><p>78 Second, isolating where complexity is lower can help determine what task structures and agent interrelationships lead to intractability. [sent-310, score-0.493]
</p><p>79 Such problems are not wholly decoupled, however, as the actions available to each agent at any point depend upon the global system state. [sent-316, score-0.823]
</p><p>80 Thus, agents interact by making choices that restrict or broaden the range of actions available to others. [sent-317, score-0.514]
</p><p>81 • Each Bi : S → 2Ai is a mapping from global states of the system to some set of available actions for each agent αi . [sent-320, score-0.754]
</p><p>82 • Ri : (S0 × Si ) → is a local reward function for agent αi . [sent-324, score-0.672]
</p><p>83 We let the global reward function be the sum of local rewards. [sent-325, score-0.28]
</p><p>84 Note that there need be no observations in such a problem; given local full observability, each agent observes only its local states. [sent-326, score-0.667]
</p><p>85 Furthermore, it is presumed that each agent can observe its own available actions in any state; a local policy is thus a mapping from local states to available actions. [sent-327, score-0.981]
</p><p>86 Again, we “record” actions of each agent in the state-space of the other, ensuring purely local rewards and local full observability. [sent-337, score-0.997]
</p><p>87 That is, we add special state-dependent actions that, based on their availability (or lack thereof), affect each agent’s local reward. [sent-339, score-0.278]
</p><p>88 1  Discussion of the result  Guo and Lesser [16, 14] were able to show that deciding whether a decentralized problem with state-based actions had an equilibrium solution with value greater than k was NP-hard. [sent-344, score-0.584]
</p><p>89 Such decentralized problems indeed appear to be quite simple in structure, requiring wholly independent rewards and action-transitions, so that agents can only interact with one another via choices that affect which actions are available. [sent-349, score-1.146]
</p><p>90 (A typical example involves two persons acting completely regardless of one another, except for the existence of a single rowboat, used for crossing a stream; if either agent uses the rowboat to get to the other side, then that action is no longer available to the other. [sent-350, score-0.56]
</p><p>91 At the same time, however, our results show that the same structures can be intractable in the worst case, establishing that even seemingly simple interactions between agents can lead to prohibitively high complexity in decentralized problems. [sent-352, score-0.838]
</p><p>92 6  Conclusions  This work addresses a number of existing models for decentralized problem-solving. [sent-353, score-0.407]
</p><p>93 In each case, the models restrict agent interaction in some way, in order to produce a special sub-case of the general Dec-POMDP problem. [sent-354, score-0.431]
</p><p>94 It has been known for some time that systems where agents act entirely independently, but share rewards, have reduced worst-case complexity. [sent-355, score-0.27]
</p><p>95 This fact, combined with results showing many other decentralized problem models to be equivalent to the general Dec-POMDP model, or strictly harder [17], reveals the essential difﬁculty of optimal planning in decentralized settings. [sent-358, score-0.904]
</p><p>96 Not all decentralized domains are going to be intractable, and indeed the event-based and action-set models have been shown to yield to specialized solution methods in many cases, enabling us to solve interesting instances in reasonable amounts of time. [sent-361, score-0.48]
</p><p>97 When the number of actiondependencies is small, or there are few ways that agents can affect available action-sets, it may well be possible to provide optimal solutions effectively. [sent-362, score-0.296]
</p><p>98 The complexity of decentralized control of Markov decision processes. [sent-373, score-0.521]
</p><p>99 The complexity of decentralized control of Markov decision processes. [sent-377, score-0.521]
</p><p>100 Formal models and algorithms for decentralized decision making under uncertainty. [sent-451, score-0.459]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agent', 0.431), ('decentralized', 0.407), ('agents', 0.27), ('tiling', 0.204), ('primitive', 0.187), ('actions', 0.177), ('si', 0.159), ('factored', 0.157), ('rewards', 0.153), ('shlomo', 0.141), ('reward', 0.14), ('observability', 0.127), ('events', 0.127), ('oi', 0.121), ('ai', 0.116), ('local', 0.101), ('lesser', 0.094), ('aj', 0.09), ('history', 0.088), ('observable', 0.085), ('dependencies', 0.083), ('becker', 0.078), ('state', 0.077), ('independence', 0.076), ('victor', 0.072), ('nexp', 0.072), ('bernstein', 0.071), ('transition', 0.071), ('sn', 0.067), ('action', 0.067), ('dj', 0.066), ('event', 0.066), ('sj', 0.065), ('planning', 0.065), ('policy', 0.064), ('autonomous', 0.064), ('cooperative', 0.063), ('zilberstein', 0.063), ('upon', 0.062), ('complexity', 0.062), ('transitions', 0.058), ('guo', 0.056), ('states', 0.055), ('supplemental', 0.054), ('interactions', 0.054), ('anyuan', 0.054), ('bsj', 0.054), ('claudia', 0.054), ('raphen', 0.054), ('decision', 0.052), ('proper', 0.051), ('ei', 0.051), ('materials', 0.048), ('dependency', 0.048), ('joint', 0.048), ('board', 0.047), ('establishing', 0.045), ('dk', 0.045), ('nition', 0.044), ('iff', 0.043), ('enabling', 0.042), ('pi', 0.041), ('interact', 0.041), ('multiagent', 0.04), ('fully', 0.04), ('partially', 0.039), ('global', 0.039), ('massachusetts', 0.037), ('independent', 0.036), ('bsi', 0.036), ('decker', 0.036), ('goldsmith', 0.036), ('posgs', 0.036), ('rowboat', 0.036), ('taems', 0.036), ('full', 0.034), ('amherst', 0.034), ('problems', 0.033), ('reduction', 0.032), ('domains', 0.031), ('solvable', 0.031), ('tile', 0.031), ('ri', 0.031), ('bound', 0.031), ('theorem', 0.031), ('np', 0.029), ('wholly', 0.029), ('vital', 0.029), ('accompanied', 0.029), ('observation', 0.028), ('individual', 0.028), ('neil', 0.027), ('restricted', 0.027), ('involving', 0.027), ('available', 0.026), ('system', 0.026), ('occurs', 0.026), ('martin', 0.025), ('nitions', 0.025), ('showing', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="53-tfidf-1" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>2 0.38516298 <a title="53-tfidf-2" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>Author: Finale Doshi-velez</p><p>Abstract: The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difﬁcult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We deﬁne an inﬁnite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems. 1</p><p>3 0.3727735 <a title="53-tfidf-3" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>4 0.21580371 <a title="53-tfidf-4" href="./nips-2009-Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining.html">218 nips-2009-Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></p>
<p>Author: George Konidaris, Andre S. Barreto</p><p>Abstract: We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance beneﬁts in a challenging continuous domain. 1</p><p>5 0.13513424 <a title="53-tfidf-5" href="./nips-2009-Learning_to_Explore_and_Exploit_in_POMDPs.html">134 nips-2009-Learning to Explore and Exploit in POMDPs</a></p>
<p>Author: Chenghui Cai, Xuejun Liao, Lawrence Carin</p><p>Abstract: A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the speciﬁc problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.</p><p>6 0.13269897 <a title="53-tfidf-6" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>7 0.13147318 <a title="53-tfidf-7" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>8 0.1185194 <a title="53-tfidf-8" href="./nips-2009-Solving_Stochastic_Games.html">221 nips-2009-Solving Stochastic Games</a></p>
<p>9 0.11649932 <a title="53-tfidf-9" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>10 0.11132964 <a title="53-tfidf-10" href="./nips-2009-A_Parameter-free_Hedging_Algorithm.html">14 nips-2009-A Parameter-free Hedging Algorithm</a></p>
<p>11 0.10118252 <a title="53-tfidf-11" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>12 0.086311862 <a title="53-tfidf-12" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>13 0.076582827 <a title="53-tfidf-13" href="./nips-2009-Robust_Value_Function_Approximation_Using_Bilinear_Programming.html">209 nips-2009-Robust Value Function Approximation Using Bilinear Programming</a></p>
<p>14 0.072569102 <a title="53-tfidf-14" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>15 0.070400678 <a title="53-tfidf-15" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>16 0.069511689 <a title="53-tfidf-16" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>17 0.064563312 <a title="53-tfidf-17" href="./nips-2009-A_Generalized_Natural_Actor-Critic_Algorithm.html">12 nips-2009-A Generalized Natural Actor-Critic Algorithm</a></p>
<p>18 0.063132904 <a title="53-tfidf-18" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>19 0.060405493 <a title="53-tfidf-19" href="./nips-2009-Noisy_Generalized_Binary_Search.html">166 nips-2009-Noisy Generalized Binary Search</a></p>
<p>20 0.055515625 <a title="53-tfidf-20" href="./nips-2009-Monte_Carlo_Sampling_for_Regret_Minimization_in_Extensive_Games.html">156 nips-2009-Monte Carlo Sampling for Regret Minimization in Extensive Games</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.013), (2, 0.257), (3, -0.302), (4, -0.37), (5, 0.024), (6, 0.046), (7, -0.039), (8, -0.021), (9, 0.034), (10, 0.144), (11, 0.112), (12, 0.076), (13, 0.031), (14, 0.033), (15, 0.041), (16, -0.013), (17, 0.054), (18, 0.009), (19, 0.068), (20, 0.007), (21, 0.082), (22, -0.249), (23, -0.001), (24, 0.191), (25, -0.02), (26, 0.112), (27, -0.074), (28, -0.038), (29, -0.055), (30, 0.027), (31, -0.009), (32, -0.005), (33, 0.03), (34, 0.072), (35, 0.011), (36, -0.009), (37, -0.046), (38, 0.036), (39, -0.027), (40, -0.052), (41, -0.028), (42, -0.071), (43, 0.021), (44, 0.006), (45, -0.021), (46, 0.061), (47, 0.016), (48, -0.051), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97011387 <a title="53-lsi-1" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>2 0.91699356 <a title="53-lsi-2" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>3 0.88782358 <a title="53-lsi-3" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>Author: Finale Doshi-velez</p><p>Abstract: The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difﬁcult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We deﬁne an inﬁnite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems. 1</p><p>4 0.87504131 <a title="53-lsi-4" href="./nips-2009-Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining.html">218 nips-2009-Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></p>
<p>Author: George Konidaris, Andre S. Barreto</p><p>Abstract: We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance beneﬁts in a challenging continuous domain. 1</p><p>5 0.72280878 <a title="53-lsi-5" href="./nips-2009-Learning_to_Explore_and_Exploit_in_POMDPs.html">134 nips-2009-Learning to Explore and Exploit in POMDPs</a></p>
<p>Author: Chenghui Cai, Xuejun Liao, Lawrence Carin</p><p>Abstract: A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the speciﬁc problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.</p><p>6 0.47094858 <a title="53-lsi-6" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>7 0.44933993 <a title="53-lsi-7" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>8 0.39939907 <a title="53-lsi-8" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>9 0.365753 <a title="53-lsi-9" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>10 0.35034955 <a title="53-lsi-10" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>11 0.34942389 <a title="53-lsi-11" href="./nips-2009-Solving_Stochastic_Games.html">221 nips-2009-Solving Stochastic Games</a></p>
<p>12 0.34536624 <a title="53-lsi-12" href="./nips-2009-A_Parameter-free_Hedging_Algorithm.html">14 nips-2009-A Parameter-free Hedging Algorithm</a></p>
<p>13 0.3202211 <a title="53-lsi-13" href="./nips-2009-A_Generalized_Natural_Actor-Critic_Algorithm.html">12 nips-2009-A Generalized Natural Actor-Critic Algorithm</a></p>
<p>14 0.31825402 <a title="53-lsi-14" href="./nips-2009-Multi-Step_Dyna_Planning_for_Policy_Evaluation_and_Control.html">159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</a></p>
<p>15 0.31564999 <a title="53-lsi-15" href="./nips-2009-Bayesian_Belief_Polarization.html">39 nips-2009-Bayesian Belief Polarization</a></p>
<p>16 0.30186215 <a title="53-lsi-16" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>17 0.25933772 <a title="53-lsi-17" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>18 0.25718591 <a title="53-lsi-18" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>19 0.25115052 <a title="53-lsi-19" href="./nips-2009-A_Data-Driven_Approach_to_Modeling_Choice.html">7 nips-2009-A Data-Driven Approach to Modeling Choice</a></p>
<p>20 0.24055216 <a title="53-lsi-20" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.041), (25, 0.05), (35, 0.038), (36, 0.062), (39, 0.026), (58, 0.055), (61, 0.062), (71, 0.518), (86, 0.049), (91, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.966048 <a title="53-lda-1" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>Author: Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</p><p>Abstract: We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject ﬁrst studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle ﬁlter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability. 1</p><p>same-paper 2 0.96271282 <a title="53-lda-2" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>3 0.95858651 <a title="53-lda-3" href="./nips-2009-Localizing_Bugs_in_Program_Executions_with_Graphical_Models.html">143 nips-2009-Localizing Bugs in Program Executions with Graphical Models</a></p>
<p>Author: Laura Dietz, Valentin Dallmeier, Andreas Zeller, Tobias Scheffer</p><p>Abstract: We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reﬂects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the Bernoulli graph model on data of the software projects AspectJ and Rhino. 1</p><p>4 0.92374778 <a title="53-lda-4" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>Author: Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</p><p>Abstract: Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. 1</p><p>5 0.83312309 <a title="53-lda-5" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><p>6 0.8319971 <a title="53-lda-6" href="./nips-2009-Learning_from_Multiple_Partially_Observed_Views_-_an_Application_to_Multilingual_Text_Categorization.html">130 nips-2009-Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization</a></p>
<p>7 0.75915998 <a title="53-lda-7" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>8 0.74497372 <a title="53-lda-8" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>9 0.65927184 <a title="53-lda-9" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>10 0.65102386 <a title="53-lda-10" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>11 0.63806391 <a title="53-lda-11" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>12 0.63506794 <a title="53-lda-12" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>13 0.62677938 <a title="53-lda-13" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>14 0.61331403 <a title="53-lda-14" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>15 0.60868222 <a title="53-lda-15" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>16 0.60711753 <a title="53-lda-16" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>17 0.60318351 <a title="53-lda-17" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>18 0.60012472 <a title="53-lda-18" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>19 0.59983677 <a title="53-lda-19" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>20 0.59300512 <a title="53-lda-20" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
