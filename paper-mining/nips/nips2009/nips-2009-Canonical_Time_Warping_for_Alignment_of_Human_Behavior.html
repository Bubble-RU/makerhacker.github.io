<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-50" href="#">nips2009-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</h1>
<br/><p>Source: <a title="nips-2009-50-pdf" href="http://papers.nips.cc/paper/3728-canonical-time-warping-for-alignment-of-human-behavior.pdf">pdf</a></p><p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>Reference: <a title="nips-2009-50-reference" href="../nips2009_reference/nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Alignment of time series is an important problem to solve in many scientiﬁc disciplines. [sent-5, score-0.056]
</p><p>2 In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. [sent-6, score-0.48]
</p><p>3 In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. [sent-7, score-0.951]
</p><p>4 CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. [sent-8, score-0.538]
</p><p>5 We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. [sent-9, score-0.782]
</p><p>6 Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. [sent-10, score-0.227]
</p><p>7 1  Introduction  Temporal alignment of time series has been an active research topic in many scientiﬁc disciplines such as bioinformatics, text analysis, computer graphics, and computer vision. [sent-11, score-0.3]
</p><p>8 In particular, temporal alignment of human behavior is a fundamental step in many applications such as recognition [1], temporal segmentation [2] and synthesis of human motion [3]. [sent-12, score-0.482]
</p><p>9 1a which shows one subject walking with varying speed and different styles and Fig. [sent-14, score-0.052]
</p><p>10 1b which shows two subjects reading the same text. [sent-15, score-0.049]
</p><p>11 Previous work on alignment of human motion has been addressed mostly in the context of recognizing human activities and synthesizing realistic motion. [sent-16, score-0.42]
</p><p>12 Typically, some models such as hidden Markov models [4, 5, 6], weighted principal component analysis [7], independent component analysis [8, 9] or multi-linear models [10] are learned from training data and in the testing phase the time series is aligned w. [sent-17, score-0.094]
</p><p>13 In the context of computer vision a key aspect for successful recognition of activities is building view-invariant representations. [sent-21, score-0.033]
</p><p>14 [1] proposed a view-invariant descriptor for actions making use of the afﬁnity matrix between time instances. [sent-23, score-0.053]
</p><p>15 Caspi and Irani [11] temporally aligned videos from two closely attached cameras. [sent-24, score-0.038]
</p><p>16 [12, 13] aligned trajectories of two moving points using constraints from the fundamental matrix. [sent-26, score-0.038]
</p><p>17 [3] proposed the iterative motion warping, a method that ﬁnds a spatio-temporal warping between two instances of motion captured data. [sent-28, score-0.446]
</p><p>18 In the context of data mining there have been several extensions of DTW [14] to align time series. [sent-29, score-0.149]
</p><p>19 Keogh and Pazzani [15] used derivatives of the original signal to improve alignment with DTW. [sent-30, score-0.227]
</p><p>20 [16] proposed continuous proﬁle models, a probabilistic method for simultaneously aligning and normalizing sets of time series. [sent-32, score-0.104]
</p><p>21 A relatively unexplored problem in behavioral analysis is the alignment between the motion of the body of face in two or more subjects (e. [sent-33, score-0.371]
</p><p>22 Major challenges to solve human motion align1  (a)  (b)  Figure 1: Temporal alignment of human behavior. [sent-37, score-0.37]
</p><p>23 (a) One person walking in normal pose, slow speed, another viewpoint and exaggerated steps (clockwise). [sent-38, score-0.049]
</p><p>24 ment problems are: (i) allowing alignment between different sets of multidimensional features (e. [sent-40, score-0.252]
</p><p>25 , audio/video), (ii) introducing a feature selection or feature weighting mechanism to compensate for subject variability or irrelevant features and (iii) execution rate [17]. [sent-42, score-0.087]
</p><p>26 To solve these problems, this paper proposes canonical time warping (CTW) for accurate spatio-temporal alignment between two behavioral time series. [sent-43, score-0.735]
</p><p>27 We pose the problem as ﬁnding the temporal alignment that maximizes the spatial correlation between two behavioral samples coming from two subjects. [sent-44, score-0.412]
</p><p>28 To accommodate for subject variability and take into account the difference in the dimensionally of the signals, CTW uses CCA as a measure of spatial alignment. [sent-45, score-0.11]
</p><p>29 CTW extends DTW by adding a feature weighting mechanism that is able to align signals of different dimensionality. [sent-47, score-0.251]
</p><p>30 CTW also extends CCA by incorporating time warping and allowing local spatial transformations. [sent-48, score-0.444]
</p><p>31 Section 2 reviews related work on dynamic time warping and canonical correlation analysis. [sent-50, score-0.541]
</p><p>32 Section 4 extends CTW to take into account local transformations. [sent-52, score-0.047]
</p><p>33 2  Previous work  This section describes previous work on canonical correlation analysis and dynamic time warping. [sent-54, score-0.241]
</p><p>34 1 Canonical correlation analysis Canonical correlation analysis (CCA) [18] is a technique to extract common features from a pair of multivariate data. [sent-56, score-0.084]
</p><p>35 The pair of canonical variates T T (vx X, vy Y) is uncorrelated with other canonical variates of lower order. [sent-61, score-0.681]
</p><p>36 Each successive canonical variate pair achieves the maximum correlation orthogonal to the preceding pairs. [sent-62, score-0.181]
</p><p>37 1 has a closed form solution in terms of a generalized eigenvalue problem. [sent-64, score-0.02]
</p><p>38 See [19] for a uniﬁcation of several component analysis methods and a review of numerical techniques to efﬁciently solve the generalized eigenvalue problems. [sent-65, score-0.02]
</p><p>39 In computer vision, CCA has been used for matching sets of images in problems such as activity recognition from video [20] and activity correlation from cameras [21]. [sent-66, score-0.042]
</p><p>40 [22] Bold capital letters denote a matrix X, bold lower-case letters a column vector x. [sent-68, score-0.063]
</p><p>41 xij denotes the scalar in the ith row and j th column of the matrix X. [sent-70, score-0.046]
</p><p>42 1m×n , 0m×n ∈ Rm×n are matrices of ones and zeros. [sent-72, score-0.02]
</p><p>43 proposed an extension of CCA with parameterized warping functions to align protein expressions. [sent-85, score-0.419]
</p><p>44 The learned warping function is a linear combination of hyperbolic tangent functions with nonnegative coefﬁcients, ensuring monotonicity. [sent-86, score-0.3]
</p><p>45 Unlike our method, the warping function is unable to deal with feature weighting. [sent-87, score-0.3]
</p><p>46 The correspondence matrix P can be parameterized by a pair of path vectors, P = [px , py ]T ∈ R2×m , in which px ∈ {1 : nx }m×1 and py ∈ {1 : ny }m×1 denote the composition of alignment in frames. [sent-90, score-0.641]
</p><p>47 For instance, the ith frame in X and the j th frame in Y are aligned iff there exists pt = [px , py ]T = [i, j]T t t for some t. [sent-91, score-0.378]
</p><p>48 P has to satisfy three additional constraints: boundary condition (p1 ≡ [1, 1]T and pm ≡ [nx , ny ]T ), continuity (0 ≤ pt − pt−1 ≤ 1) and monotonicity (t1 ≥ t2 ⇒ pt1 − pt2 ≥ 0). [sent-92, score-0.179]
</p><p>49 The policy function, π : {1 : nx } × {1 : ny } → {[1, 0]T , [0, 1]T , [1, 1]T }, deﬁnes the deterministic transition between consecutive steps, pt+1 = pt + π(pt ). [sent-94, score-0.4]
</p><p>50 Once the policy queue is known, the alignment steps can be recursively constructed from the starting point, p1 = [1, 1]T . [sent-95, score-0.29]
</p><p>51 2 shows an example of DTW to align two 1-D time series. [sent-97, score-0.149]
</p><p>52 3  Canonical time warping (CTW)  This section describes the energy function and optimization strategies for CTW. [sent-98, score-0.358]
</p><p>53 1 Energy function for CTW In order to have a compact and compressible energy function for CTW, it is important to notice that Eq. [sent-100, score-0.045]
</p><p>54 2 can be rewritten as: nx  ny y x wi T wj x i − y j  Jdtw (Wx , Wy ) =  2  T T = XWx − YWy  2 F,  (4)  i=1 j=1  where Wx ∈ {0, 1}m×nx , Wy ∈ {0, 1}m×ny are binary selection matrices that need to be inferred to align X and Y. [sent-101, score-0.388]
</p><p>55 4 the matrices Wx and Wy encode the alignment path. [sent-103, score-0.247]
</p><p>56 For instance, 3  y x wtpx = wtpy = 1 assigns correspondence between the px th frame in X and py t t t  th  frame in Y. [sent-104, score-0.283]
</p><p>57 CCA applies a linear transformation to the rows (features), while DTW applies binary transformations to the columns (time). [sent-109, score-0.032]
</p><p>58 In order to accommodate for differences in style and subject variability, add a feature selection mechT T anism, and reduce the dimensionality of the signals, CTW adds a linear transformation (Vx , Vy ) (as CCA) to the least-squares form of DTW (Eq. [sent-110, score-0.078]
</p><p>59 Moreover, this transformation allows aligning temporal signals with different dimensionality (e. [sent-112, score-0.204]
</p><p>60 CTW combines DTW and CCA by minimizing: T T T T Jctw (Wx , Wy , Vx , Vy ) = Vx XWx − Vy YWy dx ×b  2 F,  (5)  dy ×b  where Vx ∈ R , Vy ∈ R , b ≤ min(dx , dy ) parameterize the spatial warping by projecting the sequences into the same coordinate system. [sent-115, score-0.526]
</p><p>61 Wx and Wy warp the signal in time to achieve optimum temporal alignment. [sent-116, score-0.103]
</p><p>62 CTW is a direct and clean extension of CCA and DTW to align two signals X and Y in space and time. [sent-120, score-0.161]
</p><p>63 It extends previous work on CCA by adding temporal alignment and on DTW by allowing a feature selection and dimensionality reduction mechanism for aligning signals of different dimensions. [sent-121, score-0.492]
</p><p>64 We alternate between solving for Wx , Wy using DTW, and optimally computing the spatial projections using CCA. [sent-124, score-0.059]
</p><p>65 These steps monotonically decrease Jctw and since the function is bounded below it will converge to a critical point. [sent-125, score-0.019]
</p><p>66 Alternatively, PCA can be applied independently to each set, and used as initial estimation of Vx and Vy if dx = dy . [sent-132, score-0.117]
</p><p>67 In the case of high-dimensional data, the generalized eigenvalue problem is solved by regularizing the covariance matrices adding a scaled identity matrix. [sent-133, score-0.04]
</p><p>68 We consider the algorithm to converge when the difference between two consecutive values of Jctw is small. [sent-135, score-0.017]
</p><p>69 4  Local canonical time warping (LCTW)  In the previous section we have illustrated how CTW can align in space and time two time series of different dimensionality. [sent-136, score-0.657]
</p><p>70 , aligning long sequences) where a global transformation of the whole time series is not accurate. [sent-139, score-0.162]
</p><p>71 This section extends CTW by allowing multiple local spatial deformations. [sent-141, score-0.114]
</p><p>72 1 Energy function for LCTW Let us assume that the spatial transformation for each frame in X and Y can be model as a x x linear combination of kx or ky bases. [sent-143, score-0.305]
</p><p>73 Let be Vx = [V1 T , · · · , Vkx T ]T ∈ Rkx dx ×b , Vy = yT y T T [V1 , · · · , Vky ] ∈ Rky dy ×b and b ≤ min(kx dx , ky dy ). [sent-144, score-0.323]
</p><p>74 ricx denotes the coefﬁcient (or y weight) of the cth basis for the ith frame of X (similarly for rjcy ). [sent-146, score-0.178]
</p><p>75 The last two regularization terms, Fx ∈ Rnx ×nx , Fy ∈ Rny ×ny , are 1st order differential operators of rxx ∈ Rnx ×1 , ryy ∈ Rny ×1 , encouraging smooth solutions over time. [sent-152, score-0.078]
</p><p>76 c c Observe that Jctw is a special case of Jlctw when kx = ky = 1. [sent-153, score-0.173]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ctw', 0.379), ('vx', 0.369), ('vy', 0.369), ('warping', 0.3), ('cca', 0.266), ('alignment', 0.227), ('dtw', 0.216), ('wy', 0.2), ('wx', 0.178), ('nx', 0.16), ('canonical', 0.122), ('align', 0.119), ('jctw', 0.118), ('pt', 0.111), ('ry', 0.095), ('ky', 0.089), ('kx', 0.084), ('aligning', 0.074), ('motion', 0.073), ('ny', 0.068), ('dy', 0.067), ('py', 0.067), ('rx', 0.062), ('jdtw', 0.059), ('lctw', 0.059), ('ricx', 0.059), ('ywy', 0.059), ('frame', 0.058), ('temporal', 0.056), ('cy', 0.056), ('px', 0.052), ('rnx', 0.052), ('rny', 0.052), ('xwx', 0.052), ('cx', 0.051), ('dx', 0.05), ('extends', 0.047), ('dynamic', 0.047), ('fy', 0.044), ('policy', 0.044), ('spatial', 0.042), ('correlation', 0.042), ('signals', 0.042), ('fx', 0.042), ('idx', 0.039), ('idy', 0.039), ('jlctw', 0.039), ('rdx', 0.039), ('rjcy', 0.039), ('rxx', 0.039), ('ryy', 0.039), ('xwyt', 0.039), ('ydy', 0.039), ('aligned', 0.038), ('human', 0.035), ('variates', 0.034), ('xdx', 0.034), ('activities', 0.033), ('transformation', 0.032), ('ib', 0.032), ('time', 0.03), ('walking', 0.03), ('xt', 0.029), ('energy', 0.028), ('subjects', 0.028), ('behavioral', 0.026), ('series', 0.026), ('allowing', 0.025), ('yt', 0.024), ('th', 0.024), ('accommodate', 0.024), ('actions', 0.023), ('variability', 0.022), ('weighting', 0.022), ('letters', 0.022), ('subject', 0.022), ('ith', 0.022), ('mechanism', 0.021), ('wj', 0.021), ('reading', 0.021), ('robotics', 0.021), ('matrices', 0.02), ('eigenvalue', 0.02), ('pose', 0.019), ('steps', 0.019), ('bold', 0.019), ('graphics', 0.018), ('rt', 0.018), ('optimally', 0.017), ('unexplored', 0.017), ('xpx', 0.017), ('compressible', 0.017), ('disciplines', 0.017), ('synthesizing', 0.017), ('variate', 0.017), ('vectorization', 0.017), ('warp', 0.017), ('xnx', 0.017), ('yyt', 0.017), ('consecutive', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="50-tfidf-1" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>2 0.31393927 <a title="50-tfidf-2" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>3 0.15344742 <a title="50-tfidf-3" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>Author: Hamed Pirsiavash, Deva Ramanan, Charless C. Fowlkes</p><p>Abstract: We describe an algorithm for learning bilinear SVMs. Bilinear classiﬁers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classiﬁer yields a separable ﬁlter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classiﬁers. We also use bilinear classiﬁers for transfer learning by sharing linear factors between different classiﬁcation tasks. Bilinear classiﬁers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difﬁcult problems of people detection in video sequences and action classiﬁcation of video sequences, achieving state-of-the-art results in both. 1</p><p>4 0.11611633 <a title="50-tfidf-4" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>5 0.085320503 <a title="50-tfidf-5" href="./nips-2009-fMRI-Based_Inter-Subject_Cortical_Alignment_Using_Functional_Connectivity.html">261 nips-2009-fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity</a></p>
<p>Author: Bryan Conroy, Ben Singer, James Haxby, Peter J. Ramadge</p><p>Abstract: The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment. 1</p><p>6 0.074777782 <a title="50-tfidf-6" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>7 0.066367842 <a title="50-tfidf-7" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>8 0.064645544 <a title="50-tfidf-8" href="./nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</a></p>
<p>9 0.053623799 <a title="50-tfidf-9" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>10 0.052206557 <a title="50-tfidf-10" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>11 0.049228132 <a title="50-tfidf-11" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>12 0.043699548 <a title="50-tfidf-12" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>13 0.039530497 <a title="50-tfidf-13" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>14 0.039343014 <a title="50-tfidf-14" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>15 0.036142994 <a title="50-tfidf-15" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>16 0.033657365 <a title="50-tfidf-16" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>17 0.033570573 <a title="50-tfidf-17" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>18 0.033010382 <a title="50-tfidf-18" href="./nips-2009-Time-Varying_Dynamic_Bayesian_Networks.html">246 nips-2009-Time-Varying Dynamic Bayesian Networks</a></p>
<p>19 0.032626815 <a title="50-tfidf-19" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>20 0.032528587 <a title="50-tfidf-20" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.112), (1, -0.029), (2, 0.005), (3, -0.011), (4, 0.009), (5, 0.032), (6, 0.089), (7, -0.071), (8, 0.019), (9, 0.04), (10, 0.099), (11, 0.021), (12, -0.143), (13, 0.044), (14, -0.203), (15, -0.061), (16, 0.18), (17, 0.164), (18, 0.055), (19, -0.027), (20, 0.115), (21, -0.063), (22, 0.125), (23, -0.204), (24, -0.159), (25, -0.039), (26, 0.237), (27, -0.008), (28, -0.11), (29, -0.183), (30, 0.016), (31, 0.139), (32, -0.069), (33, -0.013), (34, 0.012), (35, 0.129), (36, 0.037), (37, -0.057), (38, 0.064), (39, -0.051), (40, 0.003), (41, -0.021), (42, -0.06), (43, 0.09), (44, -0.061), (45, 0.058), (46, 0.011), (47, 0.132), (48, -0.046), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96844685 <a title="50-lsi-1" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>2 0.77084255 <a title="50-lsi-2" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>3 0.53572321 <a title="50-lsi-3" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>4 0.4296208 <a title="50-lsi-4" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>Author: Hamed Pirsiavash, Deva Ramanan, Charless C. Fowlkes</p><p>Abstract: We describe an algorithm for learning bilinear SVMs. Bilinear classiﬁers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classiﬁer yields a separable ﬁlter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classiﬁers. We also use bilinear classiﬁers for transfer learning by sharing linear factors between different classiﬁcation tasks. Bilinear classiﬁers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difﬁcult problems of people detection in video sequences and action classiﬁcation of video sequences, achieving state-of-the-art results in both. 1</p><p>5 0.2510426 <a title="50-lsi-5" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>Author: Yee W. Teh, Dilan Gorur</p><p>Abstract: The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the stable-beta process and deriving the IBP corresponding to it. We ﬁnd interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and ﬁnd that our power-law IBP is a good model for word occurrences in document corpora. 1</p><p>6 0.2385565 <a title="50-lsi-6" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>7 0.23505522 <a title="50-lsi-7" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>8 0.23219168 <a title="50-lsi-8" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>9 0.22916697 <a title="50-lsi-9" href="./nips-2009-fMRI-Based_Inter-Subject_Cortical_Alignment_Using_Functional_Connectivity.html">261 nips-2009-fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity</a></p>
<p>10 0.19390389 <a title="50-lsi-10" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>11 0.19390333 <a title="50-lsi-11" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>12 0.19284457 <a title="50-lsi-12" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>13 0.18994479 <a title="50-lsi-13" href="./nips-2009-The_Ordered_Residual_Kernel_for_Robust_Motion_Subspace_Clustering.html">243 nips-2009-The Ordered Residual Kernel for Robust Motion Subspace Clustering</a></p>
<p>14 0.18367806 <a title="50-lsi-14" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>15 0.17552884 <a title="50-lsi-15" href="./nips-2009-Robust_Value_Function_Approximation_Using_Bilinear_Programming.html">209 nips-2009-Robust Value Function Approximation Using Bilinear Programming</a></p>
<p>16 0.16741817 <a title="50-lsi-16" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>17 0.16537699 <a title="50-lsi-17" href="./nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</a></p>
<p>18 0.16489176 <a title="50-lsi-18" href="./nips-2009-Localizing_Bugs_in_Program_Executions_with_Graphical_Models.html">143 nips-2009-Localizing Bugs in Program Executions with Graphical Models</a></p>
<p>19 0.16444971 <a title="50-lsi-19" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>20 0.15778168 <a title="50-lsi-20" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.431), (24, 0.03), (25, 0.032), (35, 0.03), (36, 0.113), (39, 0.025), (57, 0.011), (58, 0.081), (61, 0.015), (71, 0.05), (86, 0.045), (91, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79317516 <a title="50-lda-1" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>2 0.58953995 <a title="50-lda-2" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>Author: Douglas Eck, Yoshua Bengio, Aaron C. Courville</p><p>Abstract: The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an inﬁnite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer deﬁnes a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment. 1</p><p>3 0.48224801 <a title="50-lda-3" href="./nips-2009-Ranking_Measures_and_Loss_Functions_in_Learning_to_Rank.html">199 nips-2009-Ranking Measures and Loss Functions in Learning to Rank</a></p>
<p>Author: Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, Hang Li</p><p>Abstract: Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learningto-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measurebased ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classiﬁcation tasks, and deﬁne a so-called essential loss for ranking as the weighted sum of the classiﬁcation errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modiﬁcations can lead to better ranking performances, demonstrating the correctness of our theoretical analysis. 1</p><p>4 0.34776151 <a title="50-lda-4" href="./nips-2009-Accelerated_Gradient_Methods_for_Stochastic_Optimization_and_Online_Learning.html">22 nips-2009-Accelerated Gradient Methods for Stochastic Optimization and Online Learning</a></p>
<p>Author: Chonghai Hu, Weike Pan, James T. Kwok</p><p>Abstract: Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., ℓ1 -regularizer). Gradient methods, though highly scalable and easy to implement, are known to converge slowly. In this paper, we develop a novel accelerated gradient method for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic composite optimization with convex or strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple algorithm but with the best regret bounds currently known for these problems. 1</p><p>5 0.3474665 <a title="50-lda-5" href="./nips-2009-Efficient_Learning_using_Forward-Backward_Splitting.html">76 nips-2009-Efficient Learning using Forward-Backward Splitting</a></p>
<p>Author: Yoram Singer, John C. Duchi</p><p>Abstract: We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We 2 also show how to construct efﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets. 1</p><p>6 0.34736902 <a title="50-lda-6" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>7 0.34652182 <a title="50-lda-7" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>8 0.34639353 <a title="50-lda-8" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>9 0.34566835 <a title="50-lda-9" href="./nips-2009-Adaptive_Regularization_of_Weight_Vectors.html">27 nips-2009-Adaptive Regularization of Weight Vectors</a></p>
<p>10 0.34523723 <a title="50-lda-10" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>11 0.34514281 <a title="50-lda-11" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>12 0.34445742 <a title="50-lda-12" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>13 0.343853 <a title="50-lda-13" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>14 0.34385282 <a title="50-lda-14" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>15 0.34253439 <a title="50-lda-15" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>16 0.34193119 <a title="50-lda-16" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>17 0.34180811 <a title="50-lda-17" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>18 0.34081221 <a title="50-lda-18" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>19 0.34054846 <a title="50-lda-19" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>20 0.34037888 <a title="50-lda-20" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
