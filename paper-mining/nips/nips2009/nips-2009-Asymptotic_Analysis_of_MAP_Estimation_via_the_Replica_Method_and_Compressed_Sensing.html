<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-36" href="#">nips2009-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</h1>
<br/><p>Source: <a title="nips-2009-36-pdf" href="http://papers.nips.cc/paper/3635-asymptotic-analysis-of-map-estimation-via-the-replica-method-and-compressed-sensing.pdf">pdf</a></p><p>Author: Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</p><p>Abstract: The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.</p><p>Reference: <a title="nips-2009-36-reference" href="../nips2009_reference/nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. [sent-9, score-0.872]
</p><p>2 This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. [sent-10, score-0.762]
</p><p>3 It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. [sent-11, score-0.365]
</p><p>4 The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. [sent-12, score-0.746]
</p><p>5 u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. [sent-13, score-1.025]
</p><p>6 In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. [sent-14, score-0.527]
</p><p>7 Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability. [sent-15, score-0.807]
</p><p>8 1 Introduction Estimating a vector x ∈ Rn from measurements of the form y = Φx + w, (1) where Φ ∈ Rm×n represents a known measurement matrix and w ∈ Rm represents measurement errors or noise, is a generic problem that arises in a range of circumstances. [sent-16, score-0.143]
</p><p>9 One of the most basic estimators for x is the maximum a posteriori (MAP) estimate ˆ xmap (y) = arg max px|y (x|y), (2) x∈Rn  which is deﬁned assuming some prior on x. [sent-17, score-0.371]
</p><p>10 For most priors, the MAP estimate is nonlinear and its behavior is not easily characterizable. [sent-18, score-0.076]
</p><p>11 Even if the priors for x and w are separable, the analysis of the MAP estimate may be difﬁcult since the matrix Φ couples the n unknown components of x with the m measurements in the vector y. [sent-19, score-0.14]
</p><p>12 The primary contribution of this paper—an abridged version of [1]—is to show that with certain large random Φ and Gaussian w, there is an asymptotic decoupling of (1) into n scalar MAP estimation problems. [sent-20, score-0.305]
</p><p>13 Each equivalent scalar problem has an appropriate scalar prior and Gaussian noise with an effective noise level. [sent-21, score-0.555]
</p><p>14 The analysis yields the asymptotic joint distribution of each component ˆ xj of x and its corresponding estimate xj in the MAP estimate vector xmap (y). [sent-22, score-0.554]
</p><p>15 Our analysis is based on a powerful but non-rigorous technique from statistical physics known as the replica method. [sent-26, score-0.777]
</p><p>16 The replica method was originally developed by Edwards and Anderson [2] to study the statistical mechanics of spin glasses. [sent-27, score-0.797]
</p><p>17 The replica method was ﬁrst applied to the study of nonlinear MAP estimation problems by Tanaka [4] and M¨ ller [5]. [sent-29, score-0.81]
</p><p>18 These papers studied the behavior of the MAP estimator of a vector u x with i. [sent-30, score-0.144]
</p><p>19 binary components observed through linear measurements of the form (1) with a large random Φ and Gaussian w. [sent-33, score-0.075]
</p><p>20 Guo and Verd´ ’s result was also able to incoru u porate a large class of minimum postulated MSE estimators, where the estimator may assume a prior that is different from the actual prior. [sent-35, score-0.27]
</p><p>21 The non-rigorous aspect of the replica method involves a set of assumptions that include a selfaveraging property, the validity of a “replica trick,” and the ability to exchange certain limits. [sent-38, score-0.777]
</p><p>22 Also, some of the predictions of the replica method have been validated rigorously by other means [8]. [sent-40, score-0.746]
</p><p>23 As an application of our main result, we will develop a few analyses of estimation problems that arise in compressed sensing [9–11]. [sent-44, score-0.185]
</p><p>24 In compressed sensing, one estimates a sparse vector x from random linear measurements. [sent-45, score-0.132]
</p><p>25 Generically, optimal estimation of x with a sparse prior is NP-hard [12]. [sent-46, score-0.098]
</p><p>26 Thus, most attention has focused on greedy heuristics such as matching pursuit and convex relaxations such as basis pursuit [13] or lasso [14]. [sent-47, score-0.218]
</p><p>27 Recent compressed sensing research has provided scaling laws on numbers of measurements that guarantee good performance of these methods [15–17]. [sent-49, score-0.202]
</p><p>28 There are, of course, notable exceptions including [18] and [19] which provide matching necessary and sufﬁcient conditions for recovery of strictly sparse vectors with basis pursuit and lasso. [sent-51, score-0.142]
</p><p>29 However, even these results only consider exact recovery and are limited to measurements that are noise-free or measurements with a signal-to-noise ratio (SNR) that scales to inﬁnity. [sent-52, score-0.137]
</p><p>30 Many common sparse estimators can be seen as MAP estimators with certain postulated priors. [sent-53, score-0.403]
</p><p>31 Most importantly, lasso and basis pursuit are MAP estimators assuming a Laplacian prior. [sent-54, score-0.271]
</p><p>32 Other commonly-used sparse estimation algorithms, including linear estimation with and without thresholding and zero norm-regularized estimators, can also be seen as MAP-based estimators. [sent-55, score-0.184]
</p><p>33 For these algorithms, the replica method provides—under the assumption of the replica hypotheses—not just bounds, but the exact asymptotic behavior. [sent-56, score-1.573]
</p><p>34 2 Estimation Problem and Assumptions Consider the estimation of a random vector x ∈ Rn from linear measurements of the form y = Φx + w = AS1/2 x + w,  m  where y ∈ R is a vector of observations, Φ = AS a diagonal matrix of positive scale factors,  1/2  ,A∈R  m×n  S = diag (s1 , . [sent-59, score-0.13]
</p><p>35 , sn ) , sj > 0,  (3) is a measurement matrix, S is (4)  and w ∈ Rm is zero-mean, white Gaussian noise. [sent-62, score-0.09]
</p><p>36 2  The components xj of x are modeled as zero mean and i. [sent-65, score-0.141]
</p><p>37 The per-component variance of the Gaussian noise is E|wj |2 = σ0 . [sent-69, score-0.09]
</p><p>38 We use the subscript “0” on the prior and noise level to differentiate these quantities from certain “postulated” values to be deﬁned later. [sent-70, score-0.145]
</p><p>39 Variations in the power of x that are not known to the estimator should be captured in the distribution of x. [sent-78, score-0.098]
</p><p>40 We summarize the situation and make additional assumptions to specify the problem precisely as follows: (a) The number of measurements m = m(n) is a deterministic quantity that varies with n and satisﬁes lim n/m(n) = β n→∞  for some β ≥ 0. [sent-79, score-0.14]
</p><p>41 (c) (d) (e) (f)  2 The noise w is Gaussian with w ∼ N (0, σ0 Im ). [sent-85, score-0.09]
</p><p>42 The scale factor matrix S, measurement matrix A, vector x and noise w are independent. [sent-94, score-0.147]
</p><p>43 Suppose one is given a u 2 “postulated” prior distribution ppost and a postulated noise level σpost that may be different from 2 the true values p0 and σ0 . [sent-96, score-0.426]
</p><p>44 The Replica MMSE Claim describes the asymptotic behavior of the postulated MMSE estimator via an equivalent scalar estimator. [sent-98, score-0.52]
</p><p>45 2πµ  (7)  The distribution px|z (x|z ; q, µ) is the conditional distribution of the scalar random variable x ∼ q(x) from an observation of the form √ (8) z = x + µv, where v ∼ N (0, 1). [sent-101, score-0.159]
</p><p>46 Using this distribution, we can deﬁne the scalar conditional MMSE estimate, xmmse (z ; q, µ) = ˆscalar  xpx|z (x|z ; µ) dx. [sent-102, score-0.207]
</p><p>47 Let xmpmse (y) be the 2 MPMSE estimator based on a postulated prior ppost and postulated noise level σpost . [sent-106, score-0.725]
</p><p>48 (b) The effective noise levels satisfy the equations 2 σeﬀ 2 σp−eﬀ  2 = σ0 + βE [s mse(ppost , p0 , µp , µ, z)]  =  2 σpost  + βE [s mse(ppost , ppost , µp , µp , z)] ,  (12a) (12b)  where the expectations are taken over s ∼ pS (s) and z generated by (11). [sent-113, score-0.408]
</p><p>49 The Replica MMSE Claim asserts that the asymptotic behavior of the joint estimation of the ndimensional vector x can be described by n equivalent scalar estimators. [sent-114, score-0.359]
</p><p>50 In the scalar estimation problem, a component x ∼ p0 (x) is corrupted by additive Gaussian noise yielding a noisy mea2 surement z. [sent-115, score-0.352]
</p><p>51 The additive noise variance is µ = σeﬀ /s, which is the effective noise divided by the scale factor s. [sent-116, score-0.218]
</p><p>52 The estimate of that component is then described by the (generally nonlinear) scalar estimator x(z ; ppost , µp ). [sent-117, score-0.462]
</p><p>53 ˆ 2 2 The effective noise levels σeﬀ and σp−eﬀ are described by the solutions to ﬁxed-point equations 2 2 (12). [sent-118, score-0.217]
</p><p>54 Let X ⊆ R be some (measurable) set and consider an estimator of the form n 1 ˆ f (xj ), (13) y − AS1/2 x 2 + xmap (y) = arg min 2 x∈X n 2γ j=1 where γ > 0 is an algorithm parameter and f : X → R is some scalar-valued, non-negative cost function. [sent-122, score-0.315]
</p><p>55 The estimator (13) can be interpreted as a MAP estimator. [sent-124, score-0.098]
</p><p>56 Speciﬁcally, for any u > 0, it can be ˆ veriﬁed that xmap (y) is the MAP estimate 2 ˆ xmap (y) = arg max px|y (x | y ; pu , σu ), x∈X n  2 where pu (x) and σu are the prior and noise level −1  pu (x) =  exp(−uf (x))dx x∈X n  4  2 exp(−uf (x)), σu = γ/u,  (14)  where f (x) = estimators  j  f (xj ). [sent-125, score-0.851]
</p><p>57 To analyze this MAP estimator, we consider a sequence of MMSE  2 (15) xu (y) = E x | y ; pu , σu . [sent-126, score-0.097]
</p><p>58 The proof of the Replica MAP Claim below (see [1]) uses a standard large deviations argument to show that ˆ lim xu (y) = xmap (y) u→∞  for all y. [sent-127, score-0.273]
</p><p>59 Under the assumption that the behaviors of the MMSE estimators are described by the Replica MMSE Claim, we can then extrapolate the behavior of the MAP estimator. [sent-128, score-0.132]
</p><p>60 To state the claim, deﬁne the scalar MAP estimator xmap (z ; λ) = arg min F (x, z, λ), F (x, z, λ) = ˆscalar x∈X  1 |z − x|2 + f (x). [sent-129, score-0.457]
</p><p>61 We also assume that the limit |x − x|2 ˆ , x→ˆ 2(F (x, z, λ) − F (ˆ, z, λ)) x x  σ 2 (z, λ) = lim  (17)  exists where x = xmap (z; λ). [sent-131, score-0.244]
</p><p>62 We make the following additional assumptions: ˆ ˆscalar Assumption 1 Consider the MAP estimator (13) applied to the estimation problem in Section 2. [sent-132, score-0.148]
</p><p>63 Assume: 2 (a) For all u > 0 sufﬁciently large, assume the postulated prior pu and noise level σu satisfy the Replica MMSE Claim. [sent-133, score-0.373]
</p><p>64 Also, assume that for the corresponding effective noise levels, 2 2 σeﬀ (u) and σp−eﬀ (u), the following limits exists: 2 2 2 σeﬀ,map = lim σeﬀ (u), γp = lim uσp−eﬀ (u). [sent-134, score-0.285]
</p><p>65 u→∞  u→∞  (b) Suppose for each n, xu (n) is the MMSE estimate of the component xj for some index ˆj 2 j ∈ {1, . [sent-135, score-0.2]
</p><p>66 , n} based on the postulated prior pu and noise level σu . [sent-138, score-0.351]
</p><p>67 Then, assume that the following limits can be interchanged: lim lim xu (n) = lim lim xu (n), ˆj ˆj  u→∞ n→∞  n→∞ u→∞  where the limits are in distribution. [sent-139, score-0.372]
</p><p>68 Let xmap (y) be the MAP estimator (13) deﬁned for some f (x) and γ > 0 satisfying Assumption 1. [sent-146, score-0.279]
</p><p>69 Then: (a) As n → ∞, the random vectors (xj , sj , xmap ) converge in distribution to the random ˆj vector (x, s, x) where x, s, and v are independent with x ∼ p0 (x), s ∼ pS (s), v ∼ N (0, 1), ˆ and √ x = xmap (z, λp ), z = x + µv, ˆ ˆscalar (18) 2 where µ = σeﬀ,map /s and λp = γp /s. [sent-151, score-0.429]
</p><p>70 2 (b) The limiting effective noise levels σeﬀ,map and γp satisfy the equations 2 σeﬀ,map  γp  2 = σ0 + βE s|x − x|2 ˆ 2  = γ + βE sσ (z, λp ) ,  (19a) (19b)  where the expectations are taken over x ∼ p0 (x), s ∼ pS (s), and v ∼ N (0, 1), with x and ˆ z deﬁned in (18). [sent-152, score-0.265]
</p><p>71 5  Analogously to the Replica MMSE Claim, the Replica MAP Claim asserts that asymptotic behavior of the MAP estimate of any single component of x is described by a simple equivalent scalar estimator. [sent-153, score-0.354]
</p><p>72 In the equivalent scalar model, the component of the true vector x is corrupted by Gaussian noise and the estimate of that component is given by a scalar MAP estimate of the component from the noise-corrupted version. [sent-154, score-0.602]
</p><p>73 In this section, we ﬁrst consider choices of f that yield MAP estimators relevant to compressed sensing. [sent-157, score-0.189]
</p><p>74 We then additionally impose a sparse prior for x for numerical evaluations of asymptotic performance. [sent-158, score-0.146]
</p><p>75 We ﬁrst consider the lasso or basis pursuit estimate [13, 14] given by ˆ xlasso (y) = arg min x∈Rn  1 y − AS1/2 x 2γ  2 2  + x 1,  (20)  where γ > 0 is an algorithm parameter. [sent-160, score-0.22]
</p><p>76 This estimator is identical to the MAP estimator (13) with the cost function f (x) = |x|. [sent-161, score-0.213]
</p><p>77 With this cost function, the scalar MAP estimator in (16) is given by soft xmap (z ; λ) = Tλ (z), ˆscalar  (21)  soft where Tλ (z) is the soft thresholding operator soft Tλ (z) =  z − λ, if z > λ; 0, if |z| ≤ λ; z + λ, if z < −λ. [sent-162, score-0.668]
</p><p>78 Hence, the asymptotic behavior of lasso has a remarkably simple description: the asymptotic distribution of the lasso estimate xj of the ˆ component xj is identical to xj being corrupted by Gaussian noise and then soft-thresholded to yield the estimate xj . [sent-164, score-0.96]
</p><p>79 ˆ  To calculate the effective noise levels, one can perform a simple calculation to show that σ 2 (z, λ) in (17) is given by λ, if |z| > λ; σ 2 (z, λ) = (24) 0, if |z| ≤ λ. [sent-165, score-0.128]
</p><p>80 Substituting (21) and (25) into (19), we obtain the ﬁxed-point equations 2 σeﬀ,map  γp  2 soft = σ0 + βE s|x − Tλp (z)|2  (26a)  = γ + βγp Pr(|z| > γp /s),  (26b)  where the expectations are taken with respect to x ∼ p0 (x), s ∼ pS (s), and z in (23). [sent-167, score-0.109]
</p><p>81 Lasso can be regarded as a convex relaxation of zero normregularized estimation ˆ xzero (y) = arg min x∈Rn  1 y − AS1/2 x 2γ  2 2  + x 0,  (27)  where x 0 is the number of nonzero components of x. [sent-170, score-0.168]
</p><p>82 For certain strictly sparse priors, zero norm-regularized estimation may provide better performance than lasso. [sent-171, score-0.116]
</p><p>83 While computing the zero norm-regularized estimate is generally very difﬁcult, we can use the replica analysis to provide a simple characterization of its performance. [sent-172, score-0.801]
</p><p>84 The zero norm-regularized estimator is identical to the MAP estimator (13) with the cost function 0, if x = 0; 1, if x = 0. [sent-174, score-0.235]
</p><p>85 With f (x) given by (28), the scalar MAP estimator in (16) is given by √ xmap (z ; λ) = Tthard(z), ˆscalar t = 2λ,  (29)  where Tthard is the hard thresholding operator, Tthard (z) =  z, if |z| > t; 0, if |z| ≤ t. [sent-179, score-0.471]
</p><p>86 (32)  Thus, the zero norm-regularized estimation of a vector x is equivalent to n scalar components cor2 rupted by some effective noise level σeﬀ,map and hard-thresholded based on a effective noise level γp . [sent-181, score-0.575]
</p><p>87 2 The ﬁxed-point equations for the effective noise levels σeﬀ,map and γp can be computed similarly to the case of lasso. [sent-182, score-0.217]
</p><p>88 Plotted is the median normalized SE for various sparse recovery algorithms: linear MMSE estimation, lasso, zero normregularized estimation, and optimal MMSE estimation. [sent-192, score-0.176]
</p><p>89 Solid lines show the asymptotic predicted MSE from the Replica MAP Claim. [sent-193, score-0.081]
</p><p>90 For the linear and lasso estimators, the circles and triangles show the actual median SE over 1000 Monte Carlo simulations. [sent-194, score-0.132]
</p><p>91 For each problem size, we simulated the lasso and linear MMSE estimators over 1000 independent instances with noise levels chosen such that the SNR with perfect side information is 10 dB. [sent-207, score-0.339]
</p><p>92 The simulated performance is matched very closely by the asymptotic values predicted by the replica analysis. [sent-210, score-0.827]
</p><p>93 (Analysis of the linear MMSE estimator using the Replica MAP Claim is detailed in [1]; the Replica MMSE Claim is also applicable to this estimator. [sent-211, score-0.098]
</p><p>94 ) In addition, the replica analysis can be applied to zero norm-regularized and optimal MMSE estimators that are computationally infeasible for large problems. [sent-212, score-0.871]
</p><p>95 1, illustrating the potential of the replica method to quantify the precise performance losses of practical algorithms. [sent-214, score-0.746]
</p><p>96 Additional numerical simulations in [1] illustrate convergence to the replica MAP limit, applicability to discrete distributions for x, effects of power variations in the components, and accurate prediction of the probability of sparsity pattern recovery. [sent-215, score-0.797]
</p><p>97 6 Conclusions We have shown that the behavior of vector MAP estimators with large random measurement matrices and Gaussian noise asymptotically matches that of a set of decoupled scalar estimation problems. [sent-216, score-0.488]
</p><p>98 We believe that this equivalence to a simple scalar model will open up numerous doors for analysis, particularly in problems of interest in compressed sensing. [sent-217, score-0.245]
</p><p>99 Asymptotic analysis of MAP estimation via the replica method and applications to compressed sensing. [sent-226, score-0.882]
</p><p>100 Statistical physics of spin glasses and information processing: An introduction. [sent-242, score-0.105]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('replica', 0.746), ('mmse', 0.306), ('xmap', 0.181), ('scalar', 0.159), ('postulated', 0.153), ('ppost', 0.143), ('verd', 0.143), ('map', 0.141), ('claim', 0.127), ('estimators', 0.103), ('lasso', 0.1), ('estimator', 0.098), ('mse', 0.093), ('xj', 0.09), ('noise', 0.09), ('guo', 0.089), ('compressed', 0.086), ('asymptotic', 0.081), ('tthard', 0.079), ('ps', 0.074), ('pu', 0.068), ('px', 0.067), ('lim', 0.063), ('post', 0.052), ('spin', 0.051), ('sj', 0.05), ('pursuit', 0.05), ('estimation', 0.05), ('sensing', 0.049), ('cdma', 0.048), ('normregularized', 0.048), ('xmmse', 0.048), ('xmpmse', 0.048), ('measurements', 0.046), ('levels', 0.046), ('recovery', 0.045), ('equations', 0.043), ('soft', 0.04), ('measurement', 0.04), ('effective', 0.038), ('thresholding', 0.033), ('estimate', 0.033), ('median', 0.032), ('mpmse', 0.032), ('limits', 0.031), ('physics', 0.031), ('xu', 0.029), ('behavior', 0.029), ('sparse', 0.029), ('components', 0.029), ('component', 0.029), ('alyson', 0.028), ('rangan', 0.028), ('uf', 0.028), ('xpx', 0.028), ('dx', 0.027), ('expectations', 0.026), ('fletcher', 0.025), ('tanaka', 0.025), ('corrupted', 0.024), ('donoho', 0.024), ('edwards', 0.024), ('glasses', 0.023), ('asserts', 0.023), ('satisfy', 0.022), ('zero', 0.022), ('pr', 0.022), ('level', 0.021), ('february', 0.021), ('laws', 0.021), ('snr', 0.021), ('operator', 0.02), ('gaussian', 0.02), ('prior', 0.019), ('arg', 0.019), ('rn', 0.019), ('minimizer', 0.019), ('index', 0.019), ('basis', 0.018), ('variations', 0.018), ('numerical', 0.017), ('cand', 0.017), ('vector', 0.017), ('cost', 0.017), ('january', 0.016), ('se', 0.016), ('sparsity', 0.016), ('november', 0.016), ('assumptions', 0.016), ('ieee', 0.016), ('posteriori', 0.016), ('expressions', 0.016), ('theory', 0.015), ('certain', 0.015), ('priors', 0.015), ('deterministic', 0.015), ('rm', 0.014), ('arxiv', 0.014), ('april', 0.014), ('nonlinear', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="36-tfidf-1" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>Author: Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</p><p>Abstract: The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.</p><p>2 0.091283783 <a title="36-tfidf-2" href="./nips-2009-Orthogonal_Matching_Pursuit_From_Noisy_Random_Measurements%3A_A_New_Analysis.html">185 nips-2009-Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></p>
<p>Author: Sundeep Rangan, Alyson K. Fletcher</p><p>Abstract: A well-known analysis of Tropp and Gilbert shows that orthogonal matching pursuit (OMP) can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free linear measurements obtained through a random Gaussian measurement matrix with a probability that approaches one as n → ∞. This work strengthens this result by showing that a lower number of measurements, m = 2k log(n − k), is in fact sufﬁcient for asymptotic recovery. More generally, when the sparsity level satisﬁes kmin ≤ k ≤ kmax but is unknown, m = 2kmax log(n − kmin ) measurements is sufﬁcient. Furthermore, this number of measurements is also sufﬁcient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (SNR) scales to inﬁnity. The scaling m = 2k log(n − k) exactly matches the number of measurements required by the more complex lasso method for signal recovery in a similar SNR scaling.</p><p>3 0.079293855 <a title="36-tfidf-3" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>Author: Shuheng Zhou</p><p>Abstract: Given n noisy samples with p dimensions, where n ≪ p, we show that the multistep thresholding procedure can accurately estimate a sparse vector β ∈ Rp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very signiﬁcant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufﬁciently sparse, the Gauss-Dantzig selector (Cand` se Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufﬁciently sparse model. 1</p><p>4 0.075395942 <a title="36-tfidf-4" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>Author: Arthur Choi, Adnan Darwiche</p><p>Abstract: We introduce a new perspective on approximations to the maximum a posteriori (MAP) task in probabilistic graphical models, that is based on simplifying a given instance, and then tightening the approximation. First, we start with a structural relaxation of the original model. We then infer from the relaxation its deﬁciencies, and compensate for them. This perspective allows us to identify two distinct classes of approximations. First, we ﬁnd that max-product belief propagation can be viewed as a way to compensate for a relaxation, based on a particular idealized case for exactness. We identify a second approach to compensation that is based on a more reﬁned idealized case, resulting in a new approximation with distinct properties. We go on to propose a new class of algorithms that, starting with a relaxation, iteratively seeks tighter approximations. 1</p><p>5 0.07482259 <a title="36-tfidf-5" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>Author: Jacob Goldberger, Amir Leshem</p><p>Abstract: This paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a ﬁnite set. The factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph. Hence, applying the Belief Propagation (BP) algorithm yields very poor results. The algorithm described here is based on an optimal tree approximation of the Gaussian density of the unconstrained linear system. It is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the BP algorithm to the modiﬁed factor graph outperforms current methods in terms of both performance and complexity. The improved performance of the proposed algorithm is demonstrated on the problem of MIMO detection.</p><p>6 0.068357922 <a title="36-tfidf-6" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>7 0.067538433 <a title="36-tfidf-7" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>8 0.065495238 <a title="36-tfidf-8" href="./nips-2009-Compressed_Least-Squares_Regression.html">55 nips-2009-Compressed Least-Squares Regression</a></p>
<p>9 0.064461954 <a title="36-tfidf-9" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>10 0.063565955 <a title="36-tfidf-10" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>11 0.062332876 <a title="36-tfidf-11" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>12 0.057331409 <a title="36-tfidf-12" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>13 0.056640293 <a title="36-tfidf-13" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>14 0.051071659 <a title="36-tfidf-14" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>15 0.050081156 <a title="36-tfidf-15" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>16 0.047024999 <a title="36-tfidf-16" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>17 0.045918513 <a title="36-tfidf-17" href="./nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></p>
<p>18 0.04471989 <a title="36-tfidf-18" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>19 0.044457078 <a title="36-tfidf-19" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<p>20 0.044170015 <a title="36-tfidf-20" href="./nips-2009-Robust_Principal_Component_Analysis%3A_Exact_Recovery_of_Corrupted_Low-Rank_Matrices_via_Convex_Optimization.html">208 nips-2009-Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.116), (1, 0.052), (2, 0.023), (3, 0.054), (4, -0.008), (5, -0.055), (6, 0.071), (7, -0.101), (8, 0.086), (9, -0.014), (10, -0.005), (11, 0.019), (12, -0.002), (13, 0.024), (14, 0.036), (15, 0.051), (16, -0.047), (17, -0.001), (18, -0.015), (19, 0.014), (20, -0.03), (21, 0.017), (22, 0.036), (23, 0.068), (24, 0.022), (25, 0.059), (26, 0.027), (27, -0.042), (28, -0.051), (29, 0.01), (30, -0.149), (31, 0.141), (32, -0.016), (33, 0.082), (34, 0.033), (35, 0.035), (36, 0.086), (37, 0.027), (38, 0.032), (39, 0.005), (40, 0.051), (41, 0.054), (42, 0.023), (43, -0.091), (44, -0.045), (45, -0.005), (46, 0.05), (47, 0.019), (48, -0.051), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93398494 <a title="36-lsi-1" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>Author: Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</p><p>Abstract: The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.</p><p>2 0.72289693 <a title="36-lsi-2" href="./nips-2009-Orthogonal_Matching_Pursuit_From_Noisy_Random_Measurements%3A_A_New_Analysis.html">185 nips-2009-Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></p>
<p>Author: Sundeep Rangan, Alyson K. Fletcher</p><p>Abstract: A well-known analysis of Tropp and Gilbert shows that orthogonal matching pursuit (OMP) can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free linear measurements obtained through a random Gaussian measurement matrix with a probability that approaches one as n → ∞. This work strengthens this result by showing that a lower number of measurements, m = 2k log(n − k), is in fact sufﬁcient for asymptotic recovery. More generally, when the sparsity level satisﬁes kmin ≤ k ≤ kmax but is unknown, m = 2kmax log(n − kmin ) measurements is sufﬁcient. Furthermore, this number of measurements is also sufﬁcient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (SNR) scales to inﬁnity. The scaling m = 2k log(n − k) exactly matches the number of measurements required by the more complex lasso method for signal recovery in a similar SNR scaling.</p><p>3 0.59153843 <a title="36-lsi-3" href="./nips-2009-Grouped_Orthogonal_Matching_Pursuit_for_Variable_Selection_and_Prediction.html">105 nips-2009-Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</a></p>
<p>Author: Grzegorz Swirszcz, Naoki Abe, Aurelie C. Lozano</p><p>Abstract: We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efﬁciently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as “forward greedy feature selection algorithm” for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the l∞ norm of the difference between the estimated regression coefﬁcients and the true coefﬁcients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy. 1</p><p>4 0.59020776 <a title="36-lsi-4" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-ofthe-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justiﬁcations of speciﬁc versions of the additive forward regression. 1</p><p>5 0.58895665 <a title="36-lsi-5" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>Author: Shuheng Zhou</p><p>Abstract: Given n noisy samples with p dimensions, where n ≪ p, we show that the multistep thresholding procedure can accurately estimate a sparse vector β ∈ Rp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very signiﬁcant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufﬁciently sparse, the Gauss-Dantzig selector (Cand` se Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufﬁciently sparse model. 1</p><p>6 0.52375144 <a title="36-lsi-6" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>7 0.51568425 <a title="36-lsi-7" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>8 0.49772933 <a title="36-lsi-8" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>9 0.49336553 <a title="36-lsi-9" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<p>10 0.49235579 <a title="36-lsi-10" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>11 0.48268241 <a title="36-lsi-11" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>12 0.4639639 <a title="36-lsi-12" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>13 0.46396026 <a title="36-lsi-13" href="./nips-2009-A_Data-Driven_Approach_to_Modeling_Choice.html">7 nips-2009-A Data-Driven Approach to Modeling Choice</a></p>
<p>14 0.42832002 <a title="36-lsi-14" href="./nips-2009-Compressed_Least-Squares_Regression.html">55 nips-2009-Compressed Least-Squares Regression</a></p>
<p>15 0.42292932 <a title="36-lsi-15" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>16 0.40762734 <a title="36-lsi-16" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>17 0.3982079 <a title="36-lsi-17" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>18 0.38637117 <a title="36-lsi-18" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>19 0.38035759 <a title="36-lsi-19" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>20 0.36967632 <a title="36-lsi-20" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.041), (25, 0.082), (35, 0.031), (36, 0.101), (39, 0.027), (54, 0.281), (58, 0.118), (61, 0.04), (71, 0.054), (77, 0.032), (81, 0.02), (86, 0.049), (91, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75738078 <a title="36-lda-1" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>Author: Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</p><p>Abstract: The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.</p><p>2 0.75406706 <a title="36-lda-2" href="./nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</a></p>
<p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><p>3 0.74168813 <a title="36-lda-3" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>Author: Marco Grzegorczyk, Dirk Husmeier</p><p>Abstract: Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many realworld scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-ﬂexible models that lack any information sharing among time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary among segments, and in which a common network structure provides essential information sharing across segments. Our model is based on a Bayesian multiple change-point process, where the number and location of the change-points is sampled from the posterior distribution. 1</p><p>4 0.67357087 <a title="36-lda-4" href="./nips-2009-Beyond_Convexity%3A_Online_Submodular_Minimization.html">45 nips-2009-Beyond Convexity: Online Submodular Minimization</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efﬁcient and are Hannan-consistent in both the full information and bandit settings. 1</p><p>5 0.56234622 <a title="36-lda-5" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>Author: Jaakko Luttinen, Alexander T. Ihler</p><p>Abstract: We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</p><p>6 0.55970091 <a title="36-lda-6" href="./nips-2009-Accelerated_Gradient_Methods_for_Stochastic_Optimization_and_Online_Learning.html">22 nips-2009-Accelerated Gradient Methods for Stochastic Optimization and Online Learning</a></p>
<p>7 0.55861849 <a title="36-lda-7" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>8 0.55790091 <a title="36-lda-8" href="./nips-2009-Orthogonal_Matching_Pursuit_From_Noisy_Random_Measurements%3A_A_New_Analysis.html">185 nips-2009-Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></p>
<p>9 0.55633479 <a title="36-lda-9" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>10 0.55488062 <a title="36-lda-10" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>11 0.55428743 <a title="36-lda-11" href="./nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</a></p>
<p>12 0.55349082 <a title="36-lda-12" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>13 0.55256492 <a title="36-lda-13" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>14 0.55163062 <a title="36-lda-14" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>15 0.55002338 <a title="36-lda-15" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>16 0.54851735 <a title="36-lda-16" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>17 0.5484786 <a title="36-lda-17" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>18 0.54826736 <a title="36-lda-18" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>19 0.54798973 <a title="36-lda-19" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>20 0.54791534 <a title="36-lda-20" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
