<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-17" href="#">nips2009-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</h1>
<br/><p>Source: <a title="nips-2009-17-pdf" href="http://papers.nips.cc/paper/3668-a-sparse-non-parametric-approach-for-single-channel-separation-of-known-sounds.pdf">pdf</a></p><p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>Reference: <a title="nips-2009-17-reference" href="../nips2009_reference/nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we present an algorithm for separating mixed sounds from a monophonic recording. [sent-8, score-0.166]
</p><p>2 Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. [sent-9, score-0.164]
</p><p>3 In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. [sent-10, score-0.454]
</p><p>4 We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. [sent-11, score-0.394]
</p><p>5 1  Introduction  This paper deals with the problem of single-channel signal separation – separating out signals from individual sources in a mixed recording. [sent-13, score-0.451]
</p><p>6 As of recently, a popular statistical approach has been to obtain compact characterizations of individual sources and employ them to identify and extract their counterpart components from mixture signals. [sent-14, score-0.404]
</p><p>7 Statistical characterizations may include codebooks [1], Gaussian mixture densities [2], HMMs [3], independent components [4, 5], sparse dictionaries [6], non-negative decompositions [7–9] and latent variable models [10, 11]. [sent-15, score-0.372]
</p><p>8 Separation is achieved by abstracting components from the mixed signal that conform to the statistical characterizations of the individual sources. [sent-17, score-0.163]
</p><p>9 The key here is the speciﬁc statistical model employed – the more eﬀectively it captures the speciﬁc characteristics of the signal sources, the better the separation that may be achieved. [sent-18, score-0.235]
</p><p>10 This has been the basis of several example-based characterizations of a data source, such as nearest-neighbor, K-nearest neighbor, Parzen-window based models of source distributions etc. [sent-20, score-0.536]
</p><p>11 Here, we use the same idea to develop a monaural source-separation algorithm that directly uses samples from the training data to represent the sources in a mixture. [sent-21, score-0.233]
</p><p>12 Identifying the proper samples from the training data that best approximate a sample of the mixture is of course a hard combinatorial problem, which can be computationally demanding. [sent-23, score-0.27]
</p><p>13 We additionally show that this approach results in 1  source estimates which are guaranteed to lie on the source manifold, as opposed to trainedbasis approaches which can produce arbitrary outputs that will not necessarily be plausible source estimates. [sent-25, score-1.541]
</p><p>14 Experimental evaluations show that this approach results in separated signals that exhibit signiﬁcantly higher performance metrics as compared to conceptually similar techniques which are based on various types of combinations of generalizable bases representing the sources. [sent-26, score-0.401]
</p><p>15 1  The Basic Model  Given a magnitude spectrogram of a single source, each spectral frame is modeled as a histogram of repeated draws from a multinomial distribution over the frequency bins. [sent-29, score-0.344]
</p><p>16 At a given time frame t, consider a random process characterized by the probability Pt (f ) of drawing frequency f in a given draw. [sent-30, score-0.15]
</p><p>17 The model assumes that Pt (f ) is comprised of bases indexed by a latent variable z. [sent-32, score-0.266]
</p><p>18 We use this model to learn the source-speciﬁc bases given by Pt (f |z) as done in [10, 11]. [sent-35, score-0.226]
</p><p>19 Now let the matrix VF ×T of entries vf t represent the magnitude spectrogram of the mixture sound and vt represent time frame t (the t-th column vector of matrix V). [sent-37, score-0.665]
</p><p>20 Each mixture spectral frame is again modeled as a histogram of repeated draws, from the multinomial distributions corresponding to every source. [sent-38, score-0.337]
</p><p>21 We can assume that for each source in the mixture we have an already trained model in the form of basis vectors Ps (f |z). [sent-40, score-0.657]
</p><p>22 These bases will represent a dictionary of spectra that best describe each source. [sent-41, score-0.334]
</p><p>23 Armed with this knowledge we can decompose a new mixture of these known sources in terms of the contributions of the dictionaries for each source. [sent-42, score-0.368]
</p><p>24 The triangles denote the position of basis functions for two source classes. [sent-44, score-0.476]
</p><p>25 The square is an instance of a mixture of the two sources. [sent-45, score-0.181]
</p><p>26 The mixture point is not within the convex hull which covers either source, but it is within the convex hull deﬁned by all the bases combined. [sent-46, score-0.783]
</p><p>27 These reconstructions will approximate the magnitude spectrogram of each source in the mixture. [sent-47, score-0.634]
</p><p>28 Once we obtain these reconstructions we can use them to modulate the original phase spectrogram of the mixture and obtain the time-series representation of the sources. [sent-48, score-0.381]
</p><p>29 Each basis vector and the mixture input will lie in a F − 1 dimensional simplex (due to the fact that these quantities are normalized to sum to unity). [sent-50, score-0.302]
</p><p>30 Each source’s basis set will deﬁne a convex hull within which any point can be approximated using these bases. [sent-51, score-0.249]
</p><p>31 Assuming that the training data is accurate, all potential inputs from that source should lie in that area. [sent-52, score-0.602]
</p><p>32 The union of all the source bases will deﬁne a larger space in which a mixture input will be inside. [sent-53, score-0.841]
</p><p>33 Any mixture point can then be approximated as a weighted sum of multiple bases from both sources. [sent-54, score-0.426]
</p><p>34 2  Using Training Data Directly as a Dictionary  In this paper, we would like to explain the mixture frame from the training spectral frames instead of using a smaller set of learned bases. [sent-57, score-0.499]
</p><p>35 The secondary rationale behind this operation is based on the observation that the points deﬁned by the convex hull of a source’s model, do not necessarily all fall on that source’s manifold. [sent-61, score-0.188]
</p><p>36 In both of these plots the sources exhibit a clear structure. [sent-63, score-0.195]
</p><p>37 In the left plot both sources appear in a circular pattern, and in the right plot in a spiral form. [sent-64, score-0.247]
</p><p>38 As shown in [12], learning a set of bases that explains these sources results in deﬁning a convex hull that surrounds the training data. [sent-65, score-0.647]
</p><p>39 Under this model potential source estimates can now lie anywhere inside these hulls. [sent-66, score-0.541]
</p><p>40 Using trainedbasis models, if we decompose the mixture points in these ﬁgures we obtain two source estimates which do not lie in the same manifold as the original sources. [sent-67, score-0.804]
</p><p>41 Although the input was adequately approximated, there is no guarantee that the extracted sources are indeed appropriate outcomes for their sound class. [sent-68, score-0.212]
</p><p>42 In order to address this problem and to also provide a richer dictionary for the source reconstructions, we will make direct use of the training data in order to explain the mixture, and bypass the basis representation as an abstraction. [sent-69, score-0.673]
</p><p>43 To do so we will use each frame of the (s) spectrograms of the training sequences as the bases Ps (f |z). [sent-70, score-0.494]
</p><p>44 More speciﬁcally, let WF ×T (s) (s)  be the training spectrogram from source s and let wt represent the time frame t from the spectrogram. [sent-71, score-0.79]
</p><p>45 In this case, the latent variable z for source s takes T (s) values, and the z-th basis function will be given by the (normalized) z-th column vector of W(s) . [sent-72, score-0.516]
</p><p>46 In both plots the training data for each source are denoted by △ and ▽, and the mixture sample by . [sent-74, score-0.734]
</p><p>47 The learned bases of each source are the vertices of the two dashed convex hulls that enclose each class. [sent-75, score-0.699]
</p><p>48 The source estimates and the approximation of the mixture are denoted by ×, + and . [sent-76, score-0.664]
</p><p>49 In the left case the two sources lie on two overlapping circular areas, the source estimates however lie outside these areas. [sent-77, score-0.787]
</p><p>50 The recovered sources lie very closely on the competing source’s area, thereby providing a highly inappropriate decomposition. [sent-79, score-0.223]
</p><p>51 Although the mixture was well approximated in both cases, the estimated sources were poor representations of their classes. [sent-80, score-0.344]
</p><p>52 With the above model we would ideally want to use one dictionary element per source at any point in time. [sent-81, score-0.542]
</p><p>53 Doing so will ensure that the outputs would lie on the source manifold, and also oﬀset any issues of potential overcompleteness. [sent-82, score-0.513]
</p><p>54 One way to ensure this is to perform a reconstruction such that we only use one element of each source at any time, much akin to a nearest-neighbor model, albeit in an additive setting. [sent-83, score-0.472]
</p><p>55 The intuition is that at any given point in time, the mixture frame is explained by very few active elements from the training data. [sent-85, score-0.42]
</p><p>56 In other words, we need the mixture weight distributions and the speaker priors to be sparse at every time instant. [sent-86, score-0.364]
</p><p>57 We would like to minimize the entropies of both the speaker dependent mixture weight distributions (given by Pt (z|s)) and the source priors (given by Pt (s)) at every frame. [sent-91, score-0.77]
</p><p>58 Thus, reducing the entropy of the joint distribution Pt (z, s) is equivalent to reducing the conditional entropy of the source dependent mixture weights and the entropy of the source priors. [sent-94, score-1.186]
</p><p>59 Since the dictionary is already known and is given by the normalized spectral frames from source training spectrograms, the parameter to be estimated is given by Pt (z, s). [sent-95, score-0.733]
</p><p>60 We impose an entropic prior distribution on Pt (z, s) 4  Source A Source B Mixture Estimate for A Estimate for B Approximation of mixture  Source A Source B Mixture Estimate for A Estimate for B Approximation of mixture  Figure 3: Using a sparse reconstruction on the data in ﬁgure 2. [sent-100, score-0.528]
</p><p>61 Note how in contrast to that ﬁgure the source estimates are now identiﬁed as training data points, and are thus plausible solutions. [sent-101, score-0.601]
</p><p>62 The approximation of the mixture is the nearest point of the line connecting the two source estimates, to the actual mixture input. [sent-102, score-0.837]
</p><p>63 Note that the proper solution is the one that results in such a line that is as close as possible to the mixture point, and not one that is deﬁned by two training points close to the mixture. [sent-103, score-0.27]
</p><p>64 Once Pt (z, s) is estimated, the reconstruction of source s can be computed as z∈{zs }  (s)  vf t = ˆ  s  Ps (f |z)Pt (z, s)  z∈{zs }  Ps (f |z)Pt (z, s)  vf t  Now let us consider how this problem resolves the issues presented in ﬁgure 2. [sent-110, score-0.805]
</p><p>65 In both plots we see that the source reconstructions lie on a training point, thereby being a plausible source estimate. [sent-114, score-1.176]
</p><p>66 The approximation of the mixture is not as exact as before, since now it has to lie on the line connecting the two active source elements. [sent-115, score-0.738]
</p><p>67 This is not however an issue of concern since in practice the approximation is always good enough, and the guarantee of a plausible source estimate is more valuable than the exact approximation of the mixture. [sent-116, score-0.568]
</p><p>68 In these approaches the priors are imposed on the mixture weights and thus are not as eﬀective for this particular task since they still suﬀer from the symptoms of learned-basis models. [sent-118, score-0.207]
</p><p>69 The top plots show the input waveforms, and the bottom plots shows the estimated weights multiplied with the source priors. [sent-121, score-0.494]
</p><p>70 The sources were sampled as 16 kHz, we used 64 ms windows for the spectrogram computation, and an overlap of 32 ms. [sent-125, score-0.284]
</p><p>71 The training data was around 25 sec worth of speech for each speaker, and the testing mixture was about 3 sec long. [sent-127, score-0.445]
</p><p>72 We evaluated the separation performance using the metrics provided in [17]. [sent-128, score-0.225]
</p><p>73 The ﬁrst is a measure of how well we suppress the interfering speaker, whereas the other two provide us with a sense of how much the extracted source is corrupted due to the separation process. [sent-130, score-0.597]
</p><p>74 The ﬁrst experiment we perform is on a mixture for which the training data includes its isolated constituent sentences. [sent-137, score-0.27]
</p><p>75 The primary observation from this experiment is that the more bases we use the better the results get. [sent-146, score-0.226]
</p><p>76 01 β  0  5  10  20  40  80  Train 320 160  Bases  Figure 5: Average separation performance metrics for oracle cases, as dependent on the choice of diﬀerent number of elements in the speaker’s dictionary, and diﬀerent choices of the entropic prior parameter β. [sent-166, score-0.388]
</p><p>77 The basis row labeled as “Train” is the case where we use all the training data as a basis set. [sent-168, score-0.173]
</p><p>78 01 β  0  5  10  20  40  80  Train 320 160  Bases  Figure 6: Average separation performance metrics for real-world cases, as dependent on the choice of diﬀerent number of elements in the speaker’s dictionary, and diﬀerent choices of the entropic prior parameter β. [sent-187, score-0.325]
</p><p>79 2  Results on Realistic Situations  Let us now consider the more realistic case where the mixture data is diﬀerent from the training set. [sent-191, score-0.292]
</p><p>80 The input mixture has to be reconstructed using approximate samples. [sent-193, score-0.181]
</p><p>81 We do not obtain such high numbers in performance as in the oracle case, but we also see a stronger trend in favor of sparsity and the use of all the training data as a dictionary. [sent-195, score-0.228]
</p><p>82 We can clearly see that in all metrics using all the training data signiﬁcantly outperforms trained-basis models. [sent-197, score-0.151]
</p><p>83 The use of sparsity ensures that the output is a plausible speech signal devoid of artifacts like distortion and musical noise. [sent-204, score-0.368]
</p><p>84 dB  15 10 5 SDR  0  0%  SIR  20%  SAR  40% 60% 70% 80% Percentage of discarded training frames  90%  95%  Figure 8: Eﬀect of discarding low energy training frames. [sent-214, score-0.323]
</p><p>85 The horizontal axis denotes the percentage of training frames that have been discarded. [sent-215, score-0.162]
</p><p>86 In order to address this concern we show that the size of the training data can be easily pruned down to a size comparable to trainedbasis models and still outperform them. [sent-219, score-0.17]
</p><p>87 Since sound signals, especially speech, tend to have a considerable amount of short-term pauses and regions of silence, we can use an energy threshold to in order to select the loudest frames of the training spectrogram as bases. [sent-220, score-0.406]
</p><p>88 In ﬁgure 8 we show how the separation performance metrics are inﬂuenced as we increasingly remove bases which lie under various energy percentiles. [sent-221, score-0.566]
</p><p>89 It is clear that even after discarding up to at least 70% of the lowest energy training frames the performance is still approximately the same. [sent-222, score-0.234]
</p><p>90 Overall computations for a single mixture took roughly 4 sec when not using the sparsity prior, 14 sec when using the sparsity prior (primarily due to slow computation of Lambert’s function), and dropped down to 5 sec when using the 30% highest energy frames from the training data. [sent-227, score-0.684]
</p><p>91 4  Conclusion  In this paper we present a new approach to solving the monophonic source separation problem. [sent-228, score-0.637]
</p><p>92 In order to do so we present a sparse learning algorithm which can eﬃciently solve this problem, and also guarantees that the returned source estimates are plausible given the training data. [sent-230, score-0.649]
</p><p>93 We provide experiments that show how this approach is inﬂuenced by the use of varying sparsity constraints and training data selection. [sent-231, score-0.165]
</p><p>94 Roweis, One microphone source separation, in Advances in Neural Information Processing Systems, 2001. [sent-235, score-0.434]
</p><p>95 Gopinath, Super-human multitalker speech recognition: The IBM 2006 speech separation challenge system, in International Conference on Spoken Language Processing (INTERSPEECH), 2006, pp. [sent-246, score-0.309]
</p><p>96 Separation of mixed audio sources by independent subspace analysis, in Proceedings of the International Conference of Computer Music, 2000. [sent-254, score-0.207]
</p><p>97 Gribonval, Non negative sparse representation for wiener based source separation with a single sensor, in Acoustics, Speech, and Signal Processing, IEEE International Conference on, 2003, pp. [sent-272, score-0.645]
</p><p>98 Olsson, Single-channel speech separation using sparse nonnegative matrix factorization, in International Conference on Spoken Language Processing (INTERSPEECH), 2006. [sent-278, score-0.284]
</p><p>99 Virtanen, Sound source separation using sparse coding with temporal continuity objective, in International Computer Music Conference, ICMC, 2003. [sent-280, score-0.645]
</p><p>100 Using unsupervised learning of a ﬁnite Dirichlet mixture model to improve pattern recognition applications, Pattern Recognition Letters, Volume 26, Issue 12, September 2005. [sent-324, score-0.181]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pt', 0.533), ('source', 0.434), ('bases', 0.226), ('mixture', 0.181), ('separation', 0.163), ('ps', 0.152), ('hull', 0.149), ('vf', 0.149), ('zs', 0.145), ('sources', 0.144), ('spectrogram', 0.14), ('frame', 0.127), ('speaker', 0.109), ('dictionary', 0.108), ('sar', 0.105), ('sir', 0.09), ('training', 0.089), ('erent', 0.087), ('entropic', 0.08), ('sdr', 0.08), ('shashanka', 0.08), ('smaragdis', 0.08), ('lie', 0.079), ('sparsity', 0.076), ('sounds', 0.075), ('frames', 0.073), ('speech', 0.073), ('signal', 0.072), ('sound', 0.068), ('oracle', 0.063), ('metrics', 0.062), ('reconstructions', 0.06), ('characterizations', 0.06), ('raj', 0.06), ('trainedbasis', 0.06), ('gure', 0.057), ('di', 0.055), ('lambert', 0.052), ('spectrograms', 0.052), ('sec', 0.051), ('artifacts', 0.05), ('plausible', 0.05), ('sparse', 0.048), ('distortion', 0.047), ('generalizable', 0.045), ('ratio', 0.045), ('dictionaries', 0.043), ('interference', 0.043), ('basis', 0.042), ('bhiksha', 0.04), ('brand', 0.04), ('gribonval', 0.04), ('monophonic', 0.04), ('plot', 0.04), ('latent', 0.04), ('entropy', 0.039), ('convex', 0.039), ('db', 0.039), ('train', 0.038), ('reconstruction', 0.038), ('discarding', 0.036), ('energy', 0.036), ('resolves', 0.035), ('audio', 0.032), ('mixed', 0.031), ('plots', 0.03), ('spoken', 0.03), ('spectral', 0.029), ('interspeech', 0.028), ('estimates', 0.028), ('speakers', 0.027), ('conceptually', 0.026), ('priors', 0.026), ('draws', 0.025), ('frequency', 0.023), ('acoustics', 0.023), ('active', 0.023), ('su', 0.023), ('circular', 0.023), ('opposed', 0.022), ('paris', 0.022), ('boost', 0.022), ('manifold', 0.022), ('realistic', 0.022), ('music', 0.022), ('signals', 0.021), ('approximation', 0.021), ('exhibit', 0.021), ('concern', 0.021), ('estimate', 0.021), ('separating', 0.02), ('actual', 0.02), ('channel', 0.02), ('ect', 0.02), ('uenced', 0.02), ('dependent', 0.02), ('tests', 0.019), ('observing', 0.019), ('approximated', 0.019), ('compact', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="17-tfidf-1" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>2 0.18266319 <a title="17-tfidf-2" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>3 0.16911611 <a title="17-tfidf-3" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>Author: Mikkel Schmidt</p><p>Abstract: We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efﬁcient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of nonnegative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</p><p>4 0.13494855 <a title="17-tfidf-4" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>Author: Honglak Lee, Peter Pham, Yan Largman, Andrew Y. Ng</p><p>Abstract: In recent years, deep learning approaches have gained signiﬁcant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classiﬁcation tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classiﬁcation tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks. 1</p><p>5 0.12996544 <a title="17-tfidf-5" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>6 0.12573583 <a title="17-tfidf-6" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>7 0.097353257 <a title="17-tfidf-7" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>8 0.095389761 <a title="17-tfidf-8" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>9 0.089184508 <a title="17-tfidf-9" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>10 0.08646895 <a title="17-tfidf-10" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>11 0.076050408 <a title="17-tfidf-11" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>12 0.074777782 <a title="17-tfidf-12" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>13 0.073812455 <a title="17-tfidf-13" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>14 0.071992107 <a title="17-tfidf-14" href="./nips-2009-Manifold_Regularization_for_SIR_with_Rate_Root-n_Convergence.html">146 nips-2009-Manifold Regularization for SIR with Rate Root-n Convergence</a></p>
<p>15 0.065755561 <a title="17-tfidf-15" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>16 0.057068553 <a title="17-tfidf-16" href="./nips-2009-Adapting_to_the_Shifting_Intent_of_Search_Queries.html">24 nips-2009-Adapting to the Shifting Intent of Search Queries</a></p>
<p>17 0.056610744 <a title="17-tfidf-17" href="./nips-2009-Information-theoretic_lower_bounds_on_the_oracle_complexity_of_convex_optimization.html">116 nips-2009-Information-theoretic lower bounds on the oracle complexity of convex optimization</a></p>
<p>18 0.056335777 <a title="17-tfidf-18" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>19 0.055841062 <a title="17-tfidf-19" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>20 0.055269491 <a title="17-tfidf-20" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.172), (1, -0.037), (2, 0.009), (3, 0.031), (4, 0.008), (5, -0.026), (6, 0.074), (7, -0.05), (8, 0.055), (9, 0.108), (10, -0.034), (11, 0.057), (12, -0.139), (13, 0.113), (14, -0.041), (15, -0.006), (16, 0.047), (17, 0.146), (18, -0.124), (19, -0.108), (20, -0.039), (21, -0.05), (22, -0.131), (23, 0.074), (24, -0.173), (25, -0.165), (26, 0.213), (27, 0.05), (28, 0.076), (29, -0.032), (30, 0.051), (31, -0.123), (32, 0.044), (33, -0.226), (34, -0.01), (35, -0.154), (36, 0.072), (37, 0.108), (38, 0.107), (39, -0.02), (40, -0.049), (41, -0.055), (42, 0.111), (43, 0.061), (44, 0.022), (45, -0.068), (46, 0.002), (47, 0.014), (48, -0.078), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96719766 <a title="17-lsi-1" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>2 0.71016073 <a title="17-lsi-2" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>Author: Mikkel Schmidt</p><p>Abstract: We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efﬁcient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of nonnegative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</p><p>3 0.61100781 <a title="17-lsi-3" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>4 0.45505655 <a title="17-lsi-4" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>Author: Honglak Lee, Peter Pham, Yan Largman, Andrew Y. Ng</p><p>Abstract: In recent years, deep learning approaches have gained signiﬁcant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classiﬁcation tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classiﬁcation tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks. 1</p><p>5 0.42493784 <a title="17-lsi-5" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>Author: Zahi Karam, Douglas Sturim, William M. Campbell</p><p>Abstract: Speaker comparison, the process of ﬁnding the speaker similarity between two speech signals, occupies a central role in a variety of applications—speaker veriﬁcation, clustering, and identiﬁcation. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process. For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM). Speaker comparison can then be viewed as the process of compensating and ﬁnding metrics on the space of adapted models. We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison—support vector machines, joint factor analysis, and linear scoring. The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods. Compensation of nuisances is performed via linear transforms on GMM parameter vectors. Using the IPDF framework, we show that many current techniques are simple variations of each other. We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require signiﬁcantly less computation than current techniques.</p><p>6 0.36764807 <a title="17-lsi-6" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>7 0.34773347 <a title="17-lsi-7" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>8 0.34615946 <a title="17-lsi-8" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>9 0.33736432 <a title="17-lsi-9" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<p>10 0.31620958 <a title="17-lsi-10" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<p>11 0.31347367 <a title="17-lsi-11" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>12 0.31094036 <a title="17-lsi-12" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>13 0.30807751 <a title="17-lsi-13" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>14 0.30471906 <a title="17-lsi-14" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>15 0.30328858 <a title="17-lsi-15" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>16 0.30082417 <a title="17-lsi-16" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>17 0.29950958 <a title="17-lsi-17" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>18 0.29371169 <a title="17-lsi-18" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>19 0.2903538 <a title="17-lsi-19" href="./nips-2009-Manifold_Regularization_for_SIR_with_Rate_Root-n_Convergence.html">146 nips-2009-Manifold Regularization for SIR with Rate Root-n Convergence</a></p>
<p>20 0.28485742 <a title="17-lsi-20" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.063), (7, 0.155), (24, 0.045), (25, 0.062), (31, 0.017), (35, 0.04), (36, 0.097), (39, 0.057), (58, 0.079), (61, 0.018), (71, 0.051), (81, 0.071), (86, 0.148)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9260124 <a title="17-lda-1" href="./nips-2009-Fast_subtree_kernels_on_graphs.html">95 nips-2009-Fast subtree kernels on graphs</a></p>
<p>Author: Nino Shervashidze, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon & G¨ rtner scales as O(n2 4d h). Key to this efﬁciency is the observation that the a Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classiﬁcation benchmark datasets in terms of accuracy and runtime. 1</p><p>2 0.90688103 <a title="17-lda-2" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>Author: Zhi Yang, Qi Zhao, Edward Keefer, Wentai Liu</p><p>Abstract: Studying signal and noise properties of recorded neural data is critical in developing more efﬁcient algorithms to recover the encoded information. Important issues exist in this research including the variant spectrum spans of neural spikes that make it difﬁcult to choose a globally optimal bandpass ﬁlter. Also, multiple sources produce aggregated noise that deviates from the conventional white Gaussian noise. In this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass ﬁlter that ﬁts the spectrum of individual spikes is proposed. Multiple noise sources have been studied through analytical models as well as empirical measurements. The dominant noise source is identiﬁed as neuron noise followed by interface noise of the electrode. This suggests that major efforts to reduce noise from electronics are not well spent. The measured noise from in vivo experiments shows a family of 1/f x spectrum that can be reduced using noise shaping techniques. In summary, the methods of adaptive bandpass ﬁltering and noise shaping together result in several dB signal-to-noise ratio (SNR) enhancement.</p><p>same-paper 3 0.89090562 <a title="17-lda-3" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>4 0.88406527 <a title="17-lda-4" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>5 0.87726426 <a title="17-lda-5" href="./nips-2009-Efficient_and_Accurate_Lp-Norm_Multiple_Kernel_Learning.html">80 nips-2009-Efficient and Accurate Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Ulf Brefeld, Pavel Laskov, Klaus-Robert Müller, Alexander Zien, Sören Sonnenburg</p><p>Abstract: Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations to support interpretability. Unfortunately, 1 -norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures, we generalize MKL to arbitrary p -norms. We devise new insights on the connection between several existing MKL formulations and develop two efﬁcient interleaved optimization strategies for arbitrary p > 1. Empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally, we apply p -norm MKL to real-world problems from computational biology, showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art. 1</p><p>6 0.86806428 <a title="17-lda-6" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>7 0.79868138 <a title="17-lda-7" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>8 0.79730999 <a title="17-lda-8" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>9 0.78689367 <a title="17-lda-9" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>10 0.78419966 <a title="17-lda-10" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>11 0.77127153 <a title="17-lda-11" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>12 0.77068698 <a title="17-lda-12" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>13 0.77004528 <a title="17-lda-13" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>14 0.76646286 <a title="17-lda-14" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>15 0.76516187 <a title="17-lda-15" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>16 0.76471961 <a title="17-lda-16" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>17 0.76259011 <a title="17-lda-17" href="./nips-2009-Subject_independent_EEG-based_BCI_decoding.html">237 nips-2009-Subject independent EEG-based BCI decoding</a></p>
<p>18 0.76006335 <a title="17-lda-18" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>19 0.75929129 <a title="17-lda-19" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>20 0.75791413 <a title="17-lda-20" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
