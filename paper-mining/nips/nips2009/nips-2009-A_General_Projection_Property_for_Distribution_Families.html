<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 nips-2009-A General Projection Property for Distribution Families</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-11" href="#">nips2009-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 nips-2009-A General Projection Property for Distribution Families</h1>
<br/><p>Source: <a title="nips-2009-11-pdf" href="http://papers.nips.cc/paper/3811-a-general-projection-property-for-distribution-families.pdf">pdf</a></p><p>Author: Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</p><p>Abstract: Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. 1</p><p>Reference: <a title="nips-2009-11-reference" href="../nips2009_reference/nips-2009-A_General_Projection_Property_for_Distribution_Families_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. [sent-3, score-0.308]
</p><p>2 We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. [sent-4, score-0.621]
</p><p>3 By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. [sent-5, score-0.713]
</p><p>4 A parametric approach to handling uncertainty in such a setting would be to ﬁt a speciﬁc parametric model to the known moments and then apply stochastic programming techniques to solve for an optimal decision. [sent-11, score-0.207]
</p><p>5 However, such a parametric strategy can be too bold, hard to justify, and might incur signiﬁcant loss if the ﬁtting distribution does not match the true underlying distribution very well. [sent-13, score-0.156]
</p><p>6 Such a minimax formulation has been studied in several ﬁelds [1; 2; 3; 4; 5; 6] and is also the focus of this paper. [sent-15, score-0.22]
</p><p>7 Although Bayesian optimal decision theory is a rightfully wellestablished approach for decision making under uncertainty, minimax has proved to be a useful alternative in many domains, such as ﬁnance, where it is difﬁcult to formulate appropriate priors over models. [sent-16, score-0.235]
</p><p>8 In these ﬁelds, minimax formulation combined with stochastic programming [7] have been extensively studied and successfully applied. [sent-17, score-0.298]
</p><p>9 We make a contribution to minimax probability theory and apply the results to problems arising in four different areas. [sent-18, score-0.161]
</p><p>10 Speciﬁcally, we generalize a classic result on the linear projection property of distribution families: we show that any linear projection between distribution families with ﬁxed mean and covariance, regardless of their dimensions, is surjective. [sent-19, score-0.62]
</p><p>11 Our proof imposes no conditions on the deterministic matrix X, hence extends the classic projection result in [6], which assumes X is a vector. [sent-21, score-0.21]
</p><p>12 We furthermore extend this surjective property to some restricted distribution 1  families, which allows additional prior information to be incorporated and hence less conservative solutions to be obtained. [sent-22, score-0.361]
</p><p>13 In particular, we prove that surjectivity of linear projections remains to hold for distribution families that are additionally symmetric, log-concave, or symmetric linear unimodal. [sent-23, score-0.502]
</p><p>14 An immediate application of these results is to reduce the worst-case analysis of multivariate expectations to the univariate (or reduced multivariate) ones, which have been long studied and produced many fruitful results. [sent-25, score-0.332]
</p><p>15 In this direction, we conduct worst-case analyses of some common restricted distribution families. [sent-26, score-0.136]
</p><p>16 We illustrate our results on problems that incorporate a classic worst case value-at-risk constraint: minimax probability classiﬁcation [2]; chance constrained linear programming (CCLP) [3]; portfolio selection [4]; and Markov decision processes (MDPs) with reward uncertainty [8]. [sent-27, score-0.912]
</p><p>17 Additionally, we provide extensions to other constrained distribution families, which makes the minimax formulation less conservative in each case. [sent-29, score-0.321]
</p><p>18 These results are then extended to the more recent conditional value-at-risk constraint, and new bounds are proved, including a new bound on the survival function for symmetric unimodal distributions. [sent-30, score-0.511]
</p><p>19 2  A General Projection Property  First we establish a generalized linear projection property for distribution families. [sent-31, score-0.202]
</p><p>20 The key application will be to reduce worst-case multivariate stochastic programming problems to lower dimensional equivalents; see Corollary 1. [sent-32, score-0.178]
</p><p>21 Popescu [6] has proved the special case of reduction to one dimension, however we provide a simpler proof that can be more easily extended to other distribution families1 . [sent-33, score-0.137]
</p><p>22 Let (µ, Σ) denote the family of distributions sharing common mean µ and covariance Σ, and let µX = X T µ and ΣX = X T ΣX. [sent-34, score-0.139]
</p><p>23 Theorem 1 (General Projection Property (GPP)) For all µ, Σ ≽ 0, and X ∈ Rm×d , the projection X T R = r from m-variate distributions R ∼ (µ, Σ) to d-variate distributions r ∼ (µX , ΣX ) is surjective and many-to-one. [sent-37, score-0.263]
</p><p>24 Although this establishes the general result, we extend it to distribution families under additional constraints below. [sent-44, score-0.363]
</p><p>25 In such cases, if a general linear projection property can still be shown to hold, the additional assumptions can be used to make the minimax approach less conservative in a simple, direct manner. [sent-46, score-0.449]
</p><p>26 We thus consider a number of additionally restricted distribution families. [sent-47, score-0.132]
</p><p>27 Deﬁnition 1 A random vector X is called (centrally) symmetric about µ, if for all vectors x, Pr(X ≥ µ + x) = Pr(X ≤ µ − x). [sent-48, score-0.125]
</p><p>28 A univariate random variable is called unimodal about a if its cumulative distribution function (c. [sent-49, score-0.444]
</p><p>29 A random m-vector X is called linear unimodal about 0m if for all a ∈ Rm , aT X is (univariate) unimodal about 0. [sent-57, score-0.477]
</p><p>30 2  let (µ, Σ)SU denote the family of distributions that are additionally symmetric and linear unimodal about µ. [sent-59, score-0.515]
</p><p>31 Lemma 1 (a) If random vector X is symmetric about 0, then AX + µ is symmetric about µ. [sent-61, score-0.25]
</p><p>32 (b) If X, Y are independent and both symmetric about 0, Z = X + Y is also symmetric about 0. [sent-62, score-0.25]
</p><p>33 Although once misbelieved, it is now clear that the convolution of two (univariate) unimodal distributions need not be unimodal. [sent-63, score-0.308]
</p><p>34 However, for symmetric, unimodal distributions we have Lemma 2 ([10] Theorem 1. [sent-64, score-0.28]
</p><p>35 6) If two independent random variables x and y are both symmetric and unimodal about 0, then z = x + y is also unimodal about 0. [sent-65, score-0.573]
</p><p>36 There are several non-equivalent extensions of unimodality to multivariate random variables. [sent-66, score-0.331]
</p><p>37 Theorem 2 (GPP for Symmetric, Log-concave, and Symmetric Linear Unimodal Distributions) For all µ, Σ ≽ 0 and X ∈ Rm×d , the projection X T R = r from m-variate R ∼ (µ, Σ)S to d-variate r ∼ (µX , ΣX )S is surjective and many-to-one. [sent-78, score-0.151]
</p><p>38 Then, respectively, symmetry of the constructed R follows from Lemma 1; log-concavity of R follows from Lemma 3; and linear unimodality of R follows from the deﬁnition and Lemma 2. [sent-81, score-0.423]
</p><p>39 An immediate application of the general projection property is to reduce worst-case analyses of multivariate expectations to the univariate case. [sent-83, score-0.475]
</p><p>40 Corollary 1 For any matrix X and any function g(·) (including in particular when X is a vector) sup  E[g(X T R)]  =  sup  E[g(r)]. [sent-85, score-0.198]
</p><p>41 4  3  Application to Worst-case Value-at-risk  We now apply these projection properties to analyze the worst case value-at-risk (VaR) —a useful risk criterion in many application areas. [sent-90, score-0.29]
</p><p>42 Consider the following constraint on a distribution R Pr(−xT R ≤ α) ≥ 1 − ϵ, (2) 2 A sufﬁcient but not necessary condition for log-concavity is having log-concave densities. [sent-91, score-0.12]
</p><p>43 In the univariate case, log-concave distributions are called strongly unimodal, which is only a proper subset of univariate unimodal distributions [10]. [sent-93, score-0.682]
</p><p>44 3 If X is a vector we can also extend this theorem to other multivariate unimodalities such as symmetric star/block/convex unimodal. [sent-94, score-0.332]
</p><p>45 4 The closure of (µ, Σ), (µ, Σ)S , (µ, Σ)L , and (µ, Σ)SU under linear projection is critical for Corollary 1 to hold. [sent-95, score-0.117]
</p><p>46 Corollary 1 fails for other kinds of multivariate unimodalities, such as symmetric star/block/convex unimodal. [sent-96, score-0.194]
</p><p>47 Within certain restricted distribution families, such as Q-radially symmetric distributions, (2) can be (equivalently) transformed to a deterministic second order cone constraint (depending on the range of ϵ) [3]. [sent-101, score-0.287]
</p><p>48 Suppose however that one knew the distribution of R belonged to a certain family, such as (µ, Σ). [sent-103, score-0.14]
</p><p>49 If we have additional information about the underlying distribution, such as symmetry or unimodality, the worstcase ϵ-VaR can be reduced. [sent-107, score-0.209]
</p><p>50 ) Proof: From Corollary 1 it follows that inf R∼(µ,Σ)  Pr(−xT R ≤ α) =  inf  2 r∼(−µx ,σx )  Pr(r ≤ α) = 1 −  sup  Pr(r > α). [sent-118, score-0.247]
</p><p>51 [2] and [3] provide a proof of (4) based on the multivariate Chebyshev inequality in [12]; [4] proves (4) from dual optimality; and the proof in [6] utilizes two point support property of the general constraint (3). [sent-124, score-0.29]
</p><p>52 6  4  Figure 1: Comparison of the coefﬁcients in front of σx for different distribution families in Proposition 1 (left) and Proposition 2 (right). [sent-129, score-0.234]
</p><p>53 Figure 1 compares the coefﬁcients on σx among the different worst case VaR for different distribution families. [sent-132, score-0.111]
</p><p>54 The large gap between coefﬁcients for general and symmetric (linear) unimodal distributions demonstrates how additional constraints can generate much less conservative solutions while still ensuring robustness. [sent-133, score-0.58]
</p><p>55 Beyond simplifying existing proofs, Proposition 1 can be used to extend some of the uses of the VaR criterion in different application areas. [sent-134, score-0.133]
</p><p>56 From the data, the distribution families (µ1 , Σ1 ) and (µ2 , Σ2 ) can be estimated. [sent-138, score-0.234]
</p><p>57 Then a robust hyperplane can be recovered by minimizing the worst-case error [ ] [ ] min ϵ s. [sent-139, score-0.153]
</p><p>58 inf Pr(xT R1 ≤ α) ≥ 1 − ϵ and inf Pr(xT R2 ≥ α) ≥ 1 − ϵ, (12) x̸=0,α,ϵ  R1 ∼(µ1 ,Σ1 )  R2 ∼(µ2 ,Σ2 )  where x is the normal vector of the hyperplane, α is the offset and ϵ controls the error probability. [sent-141, score-0.148]
</p><p>59 In this case, suppose we knew in advance that the optimal ϵ lay in [ 2 , 1), meaning that no hyperplane predicts better than random guessing. [sent-145, score-0.113]
</p><p>60 Then the constraints in (12) become linear, covariance information becomes useless in determining the optimal hyperplane, and the optimization concentrates solely on separating the means of two classes. [sent-146, score-0.158]
</p><p>61 Although such a result might seem surprising, it is a direct consequence of symmetry: the worst-case distributions are forced to put probability mass arbitrarily far away on both sides of the mean, thereby eliminating any information brought by covariance. [sent-147, score-0.13]
</p><p>62 When the optimal ϵ lies in (0, 1 ), however, covariance information becomes 2 meaningful, since the worst-case distributions can no longer put probability mass arbitrarily far away on both sides of the mean (owing to the existence of a hyperplane that predicts labels better than random guessing). [sent-148, score-0.207]
</p><p>63 Calaﬁore and El Ghaoui studied this problem in [3], and imposed the inequality constraint with high probability, leading to the the so-called chance constrained linear program (CCLP): [ ] min aT x s. [sent-154, score-0.238]
</p><p>64 Depending on the value of ϵ, the chance constraint can be equivalently transformed into a second order cone constraint or a linear constraint. [sent-158, score-0.215]
</p><p>65 The work in [3] concentrates on the general and symmetric distribution families. [sent-159, score-0.206]
</p><p>66 Portfolio Selection [4]: In portfolio selection, let R represent the (uncertain) returns of a suite of ﬁnancial assets, and x the weighting one would like to put on the various assets. [sent-163, score-0.313]
</p><p>67 Previous work has not addressed the case when additional symmetry or linear unimodal information is available. [sent-170, score-0.434]
</p><p>68 However, comparing the minimal value of α in Proposition 1, we see that such additional information, such as symmetry or unimodality, indeed decreases our potential loss, as shown clearly in Figure 1. [sent-171, score-0.181]
</p><p>69 This makes sense, since the more one knows about uncertain returns the less risk one should have to bear. [sent-172, score-0.133]
</p><p>70 Delage and Mannor [8] extend this problem to the uncertain case by employing the value-at-risk type constraint (2) and assuming the unknown reward model and transition kernel are drawn from a known distribution (Gaussian and Dirichlet respectively). [sent-177, score-0.303]
</p><p>71 Unfortunately, [8] also proves that the constraint (2) is generally NP-hard to satisfy unless one assumes some very restricted form of distribution, such as Gaussian. [sent-178, score-0.115]
</p><p>72 Alternatively, note that one can use the worst case value-at-risk formulation (3) to obtain a tractable approximation to (2) [ ] min α s. [sent-179, score-0.129]
</p><p>73 inf Pr(−xT R ≤ α) ≥ 1 − ϵ, (15) x,α  R∼(µ,Σ)  where R is the reward function (unknown but assumed to belong to (µ, Σ)) and x represents a discounted-stationary state-action visitation distribution (which can be used to recover an optimal behavior policy). [sent-181, score-0.176]
</p><p>74 Thus, given reasonable constraints, the minimax approach does not have to be overly conservative, while providing robustness and tractability. [sent-183, score-0.161]
</p><p>75 4  Application to Worst-case Conditional Value-at-risk  Finally, we investigate the more reﬁned conditional value-at-risk (CVaR) criterion that bounds the conditional expectation of losses beyond the value-at-risk (VaR). [sent-184, score-0.16]
</p><p>76 For example, if ϵ = 0, the optimization problem trivially says that the loss of any portfolio will be no larger than ∞. [sent-192, score-0.35]
</p><p>77 Although one potential remedy might be to use Monte Carlo techniques to approximate the expectation, we instead take a robust approach: As before, suppose one knew the distribution of R belonged to a certain family, such as (µ, Σ). [sent-197, score-0.189]
</p><p>78 Given such knowledge, it is natural to consider the worst-case CVaR ] ] 1 [ 1 [ f = sup min α + E (−xT R − α)+ = min sup α + E (−xT R − α)+ , (18) α ϵ ϵ R∼(µ,Σ) α R∼(µ,Σ) where the interchangeability of the min and sup operators follows from the classic minimax theorem [15]. [sent-198, score-0.661]
</p><p>79 If one has additional information about the underlying distribution, such as symmetry or unimodality, the worst-case CVaR can be reduced. [sent-200, score-0.209]
</p><p>80 Proof: We know from Corollary 1 that [ ] sup E (−xT R − α)+ = R∼(µ,Σ)  sup 2 r∼(−µx ,σx )  [ ] E (r − α)+ ,  (23)  which reduces the problem to the univariate case. [sent-207, score-0.371]
</p><p>81 To proceed, we will need to make use of the univariate results given in Proposition 3 below. [sent-208, score-0.173]
</p><p>82 α 2ϵ This is a convex univariate optimization problem in α. [sent-210, score-0.21]
</p><p>83 Comparing VaR and CVaR in Figure 1 shows that unimodality has less impact on improving CVaR. [sent-216, score-0.262]
</p><p>84 A key component of Proposition 2 is its reliance on the following important univariate results. [sent-217, score-0.173]
</p><p>85 The following proposition gives tight bounds of the expected survival function for the various families. [sent-218, score-0.269]
</p><p>86 Given the space constraints, we can only discuss the direct application of worst-case CVaR to the portfolio selection problem. [sent-222, score-0.377]
</p><p>87 Next, when additional symmetry information is taken into account and ϵ ∈ (0, 1 ), CVaR and VaR again select the same portfolio but under different worst-case distri2 butions. [sent-226, score-0.494]
</p><p>88 When unimodality is added, the CVaR criterion ﬁnally begins to select different portfolios than VaR. [sent-227, score-0.326]
</p><p>89 5 Concluding Remarks We have provided a simpler yet broader proof of the general linear projection property for distribution families with given mean and covariance. [sent-228, score-0.479]
</p><p>90 The proof strategy can be easily extended to more restricted distribution families. [sent-229, score-0.178]
</p><p>91 A direct implication of our results is that worst-case analyses of multivariate expectations can often be reduced to those of univariate ones. [sent-230, score-0.318]
</p><p>92 By combining this trick with classic univariate inequalities, we were able to provide worst-case analyses of two widely adopted constraints (based on value-at-risk criteria). [sent-231, score-0.329]
</p><p>93 Our analysis recovers some existing results in a simpler way while also provides new insights on incorporating additional information. [sent-232, score-0.111]
</p><p>94 Above, we assumed the ﬁrst and second moments of the underlying distribution were precisely known, which of course is questionable in practice. [sent-233, score-0.141]
</p><p>95 One strategy, proposed in [2], is to construct a (bounded and convex) uncertainty set U over (µ, Σ), and then applying a similar minimax formulation but with respect to (µ, Σ) ∈ U. [sent-235, score-0.253]
</p><p>96 A second approach is simply to lower one’s conﬁdence of the constraints and rely on the fact that the moment estimates are close to their true values within some additional conﬁdence bound [17]. [sent-237, score-0.155]
</p><p>97 That is, instead of enforcing the constraint (3) or (18) surely, one can instead plug-in the estimated moments and argue that constraints will be satisﬁed within some diminished probability. [sent-238, score-0.181]
</p><p>98 9  Except the very recent work of [9] on portfolio selection. [sent-243, score-0.313]
</p><p>99 “Worst-case value-at-risk and robust portfolio optimization: a conic programming approach”. [sent-266, score-0.412]
</p><p>100 “Worst-case conditional value-at-risk with application to robust portfolio management”. [sent-271, score-0.425]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cvar', 0.477), ('portfolio', 0.313), ('unimodality', 0.262), ('unimodal', 0.224), ('families', 0.187), ('proposition', 0.174), ('univariate', 0.173), ('su', 0.163), ('minimax', 0.161), ('var', 0.149), ('pr', 0.149), ('xt', 0.142), ('symmetry', 0.132), ('symmetric', 0.125), ('cclp', 0.119), ('sup', 0.099), ('im', 0.098), ('uncertain', 0.09), ('projection', 0.088), ('conservative', 0.084), ('inf', 0.074), ('constraint', 0.073), ('ioana', 0.072), ('unimodalities', 0.072), ('multivariate', 0.069), ('hyperplane', 0.068), ('classic', 0.067), ('moments', 0.066), ('criterion', 0.064), ('worst', 0.064), ('uncertainty', 0.063), ('surjective', 0.063), ('survival', 0.063), ('corollary', 0.062), ('chebyshev', 0.057), ('laurent', 0.057), ('surely', 0.056), ('distributions', 0.056), ('reward', 0.055), ('proof', 0.055), ('inequalities', 0.052), ('el', 0.051), ('programming', 0.05), ('robust', 0.049), ('additional', 0.049), ('belonged', 0.048), ('delage', 0.048), ('gpp', 0.048), ('analyses', 0.047), ('distribution', 0.047), ('covariance', 0.045), ('ghaoui', 0.045), ('knew', 0.045), ('alberta', 0.043), ('risk', 0.043), ('additionally', 0.043), ('operations', 0.043), ('constraints', 0.042), ('ore', 0.042), ('surjectivity', 0.042), ('restricted', 0.042), ('mdps', 0.041), ('chance', 0.04), ('lemma', 0.039), ('csaba', 0.038), ('ingenuity', 0.038), ('szepesv', 0.038), ('extend', 0.038), ('family', 0.038), ('property', 0.038), ('sides', 0.038), ('optimization', 0.037), ('decision', 0.037), ('min', 0.036), ('brought', 0.036), ('simpler', 0.035), ('bound', 0.035), ('strategy', 0.034), ('concentrates', 0.034), ('mum', 0.034), ('selection', 0.033), ('proofs', 0.033), ('nance', 0.032), ('conditional', 0.032), ('bounds', 0.032), ('application', 0.031), ('studied', 0.03), ('coef', 0.03), ('program', 0.03), ('moment', 0.029), ('expectations', 0.029), ('rm', 0.029), ('cients', 0.029), ('formulation', 0.029), ('linear', 0.029), ('theorem', 0.028), ('underlying', 0.028), ('convolution', 0.028), ('stochastic', 0.028), ('incorporating', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="11-tfidf-1" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>Author: Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</p><p>Abstract: Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. 1</p><p>2 0.18989947 <a title="11-tfidf-2" href="./nips-2009-On_Stochastic_and_Worst-case_Models_for_Investing.html">178 nips-2009-On Stochastic and Worst-case Models for Investing</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best ﬁxed portfolio in hindsight. In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions. 1</p><p>3 0.1435862 <a title="11-tfidf-3" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright</p><p>Abstract: We study minimax rates for estimating high-dimensional nonparametric regression models with sparse additive structure and smoothness constraints. More precisely, our goal is to estimate a function f ∗ : Rp → R that has an additive decomposition of the form ∗ ∗ f ∗ (X1 , . . . , Xp ) = j∈S hj (Xj ), where each component function hj lies in some class H of “smooth” functions, and S ⊂ {1, . . . , p} is an unknown subset with cardinality s = |S|. Given n i.i.d. observations of f ∗ (X) corrupted with additive white Gaussian noise where the covariate vectors (X1 , X2 , X3 , ..., Xp ) are drawn with i.i.d. components from some distribution P, we determine lower bounds on the minimax rate for estimating the regression function with respect to squared-L2 (P) error. Our main result is a lower bound on the minimax rate that scales as max s log(p/s) , s ǫ2 (H) . The ﬁrst term reﬂects the sample size required for n n performing subset selection, and is independent of the function class H. The second term s ǫ2 (H) is an s-dimensional estimation term corresponding to the sample size required for n estimating a sum of s univariate functions, each chosen from the function class H. It depends linearly on the sparsity index s but is independent of the global dimension p. As a special case, if H corresponds to functions that are m-times differentiable (an mth -order Sobolev space), then the s-dimensional estimation term takes the form sǫ2 (H) ≍ s n−2m/(2m+1) . Either of n the two terms may be dominant in different regimes, depending on the relation between the sparsity and smoothness of the additive decomposition.</p><p>4 0.12680678 <a title="11-tfidf-4" href="./nips-2009-Information-theoretic_lower_bounds_on_the_oracle_complexity_of_convex_optimization.html">116 nips-2009-Information-theoretic lower bounds on the oracle complexity of convex optimization</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, Peter L. Bartlett, Pradeep K. Ravikumar</p><p>Abstract: Despite a large literature on upper bounds on complexity of convex optimization, relatively less attention has been paid to the fundamental hardness of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining a understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for various function classes. We also discuss implications of these results for the understanding the inherent complexity of large-scale learning and estimation problems. 1</p><p>5 0.080660194 <a title="11-tfidf-5" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Inﬁnitesimal Perturbation Analysis (IPA) on the ﬁltering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the ﬁltering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.</p><p>6 0.079398774 <a title="11-tfidf-6" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>7 0.076204427 <a title="11-tfidf-7" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>8 0.074444108 <a title="11-tfidf-8" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>9 0.067417555 <a title="11-tfidf-9" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>10 0.065969966 <a title="11-tfidf-10" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>11 0.064574331 <a title="11-tfidf-11" href="./nips-2009-Adaptive_Regularization_of_Weight_Vectors.html">27 nips-2009-Adaptive Regularization of Weight Vectors</a></p>
<p>12 0.064406887 <a title="11-tfidf-12" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>13 0.064030878 <a title="11-tfidf-13" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>14 0.059315454 <a title="11-tfidf-14" href="./nips-2009-Beyond_Convexity%3A_Online_Submodular_Minimization.html">45 nips-2009-Beyond Convexity: Online Submodular Minimization</a></p>
<p>15 0.058916263 <a title="11-tfidf-15" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>16 0.057480175 <a title="11-tfidf-16" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>17 0.057445858 <a title="11-tfidf-17" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>18 0.05635713 <a title="11-tfidf-18" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>19 0.055982906 <a title="11-tfidf-19" href="./nips-2009-Kernel_Choice_and_Classifiability_for_RKHS_Embeddings_of_Probability_Distributions.html">118 nips-2009-Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions</a></p>
<p>20 0.055535242 <a title="11-tfidf-20" href="./nips-2009-Compressed_Least-Squares_Regression.html">55 nips-2009-Compressed Least-Squares Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, 0.159), (2, 0.083), (3, -0.034), (4, 0.08), (5, 0.028), (6, 0.036), (7, -0.028), (8, 0.005), (9, -0.03), (10, 0.03), (11, -0.043), (12, -0.012), (13, 0.004), (14, 0.086), (15, -0.03), (16, -0.055), (17, 0.01), (18, 0.061), (19, 0.07), (20, 0.042), (21, -0.013), (22, -0.044), (23, 0.067), (24, -0.128), (25, -0.088), (26, 0.005), (27, -0.022), (28, -0.082), (29, -0.007), (30, -0.021), (31, -0.051), (32, -0.027), (33, -0.011), (34, 0.058), (35, -0.002), (36, -0.135), (37, 0.079), (38, -0.057), (39, -0.059), (40, -0.041), (41, 0.07), (42, -0.014), (43, 0.115), (44, 0.035), (45, 0.009), (46, -0.003), (47, -0.094), (48, 0.026), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94257528 <a title="11-lsi-1" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>Author: Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</p><p>Abstract: Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. 1</p><p>2 0.75652945 <a title="11-lsi-2" href="./nips-2009-Information-theoretic_lower_bounds_on_the_oracle_complexity_of_convex_optimization.html">116 nips-2009-Information-theoretic lower bounds on the oracle complexity of convex optimization</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, Peter L. Bartlett, Pradeep K. Ravikumar</p><p>Abstract: Despite a large literature on upper bounds on complexity of convex optimization, relatively less attention has been paid to the fundamental hardness of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining a understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for various function classes. We also discuss implications of these results for the understanding the inherent complexity of large-scale learning and estimation problems. 1</p><p>3 0.67526895 <a title="11-lsi-3" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright</p><p>Abstract: We study minimax rates for estimating high-dimensional nonparametric regression models with sparse additive structure and smoothness constraints. More precisely, our goal is to estimate a function f ∗ : Rp → R that has an additive decomposition of the form ∗ ∗ f ∗ (X1 , . . . , Xp ) = j∈S hj (Xj ), where each component function hj lies in some class H of “smooth” functions, and S ⊂ {1, . . . , p} is an unknown subset with cardinality s = |S|. Given n i.i.d. observations of f ∗ (X) corrupted with additive white Gaussian noise where the covariate vectors (X1 , X2 , X3 , ..., Xp ) are drawn with i.i.d. components from some distribution P, we determine lower bounds on the minimax rate for estimating the regression function with respect to squared-L2 (P) error. Our main result is a lower bound on the minimax rate that scales as max s log(p/s) , s ǫ2 (H) . The ﬁrst term reﬂects the sample size required for n n performing subset selection, and is independent of the function class H. The second term s ǫ2 (H) is an s-dimensional estimation term corresponding to the sample size required for n estimating a sum of s univariate functions, each chosen from the function class H. It depends linearly on the sparsity index s but is independent of the global dimension p. As a special case, if H corresponds to functions that are m-times differentiable (an mth -order Sobolev space), then the s-dimensional estimation term takes the form sǫ2 (H) ≍ s n−2m/(2m+1) . Either of n the two terms may be dominant in different regimes, depending on the relation between the sparsity and smoothness of the additive decomposition.</p><p>4 0.60050237 <a title="11-lsi-4" href="./nips-2009-On_Stochastic_and_Worst-case_Models_for_Investing.html">178 nips-2009-On Stochastic and Worst-case Models for Investing</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best ﬁxed portfolio in hindsight. In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions. 1</p><p>5 0.58788824 <a title="11-lsi-5" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>Author: Marcus Hutter</p><p>Abstract: The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identiﬁability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed. 1</p><p>6 0.56161982 <a title="11-lsi-6" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>7 0.54255521 <a title="11-lsi-7" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>8 0.53295279 <a title="11-lsi-8" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>9 0.51865107 <a title="11-lsi-9" href="./nips-2009-Adaptive_Regularization_of_Weight_Vectors.html">27 nips-2009-Adaptive Regularization of Weight Vectors</a></p>
<p>10 0.50576544 <a title="11-lsi-10" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>11 0.50236666 <a title="11-lsi-11" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<p>12 0.49261451 <a title="11-lsi-12" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>13 0.46449631 <a title="11-lsi-13" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>14 0.46086338 <a title="11-lsi-14" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>15 0.45222816 <a title="11-lsi-15" href="./nips-2009-A_Data-Driven_Approach_to_Modeling_Choice.html">7 nips-2009-A Data-Driven Approach to Modeling Choice</a></p>
<p>16 0.45180696 <a title="11-lsi-16" href="./nips-2009-On_the_Convergence_of_the_Concave-Convex_Procedure.html">180 nips-2009-On the Convergence of the Concave-Convex Procedure</a></p>
<p>17 0.44924229 <a title="11-lsi-17" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>18 0.43502554 <a title="11-lsi-18" href="./nips-2009-A_Smoothed_Approximate_Linear_Program.html">16 nips-2009-A Smoothed Approximate Linear Program</a></p>
<p>19 0.43215299 <a title="11-lsi-19" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>20 0.42509228 <a title="11-lsi-20" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.077), (25, 0.051), (35, 0.041), (36, 0.072), (39, 0.025), (58, 0.072), (61, 0.03), (71, 0.495), (86, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97489178 <a title="11-lda-1" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>2 0.97406471 <a title="11-lda-2" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>Author: Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</p><p>Abstract: We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject ﬁrst studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle ﬁlter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability. 1</p><p>3 0.96757013 <a title="11-lda-3" href="./nips-2009-Localizing_Bugs_in_Program_Executions_with_Graphical_Models.html">143 nips-2009-Localizing Bugs in Program Executions with Graphical Models</a></p>
<p>Author: Laura Dietz, Valentin Dallmeier, Andreas Zeller, Tobias Scheffer</p><p>Abstract: We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reﬂects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the Bernoulli graph model on data of the software projects AspectJ and Rhino. 1</p><p>same-paper 4 0.94894868 <a title="11-lda-4" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>Author: Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</p><p>Abstract: Surjectivity of linear projections between distribution families with ﬁxed mean and covariance (regardless of dimension) is re-derived by a new proof. We further extend this property to distribution families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov decision processes. 1</p><p>5 0.86323327 <a title="11-lda-5" href="./nips-2009-Learning_from_Multiple_Partially_Observed_Views_-_an_Application_to_Multilingual_Text_Categorization.html">130 nips-2009-Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization</a></p>
<p>Author: Massih Amini, Nicolas Usunier, Cyril Goutte</p><p>Abstract: We address the problem of learning classiﬁers when observations have multiple views, some of which may not be observed for all examples. We assume the existence of view generating functions which may complete the missing views in an approximate way. This situation corresponds for example to learning text classiﬁers from multilingual collections where documents are not available in all languages. In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages. We derive a generalization error bound for classiﬁers learned on examples with multiple artiﬁcially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning. An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning. Experimental results on a subset of the Reuters RCV1/RCV2 collections support our ﬁndings by showing that additional views obtained from MT may signiﬁcantly improve the classiﬁcation performance in the cases identiﬁed by our trade-off. 1</p><p>6 0.85662067 <a title="11-lda-6" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>7 0.78121924 <a title="11-lda-7" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>8 0.77621073 <a title="11-lda-8" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>9 0.68726975 <a title="11-lda-9" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>10 0.68623966 <a title="11-lda-10" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>11 0.67119485 <a title="11-lda-11" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>12 0.66648191 <a title="11-lda-12" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>13 0.66379339 <a title="11-lda-13" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>14 0.64443302 <a title="11-lda-14" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>15 0.64163715 <a title="11-lda-15" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>16 0.63997263 <a title="11-lda-16" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>17 0.63541257 <a title="11-lda-17" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>18 0.62984025 <a title="11-lda-18" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>19 0.62941575 <a title="11-lda-19" href="./nips-2009-Predicting_the_Optimal_Spacing_of_Study%3A_A_Multiscale_Context_Model_of_Memory.html">194 nips-2009-Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></p>
<p>20 0.6205163 <a title="11-lda-20" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
