<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>206 nips-2009-Riffled Independence for Ranked Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-206" href="#">nips2009-206</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>206 nips-2009-Riffled Independence for Ranked Data</h1>
<br/><p>Source: <a title="nips-2009-206-pdf" href="http://papers.nips.cc/paper/3775-riffled-independence-for-ranked-data.pdf">pdf</a></p><p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>Reference: <a title="nips-2009-206-reference" href="../nips2009_reference/nips-2009-Riffled_Independence_for_Ranked_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. [sent-3, score-0.181]
</p><p>2 One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. [sent-4, score-0.203]
</p><p>3 We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. [sent-5, score-0.168]
</p><p>4 In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. [sent-6, score-0.166]
</p><p>5 In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. [sent-7, score-0.222]
</p><p>6 We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. [sent-8, score-0.142]
</p><p>7 1  Introduction  Distributions over permutations play an important role in applications such as multi-object tracking, visual feature matching, and ranking. [sent-9, score-0.07]
</p><p>8 In tracking, for example, permutations represent joint assignments of individual identities to track positions, and in ranking, permutations represent the preference orderings of a list of items. [sent-10, score-0.152]
</p><p>9 Representing distributions over permutations is a notoriously difﬁcult problem since there are n! [sent-11, score-0.096]
</p><p>10 The quest for exploitable problem structure has led researchers to consider a number of possibilities including distribution sparsity [17, 9], exponential family parameterizations [15, 5, 14, 16], algebraic/Fourier structure [13, 12, 6, 7], and probabilistic independence [8]. [sent-13, score-0.177]
</p><p>11 While sparse distributions have been successfully applied in certain tracking domains, we argue that they are less suitable in ranking problems where it might be necessary to model indifference over a number of objects. [sent-14, score-0.151]
</p><p>12 In contrast, Fourier methods handle smooth distributions well but are not easily scalable without making aggressive independence assumptions [8]. [sent-15, score-0.168]
</p><p>13 In this paper, we argue that while probabilistic independence might be useful in tracking, it is a poor assumption in ranking. [sent-16, score-0.179]
</p><p>14 We propose a novel generalization of independence, called rifﬂed independence, which we argue to be far more suitable for modeling distributions over rankings, and develop algorithms for working with rifﬂed independence in the Fourier domain. [sent-17, score-0.18]
</p><p>15 • We introduce an intuitive generalization of independence on permutations, which we call rifﬂed independence, and show it to be a more appropriate notion of independence for ranked data, offering possibilities for efﬁcient inference and reduced sample complexity. [sent-19, score-0.33]
</p><p>16 • We introduce a novel family of distributions, called biased rifﬂe shufﬂes, that are useful for rifﬂed independence and propose an algorithm for computing its Fourier transform. [sent-20, score-0.191]
</p><p>17 By rearranging rows, one sees that independence imposes block-diagonal structure on the matrices. [sent-24, score-0.142]
</p><p>18 2  Distributions on permutations and independence relations  In the context of ranking, a permutation σ = [σ1 , . [sent-25, score-0.265]
</p><p>19 If we are ranking a list of fruits/vegetables enumerated as (1) Artichoke, (2) Broccoli, (3) Cherry, and (4) Dates, then the permutation σ = [σA σB σC σD ] = [2 3 1 4] ranks Cherry ﬁrst, Artichoke second, Broccoli third, Dates last. [sent-29, score-0.108]
</p><p>20 There have been a variety of methods proposed for summarizing distributions over permutations ranging from older ad-hoc methods such as maintaining k-best hypotheses [17] to the more recent Fourier-based methods which maintain a set of low-order summary statistics [18, 2, 11, 7]. [sent-44, score-0.096]
</p><p>21 , for tracking problems, since confusion in data association only occurs over small independent subgroups of objects in many problems. [sent-55, score-0.073]
</p><p>22 Probabilistic independence assumptions on the symmetric group can simply be stated as follows. [sent-57, score-0.142]
</p><p>23 Typically, one decomposes the distribution recursively and stores factors exactly for small enough factors, or compresses factors using Fourier coefﬁcients (but using higher frequency terms than what would be possible without the independence assumption). [sent-89, score-0.22]
</p><p>24 In order to exploit probabilistic independence in the Fourier domain, Huang et al. [sent-90, score-0.153]
</p><p>25 [8] proposed algorithms for joining factors and splitting distributions into independent components in the Fourier domain. [sent-91, score-0.071]
</p><p>26 Despite its utility for many tracking problems, however, we argue that the independence assumption on permutations implies a rather restrictive constraint on distributions, rendering independence highly unrealistic in ranking applications. [sent-93, score-0.443]
</p><p>27 , n}, σX is a permutation of elements in Y and σX is a permutation ¯ ¯ of its complement, Y , with probability 1. [sent-97, score-0.078]
</p><p>28 In our ranking example however, the ﬁrst-order condition forces the probability of any vegetable being in third place to be zero, even though both vegetables will, in general, have nonzero marginal probability of being in second place, which seems quite unrealistic. [sent-103, score-0.158]
</p><p>29 3  Going beyond full independence: Rifﬂed independence  The rifﬂe (or dovetail) shufﬂe [1] is perhaps the most popular method of card shufﬂing, in ¯ which one cuts a deck of n cards into two piles, X = {1, . [sent-105, score-0.257]
</p><p>30 Rankings that are rifﬂe independent are formed by independently selecting rankings for two disjoint subsets of objects, then interleaving the rankings using a rifﬂe shufﬂe to form a ranking over all objects. [sent-114, score-0.193]
</p><p>31 For example, we might ﬁrst ‘cut the deck’ into two piles, vegetables (X) and fruits ¯ (X), independently decide that Broccoli is preferred over Artichoke (σB < σA ) and that Dates is preferred over Cherry (σD < σC ), then interleave the fruit and vegetable rankings to form σB < σD < σA < σC (i. [sent-115, score-0.305]
</p><p>32 Intuitively, rifﬂed independence models complex ¯ relationships within each of set X and X while allowing correlations Figure 2: Rifﬂe shufﬂing a between sets to be modeled only by a constrained form of shufﬂing. [sent-118, score-0.142]
</p><p>33 The ranking σ after a shufﬂe is generated from the ranking prior to that shufﬂe, σ, by drawing a permutation, τ from a shufﬂing distribution m(τ ), and setting σ = τ σ. [sent-122, score-0.126]
</p><p>34 To answer this question, we use the distinguishing property of the rifﬂe shufﬂe, that, after cutting the deck into two piles of size p and q = n − p, the relative ranking relations within each pile are preserved. [sent-126, score-0.201]
</p><p>35 Thus, if the ith card lies above the j th in one of the piles, then after shufﬂing, the ith card remains above the j th . [sent-127, score-0.08]
</p><p>36 Any allowable rifﬂe shufﬂing distribution must therefore assign zero probability to permutations which do not preserve relative ranking relations. [sent-129, score-0.131]
</p><p>37 The set of permutations which do preserve these relations have a simple description. [sent-130, score-0.084]
</p><p>38 The (p, q)-interleavings can be shown to preserve the relative ranking relations within each ¯ of the subsets X = {1, . [sent-143, score-0.064]
</p><p>39 One possible rifﬂe shufﬂing distribution on S4 p q might, for example, assign uniform probability (munif (σ) = 1/6) to each permutation in Ω2,2 2,2 and zero probability to everything else, reﬂecting indifference between vegetables and fruits. [sent-155, score-0.155]
</p><p>40 We now formally deﬁne our generalization of independence where a distribution which fully factors independently is allowed to undergo a single rifﬂe shufﬂe. [sent-156, score-0.19]
</p><p>41 We denote rifﬂed independence by: h = f ⊥mp,q g, and refer to f, g as rifﬂed factors. [sent-165, score-0.142]
</p><p>42 To draw from h, one independently draws a permutation σp , of cards {1, . [sent-166, score-0.105]
</p><p>43 In our example, the rankings σp = [2 1] (Broccoli preferred to Artichoke) and σq = [4 3] (Cherry preferred to Dates) are selected, then shufﬂed (multiplied by τ{1,3} = [1 3 2 4]) to obtain σ = [3 1 4 2]. [sent-173, score-0.096]
</p><p>44 We remark that setting mp,q to be the delta distribution on any of the (p, q)-interleavings in Ωp,q recovers the deﬁnition of ordinary probabilistic independence, and thus rifﬂed independence is a strict generalization thereof. [sent-174, score-0.18]
</p><p>45 Just as in the full independence regime, where the distributions f and ¯ g are marginal distributions of rankings of X and X, in the rifﬂed independence regime, they can ¯ be thought of as marginal distributions of the relative rankings of X and X. [sent-175, score-0.468]
</p><p>46 There is, in the general case, a signiﬁcant increase in storage required for rifﬂed independence over full independence. [sent-177, score-0.166]
</p><p>47 ) storage required for distributions f and g, we now require O( n ) storage for the nonzero terms of the rifﬂe shufﬂing p distribution mp,q . [sent-180, score-0.065]
</p><p>48 We might, for example, have complex preference relations amongst vegetables and amongst fruits, but be completely indifferent with respect to the subsets, vegetables and fruits, as a whole. [sent-184, score-0.192]
</p><p>49 Starting with a deck of n cards cut into a left pile ({1, . [sent-186, score-0.136]
</p><p>50 , n}), pick one of the piles with probability proportional to its size (p/n for the left pile, q/n for the right) and drop the bottommost card, thus mapping either card p or card n to rank n. [sent-192, score-0.119]
</p><p>51 Then recurse on the n − 1 remaining undropped cards, drawing a (p − 1, q)-interleaving if the right pile was picked, or a (p, q − 1)-interleaving if the left pile was picked. [sent-193, score-0.129]
</p><p>52 We model this bias using a simple one-parameter family of distributions in which cards from the left and right piles drop with probability proportional to αp and (1 − α)q, respectively, instead of p and q. [sent-198, score-0.136]
</p><p>53 We will refer to α as the bias parameter, and the family of distributions parameterized by α as the biased rifﬂe shufﬂes. [sent-199, score-0.086]
</p><p>54 The bias parameter α can be thought of as a knob controlling the preference for one subset over the other, and might reﬂect, for example, a preference for fruits over vegetables, or perhaps indifference between the two subsets. [sent-201, score-0.135]
</p><p>55 Setting α = 0 or 1 recovers the full independence assumption, preferring objects in X (vegetables) ¯ over objects in X (fruits) with probability one (or vice-versa), and setting α = . [sent-202, score-0.198]
</p><p>56 For example, α might depend on the number of cards that have been dropped from each pile (allowing perhaps, for distributions to prefer crunchy fruits over crunchy vegetables, but soft vegetables over soft fruits). [sent-206, score-0.313]
</p><p>57 We are the ﬁrst to (1) use the recurrence to Fourier transform mp,q , and to (2) consider biased versions. [sent-209, score-0.097]
</p><p>58 The biased rifﬂe shufﬂes in [4] are not similar to our biased rifﬂe shufﬂes. [sent-210, score-0.098]
</p><p>59 Setting α = 0 or 1 recovers full independence, where a subset of objects is preferred over the other with probability one. [sent-224, score-0.065]
</p><p>60 4  Between independence and conditional independence  We have presented rifﬂe independent distributions as fully independent distributions which have been convolved by a certain class of shufﬂing distributions. [sent-225, score-0.376]
</p><p>61 In this section, we provide an alternative view of rifﬂed independence based on conditional independence, showing that the notion of rifﬂed independence lies somewhere between full and conditional independence. [sent-226, score-0.294]
</p><p>62 In Section 3, we formed a ranking by ﬁrst independently drawing permutations πp and πq , of object sets {1, . [sent-227, score-0.147]
</p><p>63 An equivalent way to form the same σ, however, is to ﬁrst draw an interleaving τY ∈ Ωp,q , then, conditioned on the choice of Y , draw independent permutations of ¯ the sets Y and Y . [sent-243, score-0.127]
</p><p>64 In our example, we might ﬁrst draw the (2,2)-interleaving [1 3 2 4] (so that after shufﬂing, we would obtain σV eg < σF ruit < σV eg < σF ruit ). [sent-244, score-0.063]
</p><p>65 Then we would draw a permutation ¯ of the vegetable ranks (Y = {1, 3}), say, [3 1], and a permutation of the fruit ranks (Y = {2, 4}), [4 2], to obtain a ﬁnal ranking over all items: σ = [3 1 4 2], or σB < σD < σA < σC . [sent-245, score-0.213]
</p><p>66 It is tempting to think that rifﬂed independence is exactly the conditional independence assumption, in which case the distribution would factor as h(σ) = h(Y ) · h(σX |Y ) · h(σX |Y ). [sent-246, score-0.295]
</p><p>67 + 1)) parameters, while rifﬂed p independence requires only O( n + p! [sent-249, score-0.142]
</p><p>68 p We now provide a simple correspondence between the conditional independence view of rifﬂed independence presented in this section to the shufﬂe theoretic deﬁnition from Section 3 (Def. [sent-252, score-0.296]
</p><p>69 ¯ Deﬁne the map φ, which, given a permutation of Y (or Y ), returns the permutation in σp ∈ Sp (or Sq ) such that [σp ]i is the rank of [σX ]i relative to the set Y . [sent-254, score-0.089]
</p><p>70 For example, if the permutation of the vegetable ranks is σX = [3 1] (with Artichoke ranked third, Broccoli ﬁrst), then φ(σX ) = [2 1] since, relative to the set of vegetables, Artichoke is ranked second, and Broccoli ﬁrst. [sent-255, score-0.149]
</p><p>71 ¯ ¯ ¯ Proposition 3 is useful because it shows that the probability of a single ranking can be computed without summing over the entire symmetric group (a convolution)— a fact that might not be obvious from Deﬁnition 2. [sent-259, score-0.064]
</p><p>72 The factorization h(σ) = m(τY )f (φ(σX ))g(φ(σX )) also suggests that ¯ rifﬂed independence behaves essentially like full independence (without the ﬁrst-order condition), where, in addition to the independent variables σX and σX , we also independently randomize ¯ over the subset Y . [sent-260, score-0.326]
</p><p>73 An immediate consequence is thatjust as in the full independence regime, conditioning operations on certain observations and MAP (maximum a posteriori) assignment problems decompose according to rifﬂed independence structure. [sent-261, score-0.294]
</p><p>74 Consider rifﬂe independent prior and likelihood functions, hprior and hlike , on Sn which factor as: hprior = fprior ⊥mprior gprior and hlike = flike ⊥mlike glike , respectively. [sent-263, score-0.124]
</p><p>75 The posterior distribution under Bayes rule can be written as the rifﬂe independent distribution: hpost ∝ (fprior flike ) ⊥mprior mlike (gprior glike ),where the symbol denotes the pointwise product operation. [sent-264, score-0.07]
</p><p>76 As a corollary, it follows that conditioning on simple pairwise ranking likelihood functions (that depend only on whether object i is preferred to object j) decomposes along rifﬂed independence structures. [sent-266, score-0.216]
</p><p>77 We begin with a brief introduction to Fourier theoretic inference on permutations (see [11, 7] for a detailed exposition). [sent-268, score-0.082]
</p><p>78 • (Convolution) The Fourier transform of a convolution is a product of Fourier transforms: [f ∗ g]i = fi · gi , for each frequency level i, where the operation · is matrix multiplication. [sent-281, score-0.066]
</p><p>79 σ∈Sn  A number of papers in recent years ([13, 6, 8, 7]) have considered approximating distributions over permutations using a truncated (bandlimited) set of Fourier coefﬁcients and have proposed inference algorithms that operate on these Fourier coefﬁcient matrices. [sent-283, score-0.096]
</p><p>80 [8] introduced Fourier domain algorithms, Join and Split, for combining independent factors to form joints and for extracting the factors from a joint distribution, respectively. [sent-286, score-0.07]
</p><p>81 However, for the class of biased rifﬂe shufﬂes from Section 3, one can efﬁciently compute the low-frequency terms of mα by employing p,q the recurrence relation in Alg. [sent-303, score-0.074]
</p><p>82 1 expresses a biased rifﬂe shufﬂe on Sn as a linear combination of biased rifﬂe shufﬂes on Sn−1 . [sent-306, score-0.098]
</p><p>83 Instead, our RifﬂeSplit algorithm left-multiplies each hi term by munif i , which p,q can be shown to be equivalent to convolving the distribution h by the ‘dual shufﬂe’, m∗ , deﬁned as m∗ (σ) = munif (σ −1 ). [sent-315, score-0.111]
</p><p>84 3) (with h ˆ p,q  As with RifﬂeJoin, it is necessary Fourier transform munif , which we can again accomplish via the p,q recurrence in Alg. [sent-319, score-0.092]
</p><p>85 Given enough Fourier terms to reconstruct the k th -order marginals of f and g, RifﬂeJoin returns enough Fourier terms to exactly reconstruct the k th -order marginals of h. [sent-326, score-0.132]
</p><p>86 Likewise, given enough Fourier terms to reconstruct the k th -order marginals of h, RifﬂeSplit returns enough Fourier terms to exactly reconstruct the k th -order marginals of both f and g. [sent-327, score-0.132]
</p><p>87 6  Experiments  In this section, we validate our algorithms and show that rifﬂed independence exists in real data. [sent-331, score-0.142]
</p><p>88 We ﬁrst perform an exhaustive search for subsets X and X that are closest to rifﬂe independent (with respect to DKL ), and ﬁnd that candidate 2 is nearly rifﬂe independent of the remaining candidates. [sent-334, score-0.064]
</p><p>89 4(a) we plot the true vote distribution and the best approximation by a distribution in which candidate 2 is rifﬂe independent of the rest. [sent-336, score-0.066]
</p><p>90 We are now able to verify his conjecture in a rifﬂed independence sense. [sent-341, score-0.142]
</p><p>91 We ﬁnd that X = {1, 3} and ¯ X = {4, 5} are very nearly rifﬂe independent and thus are able to verify that candidate sets {2}, {1, 3}, {4, 5} are indeed grouped in a rifﬂe independent sense in the APA data. [sent-343, score-0.064]
</p><p>92 The sushi dataset [10] consists of 5000 full rankings of ten types of sushi. [sent-350, score-0.083]
</p><p>93 4(b) plots testset log-likelihood as a function of training set size — we see that rifﬂe independence assumptions can help signiﬁcantly to lower the sample complexity of learning. [sent-354, score-0.142]
</p><p>94 4(c) which shows the ﬁrst-order marginals of Uni (Sea Urchin) rankings, and the biased rifﬂe approximation. [sent-369, score-0.085]
</p><p>95 To understand the behavior of RifﬂeSplit in approximately rifﬂe independent situations, we draw sample sets of varying sizes from a rifﬂe independent distribution on S8 (with bias parameter α = . [sent-371, score-0.073]
</p><p>96 For example, several papers note that graphical models cannot compactly represent distributions over permutations due to mutual exclusivity. [sent-386, score-0.096]
</p><p>97 An interesting question which our paper opens, is whether it is possible to use something similar to graphical models by substituting conditional generalizations of rifﬂed independence for ordinary conditional independence. [sent-387, score-0.154]
</p><p>98 Other possibilities include going beyond the algebraic approach and studying rifﬂed independence in non-Fourier frameworks and developing statistical (rifﬂed) independence tests. [sent-388, score-0.297]
</p><p>99 In summary, we have introduced rifﬂed independence and discussed how to exploit such structure in a Fourier-theoretic framework. [sent-389, score-0.142]
</p><p>100 Rifﬂed independence is a new tool for analyzing ranked data and has the potential to offer novel insights into datasets both new and old. [sent-390, score-0.175]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rif', 0.883), ('shuf', 0.286), ('independence', 0.142), ('fourier', 0.138), ('esplit', 0.089), ('vegetables', 0.083), ('ing', 0.076), ('ejoin', 0.076), ('permutations', 0.07), ('fruits', 0.064), ('pile', 0.057), ('sn', 0.054), ('broccoli', 0.051), ('ranking', 0.05), ('biased', 0.049), ('rankings', 0.048), ('apa', 0.044), ('munif', 0.044), ('piles', 0.044), ('cards', 0.043), ('permutation', 0.039), ('artichoke', 0.039), ('iffle', 0.038), ('marginals', 0.036), ('deck', 0.036), ('coef', 0.035), ('ranked', 0.033), ('tracking', 0.027), ('distributions', 0.026), ('convolution', 0.026), ('card', 0.026), ('recurrence', 0.025), ('cherry', 0.025), ('sushi', 0.025), ('vegetable', 0.025), ('factors', 0.025), ('cients', 0.025), ('preferred', 0.024), ('candidate', 0.024), ('transform', 0.023), ('indifference', 0.022), ('huang', 0.022), ('dkl', 0.021), ('independent', 0.02), ('ranks', 0.019), ('foreach', 0.019), ('plit', 0.019), ('riffle', 0.019), ('ruit', 0.019), ('urchin', 0.019), ('split', 0.017), ('frequency', 0.017), ('nif', 0.017), ('reconstruct', 0.016), ('recovers', 0.016), ('dates', 0.016), ('objects', 0.015), ('interleaving', 0.015), ('exclusivity', 0.015), ('drawing', 0.015), ('guestrin', 0.014), ('relations', 0.014), ('favorite', 0.014), ('join', 0.014), ('might', 0.014), ('th', 0.014), ('storage', 0.014), ('possibilities', 0.013), ('crunchy', 0.013), ('dovetail', 0.013), ('flike', 0.013), ('fprior', 0.013), ('glike', 0.013), ('gprior', 0.013), ('hlike', 0.013), ('hprior', 0.013), ('mlike', 0.013), ('mprior', 0.013), ('oin', 0.013), ('preference', 0.012), ('hi', 0.012), ('ed', 0.012), ('independently', 0.012), ('theoretic', 0.012), ('argue', 0.012), ('generalizations', 0.012), ('drop', 0.012), ('distribution', 0.011), ('rank', 0.011), ('confusion', 0.011), ('diaconis', 0.011), ('fruit', 0.011), ('draw', 0.011), ('bias', 0.011), ('probabilistic', 0.011), ('full', 0.01), ('candidates', 0.01), ('bb', 0.01), ('team', 0.01), ('sth', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="206-tfidf-1" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>2 0.056160159 <a title="206-tfidf-2" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>3 0.043118425 <a title="206-tfidf-3" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>Author: Chunxiao Zhou, Huixia J. Wang, Yongmei M. Wang</p><p>Abstract: In this paper, we develop an efficient moments-based permutation test approach to improve the test s computational efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation. Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency. 1 In t ro d u c t i o n Permutation tests are flexible nonparametric alternatives to parametric tests in small samples, or when the distribution of a test statistic is unknown or mathematically intractable. In permutation tests, except exchangeability, no other statistical assumptions are required. The p-values can be obtained by using the permutation distribution. Permutation tests are appealing in many biomedical studies, which often have limited observations with unknown distribution. They have been used successfully in structural MR image analysis [1, 2, 3], in functional MR image analysis [4], and in 3D face analysis [5]. There are three common approaches to construct the permutation distribution [6, 7, 8]: (1) exact permutation enumerating all possible arrangements; (2) approximate permutation based on random sampling from all possible permutations; (3) approximate permutation using the analytical moments of the exact permutation distribution under the null hypothesis. The main disadvantage of the exact permutation is the computational cost, due to the factorial increase in the number of permutations with the increasing number of subjects. The second technique often gives inflated type I errors caused by random sampling. When a large number of repeated tests are needed, the random permutation strategy is also computationally expensive to achieve satisfactory accuracy. Regarding the third approach, the exact permutation distribution may not have moments or moments with tractability. In most applications, it is not the existence but the derivation of moments that limits the third approach. To the best of our knowledge, there is no systematic and efficient way to derive the moments of the permutation distribution. Recently, Zhou [3] proposed a solution by converting the permutation of data to that of the statistic coefficients that are symmetric to the permutation. Since the test statistic coefficients usually have simple presentations, it is easier to track the permutation of the test statistic coefficients than that of data. However, this method requires the derivation of the permutation for each specific test statistic, which is not accessible to practical users. In this paper, we propose a novel strategy by employing a general theoretical method to derive the moments of the permutation distribution of any weighted v-statistics, for both univariate and multivariate data. We note that any moments of the permutation distribution for weighted v-statistics [9] can be considered as a summation of the product of data function term and index function term over a high dimensional index set and all possible permutations. Our key idea is to divide the whole index set into several permutation equivalent (see Definition 2) index subsets such that the summation of the data/index function term over all permutations is invariant within each subset and can be calculated without conducting any permutation. Then we can obtain the moments by summing up several subtotals. The proposed method can be extended to equivalent weighted v-statistics by replacing them with monotonic weighted v-statistics. This is due to the fact that only the order of test statistics of all permutations matters for obtaining the p-values, so that the monotonic weighted v-statistics shares the same p-value with the original test statistic. Given the first four moments, the permutation distribution can be well fitted by Pearson distribution series. The p-values are then obtained without conducting any real permutation. For multiple comparison of two-group difference, given the sample size n1 = 21 and n2 = 21, the number of tests m = 2,000, we need to conduct m×(n1 + n2 )!/n1 !/n2 ! 1.1×1015 permutations for the exact permutation test. Even for 20,000 random permutations per test, we still need m×20,000 4×107 permutations. Alternatively, our moments-based permutation method using Pearson distribution approximation only involves the calculation of the first four analytically-derived moments of exact permutation distributions to achieve high accuracy (see section 3). Instead of calculating test statistics in factorial scale with exact permutation, our moments-based permutation only requires computation of polynomial order. For example, the computational cost for univariate mean difference test statistic and modified multivariate Hotelling's T2 test statistics [8] are O(n) and O(n3 ), respectively, where n = n 1 + n2 . 2 M e t h o d o lo g y In this section, we shall mainly discuss how to calculate the moments of the permutation distribution for weighted v-statistics. For other test statistics, a possible solution is to replace them with their equivalent weighted v-statistics by monotonic transforms. The detailed discussion about equivalent test statistics can be found in [7, 8, 10]. 2.1 C o m p ut a t i o n a l c h a l l e n g e Let us first look at a toy example. Suppose we have a two-group univariate data x = ( x1 , L , xn1 , xn1 +1 ,L , xn1 + n2 ) , where the first n1 elements are in group A and the rest, n2 ,are in group B. For comparison of the two groups, the hypothesis is typically constructed as: H 0 : m A = mB vs. H a : m A ¹ m B , where m A , m B are the population means of the groups A n1 n i =1 i = n1 +1 and B, respectively. Define x A = å xi / n1 and xB = å xi / n2 as the sample means of two groups, where n=n1+n2. We choose the univariate group mean difference statistic, i.e., n T ( x) = x A - xB = å w(i ) xi , i =1 where the index function as the test w(i ) = 1/ n1 , if i Î {1, L , n1} and w(i ) = -1/ n2 , if i Î {n1 + 1, L, n} . Then the total number of all possible permutations of {1, L, n} is n!. To calculate the fourth moment of the permutation distribution, n n n n n 1 1 4 å ( å w(i ) xp (i ) ) = å å å å å w(i1 ) w(i2 ) w(i3 ) w(i4 )xp ( i1 ) xp ( i2 ) xp ( i3 ) xp ( i4 ) , n ! p ÎS n i =1 n ! p ÎS n i1 =1 i2 =1 i3 =1 i4 =1 where is the permutation operator and the symmetric group Sn [11] includes all distinct permutations. The above example shows that the moment calculation can be considered as a summation over all possible permutations and a large index set. It is noticeable that the computational challenge here is to go through the factorial level permutations and polynomial level indices. Ep (T 4 ( x ))= 2.2 P a r t i t i o n t h e i n de x s e t In this paper, we assume that the test statistic T can be expressed as a weighted v-statistic of n n i1 =1 id =1 degree d [9], that is, T ( x) = å L å w(i1 , L , id ) h( xi1 ,L , xid ) , where x = ( x1 , x 2 , L , x n ) T is a data with n observations, and w is a symmetric index function. h is a symmetric data function, i.e., invariant under permutation of (i1 ,L , id ) . Though the symmetry property is not required for our method, it helps reduce the computational cost. Here, each observation xk can be either univariate or multivariate. In the above toy example, d=1 and h is the identity function. Therefore, the r-th moment of the test statistic from the permutated data is: Ep (T r ( x)) = Ep ( å w(i1 ,L , id )h( xp (i1 ) ,L , xp ( id ) ))r i1 ,i2 ,L,id = Ep [ å i1(1) ,L, id (1) , r r k =1 k =1 { Õ w(i1(k ) ,L, id ( k ) ) Õ h( xp (i ( k ) ) ,L, xp (i ( k ) ) )}] . L i1( r ) ,L,id ( r ) d 1 Then we can exchange the summation order of permutations and that of indices, Ep (T r ( x)) = å i1(1) ,L, id (1) , L i1( r ) ,L,id ( r ) r r k =1 k =1 {( Õ w(i1( k ) ,L , id ( k ) )) Ep ( Õ h( xp (i ( k ) ) ,L, xp (i ( k ) ) ))}. d 1 Thus any moment of permutation distribution can be considered as a summation of the product of data function term and index function term over a high dimensional index set and all possible permutations. Since all possible permutations map any index value between 1 and n to all possible index r values from 1 to n with equal probability, Ep ( Õ h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) , the summation of k =1 1 d data function over all permutations is only related to the equal/unequal relationship among indices. It is natural to divide the whole index set U = {i1 ,L , id }r = {(i1(1) , L , id (1) ), L (r ) , ( i1 , L , id (r ) r )} into the union of disjoint index subsets, in which Ep ( Õ h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) k =1 1 d is invariant. Definition 1. Since h is a symmetric function, two index elements (i1 ,L , id ) and ( j1 ,L , jd ) are said to be equivalent if they are the same up to the order. For example, for d = 3, (1, 4, 5) = (1,5,4) = (4,1,5) = (4,5,1) = (5,1,4) = (5,4,1). Definition 2. Two indices {(i1(1) , L , id (1) ), L , (i1( r ) , L , id ( r ) )} and {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} are said to be permutation equivalent/ if there exists a permutation p Î Sn such that {(p (i1(1) ), L , p (id (1) )), L , (p (i1( r ) ), L , p (id ( r ) ))} = {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} . Here</p><p>4 0.039862346 <a title="206-tfidf-4" href="./nips-2009-Ranking_Measures_and_Loss_Functions_in_Learning_to_Rank.html">199 nips-2009-Ranking Measures and Loss Functions in Learning to Rank</a></p>
<p>Author: Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, Hang Li</p><p>Abstract: Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learningto-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measurebased ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classiﬁcation tasks, and deﬁne a so-called essential loss for ranking as the weighted sum of the classiﬁcation errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modiﬁcations can lead to better ranking performances, demonstrating the correctness of our theoretical analysis. 1</p><p>5 0.036644895 <a title="206-tfidf-5" href="./nips-2009-Statistical_Consistency_of_Top-k_Ranking.html">230 nips-2009-Statistical Consistency of Top-k Ranking</a></p>
<p>Author: Fen Xia, Tie-yan Liu, Hang Li</p><p>Abstract: This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we deﬁne a top-k ranking framework, where the true loss (and thus the risks) are deﬁned on the basis of top-k subgroup of permutations. This framework can include the permutationlevel ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufﬁcient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modiﬁcations, the methods can work signiﬁcantly better than their original versions. 1</p><p>6 0.026701314 <a title="206-tfidf-6" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>7 0.026665768 <a title="206-tfidf-7" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>8 0.025530649 <a title="206-tfidf-8" href="./nips-2009-A_Data-Driven_Approach_to_Modeling_Choice.html">7 nips-2009-A Data-Driven Approach to Modeling Choice</a></p>
<p>9 0.023733007 <a title="206-tfidf-9" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>10 0.021891547 <a title="206-tfidf-10" href="./nips-2009-Learning_to_Rank_by_Optimizing_NDCG_Measure.html">136 nips-2009-Learning to Rank by Optimizing NDCG Measure</a></p>
<p>11 0.020765223 <a title="206-tfidf-11" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>12 0.020359566 <a title="206-tfidf-12" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>13 0.019856272 <a title="206-tfidf-13" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>14 0.019204322 <a title="206-tfidf-14" href="./nips-2009-Locality-sensitive_binary_codes_from_shift-invariant_kernels.html">142 nips-2009-Locality-sensitive binary codes from shift-invariant kernels</a></p>
<p>15 0.018494485 <a title="206-tfidf-15" href="./nips-2009-Toward_Provably_Correct_Feature_Selection_in_Arbitrary_Domains.html">248 nips-2009-Toward Provably Correct Feature Selection in Arbitrary Domains</a></p>
<p>16 0.01775826 <a title="206-tfidf-16" href="./nips-2009-Boosting_with_Spatial_Regularization.html">47 nips-2009-Boosting with Spatial Regularization</a></p>
<p>17 0.017752709 <a title="206-tfidf-17" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>18 0.016543582 <a title="206-tfidf-18" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>19 0.016389297 <a title="206-tfidf-19" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>20 0.016166447 <a title="206-tfidf-20" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.055), (1, -0.0), (2, -0.001), (3, -0.009), (4, -0.003), (5, -0.02), (6, -0.04), (7, -0.037), (8, 0.02), (9, -0.048), (10, 0.012), (11, -0.009), (12, -0.015), (13, -0.015), (14, 0.005), (15, 0.009), (16, -0.013), (17, 0.028), (18, 0.008), (19, 0.008), (20, -0.042), (21, 0.016), (22, -0.035), (23, 0.023), (24, -0.04), (25, 0.029), (26, -0.027), (27, -0.02), (28, -0.038), (29, -0.015), (30, -0.027), (31, -0.04), (32, -0.052), (33, -0.017), (34, 0.029), (35, 0.008), (36, -0.045), (37, 0.023), (38, -0.066), (39, -0.012), (40, 0.029), (41, -0.027), (42, 0.028), (43, -0.003), (44, -0.018), (45, -0.028), (46, 0.078), (47, -0.046), (48, 0.001), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89724457 <a title="206-lsi-1" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>2 0.63754898 <a title="206-lsi-2" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>Author: Chunxiao Zhou, Huixia J. Wang, Yongmei M. Wang</p><p>Abstract: In this paper, we develop an efficient moments-based permutation test approach to improve the test s computational efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation. Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency. 1 In t ro d u c t i o n Permutation tests are flexible nonparametric alternatives to parametric tests in small samples, or when the distribution of a test statistic is unknown or mathematically intractable. In permutation tests, except exchangeability, no other statistical assumptions are required. The p-values can be obtained by using the permutation distribution. Permutation tests are appealing in many biomedical studies, which often have limited observations with unknown distribution. They have been used successfully in structural MR image analysis [1, 2, 3], in functional MR image analysis [4], and in 3D face analysis [5]. There are three common approaches to construct the permutation distribution [6, 7, 8]: (1) exact permutation enumerating all possible arrangements; (2) approximate permutation based on random sampling from all possible permutations; (3) approximate permutation using the analytical moments of the exact permutation distribution under the null hypothesis. The main disadvantage of the exact permutation is the computational cost, due to the factorial increase in the number of permutations with the increasing number of subjects. The second technique often gives inflated type I errors caused by random sampling. When a large number of repeated tests are needed, the random permutation strategy is also computationally expensive to achieve satisfactory accuracy. Regarding the third approach, the exact permutation distribution may not have moments or moments with tractability. In most applications, it is not the existence but the derivation of moments that limits the third approach. To the best of our knowledge, there is no systematic and efficient way to derive the moments of the permutation distribution. Recently, Zhou [3] proposed a solution by converting the permutation of data to that of the statistic coefficients that are symmetric to the permutation. Since the test statistic coefficients usually have simple presentations, it is easier to track the permutation of the test statistic coefficients than that of data. However, this method requires the derivation of the permutation for each specific test statistic, which is not accessible to practical users. In this paper, we propose a novel strategy by employing a general theoretical method to derive the moments of the permutation distribution of any weighted v-statistics, for both univariate and multivariate data. We note that any moments of the permutation distribution for weighted v-statistics [9] can be considered as a summation of the product of data function term and index function term over a high dimensional index set and all possible permutations. Our key idea is to divide the whole index set into several permutation equivalent (see Definition 2) index subsets such that the summation of the data/index function term over all permutations is invariant within each subset and can be calculated without conducting any permutation. Then we can obtain the moments by summing up several subtotals. The proposed method can be extended to equivalent weighted v-statistics by replacing them with monotonic weighted v-statistics. This is due to the fact that only the order of test statistics of all permutations matters for obtaining the p-values, so that the monotonic weighted v-statistics shares the same p-value with the original test statistic. Given the first four moments, the permutation distribution can be well fitted by Pearson distribution series. The p-values are then obtained without conducting any real permutation. For multiple comparison of two-group difference, given the sample size n1 = 21 and n2 = 21, the number of tests m = 2,000, we need to conduct m×(n1 + n2 )!/n1 !/n2 ! 1.1×1015 permutations for the exact permutation test. Even for 20,000 random permutations per test, we still need m×20,000 4×107 permutations. Alternatively, our moments-based permutation method using Pearson distribution approximation only involves the calculation of the first four analytically-derived moments of exact permutation distributions to achieve high accuracy (see section 3). Instead of calculating test statistics in factorial scale with exact permutation, our moments-based permutation only requires computation of polynomial order. For example, the computational cost for univariate mean difference test statistic and modified multivariate Hotelling's T2 test statistics [8] are O(n) and O(n3 ), respectively, where n = n 1 + n2 . 2 M e t h o d o lo g y In this section, we shall mainly discuss how to calculate the moments of the permutation distribution for weighted v-statistics. For other test statistics, a possible solution is to replace them with their equivalent weighted v-statistics by monotonic transforms. The detailed discussion about equivalent test statistics can be found in [7, 8, 10]. 2.1 C o m p ut a t i o n a l c h a l l e n g e Let us first look at a toy example. Suppose we have a two-group univariate data x = ( x1 , L , xn1 , xn1 +1 ,L , xn1 + n2 ) , where the first n1 elements are in group A and the rest, n2 ,are in group B. For comparison of the two groups, the hypothesis is typically constructed as: H 0 : m A = mB vs. H a : m A ¹ m B , where m A , m B are the population means of the groups A n1 n i =1 i = n1 +1 and B, respectively. Define x A = å xi / n1 and xB = å xi / n2 as the sample means of two groups, where n=n1+n2. We choose the univariate group mean difference statistic, i.e., n T ( x) = x A - xB = å w(i ) xi , i =1 where the index function as the test w(i ) = 1/ n1 , if i Î {1, L , n1} and w(i ) = -1/ n2 , if i Î {n1 + 1, L, n} . Then the total number of all possible permutations of {1, L, n} is n!. To calculate the fourth moment of the permutation distribution, n n n n n 1 1 4 å ( å w(i ) xp (i ) ) = å å å å å w(i1 ) w(i2 ) w(i3 ) w(i4 )xp ( i1 ) xp ( i2 ) xp ( i3 ) xp ( i4 ) , n ! p ÎS n i =1 n ! p ÎS n i1 =1 i2 =1 i3 =1 i4 =1 where is the permutation operator and the symmetric group Sn [11] includes all distinct permutations. The above example shows that the moment calculation can be considered as a summation over all possible permutations and a large index set. It is noticeable that the computational challenge here is to go through the factorial level permutations and polynomial level indices. Ep (T 4 ( x ))= 2.2 P a r t i t i o n t h e i n de x s e t In this paper, we assume that the test statistic T can be expressed as a weighted v-statistic of n n i1 =1 id =1 degree d [9], that is, T ( x) = å L å w(i1 , L , id ) h( xi1 ,L , xid ) , where x = ( x1 , x 2 , L , x n ) T is a data with n observations, and w is a symmetric index function. h is a symmetric data function, i.e., invariant under permutation of (i1 ,L , id ) . Though the symmetry property is not required for our method, it helps reduce the computational cost. Here, each observation xk can be either univariate or multivariate. In the above toy example, d=1 and h is the identity function. Therefore, the r-th moment of the test statistic from the permutated data is: Ep (T r ( x)) = Ep ( å w(i1 ,L , id )h( xp (i1 ) ,L , xp ( id ) ))r i1 ,i2 ,L,id = Ep [ å i1(1) ,L, id (1) , r r k =1 k =1 { Õ w(i1(k ) ,L, id ( k ) ) Õ h( xp (i ( k ) ) ,L, xp (i ( k ) ) )}] . L i1( r ) ,L,id ( r ) d 1 Then we can exchange the summation order of permutations and that of indices, Ep (T r ( x)) = å i1(1) ,L, id (1) , L i1( r ) ,L,id ( r ) r r k =1 k =1 {( Õ w(i1( k ) ,L , id ( k ) )) Ep ( Õ h( xp (i ( k ) ) ,L, xp (i ( k ) ) ))}. d 1 Thus any moment of permutation distribution can be considered as a summation of the product of data function term and index function term over a high dimensional index set and all possible permutations. Since all possible permutations map any index value between 1 and n to all possible index r values from 1 to n with equal probability, Ep ( Õ h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) , the summation of k =1 1 d data function over all permutations is only related to the equal/unequal relationship among indices. It is natural to divide the whole index set U = {i1 ,L , id }r = {(i1(1) , L , id (1) ), L (r ) , ( i1 , L , id (r ) r )} into the union of disjoint index subsets, in which Ep ( Õ h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) k =1 1 d is invariant. Definition 1. Since h is a symmetric function, two index elements (i1 ,L , id ) and ( j1 ,L , jd ) are said to be equivalent if they are the same up to the order. For example, for d = 3, (1, 4, 5) = (1,5,4) = (4,1,5) = (4,5,1) = (5,1,4) = (5,4,1). Definition 2. Two indices {(i1(1) , L , id (1) ), L , (i1( r ) , L , id ( r ) )} and {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} are said to be permutation equivalent/ if there exists a permutation p Î Sn such that {(p (i1(1) ), L , p (id (1) )), L , (p (i1( r ) ), L , p (id ( r ) ))} = {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} . Here</p><p>3 0.54336053 <a title="206-lsi-3" href="./nips-2009-A_Data-Driven_Approach_to_Modeling_Choice.html">7 nips-2009-A Data-Driven Approach to Modeling Choice</a></p>
<p>Author: Vivek Farias, Srikanth Jagabathula, Devavrat Shah</p><p>Abstract: We visit the following fundamental problem: For a ‘generic’ model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from oﬀering a particular assortment of choices? This problem is central to areas within operations research, marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same. 1</p><p>4 0.52410352 <a title="206-lsi-4" href="./nips-2009-Statistical_Consistency_of_Top-k_Ranking.html">230 nips-2009-Statistical Consistency of Top-k Ranking</a></p>
<p>Author: Fen Xia, Tie-yan Liu, Hang Li</p><p>Abstract: This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we deﬁne a top-k ranking framework, where the true loss (and thus the risks) are deﬁned on the basis of top-k subgroup of permutations. This framework can include the permutationlevel ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufﬁcient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modiﬁcations, the methods can work signiﬁcantly better than their original versions. 1</p><p>5 0.47744063 <a title="206-lsi-5" href="./nips-2009-Ranking_Measures_and_Loss_Functions_in_Learning_to_Rank.html">199 nips-2009-Ranking Measures and Loss Functions in Learning to Rank</a></p>
<p>Author: Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, Hang Li</p><p>Abstract: Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learningto-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measurebased ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classiﬁcation tasks, and deﬁne a so-called essential loss for ranking as the weighted sum of the classiﬁcation errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modiﬁcations can lead to better ranking performances, demonstrating the correctness of our theoretical analysis. 1</p><p>6 0.44927832 <a title="206-lsi-6" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>7 0.38921249 <a title="206-lsi-7" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>8 0.38080674 <a title="206-lsi-8" href="./nips-2009-Learning_to_Rank_by_Optimizing_NDCG_Measure.html">136 nips-2009-Learning to Rank by Optimizing NDCG Measure</a></p>
<p>9 0.37684694 <a title="206-lsi-9" href="./nips-2009-A_Fast%2C_Consistent_Kernel_Two-Sample_Test.html">8 nips-2009-A Fast, Consistent Kernel Two-Sample Test</a></p>
<p>10 0.36724317 <a title="206-lsi-10" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>11 0.3527211 <a title="206-lsi-11" href="./nips-2009-Toward_Provably_Correct_Feature_Selection_in_Arbitrary_Domains.html">248 nips-2009-Toward Provably Correct Feature Selection in Arbitrary Domains</a></p>
<p>12 0.34799641 <a title="206-lsi-12" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>13 0.34050742 <a title="206-lsi-13" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>14 0.33464345 <a title="206-lsi-14" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>15 0.3315616 <a title="206-lsi-15" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>16 0.32581934 <a title="206-lsi-16" href="./nips-2009-The_Wisdom_of_Crowds_in_the_Recollection_of_Order_Information.html">244 nips-2009-The Wisdom of Crowds in the Recollection of Order Information</a></p>
<p>17 0.32431442 <a title="206-lsi-17" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>18 0.31879911 <a title="206-lsi-18" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>19 0.30922773 <a title="206-lsi-19" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>20 0.30259329 <a title="206-lsi-20" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.035), (25, 0.045), (35, 0.039), (36, 0.094), (39, 0.038), (58, 0.06), (61, 0.015), (71, 0.081), (81, 0.015), (86, 0.049), (91, 0.017), (98, 0.356)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75287712 <a title="206-lda-1" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>2 0.62693983 <a title="206-lda-2" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>3 0.60132229 <a title="206-lda-3" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>Author: Daniel Cavagnaro, Jay Myung, Mark A. Pitt</p><p>Abstract: In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efﬁciently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to ﬁnd designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1</p><p>4 0.53251231 <a title="206-lda-4" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>5 0.41542572 <a title="206-lda-5" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>Author: Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, Tom M. Mitchell</p><p>Abstract: We consider the problem of zero-shot learning, where the goal is to learn a classiﬁer f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we deﬁne the notion of a semantic output code classiﬁer (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classiﬁer and study its theoretical properties in a PAC framework, showing conditions under which the classiﬁer can accurately predict novel classes. As a case study, we build a SOC classiﬁer for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words. 1</p><p>6 0.41467521 <a title="206-lda-6" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>7 0.41108739 <a title="206-lda-7" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>8 0.41060063 <a title="206-lda-8" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>9 0.41046196 <a title="206-lda-9" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>10 0.41028622 <a title="206-lda-10" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>11 0.40979362 <a title="206-lda-11" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>12 0.40944576 <a title="206-lda-12" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>13 0.4087567 <a title="206-lda-13" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>14 0.40834525 <a title="206-lda-14" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>15 0.40781885 <a title="206-lda-15" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>16 0.40772477 <a title="206-lda-16" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>17 0.40768334 <a title="206-lda-17" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>18 0.40582451 <a title="206-lda-18" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>19 0.4049266 <a title="206-lda-19" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>20 0.40473726 <a title="206-lda-20" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
