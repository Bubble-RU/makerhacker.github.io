<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-203" href="#">nips2009-203</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</h1>
<br/><p>Source: <a title="nips-2009-203-pdf" href="http://papers.nips.cc/paper/3672-replacing-supervised-classification-learning-by-slow-feature-analysis-in-spiking-neural-networks.pdf">pdf</a></p><p>Author: Stefan Klampfl, Wolfgang Maass</p><p>Abstract: It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal ﬁring patterns of presynaptic neurons. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classiﬁcation capability of Fisher’s Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating ﬁring patterns within a stream of spike trains with the same ﬁring statistics, as well as discrimination of spoken digits, in an unsupervised manner.</p><p>Reference: <a title="nips-2009-203-reference" href="../nips2009_reference/nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at  Abstract It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal ﬁring patterns of presynaptic neurons. [sent-3, score-0.116]
</p><p>2 We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classiﬁcation capability of Fisher’s Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. [sent-4, score-0.143]
</p><p>3 We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating ﬁring patterns within a stream of spike trains with the same ﬁring statistics, as well as discrimination of spoken digits, in an unsupervised manner. [sent-5, score-0.478]
</p><p>4 More precisely, we show that SFA approximates the classiﬁcation capability of FLD by replacing the supervisor with the simple heuristics that two temporally adjacent samples in the input time series are likely to be from the same class. [sent-10, score-0.203]
</p><p>5 Furthermore, we demonstrate in simulations of a cortical microcircuit model that SFA could also be an important ingredient in extracting temporally stable information from trajectories of network states and that it supports the idea of “anytime” computing, i. [sent-11, score-0.462]
</p><p>6 , it provides information about the stimulus identity not only at the end of a trajectory of network states, but already much earlier. [sent-13, score-0.138]
</p><p>7 We discuss the relationship between these methods for unsupervised and supervised learning in section 3, and investigate the application of SFA to trajectories in section 4. [sent-16, score-0.305]
</p><p>8 In section 5 we report results of computer simulations of several SFA readouts of a cortical microcircuit model. [sent-17, score-0.26]
</p><p>9 If multiple slow features are extracted, a third constraint ( yi yj t = 0, ∀j < i) ensures that they are decorrelated and ordered by decreasing slowness, i. [sent-24, score-0.136]
</p><p>10 , y1 is the slowest feature extracted, y2 the second slowest feature, and so on. [sent-26, score-0.473]
</p><p>11 In other words, SFA ﬁnds those functions gi out of a certain predeﬁned function space that produce the slowest possible outputs yi = gi (x) under these constraints. [sent-27, score-0.246]
</p><p>12 This optimization problem is hard to solve in the general case [4], but if we assume that the time series x has zero mean ( x t = 0) and if we only allow linear functions y = wT x the problem simpliﬁes to the objective ˙˙ wT xxT t w min JSF A (w) := T . [sent-28, score-0.126]
</p><p>13 (2) w xxT t w ˙˙ The matrix xxT t is the covariance matrix of the input time series and xxT t denotes the covariance matrix of time derivatives (or time differences, for discrete time) of the input time series. [sent-29, score-0.322]
</p><p>14 The weight vector w which minimizes (2) is the solution to the generalized eigenvalue problem ˙˙ xxT t w = λ xxT t w (3) corresponding to the smallest eigenvalue λ. [sent-30, score-0.117]
</p><p>15 FLD searches for that projection direction w which maximizes the separation between classes while at the same time minimizing the variance within classes, thereby minimizing the class overlap of the projected values: wT SB w max JF LD (w) := T . [sent-38, score-0.118]
</p><p>16 3 SFA can acquire the classiﬁcation capability of FLD SFA and FLD receive different data types as inputs: unlabeled time series for SFA, in contrast to labeled single data points for the FLD. [sent-41, score-0.182]
</p><p>17 Therefore, in order to apply the unsupervised SFA learning algorithm to the same classiﬁcation problem as the supervised FLD, we have to convert the labeled training samples into a time series of unlabeled data points that can serve as an input to the SFA algorithm1. [sent-42, score-0.234]
</p><p>18 In the following we investigate the relationship between the weight vectors found by both methods for a particular way of time series generation. [sent-43, score-0.168]
</p><p>19 We take a different approach and apply the standard SFA algorithm to a time series consisting of randomly selected patterns of the classiﬁcation problem, where the class at each time step is switched with a certain probability. [sent-49, score-0.215]
</p><p>20 In order to create a time series xt out of these point sets we deﬁne a Markov model with C states S = {1, 2, . [sent-57, score-0.144]
</p><p>21 In this case the time series consists mainly of transitions within a class, whereas switching between the two classes is relatively rare. [sent-74, score-0.211]
</p><p>22 Thus, the subspace spanned by the i slowest features is the same that optimizes separability in terms of Fisher’s Discriminant, and the slowest feature is the weight vector which achieves maximal separation. [sent-88, score-0.537]
</p><p>23 We interpret the weight vectors found by both methods as normal vectors of hyperplanes in the input space, which we place simply onto the mean value µ of all training data points (i. [sent-91, score-0.134]
</p><p>24 One sees that the weight vector found by the application of SFA to the training time series xt generated with p = 0. [sent-94, score-0.164]
</p><p>25 This demonstrates that SFA has extracted the class of the points as the slowest varying feature by ﬁnding a direction that separates both classes. [sent-96, score-0.337]
</p><p>26 The black solid line shows a hyperplane for the weight vector wSF A resulting from SFA applied to the time series generated from these training points as described in the text (T = 5000, p = 0. [sent-100, score-0.237]
</p><p>27 The dotted line displays an additional SFA hyperplane resulting from a time series generated with p = 0. [sent-102, score-0.152]
</p><p>28 Figure 1B quantiﬁes the deviation of the weight vector resulting from the application of SFA to the time series from the one found by FLD on the original points. [sent-108, score-0.143]
</p><p>29 , transitions between classes are rare compared to transitions within a class, the angle between the vectors is small and SFA approximates FLD very well. [sent-112, score-0.144]
</p><p>30 45) the approximation is reasonable and a good classiﬁcation by the slowest feature can be achieved (see dotted hyperplane in Figure 1A). [sent-114, score-0.309]
</p><p>31 4 Application to trajectories of training examples In the previous section we have shown that SFA approximates the classiﬁcation capability of FLD if the probability is low that two successive points in the input time series to SFA are from different classes. [sent-120, score-0.444]
</p><p>32 Apart from this temporal structure induced by the class information, however, these samples are chosen independently at each time step. [sent-121, score-0.114]
</p><p>33 In this section we investigate how the SFA objective changes when the input time series consists of a sequence of trajectories of samples instead of individual points only. [sent-122, score-0.42]
</p><p>34 First, we consider a time series xt consisting of multiple repetitions of a ﬁxed predeﬁned trajectory ˜ which is embedded into noise input consisting of a random number of points drawn from the t, same distribution as the trajectory points, but independently at each time step. [sent-123, score-0.43]
</p><p>35 It is easy to show [6] that for such a time series the SFA objective (2) reduces to ﬁnding the eigenvector of the matrix ˜ ˜ Σt corresponding to the largest eigenvalue. [sent-124, score-0.126]
</p><p>36 Σt is the covariance matrix of the trajectory ˜ with ˜ t t delayed by one time step, i. [sent-125, score-0.131]
</p><p>37 Since the transitions between two successive points of the trajectory ˜ occur much more often t in the time series xt than transitions between any other possible pair of points, SFA has to respond as smoothly as possible (i. [sent-128, score-0.319]
</p><p>38 , maximize the temporal correlations) during ˜ in order to produce the t 4  slowest possible output. [sent-130, score-0.27]
</p><p>39 We generate a time series according to the same Markov model as described in the previous section, except that we do not choose individual points at each time step, rather we generate a sequence of trajectories. [sent-138, score-0.188]
</p><p>40 ˙˙ For this time series we can express the matrices xxT t and xxT t in terms of the within-class and between-class scatter of the individual points of the trajectories in Tc , analogously to (7) and (8) [6]. [sent-139, score-0.387]
</p><p>41 While the expression for xxT t is unchanged the temporal correlations induced by the ˙˙ use of trajectories however have an effect on the covariance of temporal differences xxT t . [sent-140, score-0.368]
</p><p>42 First, ˜ this matrix additionally depends on the temporal covariance Σt with time lag 1 of all available ˜ trajectories in all sets Tc . [sent-141, score-0.326]
</p><p>43 The weight vector w which minimizes the ﬁrst term in (11) is equal to the weight vector found by the application of FLD to the classiﬁcation problem of the individual trajectory points (note that SB enters (11) through xxT t , cf. [sent-145, score-0.208]
</p><p>44 The weight vector which maximizes the second term is the one which produces the slowest possible response during individual trajectories. [sent-148, score-0.361]
</p><p>45 If the separation between the trajectory classes is large compared to the temporal correlations (i. [sent-149, score-0.202]
</p><p>46 , the ﬁrst term in (11) dominates for the resulting w) the slowest feature will be similar to the weight vector found by FLD on the corresponding classiﬁcation problem. [sent-151, score-0.3]
</p><p>47 On the other hand, as the temporal correlations of the trajectories increase, i. [sent-152, score-0.285]
</p><p>48 , the trajectories themselves become smoother, the slowest feature will tend to favor exploiting this temporal structure of the trajectories over the separation of different classes (in this case, (11) is dominated by the second term for the resulting w). [sent-154, score-0.787]
</p><p>49 5 Application to linear readouts of a cortical microcircuit model In the following we discuss several computer simulations of a cortical microcircuit of spiking neurons that demonstrate the theoretical arguments given in the previous section. [sent-155, score-0.489]
</p><p>50 We trained a number of linear SFA readouts3 on a sequence of trajectories of network states, each of which is deﬁned by the low-pass ﬁltered spike trains of the neurons in the circuit. [sent-156, score-0.495]
</p><p>51 Such recurrent circuits typically provide a temporal integration of the input stream and project it nonlinearly into a high-dimensional space [7], thereby boosting the expressive power of the subsequent linear SFA readouts. [sent-157, score-0.165]
</p><p>52 As a model for a cortical microcircuit model we use the laminar circuit from [8] consisting of 560 spiking neurons organized into layers 2/3, 4, and 5, with layer-speciﬁc connection probabilities obtained from experimental data [9, 10]. [sent-159, score-0.364]
</p><p>53 We recorded circuit trajectories in response to 200 repetitions of a ﬁxed spike pattern which are embedded into a continuous Poisson input stream of the same rate. [sent-161, score-0.643]
</p><p>54 We then trained linear SFA readouts on this sequence of circuit trajectories (we used an exponential 2 In fact, for sufﬁciently long trajectories the SFA objective becomes effectively independent of the switching probability. [sent-162, score-0.643]
</p><p>55 3 We interpret the linear combination deﬁned by each slow feature as the weight vector of a hypothetical linear readout. [sent-163, score-0.169]
</p><p>56 (A) From top to bottom: sample stimulus sequence, response spike trains of the network, and slowest features. [sent-165, score-0.538]
</p><p>57 The stimulus consists of 10 channels and is deﬁned by repetitions of a ﬁxed spike pattern (dark gray) which are embedded into random Poisson input of the same rate. [sent-166, score-0.295]
</p><p>58 The pattern has a length of 250ms and is made up by Poisson spike trains of rate 20Hz. [sent-167, score-0.232]
</p><p>59 The response spike trains of the laminar circuit of [8] are shown separated into layers 2/3, 4, and 5. [sent-169, score-0.418]
</p><p>60 The numbers of neurons in the layers are indicated on the left, but only the response of every 12th neuron is plotted. [sent-170, score-0.163]
</p><p>61 Shown are the 5 slowest features, y1 to y5 , for the network response shown above. [sent-171, score-0.327]
</p><p>62 (B) Phase plots of low-pass ﬁltered versions (leaky integration, τ = 100ms) of individual slow features in response to a test sequence of 50 embedded patterns plotted against each other (black: traces during the pattern, gray: during random Poisson input). [sent-173, score-0.33]
</p><p>63 However, we found that on average the slow feature responses during noise input are zero, whereas a characteristic response remains during pattern presentations. [sent-177, score-0.301]
</p><p>64 That is, by simple threshold operations on the low-pass ﬁltered versions of the slowest features one can in principle detect the presence of patterns within the continuous input stream. [sent-180, score-0.323]
</p><p>65 In the second experiment we tested whether SFA is able to discriminate two classes of trajectories as described in section 4. [sent-182, score-0.267]
</p><p>66 We preprocessed the raw audio ﬁles with a model of the cochlea [14] and converted the resulting analog cochleagrams into 20 spike trains (using the algorithm in [15]) that serve as input to our microcircuit model (see Figure 3A). [sent-188, score-0.373]
</p><p>67 (A) From top to bottom: cochleagrams, input spike trains, response spike trains of the network, and traces of different linear readouts. [sent-190, score-0.466]
</p><p>68 Stimulus spike trains are shown for two different utterances of the given digit (black and gray, the black spike times correspond to the cochleagram shown above). [sent-192, score-0.523]
</p><p>69 The response spike trains of the laminar circuit from [8] are shown separated into layers 2/3, 4, and 5. [sent-193, score-0.418]
</p><p>70 The number of neurons in each layer is indicated on the left, but only the response of every 12th neuron is plotted. [sent-194, score-0.132]
</p><p>71 The responses to the two stimulus spike trains in the panel above are shown superimposed with the corresponding color. [sent-195, score-0.277]
</p><p>72 Each readout trace corresponds to a weighted sum (Σ) of network states of the black responses in the panel above. [sent-196, score-0.157]
</p><p>73 The trace of the slowest feature (“SF1”, see B) is compared to traces of readouts trained by FLD and SVM with linear kernel to discriminate at any time between the network states of the two classes. [sent-197, score-0.49]
</p><p>74 (B) Response of the 5 slowest features y1 to y5 of the previously learned SFA in response to trajectories of the three test utterances of each class not used for training (black, digit “one”; gray, digit “two”). [sent-200, score-0.757]
</p><p>75 The thick curves in the shaded area display the mean SFA responses over all three test trajectories for each class. [sent-203, score-0.244]
</p><p>76 (C) Phase plots of individual slow features plotted against each other (thin lines: individual responses, thick lines: mean response over all test trajectories). [sent-204, score-0.251]
</p><p>77 criminate between trajectories in response to inputs corresponding to utterances of digits “one” and “two”, of a single speaker. [sent-205, score-0.391]
</p><p>78 We kept three utterances of each digit for testing and generated from the remaining training samples a sequence of 100 input samples, recorded for each sample the response of the circuit, and concatenated the resulting trajectories in time. [sent-206, score-0.462]
</p><p>79 Note that here we did not switch the classes of two successive trajectories with a certain probability because, as explained in the previous section, for long trajectories the SFA response is independent of this switching probability. [sent-207, score-0.593]
</p><p>80 Rather, we trained linear SFA readouts on a completely random trajectory sequence. [sent-208, score-0.151]
</p><p>81 7  Figure 3B shows the 5 slowest features, y1 to y5 , ordered by decreasing slowness in response to the trajectories corresponding to the three remaining test utterances for each class, digit “one” and digit “two”. [sent-209, score-0.754]
</p><p>82 In this example, already the slowest feature y1 extracts the class of the input patterns almost perfectly: it responds with positive values for trajectories in response to utterances of digit “two” and with negative values for utterances of digit “one”. [sent-210, score-0.981]
</p><p>83 The second slowest feature y2 , on the other hand, responds with shapes whose sign is independent of the pattern identity. [sent-212, score-0.316]
</p><p>84 , its quality as a possible classiﬁer, we measured the angle between the projection direction corresponding to this slow feature and the direction of the FLD. [sent-220, score-0.166]
</p><p>85 It can be seen in Figure 3B that the slowest feature y1 is closest to the FLD. [sent-221, score-0.257]
</p><p>86 Hence, according to (11), this constitutes an example where the separation between classes dominates, but is already signiﬁcantly inﬂuenced by the temporal correlations of the circuit trajectories. [sent-222, score-0.205]
</p><p>87 Figure 3C shows phase plots of these slow features shown in Figure 3B plotted against each other. [sent-223, score-0.152]
</p><p>88 In the three plots involving feature y1 it can be seen that the directions of the response vector (i. [sent-224, score-0.142]
</p><p>89 , the vector composed of the slow feature values at a particular point in time) cluster at class-speciﬁc angles, which is characteristic for What-information. [sent-226, score-0.126]
</p><p>90 Similar responses have been theoretically predicted in [4] and found in simulations of a hierarchical (nonlinear) SFA network trained with a sequence of one-dimensional trajectories [2]. [sent-229, score-0.297]
</p><p>91 This experiment demonstrates that SFA extracts information about the spoken digit in an unsupervised manner by projecting the circuit trajectories onto a subspace where they are nicely separable so that they can easily be classiﬁed by later processing stages. [sent-230, score-0.453]
</p><p>92 After sufﬁcient training, the slowest feature y1 in Figure 3B responds with positive or negative values indicating the stimulus class almost during the whole duration of of the network trajectory. [sent-232, score-0.383]
</p><p>93 It can be seen in the bottom panel of Figure 3A that the slowest feature, which is obtained in an unsupervised manner, achieves a good separation between the two test trajectories, comparable to the supervised methods of FLD and Support Vector Machine (SVM) [16] with linear kernel. [sent-234, score-0.317]
</p><p>94 If temporal contiguous points in the time series are likely to belong to the same class, SFA is able to extract the class as a slowly varying feature in an unsupervised manner. [sent-238, score-0.291]
</p><p>95 This ability is of particular interest in the context of biologically realistic neural circuits because it could enable readout neurons to extract from the trajectories of network states information about the stimulus – without any “teacher”, whose existence is highly dubious in the brain. [sent-239, score-0.424]
</p><p>96 We have shown in computer simulations of a cortical microcircuit model that linear readouts trained with SFA are able to detect speciﬁc spike patterns within a stream of spike trains with the same ﬁring statistics and to discriminate between different spoken digits. [sent-240, score-0.737]
</p><p>97 A theoretical basis for emergent pattern discrimination in neural systems through slow feature extraction. [sent-274, score-0.155]
</p><p>98 A statistical analysis of information processing properties of lamina-speciﬁc a cortical microcircuit models. [sent-285, score-0.155]
</p><p>99 Synaptic connections and small circuits involving excitatory and inhibitory neurons in layers 2–5 of adult rat and cat neocortex: triple intracellular recordings and biocytin labelling in vitro. [sent-301, score-0.115]
</p><p>100 BSA, a fast and accurate spike train encoding scheme. [sent-343, score-0.12]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sfa', 0.679), ('fld', 0.36), ('xxt', 0.285), ('slowest', 0.216), ('trajectories', 0.209), ('sb', 0.125), ('spike', 0.12), ('sw', 0.117), ('microcircuit', 0.111), ('slow', 0.085), ('trains', 0.083), ('readouts', 0.083), ('response', 0.08), ('utterances', 0.079), ('circuit', 0.071), ('wt', 0.069), ('trajectory', 0.068), ('series', 0.066), ('digit', 0.063), ('jsf', 0.055), ('temporal', 0.054), ('neurons', 0.052), ('capability', 0.05), ('switching', 0.045), ('ring', 0.045), ('slowness', 0.044), ('cortical', 0.044), ('weight', 0.043), ('tc', 0.042), ('spoken', 0.042), ('feature', 0.041), ('repetitions', 0.041), ('angle', 0.04), ('stimulus', 0.039), ('ld', 0.039), ('nc', 0.039), ('unsupervised', 0.038), ('transitions', 0.038), ('sc', 0.038), ('readout', 0.038), ('anytime', 0.038), ('eigenvalue', 0.037), ('discriminant', 0.036), ('embedded', 0.035), ('responses', 0.035), ('patterns', 0.034), ('fisher', 0.034), ('time', 0.034), ('laminar', 0.033), ('supervised', 0.033), ('traces', 0.032), ('circuits', 0.032), ('points', 0.032), ('hyperplane', 0.032), ('input', 0.031), ('layers', 0.031), ('network', 0.031), ('yi', 0.03), ('classi', 0.03), ('black', 0.03), ('discriminate', 0.03), ('separation', 0.03), ('extracts', 0.03), ('responds', 0.03), ('covariance', 0.029), ('pattern', 0.029), ('poisson', 0.028), ('hyperplanes', 0.028), ('classes', 0.028), ('cochleagram', 0.028), ('cochleagrams', 0.028), ('klamp', 0.028), ('schrauwen', 0.028), ('stream', 0.027), ('objective', 0.026), ('class', 0.026), ('relationship', 0.025), ('phase', 0.025), ('jf', 0.024), ('scatter', 0.024), ('digits', 0.023), ('states', 0.023), ('correlations', 0.022), ('spiking', 0.022), ('ltered', 0.022), ('temporally', 0.022), ('wiskott', 0.022), ('extracted', 0.022), ('individual', 0.022), ('simulations', 0.022), ('successive', 0.022), ('detect', 0.021), ('gray', 0.021), ('plots', 0.021), ('project', 0.021), ('maass', 0.021), ('switched', 0.021), ('features', 0.021), ('xt', 0.021), ('dotted', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="203-tfidf-1" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>Author: Stefan Klampfl, Wolfgang Maass</p><p>Abstract: It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal ﬁring patterns of presynaptic neurons. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classiﬁcation capability of Fisher’s Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating ﬁring patterns within a stream of spike trains with the same ﬁring statistics, as well as discrimination of spoken digits, in an unsupervised manner.</p><p>2 0.11468047 <a title="203-tfidf-2" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><p>3 0.097662888 <a title="203-tfidf-3" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>4 0.083100289 <a title="203-tfidf-4" href="./nips-2009-Optimal_context_separation_of_spiking_haptic_signals_by_second-order_somatosensory_neurons.html">183 nips-2009-Optimal context separation of spiking haptic signals by second-order somatosensory neurons</a></p>
<p>Author: Romain Brasselet, Roland Johansson, Angelo Arleo</p><p>Abstract: We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve ﬁbers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efﬁciency of the haptic discrimination process is quantiﬁed by a novel deﬁnition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the ﬁrst afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system. 1</p><p>5 0.069439292 <a title="203-tfidf-5" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>Author: Theodore J. Perkins</p><p>Abstract: Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random. Many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control. However, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved. We study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given ﬁnal time, (ii) a boundary value problem, in which initial and ﬁnal states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to ﬁnding maximum likelihood trajectories for hidden Markov models. We show that maximum likelihood trajectories are not always well-deﬁned, and describe a polynomial time test for well-deﬁnedness. When well-deﬁnedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efﬁcient dynamic programming algorithms for doing so. 1</p><p>6 0.06497971 <a title="203-tfidf-6" href="./nips-2009-Reconstruction_of_Sparse_Circuits_Using_Multi-neuronal_Excitation_%28RESCUME%29.html">200 nips-2009-Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></p>
<p>7 0.064176321 <a title="203-tfidf-7" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>8 0.062904157 <a title="203-tfidf-8" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>9 0.060705908 <a title="203-tfidf-9" href="./nips-2009-Manifold_Regularization_for_SIR_with_Rate_Root-n_Convergence.html">146 nips-2009-Manifold Regularization for SIR with Rate Root-n Convergence</a></p>
<p>10 0.059999071 <a title="203-tfidf-10" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>11 0.059816904 <a title="203-tfidf-11" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>12 0.05890815 <a title="203-tfidf-12" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>13 0.055342838 <a title="203-tfidf-13" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>14 0.054686222 <a title="203-tfidf-14" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>15 0.05439724 <a title="203-tfidf-15" href="./nips-2009-Efficient_Learning_using_Forward-Backward_Splitting.html">76 nips-2009-Efficient Learning using Forward-Backward Splitting</a></p>
<p>16 0.048631437 <a title="203-tfidf-16" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>17 0.048261855 <a title="203-tfidf-17" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>18 0.046694998 <a title="203-tfidf-18" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>19 0.046173953 <a title="203-tfidf-19" href="./nips-2009-Know_Thy_Neighbour%3A_A_Normative_Theory_of_Synaptic_Depression.html">121 nips-2009-Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></p>
<p>20 0.045444038 <a title="203-tfidf-20" href="./nips-2009-Time-Varying_Dynamic_Bayesian_Networks.html">246 nips-2009-Time-Varying Dynamic Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, -0.088), (2, 0.123), (3, 0.075), (4, 0.028), (5, -0.001), (6, -0.041), (7, 0.041), (8, 0.0), (9, 0.013), (10, 0.002), (11, 0.022), (12, -0.029), (13, -0.037), (14, -0.031), (15, -0.025), (16, 0.051), (17, -0.034), (18, -0.047), (19, 0.008), (20, 0.043), (21, -0.038), (22, -0.058), (23, -0.017), (24, -0.052), (25, 0.052), (26, 0.023), (27, 0.039), (28, 0.088), (29, -0.076), (30, 0.024), (31, 0.002), (32, 0.023), (33, -0.061), (34, 0.028), (35, 0.027), (36, -0.005), (37, 0.047), (38, -0.039), (39, -0.026), (40, -0.041), (41, -0.016), (42, -0.007), (43, 0.091), (44, -0.04), (45, 0.071), (46, 0.002), (47, 0.042), (48, 0.025), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89449036 <a title="203-lsi-1" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>Author: Stefan Klampfl, Wolfgang Maass</p><p>Abstract: It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal ﬁring patterns of presynaptic neurons. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classiﬁcation capability of Fisher’s Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating ﬁring patterns within a stream of spike trains with the same ﬁring statistics, as well as discrimination of spoken digits, in an unsupervised manner.</p><p>2 0.71494478 <a title="203-lsi-2" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>Author: Jonathan W. Pillow</p><p>Abstract: Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a conditional renewal (CR) model for neural spike trains. This model captures both real-time and rescaled-time history effects, and can be ﬁt by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear ﬁlter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with shape κ = 1), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-ﬁt tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data. 1</p><p>3 0.65351582 <a title="203-lsi-3" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>4 0.63987142 <a title="203-lsi-4" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><p>5 0.63005269 <a title="203-lsi-5" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>6 0.55618107 <a title="203-lsi-6" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>7 0.53672671 <a title="203-lsi-7" href="./nips-2009-Optimal_context_separation_of_spiking_haptic_signals_by_second-order_somatosensory_neurons.html">183 nips-2009-Optimal context separation of spiking haptic signals by second-order somatosensory neurons</a></p>
<p>8 0.52735174 <a title="203-lsi-8" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>9 0.51548171 <a title="203-lsi-9" href="./nips-2009-Know_Thy_Neighbour%3A_A_Normative_Theory_of_Synaptic_Depression.html">121 nips-2009-Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></p>
<p>10 0.49498913 <a title="203-lsi-10" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>11 0.48200977 <a title="203-lsi-11" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>12 0.44816425 <a title="203-lsi-12" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>13 0.44574222 <a title="203-lsi-13" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>14 0.43583015 <a title="203-lsi-14" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>15 0.4338617 <a title="203-lsi-15" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>16 0.41483486 <a title="203-lsi-16" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>17 0.41481867 <a title="203-lsi-17" href="./nips-2009-Reconstruction_of_Sparse_Circuits_Using_Multi-neuronal_Excitation_%28RESCUME%29.html">200 nips-2009-Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></p>
<p>18 0.41438729 <a title="203-lsi-18" href="./nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></p>
<p>19 0.40547815 <a title="203-lsi-19" href="./nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</a></p>
<p>20 0.40033355 <a title="203-lsi-20" href="./nips-2009-Clustering_sequence_sets_for_motif_discovery.html">51 nips-2009-Clustering sequence sets for motif discovery</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.016), (24, 0.034), (25, 0.061), (35, 0.056), (36, 0.068), (39, 0.038), (58, 0.064), (61, 0.027), (62, 0.011), (71, 0.058), (81, 0.055), (86, 0.114), (87, 0.251), (91, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79029197 <a title="203-lda-1" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>Author: Stefan Klampfl, Wolfgang Maass</p><p>Abstract: It is open how neurons in the brain are able to learn without supervision to discriminate between spatio-temporal ﬁring patterns of presynaptic neurons. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), is able to acquire the classiﬁcation capability of Fisher’s Linear Discriminant (FLD), a powerful algorithm for supervised learning, if temporally adjacent samples are likely to be from the same class. We also demonstrate that it enables linear readout neurons of cortical microcircuits to learn the detection of repeating ﬁring patterns within a stream of spike trains with the same ﬁring statistics, as well as discrimination of spoken digits, in an unsupervised manner.</p><p>2 0.59167749 <a title="203-lda-2" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or “causes”) of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these neurons will ﬁre after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artiﬁcial neural networks. 1</p><p>3 0.58835721 <a title="203-lda-3" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>Author: Steven Chase, Andrew Schwartz, Wolfgang Maass, Robert A. Legenstein</p><p>Abstract: The control of neuroprosthetic devices from the activity of motor cortex neurons beneﬁts from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is ﬁtted to experimental data, the model produces learning effects similar to those found in monkey experiments.</p><p>4 0.58558238 <a title="203-lda-4" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>5 0.57609874 <a title="203-lda-5" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>Author: Keith Bush, Joelle Pineau</p><p>Abstract: Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by ﬁrst principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning. We experiment with manifold embeddings to reconstruct the observable state-space in the context of offline, model-based reinforcement learning. We demonstrate that the embedding of a system can change as a result of learning, and we argue that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system. We apply this approach to learn a neurostimulation policy that suppresses epileptic seizures on animal brain slices. 1</p><p>6 0.5758639 <a title="203-lda-6" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>7 0.57502931 <a title="203-lda-7" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>8 0.57356048 <a title="203-lda-8" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>9 0.5734539 <a title="203-lda-9" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>10 0.57303005 <a title="203-lda-10" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>11 0.57299095 <a title="203-lda-11" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>12 0.57292682 <a title="203-lda-12" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>13 0.57027096 <a title="203-lda-13" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>14 0.56953537 <a title="203-lda-14" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>15 0.5694406 <a title="203-lda-15" href="./nips-2009-Locality-sensitive_binary_codes_from_shift-invariant_kernels.html">142 nips-2009-Locality-sensitive binary codes from shift-invariant kernels</a></p>
<p>16 0.5676136 <a title="203-lda-16" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>17 0.56645477 <a title="203-lda-17" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>18 0.56644112 <a title="203-lda-18" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>19 0.56465501 <a title="203-lda-19" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>20 0.56313181 <a title="203-lda-20" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
