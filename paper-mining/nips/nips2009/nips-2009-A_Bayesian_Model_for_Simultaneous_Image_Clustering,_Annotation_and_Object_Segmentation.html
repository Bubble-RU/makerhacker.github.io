<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-5" href="#">nips2009-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</h1>
<br/><p>Source: <a title="nips-2009-5-pdf" href="http://papers.nips.cc/paper/3655-a-bayesian-model-for-simultaneous-image-clustering-annotation-and-object-segmentation.pdf">pdf</a></p><p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>Reference: <a title="nips-2009-5-reference" href="../nips2009_reference/nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The analysis employs image features and, when present, the words associated with accompanying annotations. [sent-7, score-0.352]
</p><p>2 The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). [sent-8, score-0.649]
</p><p>3 Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. [sent-9, score-0.572]
</p><p>4 The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. [sent-10, score-0.445]
</p><p>5 Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases. [sent-12, score-0.226]
</p><p>6 1 Introduction There has recently been much interest in developing statistical models for analyzing and organizing images, based on image features and, when available, auxiliary information, such as words (e. [sent-13, score-0.284]
</p><p>7 Three important aspects of this problem are: (i) sorting multiple images into scene-level classes, (ii) image annotation, and (iii) segmenting and labeling localized objects within images. [sent-16, score-0.56]
</p><p>8 Although good classiﬁcation performance was achieved using this approach, the model is employed in a supervised manner, utilizing scene-labeled images for scene classiﬁcation. [sent-24, score-0.262]
</p><p>9 Nevertheless, to improve performance, in [16] some images are required for supervised learning, based on the segmented and labeled objects obtained via the method proposed in [10], with these used to initialize the algorithm. [sent-26, score-0.393]
</p><p>10 The four main contributions of this paper are: • Each object in an image is represented as a mixture of image-feature model parameters, accounting for the heterogeneous character of individual objects. [sent-29, score-0.427]
</p><p>11 By contrast, each object is only associated with one image-feature component/atom in the Corr-LDA-like models [6, 16, 23]. [sent-31, score-0.207]
</p><p>12 • Multiple images are processed jointly; all, none or a subset of the images may be annotated. [sent-32, score-0.363]
</p><p>13 The model infers the linkage between image-feature parameters and object types, with this linkage used to yield localized labeling of objects within all images. [sent-33, score-0.509]
</p><p>14 • A novel logistic stick-breaking process (LSBP) is proposed, imposing the belief that proximate portions of an image are more likely to reside within the same segment (object). [sent-35, score-0.347]
</p><p>15 This spatially constrained prior yields contiguous objects with sharp boundaries, and via the aforementioned mixture models the segmented objects may be composed of heterogeneous building blocks. [sent-36, score-0.61]
</p><p>16 The number of image classes, number of object types, number of image-feature mixture components per object, and the linkage between words and image model parameters are inferred nonparametrically. [sent-38, score-0.807]
</p><p>17 1 Bag of image features We jointly process data from M images, and each image is assumed to come from an associated class type (e. [sent-40, score-0.464]
</p><p>18 The class type associated with image m is denoted by zm ∈ {1, . [sent-44, score-0.395]
</p><p>19 , I}, and it is drawn from the mixture model I  zm ∼  ui δi , u ∼ StickI (αu )  (1)  i=1  where StickI (αu ) is a stick-breaking process [13] that is truncated to I sticks, with hyper-parameter αu > 0. [sent-47, score-0.212]
</p><p>20 The symbol δi represents a unit measure at the integer i, and the parameter ui denotes the probability that image type i will be observed across the M images. [sent-48, score-0.214]
</p><p>21 The observed data are image feature vectors, each tied to a local region in the image (for example, associated with an over-segmented portion of the image). [sent-49, score-0.432]
</p><p>22 The Lm observed image feature vectors associated with image m are {xml }Lm , and the lth feature vector is assumed drawn xml ∼ F (θ ml ). [sent-50, score-0.714]
</p><p>23 Each image is assumed to be composed of a set of latent objects. [sent-52, score-0.234]
</p><p>24 An indicator variable ζml deﬁnes which object type the lth feature vector from image m is associated with, and it is drawn K  ζml ∼  wzm k δk , w i ∼ StickK (αw )  (2)  k=1  where index k corresponds to the kth type of object that may reside within an image. [sent-53, score-0.88]
</p><p>25 The vector wi deﬁnes the probability that each of the K object types will occur, conditioned on the image type i ∈ {1, . [sent-54, score-0.385]
</p><p>26 , I}; the kth component of w zm , wzm k , denotes the probability of observing object type k in image m, when image m was drawn from class zm ∈ {1, . [sent-57, score-0.915]
</p><p>27 The image class zm and corresponding objects {ζml }Lm associated with image m are latent varil=1 ables. [sent-61, score-0.736]
</p><p>28 Speciﬁcally, a separate such mixture model is manifested for each of the K object types, motivated by the idea that each object will in general be composed of a different set of image-feature building blocks. [sent-63, score-0.452]
</p><p>29 2 Bag of clustered image features While the model described above is straightforward to understand, it has been found to be ineffecK tive. [sent-69, score-0.209]
</p><p>30 from k=1 wzm k δk , and therefore there is 2  nothing in the model that encourages the image features, xml and xml′ , which are associated with the same image-feature atom θ∗ , to be assigned to the same object k. [sent-73, score-0.737]
</p><p>31 Speciﬁcally, consider the following augmented model: T  xml ∼ F (θ ml ) , θml ∼ Gcml , cml ∼  K  vmt δζmt , ζmt ∼ t=1  I  wzm k δk , zm ∼  u i δi  (4)  i=1  k=1  where v m ∼ StickT (αv ), and Gk is as deﬁned in (3). [sent-75, score-0.861]
</p><p>32 3 Linking words with images In the above discussion it was assumed that the only observed data are the image feature vectors {xml }Lm . [sent-78, score-0.448]
</p><p>33 In this setting we assume that we have a K-dimensional dictionary of words associated with objects in images, and a word is assigned to each of the objects k ∈ {1, . [sent-80, score-0.495]
</p><p>34 Of the collection of M images, some may be annotated and some not, and all will be processed simultaneously by the joint model; in so doing, annotations will be inferred for the originally non-annotated images. [sent-84, score-0.345]
</p><p>35 For an image for which no annotation is given, the image is assumed generated via (4). [sent-85, score-0.511]
</p><p>36 If image m is in class zm , then we simply set y m ∼ Mult(wzm ) , wi ∼ StickK (αw ) (5) Namely, ϕm = w zm , recalling that wi deﬁnes the probability of observing each object type for image class i. [sent-87, score-0.825]
</p><p>37 1 Logistic stick-breaking process (LSBP) In (5), note that once the image class zm is drawn for image m, the order/location of the xml within the image may be interchanged, and nothing in the generative process will change. [sent-93, score-0.873]
</p><p>38 This is because the indicator variable cml , which deﬁnes the object class associated with feature vector l in image m, T is drawn i. [sent-94, score-0.611]
</p><p>39 With each feature vector xml there is an associated spatial location, which we denote sml (this is a two-dimensional vector). [sent-99, score-0.399]
</p><p>40 We wish to draw T  cml ∼  K  vmt (sml )δζmt , t=1  ζmt ∼  wzm k δk  (6)  k=1  where the cluster probabilities vmt (sml ) are now a function of position sml (the ζmt ∈ {1, . [sent-100, score-0.897]
</p><p>41 The challenge, therefore, becomes development of a means of constructing vmt (s) to encourage nearby feature vectors to come from the same object type. [sent-104, score-0.379]
</p><p>42 , T − 1 we impose t−1 vmt (s) = σ[gmt (s)] {1 − σ[gmτ (s)]} (7) τ =1 T −1 t=1 vmt (s). [sent-109, score-0.48]
</p><p>43 L  (m)  (m)  m where vmT (s) = 1 − We deﬁne gmt (s) = l=1 Wtl K(s, sml ) + Wt0 where K(s, sml ) is a kernel, and here we utilize the radial basis function kernel K(s, sml ) = exp[− s − sml 2 /φmt ]. [sent-110, score-0.63]
</p><p>44 3  We desire that a given stick vmt (s) has importance (at most) over a localized region, and therefore (m) (m) (m) we impose sparseness priors on parameters {Wtl }Lm . [sent-113, score-0.316]
</p><p>45 For notational convenience, cml ∼ t=1 vmt (sml )δζmt and ζmt ∼ k=1 wzm k δk constructed as above is represented as a draw from LSBPT (wzm ). [sent-117, score-0.528]
</p><p>46 A particular non-zero Wtl is (via the kernel) associated with the lth local spatial region, with spatial extent deﬁned by φmt . [sent-126, score-0.188]
</p><p>47 All locations s for which (roughly) gmt (s) ≥ 4 will have – via the “clipping” manifested via the logistic – nearly the same high probability of being associated with model layer t. [sent-129, score-0.275]
</p><p>48 2 Processing images with no words given If one is given M images, all non-annotated, then the model may be employed on the data {xml }Lm , l=1 for m = 1, . [sent-140, score-0.293]
</p><p>49 , M , from which a posterior distribution is inferred on the image model parameters ∗ J {θj }j=1 , and on {Gk }K . [sent-143, score-0.32]
</p><p>50 Note that properties of the image classes and of the objects within k=1 images is inferred by processing all M images jointly. [sent-144, score-0.787]
</p><p>51 By placing all images within the context of each other, the model is able to infer which building blocks (classes and objects) are responsible for all of the data. [sent-145, score-0.304]
</p><p>52 In this sense the simultaneous processing of multiple images is critical: the learning of properties of objects in one image is aided by the properties being learned for objects in all other images, through the inference of inter-relationships and commonalities. [sent-146, score-0.624]
</p><p>53 After the M images are analyzed in the absence of annotations, one may observe example portions of the M images, to infer the link between actual object characteristics within imagery and the associated latent object indicator to which it was assigned. [sent-147, score-0.688]
</p><p>54 With this linkage made, one may assign words to all or a subset of the K object types. [sent-148, score-0.292]
</p><p>55 After words are assigned to previously latent object types, the results of the analysis (with no additional processing) may be used to automatically label regions (objects) in all of the images. [sent-149, score-0.293]
</p><p>56 This is manifested because each of the cluster indicators cml is associated with a latent localized object type (to which a word may now be assigned). [sent-150, score-0.596]
</p><p>57 3 Joint processing of images and annotations We may consider problems for which a subset of the images are provided with annotations (but not the explicit location and segmented-out objects); the words are assumed to reside in a prescribed dictionary of object types. [sent-152, score-0.939]
</p><p>58 The generation of the annotations (and images) is constituted via the model in (5), with the LSBP employed as discussed. [sent-153, score-0.229]
</p><p>59 We do not require that all images are annotated (the non-annotated images help learn the properties of the image features, and are therefore useful even if they do not provide information about the words). [sent-154, score-0.574]
</p><p>60 The presence of the same word within the annotations of multiple images encourages the model to infer what objects (represented in terms of image features) are common to the associated images, aiding the learning. [sent-156, score-0.891]
</p><p>61 Hence, the presence of annotations serves as a learning aid (encourages looking for commonalities between particular images, if words are shared in the associated annotations). [sent-157, score-0.338]
</p><p>62 Further, the annotations associated with images may disambiguate objects that appear similar in image-feature space (because they will have different annotations). [sent-158, score-0.539]
</p><p>63 From the above discussion, the model performance will improve as more images are annotated with each word, but presumably this annotation is much easier for the human than requiring one to segment out and localize words within a scene. [sent-159, score-0.559]
</p><p>64 For the MSRC dataset, 10 categories of images with manual annotations are selected: “tree”, “building”, “cow”, “face”, “car”, “sheep”, “ﬂower”, “sign”, “book” and “chair”. [sent-166, score-0.332]
</p><p>65 The number of images in the “cow” class is 45, and in the “sheep” class there are 35; there are 30 images in all other classes. [sent-167, score-0.328]
</p><p>66 From each category, we randomly choose 10 images, and remove the annotations, treating these as non-annotated images within the analysis (to allow quantiﬁcation of inferred-annotation quality). [sent-168, score-0.192]
</p><p>67 The following analysis, in which annotated and non-annotated images are processed jointly, is executed as discussed in Section 4. [sent-174, score-0.263]
</p><p>68 Here we randomly choose 25 images for each category, and each image is resized to a dimension of 240 × 320 or 320 × 240. [sent-177, score-0.346]
</p><p>69 After performing this analysis, and upon examining the properties of segmented data associated with each (latent) object class on a small subset of the data, 5  we can infer words associated with some important Gk , and then label portions (objects) within each image via the inferred words. [sent-180, score-0.817]
</p><p>70 1 Image preprocessing Each image is ﬁrst segmented into 800 “superpixels”, which are local, coherent and preserve most of the structure necessary for segmentation at the scale of interest [19]. [sent-187, score-0.343]
</p><p>71 We discretize these features using a codebook of size 64 (other codebook sizes gave similar performance), and then calculate the distribution [1] for each feature within each superpixel as visual words [3, 6, 10, 11, 20, 23, 24]. [sent-198, score-0.194]
</p><p>72 j 1j ρ 2j ρ 3j ρ The center of each superpixel is recorded as the location coordinate sml . [sent-201, score-0.193]
</p><p>73 In addition, based on the learned posterior word distribution wi for each image class i, we can further infer which words/objects are probable for each scene class. [sent-215, score-0.4]
</p><p>74 Although not shown here for brevity, the analysis on UIUC features correctly inferred the 8 image classes associated with that data (without using annotations). [sent-217, score-0.36]
</p><p>75 By examining the words and segmented objects extracted with high probability as represented by wi , we may also assign names to each of the 18 image classes across both the MSRC and UIUC data, consistent with the associated class labels provided with the data. [sent-218, score-0.645]
</p><p>76 , M } we also have a posterior distribution on the associated class indicator zm . [sent-222, score-0.25]
</p><p>77 We approximate the membership for each image by assigning it to the mixture with largest probability. [sent-223, score-0.228]
</p><p>78 This “hard” decision is employed to provide scene-level label for each image (the Bayesian analysis can also yield a “soft” decision in terms of a full posterior distribution). [sent-224, score-0.215]
</p><p>79 2, and employing results from the processed nonannotated UIUC-Sport data, we examined the properties of segmented data associated with each (latent) object type. [sent-229, score-0.364]
</p><p>80 We inferred the presence of 12 unique objects, and these objects were assigned the following words: “human”, “horse”, “grass”, “sky”, “tree”, “ground”,“water”, “rock”, “court”, “boat”, “sailboat” and “snow”. [sent-230, score-0.217]
</p><p>81 Using these words, we annotated each image and re-trained our model in the presence of annotations. [sent-231, score-0.273]
</p><p>82 The improvement in performance, relative to processing the images without annotations, is attributed to the ability of words to disambiguate distinct objects that have similar properties in image-feature space (e. [sent-235, score-0.405]
</p><p>83 1  Building  Sky Grass Tree Object Index  0  Void  Grass  Cow Tree Void Object Index  Building  Figure 2: Example inferred latent properties associated with MSRC dataset. [sent-250, score-0.198]
</p><p>84 Middle and Right: Example probability of objects for a given class, w i (probability of object/words); here we only give the top 5 words for each class. [sent-252, score-0.241]
</p><p>85 68  Figure 3: Comparisons using confusion matrices for all images in each dataset (all of the annotated and nonannotated images in MSRC; all the non-annotated images in UIUC-Sport). [sent-585, score-0.588]
</p><p>86 3 Image annotation The proposed model infers a posterior distribution for the indicator variables cml (deﬁning the object/word for super-pixel l in image m). [sent-594, score-0.612]
</p><p>87 Similar to the “hard” image-class assignment discussed above, a “hard” segmentation is employed here to provide object labels for each super-pixel. [sent-595, score-0.21]
</p><p>88 For the MSRC images for which annotations were held out, we evaluate whether the words associated with objects in a given image were given in the associated annotation (thus, our annotation is deﬁned by the words we have assigned to objects in an image). [sent-596, score-1.426]
</p><p>89 Table 1: Comparison of precision and recall values for annotation and segmentation with Corr-LDA [6], our model without LSBP (Simp. [sent-597, score-0.245]
</p><p>90 The left part of Table 1 lists detailed annotation results for ﬁve objects, as well as the overall scores from all objects classes for the MSRC data. [sent-734, score-0.318]
</p><p>91 For example, for complicated objects the Corr-LDA segmentation results are very sensitive to the feature variance, and an object is generally segmented into many small, detailed parts. [sent-739, score-0.439]
</p><p>92 The name of original images are inferred by scene-level classiﬁcation via our model. [sent-748, score-0.242]
</p><p>93 One VB run of our model with LSBP, for 70 VB iterations, required nearly 7 hours for 320 images from MSRC dataset. [sent-756, score-0.191]
</p><p>94 6 Conclusions A nonparametric Bayesian model has been developed for clustering M images into classes; the images are represented as a aggregation of distinct localized objects, to which words may be assigned. [sent-761, score-0.504]
</p><p>95 To infer the relationships between image objects and words (labels), we only need to make the association between inferred model parameters and words. [sent-762, score-0.563]
</p><p>96 This may be done as a post-processing step if no words are provided, and it may done in situ if all or a subset of the M images are annotated. [sent-763, score-0.266]
</p><p>97 Spatially contiguous objects are realized via a new logistic stick-breaking process. [sent-764, score-0.294]
</p><p>98 Spatially coherent latent topic model for concurrent segmentation and classiﬁcation of objects and scenes. [sent-838, score-0.289]
</p><p>99 Towards total scene understaning: classiﬁcation, annotation and segmentation in an automatic framework. [sent-878, score-0.289]
</p><p>100 Multi-modal hierarchical Dirichlet process model for predicting image annotation and image-object label correspondence. [sent-937, score-0.356]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsbp', 0.545), ('msrc', 0.256), ('vmt', 0.24), ('image', 0.182), ('annotations', 0.168), ('images', 0.164), ('cml', 0.16), ('ksbp', 0.16), ('xml', 0.16), ('annotation', 0.147), ('objects', 0.139), ('object', 0.139), ('sml', 0.129), ('wzm', 0.128), ('mt', 0.123), ('zm', 0.113), ('prec', 0.112), ('wtl', 0.112), ('words', 0.102), ('cow', 0.096), ('segmented', 0.09), ('rec', 0.09), ('gmt', 0.08), ('inferred', 0.078), ('lm', 0.078), ('sheep', 0.077), ('contiguous', 0.073), ('scene', 0.071), ('segmentation', 0.071), ('vb', 0.07), ('associated', 0.068), ('annotated', 0.064), ('rowing', 0.064), ('sailing', 0.064), ('superpixel', 0.064), ('ml', 0.06), ('mult', 0.057), ('chair', 0.057), ('latent', 0.052), ('linkage', 0.051), ('manifested', 0.051), ('building', 0.05), ('logistic', 0.049), ('tl', 0.048), ('bocce', 0.048), ('croquet', 0.048), ('polo', 0.048), ('ymk', 0.048), ('localized', 0.047), ('word', 0.047), ('mixture', 0.046), ('grass', 0.046), ('variational', 0.044), ('sky', 0.043), ('spatial', 0.042), ('uiuc', 0.042), ('void', 0.042), ('spatially', 0.04), ('car', 0.039), ('dir', 0.039), ('iccv', 0.037), ('book', 0.036), ('indicator', 0.036), ('blei', 0.036), ('lth', 0.036), ('tree', 0.036), ('infer', 0.035), ('processed', 0.035), ('reside', 0.034), ('constituted', 0.034), ('kernel', 0.034), ('dirichlet', 0.034), ('gk', 0.034), ('truncation', 0.034), ('posterior', 0.033), ('encourages', 0.033), ('realized', 0.033), ('heterogeneous', 0.033), ('classes', 0.032), ('boat', 0.032), ('gcml', 0.032), ('nonannotated', 0.032), ('rockc', 0.032), ('sailboat', 0.032), ('sticki', 0.032), ('stickk', 0.032), ('type', 0.032), ('wi', 0.032), ('stick', 0.029), ('contiguity', 0.028), ('clipping', 0.028), ('dunson', 0.028), ('superpixels', 0.028), ('within', 0.028), ('model', 0.027), ('segment', 0.027), ('infers', 0.027), ('objectives', 0.027), ('portions', 0.027), ('drawn', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="5-tfidf-1" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>2 0.20246781 <a title="5-tfidf-2" href="./nips-2009-Modeling_Social_Annotation_Data_with_Content_Relevance_using_a_Topic_Model.html">153 nips-2009-Modeling Social Annotation Data with Content Relevance using a Topic Model</a></p>
<p>Author: Tomoharu Iwata, Takeshi Yamada, Naonori Ueda</p><p>Abstract: We propose a probabilistic topic model for analyzing and extracting contentrelated annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy, i.e. not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classiﬁcation and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.</p><p>3 0.18105902 <a title="5-tfidf-3" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>Author: Stephen Gould, Tianshi Gao, Daphne Koller</p><p>Abstract: Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classiﬁcation of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach simultaneously reasons about pixels, regions and objects in a coherent probabilistic model. Pixel appearance features allow us to perform well on classifying amorphous background classes, while the explicit representation of regions facilitate the computation of more sophisticated features necessary for object detection. Importantly, our model gives a single uniﬁed description of the scene—we explain every pixel in the image and enforce global consistency between all random variables in our model. We run experiments on the challenging Street Scene dataset [2] and show signiﬁcant improvement over state-of-the-art results for object detection accuracy. 1</p><p>4 0.17947574 <a title="5-tfidf-4" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>5 0.14254738 <a title="5-tfidf-5" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>6 0.13745958 <a title="5-tfidf-6" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>7 0.10668937 <a title="5-tfidf-7" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>8 0.097601339 <a title="5-tfidf-8" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>9 0.090731986 <a title="5-tfidf-9" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>10 0.085019998 <a title="5-tfidf-10" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>11 0.084858291 <a title="5-tfidf-11" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>12 0.084841602 <a title="5-tfidf-12" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>13 0.084039129 <a title="5-tfidf-13" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>14 0.08311817 <a title="5-tfidf-14" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>15 0.083000861 <a title="5-tfidf-15" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>16 0.081513017 <a title="5-tfidf-16" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>17 0.079748534 <a title="5-tfidf-17" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>18 0.076057121 <a title="5-tfidf-18" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>19 0.07391572 <a title="5-tfidf-19" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>20 0.073440664 <a title="5-tfidf-20" href="./nips-2009-Semi-Supervised_Learning_in_Gigantic_Image_Collections.html">212 nips-2009-Semi-Supervised Learning in Gigantic Image Collections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.204), (1, -0.179), (2, -0.208), (3, -0.108), (4, 0.025), (5, 0.093), (6, -0.012), (7, 0.04), (8, 0.139), (9, 0.064), (10, -0.01), (11, -0.014), (12, 0.087), (13, -0.002), (14, -0.041), (15, 0.017), (16, -0.075), (17, -0.02), (18, 0.088), (19, 0.017), (20, 0.042), (21, 0.006), (22, -0.053), (23, 0.005), (24, 0.006), (25, -0.063), (26, -0.028), (27, 0.023), (28, 0.026), (29, -0.022), (30, -0.013), (31, -0.002), (32, -0.009), (33, -0.028), (34, -0.011), (35, 0.042), (36, -0.089), (37, 0.085), (38, 0.104), (39, 0.05), (40, 0.058), (41, -0.041), (42, -0.017), (43, 0.047), (44, 0.046), (45, 0.016), (46, 0.013), (47, -0.137), (48, 0.039), (49, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94873983 <a title="5-lsi-1" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>2 0.75458175 <a title="5-lsi-2" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>3 0.73652244 <a title="5-lsi-3" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>4 0.73557323 <a title="5-lsi-4" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><p>5 0.73383445 <a title="5-lsi-5" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>Author: Joseph Schlecht, Kobus Barnard</p><p>Abstract: We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g. chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances. We learn the models using joint statistical inference over category parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images. 1</p><p>6 0.71318549 <a title="5-lsi-6" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>7 0.70952022 <a title="5-lsi-7" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>8 0.63121271 <a title="5-lsi-8" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>9 0.62304455 <a title="5-lsi-9" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>10 0.59345895 <a title="5-lsi-10" href="./nips-2009-Modeling_Social_Annotation_Data_with_Content_Relevance_using_a_Topic_Model.html">153 nips-2009-Modeling Social Annotation Data with Content Relevance using a Topic Model</a></p>
<p>11 0.59174317 <a title="5-lsi-11" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>12 0.58492833 <a title="5-lsi-12" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>13 0.55218124 <a title="5-lsi-13" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>14 0.53277367 <a title="5-lsi-14" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>15 0.51476753 <a title="5-lsi-15" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>16 0.50615901 <a title="5-lsi-16" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>17 0.50011653 <a title="5-lsi-17" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>18 0.49702257 <a title="5-lsi-18" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>19 0.49692637 <a title="5-lsi-19" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>20 0.47800541 <a title="5-lsi-20" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.035), (25, 0.096), (31, 0.011), (35, 0.056), (36, 0.089), (39, 0.072), (58, 0.051), (66, 0.012), (71, 0.079), (86, 0.073), (91, 0.014), (98, 0.298)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82965541 <a title="5-lda-1" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>same-paper 2 0.75578302 <a title="5-lda-2" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>3 0.72831267 <a title="5-lda-3" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>Author: Daniel Cavagnaro, Jay Myung, Mark A. Pitt</p><p>Abstract: In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efﬁciently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to ﬁnd designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1</p><p>4 0.64637172 <a title="5-lda-4" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>5 0.54383963 <a title="5-lda-5" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>Author: Joseph Schlecht, Kobus Barnard</p><p>Abstract: We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g. chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances. We learn the models using joint statistical inference over category parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images. 1</p><p>6 0.53978091 <a title="5-lda-6" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>7 0.53848153 <a title="5-lda-7" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>8 0.53816098 <a title="5-lda-8" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>9 0.53630292 <a title="5-lda-9" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>10 0.53499436 <a title="5-lda-10" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>11 0.53389955 <a title="5-lda-11" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>12 0.53367734 <a title="5-lda-12" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>13 0.53353959 <a title="5-lda-13" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>14 0.53256017 <a title="5-lda-14" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>15 0.53193533 <a title="5-lda-15" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>16 0.53133231 <a title="5-lda-16" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>17 0.5292238 <a title="5-lda-17" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>18 0.52909547 <a title="5-lda-18" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>19 0.52869356 <a title="5-lda-19" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>20 0.52840233 <a title="5-lda-20" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
