<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-41" href="#">nips2009-41</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</h1>
<br/><p>Source: <a title="nips-2009-41-pdf" href="http://papers.nips.cc/paper/3751-bayesian-source-localization-with-the-multivariate-laplace-prior.pdf">pdf</a></p><p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>Reference: <a title="nips-2009-41-reference" href="../nips2009_reference/nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. [sent-2, score-0.646]
</p><p>2 Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. [sent-3, score-0.287]
</p><p>3 We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. [sent-6, score-0.54]
</p><p>4 Let q, p, and t denote the number of sensors, sources and time points, respectively. [sent-8, score-0.223]
</p><p>5 Sensor readings Y ∈ Rq×t and source currents S ∈ Rp×t are related by Y = XS + E  (1)  where X ∈ Rq×p is a lead ﬁeld matrix that represents how sources project onto the sensors and E ∈ Rq×t represents sensor noise. [sent-9, score-0.77]
</p><p>6 Unfortunately, localizing distributed sources is an ill-posed inverse problem that only admits a unique solution when additional constraints are deﬁned. [sent-10, score-0.313]
</p><p>7 In a Bayesian setting, these constraints take the form of a prior on the sources [3, 19]. [sent-11, score-0.306]
</p><p>8 Popular choices of prior source amplitude distributions are Gaussian or Laplace priors, whose MAP estimates correspond to minimum norm and minimum current estimates, respectively [18]. [sent-12, score-0.442]
</p><p>9 In contrast, minimum current estimates lead to focal source estimates that may be scattered too much throughout the brain volume [9]. [sent-14, score-0.521]
</p><p>10 In this paper, we take the Laplace prior as our point of departure for Bayesian source localization (instead of using just the MAP estimate). [sent-15, score-0.428]
</p><p>11 Here, in contrast, we assume a multivariate Laplace distribution over all sources, which allows sources to be coupled. [sent-17, score-0.357]
</p><p>12 Since the posterior cannot be computed exactly, we formulate an efﬁcient expectation propagation 1  algorithm [12] which allows us to approximate the posterior of interest for very large models. [sent-20, score-0.21]
</p><p>13 Efﬁciency arises from the block diagonal form of the approximate posterior covariance matrix due to properties of the scale mixture representation. [sent-21, score-0.301]
</p><p>14 The computational bottleneck then reduces to computation of the diagonal elements of a sparse matrix inverse, which can be solved through Cholesky decomposition of a sparse matrix and application of the Takahashi equation [17]. [sent-22, score-0.232]
</p><p>15 2  Bayesian source localization  In a Bayesian setting, the goal of source localization is to estimate the posterior p(S | Y, X, Σ, Θ) ∝ p(Y | S, X, Σ)p(S | Θ)  (2)  where the likelihood term p(Y | S) = t N (yt | Xst , Σ) factorizes over time and Σ represents sensor noise. [sent-25, score-0.921]
</p><p>16 1 The source localization problem can be formulated as a (Bayesian) linear regression problem where the source currents s play the role of the regression coefﬁcients and rows of the lead ﬁeld matrix X can be interpreted as covariates. [sent-31, score-0.84]
</p><p>17 In the following, we deﬁne a multivariate Laplace distribution, represented in terms of a scale mixture, as a convenient prior that incorporates both spatio-temporal and sparsity constraints. [sent-32, score-0.276]
</p><p>18 L (s | λ) =  (4)  Eltoft et al [5] deﬁned the multivariate Laplace distribution as a scale mixture of a multivariate √ Gaussian given by zΣ1/2 s where s is a standard normal multivariate Gaussian, Σ is a positive deﬁnite matrix, and z is drawn from a univariate exponential distribution. [sent-36, score-0.604]
</p><p>19 In contrast, we use an alternative formulation of the multivariate Laplace distribution that couples the variances of the sources rather than the source currents themselves. [sent-38, score-0.809]
</p><p>20 For an uncoupled multivariate Laplace distribution, this generalization reads L (s | λ) =  2 N si | 0, u2 + vi N vi | 0, 1/λ2 N ui | 0, 1/λ2 i  dudv  (5)  i  such that each source current si gets assigned scale variables ui and vi . [sent-41, score-3.115]
</p><p>21 We can interpret the scale variables corresponding to source i as indicators of its relevance: the larger (the posterior estimate 2 of) u2 + vi , the more relevant the corresponding source. [sent-42, score-0.802]
</p><p>22 2  f s1  s2  ···  sp  g1  g2  ···  gp  ···  up  u1  v1  u2  v2  h1  vp  h2  Figure 1: Factor graph representation of Bayesian source localization with a multivariate Laplace prior. [sent-44, score-0.508]
</p><p>23 Factors gi correspond to the coupling between sources and scales. [sent-46, score-0.445]
</p><p>24 Factors h1 and h2 represent the (identical) multivariate Gaussians on u and v with prior precision matrix J. [sent-47, score-0.288]
</p><p>25 This deﬁnition yields a coupling in the magnitudes of the source currents through their variances. [sent-50, score-0.604]
</p><p>26 Note that this approach is deﬁning the multivariate Laplace with the help of a multivariate exponential distribution [10]. [sent-52, score-0.292]
</p><p>27 3  Approximate inference  Our goal is to compute posterior marginals for sources si as well as scale variables ui and vi in order to determine source relevance. [sent-57, score-1.753]
</p><p>28 Using EP, we will approximate p(z) with q (z) ∝ t0 (z) i ti (z), where ¯ the ti (z) are Gaussian functions as well. [sent-68, score-0.36]
</p><p>29 Equation (6) introduces 2p auxiliary Gaussian variables (u, v) that are coupled to the si ’s by p non-Gaussian factors, thus, we have to approximate p terms. [sent-70, score-0.457]
</p><p>30 The multivariate Laplace distribution deﬁned in [5] introduces one auxiliary variable and couples all the si sj terms to it, therefore, it would lead to p2 non-Gaussian terms to be approximated. [sent-71, score-0.547]
</p><p>31 Moreover, as we will see below, the a priori independence of u and v and 3  the form of the terms ti (z) results in an approximation of the posterior with the same block-diagonal structure as that of t0 (z). [sent-72, score-0.225]
</p><p>32 ¯ ¯i ¯ In each step, EP updates ti with t∗ by deﬁning q \i ∝ t0 (z) \i tj , minimizing KL ti q \i q ∗ with ¯i respect to q ∗ and setting t∗ ∝ q ∗ /q \i . [sent-73, score-0.355]
</p><p>33 It can be shown that when ti depends only on a subset of ¯ variables zi (in our case on zi = (si , ui , vi )) then so does ti . [sent-74, score-1.289]
</p><p>34 The minimization of the KL divergence then boils down to the minimization of KL ti (zi )q \i (zi ) q ∗ (zi ) with respect to q ∗ (zi ) ¯ ¯i and ti is updated to t∗ (zi ) ∝ q ∗ (zi )/q \i (zi ). [sent-75, score-0.33]
</p><p>35 , q ∗ (si , ui , vi ) is a Gaussian with the same mean and covariance matrix as q i (zi ) ∝ ti (zi )q \i (zi ). [sent-78, score-0.982]
</p><p>36 We will now work out the EP update for the i-th term approximation in more detail to show by ¯ induction that ti (si , ui , vi ) factorizes into independent terms for si , ui , and vi . [sent-84, score-2.08]
</p><p>37 Since ui and vi play exactly the same role, it is also easy to see that the term approximation is always symmetric in ui and vi . [sent-85, score-1.565]
</p><p>38 Let us suppose that q (si , ui , vi ) and consequently q \i (si , ui , vi ) factorizes into independent terms for si , ui , and vi , e. [sent-86, score-2.663]
</p><p>39 , we can write 2 2 2 q \i (si , ui , vi ) = N (si | mi , σi )N (ui | 0, νi )N (vi | 0, νi ). [sent-88, score-0.797]
</p><p>40 (9)  ¯ By initializing ti (si , ui , vi ) = 1, we have q(z) ∝ t0 (z) and the factorization of q \i (si , ui , vi ) follows directly from the factorization of t0 (z) into independent terms for s, u, and v. [sent-89, score-1.707]
</p><p>41 (10)  (11)  2 Since q i (ui , vi ) only depends on u2 and vi and is thus invariant under sign changes of ui and vi , i we must have E [ui ] = E [vi ] = 0, as well as E [ui vi ] = 0. [sent-92, score-1.98]
</p><p>42 Because of symmetry, we further have 2 2 2 E u2 = E vi = (E u2 + E vi )/2. [sent-93, score-0.806]
</p><p>43 Since q i (ui , vi ) can be expressed as a function of u2 + vi i i i only, this variance can be computed from (11) using one-dimensional Gauss-Laguerre numerical quadrature [15]. [sent-94, score-0.893]
</p><p>44 The ﬁrst and second moments of si conditioned upon ui and vi follow directly from (10). [sent-95, score-1.136]
</p><p>45 Because both (10) and (11) are invariant under sign changes of ui and vi , we must have 2 E [si ui ] = E [si vi ] = 0. [sent-96, score-1.542]
</p><p>46 Furthermore, since the conditional moments again depend only on u2 + vi , i 2 also E [si ] and E si can be computed with one-dimensional Gauss-Laguerre integration. [sent-97, score-0.768]
</p><p>47 Summarizing, we have shown that if the old term approximations factorize into independent terms for si , ui , ¯i and vi , the new term approximation after an EP update, t∗ (si , ui , vi ) ∝ q ∗ (si , ui , vi )/q \i (si , ui , vi ), must do the same. [sent-98, score-3.427]
</p><p>48 Furthermore, given the cavity distribution q \i (si , ui , vi ), all required moments can be computed using one-dimensional numerical integration. [sent-99, score-0.878]
</p><p>49 The crucial observation here is that the terms ti (si , ui , vi ) introduce dependencies between si and (ui , vi ), as expressed in Eqs. [sent-100, score-1.636]
</p><p>50 That is, also when the expectations are taken with respect to the exact p(s, u, v) we have E [ui ] = E [vi ] = E [ui vi ] = E [si ui ] = E [si vi ] = 0 and 2 2 E u2 = E vi . [sent-103, score-1.577]
</p><p>51 The variance of the scales E u2 + vi determines the amount of regularization i i on the source parameter si such that large variance implies little regularization. [sent-104, score-1.079]
</p><p>52 Calculating q \i (si , ui , vi ) 4  requires the computation of the marginal moments q(si ), q(ui ) and q(vi ). [sent-107, score-0.839]
</p><p>53 This precision matrix has the block-diagonal form   XT X/σ 2 + Ks 0 0  K= (12) 0 λ2 J + Ku 0 2 0 0 λ J + Kv where J is a sparse precision matrix which determines the coupling, and Ks , Ku , and Kv = Ku are diagonal matrices that contain the contributions of the term approximations. [sent-109, score-0.297]
</p><p>54 4  Experiments  Returning to the source localization problem, we will show that the MVL prior can be used to induce constraints on the source estimates. [sent-113, score-0.74]
</p><p>55 The lead ﬁeld matrix is deﬁned for the three x, y, and z orientations in each of the source locations and was normalized to correct for depth bias. [sent-124, score-0.447]
</p><p>56 In the next section, we compare source estimates for the MMN difference wave that have been obtained when using either a decoupled or a coupled MVL prior. [sent-127, score-0.554]
</p><p>57 For ease of exposition, we focus on a spatial prior induced by the coupling of neighboring sources. [sent-128, score-0.354]
</p><p>58 Differences in the source estimates will therefore arise only from the form of the 11589 × 11589 sparse precision matrix J. [sent-130, score-0.46]
</p><p>59 The ﬁrst estimate is obtained by assuming that there is no coupling between elements of the lead ﬁeld matrix, such that J = I. [sent-131, score-0.26]
</p><p>60 The second estimate is obtained by assuming a coupling between neighboring sources i and j within the brain volume with ﬁxed strength c. [sent-133, score-0.577]
</p><p>61 This coupling is speciﬁed through the unnormalized precision matrix ˆ ˆ ˆ ˆ ˆ ˆ J by assuming Jix ,jx = Jiy ,jy = Jiz ,jz = −c while diagonal elements Jii are set to 1 − j=i Jij . [sent-134, score-0.362]
</p><p>62 2 This prior dictates that the magnitude of the variances of the source currents are coupled between sources. [sent-135, score-0.558]
</p><p>63 For the coupling strength c, we use correlation as a guiding principle. [sent-136, score-0.271]
</p><p>64 Speciﬁcally, correlation between sources si and sj is given by ˆ rij = J−1  ˆ / J−1 ij  1 2  ii  ˆ J−1  1 2  . [sent-138, score-0.569]
</p><p>65 Note that this also leads to more distant sources having non-zero correlations. [sent-141, score-0.248]
</p><p>66 5  J  L  C  Figure 2: Spatial coupling leads to the normalized precision matrix J with coupling of neighboring source orientations in the x, y, and z directions. [sent-143, score-0.954]
</p><p>67 The correlation matrix C shows the correlations between the source orientations. [sent-145, score-0.411]
</p><p>68 neighboring sources is motivated by the notion that we expect neighboring sources to be similarly though not equivalently involved for a given task. [sent-147, score-0.544]
</p><p>69 Figure 2 demonstrates how a chosen coupling leads to a particular structure of J, where irregularities in J are caused by the structure of the imaged brain volume. [sent-149, score-0.304]
</p><p>70 The correlation matrix C shows the correlations between the sources induced by the structure of J. [sent-153, score-0.351]
</p><p>71 Zeros in the correlation matrix arise from the independence between source orientations x, y, and z. [sent-154, score-0.431]
</p><p>72 5  Results  Figure 3 depicts the difference wave that was obtained by subtracting the trial average for standard tones from the trial average for deviant tones. [sent-155, score-0.232]
</p><p>73 6  Figure 4: Source estimates using a decoupled prior (top) or a coupled prior (bottom). [sent-165, score-0.323]
</p><p>74 Figure 5: Relative variance using a decoupled prior (top) or a coupled prior (bottom). [sent-167, score-0.328]
</p><p>75 We now proceed to localizing the sources of the activation induced by mismatch negativity. [sent-170, score-0.328]
</p><p>76 Figure 4 depicts the localized sources when using either a decoupled MVL prior or a coupled MVL prior. [sent-171, score-0.481]
</p><p>77 The coupled spatial prior leads to stronger source currents that are spread over a larger brain volume. [sent-172, score-0.632]
</p><p>78 MVL source localization has correctly identiﬁed the source over left temporal cortex but does not capture the source over right temporal cortex that is also hypothesized to be present (cf. [sent-173, score-1.004]
</p><p>79 Differences between the decoupled and the coupled prior become more salient when we look at the relative variance of the auxiliary variables as shown in Fig. [sent-178, score-0.319]
</p><p>80 Relative variance is deﬁned here as posterior variance minus prior variance of the auxiliary variables, normalized to be between zero and one. [sent-180, score-0.303]
</p><p>81 This measure indicates the change in magnitude of the variance of the auxiliary variables, and thus indirectly that of the sources via Eq. [sent-181, score-0.316]
</p><p>82 Since only sources with non-zero contributions should have high variance, this measure can be used to indicate the relevance of a source. [sent-183, score-0.255]
</p><p>83 Figure 5 7  shows that temporal sources in both left and right hemispheres are relevant. [sent-184, score-0.255]
</p><p>84 The relevance of the temporal source in the right hemisphere becomes more pronounced when using the coupled prior. [sent-185, score-0.432]
</p><p>85 6  Discussion  In this paper, we introduced a multivariate Laplace prior as the basis for Bayesian source localization. [sent-186, score-0.471]
</p><p>86 By formulating this prior as a scale mixture we were able to approximate posteriors of interest using expectation propagation in an efﬁcient manner. [sent-187, score-0.291]
</p><p>87 Computation time is mainly inﬂuenced by the sparsity structure of the precision matrix J which is used to specify interactions between sources by coupling their variances. [sent-188, score-0.6]
</p><p>88 It was shown that coupling of neighboring sources leads to source estimates that are somewhat more spatially smeared as compared with a decoupled prior. [sent-190, score-0.932]
</p><p>89 However, posterior marginals can still be used to exclude irrelevant sources since these will typically have a mean activation close to zero with small variance. [sent-195, score-0.346]
</p><p>90 Note that it is straightforward to impose other constraints since this only requires the speciﬁcation of suitable interactions between sources through J. [sent-199, score-0.275]
</p><p>91 For instance, the spatial prior could be made more realistic by taking anatomical constraints into account or by the inclusion of coupling between sources over time. [sent-200, score-0.557]
</p><p>92 Other constraints that can be implemented with our approach are the coupling of individual orientations within a source, or even the coupling of source estimates between different subjects. [sent-201, score-0.852]
</p><p>93 Coupling of source orientations has been realized before in [9] through an 1 / 2 norm, although not using a fully Bayesian approach. [sent-202, score-0.336]
</p><p>94 In future work, we aim to examine the effect of the proposed priors and optimize the regularization and coupling parameters via empirical Bayes [4]. [sent-203, score-0.248]
</p><p>95 Other directions for further research are inclusion of the noise variance in the optimization procedure and dealing with the depth bias that often arises in distributed source models in a more principled way. [sent-204, score-0.379]
</p><p>96 To obtain a generalization of the univariate Laplace distribution, we used a multivariate exponential distribution of the scales, to be compared with the multivariate log-normal distribution in [11]. [sent-207, score-0.339]
</p><p>97 Since (the efﬁciency of) our method for approximate inference only depends on the sparsity of the multivariate scale distribution, and not on its precise form, it should be feasible to compute approximate marginals for the model presented in [11] as well. [sent-210, score-0.345]
</p><p>98 Concluding, we believe the scale mixture representation of the multivariate Laplace distribution to be a promising approach to Bayesian distributed source localization. [sent-211, score-0.521]
</p><p>99 Combining sparsity and rotau tional invariance in EEG/MEG source reconstruction. [sent-282, score-0.315]
</p><p>100 Classes of multivariate exponential and multivariate geometric distributions derived from Markov processes. [sent-287, score-0.292]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vi', 0.403), ('ui', 0.368), ('si', 0.297), ('source', 0.283), ('mvl', 0.226), ('sources', 0.223), ('coupling', 0.222), ('laplace', 0.216), ('ti', 0.165), ('ep', 0.144), ('multivariate', 0.134), ('negativity', 0.103), ('currents', 0.099), ('zi', 0.094), ('localization', 0.091), ('decoupled', 0.087), ('coupled', 0.085), ('deviant', 0.082), ('mismatch', 0.072), ('takahashi', 0.072), ('moments', 0.068), ('marginals', 0.063), ('meg', 0.062), ('dudv', 0.062), ('mmn', 0.062), ('tones', 0.062), ('posterior', 0.06), ('brain', 0.057), ('bayesian', 0.057), ('ku', 0.056), ('wave', 0.056), ('scale', 0.056), ('xs', 0.055), ('prior', 0.054), ('precision', 0.054), ('factorizes', 0.053), ('orientations', 0.053), ('neighboring', 0.049), ('correlation', 0.049), ('mixture', 0.048), ('variance', 0.048), ('cholesky', 0.047), ('univariate', 0.047), ('eld', 0.046), ('matrix', 0.046), ('auxiliary', 0.045), ('kl', 0.045), ('netherlands', 0.044), ('sensors', 0.044), ('posteriors', 0.043), ('estimates', 0.043), ('eltoft', 0.041), ('nijmegen', 0.041), ('oddball', 0.041), ('uncoupled', 0.041), ('diagonal', 0.04), ('rq', 0.04), ('numerical', 0.039), ('lead', 0.038), ('propagation', 0.038), ('sensor', 0.037), ('ks', 0.037), ('variances', 0.037), ('ims', 0.036), ('magnetoencephalography', 0.036), ('sparse', 0.034), ('correlations', 0.033), ('couples', 0.033), ('localizing', 0.033), ('depicts', 0.032), ('bottleneck', 0.032), ('neuroimage', 0.032), ('temporal', 0.032), ('relevance', 0.032), ('sparsity', 0.032), ('amd', 0.031), ('ministry', 0.031), ('minimum', 0.031), ('ms', 0.03), ('approximate', 0.03), ('spatial', 0.029), ('kv', 0.029), ('constraints', 0.029), ('inverse', 0.028), ('normal', 0.027), ('depth', 0.027), ('mi', 0.026), ('priors', 0.026), ('volume', 0.026), ('tj', 0.025), ('mri', 0.025), ('leads', 0.025), ('exponential', 0.024), ('characteristics', 0.024), ('gaussian', 0.024), ('interactions', 0.023), ('differs', 0.023), ('term', 0.023), ('expectation', 0.022), ('arises', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="41-tfidf-1" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>2 0.20123374 <a title="41-tfidf-2" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>Author: Jarno Vanhatalo, Pasi Jylänki, Aki Vehtari</p><p>Abstract: In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be signiﬁcantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the inﬂuence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution. 1</p><p>3 0.19220026 <a title="41-tfidf-3" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>Author: Mikkel Schmidt</p><p>Abstract: We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efﬁcient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of nonnegative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</p><p>4 0.18266319 <a title="41-tfidf-4" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>5 0.14643021 <a title="41-tfidf-5" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>6 0.10353205 <a title="41-tfidf-6" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>7 0.10253958 <a title="41-tfidf-7" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>8 0.10134234 <a title="41-tfidf-8" href="./nips-2009-Fast_Learning_from_Non-i.i.d._Observations.html">94 nips-2009-Fast Learning from Non-i.i.d. Observations</a></p>
<p>9 0.090513639 <a title="41-tfidf-9" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>10 0.08965607 <a title="41-tfidf-10" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>11 0.082360595 <a title="41-tfidf-11" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>12 0.080288745 <a title="41-tfidf-12" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>13 0.077967256 <a title="41-tfidf-13" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>14 0.07613761 <a title="41-tfidf-14" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>15 0.075075477 <a title="41-tfidf-15" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>16 0.0710335 <a title="41-tfidf-16" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>17 0.070054397 <a title="41-tfidf-17" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>18 0.069500536 <a title="41-tfidf-18" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>19 0.065473706 <a title="41-tfidf-19" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>20 0.063252762 <a title="41-tfidf-20" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.209), (1, -0.027), (2, 0.057), (3, 0.021), (4, 0.042), (5, -0.084), (6, 0.177), (7, -0.088), (8, -0.043), (9, -0.097), (10, -0.051), (11, -0.027), (12, -0.06), (13, 0.053), (14, 0.03), (15, -0.012), (16, -0.068), (17, 0.102), (18, -0.091), (19, -0.227), (20, -0.031), (21, 0.028), (22, -0.333), (23, 0.013), (24, -0.127), (25, -0.116), (26, -0.038), (27, 0.08), (28, 0.114), (29, -0.042), (30, -0.012), (31, -0.048), (32, 0.002), (33, -0.146), (34, -0.01), (35, -0.098), (36, 0.152), (37, 0.03), (38, -0.014), (39, -0.003), (40, 0.073), (41, 0.078), (42, -0.041), (43, 0.078), (44, -0.012), (45, -0.005), (46, 0.029), (47, 0.064), (48, -0.172), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98241538 <a title="41-lsi-1" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>2 0.79370308 <a title="41-lsi-2" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>Author: Mikkel Schmidt</p><p>Abstract: We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efﬁcient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of nonnegative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</p><p>3 0.62522632 <a title="41-lsi-3" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>4 0.60505736 <a title="41-lsi-4" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>Author: Jarno Vanhatalo, Pasi Jylänki, Aki Vehtari</p><p>Abstract: In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be signiﬁcantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the inﬂuence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution. 1</p><p>5 0.52761346 <a title="41-lsi-5" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>6 0.47170284 <a title="41-lsi-6" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>7 0.44243625 <a title="41-lsi-7" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>8 0.44040358 <a title="41-lsi-8" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>9 0.42356494 <a title="41-lsi-9" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>10 0.40248522 <a title="41-lsi-10" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>11 0.39893588 <a title="41-lsi-11" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>12 0.39541104 <a title="41-lsi-12" href="./nips-2009-Fast_Learning_from_Non-i.i.d._Observations.html">94 nips-2009-Fast Learning from Non-i.i.d. Observations</a></p>
<p>13 0.3859821 <a title="41-lsi-13" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>14 0.38563451 <a title="41-lsi-14" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>15 0.37988156 <a title="41-lsi-15" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>16 0.35283431 <a title="41-lsi-16" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>17 0.3466424 <a title="41-lsi-17" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>18 0.33839032 <a title="41-lsi-18" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>19 0.33569151 <a title="41-lsi-19" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>20 0.32953382 <a title="41-lsi-20" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.01), (24, 0.065), (25, 0.045), (35, 0.072), (36, 0.129), (39, 0.048), (42, 0.011), (58, 0.098), (62, 0.199), (71, 0.088), (81, 0.053), (86, 0.08), (91, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90632623 <a title="41-lda-1" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>Author: Robert Wilson, Leif Finkel</p><p>Abstract: Recent experimental evidence suggests that the brain is capable of approximating Bayesian inference in the face of noisy input stimuli. Despite this progress, the neural underpinnings of this computation are still poorly understood. In this paper we focus on the Bayesian ﬁltering of stochastic time series and introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the Kalman ﬁlter in the limit of small prediction error. When the prediction error is large we show that the network responds robustly to changepoints in a way that is qualitatively compatible with the optimal Bayesian model. The model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions. 1</p><p>2 0.9050222 <a title="41-lda-2" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>Author: Pietro Berkes, Ben White, Jozsef Fiser</p><p>Abstract: The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness. 1</p><p>same-paper 3 0.86901832 <a title="41-lda-3" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>4 0.7494604 <a title="41-lda-4" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>5 0.74455887 <a title="41-lda-5" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>6 0.73681736 <a title="41-lda-6" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>7 0.73387599 <a title="41-lda-7" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>8 0.73212683 <a title="41-lda-8" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>9 0.73061067 <a title="41-lda-9" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>10 0.72988927 <a title="41-lda-10" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>11 0.72936606 <a title="41-lda-11" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>12 0.72853529 <a title="41-lda-12" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>13 0.72555375 <a title="41-lda-13" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>14 0.72500533 <a title="41-lda-14" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>15 0.72368795 <a title="41-lda-15" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>16 0.72263318 <a title="41-lda-16" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>17 0.72229153 <a title="41-lda-17" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>18 0.72199094 <a title="41-lda-18" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>19 0.72138637 <a title="41-lda-19" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>20 0.71970153 <a title="41-lda-20" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
