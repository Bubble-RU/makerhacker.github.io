<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 nips-2009-Compositionality of optimal control laws</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-54" href="#">nips2009-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 nips-2009-Compositionality of optimal control laws</h1>
<br/><p>Source: <a title="nips-2009-54-pdf" href="http://papers.nips.cc/paper/3842-compositionality-of-optimal-control-laws.pdf">pdf</a></p><p>Author: Emanuel Todorov</p><p>Abstract: We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties – one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of LQG controllers) without requiring the ﬁnal cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Green’s function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the ﬁeld of motor control, yet they have been difﬁcult to reconcile. Our work makes this possible.</p><p>Reference: <a title="nips-2009-54-reference" href="../nips2009_reference/nips-2009-Compositionality_of_optimal_control_laws_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Compositionality of optimal control laws  Emanuel Todorov Applied Mathematics and Computer Science & Engineering University of Washington todorov@cs. [sent-1, score-0.547]
</p><p>2 edu  Abstract We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. [sent-3, score-0.485]
</p><p>3 The primitives are themselves feedback controllers pursuing their own agendas. [sent-4, score-0.326]
</p><p>4 The resulting composite control law is provably optimal when the problem belongs to a certain class. [sent-6, score-0.788]
</p><p>5 In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i. [sent-9, score-0.156]
</p><p>6 More generally, a natural set of control primitives can be constructed by applying SVD to Green’s function of the Bellman equation. [sent-12, score-0.512]
</p><p>7 The ideas of optimality and compositionality are both very prominent in the ﬁeld of motor control, yet they have been difﬁcult to reconcile. [sent-14, score-0.445]
</p><p>8 1 Introduction Stochastic optimal control is of interest in many ﬁelds of science and engineering, however it remains hard to solve. [sent-16, score-0.403]
</p><p>9 Dynamic programming [1] and reinforcement learning [2] work well in discrete state spaces of reasonable size, but cannot handle continuous high-dimensional state spaces characteristic of complex dynamical systems. [sent-17, score-0.197]
</p><p>10 With few exceptions [5, 6] this good-in-general idea is rarely used in optimal control, because it is unclear what/how can be composed in a way that guarantees optimality of the resulting control law. [sent-21, score-0.444]
</p><p>11 Since the brain remains pretty much the only system capable of solving truly complex control problems, sensorimotor neuroscience is a natural (albeit under-exploited) source of inspiration. [sent-23, score-0.362]
</p><p>12 To be sure, a satisfactory understanding of the neural control of movement is nowhere in sight. [sent-24, score-0.445]
</p><p>13 One such idea is that biological movements are near-optimal [7, 8]. [sent-26, score-0.158]
</p><p>14 For about a century, researchers have been talking about motor synergies or primitives which somehow simplify control [9–11]. [sent-29, score-0.638]
</p><p>15 However the structure and origin of the hypothetical primitives, the rules for combining them, and the ways in which they actually simplify the control problem remain unclear. [sent-31, score-0.329]
</p><p>16 1  2 Stochastic optimal control problems with linear Bellman equations We will be able to derive compositionality rules for ﬁrst-exit and ﬁnite-horizon stochastic optimal control problems which belong to a certain class. [sent-32, score-1.222]
</p><p>17 Most notably the optimal control law is found analytically given the optimal cost-to-go, which in turn is the solution to a linear equation obtained from the Bellman equation by exponentiation. [sent-34, score-0.72]
</p><p>18 The discrete-time problem is deﬁned by a state cost q (x) ≥ 0 describing how (un)desirable different states are, and passive dynamics x0 ∼ p (·|x) characterizing the behavior of the system in the absence of controls. [sent-39, score-0.406]
</p><p>19 The controller can impose any dynamics x0 ∼ u (·|x) it wishes, however it pays a price (control cost) which is the KL divergence between u and p. [sent-40, score-0.284]
</p><p>20 Thus the discrete-time problem is dynamics: cost rate:  x0 ∼ u (·|x) (x, u (·|x)) = q (x) + KL (u (·|x) ||p (·|x))  Let I denote the set of interior states and B the set of boundary states, and let f (x) ≥ 0, x ∈ B be a ﬁnal cost. [sent-42, score-0.271]
</p><p>21 The continuous-time problem is a control-afﬁne Ito diffusion with control-quadratic cost: dx = a (x) dt + B (x) (udt + σdω) 1 2 cost rate: (x, u) = q (x) + 2 kuk 2σ The control u is now a (more traditional) vector and ω is a Brownian motion process. [sent-46, score-0.503]
</p><p>22 Note that the control cost scaling by σ −2 , which is needed to make the math work, can be compensated by rescaling q. [sent-47, score-0.406]
</p><p>23 This is done by deﬁning the passive dynamics p(h) (·|x) as the h-step transition probability density of the uncontrolled diffusion (or an Euler approximation to it), and the state cost as q(h) (x) = hq (x). [sent-49, score-0.39]
</p><p>24 ¡ ¢ Then, in the limit h → 0, the integral equation exp q(h) z = G(h) [z] reduces to the differential equation qz = L [z]. [sent-50, score-0.147]
</p><p>25 From the formula for KL divergence between Gaussians, the KL control cost in the discrete-time formulation reduces to the quadratic control cost in the continuous-time formulation. [sent-52, score-0.852]
</p><p>26 3 Compositionality theory The compositionality developed in this section follows from the linearity of equations (1, 3). [sent-56, score-0.328]
</p><p>27 Consider a collection of K optimal control problems in our class which all have the same dynamics – p (·|x) in discrete time or a (x) , B (x) , σ in continuous time – the same state cost rate q (x) and the same sets I and B of interior and boundary states. [sent-59, score-0.883]
</p><p>28 These problems differ only in their ﬁnal costs fk (x). [sent-60, score-0.216]
</p><p>29 Let zk (x) denote the desirability function for problem k, and u∗ (·|x) or k u∗ (x) the corresponding optimal control law. [sent-61, score-0.886]
</p><p>30 The latter will serve as primitives for constructing k optimal control laws for new problems in our class. [sent-62, score-0.774]
</p><p>31 Suppose the ﬁnal cost for the composite problem is f (x), and there exist weights wk such that ´ ³P K (4) f (x) = − log k=1 wk exp (−fk (x)) Thus the functions fk (x) deﬁne a K-dimensional manifold of composite problems. [sent-64, score-0.968]
</p><p>32 The above condition ensures that for all boundary/terminal states x ∈ B we have P (5) z (x) = K wk zk (x) k=1  Since z is the solution to a linear equation, if (5) holds on the boundary then it must hold everywhere. [sent-65, score-0.566]
</p><p>33 Thus the desirability function for the composite problem is a linear combination of the desirability functions for the component problems. [sent-66, score-0.69]
</p><p>34 The weights in this linear combination can be interpreted as compatibilities between the control objectives in the component problems and the control objective in the composite problem. [sent-67, score-0.964]
</p><p>35 The optimal control law for the composite problem is given by (1, 3). [sent-68, score-0.788]
</p><p>36 The above construction implies that both z and zk are everywhere positive. [sent-69, score-0.303]
</p><p>37 Indeed if ³P ´ K f (x) = − log wk zk (x) (6) k=1  holds for all x ∈ B, then (5) and z (x) > 0 hold everywhere even if zk (x) ≤ 0 for some k and x. [sent-72, score-0.69]
</p><p>38 In this case the zk ’s are no longer desirability functions for well-deﬁned optimal control problems. [sent-73, score-0.886]
</p><p>39 Nevertheless we can think of them as generalized desirability functions with similar meaning: the larger zk (x) is the more compatible state x is with the agenda of component k. [sent-74, score-0.615]
</p><p>40 1 Compositionality of discrete-time control laws When zk (x) > 0 the composite control law u∗ can be expressed as a state-dependent convex combination of the component control laws u∗ . [sent-76, score-1.969]
</p><p>41 Combining (5, 1) and using the linearity of G, k u∗ (x0 |x) =  X wk G [zk ] (x) p (x0 |x) zk (x0 ) P G [zk ] (x) s ws G [zs ] (x) k  3  The second term above is u∗ . [sent-77, score-0.535]
</p><p>42 2 Compositionality of continuous-time control laws Substituting (5) in (3) and assuming zk (x) > 0, the control law given by (3) can be written as ¸ X wk zk (x) ∙ σ 2 T ∂ ∗ P B (x) zk (x) u (x) = ∂x s ws zs (x) zk (x) k  The term in brackets is  u∗ k  (x). [sent-81, score-2.335]
</p><p>43 This is surprising given that in one case the control law directly speciﬁes the probability distribution over next states, while in the other case the control law shifts the mean of the distribution given by the passive dynamics. [sent-83, score-1.095]
</p><p>44 (A) An LQG problem with quadratic cost-to-go and linear feedback control law. [sent-86, score-0.399]
</p><p>45 This composite problem is no longer LQG because it has non-quadratic ﬁnal cost (i. [sent-89, score-0.299]
</p><p>46 Applying the results from the previous section, the desirability for the composite problem is µ ¶ P 1 T z (x, t) = k wk exp − x Vk (t) x − αk (t) 2  The optimal control law can now be obtained directly from (3), or via composition from (9). [sent-93, score-1.192]
</p><p>47 Note that the constants αk (t) do not affect the component control laws (and indeed are rarely computed in the LQG framework) however they affect the composite control law through the mixing weights. [sent-94, score-1.277]
</p><p>48 We illustrate the above construction on a scalar example with integrator dynamics dx = udt+0. [sent-95, score-0.186]
</p><p>49 The component ﬁnal costs are of the form dk fk (x) = (x − ck )2 2 In order to center these quadratics at ck rather than 0 we augment the state: x = [x; 1]. [sent-100, score-0.31]
</p><p>50 Fig 1 shows the optimal cost-to-go functions v (x, t) = − log (z (x, t)) and the optimal control laws u∗ (x, t) for the following problems: {c = 0; d = 5}, {c = −1, 0, 1; d = 5, 0. [sent-102, score-0.621]
</p><p>51 As expected the cost-to-go is quadratic and the control law is linear with time-varying gain. [sent-108, score-0.532]
</p><p>52 The control law is no longer linear but instead has an elaborate shape. [sent-110, score-0.492]
</p><p>53 The third problem (Fig 1C) resembles robust control in the sense that there is a f1at region where all states are equally good. [sent-111, score-0.395]
</p><p>54 The corresponding control law uses feedback to push the state into this f1at region. [sent-112, score-0.575]
</p><p>55 Inside the region the controller does nothing, so as to save energy. [sent-113, score-0.16]
</p><p>56 5  5 Constructing minimal sets of primitives via SVD of Green’s function We showed how composite problems can be solved once the solutions to the component problems are available. [sent-115, score-0.593]
</p><p>57 The choice of component boundary conditions deﬁnes the manifold (6) of problems that can be solved exactly. [sent-116, score-0.188]
</p><p>58 Recall that the vector of desirability values z (x) at interior states x ∈ I, which we denoted zI , satisﬁes the linear equation (2). [sent-120, score-0.344]
</p><p>59 A minimal set of primitives corresponds to the best low-rank approximation to G. [sent-123, score-0.211]
</p><p>60 If we deﬁne "best" in terms of least squares, a minimal set of R primitives is obtained by approximating G using the top R singular values: G ≈ U SV T S is an R-by-R diagonal matrix, U and V are |I|-by-R and |B|-by-R orthonormal matrices. [sent-124, score-0.259]
</p><p>61 If we now set zB = V·r , which is the r-th column of V , then zI = G zB ≈ U SV T V·r = Srr U·r Thus the right singular vectors (columns of V ) are the component boundary conditions, while the left singular vectors (columns of U ) are the component solutions. [sent-125, score-0.28]
</p><p>62 The above construction does not use knowledge of the family of composite problems we aim to solve/approximate. [sent-126, score-0.3]
</p><p>63 The discretization involves |I| = 24520 interior states and |B| = 4163 boundary states. [sent-141, score-0.242]
</p><p>64 The parametric family of ﬁnal costs is f (x, θ) = 13 − 13 exp (5 cos (atan 2 (x2 , x1 ) − θ) − 5) This is an inverted von Mises function specifying the desired location where the state should exit the disk. [sent-142, score-0.158]
</p><p>65 The desirability function z is well approximated in both cases. [sent-149, score-0.214]
</p><p>66 The difﬁculty comes from the fact that the components are not always positive, and as a result the composite solution is not always positive. [sent-152, score-0.251]
</p><p>67 It is notable that the higher-order components (corresponding to smaller singular values) are only modulated near the boundary – which explains why the approximation errors in Fig 2B are near the boundary. [sent-158, score-0.181]
</p><p>68 In summary, a small number of components are sufﬁcient to construct composite control laws which are near-optimal in most of the state space. [sent-159, score-0.777]
</p><p>69 6  Figure 2: Illustration of primitives obtained via SVD. [sent-162, score-0.183]
</p><p>70 (B) Speed proﬁles for the movements shown in (A). [sent-169, score-0.158]
</p><p>71 Note that the same controller generates movements of different duration. [sent-170, score-0.318]
</p><p>72 (C) Hand paths generated by a composite controller obtained by mixing the optimal controllers for two targets. [sent-171, score-0.62]
</p><p>73 This controller "decides" online which target to go to. [sent-172, score-0.16]
</p><p>74 7  6 Application to arm movements We are currently working on an optimal control model of arm movements based on compositionality. [sent-173, score-0.853]
</p><p>75 The dynamics correspond to a 2-link arm moving in the horizontal plane, and have the form ³ ´ ¨ ˙ τ = M (θ) θ + n θ, θ  θ contains the shoulder and elbow joint angles, τ is the applied torque, M is the conﬁgurationdependent inertia, and n is the vector of Coriolis, centripetal and viscous forces. [sent-174, score-0.191]
</p><p>76 This augmentation is needed in order to express reaching movements as a ﬁrst-exit problem. [sent-181, score-0.199]
</p><p>77 Without it the movement would stop whenever the instantaneous speed becomes zero – which can happen at reversal points as well as the starting point. [sent-182, score-0.152]
</p><p>78 Note that most models of reaching movements have assumed predeﬁned ﬁnal time. [sent-183, score-0.199]
</p><p>79 However this is unrealistic because we know that movement duration scales with distance, and furthermore such scaling takes place online (i. [sent-184, score-0.145]
</p><p>80 The above second-order system is expressed in general ﬁrst-order form, and then the passive dynamics corresponding to τ = 0 are discretized in space and time. [sent-187, score-0.235]
</p><p>81 Thus we have around 20 million discrete states, and the matrix P characterizing the passive dynamics is 20 million - by - 20 million. [sent-192, score-0.264]
</p><p>82 The speed proﬁles for these movements are shown in Fig 3B. [sent-196, score-0.194]
</p><p>83 In particular, it is known that human reaching movements of different amplitude have similar speed proﬁles around movement onset, and diverge later. [sent-198, score-0.377]
</p><p>84 Fig 3C shows results for a composite controller obtained by mixing the optimal control laws for two different targets. [sent-199, score-0.979]
</p><p>85 In this example the targets are sufﬁciently far away and the ﬁnal costs are sufﬁciently steep, thus the mixing yields a switching controller instead of an interpolating controller. [sent-200, score-0.33]
</p><p>86 Depending on the starting point, this controller takes the hand to one or the other target, and can also switch online if the hand is perturbed. [sent-201, score-0.16]
</p><p>87 An interpolating controller can be created by placing the targets closer or making the component ﬁnal costs less steep. [sent-202, score-0.32]
</p><p>88 In future work we will explore this model in more detail and also build a more realistic model using 3rd-order dynamics (incorporating muscle time constants). [sent-204, score-0.163]
</p><p>89 7 Summary and relation to prior work We developed a theory of compositionality applicable to a general class of stochastic optimal control problems. [sent-206, score-0.731]
</p><p>90 Although in this paper we used simple examples, the potential of such compositionality to tackle complex control problems seems clear. [sent-207, score-0.662]
</p><p>91 While the motivation is similar, PVFs are based on intuitions (mostly from grid worlds divided into rooms) rather than mathematical results regarding optimality of the composite solution. [sent-211, score-0.263]
</p><p>92 Another difference is that PVFs do not take into account the cost rate q and the boundary B. [sent-213, score-0.181]
</p><p>93 A particularly important point made by [6] is that the primitives can be only approximately optimal (in this case obtained via local LQG approximations), and yet their combination still produces good results. [sent-218, score-0.304]
</p><p>94 Maggioni, “Proto-value functions: A Laplacian farmework for learning representation and control in Markov decision processes,” Journal of Machine Learning Research, vol. [sent-238, score-0.329]
</p><p>95 Popovic, “Linear bellman combination for control of character animation,” To appear in SIGGRAPH, 2009. [sent-244, score-0.454]
</p><p>96 Bizzi, “The construction of movement by the spinal cord,” Nature Neuroscience, vol. [sent-273, score-0.15]
</p><p>97 Bizzi, “Combinations of muscle synergies in the construction of a natural motor behavior,” Nat. [sent-280, score-0.199]
</p><p>98 [16] ——, “General duality between optimal control and estimation,” IEEE Conference on Decision and Control, 2008. [sent-295, score-0.403]
</p><p>99 Kappen, “Linear theory for control of nonlinear stochastic systems,” Physical Review Letters, vol. [sent-303, score-0.368]
</p><p>100 Todorov, “Eigen-function approximation methods for linearly-solvable optimal control problems,” IEEE International Symposium on Adaptive Dynamic Programming and Reinforcemenet Learning, 2009. [sent-306, score-0.403]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('control', 0.329), ('compositionality', 0.289), ('lqg', 0.289), ('zk', 0.269), ('composite', 0.222), ('desirability', 0.214), ('primitives', 0.183), ('fig', 0.181), ('law', 0.163), ('controller', 0.16), ('movements', 0.158), ('wk', 0.152), ('vk', 0.148), ('laws', 0.144), ('bellman', 0.125), ('dynamics', 0.124), ('movement', 0.116), ('pvfs', 0.111), ('todorov', 0.111), ('passive', 0.111), ('zb', 0.107), ('fk', 0.105), ('boundary', 0.104), ('controllers', 0.083), ('nal', 0.081), ('mk', 0.079), ('cost', 0.077), ('ws', 0.075), ('optimal', 0.074), ('svd', 0.072), ('motor', 0.068), ('zs', 0.067), ('arm', 0.067), ('costs', 0.067), ('udt', 0.067), ('synergies', 0.058), ('state', 0.053), ('zi', 0.051), ('mixing', 0.05), ('pii', 0.05), ('interior', 0.049), ('ck', 0.049), ('discretization', 0.048), ('solver', 0.048), ('singular', 0.048), ('yet', 0.047), ('agendas', 0.044), ('kuk', 0.044), ('odes', 0.044), ('pib', 0.044), ('saltiel', 0.044), ('torque', 0.044), ('problems', 0.044), ('reaching', 0.041), ('states', 0.041), ('optimality', 0.041), ('kl', 0.041), ('equation', 0.04), ('component', 0.04), ('quadratic', 0.04), ('linearity', 0.039), ('stochastic', 0.039), ('agenda', 0.039), ('bizzi', 0.039), ('muscle', 0.039), ('exp', 0.038), ('zx', 0.036), ('speed', 0.036), ('formulations', 0.035), ('les', 0.035), ('qi', 0.035), ('construction', 0.034), ('sensorimotor', 0.033), ('solutions', 0.032), ('cartesian', 0.032), ('bertsekas', 0.032), ('spaces', 0.031), ('paths', 0.031), ('pursuing', 0.03), ('pro', 0.03), ('feedback', 0.03), ('components', 0.029), ('discretize', 0.029), ('duration', 0.029), ('resemble', 0.029), ('barto', 0.029), ('discrete', 0.029), ('differential', 0.029), ('minimal', 0.028), ('dx', 0.028), ('preliminary', 0.027), ('athena', 0.027), ('interpolating', 0.027), ('gaussians', 0.026), ('amplitude', 0.026), ('targets', 0.026), ('sv', 0.025), ('diag', 0.025), ('resembles', 0.025), ('diffusion', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="54-tfidf-1" href="./nips-2009-Compositionality_of_optimal_control_laws.html">54 nips-2009-Compositionality of optimal control laws</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties – one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of LQG controllers) without requiring the ﬁnal cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Green’s function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the ﬁeld of motor control, yet they have been difﬁcult to reconcile. Our work makes this possible.</p><p>2 0.13267504 <a title="54-tfidf-2" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or “causes”) of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these neurons will ﬁre after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artiﬁcial neural networks. 1</p><p>3 0.1318593 <a title="54-tfidf-3" href="./nips-2009-Convergent_Temporal-Difference_Learning_with_Arbitrary_Smooth_Function_Approximation.html">60 nips-2009-Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></p>
<p>Author: Shalabh Bhatnagar, Doina Precup, David Silver, Richard S. Sutton, Hamid R. Maei, Csaba Szepesvári</p><p>Abstract: We introduce the ﬁrst temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(λ), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any ﬁnite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms’ effectiveness. 1</p><p>4 0.11574566 <a title="54-tfidf-4" href="./nips-2009-Unsupervised_Feature_Selection_for_the_%24k%24-means_Clustering_Problem.html">252 nips-2009-Unsupervised Feature Selection for the $k$-means Clustering Problem</a></p>
<p>Author: Christos Boutsidis, Petros Drineas, Michael W. Mahoney</p><p>Abstract: We present a novel feature selection algorithm for the k-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter ϵ ∈ (0, 1), selects and appropriately rescales in an unsupervised manner Θ(k log(k/ϵ)/ϵ2 ) features from a dataset of arbitrary dimensions. We prove that, if we run any γ-approximate k-means algorithm (γ ≥ 1) on the features selected using our method, we can ﬁnd a (1 + (1 + ϵ)γ)-approximate partition with high probability. 1</p><p>5 0.08417809 <a title="54-tfidf-5" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>Author: Steven Chase, Andrew Schwartz, Wolfgang Maass, Robert A. Legenstein</p><p>Abstract: The control of neuroprosthetic devices from the activity of motor cortex neurons beneﬁts from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is ﬁtted to experimental data, the model produces learning effects similar to those found in monkey experiments.</p><p>6 0.072656743 <a title="54-tfidf-6" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>7 0.070765726 <a title="54-tfidf-7" href="./nips-2009-Learning_to_Rank_by_Optimizing_NDCG_Measure.html">136 nips-2009-Learning to Rank by Optimizing NDCG Measure</a></p>
<p>8 0.064930104 <a title="54-tfidf-8" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>9 0.060829759 <a title="54-tfidf-9" href="./nips-2009-Robust_Value_Function_Approximation_Using_Bilinear_Programming.html">209 nips-2009-Robust Value Function Approximation Using Bilinear Programming</a></p>
<p>10 0.05939129 <a title="54-tfidf-10" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>11 0.059097465 <a title="54-tfidf-11" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>12 0.056941144 <a title="54-tfidf-12" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>13 0.053273838 <a title="54-tfidf-13" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>14 0.049007781 <a title="54-tfidf-14" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>15 0.04890554 <a title="54-tfidf-15" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>16 0.048703048 <a title="54-tfidf-16" href="./nips-2009-Solving_Stochastic_Games.html">221 nips-2009-Solving Stochastic Games</a></p>
<p>17 0.048643723 <a title="54-tfidf-17" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>18 0.045536406 <a title="54-tfidf-18" href="./nips-2009-Statistical_Analysis_of_Semi-Supervised_Learning%3A_The_Limit_of_Infinite_Unlabelled_Data.html">229 nips-2009-Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></p>
<p>19 0.043601625 <a title="54-tfidf-19" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>20 0.043584544 <a title="54-tfidf-20" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.155), (1, 0.007), (2, 0.09), (3, -0.017), (4, -0.047), (5, -0.009), (6, 0.01), (7, -0.02), (8, 0.019), (9, -0.017), (10, 0.012), (11, 0.032), (12, -0.001), (13, -0.01), (14, -0.04), (15, -0.061), (16, -0.033), (17, -0.123), (18, -0.056), (19, -0.029), (20, 0.056), (21, -0.064), (22, 0.078), (23, -0.078), (24, -0.112), (25, -0.069), (26, -0.169), (27, 0.036), (28, -0.133), (29, 0.074), (30, -0.043), (31, -0.093), (32, 0.007), (33, -0.035), (34, -0.032), (35, -0.151), (36, 0.008), (37, -0.109), (38, 0.104), (39, -0.046), (40, 0.03), (41, 0.028), (42, -0.035), (43, -0.026), (44, 0.006), (45, 0.075), (46, 0.011), (47, 0.203), (48, 0.107), (49, -0.224)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95679891 <a title="54-lsi-1" href="./nips-2009-Compositionality_of_optimal_control_laws.html">54 nips-2009-Compositionality of optimal control laws</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties – one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of LQG controllers) without requiring the ﬁnal cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Green’s function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the ﬁeld of motor control, yet they have been difﬁcult to reconcile. Our work makes this possible.</p><p>2 0.6484319 <a title="54-lsi-2" href="./nips-2009-Convergent_Temporal-Difference_Learning_with_Arbitrary_Smooth_Function_Approximation.html">60 nips-2009-Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></p>
<p>Author: Shalabh Bhatnagar, Doina Precup, David Silver, Richard S. Sutton, Hamid R. Maei, Csaba Szepesvári</p><p>Abstract: We introduce the ﬁrst temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(λ), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any ﬁnite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms’ effectiveness. 1</p><p>3 0.55124724 <a title="54-lsi-3" href="./nips-2009-Unsupervised_Feature_Selection_for_the_%24k%24-means_Clustering_Problem.html">252 nips-2009-Unsupervised Feature Selection for the $k$-means Clustering Problem</a></p>
<p>Author: Christos Boutsidis, Petros Drineas, Michael W. Mahoney</p><p>Abstract: We present a novel feature selection algorithm for the k-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter ϵ ∈ (0, 1), selects and appropriately rescales in an unsupervised manner Θ(k log(k/ϵ)/ϵ2 ) features from a dataset of arbitrary dimensions. We prove that, if we run any γ-approximate k-means algorithm (γ ≥ 1) on the features selected using our method, we can ﬁnd a (1 + (1 + ϵ)γ)-approximate partition with high probability. 1</p><p>4 0.5131768 <a title="54-lsi-4" href="./nips-2009-Multi-Step_Dyna_Planning_for_Policy_Evaluation_and_Control.html">159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</a></p>
<p>Author: Hengshuai Yao, Shalabh Bhatnagar, Dongcui Diao, Richard S. Sutton, Csaba Szepesvári</p><p>Abstract: In this paper we introduce a multi-step linear Dyna-style planning algorithm. The key element of the multi-step linear Dyna is a multi-step linear model that enables multi-step projection of a sampled feature and multi-step planning based on the simulated multi-step transition experience. We propose two multi-step linear models. The ﬁrst iterates the one-step linear model, but is generally computationally complex. The second interpolates between the one-step model and the inﬁnite-step model (which turns out to be the LSTD solution), and can be learned efﬁciently online. Policy evaluation on Boyan Chain shows that multi-step linear Dyna learns a policy faster than single-step linear Dyna, and generally learns faster as the number of projection steps increases. Results on Mountain-car show that multi-step linear Dyna leads to much better online performance than single-step linear Dyna and model-free algorithms; however, the performance of multi-step linear Dyna does not always improve as the number of projection steps increases. Our results also suggest that previous attempts on extending LSTD for online control were unsuccessful because LSTD looks inﬁnite steps into the future, and suffers from the model errors in non-stationary (control) environments.</p><p>5 0.45944309 <a title="54-lsi-5" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or “causes”) of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these neurons will ﬁre after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artiﬁcial neural networks. 1</p><p>6 0.43250811 <a title="54-lsi-6" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>7 0.42884615 <a title="54-lsi-7" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>8 0.39852712 <a title="54-lsi-8" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>9 0.3906886 <a title="54-lsi-9" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>10 0.36226693 <a title="54-lsi-10" href="./nips-2009-Robust_Value_Function_Approximation_Using_Bilinear_Programming.html">209 nips-2009-Robust Value Function Approximation Using Bilinear Programming</a></p>
<p>11 0.35418764 <a title="54-lsi-11" href="./nips-2009-Toward_Provably_Correct_Feature_Selection_in_Arbitrary_Domains.html">248 nips-2009-Toward Provably Correct Feature Selection in Arbitrary Domains</a></p>
<p>12 0.32772908 <a title="54-lsi-12" href="./nips-2009-Potential-Based_Agnostic_Boosting.html">193 nips-2009-Potential-Based Agnostic Boosting</a></p>
<p>13 0.32680777 <a title="54-lsi-13" href="./nips-2009-Grouped_Orthogonal_Matching_Pursuit_for_Variable_Selection_and_Prediction.html">105 nips-2009-Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</a></p>
<p>14 0.32197401 <a title="54-lsi-14" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>15 0.32070464 <a title="54-lsi-15" href="./nips-2009-A_Generalized_Natural_Actor-Critic_Algorithm.html">12 nips-2009-A Generalized Natural Actor-Critic Algorithm</a></p>
<p>16 0.31948975 <a title="54-lsi-16" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>17 0.31466302 <a title="54-lsi-17" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>18 0.3124837 <a title="54-lsi-18" href="./nips-2009-A_Smoothed_Approximate_Linear_Program.html">16 nips-2009-A Smoothed Approximate Linear Program</a></p>
<p>19 0.30071384 <a title="54-lsi-19" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>20 0.29464048 <a title="54-lsi-20" href="./nips-2009-Multiple_Incremental_Decremental_Learning_of_Support_Vector_Machines.html">160 nips-2009-Multiple Incremental Decremental Learning of Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.041), (25, 0.048), (35, 0.043), (36, 0.076), (39, 0.505), (58, 0.066), (61, 0.042), (71, 0.035), (81, 0.014), (86, 0.044), (91, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96926969 <a title="54-lda-1" href="./nips-2009-Hierarchical_Learning_of_Dimensional_Biases_in_Human_Categorization.html">109 nips-2009-Hierarchical Learning of Dimensional Biases in Human Categorization</a></p>
<p>Author: Adam Sanborn, Nick Chater, Katherine A. Heller</p><p>Abstract: Existing models of categorization typically represent to-be-classiﬁed items as points in a multidimensional space. While from a mathematical point of view, an inﬁnite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions – and have a strong preference to generalize along the axes of these dimensions, but not “diagonally”. What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Speciﬁcally, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. The learning behaviour of the model captures the developmental shift from roughly “isotropic” for children to the axis-aligned generalization that adults show. 1</p><p>same-paper 2 0.9388569 <a title="54-lda-2" href="./nips-2009-Compositionality_of_optimal_control_laws.html">54 nips-2009-Compositionality of optimal control laws</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties – one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of LQG controllers) without requiring the ﬁnal cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Green’s function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the ﬁeld of motor control, yet they have been difﬁcult to reconcile. Our work makes this possible.</p><p>3 0.90449131 <a title="54-lda-3" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>Author: Jing Gao, Feng Liang, Wei Fan, Yizhou Sun, Jiawei Han</p><p>Abstract: Ensemble classiﬁers such as bagging, boosting and model averaging are known to have improved accuracy and robustness over a single model. Their potential, however, is limited in applications which have no access to raw data but to the meta-level model output. In this paper, we study ensemble learning with output from multiple supervised and unsupervised models, a topic where little work has been done. Although unsupervised models, such as clustering, do not directly generate label prediction for each individual, they provide useful constraints for the joint prediction of a set of related objects. We propose to consolidate a classiﬁcation solution by maximizing the consensus among both supervised predictions and unsupervised constraints. We cast this ensemble task as an optimization problem on a bipartite graph, where the objective function favors the smoothness of the prediction over the graph, as well as penalizing deviations from the initial labeling provided by supervised models. We solve this problem through iterative propagation of probability estimates among neighboring nodes. Our method can also be interpreted as conducting a constrained embedding in a transformed space, or a ranking on the graph. Experimental results on three real applications demonstrate the beneﬁts of the proposed method over existing alternatives1 . 1</p><p>4 0.87342262 <a title="54-lda-4" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>Author: Gunhee Kim, Antonio Torralba</p><p>Abstract: This paper proposes a fast and scalable alternating optimization technique to detect regions of interest (ROIs) in cluttered Web images without labels. The proposed approach discovers highly probable regions of object instances by iteratively repeating the following two functions: (1) choose the exemplar set (i.e. a small number of highly ranked reference ROIs) across the dataset and (2) reﬁne the ROIs of each image with respect to the exemplar set. These two subproblems are formulated as ranking in two different similarity networks of ROI hypotheses by link analysis. The experiments with the PASCAL 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods. Also, we test the scalability of our approach with ﬁve objects in Flickr dataset consisting of more than 200K images. 1</p><p>5 0.86665589 <a title="54-lda-5" href="./nips-2009-Hierarchical_Mixture_of_Classification_Experts_Uncovers_Interactions_between_Brain_Regions.html">110 nips-2009-Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions</a></p>
<p>Author: Bangpeng Yao, Dirk Walther, Diane Beck, Li Fei-fei</p><p>Abstract: The human brain can be described as containing a number of functional regions. These regions, as well as the connections between them, play a key role in information processing in the brain. However, most existing multi-voxel pattern analysis approaches either treat multiple regions as one large uniform region or several independent regions, ignoring the connections between them. In this paper we propose to model such connections in an Hidden Conditional Random Field (HCRF) framework, where the classiďŹ er of one region of interest (ROI) makes predictions based on not only its voxels but also the predictions from ROIs that it connects to. Furthermore, we propose a structural learning method in the HCRF framework to automatically uncover the connections between ROIs. We illustrate this approach with fMRI data acquired while human subjects viewed images of different natural scene categories and show that our model can improve the top-level (the classiďŹ er combining information from all ROIs) and ROI-level prediction accuracy, as well as uncover some meaningful connections between ROIs. 1</p><p>6 0.63511401 <a title="54-lda-6" href="./nips-2009-Abstraction_and_Relational_learning.html">21 nips-2009-Abstraction and Relational learning</a></p>
<p>7 0.61478657 <a title="54-lda-7" href="./nips-2009-Matrix_Completion_from_Power-Law_Distributed_Samples.html">148 nips-2009-Matrix Completion from Power-Law Distributed Samples</a></p>
<p>8 0.58708107 <a title="54-lda-8" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>9 0.58690196 <a title="54-lda-9" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>10 0.58205253 <a title="54-lda-10" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>11 0.57746875 <a title="54-lda-11" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>12 0.56800759 <a title="54-lda-12" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>13 0.5637452 <a title="54-lda-13" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>14 0.56263679 <a title="54-lda-14" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>15 0.5529716 <a title="54-lda-15" href="./nips-2009-Learning_Brain_Connectivity_of_Alzheimer%27s_Disease_from_Neuroimaging_Data.html">125 nips-2009-Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data</a></p>
<p>16 0.55256754 <a title="54-lda-16" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>17 0.54756325 <a title="54-lda-17" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>18 0.53909123 <a title="54-lda-18" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>19 0.53771186 <a title="54-lda-19" href="./nips-2009-Exploring_Functional_Connectivities_of_the_Human_Brain_using_Multivariate_Information_Analysis.html">86 nips-2009-Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis</a></p>
<p>20 0.53216434 <a title="54-lda-20" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
