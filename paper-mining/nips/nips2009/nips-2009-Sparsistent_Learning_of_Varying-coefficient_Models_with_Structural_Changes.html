<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-225" href="#">nips2009-225</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</h1>
<br/><p>Source: <a title="nips-2009-225-pdf" href="http://papers.nips.cc/paper/3836-sparsistent-learning-of-varying-coefficient-models-with-structural-changes.pdf">pdf</a></p><p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>Reference: <a title="nips-2009-225-reference" href="../nips2009_reference/nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. [sent-4, score-0.257]
</p><p>2 In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. [sent-5, score-0.133]
</p><p>3 We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i. [sent-6, score-0.424]
</p><p>4 We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. [sent-9, score-0.985]
</p><p>5 We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. [sent-10, score-0.137]
</p><p>6 We also consider how this applies to structure estimation of time-varying probabilistic graphical models. [sent-12, score-0.122]
</p><p>7 , n, such as the prices of a set of stocks at time i, or the signals from some sensors deployed at location i; the noise ǫ1 , . [sent-22, score-0.13]
</p><p>8 Varying-coefﬁcient models are a non-parametric extension to the linear regression models, which unlike other non-parametric models, assume that there is a linear relationship (generalizable to log-linear relationship) between the feature variables and the output variable, albeit a changing one. [sent-33, score-0.121]
</p><p>9 At different blocks only covariates with non-zero coefﬁcient affect the response, e. [sent-85, score-0.313]
</p><p>10 (b) Schematic representation of the covariates affecting the response during the second block in panel (a), which is reminiscent of neighborhood selection in graph structure learning. [sent-90, score-0.503]
</p><p>11 (c) and (d) Application of VCVS for graph structure estimation (see Section 7) of non-piecewise constant evolving graphs. [sent-91, score-0.206]
</p><p>12 In this paper, we analyze VCVS as functions of time, and the main goal is to estimate the dynamic structure and jump points of the unknown vector function β(t). [sent-93, score-0.542]
</p><p>13 < TB = 1}, 1 < B ≤ n, of the time interval (scaled to) [0, 1], such that β(t) = γj , t ∈ [Tj−1 , Tj ) for some constant vectors γj ∈ Rp , j = 1, . [sent-100, score-0.141]
</p><p>14 Furthermore, we assume that at each time point ti only a few covariates affect the response, i. [sent-108, score-0.621]
</p><p>15 A good estimation procedure would be able to identify the correct partition of the interval [0, 1] so that within each segment the coefﬁcient function is constant. [sent-111, score-0.362]
</p><p>16 This estimation problem is particularly important in applications where one needs to uncover dynamic relational information or model structures from time series data. [sent-115, score-0.199]
</p><p>17 Another important problem is to identify structural changes in ﬁelds such as signal processing, EEG segmentation and analysis of seismic signals. [sent-117, score-0.216]
</p><p>18 In all these problems, the goal is not to estimate the optimum value of β(t) for predicting Y , but to consistently uncover the zero and non-zero patterns in β(t) at time points of interest that reveal the changing structure of the model. [sent-118, score-0.255]
</p><p>19 Our problem is remotely related to, but very different from, earlier works on linear regression models with structural changes [4], and the problem of change-point detection (e. [sent-120, score-0.303]
</p><p>20 A number of existing methods are available to identify only one structural change in the data; in order to identify multiple changes these methods can be applied sequentially on smaller intervals that are assumed to harbor only one change [14]. [sent-123, score-0.296]
</p><p>21 Another common approach is to assume that there are K changes and use Dynamic Programming to estimate them [4]. [sent-124, score-0.117]
</p><p>22 In this paper, we propose and analyze a penalized least squares approach, which automatically adapts to the unknown number of structural changes present in the data and performs the variable selection on each of the constant regions. [sent-125, score-0.386]
</p><p>23 , βk is a spline function, with potential jump points at observation times ti , i = 1, . [sent-133, score-0.783]
</p><p>24 In this particular case, the total variation penalty deﬁned above allows us to conceptualize βk as a vector in Rn , whose components βk,i ≡ βk (ti ) correspond to function values at ti , i = 1, . [sent-137, score-0.437]
</p><p>25 Observe that ℓ1 penalty encourages sparsity of the signal at each time point and enables a selection over the relevant coefﬁcients; whereas the total variation penalty ˆ is used to partition the interval [0, 1] so that βk is constant within each segment. [sent-147, score-0.545]
</p><p>26 , B, denote the set of time points that fall into the interval [Tj−1 , Tj ); when the meaning is clear from the context, we also use Bj as a shorthand of this interval. [sent-157, score-0.176]
</p><p>27 For example, XBj and YBj represent the submatrix of X and subvector of Y , respectively, that include elements only ˆ corresponding to time points within interval Bj . [sent-158, score-0.176]
</p><p>28 , T ˆ } of [0, 1] (possibly a trivial one) and unique vectors γj ∈ Rp , j = ˆ ˆ block partition T ˆ B ˆ ˆ ˆ 1, . [sent-163, score-0.132]
</p><p>29 The set of relevant covariates during inverval Bj , i. [sent-167, score-0.306]
</p><p>30 The larger the estimated segments, the smaller the relative inﬂuence of the bias from the total variation, while the magnitude of the bias introduced by the ℓ1 penalty is uniform across different segments. [sent-184, score-0.126]
</p><p>31 The additional bias coming from the total variation penalty was also noted in the problem of signal denoising [23]. [sent-185, score-0.117]
</p><p>32 3  A two-step procedure for estimating time-varying structures  In this section, we propose a new algorithm for estimating the time-varying structure of the varyingcoefﬁcient model in Eq. [sent-187, score-0.205]
</p><p>33 Estimate the block partition T , on which the coefﬁcient vector is constant within each block. [sent-191, score-0.168]
</p><p>34 This can be obtained by minimizing the following objective: p  n i=1  2  (Yi − X′ β(ti )) + 2λ2 i  k=1  ||βk ||TV ,  (7)  which we refer to as a temporal difference (TD) regression for reasons that will be clear shortly. [sent-192, score-0.123]
</p><p>35 (7) and turn it into an ℓ1 -regularized regression problem, and solve it using the randomized Lasso. [sent-194, score-0.156]
</p><p>36 For each block of the partition, Bj , 1 ≤ j ≤ B, estimate γj by minimizing the Lasso ˆ objective within the block: γj = argmin ˆ γ∈Rp  ˆ ti ∈Bj  (Yi − X′ γ)2 + 2λ1 ||γ||1 . [sent-197, score-0.494]
</p><p>37 i  (8)  We name this procedure TDB-Lasso (or TDBL), after the two steps (TD randomized Lasso, and Lasso within Blocks) given above. [sent-198, score-0.115]
</p><p>38 (7) ˆ into an equivalent ℓ1 penalized regression problem, which allows us to cast the T estimation † problem as a feature selection problem. [sent-204, score-0.302]
</p><p>39 Let βk,i denote the temporal difference between the regression coefﬁcients corresponding to the same covariate k at successive time points ti−1 and † ti : βk,i ≡ βk (ti ) − βk (ti−1 ), k = 1, . [sent-205, score-0.515]
</p><p>40 (9)  This transformation was proposed in [8] in the context of one-dimensional signal denoising, however, we are interested in the estimation of jump points in the context of time-varying coefﬁcient model. [sent-226, score-0.544]
</p><p>41 To deal with the problem of robustness, we employed the stability selection procedure of [22] (see also the bootstrap Lasso [2], however, we have decided to use the stability selection because of the weaker assumptions). [sent-231, score-0.369]
</p><p>42 The stability selection approach to estimating the jump-points is comprised of two main components: i) simulating multiple datasets using bootstrap, and ii) using the randomized Lasso outlined in Algorithm 1 (see also Appendix) to solve (9). [sent-232, score-0.266]
</p><p>43 While the bootstrap step improves the robustness of the estimator, ˆ the randomized Lasso weakens the conditions under which the estimator β † selects exactly the true features. [sent-233, score-0.167]
</p><p>44 We obtain a stable estimate of the support by selecting variables that appear in multiple supports M b=1  ˆ 1 ∈ Jb† } I{k ≥ τ }, (10) M ˆ which is then used to obtain the block partition estimate T . [sent-237, score-0.208]
</p><p>45 The parameter τ is a tuning parameter that controls the number of falsely identiﬁed jump points. [sent-238, score-0.432]
</p><p>46 1  Estimating jump points  We ﬁrst address the issue of estimating jump points by analyzing the transformed TD-regression problem Eq. [sent-242, score-0.985]
</p><p>47 To prove that all the jump points are included in J τ , we ﬁrst state a sparse eigenvalue condition on the design (e. [sent-245, score-0.463]
</p><p>48 3/2 ϕmin (CJ 2 , X† ) This condition guarantees a correlation structure between TD-transformed covariates that allows for detection of the jump points. [sent-256, score-0.697]
</p><p>49 Comparing to the irrepresentible condition [30, 21, 27], necessary for the ordinary Lasso to perform feature selection, condition A1 is much weaker [22] and is sufﬁcient for the randomized Lasso to select the relevant feature with high probability (see also [26]). [sent-257, score-0.151]
</p><p>50 If the minimum size of the jump is bounded away from zero as † min |βk | ≥ 0. [sent-259, score-0.433]
</p><p>51 3(CJ)3/2 λmin ,  k∈J †  √ where λmin = 2σ † ( CJ + 1)  (13)  and σ † ≥ V ar(Yi† ), for np > 10 and J ≥ 7, there exists ˆ some δ = δJ ∈ (0, 1) such that for all τ ≥ 1 − δ, the collection of the estimated jump points J τ satisﬁes, ˆ P(J τ = J † ) ≥ 1 − 5/np. [sent-260, score-0.552]
</p><p>52 (14) log np n  2  Remark: Note that Theorem 1 gives conditions under which we can recover every jump point in every covariates. [sent-261, score-0.432]
</p><p>53 In particular, there are no assumptions on the number of covariates that change values at a jump point. [sent-262, score-0.696]
</p><p>54 Assuming that multiple covariates change their values at a jump point, we could further relax the condition on the minimal size of a jump given in Eq. [sent-263, score-1.088]
</p><p>55 It was also pointed to us that the framework of [18] may be a more natural way to estimate jump points. [sent-265, score-0.43]
</p><p>56 2  Identifying correct covariates  Now we address the issue of selecting the relevant features for every estimated segment. [sent-267, score-0.39]
</p><p>57 Under the conditions of Theorem 1, correct jump points will be detected with probability arbitrarily close to 1. [sent-268, score-0.498]
</p><p>58 That means under the assumption A1, we can run the regular Lasso on each of the estimated segments to select the relevant features therein. [sent-269, score-0.134]
</p><p>59 (15)  The assumption A2 is a mild version of the mutual coherence condition used in [7], which is necesˆ sary for identiﬁcation of the relevant covariates in each segment. [sent-273, score-0.306]
</p><p>60 Then for a sequence δ = δn → 0, λ1 ≥ 4Lσ  ln 4Kp ln 2Kp δ δ ∨ 8L ρ ρ  we have  and  min  min |γj,k | ≥ 2λ1 ,  1≤j≤B k∈SBj  ˆ lim P(B = B) = 1,  (16)  lim max P(||ˆj − γj ||1 = 0) = 1, γ  (17)  n→∞ n→∞ 1≤j≤B  lim  ˆ min P(SBj = SBj ) = 1. [sent-282, score-0.123]
</p><p>61 , it selects the correct jump points and for each segment between two jump points it is able to select the correct covariates. [sent-285, score-1.059]
</p><p>62 5  Practical considerations  As in standard Lasso, the regularization parameters in TDB-Lasso need to be tuned appropriately to attain correct structural recovery. [sent-287, score-0.172]
</p><p>63 The TD regression procedure requires three parameters: the penalty parameter λ2 , cut-off parameter τ , and weakness parameter α. [sent-288, score-0.29]
</p><p>64 From our empirical experiˆ ence, the recovered set of jump points T vary very little with respect to these parameters in a wide range. [sent-289, score-0.463]
</p><p>65 Theorem 1 in [22] gives a way to select the cutoff τ while controlling the number of falsely included jump points. [sent-291, score-0.475]
</p><p>66 The weakness parameter can be chosen in quite a large interval (see Appendix on the randomized Lasso) and we report our results using the values α = 0. [sent-293, score-0.217]
</p><p>67 (8) on each estimated segment to select relevant variables, which requires a choice of the penalty parameter λ1 . [sent-296, score-0.231]
</p><p>68 In cases where the assumptions are violated, the resulting set of estimated jump points is larger than the true set T , e. [sent-299, score-0.512]
</p><p>69 the ˆ points close to the true jump points get included into the resulting estimate T . [sent-301, score-0.572]
</p><p>70 We propose to use an ad hoc heuristic to reﬁne the initially selected set of jump points. [sent-302, score-0.392]
</p><p>71 A commonly used procedure for estimation of linear regression models with structural changes [3] is a dynamic programming method that considers a possible structural change at every location ti , i = 1, . [sent-303, score-0.927]
</p><p>72 We modify this method to consider jump points only ˆ ˆ in the estimated set T and thus considerably reducing the computational complexity to O(|T |2 ), ˆ | ≪ n. [sent-307, score-0.512]
</p><p>73 5 0  200 400 Sample size  0  200 400 Sample size  0  0  200 400 Sample size  0  200 400 Sample size  200 400 Sample size  Figure 2: Comparison results of different estimation procedures on a synthetic dataset. [sent-318, score-0.136]
</p><p>74 to 500 time points, and ﬁxed the number of covariates is ﬁxed to p = 20. [sent-319, score-0.301]
</p><p>75 The block partition was generated randomly and consists of ten blocks with minimum length set to 10 time points. [sent-320, score-0.218]
</p><p>76 In each of the block, only 5 covariates out of 20 affected the response. [sent-321, score-0.264]
</p><p>77 A simple local regression method [13], which is commonly used for estimation in varying coefﬁcient models, was used as the simplest baseline for comparing the relative performance of estimation. [sent-331, score-0.203]
</p><p>78 Our ﬁrst competitor is an extension of the baseline, which uses the following estimator [28]: n  p  n  min  β∈Rp×n  i′ =1 i=1  (Yi − X′ βi′ )2 Kh (ti′ − ti ) + i  n 2 βi′ ,j ,  λj j=1  (19)  i′ =1  1 where Kh (·) = h K(·/h) is the kernel function. [sent-332, score-0.525]
</p><p>79 Another competitor uses the ℓ1 penalized local regression independently at each time point, which leads to the following estimator of β(t), p  n  min p  β∈R  i=1  (Yi − X′ β)2 Kh (ti − t) + i  j=1  λj |βj |. [sent-334, score-0.307]
</p><p>80 The difference between the two methods is that “Kernel ℓ1 /ℓ2 ” biases certain covariates toward zero at every time point, based on global information; whereas “Kernel ℓ1 ” biases covariates toward zero only based on local information. [sent-336, score-0.565]
</p><p>81 We report the relative estimation error, REE = 100 ×  Pp ∗ ˆ j=1 |βi,j −βi,j | Pp ˜i,j −β ∗ | , i=1 j=1 |β i,j  Pn  i=1  Pn  ˜ where β is the baseline  local linear estimator, as a measure of estimation accuracy. [sent-342, score-0.162]
</p><p>82 To asses the performance of the model selection, we report precision, recall and their harmonic mean F1 measure when estimating the relevant covariates at each time point and the percentage of correctly identiﬁed irrelevant covariates. [sent-343, score-0.402]
</p><p>83 7  Application to Time-varying Graph Structure Estimation  An interesting application of the TDB-Lasso is in structural estimation of time-varying undirected graphical models [1, 17]. [sent-350, score-0.218]
</p><p>84 A graph structure estimation can be posed as a neighborhood selection 7  problem, in which neighbors of each node are estimated independently. [sent-351, score-0.306]
</p><p>85 Neighborhood selection in the time-varying Gaussian graphical models (GGM) is equivalent to model selection in VCVS, where value of one node is regressed to the rest of nodes. [sent-352, score-0.198]
</p><p>86 Graphs estimated in this way will have neighborhoods of each node that are constant on a partition, but the graph as a whole changes more ﬂexibly (Fig. [sent-354, score-0.164]
</p><p>87 00s The graph structure estimation using the TDBLasso is demonstrated on a real dataset of electroencephalogram (EEG) measurements. [sent-359, score-0.122]
</p><p>88 We use the brain computer interface (BCI) dataset IVa from [11] in which the EEG data is collected from 5 subjects, who were given visual cues based on which they were required to imagine right hand Figure 3: Brain interactions for the subject ’aa’ or right foot for 3. [sent-360, score-0.142]
</p><p>89 3 gives a visualization of the brain interactions over the time of the experiment for the subject ’aa’ while presented with visual cues for the class 1 (right hand). [sent-365, score-0.179]
</p><p>90 We also used TDB-Lasso for estimating the time-varying gene networks from microarray data time series data, but due to space limit, results will be reported later in a biological paper. [sent-375, score-0.186]
</p><p>91 61  Discussion  We have developed the TDB-Lasso procedure, a novel approach for model selection and variable estimation in the varying-coefﬁcient varying-structure models with piecewise constant functions. [sent-387, score-0.263]
</p><p>92 Due to their ﬂexibility, important classical problems, such as linear regression with structural changes and change point detection, and some more recent problems, like structure estimation of varying graphical models, can be modeled within this class of models. [sent-389, score-0.5]
</p><p>93 The TDB-Lasso compares favorably to other commonly used [28] or latest [1] techniques for estimation in this class of models, which was demonstrated on the synthetic data. [sent-390, score-0.136]
</p><p>94 The model selection properties of the TDB-Lasso, demonstrated on the synthetic data, are also supported by the theoretical analysis. [sent-391, score-0.154]
</p><p>95 Application of the TDB-Lasso procedure goes beyond the linear varying coefﬁcient regression models. [sent-393, score-0.168]
</p><p>96 A direct extension is to generalized varying-coefﬁcient models g(m(Xi , ti )) = X′ β(ti ), i = i 1, . [sent-394, score-0.32]
</p><p>97 , n, where g(·) is a given link function and m(Xi , ti ) = E[Y |X = Xi , t = ti ] is the conditional mean. [sent-397, score-0.64]
</p><p>98 Estimating and testing linear models with multiple structural changes. [sent-417, score-0.137]
</p><p>99 Honest variable selection in linear and logistic regression models via ℓ1 and ℓ1 + ℓ2 penalization. [sent-437, score-0.186]
</p><p>100 Least-squares estimation of an unknown number of shifts in a time series. [sent-502, score-0.118]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jump', 0.392), ('ti', 0.32), ('vcvs', 0.273), ('tv', 0.264), ('covariates', 0.264), ('lasso', 0.186), ('bj', 0.178), ('sbj', 0.174), ('structural', 0.137), ('coef', 0.112), ('eeg', 0.105), ('selection', 0.099), ('eric', 0.099), ('rp', 0.096), ('cj', 0.087), ('regression', 0.087), ('estimation', 0.081), ('weakness', 0.08), ('changes', 0.079), ('penalty', 0.077), ('yi', 0.076), ('bic', 0.074), ('points', 0.071), ('partition', 0.069), ('randomized', 0.069), ('interval', 0.068), ('kolar', 0.065), ('mladen', 0.065), ('block', 0.063), ('segment', 0.063), ('tj', 0.06), ('harchaoui', 0.06), ('estimating', 0.059), ('td', 0.058), ('kernel', 0.057), ('aa', 0.056), ('competitor', 0.056), ('za', 0.056), ('synthetic', 0.055), ('prices', 0.053), ('stock', 0.053), ('estimator', 0.051), ('kh', 0.05), ('rnp', 0.05), ('sparsistent', 0.05), ('varyingcoef', 0.05), ('xbj', 0.05), ('ybj', 0.05), ('estimated', 0.049), ('blocks', 0.049), ('cues', 0.049), ('evolving', 0.048), ('series', 0.047), ('interactions', 0.047), ('piecewise', 0.047), ('bootstrap', 0.047), ('procedure', 0.046), ('brain', 0.046), ('cients', 0.045), ('preprint', 0.045), ('jb', 0.043), ('cutoff', 0.043), ('jianqing', 0.043), ('tb', 0.043), ('tesla', 0.043), ('segments', 0.043), ('gene', 0.043), ('relevant', 0.042), ('william', 0.041), ('structure', 0.041), ('min', 0.041), ('variation', 0.04), ('change', 0.04), ('np', 0.04), ('ordinary', 0.04), ('stocks', 0.04), ('ggm', 0.04), ('falsely', 0.04), ('xi', 0.04), ('stability', 0.039), ('song', 0.039), ('estimate', 0.038), ('argmin', 0.037), ('bai', 0.037), ('time', 0.037), ('cient', 0.036), ('appendix', 0.036), ('neighborhood', 0.036), ('minimizing', 0.036), ('constant', 0.036), ('rn', 0.036), ('fused', 0.035), ('penalized', 0.035), ('correct', 0.035), ('varying', 0.035), ('changing', 0.034), ('sj', 0.034), ('francis', 0.034), ('econometrics', 0.034), ('uncover', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="225-tfidf-1" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>2 0.17772773 <a title="225-tfidf-2" href="./nips-2009-Time-Varying_Dynamic_Bayesian_Networks.html">246 nips-2009-Time-Varying Dynamic Bayesian Networks</a></p>
<p>Author: Le Song, Mladen Kolar, Eric P. Xing</p><p>Abstract: Directed graphical models such as Bayesian networks are a favored formalism for modeling the dependency structures in complex multivariate systems such as those encountered in biology and neural science. When a system is undergoing dynamic transformation, temporally rewiring networks are needed for capturing the dynamic causal inﬂuences between covariates. In this paper, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series. This is a challenging problem due the non-stationarity and sample scarcity of time series data. We present a kernel reweighted 1 -regularized auto-regressive procedure for this problem which enjoys nice properties such as computational efﬁciency and provable asymptotic consistency. To our knowledge, this is the ﬁrst practical and statistically sound method for structure learning of TVDBNs. We applied TV-DBNs to time series measurements during yeast cell cycle and brain response to visual stimuli. In both cases, TV-DBNs reveal interesting dynamics underlying the respective biological systems. 1</p><p>3 0.16037875 <a title="225-tfidf-3" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>Author: Sahand Negahban, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n → 0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M -estimator) which combines a loss function (measuring how well the model ﬁts the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a uniﬁed framework for establishing consistency and convergence rates for such regularized M estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identiﬁes two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M -estimators have fast convergence rates. 1</p><p>4 0.15697215 <a title="225-tfidf-4" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>Author: Shuheng Zhou</p><p>Abstract: Given n noisy samples with p dimensions, where n ≪ p, we show that the multistep thresholding procedure can accurately estimate a sparse vector β ∈ Rp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very signiﬁcant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufﬁciently sparse, the Gauss-Dantzig selector (Cand` se Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufﬁciently sparse model. 1</p><p>5 0.15301217 <a title="225-tfidf-5" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>Author: Sylvain Arlot, Francis R. Bach</p><p>Abstract: This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which ﬁrst estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows’ CL penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves signiﬁcantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation. 1</p><p>6 0.12393834 <a title="225-tfidf-6" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>7 0.10708622 <a title="225-tfidf-7" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<p>8 0.10412663 <a title="225-tfidf-8" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>9 0.10125539 <a title="225-tfidf-9" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>10 0.096091881 <a title="225-tfidf-10" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>11 0.09225554 <a title="225-tfidf-11" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>12 0.091394708 <a title="225-tfidf-12" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>13 0.090513639 <a title="225-tfidf-13" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>14 0.085903108 <a title="225-tfidf-14" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>15 0.085213587 <a title="225-tfidf-15" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>16 0.085203215 <a title="225-tfidf-16" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>17 0.082787752 <a title="225-tfidf-17" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>18 0.082316376 <a title="225-tfidf-18" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>19 0.079334207 <a title="225-tfidf-19" href="./nips-2009-Semi-supervised_Learning_using_Sparse_Eigenfunction_Bases.html">213 nips-2009-Semi-supervised Learning using Sparse Eigenfunction Bases</a></p>
<p>20 0.077940062 <a title="225-tfidf-20" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.26), (1, 0.084), (2, 0.007), (3, 0.109), (4, -0.014), (5, -0.054), (6, 0.14), (7, -0.135), (8, 0.051), (9, -0.031), (10, 0.025), (11, -0.05), (12, 0.052), (13, -0.025), (14, 0.055), (15, 0.109), (16, -0.004), (17, -0.128), (18, -0.089), (19, 0.072), (20, 0.054), (21, 0.132), (22, -0.04), (23, 0.007), (24, 0.034), (25, 0.154), (26, -0.078), (27, 0.239), (28, -0.001), (29, -0.109), (30, -0.055), (31, 0.056), (32, -0.061), (33, -0.094), (34, -0.035), (35, 0.044), (36, -0.026), (37, 0.07), (38, -0.045), (39, 0.065), (40, 0.031), (41, -0.064), (42, -0.045), (43, -0.008), (44, 0.118), (45, -0.026), (46, -0.07), (47, 0.048), (48, -0.028), (49, -0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94996589 <a title="225-lsi-1" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>2 0.68154287 <a title="225-lsi-2" href="./nips-2009-Grouped_Orthogonal_Matching_Pursuit_for_Variable_Selection_and_Prediction.html">105 nips-2009-Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</a></p>
<p>Author: Grzegorz Swirszcz, Naoki Abe, Aurelie C. Lozano</p><p>Abstract: We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efﬁciently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as “forward greedy feature selection algorithm” for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the l∞ norm of the difference between the estimated regression coefﬁcients and the true coefﬁcients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy. 1</p><p>3 0.67539936 <a title="225-lsi-3" href="./nips-2009-Time-Varying_Dynamic_Bayesian_Networks.html">246 nips-2009-Time-Varying Dynamic Bayesian Networks</a></p>
<p>Author: Le Song, Mladen Kolar, Eric P. Xing</p><p>Abstract: Directed graphical models such as Bayesian networks are a favored formalism for modeling the dependency structures in complex multivariate systems such as those encountered in biology and neural science. When a system is undergoing dynamic transformation, temporally rewiring networks are needed for capturing the dynamic causal inﬂuences between covariates. In this paper, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series. This is a challenging problem due the non-stationarity and sample scarcity of time series data. We present a kernel reweighted 1 -regularized auto-regressive procedure for this problem which enjoys nice properties such as computational efﬁciency and provable asymptotic consistency. To our knowledge, this is the ﬁrst practical and statistically sound method for structure learning of TVDBNs. We applied TV-DBNs to time series measurements during yeast cell cycle and brain response to visual stimuli. In both cases, TV-DBNs reveal interesting dynamics underlying the respective biological systems. 1</p><p>4 0.62241381 <a title="225-lsi-4" href="./nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></p>
<p>Author: Shuheng Zhou</p><p>Abstract: Given n noisy samples with p dimensions, where n ≪ p, we show that the multistep thresholding procedure can accurately estimate a sparse vector β ∈ Rp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very signiﬁcant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufﬁciently sparse, the Gauss-Dantzig selector (Cand` se Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufﬁciently sparse model. 1</p><p>5 0.60500222 <a title="225-lsi-5" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-ofthe-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justiﬁcations of speciﬁc versions of the additive forward regression. 1</p><p>6 0.60317844 <a title="225-lsi-6" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<p>7 0.595137 <a title="225-lsi-7" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>8 0.5846557 <a title="225-lsi-8" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>9 0.56469983 <a title="225-lsi-9" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>10 0.55875307 <a title="225-lsi-10" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>11 0.54038411 <a title="225-lsi-11" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>12 0.52706158 <a title="225-lsi-12" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>13 0.49169219 <a title="225-lsi-13" href="./nips-2009-Randomized_Pruning%3A_Efficiently_Calculating_Expectations_in_Large_Dynamic_Programs.html">197 nips-2009-Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs</a></p>
<p>14 0.4867931 <a title="225-lsi-14" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>15 0.4843514 <a title="225-lsi-15" href="./nips-2009-Orthogonal_Matching_Pursuit_From_Noisy_Random_Measurements%3A_A_New_Analysis.html">185 nips-2009-Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></p>
<p>16 0.48033077 <a title="225-lsi-16" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>17 0.46208107 <a title="225-lsi-17" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>18 0.45717809 <a title="225-lsi-18" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>19 0.44416034 <a title="225-lsi-19" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>20 0.42667469 <a title="225-lsi-20" href="./nips-2009-Fast%2C_smooth_and_adaptive_regression_in_metric_spaces.html">91 nips-2009-Fast, smooth and adaptive regression in metric spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.016), (21, 0.021), (24, 0.051), (25, 0.131), (35, 0.053), (36, 0.138), (39, 0.05), (55, 0.012), (58, 0.082), (61, 0.03), (71, 0.039), (81, 0.021), (86, 0.076), (91, 0.013), (98, 0.197)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90390575 <a title="225-lda-1" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin</p><p>Abstract: Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called rifﬂed independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efﬁcient inference and reducing sample complexity. In rifﬂed independence, one draws two permutations independently, then performs the rifﬂe shufﬂe, common in card games, to combine the two permutations to form a single permutation. In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using rifﬂed independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. 1</p><p>2 0.88958865 <a title="225-lda-2" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>Author: Daniel Cavagnaro, Jay Myung, Mark A. Pitt</p><p>Abstract: In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efﬁciently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to ﬁnd designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1</p><p>3 0.886967 <a title="225-lda-3" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>same-paper 4 0.86415082 <a title="225-lda-4" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>Author: Mladen Kolar, Le Song, Eric P. Xing</p><p>Abstract: To estimate the changing structure of a varying-coefﬁcient varying-structure (VCVS) model remains an important and open problem in dynamic system modelling, which includes learning trajectories of stock prices, or uncovering the topology of an evolving gene network. In this paper, we investigate sparsistent learning of a sub-family of this model — piecewise constant VCVS models. We analyze two main issues in this problem: inferring time points where structural changes occur and estimating model structure (i.e., model selection) on each of the constant segments. We propose a two-stage adaptive procedure, which ﬁrst identiﬁes jump points of structural changes and then identiﬁes relevant covariates to a response on each of the segments. We provide an asymptotic analysis of the procedure, showing that with the increasing sample size, number of structural changes, and number of variables, the true model can be consistently selected. We demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset. We also consider how this applies to structure estimation of time-varying probabilistic graphical models. 1</p><p>5 0.75780827 <a title="225-lda-5" href="./nips-2009-Semi-supervised_Regression_using_Hessian_energy_with_an_application_to_semi-supervised_dimensionality_reduction.html">214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</a></p>
<p>Author: Kwang I. Kim, Florian Steinke, Matthias Hein</p><p>Abstract: Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Based on these observations, we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both these problems. If the data lies on or close to a low-dimensional submanifold in feature space, the Hessian energy prefers functions whose values vary linearly with respect to geodesic distance. We ﬁrst derive the Hessian energy for smooth manifolds and continue to give a stable estimation procedure for the common case where only samples of the underlying manifold are given. The preference of ‘’linear” functions on manifolds renders the Hessian energy particularly suited for the task of semi-supervised dimensionality reduction, where the goal is to ﬁnd a user-deﬁned embedding function given some labeled points which varies smoothly (and ideally linearly) along the manifold. The experimental results suggest superior performance of our method compared with semi-supervised regression using Laplacian regularization or standard supervised regression techniques applied to this task. 1</p><p>6 0.74964446 <a title="225-lda-6" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>7 0.7473436 <a title="225-lda-7" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>8 0.7470907 <a title="225-lda-8" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>9 0.74334484 <a title="225-lda-9" href="./nips-2009-Statistical_Analysis_of_Semi-Supervised_Learning%3A_The_Limit_of_Infinite_Unlabelled_Data.html">229 nips-2009-Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></p>
<p>10 0.7418561 <a title="225-lda-10" href="./nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</a></p>
<p>11 0.74118888 <a title="225-lda-11" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>12 0.73937804 <a title="225-lda-12" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>13 0.73931187 <a title="225-lda-13" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>14 0.73899001 <a title="225-lda-14" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>15 0.73686975 <a title="225-lda-15" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>16 0.73640597 <a title="225-lda-16" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>17 0.73560292 <a title="225-lda-17" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>18 0.73521209 <a title="225-lda-18" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>19 0.73291874 <a title="225-lda-19" href="./nips-2009-Distribution-Calibrated_Hierarchical_Classification.html">71 nips-2009-Distribution-Calibrated Hierarchical Classification</a></p>
<p>20 0.73226172 <a title="225-lda-20" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
