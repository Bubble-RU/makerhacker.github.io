<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-158" href="#">nips2009-158</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</h1>
<br/><p>Source: <a title="nips-2009-158-pdf" href="http://papers.nips.cc/paper/3673-multi-label-prediction-via-sparse-infinite-cca.pdf">pdf</a></p><p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>Reference: <a title="nips-2009-158-reference" href="../nips2009_reference/nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. [sent-4, score-0.35]
</p><p>2 In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. [sent-5, score-0.178]
</p><p>3 The aforementioned setting is a special case of multitask learning [6] when predicting each label is a task and all the tasks share a common source of input. [sent-15, score-0.226]
</p><p>4 However, such ı an approach ignores the label correlations and leads to sub-optimal performance [20]. [sent-18, score-0.126]
</p><p>5 One important application of CCA is in supervised dimensionality reduction, albeit in the more general setting where each example has several labels. [sent-21, score-0.162]
</p><p>6 In this setting, CCA on input-output pair (X, Y) can be used to project inputs X to a low-dimensional space directed by label information Y. [sent-22, score-0.099]
</p><p>7 An even more crucial issue is choosing the number of correlation components, which is traditionally dealt with by using cross-validation, or model-selection [21]. [sent-26, score-0.129]
</p><p>8 Another issue is the potential sparsity [18] of the underlying projections that is ignored by the standard CCA formulation. [sent-27, score-0.143]
</p><p>9 Building upon the recently suggested probabilistic interpretation of CCA [3], we propose a nonparametric, fully Bayesian framework that can deal with each of these issues. [sent-28, score-0.141]
</p><p>10 In particular, the proposed model can automatically select the number of correlation components, and effectively capture the 1  sparsity underlying the projections. [sent-29, score-0.26]
</p><p>11 Our framework is based on the Indian Buffet Process [9], a nonparametric Bayesian model to discover latent feature representation of a set of observations. [sent-30, score-0.148]
</p><p>12 In addition, our probabilistic model allows dealing with missing data and, in the supervised dimensionality reduction case, can incorporate additional unlabeled data one may have access to, making our CCA algorithm work in a semi-supervised setting. [sent-31, score-0.355]
</p><p>13 Thus, apart from being a general, nonparametric, fully Bayesian solution to the CCA problem, our framework can be readily applied for learning useful predictive features from labeled (or partially labeled) data in the context of learning a set of related tasks. [sent-32, score-0.088]
</p><p>14 In particular, we describe a fully supervised setting (when the test data is not available at the time of training), and a semi-supervised setting with partial labels (when we have access to test data at the time of training). [sent-37, score-0.179]
</p><p>15 2  Canonical Correlation Analysis  Canonical correlation analysis (CCA) is a useful technique for modeling the relationships among a set of variables. [sent-40, score-0.103]
</p><p>16 More formally, given a pair of variables x ∈ RD1 and y ∈ RD2 , CCA seeks to ﬁnd linear projections ux and uy such that the variables are maximally correlated in the projected space. [sent-42, score-0.251]
</p><p>17 The correlation coefﬁcient between the two variables in the embedded space is given by uT xyT uy x  ρ=  (uT xxT ux )(uT yyT uy ) y x Since the correlation is not affected by rescaling of the projections ux and uy , CCA is posed as a constrained optimization problem. [sent-43, score-0.817]
</p><p>18 1  Probabilistic CCA  Bach and Jordan [3] gave a probabilistic interpretation of CCA by posing it as a latent variable model. [sent-52, score-0.164]
</p><p>19 Bach and Jordan [3] showed that, given the maximum likelihood solution for the model parameters, the expectations E(z|x) and E(z|y) of the latent variable z lie in the same subspace that classical CCA ﬁnds, thereby establishing the equivalence between the above probabilistic model and CCA. [sent-59, score-0.311]
</p><p>20 However, it still assumes an apriori ﬁxed number of canonical correlation components. [sent-61, score-0.214]
</p><p>21 In addition, another important issue is the sparsity of the underlying projection matrix which is usually ignored. [sent-62, score-0.22]
</p><p>22 A crucial issue in the CCA model is choosing the number of canonical correlation components which is set to a ﬁxed value in classical CCA (and even in the probabilistic extensions of CCA). [sent-64, score-0.393]
</p><p>23 In the Bayesian formulation of CCA, one can use the Automatic Relevance Determination (ARD) prior [5] on the projection matrix W that gives a way to select this number. [sent-65, score-0.123]
</p><p>24 We propose a nonparametric Bayesian model that selects the number of canonical correlation components automatically. [sent-67, score-0.314]
</p><p>25 More speciﬁcally, we use the Indian Buffet Process [9] (IBP) as a nonparametric prior on the projection matrix W. [sent-68, score-0.164]
</p><p>26 The IBP prior allows W to have an unbounded number of columns which gives a way to automatically determine the dimensionality K of the latent space associated with Z. [sent-69, score-0.269]
</p><p>27 1  The Indian Buffet Process  The Indian Buffet Process [9] deﬁnes a distribution over inﬁnite binary matrices, originally motivated by the need to model the latent feature structure of a given set of observations. [sent-71, score-0.136]
</p><p>28 In the latent feature model, each observation can be thought of as being explained by a set of latent features. [sent-73, score-0.166]
</p><p>29 Given an N × D matrix X of N observations having D features each, we can consider a decomposition of the form X = ZA + E where Z is an N × K binary feature-assignment matrix describing which features are present in each observation. [sent-74, score-0.117]
</p><p>30 A is a K × D matrix of feature scores, and the matrix E consists of observation speciﬁc noise. [sent-76, score-0.088]
</p><p>31 A crucial issue in such models is the choosing the number K of latent features. [sent-77, score-0.109]
</p><p>32 The standard formulation of IBP lets us deﬁne a prior over the binary matrix Z such that it can have an unbounded number of columns and thus can be a suitable prior in problems dealing with such structures. [sent-78, score-0.163]
</p><p>33 The IBP derivation starts by deﬁning a ﬁnite model for K many columns of a N × K binary matrix. [sent-79, score-0.099]
</p><p>34 This equivalence can be best understood by a culinary analogy of customers coming to an Indian restaurant and selecting dishes from an inﬁnite array of dishes. [sent-82, score-0.123]
</p><p>35 In this analogy, customers represent observations and dishes represent latent features. [sent-83, score-0.206]
</p><p>36 Thereafter, each incoming customer n selects an existing dish k with a probability mk /N , where mk denotes how many previous customers chose that particular dish. [sent-85, score-0.176]
</p><p>37 This process generates a binary matrix Z with rows representing customer and columns representing dishes. [sent-87, score-0.158]
</p><p>38 Many real world datasets have a sparseness 3  Figure 1: The graphical model depicts the fully supervised case when all variables X and Y are observed. [sent-88, score-0.141]
</p><p>39 The semisupervised case can have X and/or Y consisting of missing values as well. [sent-89, score-0.09]
</p><p>40 The graphical model structure remains the same property which means that each observation depends only on a subset of all the K latent features. [sent-90, score-0.107]
</p><p>41 This means that the binary matrix Z is expected to be reasonably sparse for many datasets. [sent-91, score-0.103]
</p><p>42 This makes IBP a suitable choice for also capturing the underlying sparsity in addition to automatically discovering the number of latent features. [sent-92, score-0.255]
</p><p>43 2  The Inﬁnite CCA Model  In our proposed framework, the matrix W consisting of canonical correlation vectors is modeled using an IBP prior. [sent-94, score-0.292]
</p><p>44 , xN ] is D1 × N matrix consisting of N samples of D1 dimensions each, and Y = [y1 , . [sent-103, score-0.101]
</p><p>45 , yN ] is another matrix consisting of N samples of D2 dimensions each. [sent-106, score-0.101]
</p><p>46 This is particularly important in the case of supervised dimensionality reduction (i. [sent-110, score-0.233]
</p><p>47 , X consisting of inputs and Y associated responses) when the labels for some of the inputs are unknown, making it a model for semi-supervised dimensionality reduction with partially labeled data. [sent-112, score-0.355]
</p><p>48 In addition, placing the IBP prior on the projection matrix W (via the binary matrix B) also helps in capturing the sparsity in W (see results section for evidence). [sent-113, score-0.303]
</p><p>49 3  Inference  We take a fully Bayesian approach by treating everything at latent variables and computing the posterior distributions over them. [sent-115, score-0.116]
</p><p>50 4  In what follows, D denotes the data [X; Y], B = [Bx ; By ], and V = [Vx ; Vy ] Sampling B: Sampling the binary IBP matrix B consists of sampling existing dishes, proposing new dishes and accepting or rejecting them based on the acceptance ratio in the associated M-H step. [sent-117, score-0.186]
</p><p>51 Note that the number of columns in V is the same as number of columns in the IBP matrix B. [sent-127, score-0.136]
</p><p>52 To deal with this issue, one could sample Bx (say having Kx nonzero columns) and By (say having Ky nonzero columns) ﬁrst, introduce extra dummy columns (|Kx −Ky | in number) in the matrix having smaller number of nonzero columns, and then set all such columns to zero. [sent-133, score-0.241]
</p><p>53 4  Multitask Learning using Inﬁnite CCA  Having set up the framework for inﬁnite CCA, we now describe its applicability for the problem of multitask learning. [sent-136, score-0.091]
</p><p>54 Here predicting each individual label becomes a task to be learned. [sent-138, score-0.11]
</p><p>55 Although one can individually learn a separate model for each task, doing this would ignore the label correlations. [sent-139, score-0.104]
</p><p>56 With this motivation, we apply our inﬁnite CCA model to capture the label correlations and to learn better predictive features from the data by projective it to a subspace directed by label information. [sent-141, score-0.298]
</p><p>57 It has been empirically and theoretically [25] shown that incorporating label information in dimensionality reduction indeed leads to better projections if the ﬁnal goal is prediction. [sent-142, score-0.231]
</p><p>58 The inﬁnite CCA model is applied on the pair X and Y which is akin to doing supervised dimensionality reduction for the inputs X. [sent-153, score-0.298]
</p><p>59 Note that the generalized eigenvalue problem posed in such a supervised setting of CCA consists of cross-covariance matrix ΣXY and label covariance matrix ΣY Y . [sent-154, score-0.258]
</p><p>60 Therefore the projection takes into account both the input-output correlations and the label correlations. [sent-155, score-0.158]
</p><p>61 5  Multitask learning using the inﬁnite CCA model can be done in two settings: supervised and semisupervised depending on whether or not the inputs of test data are involved in learning the shared subspace Z. [sent-157, score-0.307]
</p><p>62 1  Fully supervised setting  In the supervised setting, CCA is done on labeled data (X, Y) to give a single shared subspace Z ∈ RK×N that is good across all tasks. [sent-159, score-0.298]
</p><p>63 A model is then learned in the Z subspace to learn M task parameters {θm } ∈ RK×1 where m ∈ {1, . [sent-160, score-0.139]
</p><p>64 With the second option, we x x x can inﬂate each learned task parameter back to D dimensions by applying the projection matrix Wx . [sent-167, score-0.153]
</p><p>65 The inﬁnite CCA model is then applied on the pair (X, Y) and the parts of Y consisting of Yte are treated as a latent variables to be imputed. [sent-172, score-0.141]
</p><p>66 With this model, we get the embeddings also for the test data and thus training and testing both take place in the K dimensional subspace, unlike model-1 in which training is done in K dimensional subspace and prediction are made in the original D dimensional subspace. [sent-173, score-0.191]
</p><p>67 We ﬁrst show our results with the inﬁnite CCA as a stand alone algorithm for CCA by using it on a synthetic dataset demonstrating its effectiveness in capturing the canonical correlations. [sent-176, score-0.15]
</p><p>68 We then also report our experiments on applying the inﬁnite CCA model to the problem of multitask learning on two real world datasets. [sent-177, score-0.115]
</p><p>69 1  Inﬁnite CCA results on synthetic data  In the ﬁrst experiment, we demonstrate the effectiveness of our proposed inﬁnite CCA model in discovering the correct number of canonical correlation components, and in capturing the sparsity pattern underlying the projection matrix. [sent-179, score-0.427]
</p><p>70 , the number of components, and the underlying sparsity of projection matrix). [sent-183, score-0.15]
</p><p>71 In particular, the dataset had 4 correlation components with a 63% sparsity in the true projection matrix. [sent-184, score-0.263]
</p><p>72 Looking at all the correlations discovered by classical CCA, we found that it discovered 8 components having signiﬁcant correlations, whereas our model correctly discovered exactly 4 components in the ﬁrst place (we extract the MAP samples for W and Z output by our Gibbs sampler). [sent-186, score-0.258]
</p><p>73 Thus on this small dataset, standard CCA indeed seems to be ﬁnding spurious correlations, indicating a case of overﬁtting (the overﬁtting problem of classical CCA was also observed in [15] when comparing Bayesian versus classical CCA). [sent-187, score-0.092]
</p><p>74 Furthermore, as expected, the projection matrix inferred by the classical CCA had no exact zero entries and even after thresholding signiﬁcantly small absolute values to zero, the uncovered sparsity was only about 25%. [sent-188, score-0.215]
</p><p>75 On the other hand, the projection matrix inferred by the inﬁnite CCA model had 57% exact zero entries and 62% zero entries after thresholding very small values, thereby demonstrating its effectiveness in also capturing the sparsity patterns. [sent-189, score-0.232]
</p><p>76 This baseline ignores the label information while learning the low dimensional subspace. [sent-243, score-0.118]
</p><p>77 • CCA: Apply classical CCA on training data to extract the shared subspace, learn separate model (i. [sent-244, score-0.136]
</p><p>78 , task parameters) for each task in this subspace, project the task parameters back to the original D dimensional feature space by applying the projection Wx , and do predictions on the test data in this feature pace. [sent-246, score-0.179]
</p><p>79 • Model-1: Use our supervised inﬁnite CCA model to learn the shared subspace using only the training data (see section 4. [sent-247, score-0.238]
</p><p>80 • Model-2: Use our semi-supervised inﬁnite CCA model to simultaneously learn the shared subspace for both training and test data (see section 4. [sent-249, score-0.154]
</p><p>81 This is possible in our probabilistic model since we could treat the unknown Y’s of the test data as latent variables to be imputed while doing the Gibbs sampling. [sent-256, score-0.155]
</p><p>82 6  Related Work  A number of approaches have been proposed in the recent past for the problem of supervised dimensionality reduction of multi-label data. [sent-259, score-0.233]
</p><p>83 The few approaches that exist include Partial Least Squares [2], multi-label informed latent semantic indexing [24], and multi-label dimensionality reduction using dependence maximization (MDDM) [26]. [sent-260, score-0.232]
</p><p>84 Somewhat similar in spirit to our approach is the work on supervised probabilistic PCA [25] that extends probabilistic PCA to the setting when we also have access to labels. [sent-262, score-0.202]
</p><p>85 However, it assumes a ﬁxed number of components and does not take into account sparsity of the projections. [sent-263, score-0.103]
</p><p>86 7  The CCA based approach to supervised dimensionality reduction is more closely related to the notion of dimension reduction for regression (DRR) which is formally deﬁned as ﬁnding a low dimensional representation z ∈ RK of inputs x ∈ RD (K ≪ D) for predicting multivariate outputs y ∈ RM . [sent-264, score-0.403]
</p><p>87 An important notion in DRR is that of sufﬁcient dimensionality reduction (SDR) [10, 8] which states that given z, x and y are conditionally independent, i. [sent-265, score-0.149]
</p><p>88 As we can see in the ⊥ graphical model shown in ﬁgure-1, the probabilistic interpretation of CCA yields the same condition with X and Y being conditionally independent given Z. [sent-268, score-0.105]
</p><p>89 Among the DRR based approaches to dimensionality reduction for real-valued multilabel data, Covariance Operator Inverse Regression (COIR) exploits the covariance structures of both the inputs and outputs [14]. [sent-269, score-0.19]
</p><p>90 In another recent work [13], a joint learning framework is proposed which performs dimensionality reduction and multi-label classiﬁcation simultaneously. [sent-277, score-0.149]
</p><p>91 In particular, sparsity improves model interpretation and has been gaining lots of attention recently. [sent-279, score-0.125]
</p><p>92 Another recent solution is based on a direct greedy approach which bounds the correlation at each stage [22]. [sent-281, score-0.103]
</p><p>93 Finally, multitask learning has been tackled using a variety of different approaches, primarily depending on what notion of task relatedness is assumed. [sent-286, score-0.144]
</p><p>94 Some of the examples include tasks generated from an IID space [4], and learning multiple tasks using a hierarchical prior over the task space [23, 7], among others. [sent-287, score-0.101]
</p><p>95 In particular, our model does not assume a ﬁxed number of correlation components and this number is determined automatically based only on the data. [sent-290, score-0.202]
</p><p>96 In addition, our model enjoys sparsity making the model more interpretable. [sent-291, score-0.116]
</p><p>97 The probabilistic nature of our model also allows dealing with missing data. [sent-292, score-0.1]
</p><p>98 Finally, we also demonstrate the model’s applicability to the problem of multi-label learning where our model, directed by label information, can be used to automatically extract useful predictive features from the data. [sent-293, score-0.127]
</p><p>99 Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. [sent-342, score-0.155]
</p><p>100 Covariance operator based dimensionality reduction with extension to semisupervised settings. [sent-384, score-0.177]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cca', 0.827), ('ibp', 0.192), ('uy', 0.133), ('canonical', 0.111), ('correlation', 0.103), ('bx', 0.094), ('ux', 0.094), ('multitask', 0.091), ('subspace', 0.086), ('supervised', 0.084), ('latent', 0.083), ('dishes', 0.083), ('wx', 0.081), ('dimensionality', 0.078), ('reduction', 0.071), ('sparsity', 0.068), ('drr', 0.067), ('xte', 0.067), ('label', 0.058), ('projection', 0.057), ('vx', 0.05), ('vy', 0.05), ('oisson', 0.05), ('yte', 0.05), ('probabilistic', 0.048), ('nite', 0.047), ('columns', 0.046), ('classical', 0.046), ('yeast', 0.045), ('ut', 0.045), ('matrix', 0.044), ('shared', 0.044), ('correlations', 0.043), ('indian', 0.043), ('bayesian', 0.041), ('inputs', 0.041), ('pca', 0.041), ('nonparametric', 0.041), ('customers', 0.04), ('automatically', 0.04), ('labels', 0.04), ('customer', 0.039), ('capturing', 0.039), ('buffet', 0.038), ('ky', 0.038), ('daum', 0.038), ('mk', 0.037), ('kx', 0.036), ('components', 0.035), ('dimensional', 0.035), ('wt', 0.035), ('wy', 0.034), ('consisting', 0.034), ('ate', 0.033), ('bik', 0.033), ('coir', 0.033), ('xyt', 0.033), ('interpretation', 0.033), ('fully', 0.033), ('bach', 0.032), ('sampling', 0.03), ('sparse', 0.03), ('yu', 0.03), ('predictive', 0.029), ('yyt', 0.029), ('acc', 0.029), ('wwt', 0.029), ('task', 0.029), ('binary', 0.029), ('semisupervised', 0.028), ('missing', 0.028), ('eigenvalue', 0.028), ('auc', 0.028), ('option', 0.028), ('ib', 0.027), ('wz', 0.027), ('deal', 0.027), ('gibbs', 0.026), ('bottleneck', 0.026), ('partially', 0.026), ('nonzero', 0.026), ('issue', 0.026), ('underlying', 0.025), ('tasks', 0.025), ('scene', 0.025), ('ignores', 0.025), ('discovered', 0.025), ('projections', 0.024), ('relatedness', 0.024), ('rai', 0.024), ('xxt', 0.024), ('ard', 0.024), ('model', 0.024), ('predicting', 0.023), ('dish', 0.023), ('dimensions', 0.023), ('rk', 0.022), ('separate', 0.022), ('prior', 0.022), ('access', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="158-tfidf-1" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>2 0.33004567 <a title="158-tfidf-2" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>3 0.31393927 <a title="158-tfidf-3" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>4 0.11254437 <a title="158-tfidf-4" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>Author: Kurt Miller, Michael I. Jordan, Thomas L. Griffiths</p><p>Abstract: As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets. 1</p><p>5 0.11206143 <a title="158-tfidf-5" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>6 0.10893483 <a title="158-tfidf-6" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>7 0.10215394 <a title="158-tfidf-7" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>8 0.074556991 <a title="158-tfidf-8" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>9 0.074076675 <a title="158-tfidf-9" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>10 0.07299269 <a title="158-tfidf-10" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>11 0.06709405 <a title="158-tfidf-11" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>12 0.064946592 <a title="158-tfidf-12" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>13 0.059292253 <a title="158-tfidf-13" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>14 0.056738537 <a title="158-tfidf-14" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>15 0.050602172 <a title="158-tfidf-15" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>16 0.04656148 <a title="158-tfidf-16" href="./nips-2009-Robust_Principal_Component_Analysis%3A_Exact_Recovery_of_Corrupted_Low-Rank_Matrices_via_Convex_Optimization.html">208 nips-2009-Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization</a></p>
<p>17 0.045651529 <a title="158-tfidf-17" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>18 0.044309378 <a title="158-tfidf-18" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>19 0.043310799 <a title="158-tfidf-19" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>20 0.042801328 <a title="158-tfidf-20" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.174), (1, -0.057), (2, -0.042), (3, -0.019), (4, 0.05), (5, -0.083), (6, 0.111), (7, -0.127), (8, 0.004), (9, 0.087), (10, 0.091), (11, -0.001), (12, -0.167), (13, 0.026), (14, -0.274), (15, -0.026), (16, 0.261), (17, 0.207), (18, 0.146), (19, 0.045), (20, 0.04), (21, 0.001), (22, 0.119), (23, -0.161), (24, -0.093), (25, -0.018), (26, 0.18), (27, 0.037), (28, -0.178), (29, -0.269), (30, -0.028), (31, 0.145), (32, -0.043), (33, 0.023), (34, 0.008), (35, 0.103), (36, 0.004), (37, -0.062), (38, 0.008), (39, -0.036), (40, 0.016), (41, -0.046), (42, -0.057), (43, 0.029), (44, -0.093), (45, 0.031), (46, -0.055), (47, 0.076), (48, -0.006), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91179484 <a title="158-lsi-1" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>2 0.86998028 <a title="158-lsi-2" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>3 0.63864136 <a title="158-lsi-3" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>4 0.44673184 <a title="158-lsi-4" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>Author: Yee W. Teh, Dilan Gorur</p><p>Abstract: The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the stable-beta process and deriving the IBP corresponding to it. We ﬁnd interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and ﬁnd that our power-law IBP is a good model for word occurrences in document corpora. 1</p><p>5 0.40037414 <a title="158-lsi-5" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>Author: Douglas Eck, Yoshua Bengio, Aaron C. Courville</p><p>Abstract: The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an inﬁnite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer deﬁnes a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment. 1</p><p>6 0.35975385 <a title="158-lsi-6" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>7 0.34617957 <a title="158-lsi-7" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>8 0.33457187 <a title="158-lsi-8" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>9 0.26359788 <a title="158-lsi-9" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>10 0.23912525 <a title="158-lsi-10" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>11 0.23368165 <a title="158-lsi-11" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>12 0.23006758 <a title="158-lsi-12" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>13 0.22584596 <a title="158-lsi-13" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>14 0.21895029 <a title="158-lsi-14" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>15 0.21766421 <a title="158-lsi-15" href="./nips-2009-Breaking_Boundaries_Between_Induction_Time_and_Diagnosis_Time_Active_Information_Acquisition.html">49 nips-2009-Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition</a></p>
<p>16 0.21682686 <a title="158-lsi-16" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>17 0.21634668 <a title="158-lsi-17" href="./nips-2009-Localizing_Bugs_in_Program_Executions_with_Graphical_Models.html">143 nips-2009-Localizing Bugs in Program Executions with Graphical Models</a></p>
<p>18 0.20653377 <a title="158-lsi-18" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>19 0.20452814 <a title="158-lsi-19" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>20 0.197291 <a title="158-lsi-20" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.013), (7, 0.013), (21, 0.011), (24, 0.039), (25, 0.051), (35, 0.075), (36, 0.112), (39, 0.068), (55, 0.013), (58, 0.116), (61, 0.012), (66, 0.055), (71, 0.071), (81, 0.019), (86, 0.076), (91, 0.013), (94, 0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88329303 <a title="158-lda-1" href="./nips-2009-Breaking_Boundaries_Between_Induction_Time_and_Diagnosis_Time_Active_Information_Acquisition.html">49 nips-2009-Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition</a></p>
<p>Author: Ashish Kapoor, Eric Horvitz</p><p>Abstract: To date, the processes employed for active information acquisition during periods of learning and diagnosis have been considered as separate and have been applied in distinct phases of analysis. While active learning centers on the collection of information about training cases in order to build better predictive models, diagnosis uses ﬁxed predictive models for guiding the collection of observations about a speciﬁc test case at hand. We introduce a model and inferential methods that bridge these phases of analysis into a holistic approach to information acquisition that considers simultaneously the extension of the predictive model and the probing of a case at hand. The bridging of active learning and real-time diagnostic feature acquisition leads to a new class of policies for learning and diagnosis. 1</p><p>same-paper 2 0.8746891 <a title="158-lda-2" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>3 0.80877191 <a title="158-lda-3" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>4 0.80317098 <a title="158-lda-4" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>Author: Andrew Frank, Padhraic Smyth, Alexander T. Ihler</p><p>Abstract: Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions deﬁned on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the beneﬁts of these more advanced inference techniques to the continuous domain. 1</p><p>5 0.7827329 <a title="158-lda-5" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>Author: Kurt Miller, Michael I. Jordan, Thomas L. Griffiths</p><p>Abstract: As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets. 1</p><p>6 0.78132433 <a title="158-lda-6" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>7 0.78020608 <a title="158-lda-7" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>8 0.77883494 <a title="158-lda-8" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>9 0.77847701 <a title="158-lda-9" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>10 0.77771425 <a title="158-lda-10" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>11 0.77640158 <a title="158-lda-11" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>12 0.77434456 <a title="158-lda-12" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>13 0.77303231 <a title="158-lda-13" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>14 0.77214831 <a title="158-lda-14" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>15 0.77137774 <a title="158-lda-15" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>16 0.76963794 <a title="158-lda-16" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>17 0.76918781 <a title="158-lda-17" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>18 0.768125 <a title="158-lda-18" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>19 0.76790369 <a title="158-lda-19" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>20 0.767802 <a title="158-lda-20" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
