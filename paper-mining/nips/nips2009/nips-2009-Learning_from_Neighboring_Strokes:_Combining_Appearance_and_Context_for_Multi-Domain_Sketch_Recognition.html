<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-131" href="#">nips2009-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</h1>
<br/><p>Source: <a title="nips-2009-131-pdf" href="http://papers.nips.cc/paper/3885-learning-from-neighboring-strokes-combining-appearance-and-context-for-multi-domain-sketch-recognition.pdf">pdf</a></p><p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>Reference: <a title="nips-2009-131-reference" href="../nips2009_reference/nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. [sent-4, score-0.872]
</p><p>2 This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. [sent-5, score-0.507]
</p><p>3 The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. [sent-6, score-0.465]
</p><p>4 We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. [sent-7, score-0.39]
</p><p>5 We propose a new framework for sketch recognition that combines a rich representation of low level visual appearance with a probabilistic model for capturing higher level relationships. [sent-14, score-0.761]
</p><p>6 Our combined approach uses a graphical model that classiﬁes each symbol jointly with its context, allowing neighboring interpretations to inﬂuence each other. [sent-17, score-0.192]
</p><p>7 The result is a recognizer that is better able to handle the range of drawing styles found in messy freehand sketches. [sent-19, score-0.465]
</p><p>8 Current work in sketch recognition can, very broadly speaking, be separated into two groups. [sent-20, score-0.299]
</p><p>9 The ﬁrst group focuses on the relationships between geometric primitives like lines, arcs, and curves, specifying them either manually [1, 4, 5] or learning them from labeled data [16, 20]. [sent-21, score-0.271]
</p><p>10 Circles may not always be round, line segments may not be straight, and stroke artifacts like pen-drag (not lifting the pen between strokes), over-tracing (drawing over a 1  previously drawn stroke), and stray ink may introduce false primitives that lead to poor recognition. [sent-24, score-0.927]
</p><p>11 In addition, recognizers that rely on extracted primitives often discard potentially useful information contained in the appearance of the original strokes. [sent-25, score-0.384]
</p><p>12 The second group of related work focuses on the visual appearance of shapes and symbols. [sent-26, score-0.471]
</p><p>13 These include parts-based methods [9, 18], which learn a set of discriminative parts or patches for each symbol class, and template-based methods [7, 11], which compare the input symbol to a library of learned prototypes. [sent-27, score-0.318]
</p><p>14 The main advantage of vision-based approaches is their robustness to many of the drawing variations commonly found in real-world sketches, including artifacts like over-tracing and pen drag. [sent-28, score-0.23]
</p><p>15 However, these methods do not model the spatial relationships between neighboring shapes, relying solely on local appearance to classify a symbol. [sent-29, score-0.527]
</p><p>16 In the following sections we describe our approach, which combines both appearance and context. [sent-30, score-0.338]
</p><p>17 2  Preprocessing  The ﬁrst step in our recognition framework is to preprocess the sketch into a set of simple segments, as shown in Figure 1(b). [sent-32, score-0.299]
</p><p>18 This is not the case when working with the strokes directly, so preprocessing allows us to handle strokes that contain more than one symbol (e. [sent-36, score-0.738]
</p><p>19 Our preprocessing algorithm divides strokes into segments by splitting them at their corner points. [sent-39, score-0.611]
</p><p>20 Previous approaches to corner detection focused primarily on local pen speed and curvature [15], but these measures are not always reliable in messy real-world sketches. [sent-40, score-0.479]
</p><p>21 Our corner detection algorithm, on the other hand, tries to ﬁnd the set of vertices that best approximates the original stroke as a whole. [sent-41, score-0.549]
</p><p>22 At the end of the preprocessing stage, the system records the length of the longest segment L (after excluding the top 5% as outliers). [sent-45, score-0.385]
</p><p>23 3  Symbol Detection  Our algorithm searches for symbols among groups of segments. [sent-47, score-0.218]
</p><p>24 Starting with each segment in isolation, we generate successively larger groups by expanding the group to include the next closest segment3 . [sent-48, score-0.347]
</p><p>25 This process ends when either the size of the group exceeds 2L (a spatial constraint) or 1  Deﬁned as the distance between vi and the line segment formed by vi−1 and vi+1 In our experiments, we set the threshold to 0. [sent-49, score-0.493]
</p><p>26 3 Distance deﬁned as mindist(s, g) + bbdist(s, g), where mindist(s, g) is the distance at the nearest point between segment s and group g and bbdist(s, g) is the diagonal length of the bounding box containing s and g. [sent-51, score-0.502]
</p><p>27 2  2  (a) Original Strokes  (b) Segments after preprocessing  (d) Graphical model G hi l d l  (c) Candidate groups  (e) Fi l d ( ) Final detections i  Figure 1: Our recognition framework. [sent-52, score-0.304]
</p><p>28 (a) An example sketch of a circuit diagram and (b) the segments after preprocessing. [sent-53, score-0.562]
</p><p>29 (c) A subset of the candidate groups extracted from the sketch (only those with an appearance potential > 0. [sent-54, score-0.684]
</p><p>30 (d) The resulting graphical model: nodes represent segment labels, dark blue edges represent group overlap potentials, and light blue edges represent context potentials. [sent-56, score-0.463]
</p><p>31 (e) The ﬁnal set of symbol detections after running loopy belief propagation. [sent-57, score-0.297]
</p><p>32 when the group spans more strokes than the temporal window speciﬁed for the domain4 . [sent-58, score-0.375]
</p><p>33 Note that we allow temporal gaps in the detection region, so symbols do not need to be drawn with consecutive strokes. [sent-59, score-0.255]
</p><p>34 We classify each candidate group using the symbol recognizer we described in [11], which converts the on-line stroke sequences into a set of low resolution feature images (see Figure 2(a)). [sent-61, score-0.907]
</p><p>35 This emphasis on visual appearance makes our method less sensitive to stroke level differences like overtracing and pen drag, improving accuracy and robustness. [sent-62, score-0.93]
</p><p>36 Since [11] was designed for classifying isolated shapes and not for detecting symbols in messy sketches, we augment its output with ﬁve geometric features and a set of local context features: stroke count: The number of strokes in the group. [sent-63, score-1.226]
</p><p>37 segment count: The number of segments in the group. [sent-64, score-0.408]
</p><p>38 group ink density: The total length of the strokes in the group divided by the diagonal length. [sent-66, score-0.622]
</p><p>39 stroke separation: Maximum distance between any stroke and its nearest neighbor in the group. [sent-68, score-0.78]
</p><p>40 local context: A set of four feature images that captures the local context around the group. [sent-69, score-0.197]
</p><p>41 Each image ﬁlters the local appearance at a speciﬁc orientation: 0, 45, 90, and 135 degrees. [sent-70, score-0.353]
</p><p>42 The symbol detector uses a linear SVM [13] to classify each candidate group, labeling it as one of the symbols in the domain or as mis-grouped “clutter”. [sent-73, score-0.532]
</p><p>43 Because the classiﬁer needs to distinguish between more than two classes, we 4 The temporal window is 8 strokes for chemistry diagrams and 20 strokes for the circuit diagrams. [sent-75, score-0.934]
</p><p>44 3  (a) Isolated recognizer features  0  45  90  135  end  (b) Local context features  0  45  90  135  (c) Local context features  0  45  90  135  Figure 2: Symbol Detection Features. [sent-77, score-0.252]
</p><p>45 The ﬁrst four images encode stroke orientation at 0, 45, 90, and 135 degrees; the ﬁfth captures the locations of stroke endpoints. [sent-79, score-0.845]
</p><p>46 For example, an isolated segment always looks like a straight line, so its visual appearance is not very informative. [sent-85, score-0.727]
</p><p>47 , wires in circuits and straight bonds in chemistry): orientation: The orientation of the segment, discretized into evenly space bins of size π/4. [sent-88, score-0.282]
</p><p>48 segment length: The length of the segment, normalized by L. [sent-89, score-0.284]
</p><p>49 segment count: The total number of segments extracted from the parent stroke. [sent-90, score-0.442]
</p><p>50 segment ink density: The length of the substroke matching the start and end points of the segment divided by the length of the segment. [sent-91, score-0.674]
</p><p>51 stroke ink density: The length of the parent stroke divided by the diagonal length of the parent stroke’s bounding box. [sent-93, score-1.169]
</p><p>52 local context: Same as the local context for multi-segment symbols, except these images are centered at the midpoint of the segment, oriented in the same direction as the segment, and scaled so that each dimension is equal to two times the length of the segment. [sent-94, score-0.259]
</p><p>53 4  Improving Recognition using Context  The ﬁnal task is to select a set of symbol detections from the competing candidate groups. [sent-96, score-0.365]
</p><p>54 Second, it should select candidates that are consistent with each other based on what the system knows about the likely spatial relationships between symbols. [sent-99, score-0.254]
</p><p>55 Under our formulation, each segment (node) in the sketch needs to be assigned to one of the candidate groups (labels). [sent-101, score-0.566]
</p><p>56 Thus, our candidate selection problem becomes a segment labeling problem, where the set of possible labels for a given segment is the set of candidate groups that contain that segment. [sent-102, score-0.73]
</p><p>57 This allows us to incorporate local appearance, group overlap consistency, and spatial context into a single uniﬁed model. [sent-103, score-0.316]
</p><p>58 The joint probability function over the entire graph is given by: appearance  log P (c|x) =  overlap  ψo (ci , cj ) + ψc (ci , cj , xi , xj ) − log(Z)  ψa (ci , x) + i  context  (2)  ij  where x is the set of segments in the sketch, c is the set of segment labels, and Z is a normalizing constant. [sent-105, score-1.177]
</p><p>59 The appearance potential ψa measures how well the candidate group’s appearance matches that of its predicted class. [sent-107, score-0.753]
</p><p>60 It uses the output of the isolated symbol classiﬁer in section 4 and is deﬁned as: ψa (ci , x) = log Pa (ci |x)  (3)  where Pa (ci |x) is the likelihood score for candidate ci returned by the isolated symbol classiﬁer. [sent-108, score-0.758]
</p><p>61 The overlap potential ψo (ci , cj ) is a pairwise compatibility that ensures the segment assignments do not conﬂict with each other. [sent-110, score-0.428]
</p><p>62 For example, if segments xi and xj are both members of candidate c and xi is assigned to c, then xj must also be assigned to c. [sent-111, score-0.481]
</p><p>63 ψo (ci , cj ) =  −100, 0,  if ((xi ∈ cj ) or (xj ∈ ci )) and (ci = cj ) otherwise  (4)  To improve efﬁciency, instead of connecting every pair of segments that are jointly considered in c, we connect the segments into a loop based on temporal ordering. [sent-112, score-0.914]
</p><p>64 The context potential ψc (ci , cj , xi , xj ) represents the spatial compatibility between segments xi and xj , conditioned on their predicted class labels (e. [sent-116, score-0.694]
</p><p>65 ψc (ci , cj , xi , xj ) = log Pc (θ(xi , xj ) | class(ci ), class(cj ))  (5)  where class(ci ) is the predicted class for candidate ci and θ(xi , xj ) is the set of three spatial relationships (θ1 , θ2 , θ3 ) between segments xi and xj . [sent-120, score-1.02]
</p><p>66 This potential is active only for pairs of segments whose distance at the closest point is less than L/2. [sent-121, score-0.218]
</p><p>67 , capturing the sketch but providing no recognition or feedback. [sent-132, score-0.336]
</p><p>68 Using the data we collected, we evaluated ﬁve versions of our system: Appearance uses only the isolated appearance-based recognizer from [11]. [sent-133, score-0.197]
</p><p>69 Complete (corner detector from [15]) is the complete framework, using the corner detector in [15]. [sent-137, score-0.281]
</p><p>70 For this domain we noticed that users almost never drew multiple symbols using a single stroke, with the exception of multiple connected straight bonds (e. [sent-145, score-0.388]
</p><p>71 Following this observation, we optimized our candidate extractor to ﬁlter out multi-segment candidates that break stroke boundaries. [sent-148, score-0.56]
</p><p>72 971  Table 1: Overall recognition accuracy for the chemistry dataset. [sent-154, score-0.229]
</p><p>73 Note that for this dataset we report only accuracy (recall), because, unlike traditional object detection, there are no overlapping detections and every stroke is assigned to a symbol. [sent-155, score-0.557]
</p><p>74 , misclassifying one segment in a three-segment “H” makes it impossible to recognize the original “H” correctly. [sent-158, score-0.253]
</p><p>75 The results in Table 1 show that our method was able to recognize 97% of the symbols correctly. [sent-159, score-0.204]
</p><p>76 We can see that the diagrams in this dataset can be very messy, 6  and exhibit a wide range of drawing styles. [sent-163, score-0.284]
</p><p>77 Circuits The second dataset is a collection of circuit diagrams collected by Oltmans and Davis [9]. [sent-165, score-0.318]
</p><p>78 Each user drew ten or eleven different circuits, and every circuit was required to include a pre-speciﬁed set of components. [sent-167, score-0.204]
</p><p>79 Also, since we do not count wire detections for this dataset (as in [9]), we report precision as well as recall. [sent-172, score-0.187]
</p><p>80 912  Table 2: Overall recognition accuracy for the circuit diagram dataset. [sent-185, score-0.32]
</p><p>81 Table 2 shows that our method was able to recognize over 91% of the circuit symbols correctly. [sent-186, score-0.331]
</p><p>82 As Figure 4 (bottom) shows, this is a very complicated and messy corpus with signiﬁcant drawing variations like overtracing and pen drag. [sent-192, score-0.394]
</p><p>83 1 seconds to process a new stroke in the circuits dataset and 0. [sent-194, score-0.506]
</p><p>84 They achieved an accuracy of 62% on a circuits dataset similar to ours, but needed to manually segment any strokes that contained more than one symbol. [sent-201, score-0.634]
</p><p>85 Gennari et al [3] developed a system that searches for symbols in high density regions of the sketch and uses domain knowledge to correct low level recognition errors. [sent-202, score-0.52]
</p><p>86 They reported an accuracy of 77% on a dataset with 6 types of circuit components. [sent-203, score-0.193]
</p><p>87 Sezgin and Davis [16] proposed using an HMM to model the temporal patterns of geometric primitives, and reported an accuracy of 87% on a dataset containing 4 types of circuit components. [sent-204, score-0.262]
</p><p>88 [17] proposed an approach that treats sketch recognition as a visual parsing problem. [sent-207, score-0.352]
</p><p>89 Our work differs from theirs in that we use a rich model of low-level visual appearance and do not require a pre-deﬁned spatial grammar. [sent-208, score-0.458]
</p><p>90 Ouyang and Davis [10] developed a sketch recognition system that uses domain knowledge to reﬁne its interpretation. [sent-209, score-0.347]
</p><p>91 Their work focused on chemical diagrams, and detection was limited to symbols drawn using consecutive strokes. [sent-210, score-0.32]
</p><p>92 Outside of the sketch recognition community, there is also a great deal of interest in combining appearance and context for problems in computer vision [6, 8, 19]. [sent-211, score-0.68]
</p><p>93 7  Figure 4: Examples of chemical diagrams (top) and circuit diagrams (bottom) recognized by our system (complete framework). [sent-212, score-0.588]
</p><p>94 Correct detections are highlighted in green (teal for hash and wedge bonds), false detections in red, and missed symbols in orange. [sent-213, score-0.406]
</p><p>95 6  Discussion  We have proposed a new framework that combines a rich representation of low level visual appearance with a probabilistic model for capturing higher level relationships. [sent-214, score-0.462]
</p><p>96 To our knowledge this is the ﬁrst paper to combine these two approaches, and the result is a recognizer that is better able to handle the range of drawing styles found in messy freehand sketches. [sent-215, score-0.465]
</p><p>97 To preserve the familiar experience of using pen and paper, our system supports the same symbols, notations, and drawing styles that people are already accustomed to. [sent-216, score-0.328]
</p><p>98 In our initial evaluation we apply our method on two real-world domains, chemical diagrams and electrical circuits (with 10 types of components), and achieve accuracy rates of 97% and 91% respectively. [sent-217, score-0.371]
</p><p>99 Compared to existing benchmarks in literature, our method achieved higher accuracy even though the other systems supported fewer symbols [3, 16], trained on data from the same user [3, 16], or required manual pre-segmentation [1]. [sent-218, score-0.237]
</p><p>100 Ladder: a language to describe drawing, display, and editing in sketch recognition. [sent-248, score-0.194]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stroke', 0.39), ('appearance', 0.308), ('strokes', 0.263), ('segment', 0.222), ('sketch', 0.194), ('segments', 0.186), ('symbols', 0.173), ('symbol', 0.159), ('diagrams', 0.158), ('ci', 0.153), ('vi', 0.128), ('circuit', 0.127), ('messy', 0.124), ('sketches', 0.122), ('cj', 0.119), ('corner', 0.109), ('pen', 0.106), ('recognizer', 0.106), ('ink', 0.106), ('candidate', 0.105), ('recognition', 0.105), ('detections', 0.101), ('chemical', 0.097), ('drawing', 0.093), ('isolated', 0.091), ('chemistry', 0.091), ('circuits', 0.083), ('styles', 0.081), ('bonds', 0.081), ('oltmans', 0.081), ('group', 0.08), ('relationships', 0.078), ('primitives', 0.076), ('context', 0.073), ('ouyang', 0.071), ('vj', 0.068), ('davis', 0.067), ('candidates', 0.065), ('spatial', 0.063), ('xj', 0.063), ('detector', 0.062), ('length', 0.062), ('freehand', 0.061), ('sezgin', 0.061), ('shilman', 0.061), ('bounding', 0.06), ('vij', 0.058), ('geometry', 0.055), ('overlap', 0.055), ('diagram', 0.055), ('wire', 0.053), ('preprocessing', 0.053), ('visual', 0.053), ('mse', 0.053), ('straight', 0.053), ('detection', 0.05), ('system', 0.048), ('complete', 0.048), ('graphics', 0.047), ('box', 0.047), ('computers', 0.046), ('drew', 0.046), ('groups', 0.045), ('local', 0.045), ('curvature', 0.045), ('alvarado', 0.04), ('bbdist', 0.04), ('drawings', 0.04), ('gennari', 0.04), ('mindist', 0.04), ('overtracing', 0.04), ('geometric', 0.037), ('belief', 0.037), ('capturing', 0.037), ('resistor', 0.035), ('users', 0.035), ('images', 0.034), ('bins', 0.034), ('parent', 0.034), ('rich', 0.034), ('accuracy', 0.033), ('dataset', 0.033), ('classify', 0.033), ('graphical', 0.033), ('kara', 0.032), ('lifting', 0.032), ('organic', 0.032), ('xi', 0.032), ('potential', 0.032), ('temporal', 0.032), ('false', 0.031), ('recognize', 0.031), ('orientation', 0.031), ('user', 0.031), ('diagonal', 0.031), ('labels', 0.031), ('variations', 0.031), ('informal', 0.03), ('shapes', 0.03), ('combines', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="131-tfidf-1" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>2 0.13296694 <a title="131-tfidf-2" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>Author: Stephen Gould, Tianshi Gao, Daphne Koller</p><p>Abstract: Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classiﬁcation of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach simultaneously reasons about pixels, regions and objects in a coherent probabilistic model. Pixel appearance features allow us to perform well on classifying amorphous background classes, while the explicit representation of regions facilitate the computation of more sophisticated features necessary for object detection. Importantly, our model gives a single uniﬁed description of the scene—we explain every pixel in the image and enforce global consistency between all random variables in our model. We run experiments on the challenging Street Scene dataset [2] and show signiﬁcant improvement over state-of-the-art results for object detection accuracy. 1</p><p>3 0.11928639 <a title="131-tfidf-3" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>4 0.088947274 <a title="131-tfidf-4" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><p>5 0.084376492 <a title="131-tfidf-5" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>Author: Tomasz Malisiewicz, Alyosha Efros</p><p>Abstract: The use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the object’s relationship to other elements of the scene (context). Most current approaches rely on modeling the relationships between object categories as a source of context. In this paper we seek to move beyond categories to provide a richer appearancebased model of context. We present an exemplar-based model of objects and their relationships, the Visual Memex, that encodes both local appearance and 2D spatial context between object instances. We evaluate our model on Torralba’s proposed Context Challenge against a baseline category-based system. Our experiments suggest that moving beyond categories for context modeling appears to be quite beneﬁcial, and may be the critical missing ingredient in scene understanding systems. 1</p><p>6 0.077810183 <a title="131-tfidf-6" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>7 0.073688678 <a title="131-tfidf-7" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>8 0.070054397 <a title="131-tfidf-8" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>9 0.067831382 <a title="131-tfidf-9" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>10 0.062603906 <a title="131-tfidf-10" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>11 0.058971204 <a title="131-tfidf-11" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>12 0.058858704 <a title="131-tfidf-12" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>13 0.05773855 <a title="131-tfidf-13" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>14 0.056947153 <a title="131-tfidf-14" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>15 0.056298859 <a title="131-tfidf-15" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>16 0.055797338 <a title="131-tfidf-16" href="./nips-2009-Who%E2%80%99s_Doing_What%3A_Joint_Modeling_of_Names_and_Verbs_for_Simultaneous_Face_and_Pose_Annotation.html">259 nips-2009-Who’s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation</a></p>
<p>17 0.052288301 <a title="131-tfidf-17" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>18 0.050356854 <a title="131-tfidf-18" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>19 0.048789132 <a title="131-tfidf-19" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>20 0.048780087 <a title="131-tfidf-20" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.177), (1, -0.088), (2, -0.068), (3, -0.015), (4, -0.028), (5, 0.047), (6, 0.034), (7, 0.035), (8, 0.042), (9, -0.064), (10, -0.017), (11, -0.005), (12, 0.067), (13, -0.028), (14, 0.047), (15, 0.02), (16, 0.014), (17, -0.059), (18, 0.018), (19, -0.134), (20, 0.007), (21, -0.004), (22, -0.083), (23, 0.026), (24, -0.083), (25, 0.04), (26, 0.025), (27, 0.055), (28, -0.016), (29, -0.003), (30, -0.022), (31, 0.105), (32, 0.08), (33, 0.024), (34, -0.025), (35, -0.086), (36, 0.052), (37, -0.01), (38, -0.064), (39, 0.082), (40, 0.048), (41, 0.124), (42, -0.055), (43, -0.097), (44, -0.024), (45, 0.008), (46, 0.061), (47, 0.048), (48, -0.02), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94852054 <a title="131-lsi-1" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>2 0.59011269 <a title="131-lsi-2" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>Author: Tomasz Malisiewicz, Alyosha Efros</p><p>Abstract: The use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the object’s relationship to other elements of the scene (context). Most current approaches rely on modeling the relationships between object categories as a source of context. In this paper we seek to move beyond categories to provide a richer appearancebased model of context. We present an exemplar-based model of objects and their relationships, the Visual Memex, that encodes both local appearance and 2D spatial context between object instances. We evaluate our model on Torralba’s proposed Context Challenge against a baseline category-based system. Our experiments suggest that moving beyond categories for context modeling appears to be quite beneﬁcial, and may be the critical missing ingredient in scene understanding systems. 1</p><p>3 0.58901268 <a title="131-lsi-3" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>4 0.57737738 <a title="131-lsi-4" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>Author: Stephen Gould, Tianshi Gao, Daphne Koller</p><p>Abstract: Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classiﬁcation of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach simultaneously reasons about pixels, regions and objects in a coherent probabilistic model. Pixel appearance features allow us to perform well on classifying amorphous background classes, while the explicit representation of regions facilitate the computation of more sophisticated features necessary for object detection. Importantly, our model gives a single uniﬁed description of the scene—we explain every pixel in the image and enforce global consistency between all random variables in our model. We run experiments on the challenging Street Scene dataset [2] and show signiﬁcant improvement over state-of-the-art results for object detection accuracy. 1</p><p>5 0.55576688 <a title="131-lsi-5" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><p>6 0.5498895 <a title="131-lsi-6" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>7 0.54727125 <a title="131-lsi-7" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>8 0.52664149 <a title="131-lsi-8" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>9 0.5231939 <a title="131-lsi-9" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>10 0.51782233 <a title="131-lsi-10" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>11 0.49556273 <a title="131-lsi-11" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>12 0.48618659 <a title="131-lsi-12" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>13 0.45286083 <a title="131-lsi-13" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>14 0.4414148 <a title="131-lsi-14" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>15 0.43015343 <a title="131-lsi-15" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>16 0.43009228 <a title="131-lsi-16" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>17 0.4190819 <a title="131-lsi-17" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>18 0.41345084 <a title="131-lsi-18" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>19 0.41179875 <a title="131-lsi-19" href="./nips-2009-Maximin_affinity_learning_of_image_segmentation.html">149 nips-2009-Maximin affinity learning of image segmentation</a></p>
<p>20 0.41043812 <a title="131-lsi-20" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.012), (21, 0.015), (24, 0.033), (25, 0.125), (33, 0.219), (35, 0.108), (36, 0.085), (39, 0.058), (58, 0.069), (71, 0.064), (81, 0.035), (86, 0.076), (91, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85111147 <a title="131-lda-1" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>2 0.80643117 <a title="131-lda-2" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>3 0.69883221 <a title="131-lda-3" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><p>4 0.69810325 <a title="131-lda-4" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>Author: Joseph Schlecht, Kobus Barnard</p><p>Abstract: We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g. chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances. We learn the models using joint statistical inference over category parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images. 1</p><p>5 0.68250525 <a title="131-lda-5" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>6 0.68249518 <a title="131-lda-6" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>7 0.68224555 <a title="131-lda-7" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>8 0.68199033 <a title="131-lda-8" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>9 0.67989266 <a title="131-lda-9" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>10 0.67729872 <a title="131-lda-10" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>11 0.6762554 <a title="131-lda-11" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>12 0.675892 <a title="131-lda-12" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>13 0.67430437 <a title="131-lda-13" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>14 0.67221236 <a title="131-lda-14" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>15 0.67135316 <a title="131-lda-15" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>16 0.67088872 <a title="131-lda-16" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>17 0.67065805 <a title="131-lda-17" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>18 0.66816705 <a title="131-lda-18" href="./nips-2009-Anomaly_Detection_with_Score_functions_based_on_Nearest_Neighbor_Graphs.html">34 nips-2009-Anomaly Detection with Score functions based on Nearest Neighbor Graphs</a></p>
<p>19 0.66793031 <a title="131-lda-19" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>20 0.66494119 <a title="131-lda-20" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
