<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-228" href="#">nips2009-228</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</h1>
<br/><p>Source: <a title="nips-2009-228-pdf" href="http://papers.nips.cc/paper/3712-speeding-up-magnetic-resonance-image-acquisition-by-bayesian-multi-slice-adaptive-compressed-sensing.pdf">pdf</a></p><p>Author: Matthias Seeger</p><p>Abstract: We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a ﬁrst study, designs are found that improve signiﬁcantly on others chosen independently for each slice or drawn at random. 1</p><p>Reference: <a title="nips-2009-228-reference" href="../nips2009_reference/nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In a ﬁrst study, designs are found that improve signiﬁcantly on others chosen independently for each slice or drawn at random. [sent-7, score-0.284]
</p><p>2 1  Introduction  Magnetic resonance imaging (MRI) [10, 6] is a very ﬂexible imaging modality. [sent-8, score-0.164]
</p><p>3 Its most serious limitation is acquisition speed, being based on a serial idea (gradient encoding) with limited scope for parallelization. [sent-10, score-0.131]
</p><p>4 Long scan times lead to patient annoyance, grave errors due to movement, and high running costs. [sent-13, score-0.187]
</p><p>5 The Nyquist sampling theorem [2] fundamentally limits traditional linear image reconstruction, but with modern 3D MRI scenarios, dense sampling is not practical anymore. [sent-14, score-0.141]
</p><p>6 Acquisition is accelerated to some extent in parallel MRI1 , by using receive coil arrays [19, 9]: the sensitivity proﬁles of different coils provide part of the localization normally done by more phase steps. [sent-15, score-0.229]
</p><p>7 A different idea is to use (nonlinear) sparse image reconstruction, with which the Nyquist limit can be undercut robustly for images, emphasized recently as compressed sensing [5, 3]. [sent-16, score-0.208]
</p><p>8 Our approach is in line with recent endeavours to extend MRI capabilities and reduce its cost, by complementing expensive, serial hardware with easily parallelizable digital computations. [sent-18, score-0.108]
</p><p>9 We extend the framework of [24], the ﬁrst approximate Bayesian method for MRI sampling optimization applicable at resolutions of clinical interest. [sent-19, score-0.132]
</p><p>10 They considered single image slices only, while stacks2 of neighbouring 1  While parallel MRI is becoming the standard, its use is not straightforward. [sent-21, score-0.49]
</p><p>11 2 “Stack-of-slices” acquisition along the z axis works by transmitting a narrow-band excitation pulse while applying a magnetic ﬁeld gradient linear in z. [sent-23, score-0.16]
</p><p>12 If the echo time (between excitation and readout) is shorter than  1  slices are typically acquired. [sent-24, score-0.233]
</p><p>13 Reconstruction can be improved signiﬁcantly by taking the strong statistical dependence between pixels of nearby slices into account [14, 26, 18]. [sent-25, score-0.233]
</p><p>14 Design optimization is a joint problem as well: using the same acquisition pattern for neighbouring slices is clearly redundant. [sent-26, score-0.465]
</p><p>15 Our extension to stacks of slices requires new technology. [sent-29, score-0.27]
</p><p>16 Global Gaussian covariances have to be approximated, a straightforward extension of which to many slices is out of the question. [sent-30, score-0.286]
</p><p>17 We show how to use approximate Kalman smoothing, implementing message passing by the Lanczos algorithm, which has not been done in machine learning before (see [20, 25] for similar proposals to oceanography problems). [sent-31, score-0.195]
</p><p>18 Our technique is complementary to mean ﬁeld variational inference approximations (“variational Bayes”), where most correlations are ruled out a priori. [sent-32, score-0.183]
</p><p>19 We track the dominating posterior covariance directions inside our method, allowing them to change during optimization. [sent-33, score-0.143]
</p><p>20 While our double loop approach may be technically more demanding to implement, relaxation as well as algorithm are characterized much better (convex problem; algorithm reducing to standard computational primitives), running orders of magnitude faster. [sent-34, score-0.177]
</p><p>21 Large scale variational inference is reviewed and extended to complex-valued data in Section 2, lifted to non-Gaussian linear dynamical systems in Section 3, and the experimental design extension is given in Section 4. [sent-38, score-0.303]
</p><p>22 A latent MR image slice u ∈ Cn (n pixels) is measured by a design matrix X ∈ Cm×n : y = Xu + ε (ε ∼ N (0, σ 2 I) models noise). [sent-41, score-0.326]
</p><p>23 , n} the sampling pattern (which partitions into complete columns or rows: phase encodes, the atomic units of the design). [sent-45, score-0.148]
</p><p>24 Sparse reconstruction works by encoding super-Gaussian image statistics in a non-Gaussian prior, then ﬁnding the posterior mode (MAP estimation): a convex quadratic program for the model employed here. [sent-46, score-0.301]
</p><p>25 To improve the measurement design X itself, posterior information beyond (and independent of) its mode is required, chieﬂy posterior covariances. [sent-47, score-0.22]
</p><p>26 The super-Gaussian image prior P (u) is adapted by placing potentials on absolute values |sj |, the posterior has the form P (u|y) ∝ N (y|Xu, σ 2 I)  q j=1  e−τj |sj /σ| ,  s = Bu ∈ Cq . [sent-49, score-0.193]
</p><p>27 We use the C → R2 embedding, s = (sj ), sj ∈ R2 , and norm potentials e−τj sj /σ . [sent-51, score-0.693]
</p><p>28 the repeat time (between phase encodes), several slices are acquired in an interleaved fashion, separated by slice gaps to avoid crosstalk [17]. [sent-61, score-0.547]
</p><p>29 2  Second, φ(γ) can be minimized very efﬁciently by a double loop algorithm [24]. [sent-62, score-0.132]
</p><p>30 In terms of Gaussian (Markov) random ﬁelds, the inner optimization needs posterior mean computations only, while OL updates require bulk Gaussian variances [21, 15]. [sent-69, score-0.241]
</p><p>31 The reason why the double loop algorithm is much faster than previous approaches is that only few variance computations are required. [sent-70, score-0.201]
</p><p>32 , T , we can use an undirected hidden Markov model over image slices u = (ut ) ∈ CnT . [sent-75, score-0.29]
</p><p>33 By the stack-of-slices methodology, the likelihood potentials P (yt |ut ) are independent, and P (ut ) from above serves as single-node potential, based on st = But . [sent-76, score-0.131]
</p><p>34 If st→ := ut − ut+1 , the dependence between neighbouring slices is captured by n additional Laplace coupling potentials i=1 e−τc,i |(st→ )i /σ| . [sent-77, score-0.539]
</p><p>35 The variational parameters γt at each node are complemented by coupling parameters γt→ ∈ Rn . [sent-78, score-0.139]
</p><p>36 How will an efﬁcient extension of the double loop algorithm look like? [sent-81, score-0.132]
</p><p>37 The IL criterion φz should be coupled between neighbouring slices, by way of potentials on st→ . [sent-82, score-0.194]
</p><p>38 We will do this by Kalman smoothing, approximating inversion in message computations (conversion from natural to moment parameters) by the Lanczos algorithm. [sent-84, score-0.153]
</p><p>39 3  Approximate Inference over Multiple Slices  We aim to extend the single slice method of [24] to the hidden Markov extension, thereby reusing code whenever possible. [sent-92, score-0.155]
</p><p>40 practice, we average over t Algorithm 1 Double loop variational inference algorithm repeat if ﬁrst iteration then Default-initialize z ∝ 1, u = 0. [sent-96, score-0.224]
</p><p>41 Determine node variances zt , pair variances zt→ , and log |A| from messages. [sent-98, score-0.19]
</p><p>42 Each local update of ut entails solving a linear system (conjugate gradients). [sent-105, score-0.112]
</p><p>43 until outer loop converged For reconstruction, we run parallel MAP estimation. [sent-107, score-0.206]
</p><p>44 Nodes return with ut φz at the line minimum ut , the next search direction is centrally determined and distributed (just a scalar has to be transferred). [sent-109, score-0.224]
</p><p>45 We brieﬂy comment on how to approximate Kalman message passing by way of the Lanczos algorithm [8], full details are given in [22]. [sent-111, score-0.153]
</p><p>46 For a low rank PCA approximation of A t→ , Mt→ has the same rank (see Appendix), which allows to run Gaussian message passing tractably. [sent-116, score-0.292]
</p><p>47 In a parallel implementation, the forward and backward ﬁlter passes run in parallel, passing low rank messages (the rank km of these should be smaller than the rank kc for subsequent marginal covariance computations). [sent-117, score-0.581]
</p><p>48 4  4  Sampling Optimization by Bayesian Experimental Design  With our multi-slice variational inference algorithm in place, we address sampling optimization by Bayesian sequential experimental design, following [24]. [sent-119, score-0.231]
</p><p>49 At slice t, the information gain score T ∆(X∗ ) := log |I +X∗ CovQ [ut |y]X∗ | is computed for a ﬁxed number of phase encode candidates d×n X∗ ∈ C not yet in Xt , the score maximizer is appended, and a novel measurement is acquired (for the maximizer only). [sent-120, score-0.348]
</p><p>50 Once messages have been passed, scores can be computed in parallel at different nodes. [sent-122, score-0.144]
</p><p>51 A purely sequential approach, extending one design Xt by one encode in each round, is not tractable. [sent-123, score-0.114]
</p><p>52 In practice, we extend several node designs Xt in each round (a ﬁxed subset Cit ⊂ {1, . [sent-124, score-0.216]
</p><p>53 In the interleaved stack-of-slices methodology, scan time is determined by the largest factor Xt (number of rows), so we strive for balanced designs here. [sent-131, score-0.225]
</p><p>54 To sum up, our adaptive design optimization algorithm starts with an initial variational inference phase for a start-up design (low frequencies only), then runs through a ﬁxed number of design rounds. [sent-132, score-0.637]
</p><p>55 Each round starts with Gaussian message passing, based on which scores are computed at nodes t ∈ Cit , new measurements are acquired, and designs Xt are extended. [sent-133, score-0.262]
</p><p>56 Finally, variational inference is run for the extended model, using a small number of OL iterations (only one in our experiments). [sent-134, score-0.185]
</p><p>57 Time can be saved by basing the ﬁrst OL update on the same messages and node marginal covariances than the design score computations (neglecting their change through new phase encodes). [sent-135, score-0.435]
</p><p>58 5  Experiments  We present experimental results, comparing designs found by our Bayesian joint design optimization method against alternative choices on real MRI data. [sent-136, score-0.286]
</p><p>59 We focus on Cartesian MRI (phase encodes are complete 3 columns in k-space): a more clinically relevant setting than spiral sampling treated in [24]. [sent-139, score-0.177]
</p><p>60 While this is not a resolution of clinical relevance, a truly parallel implementation is required in order to run our method at resolutions 256 × 256 or beyond: an important point for future work. [sent-141, score-0.21]
</p><p>61 While the relative approximation errors are rather large uniformly, there is a clear structure to them: the largest (and also the very smallest) true values zj are approximated signiﬁcantly more accurately than smaller true values. [sent-147, score-0.151]
</p><p>62 This structure can be used to motivate why, in the presence of large errors over all coefﬁcients, our inference still works well for sparse linear models, indeed in some cases better than if exact computations are used (Figure 1, upper right). [sent-148, score-0.195]
</p><p>63 88 1  2  3  4 Outer loop iteration  5  6  7  Figure 1: Lanczos approximations of Gaussian variances, at beginning of second OL iteration, 64 × 64 data (upper left). [sent-161, score-0.115]
</p><p>64 l2 reconstruction error of posterior mean estimate after subsequent OL iterations, for exact variance computation vs. [sent-163, score-0.244]
</p><p>65 Lower panel: Relative accuracy zj → zk,j /zj at beginning of second OL iteration, separately for “a” sites (on wavelet coefﬁcients; red), “r” sites (on derivatives; blue), and “i” sites (on (u); green). [sent-165, score-0.305]
</p><p>66 Contributions to the largest values zj come dominatingly from small eigenvalues (large eigenvalues of A−1 ), explaining their smaller relative error. [sent-167, score-0.105]
</p><p>67 On the other hand, smaller values zj are strongly underestimated (zk,j zj ), which means that the selective shrinkage effect underlying sparse linear models (shrink most coefﬁcients strongly, but some not at all) is strengthened by these systematic errors. [sent-168, score-0.245]
</p><p>68 Finally, the IL penalties are τj (zj + |sj /σ|2 )1/2 , enforcing sparsity more strongly for smaller zj . [sent-169, score-0.143]
</p><p>69 Therefore, Lanczos approximation errors lead to strengthened sparsity in subsequent ILs, but least so for sites with largest true zj . [sent-170, score-0.244]
</p><p>70 As an educated guess, this effect might even compensate for the fact that Laplace potentials may not be sparse enough for natural images. [sent-171, score-0.118]
</p><p>71 2  Joint Design Optimization  We use sagittal head scan data of resolution 64 × 64 in-plane, 32 slices, acquired on a Siemens 3T scanner (phase direction anterior-posterior), see [22] for further details. [sent-173, score-0.255]
</p><p>72 Hyperparameters are adjusted based on MAP reconstruction results for a ﬁxed design picked ad hoc (τa = τr = 0. [sent-177, score-0.305]
</p><p>73 08 between slices), then used for all design optimization and MAP reconstruction runs. [sent-180, score-0.348]
</p><p>74 For variational inference, we run 6 OL iterations in the initial, 1 OL iteration in each design round, with up to 30 IL steps (ILs in design rounds typically converged in 2–3 steps). [sent-185, score-0.368]
</p><p>75 The rank parameters (number of Lanczos steps)4 were km = 100, kc = 250 (here, ut has n = 8192 real coefﬁcients). [sent-186, score-0.306]
</p><p>76 ˜ First, across all designs, joint MAP reconstruction improves signiﬁcantly upon independent MAP reconstruction. [sent-188, score-0.191]
</p><p>77 Left: joint MAP reconstruction; right: independent MAP reconstruction of each slice. [sent-191, score-0.191]
</p><p>78 op-jt: {Xt } optimized jointly; op-sp: Xt optimized separately for each slice; op-eq: Xt = X, optimized on slice 16; rd: Xt variable density drawn at random (averaged over 10 repetitions). [sent-192, score-0.272]
</p><p>79 Rows 2–4: Images for op-jt (25 encodes), slices 15–17. [sent-193, score-0.233]
</p><p>80 encodes, where scan time is reduced by a factor 2–4 (Nyquist sampling requires 64 phase encodes). [sent-200, score-0.244]
</p><p>81 op-eq does worst in this domain: with a model of dependencies between slices in place, it pays 7  off to choose different Xt for each slice. [sent-201, score-0.233]
</p><p>82 While this suboptimal behaviour of our optimization will be analyzed more closely in future work, it is our experience so far that the gain in using greedy sequential Bayesian design optimization over simpler choices is generally largest below 1/2 Nyquist. [sent-203, score-0.2]
</p><p>83 6  Conclusions  We showed how to implement MRI sampling optimization by Bayesian sequential experimental design, jointly over a stack of neighbouring slices, extending the single slice technique of [24]. [sent-204, score-0.351]
</p><p>84 Our method is a general alternative to structured variational mean ﬁeld approximations typically used for non-Gaussian dynamical systems, in that dominating covariances are tracked a posteriori, rather than eliminating most of them a priori through factorization assumptions. [sent-208, score-0.289]
</p><p>85 In future work, we will develop a truly parallel implementation, with which higher resolutions can be processed. [sent-210, score-0.136]
</p><p>86 We are considering extensions of our design optimization technology to 3D MRI5 and to parallel MRI with receiver coil arrays [19, 9], whose combination with k-space undersampling can be substantially more powerful than each acceleration technique on its own [13]. [sent-211, score-0.322]
</p><p>87 Appendix For norm potentials, h∗ (sj ) = h∗ ( sj ), and the Hessians to solve for IRLS Newton directions j j do not have the form of A anymore. [sent-212, score-0.305]
</p><p>88 If θj := (h∗ ) , ρj := j ∗ 2 2 (h∗ ) at sj = 0, then using sj sj = sj / sj , we have sj hj = ρj I2 + κj ( sj I2 − j T 1/2 2 T T T T sj sj ), κj := (θj / sj − ρj ) / sj . [sent-215, score-3.355]
</p><p>89 Since sj I2 − sj sj = νsj (νsj ) , ν := δ2 δ1 − δ1 δ2 , ˆ the Hessian is X H X + BH (s) B T . [sent-216, score-0.915]
</p><p>90 If s := ((diag κ) ⊗ ν)s, then for any v ∈ R2q : H (s) v = T ˆ ((diag ρ)⊗I2 )v+((diag w)⊗I2 )ˆ , where wj := vj sj , j = 1, . [sent-217, score-0.305]
</p><p>91 Given all messages, node covariances are PCA-approximated by running Lanczos on T T At + V(t−1)→ V(t−1)→ + V←(t+1) V←(t+1) for kc iterations. [sent-224, score-0.193]
</p><p>92 Pair variances VarQ [st→ |y] are estimated by running Lanczos on vectors of size 2˜ (say for kc /2 iterations; the precision matrix is n given in Section 3). [sent-225, score-0.178]
</p><p>93 u 5 In 3D MRI, image volumes are acquired without slice selection, using phase encoding along two dimensions. [sent-229, score-0.371]
</p><p>94 There are no unmeasured slice gaps and voxels are isotropic, but scan time is much longer. [sent-230, score-0.251]
</p><p>95 Robust uncertainty principles: Exact signal reconstruction from e highly incomplete frequency information. [sent-248, score-0.191]
</p><p>96 A variational method for learning sparse and overcomplete representations. [sent-279, score-0.136]
</p><p>97 Sparse MRI: The application of compressed sensing for rapid MR imaging. [sent-320, score-0.116]
</p><p>98 SPIR-iT: Iterative self consistent parallel imaging reconstruction from arbitrary k-space. [sent-328, score-0.332]
</p><p>99 Speeding up magnetic resonance image acquisition by Bayesian multi-slice adaptive compressed sensing. [sent-405, score-0.339]
</p><p>100 Bayesian experimental design of magnetic o resonance imaging sequences. [sent-417, score-0.308]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lanczos', 0.4), ('sj', 0.305), ('mri', 0.293), ('slices', 0.233), ('ol', 0.206), ('reconstruction', 0.191), ('slice', 0.155), ('encodes', 0.135), ('designs', 0.129), ('design', 0.114), ('ut', 0.112), ('neighbouring', 0.111), ('phase', 0.106), ('xt', 0.106), ('zj', 0.105), ('variational', 0.101), ('scan', 0.096), ('cit', 0.092), ('irls', 0.092), ('parallel', 0.089), ('km', 0.087), ('message', 0.084), ('potentials', 0.083), ('kalman', 0.082), ('magnetic', 0.082), ('loop', 0.078), ('acquisition', 0.078), ('qt', 0.078), ('variances', 0.076), ('cartesian', 0.075), ('op', 0.074), ('computations', 0.069), ('passing', 0.069), ('ils', 0.063), ('nyquist', 0.063), ('compressed', 0.062), ('resonance', 0.06), ('image', 0.057), ('kc', 0.057), ('messages', 0.055), ('parallelizable', 0.055), ('dominating', 0.055), ('sites', 0.055), ('sensing', 0.054), ('double', 0.054), ('acquired', 0.053), ('serial', 0.053), ('covariances', 0.053), ('posterior', 0.053), ('imaging', 0.052), ('map', 0.051), ('rank', 0.05), ('vt', 0.05), ('round', 0.049), ('st', 0.048), ('mr', 0.048), ('resolutions', 0.047), ('diag', 0.047), ('errors', 0.046), ('running', 0.045), ('inference', 0.045), ('seeger', 0.045), ('dynamical', 0.043), ('bayesian', 0.043), ('optimization', 0.043), ('tt', 0.043), ('borig', 0.042), ('covq', 0.042), ('lustig', 0.042), ('oceanography', 0.042), ('pohmann', 0.042), ('undersampling', 0.042), ('sampling', 0.042), ('mt', 0.04), ('xu', 0.039), ('run', 0.039), ('optimized', 0.039), ('cients', 0.038), ('node', 0.038), ('sparsity', 0.038), ('partly', 0.037), ('eq', 0.037), ('krylov', 0.037), ('sagittal', 0.037), ('cnt', 0.037), ('varq', 0.037), ('schneider', 0.037), ('stacks', 0.037), ('approximations', 0.037), ('decay', 0.036), ('images', 0.036), ('edition', 0.036), ('wavelet', 0.035), ('resolution', 0.035), ('covariance', 0.035), ('sparse', 0.035), ('candidates', 0.034), ('il', 0.034), ('coil', 0.034), ('scanner', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="228-tfidf-1" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a ﬁrst study, designs are found that improve signiﬁcantly on others chosen independently for each slice or drawn at random. 1</p><p>2 0.12991114 <a title="228-tfidf-2" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>Author: John Langford, Tong Zhang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting. 1</p><p>3 0.11264502 <a title="228-tfidf-3" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>Author: Sebastian Gerwinn, Leonard White, Matthias Kaschube, Matthias Bethge, Jakob H. Macke</p><p>Abstract: Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial and temporal scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data. In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data. We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reﬂects known properties of OPMs, and a noise covariance adjusted to the data. The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements. By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations, pinwheel locations or pinwheel counts. Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and quantitative model comparisons. We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex. 1</p><p>4 0.11189042 <a title="228-tfidf-4" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>Author: Jean Honorio, Dimitris Samaras, Nikos Paragios, Rita Goldstein, Luis E. Ortiz</p><p>Abstract: Locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes, motion trajectories, 2D and 3D images). Although these datasets are typically under-sampled and high-dimensional, they often need to be represented with low-complexity statistical models, which are comprised of only the important probabilistic dependencies in the datasets. Most methods attempt to reduce model complexity by enforcing structure sparseness. However, sparseness cannot describe inherent regularities in the structure. Hence, in this paper we ﬁrst propose a new class of Gaussian graphical models which, together with sparseness, imposes local constancy through 1 -norm penalization. Second, we propose an efﬁcient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions. Through synthetic experiments, we evaluate the closeness of the recovered models to the ground truth. We also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it captures useful structures such as the rotation and shrinking of a beating heart, motion correlations between body parts during walking and functional interactions of brain regions. Our method outperforms the state-of-the-art structure learning techniques for Gaussian graphical models both for small and large datasets. 1</p><p>5 0.10060892 <a title="228-tfidf-5" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>6 0.091620259 <a title="228-tfidf-6" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>7 0.090194114 <a title="228-tfidf-7" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>8 0.089546755 <a title="228-tfidf-8" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>9 0.085636109 <a title="228-tfidf-9" href="./nips-2009-On_Stochastic_and_Worst-case_Models_for_Investing.html">178 nips-2009-On Stochastic and Worst-case Models for Investing</a></p>
<p>10 0.084568307 <a title="228-tfidf-10" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>11 0.084179372 <a title="228-tfidf-11" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>12 0.081394292 <a title="228-tfidf-12" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>13 0.081389964 <a title="228-tfidf-13" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>14 0.080489911 <a title="228-tfidf-14" href="./nips-2009-Reconstruction_of_Sparse_Circuits_Using_Multi-neuronal_Excitation_%28RESCUME%29.html">200 nips-2009-Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></p>
<p>15 0.08041703 <a title="228-tfidf-15" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>16 0.075075477 <a title="228-tfidf-16" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>17 0.072496012 <a title="228-tfidf-17" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>18 0.071501762 <a title="228-tfidf-18" href="./nips-2009-Information-theoretic_lower_bounds_on_the_oracle_complexity_of_convex_optimization.html">116 nips-2009-Information-theoretic lower bounds on the oracle complexity of convex optimization</a></p>
<p>19 0.071460813 <a title="228-tfidf-19" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>20 0.071311675 <a title="228-tfidf-20" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.231), (1, -0.004), (2, 0.047), (3, -0.02), (4, 0.113), (5, 0.035), (6, 0.139), (7, -0.046), (8, 0.021), (9, -0.045), (10, -0.042), (11, 0.054), (12, -0.022), (13, 0.058), (14, -0.044), (15, 0.002), (16, -0.053), (17, 0.088), (18, -0.034), (19, -0.076), (20, 0.027), (21, -0.001), (22, 0.029), (23, 0.138), (24, 0.118), (25, 0.009), (26, -0.101), (27, -0.091), (28, 0.003), (29, -0.004), (30, -0.089), (31, 0.003), (32, -0.024), (33, 0.06), (34, -0.003), (35, 0.07), (36, 0.069), (37, 0.076), (38, 0.022), (39, 0.013), (40, -0.03), (41, -0.04), (42, 0.093), (43, 0.031), (44, -0.171), (45, -0.057), (46, 0.095), (47, -0.122), (48, 0.004), (49, -0.146)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95327252 <a title="228-lsi-1" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a ﬁrst study, designs are found that improve signiﬁcantly on others chosen independently for each slice or drawn at random. 1</p><p>2 0.57653803 <a title="228-lsi-2" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>Author: John Langford, Tong Zhang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting. 1</p><p>3 0.55248928 <a title="228-lsi-3" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>Author: Dilip Krishnan, Rob Fergus</p><p>Abstract: The heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising, deblurring and super-resolution. α These distributions are well modeled by a hyper-Laplacian p(x) ∝ e−k|x| , typically with 0.5 ≤ α ≤ 0.8. However, the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images. In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-Laplacian priors. We adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels. This per-pixel sub-problem may be solved with a lookup table (LUT). Alternatively, for two speciﬁc values of α, 1/2 and 2/3 an analytic solution can be found, by ﬁnding the roots of a cubic and quartic polynomial, respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve a 1 megapixel image in less than ∼3 seconds, achieving comparable quality to existing methods such as iteratively reweighted least squares (IRLS) that take ∼20 minutes. Furthermore, our method is quite general and can easily be extended to related image processing problems, beyond the deconvolution application demonstrated. 1</p><p>4 0.54644585 <a title="228-lsi-4" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>Author: Jaakko Luttinen, Alexander T. Ihler</p><p>Abstract: We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</p><p>5 0.51724654 <a title="228-lsi-5" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>Author: Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R. Movellan, Paul L. Ruvolo</p><p>Abstract: Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector [9]). New Internet-based services allow for a large number of labelers to collaborate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difﬁculty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difﬁculty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used “Majority Vote” heuristic for inferring image labels, and is robust to both noisy and adversarial labelers. 1</p><p>6 0.51216483 <a title="228-lsi-6" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>7 0.50739747 <a title="228-lsi-7" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>8 0.49901107 <a title="228-lsi-8" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>9 0.47472051 <a title="228-lsi-9" href="./nips-2009-Orthogonal_Matching_Pursuit_From_Noisy_Random_Measurements%3A_A_New_Analysis.html">185 nips-2009-Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></p>
<p>10 0.46848327 <a title="228-lsi-10" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>11 0.45631754 <a title="228-lsi-11" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>12 0.45232344 <a title="228-lsi-12" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>13 0.4351531 <a title="228-lsi-13" href="./nips-2009-Inter-domain_Gaussian_Processes_for_Sparse_Inference_using_Inducing_Features.html">117 nips-2009-Inter-domain Gaussian Processes for Sparse Inference using Inducing Features</a></p>
<p>14 0.43282026 <a title="228-lsi-14" href="./nips-2009-Breaking_Boundaries_Between_Induction_Time_and_Diagnosis_Time_Active_Information_Acquisition.html">49 nips-2009-Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition</a></p>
<p>15 0.43142217 <a title="228-lsi-15" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>16 0.43091264 <a title="228-lsi-16" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>17 0.43081591 <a title="228-lsi-17" href="./nips-2009-Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units.html">186 nips-2009-Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units</a></p>
<p>18 0.41564476 <a title="228-lsi-18" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>19 0.41095969 <a title="228-lsi-19" href="./nips-2009-Adaptive_Regularization_of_Weight_Vectors.html">27 nips-2009-Adaptive Regularization of Weight Vectors</a></p>
<p>20 0.40965062 <a title="228-lsi-20" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.016), (24, 0.036), (25, 0.065), (35, 0.065), (36, 0.119), (39, 0.052), (50, 0.217), (58, 0.123), (61, 0.022), (66, 0.014), (71, 0.05), (81, 0.044), (86, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9009006 <a title="228-lda-1" href="./nips-2009-Optimal_Scoring_for_Unsupervised_Learning.html">182 nips-2009-Optimal Scoring for Unsupervised Learning</a></p>
<p>Author: Zhihua Zhang, Guang Dai</p><p>Abstract: We are often interested in casting classiﬁcation and clustering problems as a regression framework, because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria. In this paper we illustrate optimal scoring, which was originally proposed for performing the Fisher linear discriminant analysis by regression, in the application of unsupervised learning. In particular, we devise a novel clustering algorithm that we call optimal discriminant clustering. We associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering, discriminative clustering and sparse principal component analysis. Experimental results on a collection of benchmark datasets validate the effectiveness of the optimal discriminant clustering algorithm.</p><p>same-paper 2 0.87413752 <a title="228-lda-2" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a ﬁrst study, designs are found that improve signiﬁcantly on others chosen independently for each slice or drawn at random. 1</p><p>3 0.74869025 <a title="228-lda-3" href="./nips-2009-Statistical_Analysis_of_Semi-Supervised_Learning%3A_The_Limit_of_Infinite_Unlabelled_Data.html">229 nips-2009-Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></p>
<p>Author: Boaz Nadler, Nathan Srebro, Xueyuan Zhou</p><p>Abstract: We study the behavior of the popular Laplacian Regularization method for SemiSupervised Learning at the regime of a ﬁxed number of labeled points but a large number of unlabeled points. We show that in Rd , d 2, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function. We also contrast the method with the Laplacian Eigenvector method, and discuss the “smoothness” assumptions associated with this alternate method. 1 Introduction and Setup In this paper we consider the limit behavior of two popular semi-supervised learning (SSL) methods based on the graph Laplacian: the regularization approach [15] and the spectral approach [3]. We consider the limit when the number of labeled points is ﬁxed and the number of unlabeled points goes to inﬁnity. This is a natural limit for SSL as the basic SSL scenario is one in which unlabeled data is virtually inﬁnite. We can also think of this limit as “perfect” SSL, having full knowledge of the marginal density p(x). The premise of SSL is that the marginal density p(x) is informative about the unknown mapping y(x) we are trying to learn, e.g. since y(x) is expected to be “smooth” in some sense relative to p(x). Studying the inﬁnite-unlabeled-data limit, where p(x) is fully known, allows us to formulate and understand the underlying smoothness assumptions of a particular SSL method, and judge whether it is well-posed and sensible. Understanding the inﬁnite-unlabeled-data limit is also a necessary ﬁrst step to studying the convergence of the ﬁnite-labeled-data estimator. We consider the following setup: Let p(x) be an unknown smooth density on a compact domain Ω ⊂ Rd with a smooth boundary. Let y : Ω → Y be the unknown function we wish to estimate. In case of regression Y = R whereas in binary classiﬁcation Y = {−1, 1}. The standard (transductive) semisupervised learning problem is formulated as follows: Given l labeled points, (x1 , y1 ), . . . , (xl , yl ), with yi = y(xi ), and u unlabeled points xl+1 , . . . , xl+u , with all points xi sampled i.i.d. from p(x), the goal is to construct an estimate of y(xl+i ) for any unlabeled point xl+i , utilizing both the labeled and the unlabeled points. We denote the total number of points by n = l + u. We are interested in the regime where l is ﬁxed and u → ∞. 1 2 SSL with Graph Laplacian Regularization We ﬁrst consider the following graph-based approach formulated by Zhu et. al. [15]: y (x) = arg min In (y) ˆ subject to y(xi ) = yi , i = 1, . . . , l y where 1 n2 In (y) = Wi,j (y(xi ) − y(xj ))2 (1) (2) i,j is a Laplacian regularization term enforcing “smoothness” with respect to the n×n similarity matrix W . This formulation has several natural interpretations in terms of, e.g. random walks and electrical circuits [15]. These interpretations, however, refer to a ﬁxed graph, over a ﬁnite set of points with given similarities. In contrast, our focus here is on the more typical scenario where the points xi ∈ Rd are a random sample from a density p(x), and W is constructed based on this sample. We would like to understand the behavior of the method in terms of the density p(x), particularly in the limit where the number of unlabeled points grows. Under what assumptions on the target labeling y(x) and on the density p(x) is the method (1) sensible? The answer, of course, depends on how the matrix W is constructed. We consider the common situation where the similarities are obtained by applying some decay ﬁlter to the distances: xi −xj σ Wi,j = G (3) where G : R+ → R+ is some function with an adequately fast decay. Popular choices are the 2 Gaussian ﬁlter G(z) = e−z /2 or the ǫ-neighborhood graph obtained by the step ﬁlter G(z) = 1z<1 . For simplicity, we focus here on the formulation (1) where the solution is required to satisfy the constraints at the labeled points exactly. In practice, the hard labeling constraints are often replaced with a softer loss-based data term, which is balanced against the smoothness term In (y), e.g. [14, 6]. Our analysis and conclusions apply to such variants as well. Limit of the Laplacian Regularization Term As the number of unlabeled examples grows the regularization term (2) converges to its expectation, where the summation is replaced by integration w.r.t. the density p(x): lim In (y) = I (σ) (y) = n→∞ G Ω Ω x−x′ σ (y(x) − y(x′ ))2 p(x)p(x′ )dxdx′ . (4) In the above limit, the bandwidth σ is held ﬁxed. Typically, one would also drive the bandwidth σ to zero as n → ∞. There are two reasons for this choice. First, from a practical perspective, this makes the similarity matrix W sparse so it can be stored and processed. Second, from a theoretical perspective, this leads to a clear and well deﬁned limit of the smoothness regularization term In (y), at least when σ → 0 slowly enough1 , namely when σ = ω( d log n/n). If σ → 0 as n → ∞, and as long as nσ d / log n → ∞, then after appropriate normalization, the regularizer converges to a density weighted gradient penalty term [7, 8]: d lim d+2 In (y) n→∞ Cσ (σ) d (y) d+2 I σ→0 Cσ = lim ∇y(x) 2 p(x)2 dx = J(y) = (5) Ω where C = Rd z 2 G( z )dz, and assuming 0 < C < ∞ (which is the case for both the Gaussian and the step ﬁlters). This energy functional J(f ) therefore encodes the notion of “smoothness” with respect to p(x) that is the basis of the SSL formulation (1) with the graph constructions speciﬁed by (3). To understand the behavior and appropriateness of (1) we must understand this functional and the associated limit problem: y (x) = arg min J(y) ˆ subject to y(xi ) = yi , i = 1, . . . , l (6) y p When σ = o( d 1/n) then all non-diagonal weights Wi,j vanish (points no longer have any “close by” p neighbors). We are not aware of an analysis covering the regime where σ decays roughly as d 1/n, but would be surprised if a qualitatively different meaningful limit is reached. 1 2 3 Graph Laplacian Regularization in R1 We begin by considering the solution of (6) for one dimensional data, i.e. d = 1 and x ∈ R. We ﬁrst consider the situation where the support of p(x) is a continuous interval Ω = [a, b] ⊂ R (a and/or b may be inﬁnite). Without loss of generality, we assume the labeled data is sorted in increasing order a x1 < x2 < · · · < xl b. Applying the theory of variational calculus, the solution y (x) ˆ satisﬁes inside each interval (xi , xi+1 ) the Euler-Lagrange equation d dy p2 (x) = 0. dx dx Performing two integrations and enforcing the constraints at the labeled points yields y(x) = yi + x 1/p2 (t)dt xi (yi+1 xi+1 1/p2 (t)dt xi − yi ) for xi x xi+1 (7) with y(x) = x1 for a x x1 and y(x) = xl for xl x b. If the support of p(x) is a union of disjoint intervals, the above analysis and the form of the solution applies in each interval separately. The solution (7) seems reasonable and desirable from the point of view of the “smoothness” assumptions: when p(x) is uniform, the solution interpolates linearly between labeled data points, whereas across low-density regions, where p(x) is close to zero, y(x) can change abruptly. Furthermore, the regularizer J(y) can be interpreted as a Reproducing Kernel Hilbert Space (RKHS) squared semi-norm, giving us additional insight into this choice of regularizer: b 1 Theorem 1. Let p(x) be a smooth density on Ω = [a, b] ⊂ R such that Ap = 4 a 1/p2 (t)dt < ∞. 2 Then, J(f ) can be written as a squared semi-norm J(f ) = f Kp induced by the kernel x′ ′ Kp (x, x ) = Ap − 1 2 x with a null-space of all constant functions. That is, f the RKHS induced by Kp . 1 p2 (t) dt Kp . (8) is the norm of the projection of f onto If p(x) is supported on several disjoint intervals, Ω = ∪i [ai , bi ], then J(f ) can be written as a squared semi-norm induced by the kernel 1 bi dt 4 ai p2 (t) ′ Kp (x, x ) = − 1 2 x′ dt x p2 (t) if x, x′ ∈ [ai , bi ] (9) if x ∈ [ai , bi ], x′ ∈ [aj , bj ], i = j 0 with a null-space spanned by indicator functions 1[ai ,bi ] (x) on the connected components of Ω. Proof. For any f (x) = i αi Kp (x, xi ) in the RKHS induced by Kp : df dx J(f ) = 2 p2 (x)dx = αi αj Jij (10) i,j where Jij = d d Kp (x, xi ) Kp (x, xj )p2 (x)dx dx dx When xi and xj are in different connected components of Ω, the gradients of Kp (·, xi ) and Kp (·, xj ) are never non-zero together and Jij = 0 = Kp (xi , xj ). When they are in the same connected component [a, b], and assuming w.l.o.g. a xi xj b: Jij = = xi 1 4 1 4 a b a 1 dt + p2 (t) 1 1 dt − p2 (t) 2 xj xi xj xi −1 dt + p2 (t) xj 1 dt p2 (t) 1 dt = Kp (xi , xj ). p2 (t) Substituting Jij = Kp (xi , xj ) into (10) yields J(f ) = 3 b αi αj Kp (xi , xj ) = f (11) Kp . Combining Theorem 1 with the Representer Theorem [13] establishes that the solution of (6) (or of any variant where the hard constraints are replaced by a data term) is of the form: l y(x) = αj Kp (x, xj ) + βi 1[ai ,bi ] (x), j=1 i where i ranges over the connected components [ai , bi ] of Ω, and we have: l J(y) = αi αj Kp (xi , xj ). (12) i,j=1 Viewing the regularizer as y 2 p suggests understanding (6), and so also its empirical approximaK tion (1), by interpreting Kp (x, x′ ) as a density-based “similarity measure” between x and x′ . This similarity measure indeed seems sensible: for a uniform density it is simply linearly decreasing as a function of the distance. When the density is non-uniform, two points are relatively similar only if they are connected by a region in which 1/p2 (x) is low, i.e. the density is high, but are much less “similar”, i.e. related to each other, when connected by a low-density region. Furthermore, there is no dependence between points in disjoint components separated by zero density regions. 4 Graph Laplacian Regularization in Higher Dimensions The analysis of the previous section seems promising, at it shows that in one dimension, the SSL method (1) is well posed and converges to a sensible limit. Regretfully, in higher dimensions this is not the case anymore. In the following theorem we show that the inﬁmum of the limit problem (6) is zero and can be obtained by a sequence of functions which are certainly not a sensible extrapolation of the labeled points. Theorem 2. Let p(x) be a smooth density over Rd , d 2, bounded from above by some constant pmax , and let (x1 , y1 ), . . . , (xl , yl ) be any (non-repeating) set of labeled examples. There exist continuous functions yǫ (x), for any ǫ > 0, all satisfying the constraints yǫ (xj ) = yj , j = 1, . . . , l, such ǫ→0 ǫ→0 that J(yǫ ) −→ 0 but yǫ (x) −→ 0 for all x = xj , j = 1, . . . , l. Proof. We present a detailed proof for the case of l = 2 labeled points. The generalization of the proof to more labeled points is straightforward. Furthermore, without loss of generality, we assume the ﬁrst labeled point is at x0 = 0 with y(x0 ) = 0 and the second labeled point is at x1 with x1 = 1 and y(x1 ) = 1. In addition, we assume that the ball B1 (0) of radius one centered around the origin is contained in Ω = {x ∈ Rd | p(x) > 0}. We ﬁrst consider the case d > 2. Here, for any ǫ > 0, consider the function x ǫ yǫ (x) = min ,1 which indeed satisﬁes the two constraints yǫ (xi ) = yi , i = 0, 1. Then, J(yǫ ) = Bǫ (0) p2 (x) dx ǫ2 pmax ǫ2 dx = p2 Vd ǫd−2 max (13) Bǫ (0) where Vd is the volume of a unit ball in Rd . Hence, the sequence of functions yǫ (x) satisfy the constraints, but for d > 2, inf ǫ J(yǫ ) = 0. For d = 2, a more extreme example is necessary: consider the functions 2 x yǫ (x) = log +ǫ ǫ log 1+ǫ ǫ for x 1 and yǫ (x) = 1 for x > 1. These functions satisfy the two constraints yǫ (xi ) = yi , i = 0, 1 and: J(yǫ ) = 4 h “ ”i 1+ǫ 2 log ǫ 4πp2 max h “ ”i 1+ǫ 2 log ǫ x B1 (0) log ( x 1+ǫ ǫ 2 2 +ǫ)2 p2 (x)dx 4p2 h “ max ”i2 1+ǫ log ǫ 4πp2 max ǫ→0 = −→ 0. log 1+ǫ ǫ 4 1 0 r2 (r 2 +ǫ)2 2πrdr The implication of Theorem 2 is that regardless of the values at the labeled points, as u → ∞, the solution of (1) is not well posed. Asymptotically, the solution has the form of an almost everywhere constant function, with highly localized spikes near the labeled points, and so no learning is performed. In particular, an interpretation in terms of a density-based kernel Kp , as in the onedimensional case, is not possible. Our analysis also carries over to a formulation where a loss-based data term replaces the hard label constraints, as in l 1 y = arg min ˆ (y(xj ) − yj )2 + γIn (y) y(x) l j=1 In the limit of inﬁnite unlabeled data, functions of the form yǫ (x) above have a zero data penalty term (since they exactly match the labels) and also drive the regularization term J(y) to zero. Hence, it is possible to drive the entire objective functional (the data term plus the regularization term) to zero with functions that do not generalize at all to unlabeled points. 4.1 Numerical Example We illustrate the phenomenon detailed by Theorem 2 with a simple example. Consider a density p(x) in R2 , which is a mixture of two unit variance spherical Gaussians, one per class, centered at the origin and at (4, 0). We sample a total of n = 3000 points, and label two points from each of the two components (four total). We then construct a similarity matrix using a Gaussian ﬁlter with σ = 0.4. Figure 1 depicts the predictor y (x) obtained from (1). In fact, two different predictors are shown, ˆ obtained by different numerical methods for solving (1). Both methods are based on the observation that the solution y (x) of (1) satisﬁes: ˆ n y (xi ) = ˆ n Wij y (xj ) / ˆ j=1 Wij on all unlabeled points i = l + 1, . . . , l + u. (14) j=1 Combined with the constraints of (1), we obtain a system of linear equations that can be solved by Gaussian elimination (here invoked through MATLAB’s backslash operator). This is the method used in the top panels of Figure 1. Alternatively, (14) can be viewed as an update equation for y (xi ), ˆ which can be solved via the power method, or label propagation [2, 6]: start with zero labels on the unlabeled points and iterate (14), while keeping the known labels on x1 , . . . , xl . This is the method used in the bottom panels of Figure 1. As predicted, y (x) is almost constant for almost all unlabeled points. Although all values are very ˆ close to zero, thresholding at the “right” threshold does actually produce sensible results in terms of the true -1/+1 labels. However, beyond being inappropriate for regression, a very ﬂat predictor is still problematic even from a classiﬁcation perspective. First, it is not possible to obtain a meaningful conﬁdence measure for particular labels. Second, especially if the size of each class is not known apriori, setting the threshold between the positive and negative classes is problematic. In our example, setting the threshold to zero yields a generalization error of 45%. The differences between the two numerical methods for solving (1) also point out to another problem with the ill-posedness of the limit problem: the solution is numerically very un-stable. A more quantitative evaluation, that also validates that the effect in Figure 1 is not a result of choosing a “wrong” bandwidth σ, is given in Figure 2. We again simulated data from a mixture of two Gaussians, one Gaussian per class, this time in 20 dimensions, with one labeled point per class, and an increasing number of unlabeled points. In Figure 2 we plot the squared error, and the classiﬁcation error of the resulting predictor y (x). We plot the classiﬁcation error both when a threshold ˆ of zero is used (i.e. the class is determined by sign(ˆ(x))) and with the ideal threshold minimizing y the test error. For each unlabeled sample size, we choose the bandwidth σ yielding the best test performance (this is a “cheating” approach which provides a lower bound on the error of the best method for selecting the bandwidth). As the number of unlabeled examples increases the squared error approaches 1, indicating a ﬂat predictor. Using a threshold of zero leads to an increase in the classiﬁcation error, possibly due to numerical instability. Interestingly, although the predictors become very ﬂat, the classiﬁcation error using the ideal threshold actually improves slightly. Note that 5 DIRECT INVERSION SQUARED ERROR SIGN ERROR: 45% OPTIMAL BANDWIDTH 1 0.9 1 5 0 4 2 0.85 y(x) > 0 y(x) < 0 6 0.95 10 0 0 −1 10 0 200 400 600 800 0−1 ERROR (THRESHOLD=0) 0.32 −5 10 0 5 −10 0 −10 −5 −5 0 5 10 10 1 0 0 200 400 600 800 OPTIMAL BANDWIDTH 0.5 0 0 200 400 600 800 0−1 ERROR (IDEAL THRESHOLD) 0.19 5 200 400 600 800 OPTIMAL BANDWIDTH 1 0.28 SIGN ERR: 17.1 0.3 0.26 POWER METHOD 0 1.5 8 0 0.18 −1 10 6 0.17 4 −5 10 0 5 −10 0 −5 −10 −5 0 5 10 Figure 1: Left plots: Minimizer of Eq. (1). Right plots: the resulting classiﬁcation according to sign(y). The four labeled points are shown by green squares. Top: minimization via Gaussian elimination (MATLAB backslash). Bottom: minimization via label propagation with 1000 iterations - the solution has not yet converged, despite small residuals of the order of 2 · 10−4 . 0.16 0 200 400 600 800 2 0 200 400 600 800 Figure 2: Squared error (top), classiﬁcation error with a threshold of zero (center) and minimal classiﬁcation error using ideal threhold (bottom), of the minimizer of (1) as a function of number of unlabeled points. For each error measure and sample size, the bandwidth minimizing the test error was used, and is plotted. ideal classiﬁcation performance is achieved with a signiﬁcantly larger bandwidth than the bandwidth minimizing the squared loss, i.e. when the predictor is even ﬂatter. 4.2 Probabilistic Interpretation, Exit and Hitting Times As mentioned above, the Laplacian regularization method (1) has a probabilistic interpretation in terms of a random walk on the weighted graph. Let x(t) denote a random walk on the graph with transition matrix M = D−1 W where D is a diagonal matrix with Dii = j Wij . Then, for the binary classiﬁcation case with yi = ±1 we have [15]: y (xi ) = 2 Pr x(t) hits a point labeled +1 before hitting a point labeled -1 x(0) = xi − 1 ˆ We present an interpretation of our analysis in terms of the limiting properties of this random walk. Consider, for simplicity, the case where the two classes are separated by a low density region. Then, the random walk has two intrinsic quantities of interest. The ﬁrst is the mean exit time from one cluster to the other, and the other is the mean hitting time to the labeled points in that cluster. As the number of unlabeled points increases and σ → 0, the random walk converges to a diffusion process [12]. While the mean exit time then converges to a ﬁnite value corresponding to its diffusion analogue, the hitting time to a labeled point increases to inﬁnity (as these become absorbing boundaries of measure zero). With more and more unlabeled data the random walk will fully mix, forgetting where it started, before it hits any label. Thus, the probability of hitting +1 before −1 will become uniform across the entire graph, independent of the starting location xi , yielding a ﬂat predictor. 5 Keeping σ Finite At this point, a reader may ask whether the problems found in higher dimensions are due to taking the limit σ → 0. One possible objection is that there is an intrinsic characteristic scale for the data σ0 where (with high probability) all points at a distance xi − xj < σ0 have the same label. If this is the case, then it may not necessarily make sense to take values of σ < σ0 in constructing W . However, keeping σ ﬁnite while taking the number of unlabeled points to inﬁnity does not resolve the problem. On the contrary, even the one-dimensional case becomes ill-posed in this case. To see this, consider a function y(x) which is zero everywhere except at the labeled points, where y(xj ) = yj . With a ﬁnite number of labeled points of measure zero, I (σ) (y) = 0 in any dimension 6 50 points 500 points 3500 points 1 1 0.5 0.5 0.5 0 0 0 −0.5 y 1 −0.5 −0.5 −1 −2 0 2 4 6 −1 −2 0 2 4 6 −1 −2 0 2 4 6 x Figure 3: Minimizer of (1) for a 1-d problem with a ﬁxed σ = 0.4, two labeled points and an increasing number of unlabeled points. and for any ﬁxed σ > 0. While this limiting function is discontinuous, it is also possible to construct ǫ→0 a sequence of continuous functions yǫ that all satisfy the constraints and for which I (σ) (yǫ ) −→ 0. This behavior is illustrated in Figure 3. We generated data from a mixture of two 1-D Gaussians centered at the origin and at x = 4, with one Gaussian labeled −1 and the other +1. We used two labeled points at the centers of the Gaussians and an increasing number of randomly drawn unlabeled points. As predicted, with a ﬁxed σ, although the solution is reasonable when the number of unlabeled points is small, it becomes ﬂatter, with sharp spikes on the labeled points, as u → ∞. 6 Fourier-Eigenvector Based Methods Before we conclude, we discuss a different approach for SSL, also based on the Graph Laplacian, suggested by Belkin and Niyogi [3]. Instead of using the Laplacian as a regularizer, constraining candidate predictors y(x) non-parametrically to those with small In (y) values, here the predictors are constrained to the low-dimensional space spanned by the ﬁrst few eigenvectors of the Laplacian: The similarity matrix W is computed as before, and the Graph Laplacian matrix L = D − W is considered (recall D is a diagonal matrix with Dii = j Wij ). Only predictors p j=1 aj ej y (x) = ˆ (15) spanned by the ﬁrst p eigenvectors e1 , . . . , ep of L (with smallest eigenvalues) are considered. The coefﬁcients aj are chosen by minimizing a loss function on the labeled data, e.g. the squared loss: (ˆ1 , . . . , ap ) = arg min a ˆ l j=1 (yj − y (xj ))2 . ˆ (16) Unlike the Laplacian Regularization method (1), the Laplacian Eigenvector method (15)–(16) is well posed in the limit u → ∞. This follows directly from the convergence of the eigenvectors of the graph Laplacian to the eigenfunctions of the corresponding Laplace-Beltrami operator [10, 4]. Eigenvector based methods were shown empirically to provide competitive generalization performance on a variety of simulated and real world problems. Belkin and Niyogi [3] motivate the approach by arguing that ‘the eigenfunctions of the Laplace-Beltrami operator provide a natural basis for functions on the manifold and the desired classiﬁcation function can be expressed in such a basis’. In our view, the success of the method is actually not due to data lying on a low-dimensional manifold, but rather due to the low density separation assumption, which states that different class labels form high-density clusters separated by low density regions. Indeed, under this assumption and with sufﬁcient separation between the clusters, the eigenfunctions of the graph Laplace-Beltrami operator are approximately piecewise constant in each of the clusters, as in spectral clustering [12, 11], providing a basis for a labeling that is constant within clusters but variable across clusters. In other settings, such as data uniformly distributed on a manifold but without any signiﬁcant cluster structure, the success of eigenvector based methods critically depends on how well can the unknown classiﬁcation function be approximated by a truncated expansion with relatively few eigenvectors. We illustrate this issue with the following three-dimensional example: Let p(x) denote the uniform density in the box [0, 1] × [0, 0.8] × [0, 0.6], where the box lengths are different to prevent eigenvalue multiplicity. Consider learning three different functions, y1 (x) = 1x1 >0.5 , y2 (x) = 1x1 >x2 /0.8 and y3 (x) = 1x2 /0.8>x3 /0.6 . Even though all three functions are relatively simple, all having a linear separating boundary between the classes on the manifold, as shown in the experiment described in Figure 4, the Eigenvector based method (15)–(16) gives markedly different generalization performances on the three targets. This happens both when the number of eigenvectors p is set to p = l/5 as suggested by Belkin and Niyogi, as well as for the optimal (oracle) value of p selected on the test set (i.e. a “cheating” choice representing an upper bound on the generalization error of this method). 7 Prediction Error (%) p = #labeled points/5 40 optimal p 20 labeled points 40 Approx. Error 50 20 20 0 20 20 40 60 # labeled points 0 10 20 40 60 # labeled points 0 0 5 10 15 # eigenvectors 0 0 5 10 15 # eigenvectors Figure 4: Left three panels: Generalization Performance of the Eigenvector Method (15)–(16) for the three different functions described in the text. All panels use n = 3000 points. Prediction counts the number of sign agreements with the true labels. Rightmost panel: best ﬁt when many (all 3000) points are used, representing the best we can hope for with a few leading eigenvectors. The reason for this behavior is that y2 (x) and even more so y3 (x) cannot be as easily approximated by the very few leading eigenfunctions—even though they seem “simple” and “smooth”, they are signiﬁcantly more complicated than y1 (x) in terms of measure of simplicity implied by the Eigenvector Method. Since the density is uniform, the graph Laplacian converges to the standard Laplacian and its eigenfunctions have the form ψi,j,k (x) = cos(iπx1 ) cos(jπx2 /0.8) cos(kπx3 /0.6), making it hard to represent simple decision boundaries which are not axis-aligned. 7 Discussion Our results show that a popular SSL method, the Laplacian Regularization method (1), is not wellbehaved in the limit of inﬁnite unlabeled data, despite its empirical success in various SSL tasks. The empirical success might be due to two reasons. First, it is possible that with a large enough number of labeled points relative to the number of unlabeled points, the method is well behaved. This regime, where the number of both labeled and unlabeled points grow while l/u is ﬁxed, has recently been analyzed by Wasserman and Lafferty [9]. However, we do not ﬁnd this regime particularly satisfying as we would expect that having more unlabeled data available should improve performance, rather than require more labeled points or make the problem ill-posed. It also places the user in a delicate situation of choosing the “just right” number of unlabeled points without any theoretical guidance. Second, in our experiments we noticed that although the predictor y (x) becomes extremely ﬂat, in ˆ binary tasks, it is still typically possible to ﬁnd a threshold leading to a good classiﬁcation performance. We do not know of any theoretical explanation for such behavior, nor how to characterize it. Obtaining such an explanation would be very interesting, and in a sense crucial to the theoretical foundation of the Laplacian Regularization method. On a very practical level, such a theoretical understanding might allow us to correct the method so as to avoid the numerical instability associated with ﬂat predictors, and perhaps also make it appropriate for regression. The reason that the Laplacian regularizer (1) is ill-posed in the limit is that the ﬁrst order gradient is not a sufﬁcient penalty in high dimensions. This fact is well known in spline theory, where the Sobolev Embedding Theorem [1] indicates one must control at least d+1 derivatives in Rd . In the 2 context of Laplacian regularization, this can be done using the iterated Laplacian: replacing the d+1 graph Laplacian matrix L = D − W , where D is the diagonal degree matrix, with L 2 (matrix to d+1 the 2 power). In the inﬁnite unlabeled data limit, this corresponds to regularizing all order- d+1 2 (mixed) partial derivatives. In the typical case of a low-dimensional manifold in a high dimensional ambient space, the order of iteration should correspond to the intrinsic, rather then ambient, dimensionality, which poses a practical problem of estimating this usually unknown dimensionality. We are not aware of much practical work using the iterated Laplacian, nor a good understanding of its appropriateness for SSL. A different approach leading to a well-posed solution is to include also an ambient regularization term [5]. However, the properties of the solution and in particular its relation to various assumptions about the “smoothness” of y(x) relative to p(x) remain unclear. Acknowledgments The authors would like to thank the anonymous referees for valuable suggestions. The research of BN was supported by the Israel Science Foundation (grant 432/06). 8 References [1] R.A. Adams, Sobolev Spaces, Academic Press (New York), 1975. [2] A. Azran, The rendevous algorithm: multiclass semi-supervised learning with Markov Random Walks, ICML, 2007. [3] M. Belkin, P. Niyogi, Using manifold structure for partially labelled classiﬁcation, NIPS, vol. 15, 2003. [4] M. Belkin and P. Niyogi, Convergence of Laplacian Eigenmaps, NIPS 19, 2007. [5] M. Belkin, P. Niyogi and S. Sindhwani, Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples, JMLR, 7:2399-2434, 2006. [6] Y. Bengio, O. Delalleau, N. Le Roux, label propagation and quadratic criterion, in Semi-Supervised Learning, Chapelle, Scholkopf and Zien, editors, MIT Press, 2006. [7] O. Bosquet, O. Chapelle, M. Hein, Measure Based Regularization, NIPS, vol. 16, 2004. [8] M. Hein, Uniform convergence of adaptive graph-based regularization, COLT, 2006. [9] J. Lafferty, L. Wasserman, Statistical Analysis of Semi-Supervised Regression, NIPS, vol. 20, 2008. [10] U. von Luxburg, M. Belkin and O. Bousquet, Consistency of spectral clustering, Annals of Statistics, vol. 36(2), 2008. [11] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001. [12] B. Nadler, S. Lafon, I.G. Kevrekidis, R.R. Coifman, Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators, NIPS, vol. 18, 2006. [13] B. Sch¨ lkopf, A. Smola, Learning with Kernels, MIT Press, 2002. o [14] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, B. Sch¨ lkopf, Learning with local and global consistency, o NIPS, vol. 16, 2004. [15] X. Zhu, Z. Ghahramani, J. Lafferty, Semi-Supervised Learning using Gaussian ﬁelds and harmonic functions, ICML, 2003. 9</p><p>4 0.70701116 <a title="228-lda-4" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>5 0.70334643 <a title="228-lda-5" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>Author: Jaakko Luttinen, Alexander T. Ihler</p><p>Abstract: We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</p><p>6 0.70022166 <a title="228-lda-6" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>7 0.69322813 <a title="228-lda-7" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>8 0.69241613 <a title="228-lda-8" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>9 0.69233578 <a title="228-lda-9" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>10 0.6913476 <a title="228-lda-10" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>11 0.69079167 <a title="228-lda-11" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>12 0.68943143 <a title="228-lda-12" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>13 0.68889648 <a title="228-lda-13" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>14 0.68836689 <a title="228-lda-14" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>15 0.68711394 <a title="228-lda-15" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>16 0.68482286 <a title="228-lda-16" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>17 0.68358535 <a title="228-lda-17" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>18 0.68268037 <a title="228-lda-18" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>19 0.68223709 <a title="228-lda-19" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>20 0.68173432 <a title="228-lda-20" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
