<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2009-Group Sparse Coding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-104" href="#">nips2009-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2009-Group Sparse Coding</h1>
<br/><p>Source: <a title="nips-2009-104-pdf" href="http://papers.nips.cc/paper/3691-group-sparse-coding.pdf">pdf</a></p><p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>Reference: <a title="nips-2009-104-reference" href="../nips2009_reference/nips-2009-Group_Sparse_Coding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Bag-of-words document representations are often used in text, image and video processing. [sent-5, score-0.258]
</p><p>2 While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. [sent-6, score-1.29]
</p><p>3 The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. [sent-7, score-1.652]
</p><p>4 More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. [sent-8, score-0.936]
</p><p>5 While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. [sent-9, score-0.339]
</p><p>6 In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. [sent-10, score-0.295]
</p><p>7 This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. [sent-11, score-0.871]
</p><p>8 Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. [sent-12, score-1.074]
</p><p>9 Those representations abstract from spatial and temporal order to encode a document as a vector of the numbers of occurrences in the document of descriptors from a suitable dictionary. [sent-14, score-0.555]
</p><p>10 For text documents, the dictionary might consist of all the words or of all the n-grams of a certain minimum frequency in the document collection [1]. [sent-15, score-0.686]
</p><p>11 For images or videos, however, there is no simple mapping from the raw document to descriptor counts. [sent-16, score-0.293]
</p><p>12 Instead, visual descriptors must be ﬁrst extracted and then represented in terms of a carefully constructed dictionary. [sent-17, score-0.403]
</p><p>13 For dictionary construction, the standard approach in computer vision is to use some unsupervised vector quantization (VQ) technique, often k-means clustering [14], to create the dictionary. [sent-19, score-0.628]
</p><p>14 A new image is then represented by a vector indexed by dictionary elements (codewords), which for element d counts the number of visual descriptors in the image whose closest codeword is d. [sent-20, score-1.426]
</p><p>15 VQ 1  representations are maximally sparse per descriptor occurrence since they pick a single codeword for each occurrence, but they may not be sparse for the image as a whole; furthermore, such representations are not that robust with respect to descriptor variability. [sent-21, score-0.913]
</p><p>16 Sparse representations have obvious computational beneﬁts, by saving both processing time in handling visual descriptors and space in storing encoded images. [sent-22, score-0.495]
</p><p>17 These techniques promote sparsity in determining a small set of codewords from the dictionary that can be used to efﬁciently represent each visual descriptor of each image [13]. [sent-24, score-1.144]
</p><p>18 However, those approaches consider each visual descriptor in the image as a separate coding problem and do not take into account the fact that descriptor coding is just an intermediate step in creating a bag of codewords representation for the whole image. [sent-25, score-1.038]
</p><p>19 Thus, sparse coding of each visual descriptor does not guarantee sparse coding of the whole image. [sent-26, score-0.714]
</p><p>20 In this study, we propose and evaluate the mixed-norm regularizers [12, 10, 2] to take into account the structure of bags of visual descriptors present in images. [sent-28, score-0.431]
</p><p>21 This form of regularization thus promotes the use of a small subset of codewords for each category that could be different from category to category, thus including an indirect discriminative signal in code construction. [sent-32, score-0.382]
</p><p>22 Mixed regularization can be applied at two levels: for image encoding, which can be expressed as a convex optimization problem, and for dictionary learning, using an alternating minimization procedure. [sent-33, score-0.867]
</p><p>23 Dictionary regularization promotes a small dictionary size directly, instead of indirectly through the sparse encoding step. [sent-34, score-0.897]
</p><p>24 4 extends the technique to learn the dictionary by alternating optimization. [sent-40, score-0.596]
</p><p>25 5 presents experimental results on a well-known image database. [sent-42, score-0.171]
</p><p>26 Our main goal is to encode effectively groups of instances in terms of a set of dictionary codewords |D| D = {dj }j=1 . [sent-48, score-0.809]
</p><p>27 For example, if instances are image patches, each group may be the set of patches in a particular image, and each codeword may represent some kind of average patch. [sent-49, score-0.45]
</p><p>28 Given D and G, our ﬁrst subgoal, encoding, is to minimize a tradeoff between the reconstruction error for G in terms of D, and a suitable mixed norm for the matrix of reconstruction weights that express each xi as a positive linear combination of dj ∈ D. [sent-52, score-0.631]
</p><p>29 The tradeoff between accurate reconstruction or compact encoding is governed through a regularization parameter λ. [sent-53, score-0.424]
</p><p>30 Our second subgoal, learning, is to estimate a good dictionary D given a set of training groups n {Gm }m=1 . [sent-54, score-0.601]
</p><p>31 3  Group Coding  To encode jointly all the instances in a group G with dictionary D, we solve the following convex optimization problem: A⋆ = arg minA Q(A, G, D) Q(A, G, D) i αj  where and  = 1 i∈G xi − 2 ≥ 0 ∀i, j . [sent-56, score-0.797]
</p><p>32 |D| j=1  i αj dj  2  +λ  |D| j=1  αj  p  |D|  (1)  |G|  1 The reconstruction matrix A = {αj }j=1 consists of non-negative vectors αj = (αj , . [sent-57, score-0.299]
</p><p>33 The second term of the objective weighs the mixed ℓ1 /ℓp norm of A, which measures reconstruction complexity, with the regularization parameter λ that balances reconstruction quality (the ﬁrst term) and reconstruction complexity. [sent-61, score-0.727]
</p><p>34 Its partial derivatives with respect to each αr are ∂ ˜ Q= i ∂αr  i i αj (dj · dr ) − xi · dr + αr dr  2  . [sent-67, score-1.329]
</p><p>35 (3)  j=r  Let us make the following abbreviation for a given index r, i αj (dj · dr ) . [sent-68, score-0.436]
</p><p>36 µi = xi · dr −  (4)  j=r i It is clear that if µi ≤ 0 then the optimum for αr is zero. [sent-69, score-0.493]
</p><p>37 For p = 1 the objective function is separable and we get the following sub-gradient condition for optimality, i 0 ∈ −µ+ + αr dr i  2  +λ  µ+ − [0, λ] ∂ i i |αr | ⇒ αr ∈ i . [sent-72, score-0.484]
</p><p>38 i ∂αr dr 2  (5)  ∈[0,1] r r Since αi ≥ 0 the above subgradient condition for optimality implies that αi = 0 when µ+ ≤ λ and i + r 2 otherwise αi = (µi − λ)/ dr . [sent-73, score-0.872]
</p><p>39 In this case, the gradient of Q(αr ) with an ℓ2 regularization term is αr dr 2 αr − µ+ + λ . [sent-78, score-0.536]
</p><p>40 αr 3  At the optimum this vector must be zero, so after rearranging terms we obtain αr =  dr  2  +  −1  λ αr  µ+ . [sent-79, score-0.472]
</p><p>41 (6) solely as a function of the scaling parameter s −1 λ µ+ , s µ+ = dr 2 + s µ+ which implies that 1 λ s= . [sent-82, score-0.436]
</p><p>42 (7) 1− dr 2 µ+ We now revisit the assumption that the norm of the optimal solution is greater than zero. [sent-83, score-0.479]
</p><p>43 Once the optimal group reconstruction matrix A is found, we compress the matrix into a single vector. [sent-87, score-0.247]
</p><p>44 Since visual descriptors and dictionary elements are only accessed through inner products in the above method, it could be easily generalized to work with Mercer kernels instead. [sent-91, score-0.992]
</p><p>45 4  Dictionary Learning  Now that we know how to achieve optimal reconstruction for a given dictionary, we examine how to learn a good dictionary, that is, a dictionary that balances between reconstruction error, reconstruction complexity, overall complexity relative to the given training set. [sent-92, score-1.098]
</p><p>46 In particular, we seek a learning method that facilitates both induction of new dictionary words and the removal of dictionary words with low predictive power. [sent-93, score-1.248]
</p><p>47 To achieve this goal, we will apply ℓ1 /ℓ2 regularization controlled by a new hyperparameter γ, to dictionary words. [sent-94, score-0.7]
</p><p>48 For this approach to work, we assume that instances have been mean-subtracted so that the zero vector 0 is the (uninformative) mean of the data and regularization towards 0 is equivalent to removing words that do not contribute much to compact representation of groups. [sent-95, score-0.243]
</p><p>49 , An } the corresponding reconstruction coefﬁcients relative to dictionary D. [sent-102, score-0.729]
</p><p>50 In our application we set p = 2 as the norm penalty of the dictionary words. [sent-107, score-0.613]
</p><p>51 Moreover, the same coordinate descent technique described above for ﬁnding the optimum reconstruction weights can be used again here after simple algebraic manipulations. [sent-109, score-0.195]
</p><p>52 m  i  (9)  i  Then, we can express dr compactly as follows. [sent-111, score-0.436]
</p><p>53 Calculating the gradient with respect to each dr and equating it to zero, we obtain   dr i i i i  αm,j αm,r dj + (αm,r )2 dr − αm,r xm,i  + γ =0 . [sent-113, score-1.448]
</p><p>54 dr m i∈Gm  j=r  4  Swapping the sums over m and i with the sum over j, using the auxiliary variables, and noting that dj does not depend neither on m nor on i, we obtain νj,r dj + νr,r dr − v r + γ j=r  dr =0 . [sent-114, score-1.588]
</p><p>55 dr  Similarly to the way we solved for αr , we now deﬁne the vector ur = v r − following iterate for dr : γ −1 ur , dr = νr,r 1 − ur +  (10)  j=r  νj,r dj to get the (11)  where, as above, we incorporated the case dr = 0, by applying the operator [·]+ to the term 1 − γ/ ur . [sent-115, score-2.152]
</p><p>56 The form of the solution implies that we can eliminate dr , as it becomes 0, whenever the norm of the residual vector ur is smaller than γ. [sent-116, score-0.546]
</p><p>57 Thus, the dictionary learning procedure naturally facilitates the ability to remove dictionary words whose predictive power falls below the regularization parameter. [sent-117, score-1.305]
</p><p>58 5  Experimental Setting  We compare our approach to image coding with previous sparse coding methods by measuring their impact on classiﬁcation performance on the PASCAL VOC (Visual Object Classes) 2007 dataset [4]. [sent-118, score-0.539]
</p><p>59 In the past, many features have been used for VOC classiﬁcation, with bag-of-words histograms of local descriptors like SIFT [6] being most popular. [sent-122, score-0.312]
</p><p>60 In our experiments, we extract local descriptors based on a regular grid for each image. [sent-123, score-0.312]
</p><p>61 The grid points are located at every seventh pixel horizontally and vertically, which produces an average of 3234 descriptors per image. [sent-124, score-0.369]
</p><p>62 We used a custom local descriptor that collects Gabor wavelet responses at different orientations, spatial scales, and spatial offsets from the interest point. [sent-125, score-0.249]
</p><p>63 The 27 (scale, offset) pairs were chosen by optimizing a previous image recognition task, unrelated to this paper, using a genetic algorithm. [sent-127, score-0.196]
</p><p>64 [15] independently described a descriptor that similarly uses responses at different orientations, scales, and offsets (see their Figure 2). [sent-129, score-0.207]
</p><p>65 Overall, this descriptor is generally comparable to SIFT and results in similar performance. [sent-130, score-0.175]
</p><p>66 To build an image feature vector from the descriptors, we thus investigate the following methods: 1. [sent-131, score-0.171]
</p><p>67 Build a bag-of-words histogram over hierarchical k-means codewords by looking up each descriptor in a hierarchical k-means tree [11]. [sent-132, score-0.362]
</p><p>68 Jointly train a dictionary and encode each descriptor using an ℓ1 sparse coding approach with γ = 0, which was studied previously [5, 7, 9]. [sent-136, score-1.03]
</p><p>69 Jointly train a dictionary and encode sets of descriptors where each set corresponds to a single image, using ℓ1 /ℓ2 group sparse coding, varying both γ and λ. [sent-138, score-1.137]
</p><p>70 Jointly train a dictionary and encode sets of descriptors where each set corresponds to all descriptors or all images of a single class, using ℓ1 /ℓ2 sparse coding, varying both γ and λ. [sent-140, score-1.428]
</p><p>71 Then, use ℓ1 /ℓ2 sparse coding to encode the descriptors in individual images and obtain a single α vector per image. [sent-141, score-0.691]
</p><p>72 As explained before, we normalized all descriptors to have zero mean so that regularizing dictionary words towards the zero vector implies dictionary sparsity. [sent-142, score-1.516]
</p><p>73 In all cases, the initial dictionary used during training was obtained from the same hierarchical kmeans tree, with a branching factor of 10 and depth 4 rather than 3 as used in the baseline method. [sent-143, score-0.635]
</p><p>74 This scheme yielded an initial dictionary of size 7873. [sent-144, score-0.57]
</p><p>75 15 500  1000  1500  2000  Dictionary Size Figure 1: Mean Average Precision on the 2007 PASCAL VOC database as a function of the size of the dictionary obtained by both ℓ1 and ℓ1 /ℓ2 regularization approaches when varying λ or γ. [sent-152, score-0.696]
</p><p>76 We show results where descriptors are grouped either by image or by class. [sent-153, score-0.518]
</p><p>77 To evaluate the impact of different coding methods on an important end-to-end task, image classiﬁcation, we selected the VOC 2007 training set for classiﬁer training, the VOC 2007 validation set for hyperparameter selection, and the VOC 2007 test set for for evaluation. [sent-155, score-0.345]
</p><p>78 Class average precisions on the encoded test set are then averaged across the 20 classes to produce the mean average precision shown in our graphs. [sent-157, score-0.237]
</p><p>79 6  Results and Discussion  In Figure 1 we compare the mean average precisions of the competing approaches as encoding hyperparameters are varied to control the overall dictionary size. [sent-158, score-0.778]
</p><p>80 For the ℓ1 approach, achieving different dictionary size was obtained by tuning λ while setting γ = 0. [sent-159, score-0.57]
</p><p>81 As it can be seen in Figure 1, when the dictionary is allowed to be very large, the pure ℓ1 approach yields the best performance. [sent-162, score-0.607]
</p><p>82 On the other hand, when the size of the dictionary matters, then all the approaches based on ℓ1 /ℓ2 regularization performed better than the ℓ1 counterpart. [sent-163, score-0.67]
</p><p>83 The version of ℓ1 /ℓ2 in which we allowed γ to vary provided the best tradeoff between dictionary size and classiﬁcation performance when descriptors were grouped per image, which was to be expected as γ directly promotes sparse dictionaries. [sent-165, score-1.229]
</p><p>84 More interestingly, when grouping descriptors per class instead of per image, we get even better performance for small dictionary sizes by varying λ. [sent-166, score-1.004]
</p><p>85 In Figure 2 we compare the mean average precisions of ℓ1 and ℓ1 /ℓ2 regularization as average image size varies. [sent-167, score-0.405]
</p><p>86 When image size is constrained, which is often the case is large-scale applications, all 6  Mean Average Precision vs Average Image Size 0. [sent-168, score-0.192]
</p><p>87 15 500  1000  1500  2000  Average Image Size Figure 2: Mean Average Precision on the 2007 PASCAL VOC database as a function of the average size of each image as encoded using the trained dictionary obtained by both ℓ1 and ℓ1 /ℓ2 regularization approaches when varying λ and γ. [sent-175, score-0.953]
</p><p>88 We show results where descriptors are grouped either by image or by class. [sent-176, score-0.518]
</p><p>89 200 200 400 400  600 800  600 1000 800  1200 1400  1000 1600 1200  1800 2000  1400 100  200  300  400  500  600  700  800  900  1000  100  200  300  400  500  600  700  800  900  1000  Figure 3: Comparison of the dictionary words used to reconstruct the same image. [sent-178, score-0.613]
</p><p>90 A pure ℓ1 coding was used on the left, while a mixed ℓ1 /ℓ2 encoding was used on the right plot. [sent-179, score-0.313]
</p><p>91 Each row represents the number of times each dictionary word was used in the reconstruction of the image. [sent-180, score-0.765]
</p><p>92 Once again ℓ1 regularization performed even worse than hierarchical k-means for small image sizes Figure 3 compares the usage of dictionary words to encode the same image, either using ℓ1 (on the left) or ℓ1 /ℓ2 (on the right) regularization. [sent-182, score-0.98]
</p><p>93 Each graph shows the number of times a dictionary word (a row in the plot) was used in the reconstruction of the image. [sent-183, score-0.765]
</p><p>94 Clearly, ℓ1 regularization yields an overall sparser representation in terms of total number of dictionary coefﬁcients that are used. [sent-184, score-0.715]
</p><p>95 However, almost all of the resulting dictionary vectors are non-zero and used at least once in the coding process. [sent-185, score-0.714]
</p><p>96 As expected, with ℓ1 /ℓ2 regularization, a dictionary word is either always used or never used yielding a much more compact representation in terms of the total number of dictionary words that are used. [sent-186, score-1.292]
</p><p>97 7  Overall, mixed-norm regularization yields better performance when the problem to solve includes resource constraints, either time (a smaller dictionary yields faster image encoding) or space (one can store or convey more images when they take less space). [sent-187, score-0.908]
</p><p>98 Finally, grouping descriptors per class instead of per image during dictionary learning promotes the use of the same dictionary words for all images of the same class, hence yielding some form of weak discrimination which appears to help under space or time constraints. [sent-189, score-1.92]
</p><p>99 Discriminative sparse image models for class-speciﬁc edge detection and image interpretation. [sent-251, score-0.422]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classiﬁcation. [sent-299, score-0.416]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dictionary', 0.57), ('dr', 0.436), ('descriptors', 0.312), ('descriptor', 0.175), ('image', 0.171), ('voc', 0.16), ('reconstruction', 0.159), ('coding', 0.144), ('dj', 0.14), ('codewords', 0.117), ('codeword', 0.111), ('regularization', 0.1), ('gm', 0.097), ('visual', 0.091), ('group', 0.088), ('vary', 0.081), ('encoding', 0.08), ('sparse', 0.08), ('images', 0.067), ('ur', 0.067), ('promotes', 0.067), ('encode', 0.061), ('mairal', 0.058), ('vq', 0.058), ('tradeoff', 0.057), ('encoded', 0.056), ('precisions', 0.053), ('mountain', 0.052), ('mixed', 0.052), ('document', 0.051), ('pascal', 0.049), ('precision', 0.047), ('hkmeans', 0.044), ('strelow', 0.044), ('tola', 0.044), ('words', 0.043), ('norm', 0.043), ('category', 0.039), ('subgoal', 0.039), ('google', 0.038), ('pure', 0.037), ('optimum', 0.036), ('representations', 0.036), ('word', 0.036), ('hierarchical', 0.035), ('grouped', 0.035), ('orientations', 0.034), ('vision', 0.033), ('offsets', 0.032), ('groups', 0.031), ('average', 0.03), ('hyperparameter', 0.03), ('duchi', 0.03), ('branching', 0.03), ('instances', 0.03), ('transaction', 0.029), ('compact', 0.028), ('objective', 0.028), ('bags', 0.028), ('per', 0.027), ('balances', 0.027), ('jointly', 0.027), ('varying', 0.026), ('alternating', 0.026), ('recognition', 0.025), ('videos', 0.025), ('classi', 0.025), ('ca', 0.025), ('documents', 0.025), ('objectives', 0.025), ('quantization', 0.025), ('overall', 0.024), ('yielding', 0.024), ('elad', 0.024), ('offset', 0.023), ('sift', 0.023), ('pereira', 0.023), ('occurrences', 0.023), ('text', 0.022), ('occurrence', 0.022), ('grouping', 0.022), ('facilitates', 0.022), ('cvpr', 0.022), ('bach', 0.021), ('norms', 0.021), ('spatial', 0.021), ('representation', 0.021), ('vs', 0.021), ('mean', 0.021), ('xi', 0.021), ('th', 0.021), ('iccv', 0.02), ('represent', 0.02), ('class', 0.02), ('separable', 0.02), ('discriminative', 0.02), ('negahban', 0.019), ('samy', 0.019), ('swapping', 0.019), ('accessed', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="104-tfidf-1" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>2 0.26893124 <a title="104-tfidf-2" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>Author: Mingyuan Zhou, Haojun Chen, Lu Ren, Guillermo Sapiro, Lawrence Carin, John W. Paisley</p><p>Abstract: Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be nonstationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches. 1</p><p>3 0.20696828 <a title="104-tfidf-3" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>Author: Yi-hao Kao, Benjamin V. Roy, Xiang Yan</p><p>Abstract: When used to guide decisions, linear regression analysis typically involves estimation of regression coefﬁcients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneﬁcial to account for the decision objective when computing regression coefﬁcients. Empirical optimization does so but sacriﬁces performance when features are well-chosen or training data are insufﬁcient. We propose directed regression, an efﬁcient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate signiﬁcant performance gains over either alternative. We also develop a theory that motivates the algorithm. 1</p><p>4 0.20123905 <a title="104-tfidf-4" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>5 0.13837533 <a title="104-tfidf-5" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>Author: Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: We develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efﬁcient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert [1] to include a bias term signiﬁcantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efﬁciently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing, right facing, front) with the same formulation; and (iv), most signiﬁcantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask. We demonstrate the method by training and testing on the PASCAL VOC 2007 data set – training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with signiﬁcant truncations. 1</p><p>6 0.13798127 <a title="104-tfidf-6" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>7 0.12867709 <a title="104-tfidf-7" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>8 0.11907933 <a title="104-tfidf-8" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>9 0.1133047 <a title="104-tfidf-9" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>10 0.10796107 <a title="104-tfidf-10" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>11 0.089517914 <a title="104-tfidf-11" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>12 0.089184508 <a title="104-tfidf-12" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>13 0.083800487 <a title="104-tfidf-13" href="./nips-2009-Neurometric_function_analysis_of_population_codes.html">163 nips-2009-Neurometric function analysis of population codes</a></p>
<p>14 0.08311817 <a title="104-tfidf-14" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>15 0.079896197 <a title="104-tfidf-15" href="./nips-2009-On_the_Algorithmics_and_Applications_of_a_Mixed-norm_based_Kernel_Learning_Formulation.html">179 nips-2009-On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation</a></p>
<p>16 0.079625838 <a title="104-tfidf-16" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>17 0.077786945 <a title="104-tfidf-17" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>18 0.076691464 <a title="104-tfidf-18" href="./nips-2009-Semi-Supervised_Learning_in_Gigantic_Image_Collections.html">212 nips-2009-Semi-Supervised Learning in Gigantic Image Collections</a></p>
<p>19 0.076547489 <a title="104-tfidf-19" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>20 0.07565368 <a title="104-tfidf-20" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.21), (1, -0.121), (2, -0.119), (3, 0.024), (4, -0.004), (5, 0.083), (6, -0.017), (7, 0.0), (8, 0.255), (9, 0.2), (10, -0.018), (11, 0.039), (12, -0.048), (13, 0.093), (14, -0.079), (15, 0.051), (16, -0.045), (17, 0.15), (18, 0.067), (19, -0.052), (20, -0.044), (21, -0.065), (22, -0.032), (23, 0.177), (24, 0.103), (25, 0.167), (26, 0.021), (27, 0.042), (28, -0.007), (29, 0.131), (30, 0.237), (31, -0.171), (32, -0.047), (33, -0.062), (34, -0.087), (35, 0.02), (36, -0.011), (37, -0.006), (38, -0.021), (39, -0.118), (40, 0.034), (41, 0.03), (42, -0.067), (43, 0.04), (44, 0.096), (45, 0.048), (46, 0.034), (47, 0.104), (48, 0.094), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96210426 <a title="104-lsi-1" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>2 0.74654067 <a title="104-lsi-2" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>Author: Mingyuan Zhou, Haojun Chen, Lu Ren, Guillermo Sapiro, Lawrence Carin, John W. Paisley</p><p>Abstract: Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be nonstationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches. 1</p><p>3 0.59215796 <a title="104-lsi-3" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>4 0.57293773 <a title="104-lsi-4" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>Author: Yi-hao Kao, Benjamin V. Roy, Xiang Yan</p><p>Abstract: When used to guide decisions, linear regression analysis typically involves estimation of regression coefﬁcients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneﬁcial to account for the decision objective when computing regression coefﬁcients. Empirical optimization does so but sacriﬁces performance when features are well-chosen or training data are insufﬁcient. We propose directed regression, an efﬁcient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate signiﬁcant performance gains over either alternative. We also develop a theory that motivates the algorithm. 1</p><p>5 0.53543437 <a title="104-lsi-5" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>Author: Kai Yu, Tong Zhang, Yihong Gong</p><p>Abstract: This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difﬁcult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods. 1</p><p>6 0.52008754 <a title="104-lsi-6" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>7 0.49995917 <a title="104-lsi-7" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>8 0.45660415 <a title="104-lsi-8" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>9 0.44320899 <a title="104-lsi-9" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>10 0.43680567 <a title="104-lsi-10" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>11 0.4162446 <a title="104-lsi-11" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>12 0.3842572 <a title="104-lsi-12" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>13 0.38188878 <a title="104-lsi-13" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>14 0.3746295 <a title="104-lsi-14" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>15 0.37327084 <a title="104-lsi-15" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>16 0.36706147 <a title="104-lsi-16" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>17 0.36661521 <a title="104-lsi-17" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>18 0.36035013 <a title="104-lsi-18" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>19 0.35143429 <a title="104-lsi-19" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<p>20 0.34931087 <a title="104-lsi-20" href="./nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (24, 0.048), (25, 0.061), (27, 0.122), (35, 0.083), (36, 0.131), (39, 0.048), (58, 0.147), (61, 0.013), (71, 0.041), (81, 0.029), (86, 0.153), (91, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93498111 <a title="104-lda-1" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>2 0.89648652 <a title="104-lda-2" href="./nips-2009-DUOL%3A_A_Double_Updating_Approach_for_Online_Learning.html">63 nips-2009-DUOL: A Double Updating Approach for Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C. Hoi, Rong Jin</p><p>Abstract: In most online learning algorithms, the weights assigned to the misclassiﬁed examples (or support vectors) remain unchanged during the entire learning process. This is clearly insufﬁcient since when a new misclassiﬁed example is added to the pool of support vectors, we generally expect it to affect the weights for the existing support vectors. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received in current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be signiﬁcantly improved by the proposed online learning method. Encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms. 1</p><p>3 0.88491774 <a title="104-lda-3" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>Author: Benjamin Culpepper, Bruno A. Olshausen</p><p>Abstract: We describe an unsupervised manifold learning algorithm that represents a surface through a compact description of operators that traverse it. The operators are based on matrix exponentials, which are the solution to a system of ﬁrst-order linear differential equations. The matrix exponents are represented by a basis that is adapted to the statistics of the data so that the inﬁnitesimal generator for a trajectory along the underlying manifold can be produced by linearly composing a few elements. The method is applied to recover topological structure from low dimensional synthetic data, and to model local structure in how natural images change over time and scale. 1</p><p>4 0.88007808 <a title="104-lda-4" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton Hengel</p><p>Abstract: The learning of appropriate distance metrics is a critical problem in image classiﬁcation and retrieval. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a Mahalanobis distance metric. One of the primary difﬁculties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semideﬁnite. Semideﬁnite programming is sometimes used to enforce this constraint, but does not scale well. B OOST M ETRIC is instead based on a key observation that any positive semideﬁnite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. 1</p><p>5 0.87607288 <a title="104-lda-5" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>6 0.87606221 <a title="104-lda-6" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>7 0.87368673 <a title="104-lda-7" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>8 0.87336385 <a title="104-lda-8" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>9 0.87298185 <a title="104-lda-9" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>10 0.87248057 <a title="104-lda-10" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>11 0.87235981 <a title="104-lda-11" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>12 0.87064964 <a title="104-lda-12" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>13 0.86917323 <a title="104-lda-13" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>14 0.86882257 <a title="104-lda-14" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>15 0.86855412 <a title="104-lda-15" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>16 0.86760706 <a title="104-lda-16" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>17 0.86617321 <a title="104-lda-17" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>18 0.86494434 <a title="104-lda-18" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>19 0.86264569 <a title="104-lda-19" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>20 0.86237687 <a title="104-lda-20" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
