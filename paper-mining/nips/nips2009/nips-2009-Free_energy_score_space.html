<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2009-Free energy score space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-97" href="#">nips2009-97</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>97 nips-2009-Free energy score space</h1>
<br/><p>Source: <a title="nips-2009-97-pdf" href="http://papers.nips.cc/paper/3830-free-energy-score-space.pdf">pdf</a></p><p>Author: Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</p><p>Abstract: A score function induced by a generative model of the data can provide a feature vector of a ﬁxed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a ﬁxed-length vector in a highly informative space, typically referred to as a “score space”. Discriminative classiﬁers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classiﬁers, or the discriminative classiﬁers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classiﬁcation performance that at least matches the performance of the free energy classiﬁer based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classiﬁers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.</p><p>Reference: <a title="nips-2009-97-reference" href="../nips2009_reference/nips-2009-Free_energy_score_space_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fess', 0.604), ('energy', 0.373), ('fre', 0.304), ('scor', 0.212), ('fk', 0.152), ('fq', 0.136), ('vn', 0.134), ('pan', 0.126), ('oer', 0.121), ('famy', 0.121), ('hid', 0.107), ('discrimin', 0.104), ('postery', 0.1), ('protein', 0.095), ('sk', 0.095), ('fish', 0.089), ('hmm', 0.08), ('tk', 0.078), ('nfess', 0.072), ('rfp', 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="97-tfidf-1" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>Author: Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</p><p>Abstract: A score function induced by a generative model of the data can provide a feature vector of a ﬁxed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a ﬁxed-length vector in a highly informative space, typically referred to as a “score space”. Discriminative classiﬁers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classiﬁers, or the discriminative classiﬁers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classiﬁcation performance that at least matches the performance of the free energy classiﬁer based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classiﬁers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.</p><p>2 0.14019811 <a title="97-tfidf-2" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>Author: Stephen Gould, Tianshi Gao, Daphne Koller</p><p>Abstract: Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classiﬁcation of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach simultaneously reasons about pixels, regions and objects in a coherent probabilistic model. Pixel appearance features allow us to perform well on classifying amorphous background classes, while the explicit representation of regions facilitate the computation of more sophisticated features necessary for object detection. Importantly, our model gives a single uniﬁed description of the scene—we explain every pixel in the image and enforce global consistency between all random variables in our model. We run experiments on the challenging Street Scene dataset [2] and show signiﬁcant improvement over state-of-the-art results for object detection accuracy. 1</p><p>3 0.11344221 <a title="97-tfidf-3" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>Author: Vinod Nair, Geoffrey E. Hinton</p><p>Abstract: We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under diﬀerent lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modiﬁed version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error. 1</p><p>4 0.10811478 <a title="97-tfidf-4" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>Author: Marco Grzegorczyk, Dirk Husmeier</p><p>Abstract: Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many realworld scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-ﬂexible models that lack any information sharing among time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary among segments, and in which a common network structure provides essential information sharing across segments. Our model is based on a Bayesian multiple change-point process, where the number and location of the change-points is sampled from the posterior distribution. 1</p><p>5 0.10744277 <a title="97-tfidf-5" href="./nips-2009-Graph_Zeta_Function_in_the_Bethe_Free_Energy_and_Loopy_Belief_Propagation.html">103 nips-2009-Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation</a></p>
<p>Author: Yusuke Watanabe, Kenji Fukumizu</p><p>Abstract: We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufﬁcient condition that the Hessian of the Bethe free energy is positive deﬁnite, which shows non-convexity for graphs with multiple cycles. The formula clariﬁes the relation between the local stability of a ﬁxed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP ﬁxed point, and show various conditions of uniqueness. 1</p><p>6 0.10582097 <a title="97-tfidf-6" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>7 0.093802467 <a title="97-tfidf-7" href="./nips-2009-Semi-supervised_Regression_using_Hessian_energy_with_an_application_to_semi-supervised_dimensionality_reduction.html">214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</a></p>
<p>8 0.08976154 <a title="97-tfidf-8" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>9 0.085501403 <a title="97-tfidf-9" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>10 0.08495561 <a title="97-tfidf-10" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>11 0.07858026 <a title="97-tfidf-11" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>12 0.074137934 <a title="97-tfidf-12" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>13 0.069073036 <a title="97-tfidf-13" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>14 0.067981467 <a title="97-tfidf-14" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>15 0.067765132 <a title="97-tfidf-15" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>16 0.066917777 <a title="97-tfidf-16" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>17 0.066486463 <a title="97-tfidf-17" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>18 0.064756669 <a title="97-tfidf-18" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>19 0.064704828 <a title="97-tfidf-19" href="./nips-2009-Neurometric_function_analysis_of_population_codes.html">163 nips-2009-Neurometric function analysis of population codes</a></p>
<p>20 0.064680465 <a title="97-tfidf-20" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.218), (1, 0.017), (2, -0.071), (3, 0.002), (4, 0.034), (5, -0.028), (6, 0.083), (7, 0.067), (8, 0.009), (9, -0.013), (10, -0.032), (11, -0.048), (12, 0.004), (13, 0.015), (14, -0.015), (15, -0.029), (16, 0.066), (17, -0.01), (18, 0.011), (19, -0.103), (20, -0.017), (21, -0.07), (22, -0.052), (23, -0.05), (24, -0.081), (25, -0.021), (26, 0.115), (27, 0.05), (28, 0.01), (29, -0.066), (30, 0.077), (31, 0.108), (32, 0.001), (33, -0.037), (34, -0.064), (35, 0.086), (36, 0.058), (37, 0.002), (38, -0.032), (39, -0.082), (40, 0.038), (41, 0.084), (42, 0.102), (43, 0.141), (44, -0.013), (45, -0.069), (46, 0.055), (47, -0.092), (48, 0.009), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89634347 <a title="97-lsi-1" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>Author: Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</p><p>Abstract: A score function induced by a generative model of the data can provide a feature vector of a ﬁxed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a ﬁxed-length vector in a highly informative space, typically referred to as a “score space”. Discriminative classiﬁers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classiﬁers, or the discriminative classiﬁers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classiﬁcation performance that at least matches the performance of the free energy classiﬁer based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classiﬁers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.</p><p>2 0.67829877 <a title="97-lsi-2" href="./nips-2009-Anomaly_Detection_with_Score_functions_based_on_Nearest_Neighbor_Graphs.html">34 nips-2009-Anomaly Detection with Score functions based on Nearest Neighbor Graphs</a></p>
<p>Author: Manqi Zhao, Venkatesh Saligrama</p><p>Abstract: We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below α, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the speciﬁed false alarm level, α, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efﬁcient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artiﬁcial and real data sets in high dimensional feature spaces.</p><p>3 0.61392713 <a title="97-lsi-3" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><p>4 0.57716572 <a title="97-lsi-4" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>Author: Marco Grzegorczyk, Dirk Husmeier</p><p>Abstract: Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many realworld scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-ﬂexible models that lack any information sharing among time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary among segments, and in which a common network structure provides essential information sharing across segments. Our model is based on a Bayesian multiple change-point process, where the number and location of the change-points is sampled from the posterior distribution. 1</p><p>5 0.57343286 <a title="97-lsi-5" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>Author: Nicolas Vayatis, Marine Depecker, Stéphan J. Clémençcon</p><p>Abstract: The purpose of the paper is to explore the connection between multivariate homogeneity tests and AUC optimization. The latter problem has recently received much attention in the statistical learning literature. From the elementary observation that, in the two-sample problem setup, the null assumption corresponds to the situation where the area under the optimal ROC curve is equal to 1/2, we propose a two-stage testing method based on data splitting. A nearly optimal scoring function in the AUC sense is ﬁrst learnt from one of the two half-samples. Data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the ﬁrst stage. The last step amounts to performing a standard Mann-Whitney Wilcoxon test in the onedimensional framework. We show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power, provided the ranking produced is accurate enough in the AUC sense. The results of a numerical experiment are eventually displayed in order to show the efﬁciency of the method. 1</p><p>6 0.5565185 <a title="97-lsi-6" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>7 0.51665252 <a title="97-lsi-7" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>8 0.51658082 <a title="97-lsi-8" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>9 0.50802147 <a title="97-lsi-9" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>10 0.49871102 <a title="97-lsi-10" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>11 0.49749729 <a title="97-lsi-11" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>12 0.49356249 <a title="97-lsi-12" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>13 0.48459896 <a title="97-lsi-13" href="./nips-2009-White_Functionals_for_Anomaly_Detection_in_Dynamical_Systems.html">257 nips-2009-White Functionals for Anomaly Detection in Dynamical Systems</a></p>
<p>14 0.48439568 <a title="97-lsi-14" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>15 0.48260781 <a title="97-lsi-15" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>16 0.48056555 <a title="97-lsi-16" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>17 0.47193876 <a title="97-lsi-17" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>18 0.47150439 <a title="97-lsi-18" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>19 0.46442598 <a title="97-lsi-19" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>20 0.45491183 <a title="97-lsi-20" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.021), (9, 0.288), (11, 0.012), (24, 0.027), (31, 0.132), (37, 0.026), (60, 0.082), (71, 0.218), (89, 0.028), (96, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89632756 <a title="97-lda-1" href="./nips-2009-Potential-Based_Agnostic_Boosting.html">193 nips-2009-Potential-Based Agnostic Boosting</a></p>
<p>Author: Varun Kanade, Adam Kalai</p><p>Abstract: We prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to MadaBoost (Domingo and Watanabe, 2000) and SmoothBoost (Servedio, 2003). Our analysis is in the agnostic framework of Kearns, Schapire and Sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead. Our boosting theorem gives, as easy corollaries, alternative derivations of two recent nontrivial results in computational learning theory: agnostically learning decision trees (Gopalan et al, 2008) and agnostically learning halfspaces (Kalai et al, 2005). Experiments suggest that the algorithm performs similarly to MadaBoost. 1</p><p>2 0.86624587 <a title="97-lda-2" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>Author: Sylvain Arlot, Francis R. Bach</p><p>Abstract: This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which ﬁrst estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows’ CL penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves signiﬁcantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation. 1</p><p>same-paper 3 0.85648662 <a title="97-lda-3" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>Author: Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</p><p>Abstract: A score function induced by a generative model of the data can provide a feature vector of a ﬁxed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a ﬁxed-length vector in a highly informative space, typically referred to as a “score space”. Discriminative classiﬁers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classiﬁers, or the discriminative classiﬁers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classiﬁcation performance that at least matches the performance of the free energy classiﬁer based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classiﬁers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.</p><p>4 0.84388632 <a title="97-lda-4" href="./nips-2009-Boosting_with_Spatial_Regularization.html">47 nips-2009-Boosting with Spatial Regularization</a></p>
<p>Author: Yongxin Xi, Uri Hasson, Peter J. Ramadge, Zhen J. Xiang</p><p>Abstract: By adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting. From this regularized loss framework we derive an efﬁcient boosting algorithm that uses additional weights/priors on the base classiﬁers. We prove that the proposed algorithm exhibits a “grouping effect”, which encourages the selection of all spatially local, discriminative base classiﬁers. The algorithm’s primary advantage is in applications where the trained classiﬁer is used to identify the spatial pattern of discriminative information, e.g. the voxel selection problem in fMRI. We demonstrate the algorithm’s performance on various data sets. 1</p><p>5 0.83378661 <a title="97-lda-5" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>Author: Ruslan Salakhutdinov</p><p>Abstract: Markov random ﬁelds (MRF’s), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF’s is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF’s. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.</p><p>6 0.83235389 <a title="97-lda-6" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>7 0.8303957 <a title="97-lda-7" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>8 0.83034921 <a title="97-lda-8" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>9 0.82984883 <a title="97-lda-9" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>10 0.82966161 <a title="97-lda-10" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>11 0.82935184 <a title="97-lda-11" href="./nips-2009-Statistical_Analysis_of_Semi-Supervised_Learning%3A_The_Limit_of_Infinite_Unlabelled_Data.html">229 nips-2009-Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></p>
<p>12 0.82932216 <a title="97-lda-12" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>13 0.8288514 <a title="97-lda-13" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>14 0.82851875 <a title="97-lda-14" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>15 0.82778287 <a title="97-lda-15" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>16 0.82665968 <a title="97-lda-16" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>17 0.82643235 <a title="97-lda-17" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>18 0.82635075 <a title="97-lda-18" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>19 0.82582557 <a title="97-lda-19" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>20 0.8257193 <a title="97-lda-20" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
