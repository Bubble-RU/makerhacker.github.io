<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-83" href="#">nips2009-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</h1>
<br/><p>Source: <a title="nips-2009-83-pdf" href="http://papers.nips.cc/paper/3757-estimating-image-bases-for-visual-image-reconstruction-from-human-brain-activity.pdf">pdf</a></p><p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>Reference: <a title="nips-2009-83-reference" href="../nips2009_reference/nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract Image representation based on image bases provides a framework for understanding neural representation of visual perception. [sent-4, score-0.876]
</p><p>2 A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. [sent-5, score-0.62]
</p><p>3 In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. [sent-6, score-1.138]
</p><p>4 But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. [sent-7, score-0.635]
</p><p>5 Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. [sent-8, score-0.89]
</p><p>6 We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. [sent-9, score-0.586]
</p><p>7 The mapping from the latent variables to the visual image space can be regarded as a set of image bases. [sent-10, score-0.8]
</p><p>8 We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. [sent-11, score-1.793]
</p><p>9 1 Introduction The image basis is a key concept for understanding neural representation of visual images. [sent-13, score-0.436]
</p><p>10 Using image bases, we can consider natural scenes as a combination of simple elements corresponding to neural units. [sent-14, score-0.273]
</p><p>11 Previous works have shown that image bases similar to receptive ﬁelds of simple cells are learned from natural scenes by the sparse coding algorithm [4, 9]. [sent-15, score-0.789]
</p><p>12 A recent fMRI study has shown that visual images can be reconstructed using a linear combination of multi-scale image bases (1x1, 1x2, 2x1, and 2x2 pixels covering an entire image), whose contrasts were predicted from the fMRI activity pattern [6]. [sent-16, score-1.193]
</p><p>13 The multi-scale bases produced more accurate reconstruction than the pixel-by-pixel prediction, and each scale contributed to reconstruction in a way consistent with known visual cortical representation. [sent-17, score-0.864]
</p><p>14 However, the predeﬁned shapes of image bases may not be optimal for image reconstruction. [sent-18, score-1.089]
</p><p>15 Here, we developed a method to automatically extract image bases from measured fMRI responses to visual stimuli, and used them for image reconstruction. [sent-19, score-1.142]
</p><p>16 The pixel weights for each correspondence can be thought to deﬁne an image basis. [sent-23, score-0.266]
</p><p>17 As the early visual cortex is known to be organized in a retinotopic manner, one can assume that a small set of pixels corresponds to a small set of voxels. [sent-24, score-0.272]
</p><p>18 To facilitate the mapping between small 1  (a)  (b)  Figure 1: Model for estimating image bases. [sent-25, score-0.265]
</p><p>19 The visual image I (pixels) and an fMRI activity pattern r (voxels) is linked by latent variables z. [sent-27, score-0.619]
</p><p>20 The links from each latent variable to image pixels deﬁne an image basis WI , and the links from each latent variable to fMRI voxels is called a weight vector Wr . [sent-28, score-1.107]
</p><p>21 The matrices WI and Wr , the common latent variable z, and the inverse variances αI and αr are simultaneously estimated using the variational Bayesian method. [sent-31, score-0.263]
</p><p>22 Using the estimated parameters, the predictive distribution for a visual image given a new brain activity pattern is constructed (dashed line). [sent-32, score-0.648]
</p><p>23 The transformation matrix to the visual image can be regarded as a set of image bases. [sent-35, score-0.664]
</p><p>24 We introduced a sparseness prior into each element of the matrices, such that only small subsets of voxels and pixels are related with non-zero matrix elements. [sent-37, score-0.258]
</p><p>25 We show that spatially localized image bases were extracted, especially around the foveal region, whose shapes were similar to those used in the previous work. [sent-40, score-0.945]
</p><p>26 We also demonstrate that the model using the estimated image bases produced accurate visual image reconstruction. [sent-41, score-1.188]
</p><p>27 2  Method  We constructed a model in which a visual image is related with an fMRI activity pattern via latent variables (Figure 1). [sent-42, score-0.633]
</p><p>28 Each latent variable has links to a set of pixels, which can be regarded as an image basis because links from a single latent variable construct an element of a visual image. [sent-43, score-0.768]
</p><p>29 This model is equivalent to CCA: each latent variable corresponds to a canonical coefﬁcient [3] that bundles a subset of fMRI voxels responding to a speciﬁc visual stimulus. [sent-45, score-0.463]
</p><p>30 1  Canonical Correlation Analysis  We ﬁrst consider the standard CCA for estimating image bases given visual images I and fMRI activity patterns r. [sent-48, score-1.035]
</p><p>31 Let I be an N × 1 vector and r be a K × 1 vector where N is the number of image pixels, K is the number of fMRI voxels and t is a sample index. [sent-49, score-0.4]
</p><p>32 The variables u1 and v1 are called the ﬁrst canonical variables and the vectors a1 and b1 are called the canonical coefﬁcients. [sent-55, score-0.217]
</p><p>33 Then, the second canonical variables u2 (t) = a2 · I(t) and v2 (t) = b2 · r(t) are sought by maximizing the correlation of u2 and v2 while the second canonical variables are orthogonalized to the ﬁrst canonical variables. [sent-56, score-0.306]
</p><p>34 The number M is conventionally set to the smaller dimension of the two sets of observations: in our case, M = N because the number of visual-image pixels is much smaller than that of the fMRI 2  voxels (N < K). [sent-58, score-0.222]
</p><p>35 The visual image can be reconstructed by I(t) = A−1 · B · r(t), (3) where each column vector of the inverse matrix A−1 is an image basis. [sent-61, score-0.732]
</p><p>36 2  Bayesian CCA  Bayesian CCA introduces common latent variables that relate a visual image I and the fMRI activity pattern r with image basis set WI and weight vector set Wr (Figure 1 (b)). [sent-63, score-0.946]
</p><p>37 Hyper-prior distributions are also assumed for an inverse variance of each element of the image bases and the weight vectors. [sent-65, score-0.83]
</p><p>38 The image bases and the weight vectors are estimated as a posterior distribution by the variational Bayesian method [2]. [sent-66, score-0.915]
</p><p>39 After the parameters are determined, a predictive distribution for the visual image can be calculated. [sent-67, score-0.441]
</p><p>40 One is for visual images that are generated from latent variables. [sent-69, score-0.276]
</p><p>41 The other is for fMRI activity patterns that are generated from the same latent variables. [sent-70, score-0.203]
</p><p>42 In the current case, these priors and hyper-priors lead to a sparse selection of links from each latent variable to pixels and voxels. [sent-77, score-0.266]
</p><p>43 3  (11) (12)  Parameter estimation by the variational Bayesian method  The image bases and weight vectors are estimated as a posterior distribution P (WI , Wr |I, r), given the likelihood functions (Eqs. [sent-80, score-0.93]
</p><p>44 After the algorithm converges, image bases WI are calculated by taking the expectation of Q(WI ). [sent-103, score-0.759]
</p><p>45 4 Predictive distribution for visual image reconstruction Using the estimated parameters, we can derive the predictive distribution for a visual image Inew given a new brain activity rnew (Figure 1 (b), dashed line). [sent-105, score-1.338]
</p><p>46 The predictive distribution P (Inew |rnew ) is constructed from the likelihood of the visual image (Eq. [sent-107, score-0.455]
</p><p>47 (4)), the estimated distribution of image bases Q(WI ) (Eqs. [sent-108, score-0.814]
</p><p>48 (15) - (17)), and a posterior distribution of latent variables P (znew |rnew ) as follows, ∫ P (Inew |rnew ) = dWI dznew P (Inew |WI , znew )Q(WI )P (znew |rnew ). [sent-109, score-0.47]
</p><p>49 (25) Because the multiple integral over the random variable WI and znew is intractable, we replace the random variable WI with the estimated image bases WI to vanish the integral over WI . [sent-110, score-1.091]
</p><p>50 Then the predictive distribution becomes ∫ P (Inew |rnew ) dznew P (Inew |znew )P (znew |rnew ), (26) where  ] [ 1¯ 2 P (Inew |znew ) ∝ exp − βI ||Inew − WI znew || . [sent-111, score-0.372]
</p><p>51 We construct an approximate distribution Qz (znew ), by omitting the terms related to the visual image in Eqs. [sent-114, score-0.406]
</p><p>52 (18) - (20), Qz (znew ) = N (z|¯new , Σ−1 ), z znew  (28)  ¯ ¯ znew = βr Σ−1 Wr rnew , znew ( ) ¯r W Wr + Σ−1 + E. [sent-115, score-0.984]
</p><p>53 Σznew = β  (29)  where  r  5  wr  (30)  Finally, the predictive distribution is obtained by ∫ P (Inew |rnew ) dznew P (Inew |znew )Qz (znew ) = N (Inew |¯new , Σ−1 ), I Inew  (31)  where ¯ ¯new = βr WI Σ−1 W rnew , I znew r −1 ¯ ΣInew = WI Σ W + β −1 E. [sent-116, score-0.864]
</p><p>54 znew  I  I  (32) (33)  The reconstructed visual image is calculated by taking the expectation of the predictive distribution. [sent-117, score-0.782]
</p><p>55 One is a “random image session”, in which spatially random patterns were sequentially presented for 6 s followed by a 6 s rest period. [sent-122, score-0.345]
</p><p>56 The other is a “ﬁgure image session”, in which alphabetical letters and simple geometric shapes were sequentially presented for 12 s followed by a 12 s rest period. [sent-124, score-0.487]
</p><p>57 Five alphabetical letters and ﬁve geometric shapes were presented six or eight times per subject. [sent-125, score-0.224]
</p><p>58 3  Results  We estimated image bases and weight vectors using the data from the “random image session”. [sent-129, score-1.098]
</p><p>59 Then, reconstruction performance was evaluated with the data from the “ﬁgure image session”. [sent-130, score-0.369]
</p><p>60 1  Estimated image bases  Figure 2 (a) shows representative image bases estimated by Bayesian CCA (weight values are indicated by a gray scale). [sent-132, score-1.534]
</p><p>61 The estimation algorithm extracted spatially localized image bases whose shapes were consistent with those used in the previous study [6] (1 × 1, 1 × 2, and 2 × 1 shown in 1st and 2nd row of Figure 2 (a)). [sent-133, score-0.975]
</p><p>62 We repeated the estimation using data resampled from the random image session, and calculated the distribution of the image bases (deﬁned by a pixel cluster with magnitudes over 3 SD of all pixel values) over eccentricity for different sizes (Figure 2 (a), right). [sent-137, score-1.111]
</p><p>63 The image bases of the smallest size (1 × 1) were distributed over the visual ﬁeld, and most of them were within three degrees of eccentricity. [sent-138, score-0.876]
</p><p>64 The size of the image basis tended to increase with eccentricity. [sent-139, score-0.296]
</p><p>65 For comparison, we also performed the image basis estimation using CCA, but it did not produce spatially localized image bases (Figure 2 (b)). [sent-140, score-1.129]
</p><p>66 Estimated weight vectors for fMRI voxels had high values around the retinotopic region corresponding the location of the estimated basis (data not shown). [sent-141, score-0.347]
</p><p>67 2  Visual image reconstruction using estimated image bases  The reconstruction model with the estimated image bases was tested on ﬁve alphabet letters and ﬁve geometric shapes (Figure 3 (a), 1st row). [sent-143, score-2.338]
</p><p>68 However, the peripheral reconstruction was poor and often lacked shapes of the presented images. [sent-146, score-0.284]
</p><p>69 This may be due to the lack of estimated image bases in the peripheral regions (Figure 2 (a), right). [sent-147, score-0.843]
</p><p>70 The standard CCA produced poorer reconstruction with noise scattered over the entire image (Figure 3 (a), 3rd row), as expected from the non-local image bases estimated by CCA (Figure 2 (b)). [sent-148, score-1.167]
</p><p>71 Reconstruction using ﬁxed image bases [6] showed moderate accuracy for all image types (Figure 3 (a), 4th row). [sent-149, score-0.986]
</p><p>72 To evaluate the reconstruction performance quantitatively, we calculated the spatial correlation between the presented and reconstructed images (Figure 3 (b)). [sent-150, score-0.33]
</p><p>73 The correlation values 6  (a) Estimated image bases by Bayesian CCA 1-pixel basis 40 0  40  2x1  1x2  0  1x2  Fr equency  2-pixel basis  3-pixel basis 0. [sent-151, score-0.914]
</p><p>74 5  L-shape  3x1  1x3  1  2 3 4 5 Eccentr icity [ deg]  6  0  (b) Estimated image bases by CCA  Figure 2: Image basis estimation: (a) Representative bases estimated by Bayesian CCA (left, sorted by the number of pixels), and their frequency as a function of eccentricity (right). [sent-153, score-1.369]
</p><p>75 3-pixel bases (L-shape, 3x1 and 1x3) were not assumed in Miyawaki et al. [sent-154, score-0.512]
</p><p>76 Negative (dark) bases were often associated with negative voxel weights, thus equivalent to positive bases with positive voxel weights. [sent-156, score-1.012]
</p><p>77 (b) Examples of image bases estimated by the standard CCA. [sent-157, score-0.798]
</p><p>78 were not signiﬁcantly different between Bayesian CCA and the ﬁxed basis method when the alphabet letters and the geometric shapes were analyzed together. [sent-158, score-0.3]
</p><p>79 However, Bayesian CCA outperformed the ﬁxed basis method for the alphabet letters, while the ﬁxed basis method outperformed Bayesian CCA for the geometric shapes (p < . [sent-159, score-0.322]
</p><p>80 This is presumably because the alphabet letters consist of more foveal pixels, which overlap the region covered by the image bases estimated by Bayesian CCA. [sent-161, score-0.945]
</p><p>81 4  Discussion  We have proposed a new method to estimate image bases from fMRI data and presented visual stimuli. [sent-163, score-0.893]
</p><p>82 The previous work used ﬁxed image bases and estimated the weights between the image bases and fMRI voxels. [sent-165, score-1.534]
</p><p>83 This estimation was conducted by the sparse logistic regression that assumed sparsenes in the weight values, which effectively removed irrelevant voxels [8]. [sent-166, score-0.238]
</p><p>84 The proposed method introduced sparseness priors not only for fMRI voxels but also for image pixels. [sent-167, score-0.46]
</p><p>85 These priors lead to automatic extraction of images bases, and the mappings between a small number of fMRI voxels and a small number of image pixels. [sent-168, score-0.47]
</p><p>86 Using this model, we successfully extracted spatially localized image bases including those not used in the previous work [6]. [sent-169, score-0.837]
</p><p>87 Using the set of image bases, we were able to accurately reconstruct arbitrary contrast-deﬁned visual images from fMRI activity patterns. [sent-170, score-0.541]
</p><p>88 The sparseness priors played an important role to estimate spatially localized image bases, and to improve reconstruction performance, as demonstrated by the comparison with the results from standard CCA (Figure 2 and 3). [sent-171, score-0.511]
</p><p>89 First, as the latent variables were assumed to have an orthogonal Gaussian distribution, it may be difﬁcult to obtain non-orthogonal image bases, which have been 7  (a)  G eom etric shapes  Alphabet letters Presented  Reconstructed Bayesian CCA  CCA  Fixed bases (M iyawaki et al. [sent-173, score-1.122]
</p><p>90 )  Spatial Correlation  (b)  Bayesian CCA CCA Fixed bases (M iyawaki et al. [sent-174, score-0.516]
</p><p>91 4 0 All  Alphabet Letters  G eom etric shapes  Figure 3: Visual image reconstruction: (a) Presented images (1st row, alphabet letters and geometric shapes) and the reconstructed images obtained from Bayesian CCA, the standard CCA, and the ﬁxed basis model (2nd - 4th rows). [sent-177, score-0.769]
</p><p>92 shown to provide an effective image representation in the framework of sparse coding [4, 9]. [sent-179, score-0.266]
</p><p>93 Different types of image bases could be generated by introducing non-orthogonality and/or non-lineality in the model. [sent-180, score-0.736]
</p><p>94 The shape of estimated image bases may also depend on the visual stimuli used for the training of the reconstruction model. [sent-181, score-1.078]
</p><p>95 Although we used random images as visual stimuli, other types of images including natural scenes may lead to more effective image bases that allow for accurate reconstruction. [sent-182, score-0.991]
</p><p>96 Finally, our method failed to estimate peripheral image bases, and as a result, only poor reconstruction was achieved for peripheral pixels. [sent-183, score-0.459]
</p><p>97 The cortical magniﬁcation factor of the visual cortex [5] suggests that a small number of voxels represent a large number of image pixels in the periphery. [sent-184, score-0.633]
</p><p>98 Elaborate assumptions about the degree of sparseness depending on eccentricity may help to improve basis estimation and image reconstruction in the periphery. [sent-185, score-0.505]
</p><p>99 Visual image reconstruction from human brain activity using a combination of multiscale local image decoders. [sent-235, score-0.734]
</p><p>100 (2008) Sparse estimation automatically selects voxels relevant for the decoding of fMRI activity patterns. [sent-247, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bases', 0.486), ('cca', 0.361), ('wr', 0.31), ('znew', 0.263), ('image', 0.25), ('fmri', 0.245), ('wi', 0.232), ('inew', 0.21), ('rnew', 0.195), ('voxels', 0.15), ('visual', 0.14), ('qw', 0.135), ('reconstruction', 0.119), ('shapes', 0.103), ('activity', 0.092), ('qz', 0.09), ('latent', 0.09), ('miyawaki', 0.075), ('alphabet', 0.073), ('pixels', 0.072), ('reconstructed', 0.071), ('canonical', 0.068), ('estimated', 0.062), ('bayesian', 0.05), ('letters', 0.05), ('links', 0.049), ('basis', 0.046), ('images', 0.046), ('peripheral', 0.045), ('dznew', 0.045), ('session', 0.044), ('spatially', 0.044), ('correlation', 0.04), ('eccentricity', 0.039), ('retinotopic', 0.039), ('localized', 0.038), ('sparseness', 0.036), ('predictive', 0.035), ('variances', 0.034), ('trial', 0.033), ('variables', 0.031), ('weight', 0.031), ('eom', 0.03), ('iyawaki', 0.03), ('yoichi', 0.03), ('correspondences', 0.029), ('vb', 0.028), ('geometric', 0.028), ('yamashita', 0.026), ('alphabetical', 0.026), ('etric', 0.026), ('kamitani', 0.026), ('sato', 0.026), ('assumed', 0.026), ('variational', 0.026), ('posterior', 0.025), ('regarded', 0.024), ('foveal', 0.024), ('priors', 0.024), ('scenes', 0.023), ('brain', 0.023), ('calculated', 0.023), ('cortex', 0.021), ('stimuli', 0.021), ('patterns', 0.021), ('inverse', 0.021), ('contrasts', 0.02), ('voxel', 0.02), ('row', 0.02), ('vectors', 0.019), ('extracted', 0.019), ('presented', 0.017), ('fixed', 0.017), ('distribution', 0.016), ('automatically', 0.016), ('sparse', 0.016), ('pixel', 0.016), ('pattern', 0.016), ('distributions', 0.016), ('factorized', 0.015), ('matrices', 0.015), ('mapping', 0.015), ('estimation', 0.015), ('variable', 0.015), ('representing', 0.015), ('receptive', 0.014), ('spatial', 0.014), ('constructed', 0.014), ('calculation', 0.014), ('sequentially', 0.013), ('bution', 0.013), ('atr', 0.013), ('attias', 0.013), ('glover', 0.013), ('hikaridai', 0.013), ('kyoto', 0.013), ('outperformed', 0.013), ('exp', 0.013), ('reconstruct', 0.013), ('gamma', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="83-tfidf-1" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>2 0.33004567 <a title="83-tfidf-2" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>3 0.15044913 <a title="83-tfidf-3" href="./nips-2009-Exploring_Functional_Connectivities_of_the_Human_Brain_using_Multivariate_Information_Analysis.html">86 nips-2009-Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis</a></p>
<p>Author: Barry Chai, Dirk Walther, Diane Beck, Li Fei-fei</p><p>Abstract: In this study, we present a new method for establishing fMRI pattern-based functional connectivity between brain regions by estimating their multivariate mutual information. Recent advances in the numerical approximation of highdimensional probability distributions allow us to successfully estimate mutual information from scarce fMRI data. We also show that selecting voxels based on the multivariate mutual information of local activity patterns with respect to ground truth labels leads to higher decoding accuracy than established voxel selection methods. We validate our approach with a 6-way scene categorization fMRI experiment. Multivariate information analysis is able to ﬁnd strong information sharing between PPA and RSC, consistent with existing neuroscience studies on scenes. Furthermore, an exploratory whole-brain analysis uncovered other brain regions that share information with the PPA-RSC scene network.</p><p>4 0.13280827 <a title="83-tfidf-4" href="./nips-2009-Boosting_with_Spatial_Regularization.html">47 nips-2009-Boosting with Spatial Regularization</a></p>
<p>Author: Yongxin Xi, Uri Hasson, Peter J. Ramadge, Zhen J. Xiang</p><p>Abstract: By adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting. From this regularized loss framework we derive an efﬁcient boosting algorithm that uses additional weights/priors on the base classiﬁers. We prove that the proposed algorithm exhibits a “grouping effect”, which encourages the selection of all spatially local, discriminative base classiﬁers. The algorithm’s primary advantage is in applications where the trained classiﬁer is used to identify the spatial pattern of discriminative information, e.g. the voxel selection problem in fMRI. We demonstrate the algorithm’s performance on various data sets. 1</p><p>5 0.12996544 <a title="83-tfidf-5" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>6 0.12813474 <a title="83-tfidf-6" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>7 0.11611633 <a title="83-tfidf-7" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>8 0.10601867 <a title="83-tfidf-8" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>9 0.10350317 <a title="83-tfidf-9" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>10 0.1000321 <a title="83-tfidf-10" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>11 0.098655291 <a title="83-tfidf-11" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>12 0.094932705 <a title="83-tfidf-12" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>13 0.093453735 <a title="83-tfidf-13" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>14 0.089517914 <a title="83-tfidf-14" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>15 0.085359275 <a title="83-tfidf-15" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>16 0.085019998 <a title="83-tfidf-16" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>17 0.084628806 <a title="83-tfidf-17" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>18 0.073044069 <a title="83-tfidf-18" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>19 0.066543266 <a title="83-tfidf-19" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>20 0.065901935 <a title="83-tfidf-20" href="./nips-2009-fMRI-Based_Inter-Subject_Cortical_Alignment_Using_Functional_Connectivity.html">261 nips-2009-fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, -0.191), (2, -0.083), (3, 0.056), (4, 0.013), (5, 0.119), (6, 0.083), (7, -0.168), (8, 0.001), (9, 0.127), (10, 0.019), (11, 0.019), (12, -0.142), (13, 0.147), (14, -0.172), (15, -0.041), (16, 0.117), (17, 0.234), (18, 0.135), (19, -0.025), (20, 0.003), (21, -0.005), (22, 0.031), (23, -0.069), (24, -0.077), (25, -0.028), (26, 0.19), (27, 0.075), (28, -0.18), (29, -0.191), (30, 0.027), (31, 0.046), (32, -0.071), (33, -0.0), (34, -0.039), (35, -0.047), (36, 0.029), (37, -0.01), (38, 0.065), (39, -0.042), (40, 0.074), (41, -0.06), (42, -0.045), (43, -0.001), (44, -0.113), (45, -0.022), (46, -0.046), (47, -0.09), (48, 0.045), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9266367 <a title="83-lsi-1" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>2 0.68656391 <a title="83-lsi-2" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>3 0.67796993 <a title="83-lsi-3" href="./nips-2009-Canonical_Time_Warping_for_Alignment_of_Human_Behavior.html">50 nips-2009-Canonical Time Warping for Alignment of Human Behavior</a></p>
<p>Author: Feng Zhou, Fernando Torre</p><p>Abstract: Alignment of time series is an important problem to solve in many scientiﬁc disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW’s effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW. 1</p><p>4 0.36485234 <a title="83-lsi-4" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>5 0.3629041 <a title="83-lsi-5" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>Author: Long Zhu, Yuanahao Chen, Bill Freeman, Antonio Torralba</p><p>Abstract: We present a nonparametric Bayesian method for texture learning and synthesis. A texture image is represented by a 2D Hidden Markov Model (2DHMM) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout (the compatibility between adjacent textons). The 2DHMM is coupled with the Hierarchical Dirichlet process (HDP) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular. The HDP makes use of Dirichlet process prior which favors regular textures by penalizing the model complexity. This framework (HDP-2DHMM) learns the texton vocabulary and their spatial layout jointly and automatically. The HDP-2DHMM results in a compact representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art patch-based rendering methods. We also show that the HDP2DHMM can be applied to perform image segmentation and synthesis. The preliminary results suggest that HDP-2DHMM is generally useful for further applications in low-level vision problems. 1</p><p>6 0.35990703 <a title="83-lsi-6" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>7 0.3508803 <a title="83-lsi-7" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>8 0.35076877 <a title="83-lsi-8" href="./nips-2009-Exploring_Functional_Connectivities_of_the_Human_Brain_using_Multivariate_Information_Analysis.html">86 nips-2009-Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis</a></p>
<p>9 0.34815413 <a title="83-lsi-9" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>10 0.34178543 <a title="83-lsi-10" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>11 0.34047565 <a title="83-lsi-11" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>12 0.33981621 <a title="83-lsi-12" href="./nips-2009-Boosting_with_Spatial_Regularization.html">47 nips-2009-Boosting with Spatial Regularization</a></p>
<p>13 0.3336904 <a title="83-lsi-13" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>14 0.33270419 <a title="83-lsi-14" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>15 0.32504684 <a title="83-lsi-15" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>16 0.32501346 <a title="83-lsi-16" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>17 0.32034352 <a title="83-lsi-17" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>18 0.31806368 <a title="83-lsi-18" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>19 0.29560608 <a title="83-lsi-19" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>20 0.29266578 <a title="83-lsi-20" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.015), (25, 0.048), (33, 0.287), (35, 0.059), (36, 0.148), (39, 0.032), (58, 0.095), (71, 0.027), (81, 0.041), (86, 0.104), (91, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78843009 <a title="83-lda-1" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>Author: Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</p><p>Abstract: Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-deﬁned visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were ﬁxed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</p><p>2 0.72250879 <a title="83-lda-2" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>3 0.59611481 <a title="83-lda-3" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>4 0.5920763 <a title="83-lda-4" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>Author: Mingyuan Zhou, Haojun Chen, Lu Ren, Guillermo Sapiro, Lawrence Carin, John W. Paisley</p><p>Abstract: Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be nonstationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches. 1</p><p>5 0.58825004 <a title="83-lda-5" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><p>6 0.58779126 <a title="83-lda-6" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>7 0.58697498 <a title="83-lda-7" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>8 0.5854764 <a title="83-lda-8" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>9 0.58458525 <a title="83-lda-9" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>10 0.58344746 <a title="83-lda-10" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>11 0.58306766 <a title="83-lda-11" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>12 0.58267248 <a title="83-lda-12" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>13 0.5805676 <a title="83-lda-13" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>14 0.57876301 <a title="83-lda-14" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>15 0.57754922 <a title="83-lda-15" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>16 0.57749969 <a title="83-lda-16" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>17 0.57708794 <a title="83-lda-17" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>18 0.57691813 <a title="83-lda-18" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>19 0.5768019 <a title="83-lda-19" href="./nips-2009-Bilinear_classifiers_for_visual_recognition.html">46 nips-2009-Bilinear classifiers for visual recognition</a></p>
<p>20 0.57655317 <a title="83-lda-20" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
