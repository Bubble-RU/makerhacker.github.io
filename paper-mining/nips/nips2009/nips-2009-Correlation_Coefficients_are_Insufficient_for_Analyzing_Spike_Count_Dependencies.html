<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-62" href="#">nips2009-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</h1>
<br/><p>Source: <a title="nips-2009-62-pdf" href="http://papers.nips.cc/paper/3839-correlation-coefficients-are-insufficient-for-analyzing-spike-count-dependencies.pdf">pdf</a></p><p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>Reference: <a title="nips-2009-62-reference" href="../nips2009_reference/nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. [sent-9, score-0.613]
</p><p>2 We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. [sent-11, score-0.83]
</p><p>3 To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. [sent-12, score-1.307]
</p><p>4 Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. [sent-13, score-0.846]
</p><p>5 We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. [sent-14, score-1.189]
</p><p>6 Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. [sent-15, score-0.826]
</p><p>7 1  Introduction  The linear correlation coefﬁcient is of central importance in many studies that deal with spike count data of neural populations. [sent-16, score-0.605]
</p><p>8 For example, a low correlation coefﬁcient is often used as an evidence for independence in recorded data and to justify simplifying model assumptions (e. [sent-17, score-0.334]
</p><p>9 In line with this many computational studies constructed distributions for observed data based solely on reported correlation coefﬁcients [3, 4, 5, 6]. [sent-20, score-0.296]
</p><p>10 The correlation coefﬁcient is also extensively used in combination with information measures such as the Fisher information (for continuous variables only) and the Shannon information to assess the importance of couplings between neurons for neural coding [7]. [sent-22, score-0.389]
</p><p>11 In [13], for example, based on the correlation coefﬁcient it was argued that the impact of correlations is negligible for small populations of neurons. [sent-30, score-0.382]
</p><p>12 The correlation coefﬁcient is one measure of dependence among others. [sent-31, score-0.499]
</p><p>13 It has become common to report only the correlation coefﬁcient of recorded spike trains without reporting any other properties  of the actual dependence structure (see e. [sent-32, score-0.795]
</p><p>14 The problem with this common practice is that it is unclear beforehand whether the linear correlation coefﬁcient sufﬁces to describe the dependence or at least the relevant part of the dependence. [sent-35, score-0.499]
</p><p>15 Yet, it might seem likely that this is not important for realistic spike count distributions which have a Poisson-like shape. [sent-37, score-0.418]
</p><p>16 Indeed, the dependence structure can have a profound impact on the information of spike count distributions with Poisson-like single neuron statistics. [sent-42, score-0.688]
</p><p>17 As a matter of fact, the correlation coefﬁcient places only a weak constraint on the dependence structure. [sent-44, score-0.499]
</p><p>18 Moreover, we show that uncorrelated or weakly correlated spike counts with strong dependencies are very likely to be common in biological networks. [sent-45, score-0.732]
</p><p>19 Thus, it is not sufﬁcient to report only the correlation coefﬁcient or to derive strong implications like independence from a low correlation coefﬁcient alone. [sent-46, score-0.605]
</p><p>20 At least a statistical test should be applied that states for a given signiﬁcance level whether the dependence is well characterized by the linear correlation coefﬁcient. [sent-47, score-0.566]
</p><p>21 The test is adjusted to the setting that a neuroscientist typically faces, namely the case of Poisson-like spike count distributions of single neurons and small numbers of samples. [sent-49, score-0.506]
</p><p>22 In the next section, we describe state-of-the-art methods for modeling dependent spike counts, to compute their entropy, and to generate network models based on integrate-and-ﬁre neurons. [sent-50, score-0.352]
</p><p>23 Section 3 shows examples of what can go wrong for entropy estimation when relying on the correlation coefﬁcient only. [sent-51, score-0.401]
</p><p>24 Section 4 introduces the linear correlation test which is tailored to the needs of neuroscience applications and the section examines its performance on different dependence structures. [sent-53, score-0.588]
</p><p>25 2  General methods  We will now describe formal aspects of spike count models and their Shannon information. [sent-55, score-0.34]
</p><p>26 1  Copula-based models with discrete marginals  A copula is a cumulative distribution function (CDF) which is deﬁned on the unit hypercube and has uniform marginals [16]. [sent-57, score-1.024]
</p><p>27 Formally, a bivariate copula C is deﬁned as follows: Deﬁnition 1. [sent-58, score-0.677]
</p><p>28 A copula is a function C : [0, 1]2 −→ [0, 1] such that: 1. [sent-59, score-0.591]
</p><p>29 There are many families of copulas representing different dependence structures. [sent-64, score-0.413]
</p><p>30 As θ → ±∞ the copula approaches deterministic positive/negative dependence: knowledge of one variable implies knowledge of the other (so-called Fr´ chet-Hoeffding bounds [16]). [sent-69, score-0.591]
</p><p>31 Thereby, we can generate a multivariate distribution with speciﬁc marginals FXi and a dependence structure determined by C. [sent-74, score-0.439]
</p><p>32 Typically, the Poisson distribution is a good approximation to spike count variations of single neurons [20]. [sent-76, score-0.434]
</p><p>33 where λi is the mean spike count of neuron i for a given bin size. [sent-78, score-0.391]
</p><p>34 2  Computation of information entropy  The Shannon entropy [21] of dependent spike counts X is a measure of the information that a decoder is missing when it does not know the value x of X. [sent-87, score-0.624]
</p><p>35 (b, e): Strong negative dependence in outer square: θ1 = −30, θ2 = 5, ω = 0. [sent-126, score-0.278]
</p><p>36 3  Counter examples  In this section we describe entropy variations that can occur when relying on the correlation coefﬁcient only. [sent-130, score-0.445]
</p><p>37 We will evaluate this effect for models of spike counts which have Poisson-like marginals and show that such effects can occur in very simple biological networks. [sent-131, score-0.679]
</p><p>38 1  Frank shufﬂe copula  We will now introduce the Frank shufﬂe copula family. [sent-133, score-1.182]
</p><p>39 This copula family allows arbitrarily strong dependencies with a correlation coefﬁcient of zero for attached Poisson-like marginals. [sent-134, score-1.079]
</p><p>40 The following function deﬁnes a copula ∀θ1 , θ2 ∈ R, ω ∈ [0, 0. [sent-138, score-0.591]
</p><p>41 The proof of the copula properties is given in Appendix A. [sent-140, score-0.591]
</p><p>42 This family is capable of modeling a continuum between independence and deterministic dependence while keeping the correlation coefﬁcient at zero. [sent-141, score-0.67]
</p><p>43 There are two regions: the outer region [0, 1]2 \ (ω, 1 − ω)2 contains a Frank copula with θ1 and the inner square (ω, 1 − ω)2 contains a Frank copula with θ2 modiﬁed by a factor z. [sent-142, score-1.182]
</p><p>44 If we would restrict our analysis to copula-based distributions with continuous marginals it would be sufﬁcient to select θ1 = −θ2 and to adjust ω such that the correlation coefﬁcient would vanish. [sent-143, score-0.501]
</p><p>45 We will now investigate the impact of this dependence structure on the entropy of copula-based distributions with Poisson-like marginals while keeping the correlation coefﬁcient at zero. [sent-148, score-0.899]
</p><p>46 Therefore, we expect that the entropy can vary considerably for different dependence strengths, even though the correlation is always zero. [sent-150, score-0.607]
</p><p>47 4  Poisson Negative Binomial  2 0 0  −10  −20  θ1  (a)  −30  −40  −50  Entropy Difference (%)  Entropy (Bits)  6  30 20 Poisson  10  Negative Binomial 0 0  −10  −20  θ1  −30  −40  −50  (b)  Figure 2: Entropy of distributions based on the Frank shufﬂe copula Cθ1 ,θ2 ,ω for ω = 0. [sent-151, score-0.622]
</p><p>48 For negative binomial marginals 2 2 we selected rates λ1 = 2. [sent-156, score-0.344]
</p><p>49 2(a) shows the entropy of the Frank shufﬂe-based models with Poisson and negative binomial marginals for uncorrelated but dependent elements. [sent-164, score-0.547]
</p><p>50 θ1 was varied while θ2 was estimated using the line-search algorithm for constrained nonlinear minimization [23] with the absolute correlation coefﬁcient as the objective function. [sent-165, score-0.292]
</p><p>51 With increasing dependence the entropy decreases until it reaches a minimum at θ1 = −20. [sent-167, score-0.366]
</p><p>52 The region of strong dependence shifts to a region with small mass. [sent-170, score-0.269]
</p><p>53 The entropy deviated by up to 25 % for the Poisson marginals and up to 15 % for the negative binomial marginals. [sent-175, score-0.423]
</p><p>54 So the entropy varies indeed considerably in spite of ﬁxed marginals and uncorrelated elements. [sent-176, score-0.412]
</p><p>55 We constructed a copula family which allowed us to vary the dependence strength systematically while keeping the variables uncorrelated. [sent-177, score-0.98]
</p><p>56 2  LIF network  We will now explore the feasibility of uncorrelated spike counts with strong dependencies in a biologically realistic network model. [sent-181, score-0.743]
</p><p>57 Note that the bottom input population does not contradict to Dale’s principle, since excitatory neurons can project to both excitatory and inhibitory neurons. [sent-187, score-0.302]
</p><p>58 We can ﬁnd a copula family which can model this relation and has two separate parameters for the strengths of the input populations: 1 −1/θ1 cm Cθ1 ,θ2 (u, v) = max u−θ1 + v −θ1 − 1, 0 2 (2) 1 −1/θ2 + , u − max u−θ2 + (1 − v)−θ2 − 1, 0 2 where θ1 , θ2 ∈ (0, ∞). [sent-188, score-0.796]
</p><p>59 It is a mixture of the well known Clayton copula and an one element survival transformation of the Clayton copula [16]. [sent-189, score-1.219]
</p><p>60 3(c) shows the correlation coefﬁcients of the network generated spike counts and of Cθ1 ,θ2 ﬁts. [sent-194, score-0.708]
</p><p>61 The rate of population D that introduces negative dependence is kept constant, while the rate of population B that introduces positive dependence is varied. [sent-195, score-0.624]
</p><p>62 2  250 300 350 Input Rate of Top Center Population (Hz)  (c)  Figure 3: Strong dependence with zero correlation in a biological network model. [sent-210, score-0.606]
</p><p>63 (a): Neural network models used to generate synthetic spike count data. [sent-211, score-0.4]
</p><p>64 3) receive spike inputs (circles for excitation, bars for inhibition) from four separate populations of neurons (rectangular boxes and circles, A-D), but only two populations (B, D) send input to both neurons. [sent-213, score-0.453]
</p><p>65 (c): Correlation coefﬁcients of cm network generated spike counts compared to correlations of a maximum likelihood ﬁt of the Cθ1 ,θ2 copula family to these counts. [sent-218, score-1.235]
</p><p>66 Solid line: correlation coefﬁcients of counts generated by the network shown in (a). [sent-219, score-0.441]
</p><p>67 We increased the absolute correlation between the spike counts by shifting the rates: we decreased the rates of A and C and increased the rate of B. [sent-222, score-0.677]
</p><p>68 At approximately 275 Hz the dependencies cancel each other out in the correlation coefﬁcient. [sent-228, score-0.346]
</p><p>69 Therefore, correlation coefﬁcients of spike counts that do not at all reﬂect the true strength of dependence are very likely to occur in biological networks. [sent-230, score-1.019]
</p><p>70 Hence, it is hard to construct an appropriate copula that is parametrized such that individual dependence strengths are revealed. [sent-233, score-0.868]
</p><p>71 The goal of the next section is to assess a test that reveals whether the linear correlation coefﬁcient provides an appropriate measure for the dependence. [sent-234, score-0.306]
</p><p>72 4  Linear correlation test  We will now describe a test for bivariate distributions with Poisson-like marginals that determines whether the dependence structure is well characterized by the linear correlation coefﬁcient. [sent-235, score-1.194]
</p><p>73 The semiparametric model that we use consists of the empirical marginals of the sample coupled by a parametric copula family. [sent-238, score-0.831]
</p><p>74 A dependence structure is well characterized by the linear correlation coefﬁcient if it is Gauss-like. [sent-239, score-0.525]
</p><p>75 So one way to test for linear dependence would be to use the Gaussian copula family. [sent-240, score-0.866]
</p><p>76 Fortunately, a whole class of copula families that are Gauss-like exists. [sent-242, score-0.631]
</p><p>77 5  (d)  Figure 4: Percent acceptance of the linear correlation hypothesis for different copula-based models with different dependence strengths and Poisson marginals with rates λ1 = λ2 = 5. [sent-254, score-0.843]
</p><p>78 On the x-axis we varied the strength of the dependence by means of the copula parameters. [sent-257, score-0.876]
</p><p>79 (a): Frank shufﬂe family with cm correlation kept at zero. [sent-258, score-0.427]
</p><p>80 For θ = 0 the Frank copula corresponds to independence. [sent-263, score-0.591]
</p><p>81 Therefore, the usual χ2 independence test is a special case of our linear correlation test. [sent-264, score-0.346]
</p><p>82 As an alternative we propose to estimate the copula parameter θ by means of Kendall’s τ . [sent-267, score-0.591]
</p><p>83 Kendall’s τ is a measure of dependence deﬁned as τ (x, y) = c−d , where c is the c+d number of elements in the set {(i, j)|(xi < xj and yi < yj ) or (xi > xj and yi > yj )} and d is the number of element in the set {(i, j)|(xi < xj and yi > yj ) or (xi > xj and yi < yj )} [16]. [sent-268, score-0.354]
</p><p>84 For the Frank copula with continuous marginals the relation between τ and θ is given by τθ = x 4 k tk 1 − θ [1 − D1 (θ)], where Dk (x) is the Debye function Dk (x) = xk 0 exp(t)−1 dt [25]. [sent-269, score-0.796]
</p><p>85 To verify the test we applied it to samples from copula-based distributions with Poisson marginals and four different copula families: the Frank shufﬂe family (Proposition 1), the Clayton mixture family (Eq. [sent-290, score-1.143]
</p><p>86 For the Frank family and the Gaussian family the linear correlation coefﬁcient is well suited to characterize their  dependence. [sent-294, score-0.479]
</p><p>87 We therefore expected that the test should accept H0 , regardless of the dependence strength. [sent-295, score-0.275]
</p><p>88 In contrast, for the Frank shufﬂe family and the Clayton mixture family the linear correlation does not reﬂect the dependence strength. [sent-296, score-0.75]
</p><p>89 For each of the families there was no dependence when the ﬁrst copula parameter was equal to zero. [sent-300, score-0.865]
</p><p>90 The Frank and the Gaussian families have only Gauss-like dependence, meaning the correlation coefﬁcient is well-suited to describe the data. [sent-301, score-0.305]
</p><p>91 The plots in (a) and (b) indicate the Type II errors: H0 was accepted although the dependence structure of the counts was not Gauss-like. [sent-306, score-0.35]
</p><p>92 5  Conclusion  We investigated a worst-case scenario for reliance on the linear correlation coefﬁcient for analyzing dependent spike counts using the Shannon information. [sent-310, score-0.673]
</p><p>93 The spike counts were uncorrelated but had a strong dependence. [sent-311, score-0.517]
</p><p>94 Thus, relying solely on the correlation coefﬁcient would lead to an oversight of such dependencies. [sent-312, score-0.293]
</p><p>95 Although uncorrelated with ﬁxed marginals the information varied by more than 25 %. [sent-313, score-0.331]
</p><p>96 Our test provides a convenient tool to verify whether the correlation coefﬁcient is the right measure for an assessment of the dependence. [sent-316, score-0.33]
</p><p>97 If the test rejects the Gauss-like dependence hypothesis, more elaborate measures of the dependence should be applied. [sent-317, score-0.509]
</p><p>98 An adequate copula family provides one way to ﬁnd such a measure. [sent-318, score-0.698]
</p><p>99 Stimulus dependence of neuronal correlation in primary visual cortex of the macaque. [sent-443, score-0.532]
</p><p>100 Modeling short-term noise dependence of u a spike counts in macaque prefrontal cortex. [sent-467, score-0.687]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('copula', 0.591), ('spike', 0.267), ('correlation', 0.265), ('dependence', 0.234), ('frank', 0.209), ('marginals', 0.205), ('coef', 0.191), ('cdf', 0.148), ('copulas', 0.139), ('counts', 0.116), ('entropy', 0.108), ('family', 0.107), ('shuf', 0.104), ('clayton', 0.104), ('uncorrelated', 0.099), ('frequencies', 0.094), ('neurons', 0.094), ('fxi', 0.087), ('imax', 0.087), ('fx', 0.086), ('bivariate', 0.086), ('dependencies', 0.081), ('count', 0.073), ('poisson', 0.071), ('acceptance', 0.067), ('binomial', 0.066), ('leaky', 0.062), ('network', 0.06), ('population', 0.056), ('cm', 0.055), ('excitatory', 0.054), ('neuron', 0.051), ('neuroscience', 0.048), ('biological', 0.047), ('berlin', 0.047), ('populations', 0.046), ('membrane', 0.045), ('occur', 0.044), ('negative', 0.044), ('inhibitory', 0.044), ('strengths', 0.043), ('cients', 0.042), ('macaque', 0.042), ('correlated', 0.041), ('test', 0.041), ('families', 0.04), ('independence', 0.04), ('shannon', 0.04), ('correlations', 0.039), ('mixture', 0.037), ('strong', 0.035), ('semiparametric', 0.035), ('realization', 0.035), ('bccn', 0.035), ('lder', 0.035), ('loukas', 0.035), ('mef', 0.035), ('onken', 0.035), ('poissonlike', 0.035), ('steffen', 0.035), ('vreset', 0.035), ('vth', 0.035), ('cient', 0.034), ('statistic', 0.034), ('ms', 0.033), ('neuronal', 0.033), ('impact', 0.032), ('distributions', 0.031), ('limt', 0.03), ('averbeck', 0.03), ('commutative', 0.03), ('yj', 0.03), ('coding', 0.03), ('recorded', 0.029), ('rates', 0.029), ('relying', 0.028), ('pathological', 0.028), ('prefrontal', 0.028), ('synapse', 0.028), ('technische', 0.028), ('grouped', 0.027), ('px', 0.027), ('varied', 0.027), ('hz', 0.027), ('characterized', 0.026), ('kendall', 0.026), ('dependent', 0.025), ('realistic', 0.025), ('strength', 0.024), ('reaches', 0.024), ('ring', 0.024), ('weakly', 0.024), ('verify', 0.024), ('keeping', 0.024), ('latham', 0.024), ('lim', 0.023), ('discrete', 0.023), ('likely', 0.022), ('universit', 0.021), ('mv', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="62-tfidf-1" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>2 0.23176068 <a title="62-tfidf-2" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><p>3 0.18218893 <a title="62-tfidf-3" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>4 0.1355672 <a title="62-tfidf-4" href="./nips-2009-Optimal_context_separation_of_spiking_haptic_signals_by_second-order_somatosensory_neurons.html">183 nips-2009-Optimal context separation of spiking haptic signals by second-order somatosensory neurons</a></p>
<p>Author: Romain Brasselet, Roland Johansson, Angelo Arleo</p><p>Abstract: We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve ﬁbers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efﬁciency of the haptic discrimination process is quantiﬁed by a novel deﬁnition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the ﬁrst afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system. 1</p><p>5 0.11931596 <a title="62-tfidf-5" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>Author: Jonathan W. Pillow</p><p>Abstract: Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a conditional renewal (CR) model for neural spike trains. This model captures both real-time and rescaled-time history effects, and can be ﬁt by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear ﬁlter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with shape κ = 1), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-ﬁt tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data. 1</p><p>6 0.10993746 <a title="62-tfidf-6" href="./nips-2009-Know_Thy_Neighbour%3A_A_Normative_Theory_of_Synaptic_Depression.html">121 nips-2009-Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></p>
<p>7 0.10360333 <a title="62-tfidf-7" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>8 0.10267469 <a title="62-tfidf-8" href="./nips-2009-Reconstruction_of_Sparse_Circuits_Using_Multi-neuronal_Excitation_%28RESCUME%29.html">200 nips-2009-Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></p>
<p>9 0.10128989 <a title="62-tfidf-9" href="./nips-2009-Neurometric_function_analysis_of_population_codes.html">163 nips-2009-Neurometric function analysis of population codes</a></p>
<p>10 0.093420826 <a title="62-tfidf-10" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>11 0.092621721 <a title="62-tfidf-11" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>12 0.087374307 <a title="62-tfidf-12" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>13 0.075087398 <a title="62-tfidf-13" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>14 0.067779906 <a title="62-tfidf-14" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>15 0.063949235 <a title="62-tfidf-15" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>16 0.059999071 <a title="62-tfidf-16" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>17 0.05888797 <a title="62-tfidf-17" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>18 0.058528714 <a title="62-tfidf-18" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>19 0.056160159 <a title="62-tfidf-19" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>20 0.05494839 <a title="62-tfidf-20" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.16), (1, -0.129), (2, 0.233), (3, 0.139), (4, 0.038), (5, -0.098), (6, -0.092), (7, 0.003), (8, 0.024), (9, 0.019), (10, -0.028), (11, -0.002), (12, 0.02), (13, -0.033), (14, -0.055), (15, -0.008), (16, 0.021), (17, -0.009), (18, 0.089), (19, 0.027), (20, 0.025), (21, 0.001), (22, -0.098), (23, 0.039), (24, -0.1), (25, 0.003), (26, 0.066), (27, 0.033), (28, 0.039), (29, -0.01), (30, -0.016), (31, 0.08), (32, -0.063), (33, -0.076), (34, 0.045), (35, 0.075), (36, -0.076), (37, 0.027), (38, -0.069), (39, 0.055), (40, -0.095), (41, -0.001), (42, 0.013), (43, 0.057), (44, 0.046), (45, -0.017), (46, 0.01), (47, -0.061), (48, 0.064), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96070242 <a title="62-lsi-1" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>2 0.7924481 <a title="62-lsi-2" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><p>3 0.78140593 <a title="62-lsi-3" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>Author: Sebastian Gerwinn, Philipp Berens, Matthias Bethge</p><p>Abstract: Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining the joint secondorder statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural ﬁring patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and ﬁtting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the ﬁring patterns of neural ensembles and the stimuli they are processing. 1</p><p>4 0.78064573 <a title="62-lsi-4" href="./nips-2009-Optimal_context_separation_of_spiking_haptic_signals_by_second-order_somatosensory_neurons.html">183 nips-2009-Optimal context separation of spiking haptic signals by second-order somatosensory neurons</a></p>
<p>Author: Romain Brasselet, Roland Johansson, Angelo Arleo</p><p>Abstract: We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve ﬁbers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efﬁciency of the haptic discrimination process is quantiﬁed by a novel deﬁnition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the ﬁrst afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system. 1</p><p>5 0.77523726 <a title="62-lsi-5" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>Author: Jonathan W. Pillow</p><p>Abstract: Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a conditional renewal (CR) model for neural spike trains. This model captures both real-time and rescaled-time history effects, and can be ﬁt by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear ﬁlter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with shape κ = 1), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-ﬁt tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data. 1</p><p>6 0.62969214 <a title="62-lsi-6" href="./nips-2009-Know_Thy_Neighbour%3A_A_Normative_Theory_of_Synaptic_Depression.html">121 nips-2009-Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></p>
<p>7 0.62270337 <a title="62-lsi-7" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>8 0.60034144 <a title="62-lsi-8" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>9 0.55043161 <a title="62-lsi-9" href="./nips-2009-Neurometric_function_analysis_of_population_codes.html">163 nips-2009-Neurometric function analysis of population codes</a></p>
<p>10 0.47013277 <a title="62-lsi-10" href="./nips-2009-Reconstruction_of_Sparse_Circuits_Using_Multi-neuronal_Excitation_%28RESCUME%29.html">200 nips-2009-Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></p>
<p>11 0.4258616 <a title="62-lsi-11" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>12 0.42068541 <a title="62-lsi-12" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>13 0.40302503 <a title="62-lsi-13" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>14 0.38355517 <a title="62-lsi-14" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>15 0.36645803 <a title="62-lsi-15" href="./nips-2009-No_evidence_for_active_sparsification_in_the_visual_cortex.html">164 nips-2009-No evidence for active sparsification in the visual cortex</a></p>
<p>16 0.35813105 <a title="62-lsi-16" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>17 0.34975198 <a title="62-lsi-17" href="./nips-2009-Clustering_sequence_sets_for_motif_discovery.html">51 nips-2009-Clustering sequence sets for motif discovery</a></p>
<p>18 0.34882563 <a title="62-lsi-18" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>19 0.33931768 <a title="62-lsi-19" href="./nips-2009-A_General_Projection_Property_for_Distribution_Families.html">11 nips-2009-A General Projection Property for Distribution Families</a></p>
<p>20 0.32028216 <a title="62-lsi-20" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.028), (24, 0.038), (25, 0.055), (35, 0.056), (36, 0.072), (39, 0.039), (44, 0.011), (58, 0.12), (61, 0.015), (62, 0.021), (71, 0.054), (81, 0.05), (86, 0.066), (91, 0.062), (92, 0.015), (93, 0.011), (96, 0.198), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85757673 <a title="62-lda-1" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Klaus Obermayer</p><p>Abstract: The linear correlation coefﬁcient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefﬁcient is in general insufﬁcient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-ﬁre neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We ﬁnd that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poissonlike marginals is well characterized by the linear correlation coefﬁcient and verify it for different copula-based models. 1</p><p>2 0.82850975 <a title="62-lda-2" href="./nips-2009-Manifold_Regularization_for_SIR_with_Rate_Root-n_Convergence.html">146 nips-2009-Manifold Regularization for SIR with Rate Root-n Convergence</a></p>
<p>Author: Wei Bian, Dacheng Tao</p><p>Abstract: In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.</p><p>3 0.80603528 <a title="62-lda-3" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>Author: Baback Moghaddam, Emtiyaz Khan, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We make several contributions in accelerating approximate Bayesian structural inference for non-decomposable GGMs. Our ﬁrst contribution is to show how to efﬁciently compute a BIC or Laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation. This optimization technique can be used as a fast scoring function inside standard Stochastic Local Search (SLS) for generating posterior samples. Our second contribution is a novel framework for efﬁciently generating large sets of high-quality graph topologies without performing local search. This graph proposal method, which we call “Neighborhood Fusion” (NF), samples candidate Markov blankets at each node using sparse regression techniques. Our third contribution is a hybrid method combining the complementary strengths of NF and SLS. Experimental results in structural recovery and prediction tasks demonstrate that NF and hybrid NF/SLS out-perform state-of-the-art local search methods, on both synthetic and real-world datasets, when realistic computational limits are imposed.</p><p>4 0.76365042 <a title="62-lda-4" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>Author: Hongjing Lu, Matthew Weiden, Alan L. Yuille</p><p>Abstract: We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We deﬁne conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efﬁciently perform model selection to decide whether observations are from one or multiple categories.</p><p>5 0.74693817 <a title="62-lda-5" href="./nips-2009-Fast_Learning_from_Non-i.i.d._Observations.html">94 nips-2009-Fast Learning from Non-i.i.d. Observations</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from α-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case. 1</p><p>6 0.68615294 <a title="62-lda-6" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>7 0.67219204 <a title="62-lda-7" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>8 0.65840948 <a title="62-lda-8" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>9 0.65325898 <a title="62-lda-9" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>10 0.65321767 <a title="62-lda-10" href="./nips-2009-Neurometric_function_analysis_of_population_codes.html">163 nips-2009-Neurometric function analysis of population codes</a></p>
<p>11 0.6526286 <a title="62-lda-11" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>12 0.65157497 <a title="62-lda-12" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>13 0.65119302 <a title="62-lda-13" href="./nips-2009-A_Neural_Implementation_of_the_Kalman_Filter.html">13 nips-2009-A Neural Implementation of the Kalman Filter</a></p>
<p>14 0.65027833 <a title="62-lda-14" href="./nips-2009-Semi-supervised_Learning_using_Sparse_Eigenfunction_Bases.html">213 nips-2009-Semi-supervised Learning using Sparse Eigenfunction Bases</a></p>
<p>15 0.6502285 <a title="62-lda-15" href="./nips-2009-Know_Thy_Neighbour%3A_A_Normative_Theory_of_Synaptic_Depression.html">121 nips-2009-Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></p>
<p>16 0.64858764 <a title="62-lda-16" href="./nips-2009-Augmenting_Feature-driven_fMRI_Analyses%3A_Semi-supervised_learning_and_resting_state_activity.html">38 nips-2009-Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></p>
<p>17 0.64777881 <a title="62-lda-17" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>18 0.64721048 <a title="62-lda-18" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>19 0.64612728 <a title="62-lda-19" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>20 0.64586568 <a title="62-lda-20" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
