<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-127" href="#">nips2009-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</h1>
<br/><p>Source: <a title="nips-2009-127-pdf" href="http://papers.nips.cc/paper/3845-learning-label-embeddings-for-nearest-neighbor-multi-class-classification-with-an-application-to-speech-recognition.pdf">pdf</a></p><p>Author: Natasha Singh-miller, Michael Collins</p><p>Abstract: We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. 1</p><p>Reference: <a title="nips-2009-127-reference" href="../nips2009_reference/nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. [sent-4, score-0.446]
</p><p>2 We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. [sent-5, score-0.698]
</p><p>3 The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. [sent-6, score-0.212]
</p><p>4 We apply these estimates to the problem of acoustic modeling for speech recognition. [sent-7, score-0.507]
</p><p>5 We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. [sent-8, score-0.218]
</p><p>6 These approaches learn an embedding (for example a linear projection) of input points, and give signiﬁcant improvements in the performance of NN classiﬁers. [sent-10, score-0.136]
</p><p>7 In this paper we focus on the application of NN methods to multi-class problems, where the number of possible labels is large, and where there is signiﬁcant structure within the space of possible labels. [sent-11, score-0.152]
</p><p>8 We describe an approach that induces prototype vectors My ∈ ℜL (similar to error-correcting output codes (ECOCs)) for each label y, from a set of training examples {(ai , yi )} for i = 1 . [sent-12, score-0.604]
</p><p>9 The prototype vectors are embedded within a NN model that estimates P (y|a); the vectors are learned using a leave-one-out estimate of conditional log-likelihood (CLL) derived from the training examples. [sent-16, score-0.457]
</p><p>10 The end result is a method that embeds labels y into ℜL in a way that signiﬁcantly improves conditional log-likelihood estimates for multi-class problems under a NN classiﬁer. [sent-17, score-0.208]
</p><p>11 The application we focus on is acoustic modeling for speech recognition, where each input a ∈ ℜD is a vector of measured acoustic features, and each label y ∈ Y is an acoustic-phonetic label. [sent-18, score-0.833]
</p><p>12 We describe experiments measuring both conditional log-likelihood of test data, and word error rates when the method is incorporated within a full speech recogniser. [sent-20, score-0.313]
</p><p>13 In both settings the experiments show signiﬁcant improvements for the ECOC method over both baseline NN methods (e. [sent-21, score-0.115]
</p><p>14 , the method of [8]), as well as Gaussian mixture models (GMMs), as conventionally used in speech recognition systems. [sent-23, score-0.294]
</p><p>15 While our experiments are on speech recognition, the method should be relevant to other domains which involve large multi-class problems with structured labels—for example problems in natural language processing, or in computer vision (e. [sent-24, score-0.183]
</p><p>16 Our work is related to previous work on error-correcting output codes for multi-class problems. [sent-31, score-0.247]
</p><p>17 [1, 2, 4, 9] describe error-correcting output codes; more recently [2, 3, 11] have described algorithms for learning ECOCs. [sent-32, score-0.09]
</p><p>18 Our work differs from previous work in that ECOC codes are learned within a nearest-neighbor framework. [sent-33, score-0.25]
</p><p>19 Also, we learn the ECOC codes in order to model the underlying structure of the label space and not speciﬁcally to combine the results of multiple classiﬁers. [sent-34, score-0.35]
</p><p>20 3  Background  The goal of our work is to derive a model that estimates P (y|a) where a ∈ ℜD is a feature vector representing some input, and y is a label drawn from a set of possible labels Y. [sent-35, score-0.3]
</p><p>21 For the speech recognition application considered in this paper, Y consists of 1871 acoustic-phonetic classes that may be highly correlated with one another. [sent-41, score-0.305]
</p><p>22 Leveraging structure in the label space will be crucial to providing good estimates of P (y|a); we would like to learn the inherent structure of the label space automatically. [sent-42, score-0.24]
</p><p>23 Note in addition that efﬁciency is important within the speech recognition application: in our experiments we make use of around 11 million training samples, while the dimensionality of the data is D = 50. [sent-43, score-0.329]
</p><p>24 As a ﬁrst baseline approach—and as a starting point for the methods we develop—consider the neighbor components analysis (NCA) method introduced by [8]. [sent-45, score-0.145]
</p><p>25 In NCA, for any test point a, a distribution α(j|a) over the training examples is deﬁned as follows where α(j|a) decreases rapidly as the distance between a and aj increases. [sent-46, score-0.122]
</p><p>26 α(j|a) =  e−||a−aj ||  2  N −||a−am ||2 m=1 e  (1)  The estimate of P (y|a) is then deﬁned as follows: N  Pnca (y|a) =  α(i|a) i=1,yi =y  2  (2)  ′  In NCA the original training data consists of points (xi , yi ) for i = 1 . [sent-47, score-0.116]
</p><p>27 The method learns a projection matrix A that deﬁnes the modiﬁed representation ai = Axi (the same transformation is applied to test points). [sent-51, score-0.148]
</p><p>28 The matrix A is learned from training examples, to optimize log-likelihood under the model in Eq. [sent-52, score-0.127]
</p><p>29 In our experiments we assume that a = Ax for some underlying representation x and a projection matrix A that has been learned using NCA to optimize the log-likelihood of the training set. [sent-54, score-0.154]
</p><p>30 As a result the matrix A, and consequently the representation a, are well-calibrated in terms of using nearest neighbors to estimate P (y|a) through Eq. [sent-55, score-0.128]
</p><p>31 A ﬁrst baseline method for our problem is therefore to directly use the estimates deﬁned by Eq. [sent-57, score-0.128]
</p><p>32 We will, however, see that this baseline method performs poorly at providing estimates of P (y|a) within the speech recognition application. [sent-59, score-0.418]
</p><p>33 Importantly, the model fails to exploit the underlying structure or correlations within the label space. [sent-60, score-0.18]
</p><p>34 For example, consider a test point that has many neighbors with the phonemic label /s/. [sent-61, score-0.21]
</p><p>35 As a second baseline, an alternative method for estimating P (y|a) using nearest neighbor information is the following:  Pk (y|a) =  # of k-nearest neighbors of a in training set with label y k  Here the choice of k is crucial. [sent-63, score-0.294]
</p><p>36 We will describe a baseline method that interpolates estimates from several different values of k. [sent-66, score-0.153]
</p><p>37 This baseline will be useful with our approach, but again suffers from the fact that it does not model the underlying structure of the label space. [sent-67, score-0.232]
</p><p>38 4  Error-Correcting Output Codes for Nearest-Neighbor Classiﬁers  We now describe a model that uses error correcting output codes to explicitly represent and learn the underlying structure of the label space Y. [sent-68, score-0.468]
</p><p>39 For each label y, we deﬁne My ∈ ℜL to be a prototype vector. [sent-69, score-0.216]
</p><p>40 We assume that the inner product My , Mz will in some sense represent the similarity between labels y and z. [sent-70, score-0.119]
</p><p>41 The vectors My will be learned automatically, effectively representing an embedding of the labels in ℜL . [sent-71, score-0.303]
</p><p>42 In this section we ﬁrst describe the structure of the model, and then describe a method for training the parameters of the model (i. [sent-72, score-0.117]
</p><p>43 When considering a test sample a, we ﬁrst assign weights α(j|a) to points aj from the training set through the NCA deﬁnition in Eq. [sent-77, score-0.122]
</p><p>44 Let M be a matrix that contains all the prototype vectors My as its rows. [sent-79, score-0.17]
</p><p>45 We can then construct a vector H(a; M) that uses the weights α(j|a) and the true labels of the training samples to calculate the expected value of the output code representing a. [sent-80, score-0.245]
</p><p>46 481  Table 1: Average CLL achieved by Pecoc over DevSet1 for different values of L This distribution assigns most of the probability for a sample vector a to classes whose prototype vectors have a large inner product with H(a; M). [sent-88, score-0.197]
</p><p>47 All labels receive a non-zero weight under Pecoc (y|a; M). [sent-89, score-0.119]
</p><p>48 The optimization problem will be to maximize the conditional log-likelihood function N (loo) log Pecoc (yi |ai ; M)  F (M) = i=1 (loo)  where Pecoc (yi |ai ; M) is a leave-one-out estimate of the probability of label yi given the input ai , assuming an ECOC matrix M. [sent-93, score-0.288]
</p><p>49 The corresponding perplexity values are indicated as well where the perplexity is deﬁned as e−x given that x is the average CLL. [sent-108, score-0.092]
</p><p>50 We select L = 40 as the length of the prototype vectors My . [sent-114, score-0.17]
</p><p>51 The average conditional log-likelihood achieved on a development set of approximately 115,000 samples (DevSet1) is listed in Table 1. [sent-116, score-0.093]
</p><p>52 5  Experiments on Log-Likelihood  We test our approach on a large-vocabulary lecture recognition task [6]. [sent-118, score-0.129]
</p><p>53 The acoustic vectors we use are 112 dimensional vectors consisting of eight concatenated 14 dimensional vectors of MFCC measurements. [sent-121, score-0.44]
</p><p>54 This section describes experiments comparing the ECOC model to several baseline models in terms of their performance on the conditional log-likelihood of sample acoustic vectors. [sent-123, score-0.44]
</p><p>55 The baseline model, Pnn , makes use of estimates Pk (y|a) as deﬁned in section 3. [sent-124, score-0.128]
</p><p>56 The set K is a set of integers representing different values for k, the number of nearest neighbors used to evaluate Pk . [sent-125, score-0.129]
</p><p>57 We have found these functions over the labels are useful within our speech recognition application. [sent-131, score-0.409]
</p><p>58 Table 2 contains the average conditional loglikelihood achieved on a development set (DevSet1) by Pnca , Pnn and Pecoc . [sent-138, score-0.093]
</p><p>59 These results show that Pecoc clearly outperforms these two baseline models. [sent-139, score-0.085]
</p><p>60 In a second experiment we combined Pecoc with Pnn to create a third model Pf ull (y|a). [sent-140, score-0.167]
</p><p>61 This model includes information from the nearest neighbors, the output codes, as well as the distributions over the label space. [sent-141, score-0.272]
</p><p>62 The model takes the following form: d  ¯ Pf ull (y|a; λ) =  λ0 Pj (y) + λecoc Pecoc (y|a; M) j  λk Pk (y|a) + k∈K  j=1  5  Acoustic Model Baseline Model Augmented Model  WER (DevSet3) 36. [sent-142, score-0.167]
</p><p>63 5  Table 3: WER of recognizer for different acoustic models on the development and test set. [sent-146, score-0.422]
</p><p>64 We also compare ECOC to a GMM model, as conventionally used in speech recognition systems. [sent-149, score-0.294]
</p><p>65 Table 2 shows that Pf ull and Pgmm are close in performance, with Pgmm giving slightly improved ¯ results. [sent-152, score-0.139]
</p><p>66 6  Recognition Experiments  In this section we describe experiments that integrate the ECOC model within a full speech recog¯ nition system. [sent-156, score-0.269]
</p><p>67 We learn parameters λ using both DevSet1 and DevSet2 for Pf ull (y|a). [sent-157, score-0.16]
</p><p>68 The estimates for P (y) are derived directly from the proportions of occurrences of each acoustic-phonetic class in the training set. [sent-160, score-0.104]
</p><p>69 In our experiments we consider the following two methods for calculating the acoustic model. [sent-161, score-0.281]
</p><p>70 • Baseline Model: β1 log Pgmm (a|y) • Augmented Model: β2 log  γPgmm (y|a)+(1−γ)Pf ull (y|a) P (y)  The baseline method is just a GMM model with the commonly used scaling parameter β1 . [sent-162, score-0.252]
</p><p>71 The augmented model combines Pgmm linearly with Pf ull using parameter γ and the log of the combination is scaled by parameter β2 . [sent-163, score-0.221]
</p><p>72 Our development set (DevSet3) consists of eight hours of data including six speakers and our test set consists of eight hours of data including ﬁve speakers. [sent-165, score-0.223]
</p><p>73 Results for both methods on the development set and test set are presented in Table 3. [sent-166, score-0.073]
</p><p>74 This indicates that the nearest neighbor information along with the ECOC embedding, can signiﬁcantly improve the acoustic model. [sent-168, score-0.412]
</p><p>75 9% on the test set are achieved using the augmented acoustic model. [sent-171, score-0.361]
</p><p>76 0  em ah aw el ow axr uh l rahf p er ae  eh  g d dx y w aw  uw ih ey  en ng m  n  iy  Figure 1: Plot of 2-dimensional output codes corresponding to 73 acoustic phonetic classes. [sent-176, score-0.716]
</p><p>77 The phonemic classes are divided as follows: vowels, semivowels, nasals, stops and stop closures, fricatives, affricates, and the aspirant /hh/. [sent-178, score-0.118]
</p><p>78 1  Discussion  Plot of a low-dimensional embedding  In order to get a sense of what is learned by the output codes of Pecoc we can plot the output codes directly. [sent-180, score-0.648]
</p><p>79 Figure 1 shows a plot of the output codes learned when L = 2. [sent-181, score-0.316]
</p><p>80 The output codes are learned for 1871 classes, but only 73 internal acoustic-phonetic classes are shown in the plot for clarity. [sent-182, score-0.363]
</p><p>81 We can see that items of similar acoustic categories are grouped closely together. [sent-184, score-0.308]
</p><p>82 For example, the vowels are close to each other in the bottom left quadrant, while the stop-closures are grouped together in the top right, the affricates in the top left, and the nasals in the bottom right. [sent-185, score-0.159]
</p><p>83 The fricatives are a little more spread out but usually grouped close to another fricative that shares some underlying phonological feature such as /sh/ and /zh/ which are both palatal and /f/ and /th/ which are both unvoiced. [sent-186, score-0.144]
</p><p>84 For example the voiced stops /b/, /d/, /g/ are placed close to other voiced items of different acoustic categories. [sent-188, score-0.404]
</p><p>85 2  Extensions  The ECOC embedding of the label space could also be co-learned with an embedding of the input acoustic vector space by extending the approach of NCA [8]. [sent-190, score-0.539]
</p><p>86 8  Conclusion  We have shown that nearest neighbor methods can be used to improve the performance of a GMMbased acoustic model and reduce the WER on a challenging speech recognition task. [sent-197, score-0.697]
</p><p>87 We have also developed a model for using error-correcting output codes to represent an embedding of the acoustic-phonetic label space that helps us capture cross-class information. [sent-198, score-0.448]
</p><p>88 Future work on this task could include co-learning an embedding of the input acoustic vector space with the ECOC matrix to attempt to achieve further gains. [sent-199, score-0.366]
</p><p>89 Appendix We deﬁne three distributions based on the prior probabilities, P (y), of the acoustic phonetic classes. [sent-200, score-0.336]
</p><p>90 The SUMMIT recognizer makes use of 1871 distinct acoustic phonetic labels [5]. [sent-201, score-0.523]
</p><p>91 • Y (1) includes labels involving internal phonemic events (e. [sent-203, score-0.239]
</p><p>92 /ay/) • Y (2) includes labels involving the transition from one acoustic-phonetic event to another (e. [sent-205, score-0.159]
</p><p>93 /ow/->/ch/) • Y (3) includes labels involving only non-phonetic events like noise and silence We deﬁne a distribution P (1) (y) as follows. [sent-207, score-0.191]
</p><p>94 On the learnability and design of output codes for multiclass problems. [sent-226, score-0.287]
</p><p>95 On nearest-neighbor error-correcting output codes with aplication to all-pairs multiclass support vector machines. [sent-271, score-0.287]
</p><p>96 Discriminant ecoc: a heuristic method for application dependent design of error correcting output codes. [sent-288, score-0.093]
</p><p>97 Learning a nonlinear embedding by preserving class neighbourhood structure. [sent-293, score-0.116]
</p><p>98 Dimensionality reduction for speech recognition using neighborhood components analysis. [sent-300, score-0.282]
</p><p>99 Distance metric learning for large margin nearest neighbor classiﬁcation. [sent-314, score-0.131]
</p><p>100 A hybrid segmental neural net/hidden markov model system for continuous speech recognition. [sent-322, score-0.231]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pecoc', 0.41), ('ecoc', 0.398), ('acoustic', 0.281), ('pgmm', 0.273), ('loo', 0.259), ('speech', 0.183), ('codes', 0.182), ('wer', 0.159), ('nca', 0.146), ('pnn', 0.139), ('ull', 0.139), ('cll', 0.137), ('prototype', 0.128), ('gmm', 0.123), ('labels', 0.119), ('pf', 0.113), ('ai', 0.098), ('myj', 0.091), ('pmix', 0.091), ('pnca', 0.091), ('label', 0.088), ('baseline', 0.085), ('embedding', 0.085), ('pk', 0.075), ('recognition', 0.074), ('nearest', 0.071), ('nn', 0.07), ('recognizer', 0.068), ('ecocs', 0.068), ('summit', 0.068), ('output', 0.065), ('neighbor', 0.06), ('phonemic', 0.06), ('aj', 0.057), ('pj', 0.055), ('phonetic', 0.055), ('augmented', 0.054), ('embeddings', 0.052), ('development', 0.047), ('perplexity', 0.046), ('conditional', 0.046), ('affricates', 0.046), ('mz', 0.046), ('nasals', 0.046), ('phonological', 0.046), ('voiced', 0.046), ('estimates', 0.043), ('vectors', 0.042), ('fricatives', 0.04), ('gmms', 0.04), ('vowels', 0.04), ('multiclass', 0.04), ('training', 0.039), ('conventionally', 0.037), ('neighbors', 0.036), ('yi', 0.035), ('learned', 0.035), ('plot', 0.034), ('ax', 0.034), ('eight', 0.033), ('within', 0.033), ('silence', 0.032), ('aw', 0.031), ('interpolated', 0.031), ('stops', 0.031), ('neighbourhood', 0.031), ('criterion', 0.031), ('underlying', 0.031), ('em', 0.031), ('improvements', 0.03), ('lecture', 0.029), ('crammer', 0.028), ('correcting', 0.028), ('classi', 0.028), ('model', 0.028), ('grouped', 0.027), ('classes', 0.027), ('collins', 0.026), ('test', 0.026), ('describe', 0.025), ('neighborhood', 0.025), ('optimize', 0.025), ('audio', 0.025), ('projection', 0.024), ('representing', 0.022), ('cant', 0.022), ('proportions', 0.022), ('hours', 0.021), ('learn', 0.021), ('estimate', 0.021), ('weiss', 0.021), ('consists', 0.021), ('involving', 0.02), ('internal', 0.02), ('includes', 0.02), ('teukolsky', 0.02), ('segmental', 0.02), ('closures', 0.02), ('iy', 0.02), ('ah', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="127-tfidf-1" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>Author: Natasha Singh-miller, Michael Collins</p><p>Abstract: We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. 1</p><p>2 0.087748848 <a title="127-tfidf-2" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>Author: John Langford, Tong Zhang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting. 1</p><p>3 0.072661772 <a title="127-tfidf-3" href="./nips-2009-Entropic_Graph_Regularization_in_Non-Parametric_Semi-Supervised_Classification.html">82 nips-2009-Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification</a></p>
<p>Author: Amarnag Subramanya, Jeff A. Bilmes</p><p>Abstract: We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to outperform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph. 1</p><p>4 0.064435266 <a title="127-tfidf-4" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>Author: Zahi Karam, Douglas Sturim, William M. Campbell</p><p>Abstract: Speaker comparison, the process of ﬁnding the speaker similarity between two speech signals, occupies a central role in a variety of applications—speaker veriﬁcation, clustering, and identiﬁcation. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process. For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM). Speaker comparison can then be viewed as the process of compensating and ﬁnding metrics on the space of adapted models. We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison—support vector machines, joint factor analysis, and linear scoring. The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods. Compensation of nuisances is performed via linear transforms on GMM parameter vectors. Using the IPDF framework, we show that many current techniques are simple variations of each other. We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require signiﬁcantly less computation than current techniques.</p><p>5 0.064429447 <a title="127-tfidf-5" href="./nips-2009-Rank-Approximate_Nearest_Neighbor_Search%3A_Retaining_Meaning_and_Speed_in_High_Dimensions.html">198 nips-2009-Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions</a></p>
<p>Author: Parikshit Ram, Dongryeol Lee, Hua Ouyang, Alexander G. Gray</p><p>Abstract: The long-standing problem of efďŹ cient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 ďŹ ngerprinting to bioinformatics to movie recommendations. As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1+đ?&oelig;&ndash;) distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the ďŹ rst time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN. Experiments on high-dimensional datasets show that our algorithm often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior. 1</p><p>6 0.064061493 <a title="127-tfidf-6" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>7 0.063419238 <a title="127-tfidf-7" href="./nips-2009-Label_Selection_on_Graphs.html">122 nips-2009-Label Selection on Graphs</a></p>
<p>8 0.058187027 <a title="127-tfidf-8" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>9 0.056316663 <a title="127-tfidf-9" href="./nips-2009-Distribution-Calibrated_Hierarchical_Classification.html">71 nips-2009-Distribution-Calibrated Hierarchical Classification</a></p>
<p>10 0.05582073 <a title="127-tfidf-10" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>11 0.055557694 <a title="127-tfidf-11" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>12 0.054924663 <a title="127-tfidf-12" href="./nips-2009-Locality-sensitive_binary_codes_from_shift-invariant_kernels.html">142 nips-2009-Locality-sensitive binary codes from shift-invariant kernels</a></p>
<p>13 0.054162119 <a title="127-tfidf-13" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>14 0.053963684 <a title="127-tfidf-14" href="./nips-2009-Hierarchical_Learning_of_Dimensional_Biases_in_Human_Categorization.html">109 nips-2009-Hierarchical Learning of Dimensional Biases in Human Categorization</a></p>
<p>15 0.046991296 <a title="127-tfidf-15" href="./nips-2009-Semi-Supervised_Learning_in_Gigantic_Image_Collections.html">212 nips-2009-Semi-Supervised Learning in Gigantic Image Collections</a></p>
<p>16 0.045804884 <a title="127-tfidf-16" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>17 0.044103984 <a title="127-tfidf-17" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>18 0.042416401 <a title="127-tfidf-18" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>19 0.042330291 <a title="127-tfidf-19" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>20 0.041831031 <a title="127-tfidf-20" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.005), (2, -0.021), (3, 0.015), (4, -0.039), (5, 0.015), (6, -0.054), (7, 0.029), (8, -0.044), (9, 0.065), (10, -0.012), (11, 0.048), (12, -0.007), (13, -0.027), (14, 0.018), (15, -0.013), (16, 0.154), (17, 0.015), (18, -0.093), (19, -0.047), (20, 0.046), (21, -0.037), (22, -0.04), (23, 0.093), (24, -0.013), (25, 0.025), (26, -0.007), (27, 0.032), (28, -0.035), (29, -0.015), (30, 0.017), (31, 0.005), (32, -0.006), (33, 0.116), (34, -0.033), (35, 0.045), (36, -0.01), (37, 0.125), (38, -0.012), (39, 0.038), (40, -0.057), (41, -0.05), (42, 0.125), (43, 0.017), (44, -0.117), (45, -0.071), (46, 0.083), (47, -0.037), (48, -0.026), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90107042 <a title="127-lsi-1" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>Author: Natasha Singh-miller, Michael Collins</p><p>Abstract: We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. 1</p><p>2 0.56817871 <a title="127-lsi-2" href="./nips-2009-Multi-Label_Prediction_via_Compressed_Sensing.html">157 nips-2009-Multi-Label Prediction via Compressed Sensing</a></p>
<p>Author: John Langford, Tong Zhang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting. 1</p><p>3 0.55747622 <a title="127-lsi-3" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>Author: Nan Ye, Wee S. Lee, Hai L. Chieu, Dan Wu</p><p>Abstract: Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efﬁcient inference algorithms for a conditional random ﬁeld using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences used in the features is small. This leads to efﬁcient learning algorithms for these conditional random ﬁelds. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective. 1</p><p>4 0.55312681 <a title="127-lsi-4" href="./nips-2009-Entropic_Graph_Regularization_in_Non-Parametric_Semi-Supervised_Classification.html">82 nips-2009-Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification</a></p>
<p>Author: Amarnag Subramanya, Jeff A. Bilmes</p><p>Abstract: We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to outperform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph. 1</p><p>5 0.54041052 <a title="127-lsi-5" href="./nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></p>
<p>Author: Yang Wang, Gholamreza Haffari, Shaojun Wang, Greg Mori</p><p>Abstract: We propose a novel information theoretic approach for semi-supervised learning of conditional random ﬁelds that deﬁnes a training objective to combine the conditional likelihood on labeled data and the mutual information on unlabeled data. In contrast to previous minimum conditional entropy semi-supervised discriminative learning methods, our approach is grounded on a more solid foundation, the rate distortion theory in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label conﬁgurations. Our experimental results show the rate distortion approach outperforms standard l2 regularization, minimum conditional entropy regularization as well as maximum conditional entropy regularization on both multi-class classiﬁcation and sequence labeling problems. 1</p><p>6 0.53659034 <a title="127-lsi-6" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>7 0.52191192 <a title="127-lsi-7" href="./nips-2009-Breaking_Boundaries_Between_Induction_Time_and_Diagnosis_Time_Active_Information_Acquisition.html">49 nips-2009-Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition</a></p>
<p>8 0.51183385 <a title="127-lsi-8" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>9 0.48187047 <a title="127-lsi-9" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>10 0.46660694 <a title="127-lsi-10" href="./nips-2009-Label_Selection_on_Graphs.html">122 nips-2009-Label Selection on Graphs</a></p>
<p>11 0.46540496 <a title="127-lsi-11" href="./nips-2009-Heavy-Tailed_Symmetric_Stochastic_Neighbor_Embedding.html">106 nips-2009-Heavy-Tailed Symmetric Stochastic Neighbor Embedding</a></p>
<p>12 0.43829092 <a title="127-lsi-12" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>13 0.43499455 <a title="127-lsi-13" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>14 0.42386702 <a title="127-lsi-14" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>15 0.42227456 <a title="127-lsi-15" href="./nips-2009-Distribution-Calibrated_Hierarchical_Classification.html">71 nips-2009-Distribution-Calibrated Hierarchical Classification</a></p>
<p>16 0.41400623 <a title="127-lsi-16" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>17 0.4069939 <a title="127-lsi-17" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>18 0.40645874 <a title="127-lsi-18" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>19 0.40169427 <a title="127-lsi-19" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>20 0.40129846 <a title="127-lsi-20" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.381), (24, 0.036), (25, 0.065), (35, 0.046), (36, 0.082), (39, 0.054), (58, 0.052), (71, 0.058), (81, 0.016), (86, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70312309 <a title="127-lda-1" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>Author: Natasha Singh-miller, Michael Collins</p><p>Abstract: We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. 1</p><p>2 0.55919069 <a title="127-lda-2" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>Author: Youngmin Cho, Lawrence K. Saul</p><p>Abstract: We introduce a new family of positive-deﬁnite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets. 1</p><p>3 0.46319997 <a title="127-lda-3" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>Author: Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</p><p>Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce signiﬁcantly better separation results as compared to similar systems based on compact statistical models. Keywords: Example-Based Representation, Signal Separation, Sparse Models. 1</p><p>4 0.41732281 <a title="127-lda-4" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>Author: Xiaojin Zhu, Bryan R. Gibson, Timothy T. Rogers</p><p>Abstract: We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity. Rademacher complexity measures a learner’s ability to ﬁt random labels, and can be used to bound the learner’s true error based on the observed training sample error. We ﬁrst review the deﬁnition of Rademacher complexity and its generalization bound. We then describe a “learning the noise” procedure to experimentally measure human Rademacher complexities. The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overﬁtting in human learning. Finally, we discuss the potential applications of human Rademacher complexity in cognitive science. 1</p><p>5 0.41621962 <a title="127-lda-5" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>Author: Ruslan Salakhutdinov</p><p>Abstract: Markov random ﬁelds (MRF’s), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF’s is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF’s. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.</p><p>6 0.41551539 <a title="127-lda-6" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>7 0.41497773 <a title="127-lda-7" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>8 0.41454929 <a title="127-lda-8" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>9 0.41396517 <a title="127-lda-9" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>10 0.41326272 <a title="127-lda-10" href="./nips-2009-Quantification_and_the_language_of_thought.html">196 nips-2009-Quantification and the language of thought</a></p>
<p>11 0.4128674 <a title="127-lda-11" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>12 0.41251799 <a title="127-lda-12" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>13 0.41208076 <a title="127-lda-13" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>14 0.41097927 <a title="127-lda-14" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>15 0.4109723 <a title="127-lda-15" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>16 0.41085786 <a title="127-lda-16" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>17 0.41024444 <a title="127-lda-17" href="./nips-2009-Semi-Supervised_Learning_in_Gigantic_Image_Collections.html">212 nips-2009-Semi-Supervised Learning in Gigantic Image Collections</a></p>
<p>18 0.41002935 <a title="127-lda-18" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>19 0.40998095 <a title="127-lda-19" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>20 0.40993512 <a title="127-lda-20" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
