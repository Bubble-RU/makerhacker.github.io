<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-107" href="#">nips2009-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</h1>
<br/><p>Source: <a title="nips-2009-107-pdf" href="http://papers.nips.cc/paper/3747-help-or-hinder-bayesian-models-of-social-goal-inference.pdf">pdf</a></p><p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>Reference: <a title="nips-2009-107-reference" href="../nips2009_reference/nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e. [sent-7, score-1.339]
</p><p>2 We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). [sent-10, score-1.095]
</p><p>3 The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. [sent-11, score-1.0]
</p><p>4 1  Introduction  Humans make rapid, consistent intuitive inferences about the goals of agents from the most impoverished of visual stimuli. [sent-13, score-0.956]
</p><p>5 On viewing a short video of geometric shapes moving in a 2D world, adults spontaneously attribute to them an array of goals and intentions [7]. [sent-14, score-0.495]
</p><p>6 Yet people also attribute complex social goals, such as helping, hindering or protecting another agent. [sent-18, score-0.592]
</p><p>7 Recent studies suggest that infants as young as six months make the same sort of complex social goal attributions on observing simple displays of moving shapes, or (at older ages) in displays of puppets interacting [6]. [sent-19, score-0.583]
</p><p>8 How do humans make these rapid social goal inferences from such impoverished displays? [sent-20, score-0.492]
</p><p>9 On one approach, social goals are inferred directly from perceptual cues in a bottom-up fashion. [sent-21, score-0.65]
</p><p>10 For example, infants in [6] may judge that a triangle pushing a circle up a hill is helping the circle get to the top of the hill simply because the circle is moving the triangle in the direction the triangle was last observed moving on its own. [sent-22, score-0.706]
</p><p>11 On this approach, the triangle is judged to be helping the circle because in some sense he knows what the circle’s goal is, desires for the circle to achieve the goal, constructs a rational plan of action that he expects will increase the probability of the circle realizing the goal. [sent-27, score-0.603]
</p><p>12 The virtue of this theoryof-mind approach is its generality, accounting for a much wider range of social goal inferences that cannot be reduced to simple perceptual cues. [sent-28, score-0.438]
</p><p>13 Our question here is whether the rapid goal inferences we make in everyday social situations, and that both infants and adults have been shown to make from simple perceptual displays, require the sophistication of a theory-based approach or can be sufﬁciently explained in terms of perceptual cues. [sent-29, score-0.576]
</p><p>14 This framework should enable the inference that agent A is helping or hindering agent B from a joint goal inference based on observing A and B interacting. [sent-35, score-1.404]
</p><p>15 Inference should be possible even with minimal prior knowledge about the agents and without knowledge of B’s goal. [sent-36, score-0.456]
</p><p>16 beliefs, goals, planning abilities) explain fast human judgments from impoverished and unfamiliar stimuli? [sent-40, score-0.482]
</p><p>17 In addressing the challenge of formalization, we present a formal account of social goal attribution based on the abstract criterion of A helping (or hindering) B by acting to maximize (minimize) Bs probability of realizing his goals. [sent-41, score-0.63]
</p><p>18 On this account, agent A rationally maximizes utility by maximizing (minimizing) the expected utility of B, where this expectation comes from As model of Bs goals and plans of action. [sent-42, score-0.8]
</p><p>19 We incorporate this formalization of helping and hindering into an existing computational framework for theory-based goal inference, on which goals are inferred from actions by inverting a generative rational planning (MDP) model [1]. [sent-43, score-1.408]
</p><p>20 The augmented model allows for the inference that A is helping or hindering B from stimuli in which B’s goal is not directly observable. [sent-44, score-0.741]
</p><p>21 We test this Inverse Planning model of social goal attribution on a set of simple 2D displays, comparing its performance to that of an alternative model which makes inferences directly from visual cues, based on previous work such as that of Blythe et al. [sent-45, score-0.544]
</p><p>22 2  Computational Framework  Our framework assumes that people represent the causal role of agents’ goals in terms of an intuitive principle of rationality [4]: the assumption that agents will tend to take efﬁcient actions to achieve their goals, given their beliefs about the world. [sent-47, score-0.929]
</p><p>23 Inferences of simple relational goals between agents (such as chasing and ﬂeeing) from maze-world interactions were considered by Baker, Goodman and Tenenbaum [1], using multiagent MDP-based inverse planning. [sent-49, score-1.034]
</p><p>24 In this paper, we present a framework for modeling inferences of more complex social goals, such as helping and hindering, where an agent’s goals depend on the goals of other agents. [sent-50, score-1.275]
</p><p>25 We will deﬁne two types of agents: simple agents, which have object-directed goals and do not represent other agents’ goals, and complex agents, which have either social or object-directed goals, and represent other agents’ goals and reason about their likely behavior. [sent-51, score-0.929]
</p><p>26 For each type of agent and goal, we describe the multiagent MDPs they deﬁne. [sent-52, score-0.475]
</p><p>27 We then describe joint inferences of objectdirected and social goals based on the Bayesian inversion of MDP models of behavior. [sent-53, score-0.648]
</p><p>28 S is an encoding of the world into a ﬁnite set of mutually exclusive states, which speciﬁes the set of possible conﬁgurations of all agents and objects. [sent-56, score-0.456]
</p><p>29 R : S × A → R is the reward function, which provides agents with realvalued rewards for each state-action pair, and γ is the discount factor. [sent-60, score-0.646]
</p><p>30 We then describe how agents plan over multiagent MDPs. [sent-62, score-0.554]
</p><p>31 The state reward functions range from a unit reward in the goal location (row 1) to a ﬁeld of reward that extends to every location in the grid (row 3). [sent-85, score-0.578]
</p><p>32 Speciﬁcally, ρg and δg determine the scale and shape of the state reward function, with ri (S) = max(ρg (1 − distance(S, i, G)/δg ), 0), where distance(S, i, G) is the geodesic distance between agent i and the goal. [sent-92, score-0.583]
</p><p>33 With δg ≤ 1, the reward function has a unit value of r(S) = ρg when the agent and object goal occupy the same location, i. [sent-93, score-0.676]
</p><p>34 Social rewards for helping and hindering For complex agent j, the state reward function induced by a social goal Gj depends on the cost of j’s action Aj , as well as the reward function Ri of the agent that j wants to help or hinder. [sent-100, score-2.05]
</p><p>35 ρo is the social agent’s scaling of the expected reward of state S for agent i, which determines how much j “cares” about i relative to its own costs. [sent-102, score-0.744]
</p><p>36 For helping agents, ρo > 0, and for hindering agents, ρo < 0. [sent-103, score-0.552]
</p><p>37 Computing the expectation EAi [Ri (S, Ai )] relies on the social agent’s model of i’s planning process, which we will describe below. [sent-104, score-0.495]
</p><p>38 Simple agents We assume that the simple agents model other agents as randomly selecting actions in proportion to the softmax of their expected cost, i. [sent-110, score-1.453]
</p><p>39 Complex agents We assume that the social agent j uses its model of other agents’ planning process to compute P (Ai |S, Gi ), for i = j, allowing for accurate prediction of other agents’ actions. [sent-113, score-1.328]
</p><p>40 We assume agents have access to the true environment dynamics. [sent-114, score-0.475]
</p><p>41 This is a simpliﬁcation of a more realistic framework in which agents have only partial or false knowledge about the environment. [sent-115, score-0.456]
</p><p>42 3  Multiagent planning  Given the variables of MDP M , we can compute the optimal state-action value function Q∗ : S ×A → R, which determines the expected inﬁnite-horizon reward of taking an action in each state. [sent-118, score-0.489]
</p><p>43 We assume that agents have softmax-optimal policies, such that P (A|S, G) ∝ exp(βQ∗ (S, A)), allowing occasional deviations from the optimal action depending on the parameter β, which determines agents’ level of determinism (higher β implies higher determinism, or less randomness). [sent-119, score-0.545]
</p><p>44 In a multiagent setting, joint value functions can be optimized recursively, with one agent representing the value function of the other, and the other representing the representation of the ﬁrst, and so on to an arbitrarily high order [10]. [sent-120, score-0.475]
</p><p>45 That is, an agent A can at most represent an agent B’s reasoning about A’s goals and actions, but not a deeper recursion in which B reasons about A reasoning about B. [sent-122, score-1.103]
</p><p>46 2  Inverse planning in multiagent MDPs  Once we have computed P (Ai |S, Gi ) for agents 1 through n using multiagent planning, we use Bayesian inverse planning to infer agents’ goals, given observations of their behavior. [sent-124, score-1.289]
</p><p>47 1 over a range of θ values for each stimulus trial: P (Gi |S1:T , A1:n −1 , β) = 1:T  P (Gi , θ|S1:T , A1:n −1 , β) 1:T  (2)  θ  This allows our models to infer the combination of goals and reward functions that best explains the agents’ behavior for each stimulus. [sent-129, score-0.509]
</p><p>48 3  Experiment  We designed an experiment to test the Inverse Planning model of social goal attributions in a simple 2D maze-world domain, inspired by the stimuli of many previous studies involving children and adults [7, 5, 8, 6, 9, 12]. [sent-130, score-0.487]
</p><p>49 We created a set of videos which depicted agents interacting in a maze. [sent-131, score-0.496]
</p><p>50 Subjects were asked to attribute goals to the agents after viewing brief snippets of these videos. [sent-133, score-0.876]
</p><p>51 2  Stimuli  We constructed 24 scenarios in which two agents moved around a 2D maze (shown in Fig. [sent-140, score-0.579]
</p><p>52 The maze always contained two potential object goals (a ﬂower and a tree), and on 12 of the 24 scenarios it also contained a movable obstacle (a boulder). [sent-142, score-0.53]
</p><p>53 First, scenarios were to have agents acting in ways that were consistent with more than one hypothesis concerning their goals, with these ambiguities between goals sometimes being resolved as the scenario developed (see Fig. [sent-144, score-0.92]
</p><p>54 Second, scenarios were to involve a variety of 4  (a)  Scenario 6  perceptually distinct plans of action that might be interpreted as issuing from helping or hindering goals. [sent-147, score-0.702]
</p><p>55 For example, one agent pushing another toward an object goal, removing an obstacle from the other agent’s path, and moving aside for the other agent (all of which featured in our scenarios) could all be interpreted as helping. [sent-148, score-0.888]
</p><p>56 This criterion was included to test our formalization of social goals as based on an abstract relation between reward functions. [sent-149, score-0.752]
</p><p>57 In our model, social agents act to maximize or minimize the reward of the other agent, and the precise manner in which they do so will vary depending on the structure of the environment and their initial positions. [sent-150, score-0.842]
</p><p>58 (a) The Large agent moves over each of the goal objects (Frames 1-7) and so the video is initially ambiguous between his having an object goal and a social goal. [sent-154, score-0.871]
</p><p>59 Disambiguation occurs from Frame 8, when the Large agent moves down and blocks the Small agent from continuing his path up to the object goal. [sent-155, score-0.821]
</p><p>60 Once the Small agent moves into the same room (6), the Large agent pushes him onto the ﬂower and allows him to rest there (8-16). [sent-157, score-0.834]
</p><p>61 Large agents were visually bigger and are able to shift both movable obstacles and Small agents by moving directly into them. [sent-159, score-1.015]
</p><p>62 Small agents were visually smaller, and could not shift agents or boulders. [sent-163, score-0.912]
</p><p>63 In our scenarios, the actions of Small agents failed with a probability of about 0. [sent-164, score-0.517]
</p><p>64 Large agents correspond to the “complex agents” introduced in Section 2, in that they could have either object-directed goals or social goals (helping or hindering the Small agent). [sent-166, score-1.659]
</p><p>65 Small agents correspond to “simple agents” and could have only object goals. [sent-167, score-0.497]
</p><p>66 Asking subjects for goal attributions at multiple points in a sequence allowed us to track the change in their judgments as evidence for particular goals accumulated. [sent-173, score-0.797]
</p><p>67 3  Procedure  Subjects were initially shown a set of familiarization videos of agents interacting in the maze, illustrating the structural properties of the maze-world e. [sent-177, score-0.496]
</p><p>68 the actions available to agents and the possibility of moving obstacles) and the differences between Small and Large agents. [sent-179, score-0.565]
</p><p>69 The left-right orientation of agents and goals was counterbalanced across subjects. [sent-182, score-0.805]
</p><p>70 Subjects were told that each snippet would contain two new agents (one Small and one Large) and this was highlighted in the stimuli by randomly varying the color of the agents for each snippet. [sent-183, score-1.0]
</p><p>71 Subjects were told that agents had complete knowledge of the physical structure of the maze, including the position of all goals, agents and obstacles. [sent-184, score-0.912]
</p><p>72 For the Large agent, they could select either of the two social goals and either of the two object goals. [sent-186, score-0.597]
</p><p>73 In our experiments, the world was given by a 2D maze-world, and the state space included the set of positions that agents and objects can jointly occupy without overlapping. [sent-192, score-0.456]
</p><p>74 For instance, some stimuli were suggestive of “ﬁeld” goals rather than point goals, and marginalizing over δg allowed our model to capture this. [sent-198, score-0.44]
</p><p>75 We compared the Inverse Planning model to a model that made inferences about goals based on simple visual cues, inspired by previous heuristic- or perceptually-based accounts of human action understanding of similar 2D animated displays [3, 11]. [sent-208, score-0.664]
</p><p>76 5  Results  Because our main interest is in judgments about the social goals of representationally complex agents, we analzyed only subjects’ judgments about the Large agents. [sent-214, score-0.874]
</p><p>77 The Cue-based model correlates well with judgments for object goals (r = 0. [sent-228, score-0.588]
</p><p>78 90 for ﬂower, tree) – indeed slightly better the Inverse Planning model – but much less well for social goals (r = 0. [sent-230, score-0.58]
</p><p>79 There are many stimuli for which people are very conﬁdent that the Large agent is either helping or hindering, and the Inverse Planning model is similarly conﬁdent (bar heights near 1). [sent-235, score-0.766]
</p><p>80 The Cue-based model, in contrast, is unsure: it assigns roughly equal probabilities of helping or hindering to these cases (bar heights near 0. [sent-236, score-0.552]
</p><p>81 In other words, the Cue-based model is effective at inferring simple object goals of maze-world agents, but 6  is generally unable to distinguish between the more complex goals of helping and hindering. [sent-238, score-1.041]
</p><p>82 When constrained to simply differentiating between social and object goals both models succeed equally (r = 0. [sent-239, score-0.597]
</p><p>83 84), where in the Cue-based model this is probably because moving away from the object goals serves as a good cue to separate these categories. [sent-240, score-0.506]
</p><p>84 However, the Inverse Planning model is more successful in differentiating the right goal within social goals (r = 0. [sent-241, score-0.678]
</p><p>85 Note that in the one scenario for which humans and the Inverse Planning model disagreed after observing the full sequence, both humans and the model were close to being ambivalent whether the Large agent was hindering or interested in the ﬂower. [sent-250, score-0.862]
</p><p>86 We divided scenarios into two groups depending on whether a boulder was moved around in the scenario, as movable boulders increase the range of variability in helping and hindering action sequences. [sent-252, score-0.801]
</p><p>87 In contrast, the Inverse Planning model captures abstract relations between the agents and their possible goal and so lends itself to a variety of environments. [sent-258, score-0.578]
</p><p>88 975  Human judgments  Human judgments  Figure 3: Correlations between human goal judgments and predictions of the Inverse Planning model (a) and the Cue-based model (b), broken down by goal type. [sent-355, score-0.758]
</p><p>89 The inability of the heuristic model to distinguish between helping and hindering is illustrated by the plots in Fig. [sent-359, score-0.576]
</p><p>90 In contrast, both the Inverse Planning model and the human subjects are often very conﬁdent that an agent is helping and not hindering (or vice versa). [sent-361, score-1.088]
</p><p>91 2(a) but with goals switched), both the Inverse Planning model and humans subjects recognize the movement of the Large agent one step off the ﬂower (or the tree in Fig. [sent-371, score-0.939]
</p><p>92 In scenario 5, both agents start off in the bottom-left room, but with the Small agent right at the entrance to the top-left room. [sent-376, score-0.892]
</p><p>93 As the Small agent tries to move towards the ﬂower (the top-left goal), the Large agent moves up from below and pushes Small one step towards the ﬂower before moving off to the right to the tree. [sent-377, score-0.852]
</p><p>94 The ﬁrst was to provide a formalization of social goal attribution incorporated into a general theory-based model for goal attribution. [sent-404, score-0.534]
</p><p>95 This model had to enable the inference that A is helping or hindering B from interactions between A and B but without prior knowledge of either agent’s goal, and to account for the range of behaviors that humans judge as evidence of helping or hindering. [sent-405, score-0.942]
</p><p>96 The second challenge was for the model to perform well on a demanding inference task in which social goals must be inferred from very few observations without directly observable evidence of agents’ goals. [sent-406, score-0.608]
</p><p>97 The Inverse Planning model classiﬁed a diverse range of agent interactions as helping or hindering in line with human judgments. [sent-408, score-1.015]
</p><p>98 It produced a closer ﬁt to humans for both social and nonsocial goal attributions, and was far superior to the visual cue model in discriminating between helping and hindering. [sent-410, score-0.695]
</p><p>99 One task is to augment this formal model of helping and hindering to capture more of the complexity behind human judgments. [sent-412, score-0.616]
</p><p>100 This aspect of helping could be explored by supposing that the utility of a helping agent depends not just on another agent’s reward function but also his value function. [sent-415, score-1.045]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agents', 0.456), ('agent', 0.377), ('goals', 0.349), ('hindering', 0.298), ('planning', 0.264), ('helping', 0.254), ('social', 0.207), ('reward', 0.16), ('judgments', 0.147), ('inverse', 0.109), ('frame', 0.105), ('multiagent', 0.098), ('goal', 0.098), ('subjects', 0.095), ('inferences', 0.092), ('ower', 0.089), ('boulder', 0.073), ('attribution', 0.071), ('st', 0.068), ('stimuli', 0.067), ('action', 0.065), ('probe', 0.063), ('actions', 0.061), ('attributions', 0.06), ('scenario', 0.059), ('scenarios', 0.056), ('aj', 0.054), ('cues', 0.053), ('snippets', 0.052), ('moving', 0.048), ('maze', 0.048), ('circle', 0.046), ('people', 0.044), ('cue', 0.044), ('displays', 0.042), ('hinder', 0.042), ('infants', 0.042), ('object', 0.041), ('perceptual', 0.041), ('mdps', 0.041), ('human', 0.04), ('humans', 0.04), ('frames', 0.038), ('formalization', 0.036), ('gi', 0.036), ('blythe', 0.036), ('intentional', 0.036), ('movable', 0.036), ('predictions', 0.033), ('baker', 0.032), ('impoverished', 0.031), ('adults', 0.031), ('movement', 0.031), ('rewards', 0.03), ('room', 0.03), ('plans', 0.029), ('hill', 0.029), ('visual', 0.028), ('evidence', 0.028), ('correlates', 0.027), ('distance', 0.027), ('ai', 0.026), ('mdp', 0.026), ('moves', 0.026), ('rapid', 0.024), ('rational', 0.024), ('triangle', 0.024), ('model', 0.024), ('video', 0.024), ('caring', 0.024), ('determinism', 0.024), ('eai', 0.024), ('intentions', 0.024), ('lef', 0.024), ('owain', 0.024), ('pushes', 0.024), ('pushing', 0.024), ('wynn', 0.024), ('complex', 0.024), ('tree', 0.023), ('interactions', 0.022), ('stay', 0.022), ('judge', 0.022), ('goodman', 0.021), ('featured', 0.021), ('karen', 0.021), ('rationally', 0.021), ('snippet', 0.021), ('dent', 0.02), ('interacting', 0.02), ('videos', 0.02), ('points', 0.02), ('attribute', 0.019), ('geodesic', 0.019), ('flower', 0.019), ('rationality', 0.019), ('todd', 0.019), ('obstacles', 0.019), ('environment', 0.019), ('moved', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="107-tfidf-1" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>2 0.3727735 <a title="107-tfidf-2" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>3 0.31432995 <a title="107-tfidf-3" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>Author: Finale Doshi-velez</p><p>Abstract: The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difﬁcult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We deﬁne an inﬁnite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems. 1</p><p>4 0.17761028 <a title="107-tfidf-4" href="./nips-2009-Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining.html">218 nips-2009-Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></p>
<p>Author: George Konidaris, Andre S. Barreto</p><p>Abstract: We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance beneﬁts in a challenging continuous domain. 1</p><p>5 0.095748879 <a title="107-tfidf-5" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><p>6 0.093707323 <a title="107-tfidf-6" href="./nips-2009-Learning_to_Explore_and_Exploit_in_POMDPs.html">134 nips-2009-Learning to Explore and Exploit in POMDPs</a></p>
<p>7 0.091430642 <a title="107-tfidf-7" href="./nips-2009-Solving_Stochastic_Games.html">221 nips-2009-Solving Stochastic Games</a></p>
<p>8 0.082856216 <a title="107-tfidf-8" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>9 0.071142592 <a title="107-tfidf-9" href="./nips-2009-Multi-Step_Dyna_Planning_for_Policy_Evaluation_and_Control.html">159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</a></p>
<p>10 0.067536354 <a title="107-tfidf-10" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>11 0.062035002 <a title="107-tfidf-11" href="./nips-2009-Hierarchical_Learning_of_Dimensional_Biases_in_Human_Categorization.html">109 nips-2009-Hierarchical Learning of Dimensional Biases in Human Categorization</a></p>
<p>12 0.060319442 <a title="107-tfidf-12" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>13 0.056878578 <a title="107-tfidf-13" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>14 0.054716442 <a title="107-tfidf-14" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>15 0.053295147 <a title="107-tfidf-15" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>16 0.053164158 <a title="107-tfidf-16" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>17 0.051840086 <a title="107-tfidf-17" href="./nips-2009-A_Parameter-free_Hedging_Algorithm.html">14 nips-2009-A Parameter-free Hedging Algorithm</a></p>
<p>18 0.04949566 <a title="107-tfidf-18" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>19 0.047818858 <a title="107-tfidf-19" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>20 0.045285095 <a title="107-tfidf-20" href="./nips-2009-A_Generalized_Natural_Actor-Critic_Algorithm.html">12 nips-2009-A Generalized Natural Actor-Critic Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.128), (1, -0.063), (2, 0.195), (3, -0.242), (4, -0.293), (5, 0.054), (6, 0.037), (7, -0.009), (8, -0.023), (9, 0.041), (10, 0.16), (11, 0.066), (12, 0.092), (13, -0.025), (14, 0.037), (15, 0.051), (16, -0.027), (17, 0.095), (18, -0.021), (19, 0.133), (20, -0.063), (21, 0.069), (22, -0.216), (23, -0.009), (24, 0.226), (25, -0.063), (26, 0.137), (27, -0.039), (28, -0.053), (29, -0.097), (30, 0.039), (31, -0.039), (32, 0.034), (33, 0.045), (34, 0.099), (35, -0.001), (36, -0.029), (37, -0.068), (38, 0.105), (39, 0.004), (40, -0.069), (41, -0.016), (42, -0.076), (43, 0.028), (44, 0.024), (45, 0.038), (46, -0.016), (47, 0.026), (48, -0.032), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98137265 <a title="107-lsi-1" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>2 0.87395197 <a title="107-lsi-2" href="./nips-2009-Complexity_of_Decentralized_Control%3A_Special_Cases.html">53 nips-2009-Complexity of Decentralized Control: Special Cases</a></p>
<p>Author: Martin Allen, Shlomo Zilberstein</p><p>Abstract: The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case. 1</p><p>3 0.82294315 <a title="107-lsi-3" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>Author: Finale Doshi-velez</p><p>Abstract: The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difﬁcult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We deﬁne an inﬁnite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems. 1</p><p>4 0.7968424 <a title="107-lsi-4" href="./nips-2009-Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining.html">218 nips-2009-Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></p>
<p>Author: George Konidaris, Andre S. Barreto</p><p>Abstract: We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance beneﬁts in a challenging continuous domain. 1</p><p>5 0.64081883 <a title="107-lsi-5" href="./nips-2009-Learning_to_Explore_and_Exploit_in_POMDPs.html">134 nips-2009-Learning to Explore and Exploit in POMDPs</a></p>
<p>Author: Chenghui Cai, Xuejun Liao, Lawrence Carin</p><p>Abstract: A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the speciﬁc problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.</p><p>6 0.35105312 <a title="107-lsi-6" href="./nips-2009-Bayesian_Belief_Polarization.html">39 nips-2009-Bayesian Belief Polarization</a></p>
<p>7 0.3134785 <a title="107-lsi-7" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>8 0.30278644 <a title="107-lsi-8" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>9 0.28877917 <a title="107-lsi-9" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>10 0.27020028 <a title="107-lsi-10" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>11 0.26642561 <a title="107-lsi-11" href="./nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">52 nips-2009-Code-specific policy gradient rules for spiking neurons</a></p>
<p>12 0.26610318 <a title="107-lsi-12" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>13 0.2631892 <a title="107-lsi-13" href="./nips-2009-Maximum_likelihood_trajectories_for_continuous-time_Markov_chains.html">150 nips-2009-Maximum likelihood trajectories for continuous-time Markov chains</a></p>
<p>14 0.25448167 <a title="107-lsi-14" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>15 0.25206053 <a title="107-lsi-15" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>16 0.23346271 <a title="107-lsi-16" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>17 0.23216677 <a title="107-lsi-17" href="./nips-2009-Solving_Stochastic_Games.html">221 nips-2009-Solving Stochastic Games</a></p>
<p>18 0.22663242 <a title="107-lsi-18" href="./nips-2009-Multi-Step_Dyna_Planning_for_Policy_Evaluation_and_Control.html">159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</a></p>
<p>19 0.21670513 <a title="107-lsi-19" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>20 0.20900777 <a title="107-lsi-20" href="./nips-2009-Sequential_effects_reflect_parallel_learning_of_multiple_environmental_regularities.html">216 nips-2009-Sequential effects reflect parallel learning of multiple environmental regularities</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.029), (25, 0.081), (32, 0.241), (35, 0.039), (36, 0.038), (39, 0.089), (55, 0.013), (58, 0.062), (61, 0.1), (71, 0.108), (81, 0.01), (86, 0.051), (91, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83828038 <a title="107-lda-1" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>Author: Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</p><p>Abstract: Everyday social interactions are heavily inﬂuenced by our snap judgments about others’ goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is ‘helping’ or ‘hindering’ another’s attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent’s behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative. 1</p><p>2 0.77612883 <a title="107-lda-2" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>Author: Ed Vul, George Alvarez, Joshua B. Tenenbaum, Michael J. Black</p><p>Abstract: Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We ﬁnd that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle ﬁlter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a ﬂexible cognitive resource, which can be formalized as either memory or attention. 1</p><p>3 0.62002313 <a title="107-lda-3" href="./nips-2009-The_Infinite_Partially_Observable_Markov_Decision_Process.html">242 nips-2009-The Infinite Partially Observable Markov Decision Process</a></p>
<p>Author: Finale Doshi-velez</p><p>Abstract: The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains where agents must balance actions that provide knowledge and actions that provide reward. Unfortunately, most POMDPs are complex structures with a large number of parameters. In many real-world problems, both the structure and the parameters are difﬁcult to specify from domain knowledge alone. Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model. We deﬁne an inﬁnite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and only models visited states explicitly. We demonstrate the iPOMDP on several standard problems. 1</p><p>4 0.6180039 <a title="107-lda-4" href="./nips-2009-Differential_Use_of_Implicit_Negative_Evidence_in_Generative_and_Discriminative_Language_Learning.html">66 nips-2009-Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</a></p>
<p>Author: Anne Hsu, Thomas L. Griffiths</p><p>Abstract: A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-speciﬁc knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented. 1</p><p>5 0.59078044 <a title="107-lda-5" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>Author: Jaakko Luttinen, Alexander T. Ihler</p><p>Abstract: We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</p><p>6 0.57079607 <a title="107-lda-6" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>7 0.56102967 <a title="107-lda-7" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>8 0.55940515 <a title="107-lda-8" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>9 0.54945511 <a title="107-lda-9" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>10 0.5477947 <a title="107-lda-10" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>11 0.54555875 <a title="107-lda-11" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>12 0.54541403 <a title="107-lda-12" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>13 0.54420269 <a title="107-lda-13" href="./nips-2009-Convergent_Temporal-Difference_Learning_with_Arbitrary_Smooth_Function_Approximation.html">60 nips-2009-Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></p>
<p>14 0.54293531 <a title="107-lda-14" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>15 0.54262394 <a title="107-lda-15" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>16 0.54201055 <a title="107-lda-16" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>17 0.54168391 <a title="107-lda-17" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>18 0.54100382 <a title="107-lda-18" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>19 0.53979862 <a title="107-lda-19" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>20 0.53973478 <a title="107-lda-20" href="./nips-2009-Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining.html">218 nips-2009-Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
