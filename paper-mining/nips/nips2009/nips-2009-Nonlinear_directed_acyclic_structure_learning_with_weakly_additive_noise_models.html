<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-170" href="#">nips2009-170</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</h1>
<br/><p>Source: <a title="nips-2009-170-pdf" href="http://papers.nips.cc/paper/3699-nonlinear-directed-acyclic-structure-learning-with-weakly-additive-noise-models.pdf">pdf</a></p><p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>Reference: <a title="nips-2009-170-reference" href="../nips2009_reference/nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nonlinear directed acyclic structure learning with weakly additive noise models  Peter Spirtes Arthur Gretton Robert E. [sent-1, score-0.921]
</p><p>2 com  Abstract The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. [sent-7, score-0.776]
</p><p>3 linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. [sent-10, score-0.644]
</p><p>4 We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. [sent-11, score-1.895]
</p><p>5 We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. [sent-12, score-0.852]
</p><p>6 This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. [sent-13, score-0.589]
</p><p>7 1  Introduction  Learning probabilistic graphical models from data serves two primary purposes: (i) ﬁnding compact representations of probability distributions to make inference eﬃcient and (ii) modeling unknown data generating mechanisms and predicting causal relationships. [sent-14, score-0.159]
</p><p>8 Until recently, most constraint-based and score-based algorithms for learning directed graphical models from continuous data required assuming relationships between variables are linear with Gaussian noise. [sent-15, score-0.153]
</p><p>9 While this assumption may be appropriate in many contexts, there are well known contexts, such as fMRI images, where variables have nonlinear dependencies and data do not tend towards Gaussianity. [sent-16, score-0.082]
</p><p>10 A second major limitation of the traditional algorithms is they cannot identify a unique structure; they reduce the set of possible structures to an equivalence class which entail the same Markov properties. [sent-17, score-0.084]
</p><p>11 The recently proposed additive noise model [1] for structure learning addresses both limitations; by taking advantage of observed nonlinearity and non-Gaussianity, a unique directed acyclic structure can be identiﬁed in many contexts. [sent-18, score-0.801]
</p><p>12 linear Gaussians, the model is invertible and not useful for structure learning; (ii) it was originally proposed for two variables with a multivariate extension that requires enumerating all possible DAGs, which is super-exponential in the number of variables. [sent-21, score-0.08]
</p><p>13 In this paper, we address the limitations of the additive noise model. [sent-22, score-0.564]
</p><p>14 We introduce weakly additive noise models, which have the advantages of additive noise models, but are still useful when the additive noise model is invertible and in most cases when additive noise is not present. [sent-23, score-2.459]
</p><p>15 Weakly additive noise models allow us to express greater uncertainty about the 1  data generating mechanism, but can still identify a unique structure or a smaller equivalence class in most cases. [sent-24, score-0.706]
</p><p>16 We also provide an algorithm for learning an equivalence class for such models from data that is more computationally eﬃcient in the more than two variables case. [sent-25, score-0.109]
</p><p>17 Section 2 reviews the appropriate background; section 3 introduces weakly additive noise models; section 4 describes our learning algorithm; section 5 discusses some related research; section 6 presents some experimental results; ﬁnally, section 7 oﬀers conclusions. [sent-26, score-0.687]
</p><p>18 2  Background  Let G = V, E be a directed acyclic graph (DAG), where V denotes the set of vertices and Eij ∈ E denotes a directed edge Vi → Vj . [sent-28, score-0.425]
</p><p>19 The degree of Vi G G is the number of edges with an endpoint at Vi . [sent-31, score-0.057]
</p><p>20 P is faithful to G if every conditional independence true G Vi ∈V  in P is entailed by the above factorization. [sent-35, score-0.096]
</p><p>21 A partially directed acyclic graph (PDAG) H for G is a mixed graph, i. [sent-36, score-0.266]
</p><p>22 consisting of directed and undirected edges, representing all DAGs Markov equivalent to G, i. [sent-38, score-0.175]
</p><p>23 If Vi → Vj is a directed edge in H, then all DAGs Markov equivalent to G have this directed edge; if Vi − Vj is an undirected edge in H, then some DAGs that are Markov equivalent to G have the directed edge Vi → Vj while others have the directed edge Vi ← Vj . [sent-41, score-0.779]
</p><p>24 The PC algorithm is a well known constraint-based, or conditional independence based, structure learning algorithm. [sent-42, score-0.096]
</p><p>25 PC learns the correct PDAG in the large sample limit when the Markov, faithfulness, and causal suﬃciency (that there are no unmeasured common causes of two or more measured variables) assumptions hold [2]. [sent-46, score-0.101]
</p><p>26 The partial correlation based Fisher Z-transformation test, which assumes linear Gaussian distributions, is used for conditional independence testing with continuous variables. [sent-47, score-0.096]
</p><p>27 The recently proposed additive noise model approach to structure learning [1] assumes only that each variable can be represented as a (possibly nonlinear) function f of its parents plus additive noise ǫ with some arbitrary distribution, and that the noise components are n  P(ǫi ). [sent-50, score-1.34]
</p><p>28 , ǫn ) = i=1  X → Y is the true DAG, X = ǫX , Y = sin(πX) + ǫY , ǫX ∼ U nif (−1, 1), and ǫY ∼ U nif (−1, 1). [sent-56, score-0.138]
</p><p>29 If we regress Y on X (nonparametrically), the forward model, ﬁgure 1a, and 2  0. [sent-57, score-0.087]
</p><p>30 5  1  −1 −8 −6 −4 −2 0 2 4 6 8 10 Z  (c)  (d)  Figure 1: Nonparametric regressions with data overlayed for (a) Y regressed on X, (b) X regressed on Y , (c) Z regressed on X, and (d) X regressed on Z regress X on Y , the backward model, ﬁgure 1b, we observe the residuals ǫY ⊥ X and ˆ ⊥ ǫX ⊥ Y . [sent-73, score-0.275]
</p><p>31 This provides a criterion for distinguishing X → Y from X ← Y in many cases, ˆ ⊥ / but there are counterexamples such as the linear Gaussian case, where the forward model is invertible so we ﬁnd ǫY ⊥ X and ǫX ⊥ Y . [sent-74, score-0.112]
</p><p>32 [1, 5] show, however, that whenever f is ˆ ⊥ ˆ ⊥ nonlinear, the forward model is noninvertible, and when f is linear, the forward model is only invertible when ǫ is Gaussian and a few other special cases. [sent-75, score-0.144]
</p><p>33 for X → Y → Z with X = ǫX , Y = X 3 + ǫY , Z = Y 3 + ǫZ , ǫX ∼ U nif (−1, 1), ǫY ∼ U nif (−1, 1), and ǫZ ∼ U nif (0, 1), observing only X and Z, ﬁgures 1c and 1d, causes us to reject both the forward and backward models. [sent-78, score-0.239]
</p><p>34 To test whether a DAG is compatible with the data, we regress each variable on its parents and test whether the resulting residuals are mutually independent. [sent-80, score-0.087]
</p><p>35 Since we do not assume linearity or Gaussianity in this framework, a suﬃciently powerful nonparametric independence test must be used. [sent-85, score-0.062]
</p><p>36 A Hilbert space HX of functions from X to R is a reproducing kernel Hilbert space (RKHS) if for some kernel k(·, ·) (the reproducing kernel for HX ), for every f (·) ∈ HX and x ∈ X , the inner product f (·), k(x, ·) HX = f (x). [sent-88, score-0.192]
</p><p>37 The Moore-Aronszajn theorem shows that all symmetric positive deﬁnite kernels (most popular kernels) are reproducing kernels that uniquely deﬁne corresponding RKHSs [7]. [sent-91, score-0.108]
</p><p>38 Let Y be a random variable with domain Y and l(·, ·) the reproducing kernel for HY . [sent-92, score-0.078]
</p><p>39 µX = EX [k(x, ·)]  CXY = ([k(x, ·) − µX ] ⊗ [l(y, ·) − µY ])  If the kernels are characteristic, e. [sent-94, score-0.033]
</p><p>40 3  Weakly additive noise models  We now extend the additive noise model framework to account for cases where additive noise models are invertible and cases where additive noise may not be present. [sent-108, score-2.386]
</p><p>41 ψ = Vi , PaVi G  is a local additive noise model for a distribution P over V  that is Markov to a DAG G = V, E if Vi = f PaVi + ǫ is an additive noise model. [sent-111, score-1.128]
</p><p>42 When we assume a data generating process has a weakly additive noise model representation, we assume only that there are no cases where X → Y can be written X = f (Y ) + ǫX , but not Y = f (X) + ǫY . [sent-115, score-0.72]
</p><p>43 In other words, the data cannot appear as though it admits an additive noise model representation, but only in the incorrect direction. [sent-116, score-0.564]
</p><p>44 This representation is still appropriate when additive noise models are invertible, and when additive noise is not present: such cases only lead to weakly additive noise models which express greater underdetermination of the true data generating process. [sent-117, score-1.898]
</p><p>45 We now deﬁne the notion of distribution-equivalence for weakly additive noise models. [sent-118, score-0.687]
</p><p>46 A weakly additive noise model M = G, Ψ is distribution-equivalent to N = G ′ , Ψ′ if and only if G and G ′ are Markov equivalent and ψ ∈ Ψ if and only if ψ ∈ Ψ′ . [sent-121, score-0.687]
</p><p>47 Distribution-equivalence deﬁnes what can be discovered about the true data generating mechanism using observational data. [sent-122, score-0.033]
</p><p>48 We now deﬁne a new structure to partition data generating processes which instantiate distribution-equivalent weakly additive noise models. [sent-123, score-0.72]
</p><p>49 A weakly additive noise partially directed acyclic graph (WAN-PDAG) for M = G, Ψ is a mixed graph H = V, E such that for {Vi , Vj } ⊆ V, 1. [sent-126, score-0.986]
</p><p>50 Vi → Vj is a directed edge in H if and only if Vi → Vj is a directed edge in G and in all G ′ such that N = G ′ , Ψ′ is distribution-equivalent to M 2. [sent-127, score-0.366]
</p><p>51 Vi − Vj is an undirected edge in H if and only if Vi → Vj is a directed edge in G and there exists a G ′ and N = G ′ , Ψ′ distribution-equivalent to M such that Vi ← Vj is a directed edge in G ′ We now get the following results. [sent-128, score-0.468]
</p><p>52 Let M = G, Ψ be a weakly additive noise model, ′  ′  N = G ,Ψ  be distribution equivalent to M. [sent-131, score-0.687]
</p><p>53 (i) This is correct because of Markov equivalence [2]. [sent-139, score-0.084]
</p><p>54 In the ﬁrst stage, we use a kernel-based conditional dependence measure similar to HSIC [9] (see also [11, Section 2. [sent-146, score-0.034]
</p><p>55 For ˜ a conditioning variable Z with centered Gram matrix M for a reproducing kernel m(·, ·), −1 ¨ we deﬁne the conditional cross covariance CXY |Z = CXZ CZZ CZ Y , where X = (X, Z) and ¨ ¨ 2 ¨ Y = (Y, Z). [sent-148, score-0.112]
</p><p>56 It follows from [9, Theorem 3] that HXY |Z = 0 if and only if X ⊥ Y |Z when kernels are characteristic. [sent-150, score-0.033]
</p><p>57 [9] provides the empirical estimator: ⊥ 1 ˆ ˜˜ ˜ ˜ ˜ ˜˜ ˜ ˜ ˜ ˜˜ ˜ ˜ ˜ HXY |Z = 2 tr(K L − 2K M (M + ǫIN )−2 M L + K M (M + ǫIN )−2 M LM (M + ǫIN )−2 M ) m ˆ The null distribution of HXY |Z is unknown and diﬃcult to derive so we must use the permutation approach described in section 2. [sent-151, score-0.045]
</p><p>58 We thus (making analogy to the discrete case) must cluster Z and then permute elements only within clusters for the permutation test, as in [12]. [sent-153, score-0.045]
</p><p>59 ˆ This ﬁrst stage is not computational eﬃcient, however, since each evaluation of HXY |Z is ˆ naively O N 3 and we need to evaluate HXY |Z approximately 1000 times for each permutation test. [sent-154, score-0.083]
</p><p>60 We implemented the incomplete Cholesky factorization [14], which can be used to obtain an m × p matrix G, where p ≪ m, and an m × m permutation matrix P such that K ≈ P GG⊤ P ⊤ , where K is ˆ an m × m Gram matrix. [sent-156, score-0.078]
</p><p>61 A clever implementation after replacing Gram matrices in HXY |Z with their incomplete Cholesky factorizations and using an appropriate equivalence to invert ˜ G⊤ G + ǫIp for M instead of GG⊤ + ǫIm results in a straightforward O mp3 operation. [sent-157, score-0.117]
</p><p>62 A more stable (and faster) approach is to obtain incomplete Cholesky factorizations GX , GY , and GZ with permutation matrices PX , PY , and PZ , and then obtain the thin SVDs for HPX GX , HPY GY , and HPZ GZ , e. [sent-159, score-0.078]
</p><p>63 Figure 2 shows that this method leads to a signiﬁcant increase in speed when used with a permutation test for conditional independence without signiﬁcantly aﬀecting the empirically observed type I error rate for a level-. [sent-162, score-0.141]
</p><p>64 ⊥ may ﬁnd more orientations by considering submodels, e. [sent-173, score-0.064]
</p><p>65 if all relations are linear and only one variable has a non-Gaussian noise term. [sent-175, score-0.18]
</p><p>66 The basic strategy used is a“PC-style” greedy search where we look for undirected edges in the current mixed graph (starting with the PDAG resulting from the ﬁrst stage) adjacent to the fewest other undirected edges. [sent-176, score-0.208]
</p><p>67 If these edges can be oriented using additive noise models, we make the implied orientations, apply the extended Meek rules, and then iterate until no more edges can be oriented. [sent-177, score-0.724]
</p><p>68 Let G = V, E be the resulting PDAG and ∀Vi ∈ V, let UVi denote G the nodes connected to Vi in G by an undirected edge. [sent-179, score-0.047]
</p><p>69 If an edge is oriented in the second stage of kPC, it is implied by a noninvertible local additive noise model. [sent-184, score-0.797]
</p><p>70 If the condition at line 8 is true then Vi , PaVi ∪ S is a noninvertible local additive G noise model. [sent-186, score-0.658]
</p><p>71 Suppose ψ = Vi , W is a noninvertible local additive noise model. [sent-215, score-0.658]
</p><p>72 Then kPC will make all orientations implied by ψ. [sent-216, score-0.11]
</p><p>73 Since Vi , Pa ∪ S is a noninvertible local G  G  additive noise model, line 8 is satisﬁed so all edges connected to Vi are oriented. [sent-220, score-0.715]
</p><p>74 Assume data is generated according to some weakly additive noise model M = G, Ψ . [sent-223, score-0.687]
</p><p>75 Then kPC will return the WAN-PDAG instantiated by M assuming perfect conditional independence information, Markov, faithfulness, and causal suﬃciency. [sent-224, score-0.197]
</p><p>76 The PC algorithm is correct and complete with respect to conditional independence [2]. [sent-226, score-0.096]
</p><p>77 Orientations made with respect to additive noise models are correct by lemma 4. [sent-227, score-0.614]
</p><p>78 1 and all such orientations that can be made are made by lemma 4. [sent-228, score-0.089]
</p><p>79 The Meek rules, which are correct and complete [4], are invoked after each orientation made with respect to additive noise models so they are invoked after all such orientations are made. [sent-230, score-0.653]
</p><p>80 5  Related research  kPC is similar in spirit to the PC-LiNGAM structure learning algorithm [15], which assumes dependencies are linear with either Gaussian or non-Gaussian noise. [sent-231, score-0.036]
</p><p>81 KCL [11] is a heuristic search for a mixed graph that uses the same kernel-based dependence measures as kPC (while not determining signiﬁcance threhsholds via a hypothesis test), but does not take advantage of additive noise models. [sent-233, score-0.621]
</p><p>82 [16] provides a more eﬃcient algorithm for learning additive noise models, by ﬁrst ﬁnding a causal ordering after doing a series of high dimensional regressions and HSIC independence tests and then pruning the resulting DAG implied by this ordering. [sent-234, score-0.798]
</p><p>83 Finally, [17] proposes a two-stage procedure for learning additive noise models from data that is similar to kPC, but requires the additive noise model assumptions in the ﬁrst stage where the Markov equivalence class is identiﬁed. [sent-235, score-1.275]
</p><p>84 We generated non-Gaussian noise using the same procedure as [19] and used polynomial and trigonometric functions for nonlinear dependencies. [sent-237, score-0.226]
</p><p>85 We compared kPC to PC, the score-based GES with the BIC-score [20], and the ICA-based LiNGAM [19], which assumes linear dependencies and non-Gaussian noise. [sent-238, score-0.036]
</p><p>86 proportion of directed edges in the resulting graph that are in the true DAG, and recall, i. [sent-241, score-0.218]
</p><p>87 proportion of directed edges in the true DAG that are in the resulting graph. [sent-243, score-0.185]
</p><p>88 kPC performs about the same as PC in precision and recall, which again is unsurprising since previous simulation results have shown that nonlinearity, but not non-Gaussianity can signiﬁcantly aﬀect the performance of PC. [sent-248, score-0.056]
</p><p>89 In the nonlinear non-Gaussian case, kPC performs slightly better than PC in precision. [sent-249, score-0.046]
</p><p>90 2 We also ran kPC on data from an fMRI experiment that is analyzed in [21] where nonlinear dependencies can be observed. [sent-251, score-0.082]
</p><p>91 kPC successfully found the same directed edges without using any background knowledge. [sent-255, score-0.212]
</p><p>92 7  Conclusion  We introduced weakly additive noise models, which extend the additive noise model framework to cases such as the linear Gaussian, where the additive noise model is invertible and thus unidentiﬁable, as well as cases where additive noise is not present. [sent-257, score-2.459]
</p><p>93 The weakly additive noise framework allows us to identify a unique DAG when the additive noise model assumptions hold, and a structure that is at least as speciﬁc as a PDAG (possibly still a unique DAG) when some additive noise assumptions fail. [sent-258, score-1.815]
</p><p>94 We deﬁned equivalence classes for such models and introduced the kPC algorithm for learning these equivalence classes from data. [sent-259, score-0.193]
</p><p>95 2 When simulating nonlinear data, we must be careful to ensure that variances do not blow up and result in data for which no ﬁnite sample method can show adequate performance. [sent-265, score-0.046]
</p><p>96 This has the unfortunate side eﬀect that the nonlinear data generated may be well approximated using linear methods. [sent-266, score-0.046]
</p><p>97 Causal discovery of linear acyclic models with arbitrary distributions. [sent-366, score-0.106]
</p><p>98 Regression by dependence minio mization and its application to causal inference in additive noise models. [sent-374, score-0.665]
</p><p>99 Acyclic causality discovery with additive noise: An a information-theoretical perspective. [sent-379, score-0.384]
</p><p>100 A linear non-gaussian acyclic a model for causal discovery. [sent-392, score-0.182]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kpc', 0.397), ('additive', 0.384), ('vi', 0.307), ('pavi', 0.235), ('vj', 0.224), ('hxy', 0.219), ('noise', 0.18), ('pc', 0.173), ('dag', 0.151), ('lingam', 0.137), ('directed', 0.128), ('dags', 0.126), ('pdag', 0.125), ('uvi', 0.125), ('weakly', 0.123), ('ges', 0.11), ('gz', 0.11), ('causal', 0.101), ('vk', 0.095), ('chvi', 0.094), ('noninvertible', 0.094), ('equivalence', 0.084), ('acyclic', 0.081), ('invertible', 0.08), ('gy', 0.078), ('meek', 0.069), ('cxy', 0.069), ('nif', 0.069), ('hx', 0.067), ('gx', 0.067), ('orientations', 0.064), ('janzing', 0.063), ('orient', 0.063), ('pagj', 0.063), ('independence', 0.062), ('gram', 0.059), ('edges', 0.057), ('edge', 0.055), ('regress', 0.055), ('regressed', 0.055), ('cholesky', 0.053), ('ii', 0.049), ('hsic', 0.047), ('spirtes', 0.047), ('submodels', 0.047), ('markov', 0.047), ('undirected', 0.047), ('implied', 0.046), ('nonlinear', 0.046), ('permutation', 0.045), ('gretton', 0.045), ('hilbert', 0.043), ('reproducing', 0.042), ('hyv', 0.041), ('stage', 0.038), ('dependencies', 0.036), ('kernel', 0.036), ('su', 0.036), ('sz', 0.035), ('conditional', 0.034), ('generating', 0.033), ('incomplete', 0.033), ('hs', 0.033), ('hoyer', 0.033), ('graph', 0.033), ('kernels', 0.033), ('forward', 0.032), ('rules', 0.032), ('fukumizu', 0.032), ('parents', 0.032), ('ul', 0.032), ('faithfulness', 0.031), ('gaussianity', 0.031), ('immorality', 0.031), ('lacc', 0.031), ('lifg', 0.031), ('lipl', 0.031), ('lmtg', 0.031), ('locc', 0.031), ('pagi', 0.031), ('ramsey', 0.031), ('sk', 0.031), ('precision', 0.029), ('nonlinearity', 0.028), ('scholk', 0.027), ('hanson', 0.027), ('mooij', 0.027), ('nonparametrically', 0.027), ('unsurprising', 0.027), ('background', 0.027), ('sch', 0.027), ('tests', 0.025), ('lemma', 0.025), ('pittsburgh', 0.025), ('schmidt', 0.025), ('unidenti', 0.025), ('gg', 0.025), ('pa', 0.025), ('models', 0.025), ('mixed', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="170-tfidf-1" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>2 0.20885067 <a title="170-tfidf-2" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we present a novel approach to learn directed acyclic graphs (DAGs) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identiﬁability and predictive densities to perform the model comparison. We require identiﬁability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artiﬁcial and biological data showing that our approach outperform a number of state of the art methods. 1</p><p>3 0.14643021 <a title="170-tfidf-3" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>4 0.0971426 <a title="170-tfidf-4" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright</p><p>Abstract: We study minimax rates for estimating high-dimensional nonparametric regression models with sparse additive structure and smoothness constraints. More precisely, our goal is to estimate a function f ∗ : Rp → R that has an additive decomposition of the form ∗ ∗ f ∗ (X1 , . . . , Xp ) = j∈S hj (Xj ), where each component function hj lies in some class H of “smooth” functions, and S ⊂ {1, . . . , p} is an unknown subset with cardinality s = |S|. Given n i.i.d. observations of f ∗ (X) corrupted with additive white Gaussian noise where the covariate vectors (X1 , X2 , X3 , ..., Xp ) are drawn with i.i.d. components from some distribution P, we determine lower bounds on the minimax rate for estimating the regression function with respect to squared-L2 (P) error. Our main result is a lower bound on the minimax rate that scales as max s log(p/s) , s ǫ2 (H) . The ﬁrst term reﬂects the sample size required for n n performing subset selection, and is independent of the function class H. The second term s ǫ2 (H) is an s-dimensional estimation term corresponding to the sample size required for n estimating a sum of s univariate functions, each chosen from the function class H. It depends linearly on the sparsity index s but is independent of the global dimension p. As a special case, if H corresponds to functions that are m-times differentiable (an mth -order Sobolev space), then the s-dimensional estimation term takes the form sǫ2 (H) ≍ s n−2m/(2m+1) . Either of n the two terms may be dominant in different regimes, depending on the relation between the sparsity and smoothness of the additive decomposition.</p><p>5 0.089960158 <a title="170-tfidf-5" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-ofthe-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justiﬁcations of speciﬁc versions of the additive forward regression. 1</p><p>6 0.077811129 <a title="170-tfidf-6" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>7 0.074169427 <a title="170-tfidf-7" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>8 0.073688678 <a title="170-tfidf-8" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>9 0.071510755 <a title="170-tfidf-9" href="./nips-2009-Unsupervised_Feature_Selection_for_the_%24k%24-means_Clustering_Problem.html">252 nips-2009-Unsupervised Feature Selection for the $k$-means Clustering Problem</a></p>
<p>10 0.067732356 <a title="170-tfidf-10" href="./nips-2009-A_Fast%2C_Consistent_Kernel_Two-Sample_Test.html">8 nips-2009-A Fast, Consistent Kernel Two-Sample Test</a></p>
<p>11 0.066412047 <a title="170-tfidf-11" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>12 0.05540045 <a title="170-tfidf-12" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>13 0.050939761 <a title="170-tfidf-13" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>14 0.050768949 <a title="170-tfidf-14" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>15 0.048830193 <a title="170-tfidf-15" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>16 0.048611175 <a title="170-tfidf-16" href="./nips-2009-Kernel_Choice_and_Classifiability_for_RKHS_Embeddings_of_Probability_Distributions.html">118 nips-2009-Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions</a></p>
<p>17 0.046164647 <a title="170-tfidf-17" href="./nips-2009-Time-Varying_Dynamic_Bayesian_Networks.html">246 nips-2009-Time-Varying Dynamic Bayesian Networks</a></p>
<p>18 0.043179642 <a title="170-tfidf-18" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>19 0.041993219 <a title="170-tfidf-19" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>20 0.040802088 <a title="170-tfidf-20" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.008), (2, 0.008), (3, 0.032), (4, -0.017), (5, -0.072), (6, 0.067), (7, -0.039), (8, -0.036), (9, -0.095), (10, -0.039), (11, -0.068), (12, 0.027), (13, 0.029), (14, 0.023), (15, 0.025), (16, -0.02), (17, 0.033), (18, -0.052), (19, -0.117), (20, -0.053), (21, 0.12), (22, -0.222), (23, 0.005), (24, -0.189), (25, 0.04), (26, -0.057), (27, -0.003), (28, -0.015), (29, -0.109), (30, 0.008), (31, -0.038), (32, 0.119), (33, 0.017), (34, 0.034), (35, 0.017), (36, -0.018), (37, -0.056), (38, -0.049), (39, 0.036), (40, 0.111), (41, 0.226), (42, -0.032), (43, -0.059), (44, -0.198), (45, 0.035), (46, 0.095), (47, 0.127), (48, 0.223), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96212798 <a title="170-lsi-1" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>2 0.63793898 <a title="170-lsi-2" href="./nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we present a novel approach to learn directed acyclic graphs (DAGs) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identiﬁability and predictive densities to perform the model comparison. We require identiﬁability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artiﬁcial and biological data showing that our approach outperform a number of state of the art methods. 1</p><p>3 0.4853451 <a title="170-lsi-3" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>Author: Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</p><p>Abstract: We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the speciﬁcation of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efﬁcient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. 1</p><p>4 0.47752622 <a title="170-lsi-4" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>Author: Tom Ouyang, Randall Davis</p><p>Abstract: We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach signiﬁcantly improves recognition performance. 1</p><p>5 0.43594006 <a title="170-lsi-5" href="./nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-ofthe-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justiﬁcations of speciﬁc versions of the additive forward regression. 1</p><p>6 0.42498863 <a title="170-lsi-6" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>7 0.41902304 <a title="170-lsi-7" href="./nips-2009-Unsupervised_Feature_Selection_for_the_%24k%24-means_Clustering_Problem.html">252 nips-2009-Unsupervised Feature Selection for the $k$-means Clustering Problem</a></p>
<p>8 0.36481002 <a title="170-lsi-8" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>9 0.36077511 <a title="170-lsi-9" href="./nips-2009-Sufficient_Conditions_for_Agnostic_Active_Learnable.html">240 nips-2009-Sufficient Conditions for Agnostic Active Learnable</a></p>
<p>10 0.34390366 <a title="170-lsi-10" href="./nips-2009-Efficient_Moments-based_Permutation_Tests.html">78 nips-2009-Efficient Moments-based Permutation Tests</a></p>
<p>11 0.33991846 <a title="170-lsi-11" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>12 0.33729252 <a title="170-lsi-12" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>13 0.33251271 <a title="170-lsi-13" href="./nips-2009-Riffled_Independence_for_Ranked_Data.html">206 nips-2009-Riffled Independence for Ranked Data</a></p>
<p>14 0.32087103 <a title="170-lsi-14" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>15 0.3134391 <a title="170-lsi-15" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>16 0.30835277 <a title="170-lsi-16" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>17 0.30259758 <a title="170-lsi-17" href="./nips-2009-A_Fast%2C_Consistent_Kernel_Two-Sample_Test.html">8 nips-2009-A Fast, Consistent Kernel Two-Sample Test</a></p>
<p>18 0.28112385 <a title="170-lsi-18" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>19 0.27880529 <a title="170-lsi-19" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>20 0.26920485 <a title="170-lsi-20" href="./nips-2009-The_Wisdom_of_Crowds_in_the_Recollection_of_Order_Information.html">244 nips-2009-The Wisdom of Crowds in the Recollection of Order Information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.364), (21, 0.01), (24, 0.063), (25, 0.048), (35, 0.091), (36, 0.064), (39, 0.063), (49, 0.019), (58, 0.083), (71, 0.045), (86, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89658505 <a title="170-lda-1" href="./nips-2009-Noise_Characterization%2C_Modeling%2C_and_Reduction_for_In_Vivo_Neural_Recording.html">165 nips-2009-Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></p>
<p>Author: Zhi Yang, Qi Zhao, Edward Keefer, Wentai Liu</p><p>Abstract: Studying signal and noise properties of recorded neural data is critical in developing more efﬁcient algorithms to recover the encoded information. Important issues exist in this research including the variant spectrum spans of neural spikes that make it difﬁcult to choose a globally optimal bandpass ﬁlter. Also, multiple sources produce aggregated noise that deviates from the conventional white Gaussian noise. In this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass ﬁlter that ﬁts the spectrum of individual spikes is proposed. Multiple noise sources have been studied through analytical models as well as empirical measurements. The dominant noise source is identiﬁed as neuron noise followed by interface noise of the electrode. This suggests that major efforts to reduce noise from electronics are not well spent. The measured noise from in vivo experiments shows a family of 1/f x spectrum that can be reduced using noise shaping techniques. In summary, the methods of adaptive bandpass ﬁltering and noise shaping together result in several dB signal-to-noise ratio (SNR) enhancement.</p><p>same-paper 2 0.78787935 <a title="170-lda-2" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>3 0.7470355 <a title="170-lda-3" href="./nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">69 nips-2009-Discrete MDL Predicts in Total Variation</a></p>
<p>Author: Marcus Hutter</p><p>Abstract: The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identiﬁability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed. 1</p><p>4 0.74283445 <a title="170-lda-4" href="./nips-2009-Fast_subtree_kernels_on_graphs.html">95 nips-2009-Fast subtree kernels on graphs</a></p>
<p>Author: Nino Shervashidze, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon & G¨ rtner scales as O(n2 4d h). Key to this efﬁciency is the observation that the a Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classiﬁcation benchmark datasets in terms of accuracy and runtime. 1</p><p>5 0.65899748 <a title="170-lda-5" href="./nips-2009-Efficient_and_Accurate_Lp-Norm_Multiple_Kernel_Learning.html">80 nips-2009-Efficient and Accurate Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Ulf Brefeld, Pavel Laskov, Klaus-Robert Müller, Alexander Zien, Sören Sonnenburg</p><p>Abstract: Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations to support interpretability. Unfortunately, 1 -norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures, we generalize MKL to arbitrary p -norms. We devise new insights on the connection between several existing MKL formulations and develop two efﬁcient interleaved optimization strategies for arbitrary p > 1. Empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally, we apply p -norm MKL to real-world problems from computational biology, showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art. 1</p><p>6 0.56392425 <a title="170-lda-6" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>7 0.45877337 <a title="170-lda-7" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>8 0.45572805 <a title="170-lda-8" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>9 0.45024413 <a title="170-lda-9" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>10 0.44505951 <a title="170-lda-10" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>11 0.44329244 <a title="170-lda-11" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>12 0.43909264 <a title="170-lda-12" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>13 0.43446225 <a title="170-lda-13" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>14 0.43428147 <a title="170-lda-14" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>15 0.43143779 <a title="170-lda-15" href="./nips-2009-Functional_network_reorganization_in_motor_cortex_can_be_explained_by_reward-modulated_Hebbian_learning.html">99 nips-2009-Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></p>
<p>16 0.431427 <a title="170-lda-16" href="./nips-2009-Heavy-Tailed_Symmetric_Stochastic_Neighbor_Embedding.html">106 nips-2009-Heavy-Tailed Symmetric Stochastic Neighbor Embedding</a></p>
<p>17 0.43040687 <a title="170-lda-17" href="./nips-2009-Hierarchical_Modeling_of_Local_Image_Features_through_%24L_p%24-Nested_Symmetric_Distributions.html">111 nips-2009-Hierarchical Modeling of Local Image Features through $L p$-Nested Symmetric Distributions</a></p>
<p>18 0.4289341 <a title="170-lda-18" href="./nips-2009-Subject_independent_EEG-based_BCI_decoding.html">237 nips-2009-Subject independent EEG-based BCI decoding</a></p>
<p>19 0.42742753 <a title="170-lda-19" href="./nips-2009-Time-rescaling_methods_for_the_estimation_and_assessment_of_non-Poisson_neural_encoding_models.html">247 nips-2009-Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></p>
<p>20 0.42648882 <a title="170-lda-20" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
