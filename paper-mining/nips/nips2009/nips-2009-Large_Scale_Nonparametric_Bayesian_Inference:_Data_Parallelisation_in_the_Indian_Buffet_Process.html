<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-123" href="#">nips2009-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</h1>
<br/><p>Source: <a title="nips-2009-123-pdf" href="http://papers.nips.cc/paper/3669-large-scale-nonparametric-bayesian-inference-data-parallelisation-in-the-indian-buffet-process.pdf">pdf</a></p><p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>Reference: <a title="nips-2009-123-reference" href="../nips2009_reference/nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. [sent-12, score-0.151]
</p><p>2 We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. [sent-13, score-0.367]
</p><p>3 Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. [sent-14, score-1.099]
</p><p>4 This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. [sent-15, score-0.249]
</p><p>5 Computing these high-dimensional average is thus a key challenge in scaling Bayesian inference to large datasets, especially for nonparametric models. [sent-21, score-0.159]
</p><p>6 Advances in multicore and distributed computing provide one answer to this challenge: if each processor can consider only a small part of the data, then inference in these large datasets might become more tractable. [sent-22, score-0.565]
</p><p>7 Building on work on approximate asynchronous multicore inference for topic models [2], we develop a message passing framework for data-parallel Bayesian inference applicable to a variety of models, including matrix factorization and the Indian Buffet Process (IBP). [sent-24, score-0.912]
</p><p>8 The IBP [3] is a distribution over inﬁnite sparse binary matrices that allows data points to be represented by an unbounded number of sparse latent features or factors. [sent-29, score-0.157]
</p><p>9 While the parallelisation method we present in this paper is applicable to a broad set of models, we focus on inference for the IBP because of its unique challenges and potential. [sent-30, score-0.265]
</p><p>10 Many serial procedures have been developed for inference in the IBP, including variants of Gibbs sampling [3, 4], which may be augmented with Metropolis split-merge proposals [5], slice sampling [6], particle ﬁltering [7], and variational inference [8]. [sent-31, score-0.459]
</p><p>11 With the exception of the accelerated Gibbs sampler of [4], these methods have been applied to datasets with less than 1,000 observations. [sent-32, score-0.276]
</p><p>12 Coupled with a message passing scheme over processors, this idea enables computations for inference to be distributed over many processors with few losses in accuracy. [sent-34, score-0.957]
</p><p>13 The following customers try previously sampled dishes with probability mk /n, where mk is the number of people who tried dish k before customer n. [sent-43, score-0.217]
</p><p>14 This generative process allows an unbounded set of features but guarantees that a ﬁnite dataset will contain a ﬁnite number of features with probability one. [sent-46, score-0.192]
</p><p>15 Finally, if the effect of possessing a feature is independent of the feature index, the model is also exchangeable in the columns of Z. [sent-48, score-0.158]
</p><p>16 We associate with the feature assignment matrix Z, a feature matrix A with rows that parameterise the effect that possessing each feature has on the data. [sent-49, score-0.222]
</p><p>17 (3)  Root  po  ste r  io r  s tic r tis rio sta ste po  sta tis tic s  prior  N  P2  +ε P3  (a) Representation of the linear-Gaussian model. [sent-58, score-0.518]
</p><p>18 In the Bernoulli model, the product ZA adjusts the probability of X = 1  r  K  rio  tis tic s  P1 sta  . [sent-60, score-0.154]
</p><p>19 *  D  ste  A  po  ~  K  s tic or tis ri ste po  Z  . [sent-63, score-0.3]
</p><p>20 Figure 1: Diagrammatic representation of the model structure and the message passing process. [sent-68, score-0.461]
</p><p>21 The posterior P (A|X, Z) cannot be computed in closed form; however, a mean-ﬁeld variational posterior in which we approximate P (A|X, Z) as product K,D of independent Bernoulli variables k,d qkd (akd ) can be readily derived. [sent-74, score-0.166]
</p><p>22 3  Parallel Inference  We describe both synchronous and asynchronous procedures for approximate, parallel inference in the IBP that combines MCMC with message passing. [sent-75, score-0.74]
</p><p>23 We ﬁrst partition the data among the processors, using X p to denote the subset of observations X assigned to processor p. [sent-76, score-0.337]
</p><p>24 We use Z p to denote the latent features associated with the data on processor p. [sent-77, score-0.442]
</p><p>25 In [4], the distribution P (A|X−n , Z−n ) was used to derive an accelerated sampler for sampling Zn , where n indexes the nth observation and −n is the set of all observations except n. [sent-78, score-0.303]
</p><p>26 In our parallel inference approach, each processor p maintains a distribution P p (A|X−n , Z−n ), a local approximation to P (A|X−n , Z−n ). [sent-79, score-0.54]
</p><p>27 The distributions P p are updated via message passing between the processors. [sent-80, score-0.461]
</p><p>28 The inference alternates between three steps: • Message passing: processors communicate to compute the exact P (A|X, Z). [sent-81, score-0.532]
</p><p>29 • Gibbs sampling: processors sample a new set of Z p ’s in parallel. [sent-82, score-0.362]
</p><p>30 • Hyperparameter sampling: a root processor resamples global hyperparameters The sampler is approximate because during Gibbs sampling, all processors resample elements of Z at the same time; their posteriors P p (A|X, Z) are no longer the true P (A|X, Z). [sent-83, score-1.101]
</p><p>31 The most straightforward way to compute the full posterior is to arrange processors in a tree architecture, as belief propagation is then exact. [sent-89, score-0.399]
</p><p>32 The message s from processor p to processor q is: sp→q = lp +  sr→p r∈N (p)\q  3  where N (p)\q are the processors attached to p besides q and lp are the sufﬁcient statistics from processor p. [sent-90, score-1.646]
</p><p>33 Also passed are the feature counts mp = n∈X p Znk , the popularity of k feature k within processor p. [sent-92, score-0.545]
</p><p>34 At the beginning of the Gibbs sampling stage, each processor has the correct values of mk . [sent-96, score-0.463]
</p><p>35 We compute m−p = mk − mp , and, as the processor’s internal feature counts mp are k k k updated, approximate mk ≈ m−p + mp . [sent-97, score-0.405]
</p><p>36 1 For non-conjugate models, we can use an exponential family distribution Q(A) to approximate P (A|X, Z) during message passing. [sent-101, score-0.324]
</p><p>37 The processors each compute the likelihood of the current and proposed hyperparameter values and propagate this value back to root. [sent-110, score-0.457]
</p><p>38 The two-step approach introduces a latency in the resampling but does not require any additional message passing rounds. [sent-112, score-0.515]
</p><p>39 Asynchronous Operation So far we have discussed message passing, Gibbs sampling, and hyperparameter resampling as if they occur in separate phases. [sent-113, score-0.381]
</p><p>40 In practice, these phases may occur asynchronously: between its Gibbs sweeps, each processor updates its feature posterior based on the most current messages it has received and sends likelihood messages to its parent. [sent-114, score-0.615]
</p><p>41 Likewise, the root continuously resamples hyperparameters and propagates the values down through the tree. [sent-115, score-0.155]
</p><p>42 While another layer of approximation, this asynchronous form of message passing allows faster processors to share information and perform more inference on their data instead of waiting for slower processors. [sent-116, score-1.15]
</p><p>43 Implementation Note When performing parallel inference in the IBP, a few factors need to be considered with care. [sent-117, score-0.203]
</p><p>44 Other parallel inference for nonparametric models, such as the HDP [2], simply matched features by their index, that is, assumed that the ith feature on processor p was also the ith feature on processor q. [sent-118, score-1.114]
</p><p>45 4  4  Comparison to Exact Metropolis  Because all Z p ’s are sampled at once, the posteriors P p (A|X, Z) used by each processor in section 3 are no longer exact. [sent-122, score-0.368]
</p><p>46 Below we show how Metropolis–Hastings (MH) steps can make the parallel sampler exact, but introduce signiﬁcant computational overheads both in computing the transition probabilities and in the message passing. [sent-123, score-0.568]
</p><p>47 We argue that trying to do exact inference is a poor use of computational resources (especially as any ﬁnite chain will not be exact); empirically, the approximate sampler behaves similarly to the MH sampler while ﬁnding higher likelihood regions in the data. [sent-124, score-0.656]
</p><p>48 Ideally, we would simply add an MH accept/reject step after each stage of the approximate inference to make the sampler exact. [sent-126, score-0.388]
</p><p>49 Unfortunately, the approximate sampler makes several non-independent random choices in each stage of the inference, making the reverse proposal inconvenient to compute. [sent-127, score-0.325]
</p><p>50 We circumvent this issue by ﬁxing the random seed, making the initial stage of the approximate sampler a deterministic function, and then add independent random noise to create a proposal distribution. [sent-128, score-0.325]
</p><p>51 The acceptance probability of the proposal is min(1,  P (X|Z ′ )P (Z ′ )Q(Z ′ → Z) ), P (X|Z)P (Z)Q(Z → Z ′ )  (5)  where the likelihood terms P (X|Z) and P (Z) are readily computed in a distributed fashion. [sent-132, score-0.156]
</p><p>52 For ˆ the transition distribution Q, we note that if we set the random seed r, then the matrix Z p from the Gibbs sweeps in the processor is some deterministic function of the input matrix Z p . [sent-133, score-0.468]
</p><p>53 )  To compute the backward probability, we take Z p′ and apply the same number of Gibbs sampling ′ ˆ sweeps with the same random seed r. [sent-139, score-0.204]
</p><p>54 While the transition probabilities can be computed in a distributed, asynchronous fashion, all of the processors must synchronise when deciding whether to accept the proposal. [sent-142, score-0.504]
</p><p>55 Experimental Comparison To compare the exact Metropolis and approximate inference techniques, we ran each inference type on 1000 block images of [3] on 5 simulated processors. [sent-143, score-0.326]
</p><p>56 Each sampler was run for 10,000 iterations with 5 Gibbs sweeps per iteration; statistics were collected from the second half of the chain. [sent-147, score-0.331]
</p><p>57 To keep the probability of an acceptance reasonable, we allowed each processor to change only small parts of its Z p : the feature assignments Zn for 1, 5, or 10 data points each during each sweep. [sent-148, score-0.445]
</p><p>58 In table 1, we see that the approximate sampler runs about ﬁve times faster than the exact samplers while achieving comparable (or better) predictive likelihoods and reconstruction errors on heldout data. [sent-149, score-0.457]
</p><p>59 Both the acceptance rates and the predictive likelihoods fall as the exact sampler tries to take larger steps, suggesting that the difference between the approximate and exact sampler’s performance on predictive likelihood is due to poor mixing by the exact sampler. [sent-150, score-0.652]
</p><p>60 Figure 4 shows 2 empirical CDFs for the number of features k , IBP concentration parameter α, the noise variance σn , 2 and the feature variance σa . [sent-151, score-0.177]
</p><p>61 The approximate sampler (black) produces similar CDFs to the various exact Metropolis samplers (gray) for the variances; the concentration parameter is smaller, but the feature counts are similar to the single-processor case. [sent-152, score-0.528]
</p><p>62 The approximate sampler and the MH samplers for smaller n have similar CDFs; the n = 10 MH sampler’s differing CDF indicates it did not mix in 7500 iterations (reasonable since its acceptance rate was 0. [sent-224, score-0.41]
</p><p>63 5  Analysis of Mixing Properties  We ran a series of experiments on 10,000 36-dimensional block images of [3] to study the effects of various sampler conﬁgurations on running time, performance, and mixing time properties of the sampler. [sent-226, score-0.275]
</p><p>64 Figure 4 shows test log-likelihoods using 1, 7, 31 and 127 parallel processors simulated in software, using 1000 outer iterations with 5 Gibbs inner iterations each. [sent-228, score-0.743]
</p><p>65 The parallel samplers have similar test likelihoods as the serial algorithm with signiﬁcant savings in running time. [sent-229, score-0.367]
</p><p>66 The hairiness index, based on the method of CUSUM for monitoring MCMC convergence [9, 10], monitors how often the derivatives of sampler statistics—in our case, the number of features, the test likelihood, and α—change in sign; infrequent changes in sign indicate that the sampler may not be mixed. [sent-234, score-0.544]
</p><p>67 Finally, we considered the trade-off between mixing and running time as the number of outer iterations and inner Gibbs iterations are varied. [sent-237, score-0.331]
</p><p>68 Each combination of inner and outer iterations was set so that the total number of Gibbs sweeps through the data was 5000. [sent-238, score-0.281]
</p><p>69 Mixing efﬁciency was Processors = 1  Test Loglikelihood for inner = 5 and outer = 1000 iterations  Processors = 7  0. [sent-239, score-0.2]
</p><p>70 5 0 20 40 60 80100  Figure 4: Change in likelihood for various numbers of processors over the simulation time. [sent-246, score-0.403]
</p><p>71 The corresponding hairiness index plots are shown on the left. [sent-247, score-0.163]
</p><p>72 Table 2: Test log-likelihoods on real-world datasets for the serial, synchronous and asynchronous inference types. [sent-256, score-0.415]
</p><p>73 Running time for Gibbs sampling was taken to be the time required by the slowest processor (since all processors must synchronize before message passing); the total time reﬂected the Gibbs time and the message-passing time. [sent-265, score-1.045]
</p><p>74 As seen in ﬁgure 5, completing fewer inner Gibbs iterations per outer iteration results in faster mixing, which is sensible as the processors are communicating about their data more often. [sent-266, score-0.562]
</p><p>75 However, having fewer inner iterations requires more frequent message passing; as the number of processors becomes large, the cost of message passing becomes a limiting factor. [sent-267, score-1.204]
</p><p>76 2  6  Real-world Experiments  We tested our parallel scheme on three real world datasets on a 16 node cluster using the Matlab Distributed Computing Engine, using 3 inner Gibbs iterations per outer iteration. [sent-268, score-0.344]
</p><p>77 The piano dataset [12] consisted of 57,931 samples from a 161-dimensional short-time discrete Fourier transform of a piano piece. [sent-271, score-0.23]
</p><p>78 In the faces and music datasets, the Gibbs time per iteration improved almost linearly as the number of processors increased (ﬁgure 6). [sent-276, score-0.441]
</p><p>79 Meanwhile, the message passing time remained small even with 16 processors—7% of the Gibbs time for the faces data and 0. [sent-278, score-0.497]
</p><p>80 However, waiting for synchronisation became a signiﬁcant factor in the synchronous sampler. [sent-280, score-0.202]
</p><p>81 Figure 6(c) compares the times for running inference serially, synchronously and asynchronously with 16 processors. [sent-281, score-0.173]
</p><p>82 The 2 We believe part of the timing results may be an artifact, as the simulation overestimates the message passing time. [sent-282, score-0.504]
</p><p>83 In the actual parallel system (section 6), the cost of message passing was negligible. [sent-283, score-0.559]
</p><p>84 7  7  1200  100 80 60 40 20 0  1  2 4 8 16 number of processors  (a) Timing analysis for faces dataset  −1. [sent-284, score-0.428]
</p><p>85 8  sampling waiting  1000  x 10  −2  800 log joint  sampling waiting  mean time per iteration/s  mean time per outer iteration/s  120  600 400  −2. [sent-285, score-0.398]
</p><p>86 2  1  2 4 8 16 number of processors  (b) Timing analysis for music dataset  −2. [sent-288, score-0.435]
</p><p>87 8 −2 10  0  2  10  10  4  10  time/s  (c) Timing comparison for different approaches  Figure 6: Bar charts comparing sampling time and waiting times for synchronous parallel inference. [sent-289, score-0.373]
</p><p>88 7  Discussion and Conclusion  As datasets grow, parallelisation is an increasingly attractive and important feature for doing inference. [sent-294, score-0.27]
</p><p>89 Not only does it allow multiple processors/multicore technologies to be leveraged for largescale analyses, but it also reduces the amount of data and associated structures that each processor needs to keep in memory. [sent-295, score-0.337]
</p><p>90 Existing work has focused both on general techniques to efﬁciently split variables across processors in undirected graphical models [14] and factor graphs [15] and speciﬁc models such as LDA [16, 17]. [sent-296, score-0.362]
</p><p>91 Speciﬁcally, we describe a parallel inference procedure that allows nonparametric Bayesian models based on the Indian Buffet Process to be applied to large datasets. [sent-298, score-0.257]
</p><p>92 The IBP poses speciﬁc challenges to data parallelisation in that the dimensionality of the representation changes during inference and may be unbounded. [sent-299, score-0.265]
</p><p>93 Our sampler is approximate, and we show that in conjugate models, it behaves similarly to an exact sampler—but with much less computational overhead. [sent-302, score-0.299]
</p><p>94 However, as seen in the Bernoulli case, variational message passing for non-conjugate data doesn’t always produce good results if the approximating distribution is a poor match for the true feature posterior. [sent-303, score-0.566]
</p><p>95 Determining when variational message passing is successful is an interesting question for future work. [sent-304, score-0.502]
</p><p>96 Other interesting directions include approaches for dynamically optimising the network topology (for example, slower processors could be moved lower in the tree). [sent-305, score-0.362]
</p><p>97 Finally, we note that a middle ground between synchronous and asynchronous operations as we presented them might be a system that gives each processor a certain amount of time, instead of a certain number of iterations, to do Gibbs sweeps. [sent-306, score-0.601]
</p><p>98 Ghahramani, “Inﬁnite latent feature models and the Indian buffet process,” in Advances in Neural Information Processing Systems, vol. [sent-323, score-0.216]
</p><p>99 Ghahramani, “Accelerated inference for the Indian buffet process,” in International Conference on Machine Learning, 2009. [sent-327, score-0.207]
</p><p>100 Teh, “Variational inference for the Indian buffet process,” in Proceedings of the Intl. [sent-356, score-0.207]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('processors', 0.362), ('processor', 0.337), ('ibp', 0.278), ('message', 0.273), ('znk', 0.245), ('sampler', 0.197), ('passing', 0.188), ('gibbs', 0.164), ('parallelisation', 0.16), ('mh', 0.149), ('asynchronous', 0.142), ('synchronous', 0.122), ('hairiness', 0.12), ('inference', 0.105), ('metropolis', 0.105), ('buffet', 0.102), ('piano', 0.1), ('parallel', 0.098), ('outer', 0.092), ('proc', 0.091), ('indian', 0.082), ('sweeps', 0.081), ('waiting', 0.08), ('cdfs', 0.08), ('cumilative', 0.08), ('likelihoods', 0.079), ('sampling', 0.073), ('cdf', 0.071), ('ste', 0.07), ('exact', 0.065), ('samplers', 0.065), ('sta', 0.064), ('feature', 0.064), ('serial', 0.062), ('bernoulli', 0.061), ('tic', 0.06), ('akd', 0.06), ('root', 0.058), ('concentration', 0.058), ('features', 0.055), ('inner', 0.055), ('hyperparameter', 0.054), ('nonparametric', 0.054), ('resampling', 0.054), ('mk', 0.053), ('iterations', 0.053), ('unbounded', 0.052), ('messages', 0.052), ('mp', 0.052), ('approximate', 0.051), ('seed', 0.05), ('po', 0.05), ('latent', 0.05), ('multicore', 0.048), ('customer', 0.047), ('datasets', 0.046), ('mixing', 0.045), ('cartoon', 0.045), ('acceptance', 0.044), ('music', 0.043), ('timing', 0.043), ('index', 0.043), ('proposal', 0.042), ('variational', 0.041), ('likelihood', 0.041), ('zn', 0.039), ('windows', 0.038), ('ghahramani', 0.038), ('bayesian', 0.038), ('posterior', 0.037), ('conjugate', 0.037), ('customers', 0.036), ('faces', 0.036), ('welling', 0.035), ('asynchronously', 0.035), ('stage', 0.035), ('hyperparameters', 0.035), ('running', 0.033), ('accelerated', 0.033), ('mcmc', 0.033), ('zp', 0.032), ('loglikelihood', 0.032), ('sends', 0.032), ('propagated', 0.032), ('propagates', 0.032), ('numbered', 0.032), ('uk', 0.032), ('posteriors', 0.031), ('dataset', 0.03), ('finale', 0.03), ('possessing', 0.03), ('rio', 0.03), ('resamples', 0.03), ('za', 0.03), ('test', 0.03), ('distributed', 0.029), ('flickr', 0.028), ('leaky', 0.028), ('dishes', 0.028), ('counts', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="123-tfidf-1" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>2 0.30329752 <a title="123-tfidf-2" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>Author: Douglas Eck, Yoshua Bengio, Aaron C. Courville</p><p>Abstract: The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an inﬁnite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer deﬁnes a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment. 1</p><p>3 0.16417883 <a title="123-tfidf-3" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Michael I. Jordan, Emily B. Fox</p><p>Abstract: We propose a Bayesian nonparametric approach to the problem of modeling related time series. Using a beta process prior, our approach is based on the discovery of a set of latent dynamical behaviors that are shared among multiple time series. The size of the set and the sharing pattern are both inferred from data. We develop an efﬁcient Markov chain Monte Carlo inference method that is based on the Indian buffet process representation of the predictive distribution of the beta process. In particular, our approach uses the sum-product algorithm to efﬁciently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals. We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.</p><p>4 0.15945219 <a title="123-tfidf-4" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>Author: Kurt Miller, Michael I. Jordan, Thomas L. Griffiths</p><p>Abstract: As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets. 1</p><p>5 0.15477562 <a title="123-tfidf-5" href="./nips-2009-Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units.html">186 nips-2009-Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units</a></p>
<p>Author: Feng Yan, Ningyi Xu, Yuan Qi</p><p>Abstract: The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. This partitioning scheme also balances the computational cost on each multiprocessor and enables us to easily avoid memory access conﬂicts. We use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors. The proposed partitioning scheme and data streaming make our approach scalable with more multiprocessors. Furthermore, they can be used as general techniques to parallelize other machine learning models. 1</p><p>6 0.13946138 <a title="123-tfidf-6" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>7 0.12507637 <a title="123-tfidf-7" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>8 0.12155935 <a title="123-tfidf-8" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>9 0.11206143 <a title="123-tfidf-9" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>10 0.10240141 <a title="123-tfidf-10" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>11 0.10188527 <a title="123-tfidf-11" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>12 0.10060892 <a title="123-tfidf-12" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>13 0.082733847 <a title="123-tfidf-13" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>14 0.082089193 <a title="123-tfidf-14" href="./nips-2009-Randomized_Pruning%3A_Efficiently_Calculating_Expectations_in_Large_Dynamic_Programs.html">197 nips-2009-Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs</a></p>
<p>15 0.080169864 <a title="123-tfidf-15" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>16 0.076797545 <a title="123-tfidf-16" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<p>17 0.072127938 <a title="123-tfidf-17" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>18 0.070217095 <a title="123-tfidf-18" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>19 0.069641314 <a title="123-tfidf-19" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>20 0.067744963 <a title="123-tfidf-20" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, -0.088), (2, -0.024), (3, -0.168), (4, 0.132), (5, -0.24), (6, 0.139), (7, 0.038), (8, -0.054), (9, -0.057), (10, -0.025), (11, 0.016), (12, -0.073), (13, -0.02), (14, -0.212), (15, 0.024), (16, 0.135), (17, -0.026), (18, 0.172), (19, 0.084), (20, -0.052), (21, 0.016), (22, 0.092), (23, 0.028), (24, 0.129), (25, -0.08), (26, -0.103), (27, -0.017), (28, -0.057), (29, 0.079), (30, -0.099), (31, -0.038), (32, 0.073), (33, -0.081), (34, -0.024), (35, 0.077), (36, 0.012), (37, -0.021), (38, -0.105), (39, 0.159), (40, -0.026), (41, 0.04), (42, 0.052), (43, -0.008), (44, -0.061), (45, -0.021), (46, -0.047), (47, -0.037), (48, -0.019), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94270241 <a title="123-lsi-1" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>2 0.85954499 <a title="123-lsi-2" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>Author: Douglas Eck, Yoshua Bengio, Aaron C. Courville</p><p>Abstract: The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an inﬁnite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer deﬁnes a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment. 1</p><p>3 0.74959499 <a title="123-lsi-3" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<p>Author: Yee W. Teh, Dilan Gorur</p><p>Abstract: The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the stable-beta process and deriving the IBP corresponding to it. We ﬁnd interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and ﬁnd that our power-law IBP is a good model for word occurrences in document corpora. 1</p><p>4 0.61637759 <a title="123-lsi-4" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>Author: Kurt Miller, Michael I. Jordan, Thomas L. Griffiths</p><p>Abstract: As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets. 1</p><p>5 0.60303575 <a title="123-lsi-5" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Michael I. Jordan, Emily B. Fox</p><p>Abstract: We propose a Bayesian nonparametric approach to the problem of modeling related time series. Using a beta process prior, our approach is based on the discovery of a set of latent dynamical behaviors that are shared among multiple time series. The size of the set and the sharing pattern are both inferred from data. We develop an efﬁcient Markov chain Monte Carlo inference method that is based on the Indian buffet process representation of the predictive distribution of the beta process. In particular, our approach uses the sum-product algorithm to efﬁciently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals. We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.</p><p>6 0.58863705 <a title="123-lsi-6" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>7 0.55318779 <a title="123-lsi-7" href="./nips-2009-Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units.html">186 nips-2009-Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units</a></p>
<p>8 0.52857286 <a title="123-lsi-8" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>9 0.43462631 <a title="123-lsi-9" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>10 0.42599609 <a title="123-lsi-10" href="./nips-2009-Construction_of_Nonparametric_Bayesian_Models_from_Parametric_Bayes_Equations.html">59 nips-2009-Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></p>
<p>11 0.41734934 <a title="123-lsi-11" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>12 0.41340354 <a title="123-lsi-12" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>13 0.38935491 <a title="123-lsi-13" href="./nips-2009-Breaking_Boundaries_Between_Induction_Time_and_Diagnosis_Time_Active_Information_Acquisition.html">49 nips-2009-Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition</a></p>
<p>14 0.3880415 <a title="123-lsi-14" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>15 0.38782844 <a title="123-lsi-15" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>16 0.38239747 <a title="123-lsi-16" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>17 0.37943357 <a title="123-lsi-17" href="./nips-2009-Nonparametric_Bayesian_Models_for_Unsupervised_Event_Coreference_Resolution.html">171 nips-2009-Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></p>
<p>18 0.3712278 <a title="123-lsi-18" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>19 0.36871856 <a title="123-lsi-19" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>20 0.36087075 <a title="123-lsi-20" href="./nips-2009-Bayesian_Belief_Polarization.html">39 nips-2009-Bayesian Belief Polarization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.011), (21, 0.011), (24, 0.038), (25, 0.05), (35, 0.053), (36, 0.118), (39, 0.042), (58, 0.062), (61, 0.03), (64, 0.267), (66, 0.047), (71, 0.075), (81, 0.037), (86, 0.061), (91, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80444682 <a title="123-lda-1" href="./nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></p>
<p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><p>2 0.72337651 <a title="123-lda-2" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>Author: Francois Caron, Arnaud Doucet</p><p>Abstract: Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. These models are attractive because they ensure exchangeability (over samples). We propose here extensions of these models where the dependency between samples is given by a known decomposable graph. These models have appealing properties and can be easily learned using Monte Carlo techniques. 1 Motivation The CRP and IBP have found numerous applications in machine learning over recent years [5, 10]. We consider here the case where the data we are interested in are ‘locally’ dependent; these dependencies being represented by a known graph G where each data point/object is associated to a vertex. These local dependencies can correspond to any conceptual or real (e.g. space, time) metric. For example, in the context of clustering, we might want to propose a prior distribution on partitions enforcing that data which are ‘close’ in the graph are more likely to be in the same cluster. Similarly, in the context of latent feature models, we might be interested in a prior distribution on features enforcing that data which are ‘close’ in the graph are more likely to possess similar features. The ‘standard’ CRP and IBP correspond to the case where the graph G is complete; that is it is fully connected. In this paper, we generalize the CRP and IBP to decomposable graphs. The resulting generalized versions of the CRP and IBP enjoy attractive properties. Each clique of the graph follows marginally a CRP or an IBP process and explicit expressions for the joint prior distribution on the graph is available. It makes it easy to learn those models using straightforward generalizations of Markov chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) algorithms proposed to perform inference for the CRP and IBP [5, 10, 14]. The rest of the paper is organized as follows. In Section 2, we review the popular Dirichlet multinomial allocation model and the Dirichlet Process (DP) partition distribution. We propose an extension of these two models to decomposable graphical models. In Section 3 we discuss nonparametric latent feature models, reviewing brieﬂy the construction in [5] and extending it to decomposable graphs. We demonstrate these models in Section 4 on two applications: an alternative to the hierarchical DP model [12] and a time-varying matrix factorization problem. 2 Prior distributions for partitions on decomposable graphs Assume we have n observations. When performing clustering, we associate to each of this observation an allocation variable zi ∈ [K] = {1, . . . , K}. Let Πn be the partition of [n] = {1, . . . , n} deﬁned by the equivalence relation i ↔ j ⇔ zi = zj . The resulting partition Πn = {A1 , . . . , An(Πn ) } 1 is an unordered collection of disjoint non-empty subsets Aj of [n], j = 1, . . . , n(Πn ), where ∪j Aj = [n] and n(Πn ) is the number of subsets for partition Πn . We also denote by Pn be the set of all partitions of [n] and let nj , j = 1, . . . , n(Πn ), be the size of the subset Aj . Each allocation variable zi is associated to a vertex/site of an undirected graph G, which is assumed to be known. In the standard case where the graph G is complete, we ﬁrst review brieﬂy here two popular prior distributions on z1:n , equivalently on Πn . We then extend these models to undirected decomposable graphs; see [2, 8] for an introduction to decomposable graphs. Finally we brieﬂy discuss the directed case. Note that the models proposed here are completely different from the hyper multinomial-Dirichlet in [2] and its recent DP extension [6]. 2.1 Dirichlet multinomial allocation model and DP partition distribution Assume for the time being that K is ﬁnite. When the graph is complete, a popular choice for the allocation variables is to consider a Dirichlet multinomial allocation model [11] θ θ , . . . , ), zi |π ∼ π (1) K K where D is the standard Dirichlet distribution and θ > 0. Integrating out π, we obtain the following Dirichlet multinomial prior distribution π ∼ D( Pr(z1:n ) = K j=1 Γ(θ) Γ(nj + θ K) (2) θ Γ(θ + n)Γ( K )K and then, using the straightforward equality Pr(Πn ) = PK where PK = {Πn ∈ Pn |n(Πn ) ≤ K}, we obtain K! (K−n(Πn ))! Pr(z1:n ) valid for for all Πn ∈ n(Π ) Pr(Πn ) = θ Γ(θ) j=1n Γ(nj + K ) K! . θ (K − n(Πn ))! Γ(θ + n)Γ( K )n(Πn ) (3) DP may be seen as a generalization of the Dirichlet multinomial model when the number of components K → ∞; see for example [10]. In this case the distribution over the partition Πn of [n] is given by [11] n(Π ) θn(Πn ) j=1n Γ(nj ) . (4) Pr(Πn ) = n i=1 (θ + i − 1) Let Π−k = {A1,−k , . . . , An(Π−k ),−k } be the partition induced by removing item k to Πn and nj,−k be the size of cluster j for j = 1, . . . , n(Π−k ). It follows from (4) that an item k is assigned to an existing cluster j, j = 1, . . . , n(Π−k ), with probability proportional to nj,−k / (n − 1 + θ) and forms a new cluster with probability θ/ (n − 1 + θ). This property is the basis of the CRP. We now extend the Dirichlet multinomial allocation and the DP partition distribution models to decomposable graphs. 2.2 Markov combination of Dirichlet multinomial and DP partition distributions Let G be a decomposable undirected graph, C = {C1 , . . . , Cp } a perfect ordering of the cliques and S = {S2 , . . . , Cp } the associated separators. It can be easily checked that if the marginal distribution of zC for each clique C ∈ C is deﬁned by (2) then these distributions are consistent as they yield the same distribution (2) over the separators. Therefore, the unique Markov distribution over G with Dirichlet multinomial distribution over the cliques is deﬁned by [8] Pr(zC ) S∈S Pr(zS ) C∈C Pr(z1:n ) = (5) where for each complete set B ⊆ G, we have Pr(zB ) given by (2). It follows that we have for any Πn ∈ PK Γ(θ) K! Pr(Πn ) = (K − n(Πn ))! C∈C Γ(θ) S∈S 2 K j=1 θ Γ(nj,C + K ) θ Γ(θ+nC )Γ( K )K K j=1 θ Γ(nj,S + K ) θ Γ(θ+nS )Γ( K )K (6) where for each complete set B ⊆ G, nj,B is the number of items associated to cluster j, j = 1, . . . , K in B and nB is the total number of items in B. Within each complete set B, the allocation variables deﬁne a partition distributed according to the Dirichlet-multinomial distribution. We now extend this approach to DP partition distributions; that is we derive a joint distribution over Πn such that the distribution of ΠB over each complete set B of the graph is given by (4) with θ > 0. Such a distribution satisﬁes the consistency condition over the separators as the restriction of any partition distributed according to (4) still follows (4) [7]. G Proposition. Let Pn be the set of partitions Πn ∈ Pn such that for each decomposition A, B, and any (i, j) ∈ A × B, i ↔ j ⇒ ∃k ∈ A ∩ B such that k ↔ i ↔ j. As K → ∞, the prior distribution G over partitions (6) is given for each Πn ∈ Pn by Pr(Πn ) = θn(Πn ) n(ΠC ) Γ(nj,C ) j=1 nC i=1 (θ+i−1) n(ΠS ) Γ(nj,S ) j=1 nS (θ+i−1) i=1 C∈C S∈S (7) where n(ΠB ) is the number of clusters in the complete set B. Proof. From (6), we have θ n(ΠC ) K(K − 1) . . . (K − n(Πn ) + 1) Pr(Πn ) = K C∈C n(ΠC )− S∈S n(ΠS ) C∈C θ n(ΠS ) S∈S n(ΠC ) θ Γ(nj,C + K ) j=1 nC (θ+i−1) i=1 n(ΠS ) θ Γ(nj,S + K ) j=1 nS (θ+i−1) i=1 Thus when K → ∞, we obtain (7) if n(Πn ) = C∈C n(ΠC ) − S∈S n(ΠS ) and 0 otherwise. We have n(Πn ) ≤ C∈C n(ΠC ) − S∈S n(ΠS ) for any Πn ∈ Pn and the subset of Pn verifying G n(Πn ) = C∈C n(ΠC ) − S∈S n(ΠS ) corresponds to the set Pn . Example. Let the notation i ∼ j (resp. i j) indicates an edge (resp. no edge) between two sites. Let n = 3 and G be the decomposable graph deﬁned by the relations 1 ∼ 2, 2 ∼ 3 and 1 3. G The set P3 is then equal to {{{1, 2, 3}}; {{1, 2}, {3}}; {{1}, {2, 3}}; {{1}, {2}, {3}}}. Note that G the partition {{1, 3}, {2}} does not belong to P3 . Indeed, as there is no edge between 1 and 3, they cannot be in the same cluster if 2 is in another cluster. The cliques are C1 = {1, 2} and C2 = {2, 3} Pr(ΠC1 ) Pr(ΠC2 ) hence we can and the separator is S2 = {2}. The distribution is given by Pr(Π3 ) = Pr(ΠS ) 2 check that we obtain Pr({1, 2, 3}) = (θ + 1)−2 , Pr({1, 2}, {3}) = Pr({1, 2}, {3}) = θ(θ + 1)−2 and Pr({1}, {2}, {3}) = θ2 (θ + 1)−2 . Let now deﬁne the full conditional distributions. Based on (7) the conditional assignment of an item k is proportional to the conditional over the cliques divided by the conditional over the separators. G Let denote G−k the undirected graph obtained by removing vertex k from G. Suppose that Πn ∈ Pn . G−k If Π−k ∈ Pn−1 , then do not change the value of item k. Otherwise, item k is assigned to cluster j / where j = 1, . . . , n(Π−k ) with probability proportional to {C∈C|n−k,j,C >0} n−k,j,C {S∈S|n−k,j,S >0} n−k,j,S (8) and to a new cluster with probability proportional to θ, where n−k,j,C is the number of items in the set C \ {k} belonging to cluster j. The updating process is illustrated by the Chinese wedding party process1 in Fig. 1. The results of this section can be extended to the Pitman-Yor process, and more generally to species sampling models. Example (continuing). Given Π−2 = {A1 = {1}, A2 = {3}}, we have −1 Pr( item 2 assigned to A1 = {1}| Π−2 ) = Pr( item 2 assigned to A2 = {3}| Π−2 ) = (θ + 2) −1 and Pr( item 2 assigned to new cluster A3 | Π−2 ) = θ (θ + 2) . Given Π−2 = {A1 = {1, 3}}, item 2 is assigned to A1 with probability 1. 1 Note that this representation describes the full conditionals while the CRP represents the sequential updat- ing. 3 (a) (b) (d) (c) (e) Figure 1: Chinese wedding party. Consider a group of n guests attending a wedding party. Each of the n guests may belong to one or several cliques, i.e. maximal groups of people such that everybody knows everybody. The belonging of each guest to the different cliques is represented by color patches on the ﬁgures, and the graphical representation of the relationship between the guests is represented by the graphical model (e). (a) Suppose that the guests are already seated such that two guests cannot be together at the same table is they are not part of the same clique, or if there does not exist a group of other guests such that they are related (“Any friend of yours is a friend of mine”). (b) The guest number k leaves his table and either (c) joins a table where there are guests from the same clique as him, with probability proportional to the product of the number of guests from each clique over the product of the number of guests belonging to several cliques on that table or (d) he joins a new table with probability proportional to θ. 2.3 Monte Carlo inference 2.3.1 MCMC algorithm Using the full conditionals, a single site Gibbs sampler can easily be designed to approximate the posterior distribution Pr(Πn |z1:n ). Given a partition Πn , an item k is taken out of the partition. If G−k Π−k ∈ Pn−1 , item k keeps the same value. Otherwise, the item will be assigned to a cluster j, / j = 1, . . . , n(Π−k ), with probability proportional to p(z{k}∪Aj,−k ) × p(zAj,−k ) {C∈C|n−k,j,C >0} n−k,j,C {S∈S|n−k,j,S >0} n−k,j,S (9) and the item will be assigned to a new cluster with probability proportional to p(z{k} ) × θ. Similarly to [3], we can also deﬁne a procedure to sample from p(θ|n(Πn ) = k)). We assume that θ ∼ G(a, b) and use p auxiliary variables x1 , . . . , xp . The procedure is as follows. • For j = 1, . . . , p, sample xj |k, θ ∼ Beta(θ + nSj , nCj − nSj ) • Sample θ|k, x1:p ∼ G(a + k, b − j log xj ) 2.3.2 Sequential Monte Carlo We have so far only treated the case of an undirected decomposable graph G. We can formulate a sequential updating rule for the corresponding perfect directed version D of G. Indeed, let (a1 , . . . a|V | ) be a perfect ordering and pa(ak ) be the set of parents of ak which is by deﬁnition complete. Let Πk−1 = {A1,k−1 , . . . , An(Πk−1 ),k−1 } denote the partition of the ﬁrst k−1 vertices a1:k−1 and let nj,pa(ak ) be the number of elements with value j in the set pa(ak ), j = 1, . . . , n(Πk−1 ). Then the vertex ak joins the set j with probability nj,pa(ak ) / θ + cluster with probability θ/ θ + q q nq,pa(ak ) and creates a new nq,pa(ak ) . One can then design a particle ﬁlter/SMC method in a similar fashion as [4]. Consider a set of (i) (i) (i) (i) N N particles Πk−1 with weights wk−1 ∝ Pr(Πk−1 , z1:k−1 ) ( i=1 wk−1 = 1) that approximate (i) the posterior distribution Pr(Πk−1 |z1:k−1 ). For each particle i, there are n(Πk−1 ) + 1 possible 4 (i,j) allocations for component ak . We denote Πk the partition obtained by associating component ak (i,j) to cluster j. The weight associated to Πk is given by  nj,pa(ak ) (i)  if j = 1, . . . , n(Πk−1 ) θ+ q nq,pa(ak ) (i,j) (i) p(z{ak }∪Aj,k−1 ) wk−1 = wk−1 × (10) (i) θ  θ+ n p(zAj,k−1 ) if j = n(Πk−1 ) + 1 q q,pa(ak ) (i,j) Then we can perform a deterministic resampling step by keeping the N particles Πk with highest (i,j) (i) (i) weights wk−1 . Let Πk be the resampled particles and wk the associated normalized weights. 3 Prior distributions for inﬁnite binary matrices on decomposable graphs Assume we have n objects; each of these objects being associated to the vertex of a graph G. To K each object is associated a K-dimensional binary vector zn = (zn,1 , . . . , zn,K ) ∈ {0, 1} where zn,i = 1 if object n possesses feature i and zn,i = 0 otherwise. These vectors zt form a binary n × K matrix denoted Z1:n . We denote by ξ1:n the associated equivalence class of left-ordered matrices and let EK be the set of left-ordered matrices with at most K features. In the standard case where the graph G is complete, we review brieﬂy here two popular prior distributions on Z1:n , equivalently on ξ1:n : the Beta-Bernoulli model and the IBP [5]. We then extend these models to undirected decomposable graphs. This can be used for example to deﬁne a time-varying IBP as illustrated in Section 4. 3.1 Beta-Bernoulli and IBP distributions The Beta-Bernoulli distribution over the allocation Z1:n is K Pr(Z1:n ) = α + K )Γ(n − nj + 1) α Γ(n + 1 + K ) α K Γ(nj j=1 (11) where nj is the number of objects having feature j. It follows that Pr(ξ1:n ) = K K! 2n −1 h=0 α K Γ(nj α + K )Γ(n − nj + 1) α Γ(n + 1 + K ) Kh ! j=1 (12) where Kh is the number of features possessing the history h (see [5] for details). The nonparametric model is obtained by taking the limit when K → ∞ Pr(ξ1:n ) = αK K+ + 2n −1 h=1 Kh ! exp(−αHn ) where K + is the total number of features and Hn = 3.2 (n − nj )!(nj − 1)! n! j=1 n 1 k=1 k . (13) The IBP follows from (13). Markov combination of Beta-Bernoulli and IBP distributions Let G be a decomposable undirected graph, C = {C1 , . . . , Cp } a perfect ordering of the cliques and S = {S2 , . . . , Cp } the associated separators. As in the Dirichlet-multinomial case, it is easily seen that if for each clique C ∈ C, the marginal distribution is deﬁned by (11), then these distributions are consistent as they yield the same distribution (11) over the separators. Therefore, the unique Markov distribution over G with Beta-Bernoulli distribution over the cliques is deﬁned by [8] Pr(ZC ) S∈S Pr(ZS ) C∈C Pr(Z1:n ) = (14) where Pr(ZB ) given by (11) for each complete set B ⊆ G. The prior over ξ1:n is thus given, for ξ1:n ∈ EK , by Pr(ξ1:n ) = K! 2n −1 h=0 Kh ! α K α Γ(nj,C + K )Γ(nC −nj,C +1) α Γ(nC +1+ K ) α α Γ(nj,S + K )Γ(nS −nj,S +1) K K α j=1 Γ(nS +1+ K ) K j=1 C∈C S∈S 5 (15) where for each complete set B ⊆ G, nj,B is the number of items having feature j, j = 1, . . . , K in the set B and nB is the whole set of objects in set B. Taking the limit when K → ∞, we obtain after a few calculations Pr(ξ1:n ) = α + K[n] exp [−α ( C HnC − 2n −1 h=1 Kh ! HnS )] × C∈C + KC (nC −nj,C )!(nj,C −1)! j=1 nC ! S∈S S + KS (nS −nj,S )!(nj,S −1)! j=1 nS ! + + + + if K[n] = C KC − S KS and 0 otherwise, where KB is the number of different features possessed by objects in B. G Let En be the subset of En such that for each decomposition A, B and any (u, v) ∈ A × B: {u and v possess feature j} ⇒ ∃k ∈ A ∩ B such that {k possesses feature j}. Let ξ−k be the left-ordered + matrix obtained by removing object k from ξn and K−k be the total number of different features in G−k + ξ−k . For each feature j = 1, . . . , K−k , if ξ−k ∈ En−1 then we have   b C∈C nj,C if i = 1 S∈C nj,S Pr(ξk,j = i) = (16)  b C∈C (nC −nj,C ) if i = 0 (nS −nj,S ) S∈C nS where b is the appropriate normalizing constant then the customer k tries Poisson α {S∈S|k∈S} nC {C∈C|k∈C} new dishes. We can easily generalize this construction to a directed version D of G using arguments similar to those presented in Section 2; see Section 4 for an application to time-varying matrix factorization. 4 4.1 Applications Sharing clusters among relative groups: An alternative to HDP Consider that we are given d groups with nj data yi,j in each group, i = 1, . . . , nj , j = 1, . . . , d. We consider latent cluster variables zi,j that deﬁne the partition of the data. We will use alternatively the notation θi,j = Uzi,j in the following. Hierarchical Dirichlet Process [12] (HDP) is a very popular model for sharing clusters among related groups. It is based on a hierarchy of DPs G0 ∼ DP (γ, H), Gj |G0 ∼ DP (α, G0 ) j = 1, . . . d θi,j |Gj ∼ Gj , yi,j |θi,j ∼ f (θi,j ) i = 1, . . . , nj . Under conjugacy assumptions, G0 , Gj and U can be integrated out and we can approximate the marginal posterior of (zi,j ) given y = (yi,j ) with Gibbs sampling using the Chinese restaurant franchise to sample from the full conditional p(zi,j |z−{i,j} , y). Using the graph formulation deﬁned in Section 2, we propose an alternative to HDP. Let θ0,1 , . . . , θ0,N be N auxiliary variables belonging to what we call group 0. We deﬁne each clique Cj (j = 1, . . . , d) to be composed of elements from group j and elements from group 0. This deﬁnes a decomposable graphical model whose separator is given by the elements of group 0. We can rewrite the model in a way quite similar to HDP G0 ∼ DP (α, H), θ0,i |G0 ∼ G0 i = 1, ..., N α α Gj |θ0,1 , . . . , θ0,N ∼ DP (α + N, α+N H + α+N θi,j |Gj ∼ Gj , yi,j |θi,j ∼ f (θi,j ) i = 1, . . . , nj N i=1 δθ0,i ) j = 1, . . . d, N For any subset A and j = k ∈ {1, . . . , p} we have corr(Gj (A), Gk (A)) = α+N . Again, under conjugacy conditions, we can integrate out G0 , Gj and U and approximate the marginal posterior distribution over the partition using the Chinese wedding party process deﬁned in Section 2. Note that for latent variables zi,j , j = 1, . . . , d, associated to data, this is the usual CRP update. As in HDP, multiple layers can be added to the model. Figures 2 (a) and (b) resp. give the graphical DP alternative to HDP and 2-layer HDP. 6 z0 root z0 root corpora docs z1 z2 z1 z2 z3 z1,1 z1,2 z2,1 z2,2 z2,3 docs (a) Graphical DP alternative to HDP (b) Graphical DP alternative to 2-layer HDP Figure 2: Hierarchical Graphs of dependency with (a) one layer and (b) two layers of hierarchy. If N = 0, then Gj ∼ DP (α, H) for all j and this is equivalent to setting γ → ∞ in HDP. If N → ∞ then Gj = G0 for all j, G0 ∼ DP (α, H). This is equivalent to setting α → ∞ in the HDP. One interesting feature of the model is that, contrary to HDP, the marginal distribution of Gj at any layer of the tree is DP (α, H). As a consequence, the total number of clusters scales logarithmically (as in the usual DP) with the size of each group, whereas it scales doubly logarithmically in HDP. Contrary to HDP, there are at most N clusters shared between different groups. Our model is in that sense reminiscent of [9] where only a limited number of clusters can be shared. Note however that contrary to [9] we have a simple CRP-like process. The proposed methodology can be straightforwardly extended to the inﬁnite HMM [12]. The main issue of the proposed model is the setting of the number N of auxiliary parameters. Another issue is that to achieve high correlation, we need a large number of auxiliary variables. Nonetheless, the computational time used to sample from auxiliary variables is negligible compared to the time used for latent variables associated to data. Moreover, it can be easily parallelized. The model proposed offers a far richer framework and ensures that at each level of the tree, the marginal distribution of the partition is given by a DP partition model. 4.2 Time-varying matrix factorization Let X1:n be an observed matrix of dimension n × D. We want to ﬁnd a representation of this matrix in terms of two latent matrices Z1:n of dimension n × K and Y of dimension K × D. Here Z1:n 2 is a binary matrix whereas Y is a matrix of latent features. By assuming that Y ∼ N 0, σY IK×D and 2 X1:n = Z1:n Y + σX εn where εn ∼ N 0, σX In×D , we obtain p(X1:n |Z1:n ) ∝ −D/2 2 2 + Z+T Z+ + σX /σY IKn 1:n 1:n + (n−Kn )D σX exp − + Kn D σY 2 2 + where Σ−1 = I − Z+ Z+T Z+ + σX /σY IKn n 1:n 1:n 1:n −1 1 T −1 2 tr X1:n Σn X1:n 2σX (17) + Z+T , Kn the number of non-zero columns of 1:n + Z1:n and Z+ is the ﬁrst Kn columns of Z1:n . To avoid having to set K, [5, 14] assume that Z1:n 1:n follows an IBP. The resulting posterior distribution p(Z1:n |X1:n ) can be estimated through MCMC [5] or SMC [14]. We consider here a different model where the object Xt is assumed to arrive at time index t and we want a prior distribution on Z1:n ensuring that objects close in time are more likely to possess similar features. To achieve this, we consider the simple directed graphical model D of Fig. 3 where the site numbering corresponds to a time index in that case and a perfect numbering of D is (1, 2, . . .). The set of parents pa(t) is composed of the r preceding sites {{t − r}, . . . , {t − 1}}. The time-varying IBP to sample from p(Z1:n ) associated to this directed graph follows from (16) and proceeds as follows. At time t = 1 + new new • Sample K1 ∼Poisson(α), set z1,i = 1 for i = 1, ..., K1 and set K1 = Knew . At times t = 2, . . . , r n + new ∼Poisson( α ). • For k = 1, . . . Kt , sample zt,k ∼ Ber( 1:t−1,k ) and Kt t t 7   ?  ? - t−r - t−r+1 - . . . - t−1 - t - t+1        6 6 Figure 3: Directed graph. At times t = r + 1, . . . , n n + α new ∼Poisson( r+1 ). • For k = 1, . . . Kt , sample zt,k ∼ Ber( t−r:t−1,k ) and Kt r+1 + Here Kt is the total number of features appearing from time max(1, t − r) to t − 1 and nt−r:t−1,k the restriction of n1:t−1 to the r last customers. Using (17) and the prior distribution of Z1:n which can be sampled using the time-varying IBP described above, we can easily design an SMC method to sample from p(Z1:n |X1:n ). We do not detail it here. Note that contrary to [14], our algorithm does not require inverting a matrix whose dimension grows linearly with the size of the data but only a matrix of dimension r × r. In order to illustrate the model and SMC algorithm, we create 200 6 × 6 images using a ground truth Y consisting of 4 different 6 × 6 latent images. The 200 × 4 binary matrix was generated from Pr(zt,k = 1) = πt,k , where πt = ( .6 .5 0 0 ) if t = 1, . . . , 30, πt = ( .4 .8 .4 0 ) if t = 31, . . . , 50 and πt = ( 0 .3 .6 .6 ) if t = 51, . . . , 200. The order of the model is set to r = 50. The feature occurences Z1:n and true features Y and their estimates are represented in Figure 4. Two spurious features are detected by the model (features 2 and 5 on Fig. 3(c)) but quickly discarded (Fig. 4(d)). The algorithm is able to correctly estimate the varying prior occurences of the features over time. Feature1 Feature2 Feature1 Feature2 Feature3 20 20 40 40 60 60 Feature4 80 100 Feature4 Feature5 Feature6 Time Feature3 Time 80 100 120 120 140 140 160 160 180 200 180 1 2 3 200 4 Feature (a) 1 2 3 4 5 6 Feature (b) (c) (d) Figure 4: (a) True features, (b) True features occurences, (c) MAP estimate ZM AP and (d) associated E[Y|ZM AP ] t=20 t=50 t=20 t=50 t=100 t=200 t=100 t=200 (a) (b) Figure 5: (a) E[Xt |πt , Y] and (b) E[Xt |X1:t−1 ] at t = 20, 50, 100, 200. 5 Related work and Discussion The ﬁxed-lag version of the time-varying DP of Caron et al. [1] is a special case of the proposed model when G is given by Fig. 3. The bivariate DP of Walker and Muliere [13] is also a special case when G has only two cliques. In this paper, we have assumed that the structure of the graph was known beforehand and we have shown that many ﬂexible models arise from this framework. It would be interesting in the future to investigate the case where the graphical structure is unknown and must be estimated from the data. Acknowledgment The authors thank the reviewers for their comments that helped to improve the writing of the paper. 8 References [1] F. Caron, M. Davy, and A. Doucet. Generalized Polya urn for time-varying Dirichlet process mixtures. In Uncertainty in Artiﬁcial Intelligence, 2007. [2] A.P. Dawid and S.L. Lauritzen. Hyper Markov laws in the statistical analysis of decomposable graphical models. The Annals of Statistics, 21:1272–1317, 1993. [3] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995. [4] P. Fearnhead. Particle ﬁlters for mixture models with an unknown number of components. Statistics and Computing, 14:11–21, 2004. [5] T.L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In Advances in Neural Information Processing Systems, 2006. [6] D. Heinz. Building hyper dirichlet processes for graphical models. Electonic Journal of Statistics, 3:290–315, 2009. [7] J.F.C. Kingman. Random partitions in population genetics. Proceedings of the Royal Society of London, 361:1–20, 1978. [8] S.L. Lauritzen. Graphical Models. Oxford University Press, 1996. [9] P. M¨ ller, F. Quintana, and G. Rosner. A method for combining inference across related nonu parametric Bayesian models. Journal of the Royal Statistical Society B, 66:735–749, 2004. [10] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249–265, 2000. [11] J. Pitman. Exchangeable and partially exchangeable random partitions. Probability theory and related ﬁelds, 102:145–158, 1995. [12] Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581, 2006. [13] S. Walker and P. Muliere. A bivariate Dirichlet process. Statistics and Probability Letters, 64:1–7, 2003. [14] F. Wood and T.L. Grifﬁths. Particle ﬁltering for nonparametric Bayesian matrix factorization. In Advances in Neural Information Processing Systems, 2007. 9</p><p>3 0.72210741 <a title="123-lda-3" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>Author: Keith Bush, Joelle Pineau</p><p>Abstract: Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by ﬁrst principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning. We experiment with manifold embeddings to reconstruct the observable state-space in the context of offline, model-based reinforcement learning. We demonstrate that the embedding of a system can change as a result of learning, and we argue that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system. We apply this approach to learn a neurostimulation policy that suppresses epileptic seizures on animal brain slices. 1</p><p>4 0.58484238 <a title="123-lda-4" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>Author: Andrew Frank, Padhraic Smyth, Alexander T. Ihler</p><p>Abstract: Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions deﬁned on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the beneﬁts of these more advanced inference techniques to the continuous domain. 1</p><p>5 0.57593215 <a title="123-lda-5" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>6 0.56753302 <a title="123-lda-6" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>7 0.56497842 <a title="123-lda-7" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>8 0.56252384 <a title="123-lda-8" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>9 0.56150705 <a title="123-lda-9" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>10 0.56111193 <a title="123-lda-10" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>11 0.5593769 <a title="123-lda-11" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<p>12 0.55832946 <a title="123-lda-12" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>13 0.55806887 <a title="123-lda-13" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>14 0.55492806 <a title="123-lda-14" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>15 0.55460072 <a title="123-lda-15" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>16 0.55167335 <a title="123-lda-16" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>17 0.55117804 <a title="123-lda-17" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>18 0.55000532 <a title="123-lda-18" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>19 0.54963952 <a title="123-lda-19" href="./nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></p>
<p>20 0.54749542 <a title="123-lda-20" href="./nips-2009-Indian_Buffet_Processes_with_Power-law_Behavior.html">114 nips-2009-Indian Buffet Processes with Power-law Behavior</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
