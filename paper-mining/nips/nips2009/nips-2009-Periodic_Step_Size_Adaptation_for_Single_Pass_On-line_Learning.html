<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-189" href="#">nips2009-189</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</h1>
<br/><p>Source: <a title="nips-2009-189-pdf" href="http://papers.nips.cc/paper/3702-periodic-step-size-adaptation-for-single-pass-on-line-learning.pdf">pdf</a></p><p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><p>Reference: <a title="nips-2009-189-reference" href="../nips2009_reference/nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i. [sent-2, score-0.112]
</p><p>2 However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. [sent-5, score-0.037]
</p><p>3 Early works concentrate on minimizing the required number of model corrections made by the algorithm through a single pass of training examples. [sent-8, score-0.096]
</p><p>4 More recently, on-line learning is considered as a solution of large scale learning mainly because of its fast convergence property. [sent-9, score-0.023]
</p><p>5 They usually still require several passes (or epochs) through the training examples to converge at a satisfying model. [sent-11, score-0.086]
</p><p>6 Reading a large data set from disk to memory usually takes much longer than CPU time spent in learning. [sent-13, score-0.048]
</p><p>7 That is, after processing all available training examples once, the learned model should generalize as well as possible so that used training example can really be removed from memory to minimize disk I/O time. [sent-15, score-0.104]
</p><p>8 In natural learning, single-pass learning is also interesting because it allows for continual learning from unlimited training examples under the constraint of limited storage, resembling a nature learner. [sent-16, score-0.05]
</p><p>9 Previously, many authors, including [3] and [4], have established that given a sufďŹ ciently large set of training examples, 2SGD can potentially achieve generalization performance as well as empirical optimum in a single pass through the training examples. [sent-17, score-0.16]
</p><p>10 However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. [sent-18, score-0.037]
</p><p>11 Many attempts to approximate the Hessian have been made. [sent-19, score-0.016]
</p><p>12 But in online settings, we only have the surface of the loss function given one training example, as opposed to all in batch settings. [sent-22, score-0.111]
</p><p>13 The search direction obtained by line search on such a surface rarely leads to empirical optimum. [sent-23, score-0.018]
</p><p>14 A review of similar attempts can be found in Bottouâ&euro;&trade;s tutorial [6], where he suggested that none is actually sufďŹ cient to achieve theoretical single-pass performance in practice. [sent-24, score-0.03]
</p><p>15 PSA approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian 1  periodically. [sent-26, score-0.069]
</p><p>16 We analyze the accuracy of the approximation and derive the asymptotic rate of convergence for PSA. [sent-28, score-0.038]
</p><p>17 Experimental results show that for a wide variety of models and tasks, PSA is always very close to empirical optimum in a single-pass. [sent-29, score-0.029]
</p><p>18 A machine learning problem can be formulated as a ďŹ xed-point iteration that solves the equation w = â&bdquo;ł(w), where â&bdquo;ł is a mapping â&bdquo;ł : â&bdquo;? [sent-35, score-0.037]
</p><p>19 Then we can apply Aitkenâ&euro;&trade;s acceleration, which attempts to extrapolate to the local optimum in one step, to accelerate the convergence of the mapping: â&euro;˛  wâ&circ;&mdash; = w(đ? [sent-41, score-0.087]
</p><p>20 &lsquo;Ą) ),  â&circ;&mdash;  (1)  â&circ;&mdash;  where J := â&bdquo;ł (w ) is the Jacobian of the mapping â&bdquo;ł at w . [sent-44, score-0.037]
</p><p>21 &lsquo;&ndash; := eig(J) â&circ;&circ; (â&circ;&rsquo;1, 1), the mapping â&bdquo;ł is guaranteed to converge. [sent-47, score-0.037]
</p><p>22 It is usually difďŹ cult to compute J for even a simple machine learning model. [sent-51, score-0.015]
</p><p>23 (3)  In practice, Aitkenâ&euro;&trade;s acceleration alternates a step for preparing đ? [sent-80, score-0.065]
</p><p>24 A beneďŹ t of the above approximation is that the cost for performing an extrapolation is đ? [sent-87, score-0.037]
</p><p>25 3  Periodic Step-Size Adaptation  When â&bdquo;ł is a gradient descent update rule, that is, â&bdquo;ł(w) â&dagger;? [sent-90, score-0.056]
</p><p>26 &oelig;&sbquo; is a scalar step size, D is the entire set of training examples, and g(w; D) is the gradient of a loss function to be minimized, Aitkenâ&euro;&trade;s acceleration is equivalent to Newtonâ&euro;&trade;s method, because J = â&bdquo;łâ&euro;˛ (w) = I â&circ;&rsquo; đ? [sent-93, score-0.144]
</p><p>27 &lsquo;&rdquo; â&euro;˛ (w; D), the Hessian matrix of the loss function, and the extrapolation given in (1) becomes 1 w = w + (I â&circ;&rsquo; J)â&circ;&rsquo;1 (â&bdquo;ł(w) â&circ;&rsquo; w) = w â&circ;&rsquo; Hâ&circ;&rsquo;1 đ? [sent-98, score-0.059]
</p><p>28 &oelig;&sbquo; In this case, Aitkenâ&euro;&trade;s acceleration enjoys the same local quadratic convergence as Newtonâ&euro;&trade;s method. [sent-101, score-0.071]
</p><p>29 This can also be extended to a SGD update rule: w(đ? [sent-102, score-0.034]
</p><p>30 A genuine on-line learner usually has â&circ;ŁBâ&circ;Ł = 1. [sent-109, score-0.015]
</p><p>31 &rsaquo;ž is an estimated eigenvalue of J as given in (2), when H is a symmetric matrix, its eigenvalue is given by 1 â&circ;&rsquo; eig(J) eig(J) = 1 â&circ;&rsquo; đ? [sent-118, score-0.106]
</p><p>32 &lsquo;&ndash; 2  Therefore, we can update the step size component-wise by (đ? [sent-123, score-0.051]
</p><p>33 &lsquo;&ndash;  (5)  Since the mapping â&bdquo;ł in SGD involves the gradient g(w(đ? [sent-140, score-0.059]
</p><p>34 It is unlikely that we can obtain a reliable eigenvalue estimation at each single iteration. [sent-144, score-0.053]
</p><p>35 To increase stationary of the mapping, we take advantage of the law of large numbers and aggregate consecutive SGD mappings into a new mapping â&bdquo;łđ? [sent-145, score-0.082]
</p><p>36 which reduces the variance of gradient estimation by 1 , compared to the plain SGD mapping â&bdquo;ł. [sent-155, score-0.13]
</p><p>37 We can proceed to estimate the eigenvalues of â&bdquo;łđ? [sent-168, score-0.016]
</p><p>38 (6)  We note that our aggregate mapping â&bdquo;łđ? [sent-197, score-0.062]
</p><p>39 Their difference is similar to that between batch and stochastic gradient descent. [sent-202, score-0.058]
</p><p>40 chances to adjust its search direction, while mappings that use đ? [sent-205, score-0.02]
</p><p>41 With the estimated eigenvalues, we can present the complete update rule to adjust the step size vector đ? [sent-208, score-0.051]
</p><p>42 ) ,  (8)  where v is a discount factor with components deďŹ ned by đ? [sent-243, score-0.015]
</p><p>43 &rsaquo;ź â&permil;¤ 1 ensures that the step size is decreasing and approaches zero so that SGD can be guaranteed to converge [7]. [sent-315, score-0.017]
</p><p>44 In a nutshell, PSA applies SGD with a ďŹ xed step size and periodically updates the step size by approximating Jacobian of the aggregated mapping. [sent-317, score-0.054]
</p><p>45 &lsquo;&lsquo; ) because the cost of eigenvalue estimation given in (6) is 2đ? [sent-320, score-0.053]
</p><p>46 &oelig;&hellip; â&Scaron;ł Equation (10) 3: repeat 4: Choose a small batch B(đ? [sent-352, score-0.036]
</p><p>47 &lsquo;Ą) uniformly at random from the set of training examples D 5: update đ? [sent-353, score-0.084]
</p><p>48 &lsquo;&ndash; be the largest eigenvalue of J such that đ? [sent-483, score-0.053]
</p><p>49 This happens when there are high percentages of missing data for a Bayesian network model trained by EM [8] and when features are uncorrelated for training a conditional random ďŹ eld model [9]. [sent-576, score-0.068]
</p><p>50 The rate of convergence is governed by the largest eigenvalue of S(đ? [sent-620, score-0.076]
</p><p>51 The asymptotic rate of convergence of PSA is bounded by } { (0) â&circ;&rsquo;đ? [sent-625, score-0.023]
</p><p>52 â&ndash;Ą  Though this analysis suggests that for rapid convergence to đ? [sent-725, score-0.023]
</p><p>53 When the training set size â&circ;ŁDâ&circ;Ł â&permil;Ť 2000, set đ? [sent-740, score-0.035]
</p><p>54 This setting implies that the step size will be adjusted per â&circ;ŁDâ&circ;Ł/1000 examples. [sent-744, score-0.017]
</p><p>55 Similarly, we can obtain that the factors for the other two settings are 0. [sent-788, score-0.022]
</p><p>56 5  5  Experimental Results  Table 1 shows the tasks chosen for our comparison. [sent-791, score-0.021]
</p><p>57 The tasks for CRF have been used in competitions and the performance was measured by F-score. [sent-792, score-0.021]
</p><p>58 Target provides the empirical optimal performance achieved by batch learners. [sent-794, score-0.036]
</p><p>59 If PSA accurately approximates 2SGD, then its single-pass performance should be very close to Target. [sent-795, score-0.017]
</p><p>60 99%  Conditional Random Field  We compared PSA with plain SGD and SMD [1] to evaluate PSAâ&euro;&trade;s performance for training conditional random ďŹ elds (CRF). [sent-806, score-0.14]
</p><p>61 We implemented PSA by replacing the L-BFGS optimizer in CRF++ [11]. [sent-807, score-0.02]
</p><p>62 Finally, we ran the original CRF++ with default settings to obtain the performance results of LBFGS. [sent-811, score-0.045]
</p><p>63 We simply used the original parameter settings for SGD and SMD as given in the literature. [sent-812, score-0.022]
</p><p>64 All of the experiments reported here for CRF were ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM. [sent-828, score-0.023]
</p><p>65 Table 2 compares SGD variants in terms of the execution time and F-scores achieved after processing the training examples for a single pass. [sent-829, score-0.064]
</p><p>66 Since the loss function in CRF training is convex, the convergence results of L-BFGS can be considered as the empirical minimum. [sent-830, score-0.08]
</p><p>67 Though as expected, plain SGD is the fastest, it is remarkable that PSA is faster than SMD for all tasks. [sent-834, score-0.071]
</p><p>68 But PSA is still faster than SMD partly because PSA can take advantage of the sparsity trick as plain SGD [15]. [sent-836, score-0.133]
</p><p>69 2  Linear SVM  We also evaluated PSAâ&euro;&trade;s single-pass performance for training linear SVM. [sent-838, score-0.035]
</p><p>70 It is straightforward to apply PSA as a primal optimizer for linear SVM. [sent-839, score-0.036]
</p><p>71 82  Table 2: CPU time in seconds and F-scores achieved after a single pass of CRF training. [sent-883, score-0.094]
</p><p>72 We selected L2-regularized logistic regression as the loss function for PSA and Liblinear because it is twice differentiable. [sent-895, score-0.022]
</p><p>73 PSA (1) is faster than SvmSgd (1) for SVM because SvmSgd uses the sparsity trick [15], which speeds up training for sparse data, but otherwise may slow down. [sent-906, score-0.097]
</p><p>74 We implemented PSA with the sparsity trick for CRF only but not for SVM and CNN. [sent-910, score-0.062]
</p><p>75 Method (pass) Liblinear converge Liblinear (1) SvmSgd (20) SvmSgd (10) SvmSgd (1) PSA (1)  LS FD accuracy time 96. [sent-911, score-0.029]
</p><p>76 33  Table 3: Test accuracy rates and elapsed CPU time in seconds by various linear SVM solvers. [sent-933, score-0.048]
</p><p>77 7  The parameter settings for PSA are basically the same as those for CRF but with a large period đ? [sent-934, score-0.022]
</p><p>78 3  Convolutional Neural Network  Approximating Hessian is particularly challenging when the loss function is non-convex. [sent-946, score-0.022]
</p><p>79 We tested PSA in such a setting by applying PSA to train a large convolutional neural network for the original 10-class MNIST task (see Table 1). [sent-947, score-0.032]
</p><p>80 The differences include that the sub-sampling layers in LeNet-S picks only the upper-left value from a 2 Ă&mdash; 2 area and abandons the other three. [sent-951, score-0.025]
</p><p>81 We also adapted a trick given in [19] which advises that step sizes in the lower layers should be larger than in the higher layer. [sent-973, score-0.104]
</p><p>82 Following their trick, the initial step sizes for the â&circ;&scaron; ďŹ rst and the third layers were 5 and 2. [sent-974, score-0.042]
</p><p>83 The experiments were ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM. [sent-976, score-0.023]
</p><p>84 To obtain the empirical optimal error rate of our LeNet-S model, we ran plain SGD with sufďŹ cient passes and obtained 0. [sent-978, score-0.115]
</p><p>85 Single-pass performance of PSA with the layer trick is within one percentage point to the target. [sent-981, score-0.09]
</p><p>86 Starting from an initial weight closer to the optimum helped improving PSAâ&euro;&trade;s performance further. [sent-982, score-0.029]
</p><p>87 We ran SGD 100 passes with randomly selected 10K training examples then re-started training with PSA using the rest 50K training examples for a single pass. [sent-983, score-0.179]
</p><p>88 Though PSA did achieve a better error rate, this is infeasible because it took 4492 seconds to run SGD 100 passes. [sent-984, score-0.019]
</p><p>89 00  PSA w/o layer trick (1) PSA w/ layer trick (1) PSA re-start (1)  time  error  311. [sent-993, score-0.194]
</p><p>90 90  Table 4: CPU time in seconds and percentage test error rates for various neural network trainers. [sent-999, score-0.047]
</p><p>91 6  Conclusions  It has been shown that given a sufďŹ ciently large training set, a single pass of 2SGD generalizes as well as the empirical optimum. [sent-1000, score-0.096]
</p><p>92 Our results show that PSA provides a practical solution to accomplish near optimal performance of 2SGD as predicted theoretically for a variety of large scale models and tasks with a reasonably low cost per iteration compared to competing 2SGD methods. [sent-1001, score-0.021]
</p><p>93 The beneďŹ t of 2SGD with PSA over plain SGD becomes clearer when the scale of the tasks are increasingly large. [sent-1002, score-0.092]
</p><p>94 For non-convex neural network tasks, since the curvature of the error surface is so complex, it is still very challenging for an eigenvalue approximation method like PSA. [sent-1003, score-0.085]
</p><p>95 Accelerated training of conditional random ďŹ elds with stochastic gradient methods. [sent-1017, score-0.091]
</p><p>96 Exponentiated gradient algorithms for conditional random ďŹ elds and max-margin markov networks. [sent-1021, score-0.056]
</p><p>97 Global and componentwise extrapolation for accelerating data mining from large incomplete data sets with the EM algorithm. [sent-1043, score-0.108]
</p><p>98 Global and componentwise extrapolations for accelerating training of Bayesian networks and conditional random ďŹ elds. [sent-1046, score-0.107]
</p><p>99 Biomedical named entity recognition using conditional random ďŹ elds and novel feature sets. [sent-1057, score-0.034]
</p><p>100 Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning. [sent-1097, score-0.052]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('psa', 0.843), ('sgd', 0.274), ('eig', 0.2), ('crf', 0.154), ('smd', 0.149), ('svmsgd', 0.1), ('aitken', 0.071), ('plain', 0.071), ('trick', 0.062), ('jacobian', 0.061), ('pass', 0.061), ('biocreative', 0.057), ('hessian', 0.056), ('eigenvalue', 0.053), ('liblinear', 0.05), ('fd', 0.048), ('acceleration', 0.048), ('periodic', 0.046), ('yann', 0.046), ('cpu', 0.045), ('ocr', 0.043), ('basenp', 0.043), ('tonga', 0.043), ('bfgs', 0.041), ('taiwan', 0.038), ('mapping', 0.037), ('ls', 0.037), ('extrapolation', 0.037), ('batch', 0.036), ('training', 0.035), ('update', 0.034), ('componentwise', 0.034), ('chunking', 0.034), ('chang', 0.033), ('adaptation', 0.03), ('bottou', 0.03), ('sec', 0.029), ('optimum', 0.029), ('fedora', 0.029), ('lgpl', 0.029), ('lsvm', 0.029), ('taipei', 0.029), ('layer', 0.028), ('hsu', 0.026), ('layers', 0.025), ('aggregate', 0.025), ('orr', 0.025), ('huang', 0.025), ('lecun', 0.024), ('ran', 0.023), ('convergence', 0.023), ('intel', 0.023), ('svm', 0.023), ('gradient', 0.022), ('settings', 0.022), ('loss', 0.022), ('tasks', 0.021), ('passes', 0.021), ('optimizer', 0.02), ('periodically', 0.02), ('mod', 0.02), ('url', 0.02), ('mappings', 0.02), ('mnist', 0.019), ('pegasos', 0.019), ('yoshua', 0.019), ('disk', 0.019), ('conditional', 0.019), ('seconds', 0.019), ('accelerating', 0.019), ('extrapolate', 0.019), ('surface', 0.018), ('convolutional', 0.018), ('mining', 0.018), ('kong', 0.017), ('sgn', 0.017), ('approximates', 0.017), ('step', 0.017), ('usa', 0.017), ('hong', 0.016), ('http', 0.016), ('attempts', 0.016), ('score', 0.016), ('primal', 0.016), ('eigenvalues', 0.016), ('table', 0.015), ('np', 0.015), ('elds', 0.015), ('examples', 0.015), ('accuracy', 0.015), ('pc', 0.015), ('prohibitively', 0.015), ('december', 0.015), ('explores', 0.015), ('discount', 0.015), ('newton', 0.015), ('usually', 0.015), ('time', 0.014), ('network', 0.014), ('tutorial', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="189-tfidf-1" href="./nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</a></p>
<p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><p>2 0.092381746 <a title="189-tfidf-2" href="./nips-2009-Dual_Averaging_Method_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">73 nips-2009-Dual Averaging Method for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as â&bdquo;&ldquo;1 -norm for promoting sparsity. We develop a new online algorithm, the regularized dual averaging (RDA) method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. Computational experiments show that the RDA method can be very effective for sparse online learning with â&bdquo;&ldquo;1 -regularization. 1</p><p>3 0.053935621 <a title="189-tfidf-3" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><p>4 0.042960353 <a title="189-tfidf-4" href="./nips-2009-Semi-supervised_Regression_using_Hessian_energy_with_an_application_to_semi-supervised_dimensionality_reduction.html">214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</a></p>
<p>Author: Kwang I. Kim, Florian Steinke, Matthias Hein</p><p>Abstract: Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Based on these observations, we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both these problems. If the data lies on or close to a low-dimensional submanifold in feature space, the Hessian energy prefers functions whose values vary linearly with respect to geodesic distance. We ﬁrst derive the Hessian energy for smooth manifolds and continue to give a stable estimation procedure for the common case where only samples of the underlying manifold are given. The preference of ‘’linear” functions on manifolds renders the Hessian energy particularly suited for the task of semi-supervised dimensionality reduction, where the goal is to ﬁnd a user-deﬁned embedding function given some labeled points which varies smoothly (and ideally linearly) along the manifold. The experimental results suggest superior performance of our method compared with semi-supervised regression using Laplacian regularization or standard supervised regression techniques applied to this task. 1</p><p>5 0.040404305 <a title="189-tfidf-5" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>Author: Novi Quadrianto, James Petterson, Alex J. Smola</p><p>Abstract: Many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g. by providing a large margin of separation on both sets. We use this idea to design a transduction algorithm which can be used without modiﬁcation for classiﬁcation, regression, and structured estimation. At its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match. This is a classical two-sample problem which can be solved efﬁciently in its most general form by using distance measures in Hilbert Space. It turns out that a number of existing heuristics can be viewed as special cases of our approach. 1</p><p>6 0.040216215 <a title="189-tfidf-6" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>7 0.033106182 <a title="189-tfidf-7" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>8 0.033017837 <a title="189-tfidf-8" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<p>9 0.031091798 <a title="189-tfidf-9" href="./nips-2009-Evaluating_multi-class_learning_strategies_in_a_generative_hierarchical_framework_for_object_detection.html">84 nips-2009-Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></p>
<p>10 0.030747607 <a title="189-tfidf-10" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>11 0.025683567 <a title="189-tfidf-11" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>12 0.025215304 <a title="189-tfidf-12" href="./nips-2009-Multiple_Incremental_Decremental_Learning_of_Support_Vector_Machines.html">160 nips-2009-Multiple Incremental Decremental Learning of Support Vector Machines</a></p>
<p>13 0.024522595 <a title="189-tfidf-13" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>14 0.02388645 <a title="189-tfidf-14" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>15 0.023342595 <a title="189-tfidf-15" href="./nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></p>
<p>16 0.022647502 <a title="189-tfidf-16" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>17 0.022546524 <a title="189-tfidf-17" href="./nips-2009-Learning_with_Compressible_Priors.html">138 nips-2009-Learning with Compressible Priors</a></p>
<p>18 0.022012332 <a title="189-tfidf-18" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>19 0.021302046 <a title="189-tfidf-19" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>20 0.021259176 <a title="189-tfidf-20" href="./nips-2009-Efficient_Learning_using_Forward-Backward_Splitting.html">76 nips-2009-Efficient Learning using Forward-Backward Splitting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.078), (1, 0.021), (2, -0.002), (3, 0.001), (4, 0.001), (5, -0.009), (6, -0.008), (7, 0.034), (8, -0.012), (9, 0.001), (10, 0.011), (11, 0.021), (12, -0.043), (13, 0.002), (14, -0.001), (15, 0.026), (16, 0.021), (17, -0.047), (18, -0.013), (19, -0.035), (20, -0.005), (21, -0.032), (22, 0.012), (23, 0.028), (24, -0.006), (25, 0.017), (26, 0.025), (27, 0.004), (28, 0.027), (29, 0.051), (30, -0.037), (31, -0.034), (32, -0.002), (33, 0.024), (34, -0.006), (35, 0.001), (36, -0.029), (37, -0.042), (38, 0.04), (39, 0.011), (40, -0.034), (41, -0.073), (42, 0.02), (43, 0.022), (44, -0.05), (45, 0.14), (46, -0.025), (47, 0.021), (48, -0.046), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86840719 <a title="189-lsi-1" href="./nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</a></p>
<p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><p>2 0.64662904 <a title="189-lsi-2" href="./nips-2009-Dual_Averaging_Method_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">73 nips-2009-Dual Averaging Method for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as â&bdquo;&ldquo;1 -norm for promoting sparsity. We develop a new online algorithm, the regularized dual averaging (RDA) method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. Computational experiments show that the RDA method can be very effective for sparse online learning with â&bdquo;&ldquo;1 -regularization. 1</p><p>3 0.63047409 <a title="189-lsi-3" href="./nips-2009-Efficient_Large-Scale_Distributed_Training_of_Conditional_Maximum_Entropy_Models.html">75 nips-2009-Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></p>
<p>Author: Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, Gideon S. Mann</p><p>Abstract: Training conditional maximum entropy models on massive data sets requires signiﬁcant computational resources. We examine three common distributed training methods for conditional maxent: a distributed gradient computation method, a majority vote method, and a mixture weight method. We analyze and compare the CPU and network time complexity of each of these methods and present a theoretical analysis of conditional maxent models, including a study of the convergence of the mixture weight method, the most resource-efﬁcient technique. We also report the results of large-scale experiments comparing these three methods which demonstrate the beneﬁts of the mixture weight method: this method consumes less resources, while achieving a performance comparable to that of standard approaches.</p><p>4 0.58043635 <a title="189-lsi-4" href="./nips-2009-Efficient_Learning_using_Forward-Backward_Splitting.html">76 nips-2009-Efficient Learning using Forward-Backward Splitting</a></p>
<p>Author: Yoram Singer, John C. Duchi</p><p>Abstract: We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We 2 also show how to construct efﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets. 1</p><p>5 0.51766372 <a title="189-lsi-5" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>Author: Novi Quadrianto, James Petterson, Alex J. Smola</p><p>Abstract: Many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g. by providing a large margin of separation on both sets. We use this idea to design a transduction algorithm which can be used without modiﬁcation for classiﬁcation, regression, and structured estimation. At its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match. This is a classical two-sample problem which can be solved efﬁciently in its most general form by using distance measures in Hilbert Space. It turns out that a number of existing heuristics can be viewed as special cases of our approach. 1</p><p>6 0.50203133 <a title="189-lsi-6" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>7 0.4917022 <a title="189-lsi-7" href="./nips-2009-Posterior_vs_Parameter_Sparsity_in_Latent_Variable_Models.html">192 nips-2009-Posterior vs Parameter Sparsity in Latent Variable Models</a></p>
<p>8 0.4745571 <a title="189-lsi-8" href="./nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</a></p>
<p>9 0.45388275 <a title="189-lsi-9" href="./nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></p>
<p>10 0.44191405 <a title="189-lsi-10" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<p>11 0.43991855 <a title="189-lsi-11" href="./nips-2009-Multiple_Incremental_Decremental_Learning_of_Support_Vector_Machines.html">160 nips-2009-Multiple Incremental Decremental Learning of Support Vector Machines</a></p>
<p>12 0.4366056 <a title="189-lsi-12" href="./nips-2009-Heavy-Tailed_Symmetric_Stochastic_Neighbor_Embedding.html">106 nips-2009-Heavy-Tailed Symmetric Stochastic Neighbor Embedding</a></p>
<p>13 0.43423927 <a title="189-lsi-13" href="./nips-2009-Slow%2C_Decorrelated_Features_for_Pretraining_Complex_Cell-like_Networks.html">219 nips-2009-Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></p>
<p>14 0.43157807 <a title="189-lsi-14" href="./nips-2009-Analysis_of_SVM_with_Indefinite_Kernels.html">33 nips-2009-Analysis of SVM with Indefinite Kernels</a></p>
<p>15 0.41570535 <a title="189-lsi-15" href="./nips-2009-Conditional_Random_Fields_with_High-Order_Features_for_Sequence_Labeling.html">57 nips-2009-Conditional Random Fields with High-Order Features for Sequence Labeling</a></p>
<p>16 0.41471609 <a title="189-lsi-16" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>17 0.38086334 <a title="189-lsi-17" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>18 0.37306991 <a title="189-lsi-18" href="./nips-2009-Replacing_supervised_classification_learning_by_Slow_Feature_Analysis_in_spiking_neural_networks.html">203 nips-2009-Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></p>
<p>19 0.36779901 <a title="189-lsi-19" href="./nips-2009-Convergent_Temporal-Difference_Learning_with_Arbitrary_Smooth_Function_Approximation.html">60 nips-2009-Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></p>
<p>20 0.35717925 <a title="189-lsi-20" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.016), (10, 0.013), (24, 0.039), (25, 0.078), (35, 0.06), (36, 0.104), (39, 0.025), (54, 0.319), (58, 0.05), (61, 0.013), (71, 0.055), (81, 0.027), (86, 0.089), (91, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75148493 <a title="189-lda-1" href="./nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</a></p>
<p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><p>2 0.70056266 <a title="189-lda-2" href="./nips-2009-Non-stationary_continuous_dynamic_Bayesian_networks.html">168 nips-2009-Non-stationary continuous dynamic Bayesian networks</a></p>
<p>Author: Marco Grzegorczyk, Dirk Husmeier</p><p>Abstract: Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many realworld scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-ﬂexible models that lack any information sharing among time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary among segments, and in which a common network structure provides essential information sharing across segments. Our model is based on a Bayesian multiple change-point process, where the number and location of the change-points is sampled from the posterior distribution. 1</p><p>3 0.64404666 <a title="189-lda-3" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>Author: Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</p><p>Abstract: The replica method is a non-rigorous but widely-accepted technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. It is shown that with random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo and Verd´ ’s replica analysis of minimum mean-squared error estimation. u The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, and zero norm-regularized estimation. In the case of lasso estimation the scalar estimator reduces to a soft-thresholding operator, and for zero normregularized estimation it reduces to a hard-threshold. Among other beneﬁts, the replica method provides a computationally-tractable method for exactly computing various performance metrics including mean-squared error and sparsity pattern recovery probability.</p><p>4 0.6265828 <a title="189-lda-4" href="./nips-2009-Beyond_Convexity%3A_Online_Submodular_Minimization.html">45 nips-2009-Beyond Convexity: Online Submodular Minimization</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efﬁcient and are Hannan-consistent in both the full information and bandit settings. 1</p><p>5 0.50600207 <a title="189-lda-5" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>Author: Ruslan Salakhutdinov</p><p>Abstract: Markov random ﬁelds (MRF’s), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF’s is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF’s. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.</p><p>6 0.50049472 <a title="189-lda-6" href="./nips-2009-A_Sparse_Non-Parametric_Approach_for_Single_Channel_Separation_of_Known_Sounds.html">17 nips-2009-A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></p>
<p>7 0.49921933 <a title="189-lda-7" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>8 0.49840724 <a title="189-lda-8" href="./nips-2009-Nonparametric_Latent_Feature_Models_for_Link_Prediction.html">174 nips-2009-Nonparametric Latent Feature Models for Link Prediction</a></p>
<p>9 0.49819997 <a title="189-lda-9" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>10 0.49778292 <a title="189-lda-10" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>11 0.49762011 <a title="189-lda-11" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>12 0.49523145 <a title="189-lda-12" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>13 0.49480629 <a title="189-lda-13" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>14 0.49327794 <a title="189-lda-14" href="./nips-2009-3D_Object_Recognition_with_Deep_Belief_Nets.html">2 nips-2009-3D Object Recognition with Deep Belief Nets</a></p>
<p>15 0.49300849 <a title="189-lda-15" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<p>16 0.49263322 <a title="189-lda-16" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>17 0.49199283 <a title="189-lda-17" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>18 0.49161565 <a title="189-lda-18" href="./nips-2009-Distribution-Calibrated_Hierarchical_Classification.html">71 nips-2009-Distribution-Calibrated Hierarchical Classification</a></p>
<p>19 0.49156687 <a title="189-lda-19" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>20 0.49122906 <a title="189-lda-20" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
