<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-30" href="#">nips2009-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</h1>
<br/><p>Source: <a title="nips-2009-30-pdf" href="http://papers.nips.cc/paper/3756-an-integer-projected-fixed-point-method-for-graph-matching-and-map-inference.pdf">pdf</a></p><p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><p>Reference: <a title="nips-2009-30-reference" href="../nips2009_reference/nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Graph matching and MAP inference are essential problems in computer vision and machine learning. [sent-6, score-0.286]
</p><p>2 Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. [sent-8, score-0.638]
</p><p>3 They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. [sent-10, score-0.355]
</p><p>4 Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. [sent-11, score-0.233]
</p><p>5 In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. [sent-12, score-0.256]
</p><p>6 We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. [sent-13, score-0.96]
</p><p>7 In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. [sent-14, score-0.337]
</p><p>8 1  Introduction  Graph matching and MAP inference are essential problems in computer vision and machine learning that are frequently formulated as integer quadratic programs, where obtaining an exact solution is computationally intractable. [sent-17, score-0.64]
</p><p>9 Feature matching using pairwise constraints is gaining a widespread use in computer vision, especially in shape and object matching and recognition. [sent-20, score-0.532]
</p><p>10 The problem is NP-hard, and a lot of effort has been spent in ﬁnding good approximate solutions by relaxing the integer one-to-one constraints, such that the continuous global optimum of the new problem can be found efﬁciently. [sent-22, score-0.417]
</p><p>11 In the end, little computational time is spent in order to binarize the solution, based on the assumption that the continuous optimum is close to the discrete global optimum of the original combinatorial problem. [sent-23, score-0.531]
</p><p>12 In this paper we show experimentally that this is not the case and that, in fact, carefully searching for a discrete solution is essential for maximizing the quadratic score. [sent-24, score-0.433]
</p><p>13 Therefore we propose an iterative algorithm that takes as input any continuous or discrete solution, possibly given by some other graph matching method, and quickly improves it by aiming to maximize the original problem with its integer constraints. [sent-25, score-0.706]
</p><p>14 The ﬁrst stage maximizes in the discrete domain a linear approximation of the quadratic function around the current solution, which gives a direction along which the second stage maximizes the original quadratic score in the continuous domain. [sent-27, score-1.125]
</p><p>15 The algorithm always improves the quadratic score in the continuous domain ﬁnally converging to a maximum. [sent-29, score-0.603]
</p><p>16 If the quadratic function is convex the solution at every iteration is always discrete and the algorithm converges in a ﬁnite number of steps. [sent-30, score-0.516]
</p><p>17 In the case of non-convex quadratic functions, the method tends to pass through/near discrete solutions and the best discrete solution encountered along the path is returned, which, in practice is either identical or very close to the point of convergence. [sent-31, score-0.634]
</p><p>18 Some recent MAP inference algorithms [11,12,13] for Markov Random Fields formulate the problem as an integer quadratic program, for which our algorithm is also well suited, as we later explain and demonstrate in more detail. [sent-34, score-0.342]
</p><p>19 Matching Using Pairwise Constraints The graph matching problem, in its most recent and general form, consists of ﬁnding the indicator vector x∗ that maximizes a certain quadratic score function: Problem 1: x∗ = argmax(xT Mx) s. [sent-35, score-0.732]
</p><p>20 For example, spectral matching [5] (SM) drops the constraints entirely and assumes that the leading eigenvector of M is close to the optimal discrete solution. [sent-43, score-0.533]
</p><p>21 It then ﬁnds the discrete solution x by maximizing the dot-product with the leading eigenvector of M. [sent-44, score-0.28]
</p><p>22 Later, spectral graph matching with afﬁne constraints was developed [3] (SMAC), which ﬁnds the optimal solution of a modiﬁed score function, with a tighter relaxation that imposes the afﬁne constraints Ax = 1 during optimization. [sent-46, score-0.786]
</p><p>23 An important observation is that none of the previous methods are concerned with the original integer constraints during optimization, and the ﬁnal post processing step, when the continuous solution is binarized, is usually just a very simple procedure. [sent-48, score-0.378]
</p><p>24 They assume that the continuous solution is close to the discrete one. [sent-49, score-0.336]
</p><p>25 The algorithm  we propose here optimizes the original quadratic score in the continuous domain obtained by only dropping the binary constraints, but it always targets discrete solutions through which it passes most of the time. [sent-50, score-0.851]
</p><p>26 Note that even in this continuous domain the quadratic optimization problem is NPhard, so we cannot hope to get any global optimality guarantees. [sent-51, score-0.497]
</p><p>27 But we do not lose much, since guaranteed global optimality for a relaxed problem does not require closeness to the global optimum of the original problem, a fact that is evident in most of our experiments. [sent-52, score-0.351]
</p><p>28 Our algorithm aims to optimize the following continuous problem, in which we only drop the integer constraints from Problem 1: Problem 2: x∗ = argmax(xT Mx) s. [sent-54, score-0.281]
</p><p>29 Let bk+1 = Pd (Mxk ), C = xT M(bk+1 − xk ), D = (bk+1 − xk )T M(bk+1 − xk ) k 3. [sent-59, score-0.567]
</p><p>30 Else let r = min {−C/D, 1} and set xk+1 = xk + r(bk+1 − xk ) 4. [sent-61, score-0.378]
</p><p>31 This is true since all binary vectors in the given discrete domain have the same norm. [sent-68, score-0.261]
</p><p>32 Note that (see Proposition 1), in both cases (one-to-one or many-to-one constraints), the discrete bk+1 is also the one maximizing the dot-product with Mxk in the continuous domain Ab = 1, b > 0. [sent-69, score-0.375]
</p><p>33 In the domain of many-to-one constraints IPFP becomes an extension of ICM for which the updates are performed in parallel without losing the climbing property and the convergence to a discrete solution. [sent-71, score-0.462]
</p><p>34 Step 3 insures that the quadratic score increases with each iteration. [sent-76, score-0.37]
</p><p>35 In practice, the algorithm signiﬁcantly improves the initial binary solution, and the ﬁnal continuous solution is most often discrete, and always close to the best discrete one found. [sent-78, score-0.365]
</p><p>36 Intuition The intuition behind this algorithm is the following: at every iteration the quadratic score xT Mx is ﬁrst approximated by the ﬁrst order Taylor expansion around the current solution xk : xT Mx ≈ xT Mxk +2xT M(x − xk ). [sent-80, score-0.904]
</p><p>37 This approximation is maximized within the discrete domain k k of Problem 1, at Step 2, where bk+1 is found. [sent-81, score-0.261]
</p><p>38 From Proposition 1 (see next) we know that the same discrete bk+1 also maximizes the linear approximation in the continuous domain of Problem 2. [sent-82, score-0.396]
</p><p>39 The role of bk+1 is to provide a direction of largest possible increase (or ascent) in the ﬁrst-order approximation, within both the continuous domain and the discrete domain simultaneously. [sent-83, score-0.493]
</p><p>40 Along this direction the original quadratic score can be further maximized in the continuous domain of Problem 2 (as long as bk+1 = xk ). [sent-84, score-0.834]
</p><p>41 At Step 3 we ﬁnd the optimal point along this direction, also inside the continuous domain of Problem 2. [sent-85, score-0.204]
</p><p>42 The hope, also conﬁrmed in practice, is that the algorithm will tend to converge towards discrete solutions that are, or are close to, maxima of Problem 2. [sent-86, score-0.282]
</p><p>43 3  Theoretical Analysis  Proposition 1: For any vector x ∈ Rn there exists a global optimum y∗ of xT My in the domain of Problem 2 that has binary elements (thus it is also in the domain of Problem 1). [sent-87, score-0.411]
</p><p>44 Proof: Maximizing xT My with respect to y, subject to Ay = 1 and y > 0 is a linear program for which an integer optimal solution exists because the constraints matrix A is totally unimodular [9]. [sent-88, score-0.249]
</p><p>45 It follows that the maximization from Step 2 bk+1 = argmax bT Mxk in the original discrete domain, also maximizes the same dot-product in the continuous domain of Problem 2, of relaxed constraints Ax = 1 and x > 0. [sent-90, score-0.595]
</p><p>46 This ensures that the algorithm will always move towards some discrete solution that also maximizes the linear approximation of the quadratic function in the domain of Problem 2. [sent-91, score-0.601]
</p><p>47 Most often in practice, that discrete solution also maximizes the quadratic score, along the same direction and within the continuous domain. [sent-92, score-0.568]
</p><p>48 Therefore xk is likely to be discrete at every step. [sent-93, score-0.332]
</p><p>49 Property 1: The quadratic score xT Mxk increases at every step k and the sequence of xk converges. [sent-94, score-0.587]
</p><p>50 k Proof: For a given step k, if bk+1 = xk we have convergence. [sent-95, score-0.217]
</p><p>51 If bk+1 = xk , let x be a point on the line between xk and bk+1 , x = xk +t(bk+1 −xk ). [sent-96, score-0.567]
</p><p>52 Let us deﬁne the quadratic function f (t) = xT Mx = Sk + 2tC + t2 D, k which is the original function in the domain of Problem 2 on the line between xk and bk+1 . [sent-99, score-0.529]
</p><p>53 We have two cases: D ≥ 0, when xk+1 = bk+1 (Step 3) and Sk+1 = xT Mxk+1 = k+1 fq (1) ≥ Sk = xT Mxk ; and D < 0, when the quadratic function fq (t) is convex with the maximum k in the domain of Problem 2 attained at point xk+1 = xk + r(bk+1 − xk ). [sent-101, score-0.759]
</p><p>54 Therefore, the algorithm is guaranteed to k+1 k increase the score at every step. [sent-103, score-0.22]
</p><p>55 By always improving the quadratic score in the continuous domain, at each step the next solution moves towards discrete solutions that are better suited for solving the original Problem 1. [sent-105, score-0.842]
</p><p>56 At that point the gradient 2Mx∗ is non-zero since both M and x∗ have positive elements and (x∗ )T Mx∗ > 0, (it is higher than the score at the ﬁrst iteration, also greater than zero). [sent-108, score-0.217]
</p><p>57 Since x∗ is a point of convergence it follows that C = 0, that is, for any other x in the continuous domain of Problem 2, (x∗ )T Mx∗ ≥ (x∗ )T Mx. [sent-109, score-0.23]
</p><p>58 For many-to-one constraints (MAP inference) it basically follows that the algorithm will converge to a discrete solution, since the strict (local and global) maxima of Problem 2 are in the discrete domain [12]. [sent-111, score-0.564]
</p><p>59 If the maximum is not strict, IPFP still converges to a discrete solution (which is also a local maximum): the one found at Step 2. [sent-112, score-0.289]
</p><p>60 Property 3: If M is positive semideﬁnite with positive elements, then the algorithm converges in a ﬁnite number of iterations to a discrete solution, which is a maximum of Problem 2. [sent-115, score-0.21]
</p><p>61 Proof: Since M is positive semideﬁnite we always have D ≥ 0, thus xk is always discrete for any k. [sent-116, score-0.332]
</p><p>62 Since the number of discrete solutions is ﬁnite, the algorithm must converge in a ﬁnite number of steps to a local (or global) maximum, which must be discrete. [sent-117, score-0.283]
</p><p>63 When M is positive semideﬁnite, Problem 2 is a concave minimization problem for which it is well known that the global optimum has integer elements, so it is also a global optimum of the original Problem 1. [sent-119, score-0.464]
</p><p>64 In fact, if a large enough constant is added to the diagonal elements of M, every point in the original domain of possible solutions becomes a local optimum for one-to-one problems. [sent-122, score-0.36]
</p><p>65 In practice M is rarely positive semideﬁnite, but it can be close to being one if the ﬁrst eigenvalue is much larger than the rest, which is the assumption made by the spectral matching algorithm, for example. [sent-124, score-0.29]
</p><p>66 Property 4: If M has non-negative elements and is rank-1, then the algorithm will converge and return the global optimum of the original problem after the ﬁrst iteration. [sent-125, score-0.271]
</p><p>67 Since both x0 and v have positive elements it immediately follows that x1 after the ﬁrst iteration is the indicator solution vector that maximizes the dot-product with the leading eigenvector (vT x0 = 0 is a very unlikely case that never happens in practice). [sent-129, score-0.228]
</p><p>68 Spectral matching [5] also returns the optimal solution in this case and it assumes that the rank-1 assumption is the ideal matrix to which a small amount of noise is added. [sent-132, score-0.298]
</p><p>69 Probabilistic graph matching [2] makes the rank-1 approximation by assuming that each second-order element of Mia;jb is the product of the probability of feature i being matched to a and feature j being matched to b, independently. [sent-133, score-0.401]
</p><p>70 However, instead of maximizing the quadratic score function, they use this probabilistic interpretation of the pair-wise terms and ﬁnd the solution by looking for the closest rank-1 matrix to M in terms of the KL-divergence. [sent-134, score-0.481]
</p><p>71 If the assumptions in [2] were perfectly met, then spectral matching, probabilistic graph matching and our algorithm would all return the same solution. [sent-135, score-0.393]
</p><p>72 4  Experiments  We ﬁrst present some representative experiments on graph matching problems. [sent-137, score-0.313]
</p><p>73 Notice how quickly IPFP converges (fewer than 10 iterations)  Table 1: Average matching rates for the experiments with outliers on cars and motorbikes from Pascal 07. [sent-145, score-0.595]
</p><p>74 By outliers we mean the features that have no ground truth correspondences in the other image, and by inliers those that have such correspondences. [sent-163, score-0.219]
</p><p>75 The difﬁculty of the matching problems is reﬂected by the relatively low matching scores of all algorithms (Table 1). [sent-167, score-0.469]
</p><p>76 In order to ensure an optimal performance of all algorithms, we used the supervised version of the graph matching learning method from [6]. [sent-168, score-0.313]
</p><p>77 The algorithms we chose for comparison and also for combining with ours are among the current state-of-the-art in the literature: spectral matching with afﬁne constraints (SMAC) [3], spectral matching (SM) [5], probabilistic graph matching (PM) [2], and graduated assignment (GA) [4]. [sent-170, score-0.985]
</p><p>78 Firstly, we tested the matching rate of our algorithm against the others, and observed that it consistently outperforms them, both in the matching rate and in the ﬁnal quadratic score achieved by the resulting discrete solution (see Tables 1, 2). [sent-173, score-1.079]
</p><p>79 Secondly, we combined our algorithm, as a post-processing step, with the others and obtained a signiﬁcant improvement over the output matching rate and quadratic score of the other algorithms by themselves (see Figures 1, 2). [sent-174, score-0.585]
</p><p>80 In Figure 2 we show the quadratic score of our algorithm, per iteration, for several individual experiments, when it takes as initial solution the output of several other algorithms. [sent-175, score-0.453]
</p><p>81 The score at the ﬁrst iteration is the score of the ﬁnal discrete solution returned by those algorithms and the improvement in just a few iterations is substantial, sometimes more than doubling the ﬁnal quadratic score reached by the other algorithms. [sent-176, score-1.05]
</p><p>82 In Figure 1 we show the average scores of our algorithm, over 30 different experiments on cars and motorbikes, per iteration, normalized by the score of the solutions given by the human ground truth labeling. [sent-177, score-0.49]
</p><p>83 We notice that regardless of the starting condition, the ﬁnal scores are very similar, slightly above the value of 1 (Table 2), which means that the solutions reached are, on average, at least as good, in terms of the matching score function, as the manually picked solutions. [sent-178, score-0.507]
</p><p>84 We also notice that a quadratic score of 1 does not correspond to a perfect matching rate, which indicates the fact that besides the ground truth solution there are  Table 2: Quadratic scores on the Cars and Motorbikes image sets (the higher, the better). [sent-180, score-0.789]
</p><p>85 S ∗ is the score of the manually picked ground truth. [sent-181, score-0.234]
</p><p>86 Note that the ground truth score S ∗ does not affect the comparison since it is the same normalization value for all algorithms. [sent-182, score-0.273]
</p><p>87 The “Convergence to a binary solution” row shows the average rate at which our algorithm converges to a discrete solution. [sent-183, score-0.21]
</p><p>88 However, increasing the quadratic score, does increase the matching rate as can be seen by comparing the results between the Tables 2 and 1. [sent-201, score-0.394]
</p><p>89 Figure 2: Experiments on cars and motorbikes: at each iteration the score xT Mxk normalized by k the ground truth score is displayed for 30 individual matching experiments for our algorithm starting from different solutions (uniform, or given by some other algorithm). [sent-202, score-0.93]
</p><p>90 Experiments on MAP inference problems We believe that IPFP can have a greater impact in graph matching problems than in MAP inference ones, due to the lack of efﬁcient, high-quality discretization procedures in the graph matching literature. [sent-203, score-0.748]
</p><p>91 In the domain of MAP inference for MRFs, it is important to note that IPFP is strongly related to the parallel version of Iterated Conditional Modes, but, unlike parallel ICM, it has climbing, strong convergence and local optimality properties. [sent-204, score-0.324]
</p><p>92 In the case of [12] and [13], which give continuous optimal solutions to a relaxed problem, a post-processing step is required for discretization. [sent-206, score-0.221]
</p><p>93 In Figure 3 we show the average scores normalized by the score of IPFP over 30 different experiments, for different probabilities of edge generation pEdge on graphs with 50 nodes and different number of possible labels per node. [sent-215, score-0.23]
</p><p>94 In our experiments, on every single problem, IPFP  Table 3: Average objective score over 30 different experiments on 4-connected and 8-connected planar graphs with 50 sites and 10 possible labels per site Graph type 4-connected planar 8-connected planar  IPFP 79. [sent-217, score-0.293]
</p><p>95 Figure 3: Average quadratic scores normalized by the score of IPFP, over 30 different experiments, for each probability of edge generation pEdge ∈ 0. [sent-224, score-0.409]
</p><p>96 5  Conclusion  This paper presents a novel and computationally efﬁcient algorithm, Integer Projected Fixed Point (IPFP), that outperforms state-of-the-art methods for solving quadratic assignment problems in graph matching, and well-established methods in MAP inference such as BP and ICM. [sent-232, score-0.374]
</p><p>97 Also, IPFP can be employed in conjunction with existing techniques, such as SMAC or SM for graph matching or BP for inference to achieve solutions that are dramatically better than the ones produced independently by those methods alone. [sent-234, score-0.417]
</p><p>98 Shape matching and object recognition using low distortion correspondences. [sent-242, score-0.215]
</p><p>99 An algorithm for quadratic programming, Naval Research Logistics Quarterly, 1956. [sent-294, score-0.208]
</p><p>100 Solving quadratic assignment problems using convex quadratic programming relaxations, Optimization Methods and Software, 2001 [16] J. [sent-300, score-0.389]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ipfp', 0.56), ('bk', 0.273), ('mxk', 0.262), ('icm', 0.253), ('matching', 0.215), ('score', 0.191), ('xk', 0.189), ('quadratic', 0.179), ('motorbikes', 0.154), ('discrete', 0.143), ('domain', 0.118), ('cars', 0.116), ('xt', 0.104), ('graph', 0.098), ('mx', 0.095), ('integer', 0.092), ('jb', 0.092), ('cqp', 0.087), ('smac', 0.087), ('continuous', 0.086), ('optimum', 0.086), ('solution', 0.083), ('constraints', 0.074), ('outliers', 0.072), ('climbing', 0.07), ('pd', 0.068), ('leordeanu', 0.066), ('global', 0.063), ('solutions', 0.062), ('map', 0.055), ('gia', 0.052), ('mia', 0.052), ('sk', 0.052), ('ax', 0.052), ('optimality', 0.051), ('spectral', 0.051), ('sm', 0.05), ('maximizes', 0.049), ('relaxed', 0.045), ('iteration', 0.044), ('matched', 0.044), ('original', 0.043), ('ground', 0.043), ('inference', 0.042), ('inliers', 0.042), ('fq', 0.042), ('pm', 0.04), ('fixed', 0.04), ('scores', 0.039), ('truth', 0.039), ('bp', 0.039), ('converges', 0.038), ('pittsburgh', 0.038), ('discretization', 0.038), ('argmax', 0.037), ('bt', 0.037), ('graduated', 0.035), ('mbk', 0.035), ('pedge', 0.035), ('smax', 0.035), ('planar', 0.034), ('basically', 0.033), ('nal', 0.032), ('semide', 0.031), ('assignment', 0.031), ('loosely', 0.031), ('parallel', 0.031), ('cour', 0.031), ('concave', 0.031), ('stage', 0.03), ('algorithm', 0.029), ('pascal', 0.029), ('ga', 0.029), ('vision', 0.029), ('pairwise', 0.028), ('relaxing', 0.028), ('direction', 0.028), ('step', 0.028), ('maximizing', 0.028), ('returned', 0.028), ('suited', 0.027), ('eigenvector', 0.026), ('binarized', 0.026), ('avg', 0.026), ('hebert', 0.026), ('elements', 0.026), ('convergence', 0.026), ('modes', 0.025), ('projected', 0.025), ('nds', 0.025), ('unary', 0.025), ('vt', 0.025), ('local', 0.025), ('tables', 0.024), ('appearance', 0.024), ('close', 0.024), ('outperforms', 0.024), ('converge', 0.024), ('correspondences', 0.023), ('shortly', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="30-tfidf-1" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><p>2 0.13278192 <a title="30-tfidf-2" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the “topics”). In the sparse topic model (sparseTM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efﬁcient Gibbs sampler for the sparseTM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the sparseTM on four real-world datasets. Compared to traditional approaches, the empirical results will show that sparseTMs give better predictive performance with simpler inferred models. 1</p><p>3 0.088539727 <a title="30-tfidf-3" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>Author: James Petterson, Jin Yu, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufﬁcient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application–document ranking–exact inference is efﬁcient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning document ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is comparatively high. 1</p><p>4 0.080142044 <a title="30-tfidf-4" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Inﬁnitesimal Perturbation Analysis (IPA) on the ﬁltering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the ﬁltering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.</p><p>5 0.079978898 <a title="30-tfidf-5" href="./nips-2009-Free_energy_score_space.html">97 nips-2009-Free energy score space</a></p>
<p>Author: Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</p><p>Abstract: A score function induced by a generative model of the data can provide a feature vector of a ﬁxed dimension for each data sample. Data samples themselves may be of differing lengths (e.g., speech segments, or other sequence data), but as a score function is based on the properties of the data generation process, it produces a ﬁxed-length vector in a highly informative space, typically referred to as a “score space”. Discriminative classiﬁers have been shown to achieve higher performance in appropriately chosen score spaces than is achievable by either the corresponding generative likelihood-based classiﬁers, or the discriminative classiﬁers using standard feature extractors. In this paper, we present a novel score space that exploits the free energy associated with a generative model. The resulting free energy score space (FESS) takes into account latent structure of the data at various levels, and can be trivially shown to lead to classiﬁcation performance that at least matches the performance of the free energy classiﬁer based on the same generative model, and the same factorization of the posterior. We also show that in several typical vision and computational biology applications the classiﬁers optimized in FESS outperform the corresponding pure generative approaches, as well as a number of previous approaches to combining discriminating and generative models.</p><p>6 0.079046808 <a title="30-tfidf-6" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>7 0.077457592 <a title="30-tfidf-7" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>8 0.077131435 <a title="30-tfidf-8" href="./nips-2009-On_the_Convergence_of_the_Concave-Convex_Procedure.html">180 nips-2009-On the Convergence of the Concave-Convex Procedure</a></p>
<p>9 0.073691204 <a title="30-tfidf-9" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>10 0.070275903 <a title="30-tfidf-10" href="./nips-2009-On_Stochastic_and_Worst-case_Models_for_Investing.html">178 nips-2009-On Stochastic and Worst-case Models for Investing</a></p>
<p>11 0.068944424 <a title="30-tfidf-11" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>12 0.065882333 <a title="30-tfidf-12" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>13 0.063483879 <a title="30-tfidf-13" href="./nips-2009-A_Game-Theoretic_Approach_to_Hypergraph_Clustering.html">9 nips-2009-A Game-Theoretic Approach to Hypergraph Clustering</a></p>
<p>14 0.056932453 <a title="30-tfidf-14" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>15 0.055606268 <a title="30-tfidf-15" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>16 0.055274118 <a title="30-tfidf-16" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>17 0.05324224 <a title="30-tfidf-17" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>18 0.052148525 <a title="30-tfidf-18" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>19 0.050591212 <a title="30-tfidf-19" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>20 0.050005604 <a title="30-tfidf-20" href="./nips-2009-An_LP_View_of_the_M-best_MAP_problem.html">31 nips-2009-An LP View of the M-best MAP problem</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, 0.057), (2, -0.017), (3, -0.036), (4, 0.029), (5, 0.017), (6, 0.019), (7, 0.058), (8, 0.015), (9, -0.056), (10, -0.021), (11, 0.035), (12, 0.042), (13, 0.02), (14, -0.012), (15, -0.019), (16, -0.085), (17, -0.084), (18, -0.038), (19, -0.132), (20, 0.011), (21, -0.018), (22, 0.114), (23, 0.069), (24, -0.057), (25, 0.036), (26, 0.11), (27, -0.048), (28, -0.065), (29, 0.044), (30, 0.078), (31, 0.037), (32, -0.016), (33, -0.009), (34, 0.152), (35, 0.076), (36, 0.05), (37, -0.031), (38, -0.079), (39, 0.074), (40, -0.056), (41, 0.02), (42, -0.132), (43, 0.002), (44, -0.052), (45, 0.023), (46, -0.05), (47, -0.03), (48, -0.104), (49, -0.168)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94717109 <a title="30-lsi-1" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><p>2 0.65761507 <a title="30-lsi-2" href="./nips-2009-On_the_Convergence_of_the_Concave-Convex_Procedure.html">180 nips-2009-On the Convergence of the Concave-Convex Procedure</a></p>
<p>Author: Gert R. Lanckriet, Bharath K. Sriperumbudur</p><p>Abstract: The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of speciﬁc attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), its proof is more specialized and technical than actually required for the speciﬁc case of CCCP. In this paper, we follow a different reasoning and show how Zangwill’s global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwill’s theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP ﬁnd a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.</p><p>3 0.51997864 <a title="30-lsi-3" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>Author: Roy Anati, Kostas Daniilidis</p><p>Abstract: We present a system which constructs a topological map of an environment given a sequence of images. This system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously. Additionally, an MRF is constructed to model the probability of loop-closures. A locally optimal labeling is found using Loopy-BP. Finally we outline a method to generate a topological map from loop closure data. Results, presented on four urban sequences and one indoor sequence, outperform the state of the art. 1</p><p>4 0.51650697 <a title="30-lsi-4" href="./nips-2009-Local_Rules_for_Global_MAP%3A_When_Do_They_Work_%3F.html">141 nips-2009-Local Rules for Global MAP: When Do They Work ?</a></p>
<p>Author: Kyomin Jung, Pushmeet Kohli, Devavrat Shah</p><p>Abstract: We consider the question of computing Maximum A Posteriori (MAP) assignment in an arbitrary pair-wise Markov Random Field (MRF). We present a randomized iterative algorithm based on simple local updates. The algorithm, starting with an arbitrary initial assignment, updates it in each iteration by ﬁrst, picking a random node, then selecting an (appropriately chosen) random local neighborhood and optimizing over this local neighborhood. Somewhat surprisingly, we show that this algorithm ﬁnds a near optimal assignment within n log 2 n iterations with high probability for any n node pair-wise MRF with geometry (i.e. MRF graph with polynomial growth) with the approximation error depending on (in a reasonable manner) the geometric growth rate of the graph and the average radius of the local neighborhood – this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error. Through extensive simulations, we show that our algorithm ﬁnds extremely good approximate solutions for various kinds of MRFs with geometry.</p><p>5 0.45367709 <a title="30-lsi-5" href="./nips-2009-A_Gaussian_Tree_Approximation_for_Integer_Least-Squares.html">10 nips-2009-A Gaussian Tree Approximation for Integer Least-Squares</a></p>
<p>Author: Jacob Goldberger, Amir Leshem</p><p>Abstract: This paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a ﬁnite set. The factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph. Hence, applying the Belief Propagation (BP) algorithm yields very poor results. The algorithm described here is based on an optimal tree approximation of the Gaussian density of the unconstrained linear system. It is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the BP algorithm to the modiﬁed factor graph outperforms current methods in terms of both performance and complexity. The improved performance of the proposed algorithm is demonstrated on the problem of MIMO detection.</p><p>6 0.44073918 <a title="30-lsi-6" href="./nips-2009-Analysis_of_SVM_with_Indefinite_Kernels.html">33 nips-2009-Analysis of SVM with Indefinite Kernels</a></p>
<p>7 0.44023076 <a title="30-lsi-7" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>8 0.43965086 <a title="30-lsi-8" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>9 0.43321466 <a title="30-lsi-9" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>10 0.42827246 <a title="30-lsi-10" href="./nips-2009-Localizing_Bugs_in_Program_Executions_with_Graphical_Models.html">143 nips-2009-Localizing Bugs in Program Executions with Graphical Models</a></p>
<p>11 0.40629274 <a title="30-lsi-11" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>12 0.40210477 <a title="30-lsi-12" href="./nips-2009-Fast_Graph_Laplacian_Regularized_Kernel_Learning_via_Semidefinite%E2%80%93Quadratic%E2%80%93Linear_Programming.html">92 nips-2009-Fast Graph Laplacian Regularized Kernel Learning via Semidefinite–Quadratic–Linear Programming</a></p>
<p>13 0.40042329 <a title="30-lsi-13" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>14 0.39984149 <a title="30-lsi-14" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>15 0.39683893 <a title="30-lsi-15" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>16 0.39258006 <a title="30-lsi-16" href="./nips-2009-Heavy-Tailed_Symmetric_Stochastic_Neighbor_Embedding.html">106 nips-2009-Heavy-Tailed Symmetric Stochastic Neighbor Embedding</a></p>
<p>17 0.39080569 <a title="30-lsi-17" href="./nips-2009-An_LP_View_of_the_M-best_MAP_problem.html">31 nips-2009-An LP View of the M-best MAP problem</a></p>
<p>18 0.39043671 <a title="30-lsi-18" href="./nips-2009-Submodularity_Cuts_and_Applications.html">239 nips-2009-Submodularity Cuts and Applications</a></p>
<p>19 0.38567835 <a title="30-lsi-19" href="./nips-2009-Convergent_Temporal-Difference_Learning_with_Arbitrary_Smooth_Function_Approximation.html">60 nips-2009-Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></p>
<p>20 0.38449213 <a title="30-lsi-20" href="./nips-2009-Slow_Learners_are_Fast.html">220 nips-2009-Slow Learners are Fast</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.011), (24, 0.056), (25, 0.058), (35, 0.097), (36, 0.103), (39, 0.05), (58, 0.122), (61, 0.03), (71, 0.043), (76, 0.218), (81, 0.025), (86, 0.079), (91, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81906784 <a title="30-lda-1" href="./nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></p>
<p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><p>2 0.80188221 <a title="30-lda-2" href="./nips-2009-Matrix_Completion_from_Power-Law_Distributed_Samples.html">148 nips-2009-Matrix Completion from Power-Law Distributed Samples</a></p>
<p>Author: Raghu Meka, Prateek Jain, Inderjit S. Dhillon</p><p>Abstract: The low-rank matrix completion problem is a fundamental problem with many important applications. Recently, [4],[13] and [5] obtained the ﬁrst non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random. Unfortunately, most real-world datasets do not satisfy this assumption, but instead exhibit power-law distributed samples. In this paper, we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models. Our method is simpler to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs, a problem of independent interest. By analyzing the graph theoretic problem, we show that our method achieves exact recovery when the observed entries are sampled from the Chung-Lu-Vu model, which can generate power-law distributed graphs. We also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim. Furthermore, our method is easy to implement and is substantially faster than existing methods. We demonstrate the effectiveness of our method on random instances where the low-rank matrix is sampled according to the prevalent random graph models for complex networks and present promising preliminary results on the Netﬂix challenge dataset. 1</p><p>3 0.69706988 <a title="30-lda-3" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>Author: Jarno Vanhatalo, Pasi Jylänki, Aki Vehtari</p><p>Abstract: In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be signiﬁcantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the inﬂuence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution. 1</p><p>4 0.69704646 <a title="30-lda-4" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>Author: Jaakko Luttinen, Alexander T. Ihler</p><p>Abstract: We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</p><p>5 0.69313139 <a title="30-lda-5" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><p>6 0.69235837 <a title="30-lda-6" href="./nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</a></p>
<p>7 0.6871407 <a title="30-lda-7" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>8 0.68661332 <a title="30-lda-8" href="./nips-2009-Improving_Existing_Fault_Recovery_Policies.html">113 nips-2009-Improving Existing Fault Recovery Policies</a></p>
<p>9 0.68660289 <a title="30-lda-9" href="./nips-2009-AUC_optimization_and_the_two-sample_problem.html">3 nips-2009-AUC optimization and the two-sample problem</a></p>
<p>10 0.68476886 <a title="30-lda-10" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>11 0.68448144 <a title="30-lda-11" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>12 0.68430942 <a title="30-lda-12" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>13 0.68412435 <a title="30-lda-13" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>14 0.68324548 <a title="30-lda-14" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>15 0.68153387 <a title="30-lda-15" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>16 0.6804567 <a title="30-lda-16" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>17 0.68004364 <a title="30-lda-17" href="./nips-2009-Accelerating_Bayesian_Structural_Inference_for_Non-Decomposable_Gaussian_Graphical_Models.html">23 nips-2009-Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></p>
<p>18 0.6796214 <a title="30-lda-18" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>19 0.67925394 <a title="30-lda-19" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>20 0.67904866 <a title="30-lda-20" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
