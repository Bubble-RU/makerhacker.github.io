<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2009-Filtering Abstract Senses From Image Search Results</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-96" href="#">nips2009-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2009-Filtering Abstract Senses From Image Search Results</h1>
<br/><p>Source: <a title="nips-2009-96-pdf" href="http://papers.nips.cc/paper/3860-filtering-abstract-senses-from-image-search-results.pdf">pdf</a></p><p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>Reference: <a title="nips-2009-96-reference" href="../nips2009_reference/nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. [sent-5, score-0.917]
</p><p>2 When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. [sent-6, score-0.96]
</p><p>3 As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. [sent-7, score-0.561]
</p><p>4 We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. [sent-8, score-0.815]
</p><p>5 While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. [sent-9, score-0.884]
</p><p>6 We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. [sent-10, score-0.86]
</p><p>7 Our model does not require any human supervision, and takes as input only the name of an object category. [sent-11, score-0.176]
</p><p>8 We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. [sent-12, score-0.424]
</p><p>9 1  Introduction  Many practical scenarios call for robots or agents which can learn a visual model on the ﬂy given only a spoken or textual deﬁnition of an object category. [sent-13, score-0.296]
</p><p>10 A prominent example is the Semantic Robot Vision Challenge (SRVC)1 , which provides robot entrants with a text-ﬁle list of categories to be detected shortly before the competition begins. [sent-14, score-0.127]
</p><p>11 More generally, we would like a robot or agent to be able to engage in situated dialog with a human user and to understand what objects the user is refering to. [sent-15, score-0.276]
</p><p>12 It is generally unreasonable to expect users to refer only to objects covered by static, manually annotated image databases. [sent-16, score-0.339]
</p><p>13 We therefore need a way to ﬁnd images for an arbitrary object in an unsupervised manner. [sent-17, score-0.287]
</p><p>14 A common approach to learning a visual model based solely on the name of an object is to ﬁnd images on the web that co-occur with the object name by using popular web search services, and train a visual classiﬁer from the search results. [sent-18, score-1.296]
</p><p>15 mouse) and are often used in different contexts (e. [sent-21, score-0.087]
</p><p>16 mouse pad), this approach can lead to relatively noisy models. [sent-23, score-0.127]
</p><p>17 Early methods used manual intervention to identify clusters corresponding to the desired sense [2], or grouped together visually coherent sets of images using automatic image clustering (e. [sent-24, score-0.798]
</p><p>18 However, image clusters rarely exactly align with object senses because of the large variation in appearance within most categories. [sent-27, score-0.793]
</p><p>19 Also, clutter from abstract senses of the word that 1  http://www. [sent-28, score-0.683]
</p><p>20 ”  Figure 1:  WISDOM  separates the concrete (physical) senses from the abstract ones. [sent-31, score-0.528]
</p><p>21 are not associated with a physical object can further complicate matters (e. [sent-32, score-0.223]
</p><p>22 ) 2 To address these issues, we propose an unsupervised Web Image Sense DisambiguatiOn Model (WISDOM), illustrated in Figure 1. [sent-35, score-0.043]
</p><p>23 Given a word, WISDOM automatically selects concrete senses of that word from a semantic dictionary and generates images depicting the corresponding entities, ﬁrst ﬁnding coherent topics in both text and image domains, and then grounding the learned topics using the selected word senses. [sent-36, score-2.374]
</p><p>24 Images corresponding to different visual manifestations of a single physical sense are linked together based on the likelihood of their image content and surrounding text (words in close proximity to the image link) being associated with the given sense. [sent-37, score-1.046]
</p><p>25 We make use of a well-known semantic dictionary (WordNet [8]), which has been previously used together with a text-only latent topic model to construct a probabilistic model of individual word senses for use with online images [17]. [sent-38, score-1.273]
</p><p>26 We build on this work by incorporating a visual term, and by using the Wordnet semantic hierarchy to automatically infer whether a particular sense describes a physical entity or a non-physical concept. [sent-39, score-0.529]
</p><p>27 We show results of detecting such concrete senses in two available multimodal (text and image), multi-sense databases: the MIT-ISD dataset [17], and the UIUC-ISD dataset [14]. [sent-40, score-0.589]
</p><p>28 We also experiment with object classiﬁcation in novel images, using classiﬁers trained on the images collected via our method for a set of ten common objects. [sent-41, score-0.244]
</p><p>29 2  Related Work  Several approaches to building object models from image search results have been proposed. [sent-42, score-0.386]
</p><p>30 Some have relied on visual similarity, either selecting a single inlier image cluster based on a small validation set [9], or bootstrapping object classiﬁers from existing labeled images [13]. [sent-43, score-0.659]
</p><p>31 In [18] a classiﬁer based on text features (such as whether the keyword appears in the URL) was used re-rank the images before bootstrapping the image model. [sent-44, score-0.583]
</p><p>32 However, the text ranker was category-independent and thus unable to learn words predictive of a speciﬁc word sense. [sent-45, score-0.535]
</p><p>33 An approach most similar to ours [2] discovered topics in the textual context of images using Latent Dirichlet Allocation (LDA), however, manual intervention by the user was required to sort the topics into positive and negative for each category. [sent-46, score-0.726]
</p><p>34 Also, the combination of image and text features is used in some web retrieval methods (e. [sent-47, score-0.593]
</p><p>35 [7]), however, our work is focused not on instance-based image retrieval, but on category-level modeling. [sent-49, score-0.216]
</p><p>36 In Saenko and Darrell [17], the use of dictionary deﬁnitions to train an unsupervised visual sense model was proposed. [sent-51, score-0.583]
</p><p>37 However, the user was required to manually select the deﬁnition for which to build the model. [sent-52, score-0.108]
</p><p>38 2  sense model did not incorporate visual features, but rather used the text contexts to re-rank images, after which an image classiﬁer was built on the top-ranked results. [sent-54, score-0.798]
</p><p>39 [14] performed spectral clustering in both the text and image domain and evaluated how well the clusters matched different senses. [sent-56, score-0.457]
</p><p>40 However, as a pure clustering approach, this method cannot assign sense labels. [sent-57, score-0.224]
</p><p>41 In the text domain, Yarowsky [20] proposed an unsupervised method for traditional word sense disambiguation (WSD), and suggested the use of dictionary deﬁnitions as an initial seed. [sent-58, score-0.904]
</p><p>42 [4] determined which words are related to a visual domain using hypothesis testing on a target (visual) corpus, compared to a general (non-visual) corpus. [sent-60, score-0.213]
</p><p>43 A related problem is modeling word senses in images manually annotated with words, such as the caption “sky, airplane” [1]. [sent-61, score-0.989]
</p><p>44 Models of annotated images assume that there is a correspondence between each image region and a word in the caption (e. [sent-62, score-0.719]
</p><p>45 Such models predict words, which serve as category labels, based on image content. [sent-65, score-0.216]
</p><p>46 In contrast, our model predicts a category label based on all of the words in the web image’s text context, where a particular word does not necessarily have a corresponding image region, and vice versa. [sent-66, score-0.922]
</p><p>47 In work closely related to Corr-LDA, a People-LDA [11] model is used to guide topic formation in news photos and captions, using a specialized face recognizer. [sent-67, score-0.116]
</p><p>48 The caption data is less constrained than annotations, including non-category words, but still far more constrained than generic webpage text. [sent-68, score-0.102]
</p><p>49 3  Sense-Grounding with a Dictionary Model  We wish to estimate the probability that an image search result embedded in a web page is one of a concrete or abstract concept. [sent-69, score-0.565]
</p><p>50 First, we determine whether the web image is related to a particular word sense, as deﬁned by a dictionary. [sent-70, score-0.654]
</p><p>51 The dictionary model presented in [17] provides an estimate of word sense based on the text associated with the web image. [sent-71, score-1.037]
</p><p>52 We will ﬁrst describe this model, and then extend it to include both an image component and an adaptation step to better reﬂect word senses present in images. [sent-72, score-0.899]
</p><p>53 The dictionary model [17] uses LDA on a large collection of text related to the query word to learn latent senses/uses of the word. [sent-73, score-0.691]
</p><p>54 Each document is modeled as a mixture of topics z ∈ {1, . [sent-77, score-0.221]
</p><p>55 A given collection of M documents, each containing a bag of Nd words, is assumed to be generated by the following process: First, we sample the parameters φj of a multinomial distribution over words from a Dirichlet prior with parameter β for each topic j = 1, . [sent-81, score-0.227]
</p><p>56 For each document d, we sample the parameters θd of a multinomial distribution over topics from a Dirichlet prior with parameter α. [sent-85, score-0.27]
</p><p>57 Finally, for each word token i, we choose a topic zi from the multinomial θd , and then choose a word wi from the multinomial φzi . [sent-86, score-0.667]
</p><p>58 Since learning LDA topics directly from the images’ text contexts can lead to poor results due to the low quantity and irregular quality of such data, an additional dataset of text-only web pages is created for learning, using regular web search. [sent-87, score-0.85]
</p><p>59 The dictionary model then uses the limited text available in the WordNet entries to relate dictionary sense to latent text topics. [sent-88, score-1.047]
</p><p>60 For example, sense 1 of “bass” contains the deﬁnition “the lowest part of the musical range,” as well as the hypernym (“pitch”) and other semantic relations. [sent-89, score-0.409]
</p><p>61 The bag-of-words extracted from such a semantic entry for sense s ∈ {1, 2, . [sent-90, score-0.281]
</p><p>62 The dictionary model assumes that the sense is independent of the words conditioned on the distribution of topics in the document. [sent-97, score-0.665]
</p><p>63 The likelihood of a sense given latent topic z = j is deﬁned as the normalized average likelihood of words in the 3  dictionary entry es , 3 P (s|z) ∝  1 Es  Es  P (ei |z),  (2)  i=1  Incorporating Image Features. [sent-99, score-0.642]
</p><p>64 The dictionary model (1) does not take into account the image part of the image/text pair. [sent-100, score-0.412]
</p><p>65 Here, we extend it to include an image term, which can potentially provide complementary information. [sent-101, score-0.216]
</p><p>66 First, we estimate P (s|di ), or the probability of a sense given an image di . [sent-102, score-0.541]
</p><p>67 Similar to the text-only case, we learn an LDA model consisting of latent topics v ∈ {1, . [sent-103, score-0.264]
</p><p>68 , L}, using the visual bag-of-words extracted from the unlabeled images. [sent-106, score-0.124]
</p><p>69 Intuitively, this provides us with an estimate of the collocation of senses with visual topic. [sent-109, score-0.567]
</p><p>70 Recall that we can estimate θdt for the unseen web image contexts by generalizing the web-text LDA model using Gibbs sampling. [sent-112, score-0.562]
</p><p>71 However, web topics can be a poor match to image search data (e. [sent-113, score-0.668]
</p><p>72 ) Our solution is to adapt the web topics to the image search data. [sent-116, score-0.668]
</p><p>73 We do this by ﬁxing the z assignments of the web documents and sampling the z’s of the image contexts for a few iterations. [sent-117, score-0.539]
</p><p>74 This procedure updates the topics to better reﬂect the latent dimensions present in the image search data, without the overﬁtting effect mentioned earlier. [sent-118, score-0.546]
</p><p>75 4  Filtering out Abstract Senses  To our knowledge, no previous work has considered the task of detecting concrete vs. [sent-119, score-0.085]
</p><p>76 We can do so by virtue of the multimodal sense grounding method presented in the previous section. [sent-121, score-0.32]
</p><p>77 Given a set of senses for a paricular word, our task is to classify each sense as being abstract or concrete. [sent-122, score-0.635]
</p><p>78 Fortunately, WordNet contains relatively direct metadata related to the physicality of a word sense. [sent-123, score-0.27]
</p><p>79 In particular, one of the main functions of WordNet is to put words in semantic relation to each other using the concepts of hyponym and hypernym. [sent-124, score-0.178]
</p><p>80 For example, “scarlet” and “crimson” are hyponyms of “red”, while “color” is a hypernym of “red”. [sent-125, score-0.101]
</p><p>81 Thus, we can detect a concrete sense by examining its hypernym tree to see if it contains one of the following nodes: ’article’, ’instrumentality’,’article of clothing’, ’animal’, or ’body part’. [sent-127, score-0.378]
</p><p>82 What’s more, we can thus restrict the model to speciﬁc types of physical entities: living things, artifacts, clothing, etc. [sent-128, score-0.087]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('senses', 0.443), ('cup', 0.298), ('word', 0.24), ('image', 0.216), ('web', 0.198), ('dictionary', 0.196), ('sense', 0.192), ('topics', 0.188), ('text', 0.179), ('wordnet', 0.177), ('dt', 0.165), ('images', 0.14), ('di', 0.133), ('mouse', 0.127), ('visual', 0.124), ('wisdom', 0.108), ('object', 0.104), ('hypernym', 0.101), ('polysemous', 0.101), ('lda', 0.099), ('words', 0.089), ('semantic', 0.089), ('topic', 0.089), ('physical', 0.087), ('contexts', 0.087), ('concrete', 0.085), ('latent', 0.076), ('name', 0.072), ('caption', 0.072), ('container', 0.067), ('grounding', 0.067), ('intervention', 0.067), ('saenko', 0.067), ('sporting', 0.067), ('search', 0.066), ('user', 0.065), ('multimodal', 0.061), ('robot', 0.059), ('awarded', 0.059), ('trophy', 0.059), ('modality', 0.054), ('disambiguation', 0.054), ('clothing', 0.054), ('coherent', 0.052), ('annotated', 0.051), ('depicting', 0.051), ('multinomial', 0.049), ('bootstrapping', 0.048), ('manually', 0.043), ('unsupervised', 0.043), ('article', 0.042), ('filtering', 0.041), ('textual', 0.041), ('returned', 0.04), ('trevor', 0.04), ('competition', 0.038), ('documents', 0.038), ('manual', 0.037), ('animal', 0.037), ('entity', 0.037), ('entities', 0.035), ('er', 0.035), ('document', 0.033), ('clustering', 0.032), ('visually', 0.032), ('generalizing', 0.032), ('associated', 0.032), ('classi', 0.031), ('databases', 0.031), ('dirichlet', 0.03), ('clusters', 0.03), ('watched', 0.03), ('loeff', 0.03), ('pitch', 0.03), ('dialog', 0.03), ('engage', 0.03), ('entrants', 0.03), ('icsi', 0.03), ('metadata', 0.03), ('timid', 0.03), ('webpage', 0.03), ('unseen', 0.029), ('relate', 0.029), ('generally', 0.029), ('ers', 0.029), ('train', 0.028), ('inlier', 0.027), ('musical', 0.027), ('robots', 0.027), ('darrell', 0.027), ('metal', 0.027), ('pad', 0.027), ('vessel', 0.027), ('photos', 0.027), ('airplane', 0.027), ('captions', 0.027), ('drink', 0.027), ('drinking', 0.027), ('ranker', 0.027), ('situated', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="96-tfidf-1" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>2 0.20123905 <a title="96-tfidf-2" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>3 0.19300495 <a title="96-tfidf-3" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>Author: Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</p><p>Abstract: We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject ﬁrst studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle ﬁlter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability. 1</p><p>4 0.18494762 <a title="96-tfidf-4" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>Author: Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, Tom M. Mitchell</p><p>Abstract: We consider the problem of zero-shot learning, where the goal is to learn a classiﬁer f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we deﬁne the notion of a semantic output code classiﬁer (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classiﬁer and study its theoretical properties in a PAC framework, showing conditions under which the classiﬁer can accurately predict novel classes. As a case study, we build a SOC classiﬁer for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words. 1</p><p>5 0.15527919 <a title="96-tfidf-5" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>Author: Andrew McCallum, David M. Mimno, Hanna M. Wallach</p><p>Abstract: Implementations of topic models typically use symmetric Dirichlet priors with ﬁxed concentration parameters, with the implicit assumption that such “smoothing parameters” have little practical effect. In this paper, we explore several classes of structured priors for topic models. We ﬁnd that an asymmetric Dirichlet prior over the document–topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic–word distributions provides no real beneﬁt. Approximation of this prior structure through simple, efﬁcient hyperparameter optimization steps is sufﬁcient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efﬁcient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling. 1</p><p>6 0.14254738 <a title="96-tfidf-6" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>7 0.13936184 <a title="96-tfidf-7" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>8 0.13253482 <a title="96-tfidf-8" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>9 0.1315096 <a title="96-tfidf-9" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>10 0.12463949 <a title="96-tfidf-10" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>11 0.11819041 <a title="96-tfidf-11" href="./nips-2009-Modeling_Social_Annotation_Data_with_Content_Relevance_using_a_Topic_Model.html">153 nips-2009-Modeling Social Annotation Data with Content Relevance using a Topic Model</a></p>
<p>12 0.11358142 <a title="96-tfidf-12" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>13 0.1115891 <a title="96-tfidf-13" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>14 0.10909477 <a title="96-tfidf-14" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>15 0.10734523 <a title="96-tfidf-15" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>16 0.10122196 <a title="96-tfidf-16" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>17 0.094993576 <a title="96-tfidf-17" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>18 0.093453735 <a title="96-tfidf-18" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>19 0.093189128 <a title="96-tfidf-19" href="./nips-2009-Who%E2%80%99s_Doing_What%3A_Joint_Modeling_of_Names_and_Verbs_for_Simultaneous_Face_and_Pose_Annotation.html">259 nips-2009-Who’s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation</a></p>
<p>20 0.080370978 <a title="96-tfidf-20" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, -0.202), (2, -0.22), (3, -0.17), (4, 0.049), (5, 0.019), (6, -0.11), (7, 0.026), (8, 0.118), (9, 0.27), (10, -0.055), (11, 0.019), (12, 0.047), (13, 0.087), (14, 0.033), (15, -0.009), (16, -0.118), (17, 0.027), (18, 0.018), (19, 0.017), (20, 0.046), (21, 0.015), (22, -0.037), (23, 0.006), (24, -0.025), (25, 0.099), (26, -0.025), (27, 0.012), (28, -0.033), (29, -0.026), (30, 0.007), (31, -0.055), (32, -0.008), (33, -0.064), (34, -0.017), (35, -0.035), (36, -0.01), (37, 0.022), (38, 0.038), (39, -0.081), (40, -0.03), (41, 0.046), (42, 0.002), (43, 0.067), (44, 0.073), (45, 0.087), (46, -0.056), (47, 0.002), (48, 0.049), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97027385 <a title="96-lsi-1" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>2 0.72589123 <a title="96-lsi-2" href="./nips-2009-Modeling_Social_Annotation_Data_with_Content_Relevance_using_a_Topic_Model.html">153 nips-2009-Modeling Social Annotation Data with Content Relevance using a Topic Model</a></p>
<p>Author: Tomoharu Iwata, Takeshi Yamada, Naonori Ueda</p><p>Abstract: We propose a probabilistic topic model for analyzing and extracting contentrelated annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy, i.e. not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classiﬁcation and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.</p><p>3 0.71548605 <a title="96-lsi-3" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>Author: Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</p><p>Abstract: A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred nonparametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efﬁciently via variational Bayesian analysis, with example results presented on two image databases.</p><p>4 0.68265492 <a title="96-lsi-4" href="./nips-2009-A_Bayesian_Analysis_of_Dynamics_in_Free_Recall.html">4 nips-2009-A Bayesian Analysis of Dynamics in Free Recall</a></p>
<p>Author: Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</p><p>Abstract: We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject ﬁrst studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle ﬁlter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability. 1</p><p>5 0.67864686 <a title="96-lsi-5" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><p>6 0.6550948 <a title="96-lsi-6" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>7 0.64931583 <a title="96-lsi-7" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>8 0.63792765 <a title="96-lsi-8" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>9 0.59207529 <a title="96-lsi-9" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>10 0.57604742 <a title="96-lsi-10" href="./nips-2009-Polynomial_Semantic_Indexing.html">190 nips-2009-Polynomial Semantic Indexing</a></p>
<p>11 0.56943798 <a title="96-lsi-11" href="./nips-2009-Nonparametric_Bayesian_Texture_Learning_and_Synthesis.html">172 nips-2009-Nonparametric Bayesian Texture Learning and Synthesis</a></p>
<p>12 0.56297463 <a title="96-lsi-12" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>13 0.55503839 <a title="96-lsi-13" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>14 0.53142476 <a title="96-lsi-14" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>15 0.52496213 <a title="96-lsi-15" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>16 0.48836952 <a title="96-lsi-16" href="./nips-2009-Whose_Vote_Should_Count_More%3A_Optimal_Integration_of_Labels_from_Labelers_of_Unknown_Expertise.html">258 nips-2009-Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></p>
<p>17 0.47503272 <a title="96-lsi-17" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>18 0.45336246 <a title="96-lsi-18" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<p>19 0.44557548 <a title="96-lsi-19" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>20 0.44298548 <a title="96-lsi-20" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.029), (25, 0.066), (35, 0.046), (36, 0.09), (39, 0.069), (46, 0.255), (58, 0.047), (61, 0.013), (71, 0.127), (86, 0.128), (91, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82811373 <a title="96-lda-1" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>Author: Joel Veness, David Silver, Alan Blair, William W. Cohen</p><p>Abstract: In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuel’s checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play. 1</p><p>same-paper 2 0.82350564 <a title="96-lda-2" href="./nips-2009-Filtering_Abstract_Senses_From_Image_Search_Results.html">96 nips-2009-Filtering Abstract Senses From Image Search Results</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to ﬁnd images on the web that are associated with the object name and train a visual classiﬁer from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classiﬁer to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difﬁcult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classiﬁers trained on concrete-sense images returned by our method for a set of ten common ofﬁce objects. 1</p><p>3 0.81312472 <a title="96-lda-3" href="./nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the “topics”). In the sparse topic model (sparseTM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efﬁcient Gibbs sampler for the sparseTM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the sparseTM on four real-world datasets. Compared to traditional approaches, the empirical results will show that sparseTMs give better predictive performance with simpler inferred models. 1</p><p>4 0.64880669 <a title="96-lda-4" href="./nips-2009-Replicated_Softmax%3A_an_Undirected_Topic_Model.html">204 nips-2009-Replicated Softmax: an Undirected Topic Model</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We introduce a two-layer undirected graphical model, called a “Replicated Softmax”, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efﬁcient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.</p><p>5 0.64553696 <a title="96-lda-5" href="./nips-2009-Conditional_Neural_Fields.html">56 nips-2009-Conditional Neural Fields</a></p>
<p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><p>6 0.63573015 <a title="96-lda-6" href="./nips-2009-Learning_from_Multiple_Partially_Observed_Views_-_an_Application_to_Multilingual_Text_Categorization.html">130 nips-2009-Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization</a></p>
<p>7 0.6299752 <a title="96-lda-7" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>8 0.6257571 <a title="96-lda-8" href="./nips-2009-Rethinking_LDA%3A_Why_Priors_Matter.html">205 nips-2009-Rethinking LDA: Why Priors Matter</a></p>
<p>9 0.62383211 <a title="96-lda-9" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>10 0.62368691 <a title="96-lda-10" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>11 0.62235761 <a title="96-lda-11" href="./nips-2009-Quantification_and_the_language_of_thought.html">196 nips-2009-Quantification and the language of thought</a></p>
<p>12 0.6205346 <a title="96-lda-12" href="./nips-2009-Bayesian_Nonparametric_Models_on_Decomposable_Graphs.html">40 nips-2009-Bayesian Nonparametric Models on Decomposable Graphs</a></p>
<p>13 0.62005109 <a title="96-lda-13" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>14 0.61619359 <a title="96-lda-14" href="./nips-2009-Spatial_Normalized_Gamma_Processes.html">226 nips-2009-Spatial Normalized Gamma Processes</a></p>
<p>15 0.61416811 <a title="96-lda-15" href="./nips-2009-Training_Factor_Graphs_with_Reinforcement_Learning_for_Efficient_MAP_Inference.html">250 nips-2009-Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></p>
<p>16 0.61001849 <a title="96-lda-16" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>17 0.6087513 <a title="96-lda-17" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>18 0.60722178 <a title="96-lda-18" href="./nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></p>
<p>19 0.60719723 <a title="96-lda-19" href="./nips-2009-A_joint_maximum-entropy_model_for_binary_neural_population_patterns_and_continuous_signals.html">19 nips-2009-A joint maximum-entropy model for binary neural population patterns and continuous signals</a></p>
<p>20 0.60718679 <a title="96-lda-20" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
