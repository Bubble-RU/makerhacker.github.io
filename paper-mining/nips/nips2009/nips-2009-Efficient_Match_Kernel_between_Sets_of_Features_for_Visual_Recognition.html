<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-77" href="#">nips2009-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</h1>
<br/><p>Source: <a title="nips-2009-77-pdf" href="http://papers.nips.cc/paper/3874-efficient-match-kernel-between-sets-of-features-for-visual-recognition.pdf">pdf</a></p><p>Author: Liefeng Bo, Cristian Sminchisescu</p><p>Abstract: In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classiﬁers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels for large datasets due to their signiﬁcant computational cost. To address this problem, we propose efﬁcient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a setlevel feature. The local feature maps are learned so their inner products preserve, to the best possible, the values of the speciﬁed kernel function. Classiﬁers based on EMK are linear both in the number of images and in the number of local features. We demonstrate that EMK are extremely efﬁcient and achieve the current state of the art in three difﬁcult computer vision datasets: Scene-15, Caltech-101 and Caltech-256. 1</p><p>Reference: <a title="nips-2009-77-reference" href="../nips2009_reference/nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract In visual recognition, the images are frequently modeled as unordered collections of local features (bags). [sent-5, score-0.306]
</p><p>2 We show that bag-of-words representations commonly used in conjunction with linear classiﬁers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. [sent-6, score-0.462]
</p><p>3 Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. [sent-7, score-0.463]
</p><p>4 To address this problem, we propose efﬁcient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a setlevel feature. [sent-9, score-0.682]
</p><p>5 The local feature maps are learned so their inner products preserve, to the best possible, the values of the speciﬁed kernel function. [sent-10, score-0.57]
</p><p>6 1  Introduction  Models based on local features have achieved state-of-the art results in many visual object recognition tasks. [sent-13, score-0.332]
</p><p>7 For example, an image can be described by a set of local features extracted from patches around salient interest points or regular grids, or a shape can be described by a set of local features deﬁned at edge points. [sent-14, score-0.449]
</p><p>8 This raises the question on how should one measure the similarity between two images represented as sets of local features. [sent-15, score-0.196]
</p><p>9 BOW represents each local feature with the closest visual word and counts the occurrence frequencies in the image. [sent-18, score-0.25]
</p><p>10 The resulting histogram is used as an image descriptor for object recognition, often in conjunction with linear classiﬁers. [sent-19, score-0.225]
</p><p>11 Various methods for creating vocabularies exist [10], the most common being k-means clustering of all (or a subsample of) the local features to obtain visual words. [sent-21, score-0.275]
</p><p>12 An even better approach to recognition is to deﬁne kernels over sets of local features. [sent-22, score-0.296]
</p><p>13 The sum match kernel of Haussler [7] is obtained by adding local kernels over all combinations of local features from two different sets. [sent-24, score-0.843]
</p><p>14 In [17], the authors modify the sum kernel by introducing an integer exponent on local kernels. [sent-25, score-0.361]
</p><p>15 Neighborhood kernels [20] integrate the spatial location of local features into a sum match kernel. [sent-26, score-0.534]
</p><p>16 Pyramid match kernels [5, 14, 13] map local features to multi-resolution histograms and compute a weighted histogram intersection. [sent-27, score-0.553]
</p><p>17 Algebraic set kernels [26] exploit tensor products to aggregate local kernels, whereas principal angle kernels 1  [29] measure similarities based on angles between linear subspaces spanned by local features in the two sets. [sent-28, score-0.711]
</p><p>18 All of the above methods need to explicitly evaluate the full kernel matrix, hence they require space and time complexity that is quadratic in the number of images. [sent-30, score-0.225]
</p><p>19 In this paper we present efﬁcient match kernels (EMK) that combine the strengths of both bag of words and set kernels. [sent-32, score-0.355]
</p><p>20 We map local features to a low dimensional feature space and construct set-level features by averaging the resulting feature vectors. [sent-33, score-0.537]
</p><p>21 Hence EMK can be used in conjunction with linear classiﬁers and do not require the explicit computation of a full kernel matrix—this leads to both space and time complexity that is linear in the number of images. [sent-35, score-0.289]
</p><p>22 We adopt a bag of features method, which represents an image as a set of local features. [sent-39, score-0.298]
</p><p>23 , xp } be a set of local features in an image and V = {v1 , . [sent-43, score-0.255]
</p><p>24 In BOW, each local feature is quantized into a D dimensional binary indicator vector µ(x) = [µ1 (x), . [sent-47, score-0.244]
</p><p>25 1 The feature vectors for one image form a normalized histogram µ(X) = |X| x∈X µ(x), where | · | is the cardinality of a set. [sent-52, score-0.215]
</p><p>26 BOW features can be used in conjunction with either a linear or a kernel classiﬁer, albeit the latter often leads to expensive training and testing (see §4). [sent-53, score-0.495]
</p><p>27 When a linear classiﬁer is used, the resulting kernel function is: KB (X, Y) = µ(X) µ(Y) = with δ(x, y) =  1 |X||Y|  µ(x) µ(y) = x∈X y∈Y  1 |X||Y|  1, x, y ⊂ R(vi ), ∃i ∈ {1, . [sent-54, score-0.225]
</p><p>28 , D} 0, otherwise  δ(x, y)  (1)  x∈X y∈Y  (2)  δ(x, y) is obviously a positive deﬁnite kernel, measuring the similarity between two local features x and y: δ(x, y) = 1 if x and y belong the same region R(vi ), and 0 otherwise. [sent-57, score-0.222]
</p><p>29 However, this type of quantization can be too coarse when measuring the similarity of two local features (see also ﬁg. [sent-58, score-0.26]
</p><p>30 Better would be to replace δ(x, y) with a continuous kernel function that more accurately measures the similarity between x and y: 1 KS (X, Y) = k(x, y) (3) |X||Y| x∈X y∈Y  In fact, this is related to the normalized sum match kernel [7, 17]. [sent-60, score-0.632]
</p><p>31 A negative impact of kernelization is the high computational cost required to compute the summation match function, which takes O(|X||Y|) for a single kernel value rather than O(1), the cost of evaluating a single kernel function deﬁned on vectors. [sent-63, score-0.655]
</p><p>32 When used in conjunction with kernel machines, it takes O(n2 ) and O(n2 m2 d) to store and compute the entire kernel matrix, respectively, where n is the number of images in the training set, and m is the average cardinality of all sets. [sent-64, score-0.644]
</p><p>33 In addition to expensive training, the match kernel function has also a fairly high testing cost: n 2 for a test input, evaluating the discriminant f (X) = i=1 αi Ks (Xi , X) takes O(nm d). [sent-66, score-0.402]
</p><p>34 For sparse kernel machines, such as SVMs, the cost can decrease to some extent, as some of the αi are zero. [sent-68, score-0.264]
</p><p>35 The kernel function k(x, y) = φ(x) φ(y) is called ﬁnite dimensional if the feature map φ(·) is ﬁnite dimensional. [sent-71, score-0.395]
</p><p>36 PMK is the pyramid match kernel of [6], with T in PMK giving the value of the maximal feature range. [sent-76, score-0.635]
</p><p>37 D in EMK is the dimensionality of feature maps and does not change with the training set size. [sent-78, score-0.318]
</p><p>38 EMK uses linear classiﬁers and does not require the evaluation of the kernel matrix. [sent-81, score-0.225]
</p><p>39 The other four methods are used in conjunction with kernel classiﬁers, hence they all need to evaluate the entire kernel matrix. [sent-82, score-0.514]
</p><p>40 With the ﬁnite dimensional kernel, the match kernel can be simpliﬁed as: KS (X, Y) = φ(X) φ(Y)  (4)  1 where φ(X) = |X| x∈X φ(x) is the feature map on the set of vectors. [sent-85, score-0.522]
</p><p>41 The training and testing costs are O(nmDd) and O(mDd) respectively, where D is the dimensionality of the feature map φ(x). [sent-89, score-0.289]
</p><p>42 If the feature map φ(x) is low dimensional, the computational cost of EMK can be much lower than the one required to evaluate the match kernel by computing the kernel functions 1 k(x, y). [sent-90, score-0.739]
</p><p>43 Notice that we only need the feature vectors φ(X) in EMK, hence it is not necessary to compute the entire kernel matrix. [sent-92, score-0.343]
</p><p>44 While there can be many choices for the local feature maps φ(x)—and the positive deﬁniteness of k(x, y) = φ(x) φ(x) can be always guaranteed—, most do not necessarily lead to a meaningful similarity measure. [sent-99, score-0.339]
</p><p>45 In the paper, we give two principled methods to create meaningful local feature maps φ(x), by arranging for their inner products to approximate a given kernel function. [sent-100, score-0.57]
</p><p>46 3  Efﬁcient Match Kernels  In this section we present two kernel approximations, based on low-dimensional projections (§3. [sent-101, score-0.225]
</p><p>47 Given {ψ(zi )}D , a i=1 set of basis vectors zi , we can approximate the feature vector ψ(x): vx = argmin ψ(x) − Hvx vx  3  2  (5)  Approximation Exact  1. [sent-106, score-0.271]
</p><p>48 Left: approximated Gaussian kernel with 20 learned feature maps. [sent-119, score-0.313]
</p><p>49 Right: approximated Gaussian kernel based on 200 random Fourier features. [sent-121, score-0.225]
</p><p>50 The feature maps are learned from 200 samples, uniformly drawn from [-10,10]. [sent-122, score-0.202]
</p><p>51 For G G = K−1 (notice that K−1 is positive deﬁnite), the local feature maps are: ZZ ZZ φ(x) = GkZ (x)  (8)  1 The resulting full feature map is: φ(X) = |X| G x∈X kZ (x) , with computational complexity 2 O(mDd + D ) for a set of local features. [sent-128, score-0.543]
</p><p>52 A related method is the kernel codebook [28], where a set-level feature is also extracted based on a local kernel, but with different feature map φ(·). [sent-129, score-0.594]
</p><p>53 An essential difference is that inner products of our set-level features φ(X) formally approximate the sum-match kernel, whereas the ones induced by the kernel codebook do not. [sent-130, score-0.422]
</p><p>54 Therefore EMK only requires a linear classiﬁer wherea a kernel codebook would require a non-linear classiﬁer for comparable performance. [sent-131, score-0.274]
</p><p>55 Our experiments, shown in table 3, further suggest that EMK outperforms the kernel codebook, even in the non-linear case. [sent-133, score-0.225]
</p><p>56 One way is kernel principal component analysis (KPCA) [24] on a randomly selected pool of F local features, with the basis set to the topmost D eigenvectors. [sent-135, score-0.357]
</p><p>57 This faces two difﬁculties, however: (i) KPCA scales cubically in the number of selected local features, F ; (ii) O(F md) work is required to extract the set-level feature vector for one image, because F the eigenvectors are linear combinations of the selected local feature vectors, i=1 αi ψ(xi ). [sent-136, score-0.394]
</p><p>58 This motivates our constrained singular value decomposition in kernel feature space (CKSVD): argmin R(V, Z) = V,Z  1 F  F  ψ(xi ) − Hvi  2  (9)  i=1  where F is the number of the randomly selected local features, Z = [z1 , . [sent-142, score-0.448]
</p><p>59 2 Random Fourier Set Features Another tractable approach to large-scale learning is to approximate the kernel using random feature maps [22, 23]. [sent-167, score-0.427]
</p><p>60 For a given function µ(x; θ) and the probability distribution p(θ), one can deﬁne the local kernel as: kf (x, y) = p(θ)µ(x; θ)µ(y, θ)dθ. [sent-168, score-0.359]
</p><p>61 We consider feature maps of the form µ(x; θ) = cos(ω x + b) with θ = (ω, b), which project local features to a randomly chosen line, then pass the resulting scalar through a sinusoid. [sent-169, score-0.396]
</p><p>62 For example, to approximate the Gaussian kernel kf (x, y) = exp(−γ x − y 2 ), the random feature maps are: φ(x) = 2 D [cos(ω1  x + b1 ),. [sent-170, score-0.452]
</p><p>63 Although any shift invariant kernel can be represented using random Fourier features, currently these are limited to Gaussian kernels or to kernels with analytical inverse Fourier transforms. [sent-177, score-0.547]
</p><p>64 The constraint of a shiftinvariant kernel excludes a number of practically interesting similarities. [sent-179, score-0.225]
</p><p>65 For example, the χ2 kernel [8] and the histogram intersection kernel [5] are designed to compare histograms, hence they can be used as local kernels, if the features are histograms. [sent-180, score-0.68]
</p><p>66 1) can produce superior results when the dimensionality of the feature maps is small. [sent-184, score-0.247]
</p><p>67 BOW-Linear and BOW-Gaussian use a linear classiﬁer and a Gaussian kernel classiﬁer on BOW features, respectively. [sent-189, score-0.225]
</p><p>68 For the former, we learn low dimensional feature maps (§3. [sent-191, score-0.249]
</p><p>69 The local features are SIFT descriptors [16] extracted from 16×16 image patches. [sent-195, score-0.29]
</p><p>70 For EMK, our local kernel is a Gaussian 5  exp(−γ x − y 2 ). [sent-197, score-0.334]
</p><p>71 We run k-means clustering to identify the visual words and stochastic gradient descent to learn the local feature maps, using a 100,000 random set of SIFT descriptors. [sent-199, score-0.32]
</p><p>72 The regularization and the kernel parameters (if available) in SVM are tuned by ten-fold cross validation on the training set. [sent-202, score-0.296]
</p><p>73 The dimensionality of the feature maps and the vocabulary size are both set to 1000 for fair comparisons, unless otherwise speciﬁed. [sent-203, score-0.297]
</p><p>74 We vary the dimensionality of the feature maps (EMK) and the vocabulary size (BOW) from 250 to 2000 with step length 250. [sent-212, score-0.297]
</p><p>75 For this dataset, we only consider the ﬂat BOW and EMK (only pyramid level 0) in all experiments. [sent-213, score-0.195]
</p><p>76 Our second experiment is similar with the ﬁrst one, but the dimensionality of the feature maps and the vocabulary size vary from 50 to 200 with step length 25. [sent-216, score-0.297]
</p><p>77 In our third experiment, we ﬁx the dimensionality of the feature maps to 1000, and vary the training set size from 300 to 2400 with step length 300. [sent-217, score-0.318]
</p><p>78 We observe that EMK-CKSVD signiﬁcantly outperforms EMKFourier for low-dimensional feature maps, indicating that learned features preserve the values of the Gaussian kernel better than the random Fourier maps in this regime, see also ﬁg. [sent-222, score-0.512]
</p><p>79 The sum match kernel takes about 10 hours for training and 10 hours for testing, respectively whereas EMK-Fourier and EMK-CKSVD need less than 1 hour, most spent computing SIFT descriptors. [sent-231, score-0.527]
</p><p>80 In addition, we use 10,000 randomly selected SIFT descriptors to learn KPCA-based local feature maps, which takes about 12 hours for the training and testing sets on the full Scene-15 dataset, respectively. [sent-232, score-0.377]
</p><p>81 We consider three pyramid levels: L = 0, L = 1, amd L = 2 (for the latter two, spatial information is used). [sent-239, score-0.22]
</p><p>82 EMK-Fourier and EMK-CKSVD perform substantially better than BOW-Linear and BOWGaussian for all pyramid levels. [sent-242, score-0.195]
</p><p>83 The performance gap increases as more pyramid levels are added. [sent-243, score-0.222]
</p><p>84 The training set size is 1500 in the left plot; the dimensionality of feature maps and the vocabulary size are both set to 1000 in the right plot (for fair comparisons). [sent-267, score-0.368]
</p><p>85 8  Table 2: Classiﬁcation accuracy comparisons for three pyramid levels. [sent-316, score-0.277]
</p><p>86 The dimensionality of the feature maps and the vocabulary size are both set to 1000. [sent-318, score-0.297]
</p><p>87 To compare the four algorithms computationally, we select images from each category proportionally to the total number of images of that category, as the training set. [sent-328, score-0.244]
</p><p>88 To accelerate BOW-Gaussian, we precompute the entire kernel matrix. [sent-337, score-0.225]
</p><p>89 7  Algorithms 15 training 30 training 45 training 60 training  BOW-Linear 17. [sent-374, score-0.284]
</p><p>90 The dimensionality of the feature maps and the vocabulary size are both set to 1000 (for fair comparisons). [sent-408, score-0.297]
</p><p>91 We illustrate the quantization limitations of such models and propose more sophisticated kernel approximations that preserve the computational efﬁciency of bag-of-words while being just as (or more) accurate than the existing, computationally demanding, non-linear kernels. [sent-425, score-0.288]
</p><p>92 The models we propose are built around Efﬁcient Match Kernels (EMK), which map local features to a low dimensional feature space, average the resulting feature vectors to form a set-level feature, then apply a linear classiﬁer. [sent-426, score-0.482]
</p><p>93 The pyramid match kernel: discriminative classiﬁcation with sets of image features. [sent-458, score-0.383]
</p><p>94 The pyramid match kernel: Efﬁcient learning with sets of features. [sent-463, score-0.322]
</p><p>95 Local features and kernels for classiﬁcation of texture and object categories: A comprehensive study. [sent-473, score-0.283]
</p><p>96 Iterative kernel principal component analysis for image o modeling. [sent-490, score-0.309]
</p><p>97 Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. [sent-506, score-0.227]
</p><p>98 A kullback-leibler divergence based kernel for svm classiﬁcation in multimedia applications. [sent-526, score-0.248]
</p><p>99 Nonlinear component analysis as a kernel eigenvalue o u problem. [sent-562, score-0.225]
</p><p>100 Algebraic set kernels with application to inference over local image representations. [sent-574, score-0.331]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emk', 0.692), ('bow', 0.318), ('kernel', 0.225), ('pyramid', 0.195), ('kernels', 0.161), ('kz', 0.156), ('match', 0.127), ('maps', 0.114), ('fourier', 0.112), ('local', 0.109), ('cksvd', 0.104), ('pmk', 0.104), ('feature', 0.088), ('mdd', 0.086), ('features', 0.085), ('classi', 0.073), ('training', 0.071), ('zz', 0.069), ('nmdd', 0.069), ('conjunction', 0.064), ('sift', 0.064), ('image', 0.061), ('images', 0.059), ('category', 0.055), ('vi', 0.055), ('visual', 0.053), ('ks', 0.052), ('vx', 0.052), ('bowgaussian', 0.052), ('kzz', 0.052), ('testing', 0.05), ('vocabulary', 0.05), ('codebook', 0.049), ('bhattacharyya', 0.049), ('sgd', 0.049), ('kpca', 0.049), ('dimensional', 0.047), ('dimensionality', 0.045), ('accuracy', 0.044), ('bag', 0.043), ('er', 0.043), ('cvpr', 0.039), ('cost', 0.039), ('quantization', 0.038), ('comparisons', 0.038), ('zd', 0.037), ('object', 0.037), ('histogram', 0.036), ('map', 0.035), ('descriptors', 0.035), ('dimentionality', 0.035), ('emkfourier', 0.035), ('hvi', 0.035), ('hvx', 0.035), ('trainning', 0.035), ('ers', 0.034), ('products', 0.034), ('scene', 0.032), ('iccv', 0.032), ('liefeng', 0.03), ('liblinear', 0.03), ('vectors', 0.03), ('whereas', 0.029), ('notice', 0.029), ('similarity', 0.028), ('codebooks', 0.028), ('hmax', 0.028), ('rahimi', 0.028), ('bo', 0.028), ('vocabularies', 0.028), ('levels', 0.027), ('descriptor', 0.027), ('sum', 0.027), ('recognition', 0.026), ('datasets', 0.026), ('argmin', 0.026), ('grauman', 0.026), ('lazebnik', 0.026), ('approximations', 0.025), ('gaussian', 0.025), ('spatial', 0.025), ('kf', 0.025), ('caltech', 0.025), ('hours', 0.024), ('gradient', 0.024), ('words', 0.024), ('cos', 0.024), ('grids', 0.023), ('schmid', 0.023), ('nips', 0.023), ('principal', 0.023), ('svm', 0.023), ('zi', 0.023), ('svms', 0.023), ('jmlr', 0.023), ('closure', 0.022), ('mercer', 0.022), ('descent', 0.022), ('art', 0.022), ('thousands', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="77-tfidf-1" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Cristian Sminchisescu</p><p>Abstract: In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classiﬁers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels for large datasets due to their signiﬁcant computational cost. To address this problem, we propose efﬁcient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a setlevel feature. The local feature maps are learned so their inner products preserve, to the best possible, the values of the speciﬁed kernel function. Classiﬁers based on EMK are linear both in the number of images and in the number of local features. We demonstrate that EMK are extremely efﬁcient and achieve the current state of the art in three difﬁcult computer vision datasets: Scene-15, Caltech-101 and Caltech-256. 1</p><p>2 0.15758072 <a title="77-tfidf-2" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. We analyze this problem in the case of regression and the kernel ridge regression algorithm. We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.</p><p>3 0.14086351 <a title="77-tfidf-3" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>Author: Youngmin Cho, Lawrence K. Saul</p><p>Abstract: We introduce a new family of positive-deﬁnite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets. 1</p><p>4 0.11194707 <a title="77-tfidf-4" href="./nips-2009-Kernel_Choice_and_Classifiability_for_RKHS_Embeddings_of_Probability_Distributions.html">118 nips-2009-Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions</a></p>
<p>Author: Kenji Fukumizu, Arthur Gretton, Gert R. Lanckriet, Bernhard Schölkopf, Bharath K. Sriperumbudur</p><p>Abstract: Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of ﬁnite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classiﬁer, thus forming a natural link between the distance between distributions and their ease of classiﬁcation. An important consequence is that a kernel must be characteristic to guarantee classiﬁability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive deﬁnite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classiﬁer. The generalized MMD is shown to have consistent ﬁnite sample estimates, and its performance is demonstrated on a homogeneity testing example. 1</p><p>5 0.10575151 <a title="77-tfidf-5" href="./nips-2009-Fast_subtree_kernels_on_graphs.html">95 nips-2009-Fast subtree kernels on graphs</a></p>
<p>Author: Nino Shervashidze, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon & G¨ rtner scales as O(n2 4d h). Key to this efﬁciency is the observation that the a Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classiﬁcation benchmark datasets in terms of accuracy and runtime. 1</p><p>6 0.10417251 <a title="77-tfidf-6" href="./nips-2009-Kernels_and_learning_curves_for_Gaussian_process_regression_on_random_graphs.html">120 nips-2009-Kernels and learning curves for Gaussian process regression on random graphs</a></p>
<p>7 0.097839773 <a title="77-tfidf-7" href="./nips-2009-On_the_Algorithmics_and_Applications_of_a_Mixed-norm_based_Kernel_Learning_Formulation.html">179 nips-2009-On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation</a></p>
<p>8 0.081582837 <a title="77-tfidf-8" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>9 0.078270711 <a title="77-tfidf-9" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>10 0.077796616 <a title="77-tfidf-10" href="./nips-2009-Efficient_and_Accurate_Lp-Norm_Multiple_Kernel_Learning.html">80 nips-2009-Efficient and Accurate Lp-Norm Multiple Kernel Learning</a></p>
<p>11 0.076547489 <a title="77-tfidf-11" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>12 0.073777556 <a title="77-tfidf-12" href="./nips-2009-A_Fast%2C_Consistent_Kernel_Two-Sample_Test.html">8 nips-2009-A Fast, Consistent Kernel Two-Sample Test</a></p>
<p>13 0.073334888 <a title="77-tfidf-13" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>14 0.072708324 <a title="77-tfidf-14" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>15 0.071053624 <a title="77-tfidf-15" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>16 0.070884325 <a title="77-tfidf-16" href="./nips-2009-Locality-sensitive_binary_codes_from_shift-invariant_kernels.html">142 nips-2009-Locality-sensitive binary codes from shift-invariant kernels</a></p>
<p>17 0.069247432 <a title="77-tfidf-17" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>18 0.068205841 <a title="77-tfidf-18" href="./nips-2009-Zero-shot_Learning_with_Semantic_Output_Codes.html">260 nips-2009-Zero-shot Learning with Semantic Output Codes</a></p>
<p>19 0.067272618 <a title="77-tfidf-19" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>20 0.065831766 <a title="77-tfidf-20" href="./nips-2009-Linear-time_Algorithms_for_Pairwise_Statistical_Problems.html">139 nips-2009-Linear-time Algorithms for Pairwise Statistical Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.203), (1, -0.012), (2, -0.129), (3, 0.079), (4, -0.079), (5, 0.053), (6, -0.017), (7, 0.18), (8, -0.041), (9, 0.005), (10, 0.124), (11, -0.113), (12, -0.003), (13, 0.087), (14, -0.093), (15, 0.027), (16, -0.063), (17, 0.105), (18, 0.031), (19, -0.054), (20, 0.03), (21, 0.022), (22, -0.028), (23, 0.046), (24, -0.018), (25, 0.037), (26, -0.013), (27, -0.02), (28, -0.066), (29, 0.037), (30, -0.042), (31, -0.014), (32, -0.057), (33, -0.024), (34, 0.046), (35, 0.034), (36, 0.001), (37, 0.083), (38, -0.043), (39, 0.05), (40, -0.029), (41, 0.034), (42, -0.042), (43, -0.072), (44, 0.029), (45, 0.067), (46, 0.054), (47, -0.021), (48, 0.038), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95265669 <a title="77-lsi-1" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Cristian Sminchisescu</p><p>Abstract: In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classiﬁers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels for large datasets due to their signiﬁcant computational cost. To address this problem, we propose efﬁcient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a setlevel feature. The local feature maps are learned so their inner products preserve, to the best possible, the values of the speciﬁed kernel function. Classiﬁers based on EMK are linear both in the number of images and in the number of local features. We demonstrate that EMK are extremely efﬁcient and achieve the current state of the art in three difﬁcult computer vision datasets: Scene-15, Caltech-101 and Caltech-256. 1</p><p>2 0.79210585 <a title="77-lsi-2" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. We analyze this problem in the case of regression and the kernel ridge regression algorithm. We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.</p><p>3 0.77744663 <a title="77-lsi-3" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>Author: Youngmin Cho, Lawrence K. Saul</p><p>Abstract: We introduce a new family of positive-deﬁnite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets. 1</p><p>4 0.71429473 <a title="77-lsi-4" href="./nips-2009-Kernel_Choice_and_Classifiability_for_RKHS_Embeddings_of_Probability_Distributions.html">118 nips-2009-Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions</a></p>
<p>Author: Kenji Fukumizu, Arthur Gretton, Gert R. Lanckriet, Bernhard Schölkopf, Bharath K. Sriperumbudur</p><p>Abstract: Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of ﬁnite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classiﬁer, thus forming a natural link between the distance between distributions and their ease of classiﬁcation. An important consequence is that a kernel must be characteristic to guarantee classiﬁability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive deﬁnite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classiﬁer. The generalized MMD is shown to have consistent ﬁnite sample estimates, and its performance is demonstrated on a homogeneity testing example. 1</p><p>5 0.6385029 <a title="77-lsi-5" href="./nips-2009-Fast_subtree_kernels_on_graphs.html">95 nips-2009-Fast subtree kernels on graphs</a></p>
<p>Author: Nino Shervashidze, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon & G¨ rtner scales as O(n2 4d h). Key to this efﬁciency is the observation that the a Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classiﬁcation benchmark datasets in terms of accuracy and runtime. 1</p><p>6 0.63471282 <a title="77-lsi-6" href="./nips-2009-A_Fast%2C_Consistent_Kernel_Two-Sample_Test.html">8 nips-2009-A Fast, Consistent Kernel Two-Sample Test</a></p>
<p>7 0.6307168 <a title="77-lsi-7" href="./nips-2009-Fast_Graph_Laplacian_Regularized_Kernel_Learning_via_Semidefinite%E2%80%93Quadratic%E2%80%93Linear_Programming.html">92 nips-2009-Fast Graph Laplacian Regularized Kernel Learning via Semidefinite–Quadratic–Linear Programming</a></p>
<p>8 0.62325293 <a title="77-lsi-8" href="./nips-2009-Kernels_and_learning_curves_for_Gaussian_process_regression_on_random_graphs.html">120 nips-2009-Kernels and learning curves for Gaussian process regression on random graphs</a></p>
<p>9 0.53600633 <a title="77-lsi-9" href="./nips-2009-Efficient_and_Accurate_Lp-Norm_Multiple_Kernel_Learning.html">80 nips-2009-Efficient and Accurate Lp-Norm Multiple Kernel Learning</a></p>
<p>10 0.52164227 <a title="77-lsi-10" href="./nips-2009-Fast_Image_Deconvolution_using_Hyper-Laplacian_Priors.html">93 nips-2009-Fast Image Deconvolution using Hyper-Laplacian Priors</a></p>
<p>11 0.51716805 <a title="77-lsi-11" href="./nips-2009-On_the_Algorithmics_and_Applications_of_a_Mixed-norm_based_Kernel_Learning_Formulation.html">179 nips-2009-On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation</a></p>
<p>12 0.47625086 <a title="77-lsi-12" href="./nips-2009-Analysis_of_SVM_with_Indefinite_Kernels.html">33 nips-2009-Analysis of SVM with Indefinite Kernels</a></p>
<p>13 0.47481331 <a title="77-lsi-13" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>14 0.46203354 <a title="77-lsi-14" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>15 0.4556846 <a title="77-lsi-15" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>16 0.43514913 <a title="77-lsi-16" href="./nips-2009-Speaker_Comparison_with_Inner_Product_Discriminant_Functions.html">227 nips-2009-Speaker Comparison with Inner Product Discriminant Functions</a></p>
<p>17 0.43082869 <a title="77-lsi-17" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>18 0.42554399 <a title="77-lsi-18" href="./nips-2009-Locality-sensitive_binary_codes_from_shift-invariant_kernels.html">142 nips-2009-Locality-sensitive binary codes from shift-invariant kernels</a></p>
<p>19 0.42489508 <a title="77-lsi-19" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>20 0.41461727 <a title="77-lsi-20" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.011), (24, 0.034), (25, 0.064), (35, 0.055), (36, 0.141), (39, 0.064), (48, 0.256), (55, 0.011), (58, 0.087), (71, 0.04), (81, 0.013), (86, 0.116), (91, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87545794 <a title="77-lda-1" href="./nips-2009-Rank-Approximate_Nearest_Neighbor_Search%3A_Retaining_Meaning_and_Speed_in_High_Dimensions.html">198 nips-2009-Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions</a></p>
<p>Author: Parikshit Ram, Dongryeol Lee, Hua Ouyang, Alexander G. Gray</p><p>Abstract: The long-standing problem of efďŹ cient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 ďŹ ngerprinting to bioinformatics to movie recommendations. As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1+đ?&oelig;&ndash;) distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the ďŹ rst time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN. Experiments on high-dimensional datasets show that our algorithm often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior. 1</p><p>2 0.86296016 <a title="77-lda-2" href="./nips-2009-Entropic_Graph_Regularization_in_Non-Parametric_Semi-Supervised_Classification.html">82 nips-2009-Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification</a></p>
<p>Author: Amarnag Subramanya, Jeff A. Bilmes</p><p>Abstract: We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to outperform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph. 1</p><p>same-paper 3 0.79861867 <a title="77-lda-3" href="./nips-2009-Efficient_Match_Kernel_between_Sets_of_Features_for_Visual_Recognition.html">77 nips-2009-Efficient Match Kernel between Sets of Features for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Cristian Sminchisescu</p><p>Abstract: In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classiﬁers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels for large datasets due to their signiﬁcant computational cost. To address this problem, we propose efﬁcient match kernels (EMK) that map local features to a low dimensional feature space and average the resulting vectors to form a setlevel feature. The local feature maps are learned so their inner products preserve, to the best possible, the values of the speciﬁed kernel function. Classiﬁers based on EMK are linear both in the number of images and in the number of local features. We demonstrate that EMK are extremely efﬁcient and achieve the current state of the art in three difﬁcult computer vision datasets: Scene-15, Caltech-101 and Caltech-256. 1</p><p>4 0.72476274 <a title="77-lda-4" href="./nips-2009-Sensitivity_analysis_in_HMMs_with_application_to_likelihood_maximization.html">215 nips-2009-Sensitivity analysis in HMMs with application to likelihood maximization</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Inﬁnitesimal Perturbation Analysis (IPA) on the ﬁltering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the ﬁltering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.</p><p>5 0.66017228 <a title="77-lda-5" href="./nips-2009-Group_Sparse_Coding.html">104 nips-2009-Group Sparse Coding</a></p>
<p>Author: Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</p><p>Abstract: Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classiﬁcation dataset show that when compact image or dictionary representations are needed for computational efﬁciency, the proposed approach yields better mean average precision in classiﬁcation. 1</p><p>6 0.65603083 <a title="77-lda-6" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>7 0.65459454 <a title="77-lda-7" href="./nips-2009-Learning_to_Hash_with_Binary_Reconstructive_Embeddings.html">135 nips-2009-Learning to Hash with Binary Reconstructive Embeddings</a></p>
<p>8 0.65278351 <a title="77-lda-8" href="./nips-2009-Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations.html">167 nips-2009-Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></p>
<p>9 0.64965773 <a title="77-lda-9" href="./nips-2009-Learning_a_Small_Mixture_of_Trees.html">129 nips-2009-Learning a Small Mixture of Trees</a></p>
<p>10 0.64797974 <a title="77-lda-10" href="./nips-2009-Discriminative_Network_Models_of_Schizophrenia.html">70 nips-2009-Discriminative Network Models of Schizophrenia</a></p>
<p>11 0.64775652 <a title="77-lda-11" href="./nips-2009-Kernel_Methods_for_Deep_Learning.html">119 nips-2009-Kernel Methods for Deep Learning</a></p>
<p>12 0.64763129 <a title="77-lda-12" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>13 0.64686263 <a title="77-lda-13" href="./nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">169 nips-2009-Nonlinear Learning using Local Coordinate Coding</a></p>
<p>14 0.64614248 <a title="77-lda-14" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>15 0.64582193 <a title="77-lda-15" href="./nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</a></p>
<p>16 0.64546943 <a title="77-lda-16" href="./nips-2009-Learning_Non-Linear_Combinations_of_Kernels.html">128 nips-2009-Learning Non-Linear Combinations of Kernels</a></p>
<p>17 0.64527202 <a title="77-lda-17" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<p>18 0.64444917 <a title="77-lda-18" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>19 0.64336878 <a title="77-lda-19" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>20 0.64261866 <a title="77-lda-20" href="./nips-2009-Estimating_image_bases_for_visual_image_reconstruction_from_human_brain_activity.html">83 nips-2009-Estimating image bases for visual image reconstruction from human brain activity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
