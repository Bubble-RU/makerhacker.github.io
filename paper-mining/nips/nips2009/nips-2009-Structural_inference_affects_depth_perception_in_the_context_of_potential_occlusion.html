<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-235" href="#">nips2009-235</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</h1>
<br/><p>Source: <a title="nips-2009-235-pdf" href="http://papers.nips.cc/paper/3663-structural-inference-affects-depth-perception-in-the-context-of-potential-occlusion.pdf">pdf</a></p><p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>Reference: <a title="nips-2009-235-reference" href="../nips2009_reference/nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dep', 0.433), ('yc', 0.411), ('ys', 0.347), ('xc', 0.337), ('cue', 0.265), ('occlud', 0.261), ('unpair', 0.222), ('disp', 0.189), ('surround', 0.171), ('xs', 0.128), ('subject', 0.117), ('ordin', 0.116), ('arcmin', 0.104), ('perceiv', 0.1), ('dxs', 0.078), ('nxc', 0.074), ('depth', 0.074), ('bar', 0.062), ('stimul', 0.061), ('target', 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="235-tfidf-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.1884774 <a title="235-tfidf-2" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>Author: Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</p><p>Abstract: We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, speciﬁed in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research ﬁeld of multiple-causes component extraction methods. 1</p><p>3 0.13131854 <a title="235-tfidf-3" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>Author: Lei Wu, Rong Jin, Steven C. Hoi, Jianke Zhu, Nenghai Yu</p><p>Abstract: Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a ﬁxed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a nonparametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of ﬁxed metric by implicitly deriving a local distance from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We also present an efﬁcient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efﬁcient for high dimensional data. 1</p><p>4 0.12262002 <a title="235-tfidf-4" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>5 0.12010816 <a title="235-tfidf-5" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>Author: Ruben Coen-cagli, Peter Dayan, Odelia Schwartz</p><p>Abstract: A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor–like receptive ﬁelds and divisive gain modulation from local surrounds; these have led to inﬂuential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non–linear correlations between the responses of spatially distributed Gaborlike receptive ﬁelds, which, when trained on an ensemble of natural scenes, uniﬁes a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Li’s (2002) hypothesis that V1 builds a saliency map, and ﬁts data on the tilt illusion. 1</p><p>6 0.10695457 <a title="235-tfidf-6" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>7 0.097235613 <a title="235-tfidf-7" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>8 0.084475525 <a title="235-tfidf-8" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>9 0.081624337 <a title="235-tfidf-9" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>10 0.080689654 <a title="235-tfidf-10" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>11 0.076081917 <a title="235-tfidf-11" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>12 0.065153144 <a title="235-tfidf-12" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>13 0.063996859 <a title="235-tfidf-13" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>14 0.062853537 <a title="235-tfidf-14" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>15 0.060815126 <a title="235-tfidf-15" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>16 0.054010648 <a title="235-tfidf-16" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>17 0.049266782 <a title="235-tfidf-17" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>18 0.048247222 <a title="235-tfidf-18" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>19 0.047952909 <a title="235-tfidf-19" href="./nips-2009-Sparsistent_Learning_of_Varying-coefficient_Models_with_Structural_Changes.html">225 nips-2009-Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></p>
<p>20 0.043273438 <a title="235-tfidf-20" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.114), (1, -0.067), (2, -0.067), (3, 0.041), (4, -0.031), (5, 0.029), (6, 0.033), (7, 0.035), (8, -0.039), (9, -0.063), (10, 0.009), (11, -0.05), (12, -0.111), (13, -0.067), (14, 0.021), (15, -0.022), (16, -0.028), (17, 0.019), (18, 0.031), (19, 0.014), (20, -0.095), (21, -0.138), (22, -0.073), (23, 0.029), (24, 0.175), (25, 0.029), (26, -0.206), (27, 0.05), (28, 0.105), (29, -0.068), (30, 0.081), (31, 0.01), (32, -0.08), (33, 0.164), (34, -0.089), (35, -0.005), (36, -0.037), (37, -0.206), (38, 0.19), (39, 0.021), (40, 0.068), (41, 0.023), (42, 0.02), (43, -0.118), (44, -0.015), (45, -0.143), (46, -0.044), (47, -0.055), (48, 0.145), (49, -0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94865716 <a title="235-lsi-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.48475817 <a title="235-lsi-2" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>Author: Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</p><p>Abstract: We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, speciﬁed in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research ﬁeld of multiple-causes component extraction methods. 1</p><p>3 0.43323281 <a title="235-lsi-3" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>Author: Samuel Gershman, Ed Vul, Joshua B. Tenenbaum</p><p>Abstract: While many perceptual and cognitive phenomena are well described in terms of Bayesian inference, the necessary computations are intractable at the scale of realworld tasks, and it remains unclear how the human mind approximates Bayesian computations algorithmically. We explore the proposal that for some tasks, humans use a form of Markov Chain Monte Carlo to approximate the posterior distribution over hidden variables. As a case study, we show how several phenomena of perceptual multistability can be explained as MCMC inference in simple graphical models for low-level vision. 1</p><p>4 0.41245353 <a title="235-lsi-4" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>Author: Ruben Coen-cagli, Peter Dayan, Odelia Schwartz</p><p>Abstract: A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor–like receptive ﬁelds and divisive gain modulation from local surrounds; these have led to inﬂuential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non–linear correlations between the responses of spatially distributed Gaborlike receptive ﬁelds, which, when trained on an ensemble of natural scenes, uniﬁes a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Li’s (2002) hypothesis that V1 builds a saliency map, and ﬁts data on the tilt illusion. 1</p><p>5 0.34912193 <a title="235-lsi-5" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>Author: Andrew Frank, Padhraic Smyth, Alexander T. Ihler</p><p>Abstract: Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions deﬁned on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the beneﬁts of these more advanced inference techniques to the continuous domain. 1</p><p>6 0.33994207 <a title="235-lsi-6" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>7 0.32915479 <a title="235-lsi-7" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>8 0.32058352 <a title="235-lsi-8" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>9 0.31734404 <a title="235-lsi-9" href="./nips-2009-Sequential_effects_reflect_parallel_learning_of_multiple_environmental_regularities.html">216 nips-2009-Sequential effects reflect parallel learning of multiple environmental regularities</a></p>
<p>10 0.30494282 <a title="235-lsi-10" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>11 0.30271092 <a title="235-lsi-11" href="./nips-2009-Linearly_constrained_Bayesian_matrix_factorization_for_blind_source_separation.html">140 nips-2009-Linearly constrained Bayesian matrix factorization for blind source separation</a></p>
<p>12 0.29591644 <a title="235-lsi-12" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>13 0.29019725 <a title="235-lsi-13" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>14 0.27467647 <a title="235-lsi-14" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>15 0.27217311 <a title="235-lsi-15" href="./nips-2009-Measuring_model_complexity_with_the_prior_predictive.html">152 nips-2009-Measuring model complexity with the prior predictive</a></p>
<p>16 0.27077249 <a title="235-lsi-16" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>17 0.26857731 <a title="235-lsi-17" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>18 0.25865883 <a title="235-lsi-18" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>19 0.25157946 <a title="235-lsi-19" href="./nips-2009-Bayesian_Source_Localization_with_the_Multivariate_Laplace_Prior.html">41 nips-2009-Bayesian Source Localization with the Multivariate Laplace Prior</a></p>
<p>20 0.21907637 <a title="235-lsi-20" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.171), (11, 0.013), (31, 0.072), (37, 0.039), (60, 0.046), (91, 0.49), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.6405316 <a title="235-lda-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.55658978 <a title="235-lda-2" href="./nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</a></p>
<p>Author: Honglak Lee, Peter Pham, Yan Largman, Andrew Y. Ng</p><p>Abstract: In recent years, deep learning approaches have gained signiﬁcant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classiﬁcation tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classiﬁcation tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks. 1</p><p>3 0.53607124 <a title="235-lda-3" href="./nips-2009-Nonlinear_directed_acyclic_structure_learning_with_weakly_additive_noise_models.html">170 nips-2009-Nonlinear directed acyclic structure learning with weakly additive noise models</a></p>
<p>Author: Arthur Gretton, Peter Spirtes, Robert E. Tillman</p><p>Abstract: The recently proposed additive noise model has advantages over previous directed structure learning approaches since it (i) does not assume linearity or Gaussianity and (ii) can discover a unique DAG rather than its Markov equivalence class. However, for certain distributions, e.g. linear Gaussians, the additive noise model is invertible and thus not useful for structure learning, and it was originally proposed for the two variable case with a multivariate extension which requires enumerating all possible DAGs. We introduce weakly additive noise models, which extends this framework to cases where the additive noise model is invertible and when additive noise is not present. We then provide an algorithm that learns an equivalence class for such models from data, by combining a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the Markov equivalence class. This results in a more computationally eﬃcient approach that is useful for arbitrary distributions even when additive noise models are invertible. 1</p><p>4 0.46583915 <a title="235-lda-4" href="./nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</a></p>
<p>Author: Percy Liang, Guillaume Bouchard, Francis R. Bach, Michael I. Jordan</p><p>Abstract: Many types of regularization schemes have been employed in statistical learning, each motivated by some assumption about the problem domain. In this paper, we present a uniﬁed asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer. In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework. We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning. 1</p><p>5 0.43027985 <a title="235-lda-5" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>Author: Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</p><p>Abstract: In this paper, we investigate how, given an image, similar images sharing the same global description can help with unsupervised scene segmentation. In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes. This allows for a better explanation of the input scenes. We perform MRF-based segmentation that optimizes over matches, while respecting boundary information. The recovered segments are then used to re-query a large database of images to retrieve better matches for the target regions. We show improved performance in detecting the principal occluding and contact boundaries for the scene over previous methods on data gathered from the LabelMe database.</p><p>6 0.37200993 <a title="235-lda-6" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>7 0.37068751 <a title="235-lda-7" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>8 0.36912465 <a title="235-lda-8" href="./nips-2009-Graph-based_Consensus_Maximization_among_Multiple_Supervised_and_Unsupervised_Models.html">102 nips-2009-Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></p>
<p>9 0.36843222 <a title="235-lda-9" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>10 0.36768931 <a title="235-lda-10" href="./nips-2009-Learning_transport_operators_for_image_manifolds.html">137 nips-2009-Learning transport operators for image manifolds</a></p>
<p>11 0.36740315 <a title="235-lda-11" href="./nips-2009-Sharing_Features_among_Dynamical_Systems_with_Beta_Processes.html">217 nips-2009-Sharing Features among Dynamical Systems with Beta Processes</a></p>
<p>12 0.36633021 <a title="235-lda-12" href="./nips-2009-Gaussian_process_regression_with_Student-t_likelihood.html">100 nips-2009-Gaussian process regression with Student-t likelihood</a></p>
<p>13 0.36595386 <a title="235-lda-13" href="./nips-2009-Learning_in_Markov_Random_Fields_using_Tempered_Transitions.html">132 nips-2009-Learning in Markov Random Fields using Tempered Transitions</a></p>
<p>14 0.36574861 <a title="235-lda-14" href="./nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</a></p>
<p>15 0.36555228 <a title="235-lda-15" href="./nips-2009-Constructing_Topological_Maps_using_Markov_Random_Fields_and_Loop-Closure_Detection.html">58 nips-2009-Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></p>
<p>16 0.36545807 <a title="235-lda-16" href="./nips-2009-Lattice_Regression.html">124 nips-2009-Lattice Regression</a></p>
<p>17 0.36542076 <a title="235-lda-17" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>18 0.36536136 <a title="235-lda-18" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>19 0.36532119 <a title="235-lda-19" href="./nips-2009-Unsupervised_Detection_of_Regions_of_Interest_Using_Iterative_Link_Analysis.html">251 nips-2009-Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></p>
<p>20 0.36531103 <a title="235-lda-20" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
