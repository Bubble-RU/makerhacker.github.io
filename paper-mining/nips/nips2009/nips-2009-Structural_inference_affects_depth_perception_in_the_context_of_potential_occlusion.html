<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-235" href="#">nips2009-235</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</h1>
<br/><p>Source: <a title="nips-2009-235-pdf" href="http://papers.nips.cc/paper/3663-structural-inference-affects-depth-perception-in-the-context-of-potential-occlusion.pdf">pdf</a></p><p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>Reference: <a title="nips-2009-235-reference" href="../nips2009_reference/nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Structural inference affects depth perception in the context of potential occlusion  Ian H. [sent-1, score-0.708]
</p><p>2 The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. [sent-6, score-0.262]
</p><p>3 We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. [sent-7, score-0.661]
</p><p>4 To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. [sent-8, score-0.523]
</p><p>5 Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. [sent-10, score-0.943]
</p><p>6 In the past few decades, a number of studies have demonstrated that humans combine cues during visual perception to reduce uncertainty and often do so in near-optimal, probabilistic ways [1, 2, 3, 4]. [sent-14, score-0.337]
</p><p>7 Recently [5] have suggested that subjects may combine a metric cue (binocular disparity) with ordinal cues (convexity or familiarity of faces) during depth perception. [sent-16, score-1.07]
</p><p>8 In these studies ordinal cues were modeled as simple biases. [sent-17, score-0.272]
</p><p>9 We argue that the effect of such ordinal cues stems from a structural inference process where an observer estimates the structure of the visual scene along with depth cues. [sent-18, score-0.871]
</p><p>10 The importance of structural inference and occlusion constraints, particularly of hard constraints, has been noted previously [6, 7, 8]. [sent-19, score-0.279]
</p><p>11 For instance, it was found that points presented to one eye but not the other have a perceived depth that is constrained by the position of objects presented to both eyes. [sent-20, score-0.594]
</p><p>12 Although these unpaired image points do not contain depth cues in the usual sense, subjects were able to estimate their depth. [sent-21, score-0.951]
</p><p>13 Here we formalize the constraints presented by occlusion using a probabilistic framework. [sent-23, score-0.263]
</p><p>14 Then we present results from a new stereo-vision experiment in which subjects were asked to match the depth of an occluding 1  or occluded circle. [sent-25, score-0.725]
</p><p>15 The model accurately predicts human behavior in this task and describes the changes that occur when we increase depth uncertainty. [sent-26, score-0.461]
</p><p>16 Our constraint-based approach may thus be useful in understanding how subjects make sense of cluttered scenes and the impact of structural inference on perception. [sent-28, score-0.259]
</p><p>17 1  Theory An Ordinal Cue Combination Model  We assume that observers receive noisy information about the depth of objects in the world. [sent-30, score-0.486]
</p><p>18 For concreteness, we assume that there is a central object c and a surrounding object s. [sent-31, score-0.316]
</p><p>19 We assume that each of these objects has a true, hidden depth (xc and xs ) and observers receive noisy observations of these depths (yc and ys ). [sent-33, score-1.081]
</p><p>20 When there is no occlusion (structure S1 ) the depth observations of the two objects are independent. [sent-35, score-0.674]
</p><p>21 That is, we assume that the depth of the surrounding object in the scene s has no inﬂuence on our estimate of the depth of c. [sent-36, score-1.094]
</p><p>22 The distribution of observations is assumed to be Gaussian and is physically determined by disparity, shading, texture, or other depth cues and their associated uncertainties. [sent-37, score-0.558]
</p><p>23 In this case the joint distribution of the observations given the hidden positions is p(yc , ys |xc , xs , S1 ) = p(yc |xc , S1 )p(ys |xs , S1 ) = Nyc (xc , σc )Nys (xs , σs ). [sent-38, score-0.523]
</p><p>24 (1)  When occlusion does occur, however, the position of the central object c is bounded by the depth of the surrounding, occluded object (structure S2 ) p(yc , ys |xc , xs , S2 ) ∝  Nyc (xc , σc )Nys (xs , σs ) if xc > xs , 0 if xc ≤ xs . [sent-39, score-2.192]
</p><p>25 (2)  An ideal observer can then make use of this ordinal information in estimating the depth of the occluding object. [sent-40, score-0.652]
</p><p>26 The (marginal) posterior distribution over the hidden depth of the central object xc can be found by marginalizing over the depth of the surrounding object xs and possible structures (S1 and S2 ). [sent-41, score-1.661]
</p><p>27 p(xc | yc , ys ) = p(xc | yc , ys , S1 )p(S1 ) + p(xc | yc , ys , S2 )p(S2 )  Observation  s c S2  c s c  s  Constraint  B  S1  p(S1) = 0  ys  p(S1) = 0. [sent-42, score-2.624]
</p><p>28 25  ys  p(S1) = 1  ys  p(xc| yc, ys,S1) p(xc| yc, ys)  Marginal Posterior  A  (3)  yc  yc  yc  xc  Figure 1: An occlusion model with soft-constraints. [sent-43, score-2.43]
</p><p>29 (A) Two possible structures leading to the same observation: one without occlusion S1 and one with occlusion S2 . [sent-44, score-0.408]
</p><p>30 (B) Examples of biases in the posterior estimate of xc for complete (left), moderate (center), and no relevance (right). [sent-45, score-0.366]
</p><p>31 In the cases shown, the observed depth of the central stimulus yc is the same as the observed depth of the surrounding stimulus ys . [sent-46, score-1.844]
</p><p>32 Note that when yc ys the constraint will not bias estimates of xc . [sent-47, score-1.129]
</p><p>33 Combining these two terms gives the marginal posterior p(xc | yc , ys ) =  (2π)/σs and Z is a  1 [(1 − p(S1 ))(erf (ρs (xc − ys ))/2 + 1/2) + p(S1 )] Nxc (yc , σc ), Z  (6)  which describes the best estimate of the depth of the central object. [sent-50, score-1.568]
</p><p>34 Similar to models of causal inference [11, 12, 9, 10], the surrounding stimulus may be irrelevant, in which case we should simply rely on the observation of the target. [sent-53, score-0.284]
</p><p>35 Generally, we can simply split structures into those with occlusion O and those without occlusion ¬O. [sent-56, score-0.408]
</p><p>36 Above, S1 corresponds to the set of possible structures without occlusion ¬O, and S2 corresponds to the set of possible structures with occlusion O. [sent-57, score-0.434]
</p><p>37 Similar to traditional cue combination models, where there is an analytic form for the expected value of the target (linear combination weighted by the precision of each cue), we can write down analytic expressions for E[xc ] for at least one case. [sent-59, score-0.318]
</p><p>38 For p(S1 ) = 0, σs → 0 the mean of the marginal posterior is the expected value of a truncated Gaussian E(xc |ys < xc ) = yc + σc λ(  ys − yc ) σc  (7)  φ(·) Where λ(·) = [1−Φ(·)] , φ(·) is the PDF for the standard normal distribution and Φ(·) is the CDF. [sent-60, score-1.502]
</p><p>39 For yc = ys , for instance,  E(xc |ys < xc ) = yc + 0. [sent-61, score-1.483]
</p><p>40 8σc  (8)  It is important to note that, similar to classical cue combination models, estimation of the target is improved by combining depth information with the occlusion constraint. [sent-62, score-0.893]
</p><p>41 The variance of p(xc |yc , ys ) is smaller than that of p(xc | yc , ys , S1 ). [sent-63, score-1.112]
</p><p>42 To test how subjects perceive occluded objects, a small vertical bar was presented to one eye, giving the impression that the large rectangle was occluding the bar and leading to unpaired image points (Fig 2A). [sent-68, score-0.813]
</p><p>43 Subjects were then asked to match the depth of this vertical bar by changing the disparity of another image in which the bar was presented in stereo. [sent-69, score-0.909]
</p><p>44 Despite the absence of direct depth cues, subjects assigned a depth to the vertical bar. [sent-70, score-1.074]
</p><p>45 Moreover, for a range of horizontal distances, the assigned depth was consistent with the constraint provided by the stereo-rectangle (Fig 2B). [sent-71, score-0.5]
</p><p>46 These results systematically characterize the effect of structural estimation on depth estimates. [sent-72, score-0.476]
</p><p>47 Without ordinal information, the horizontal distance between the rectangle and the vertical bar should have no effect on the perceived depth of the bar. [sent-73, score-0.877]
</p><p>48 In our model yc and ys are simply observations on the depth of two objects: in this case, the unpaired vertical bar and the large rectangle. [sent-74, score-1.552]
</p><p>49 Since there isn’t direct disparity for the vertical bar, we assume that horizontal distance from the large rectangle serves as the depth cue. [sent-75, score-0.761]
</p><p>50 For the valid stimuli where occlusion can account for the vertical bar being seen in only one eye, σs = 4. [sent-81, score-0.393]
</p><p>51 A) Occlusion puts hard constraints on the possible depth of unpaired image points (top). [sent-87, score-0.683]
</p><p>52 B) When subjects were asked to judge the depth of unpaired image points they followed these hard constraints (dotted lines) for a range of distances between the large rectangle and vertical bar (top). [sent-89, score-1.087]
</p><p>53 The ordinal cue combination model can describe this behavior as well as deviations from the constraints for large distances (bottom). [sent-91, score-0.393]
</p><p>54 4  3  Experimental Methods  To test this model in a more general setting where depth is driven by both paired and unpaired image points we constructed a simple depth matching experiment. [sent-92, score-1.157]
</p><p>55 The experiment consisted of two tasks: a two-alternative forced choice task (2AFC) to measure subjects’ depth acuity and a stereo-matching task to measure their perception of depth when a surrounding object was present. [sent-94, score-1.218]
</p><p>56 In the 2AFC task, subjects were presented with two target objects with slightly different horizontal disparities and asked to indicate using the keyboard which object was closer. [sent-97, score-0.537]
</p><p>57 The reference object had a horizontal disparity of 0. [sent-98, score-0.295]
</p><p>58 After the ﬁrst 10 trials the next sample was chosen to maximize the conditional mutual information between the responses and the parameter for the just-noticeable depth difference (JND) given the sample position. [sent-103, score-0.44]
</p><p>59 In the stereo-matching task subjects were presented with two target objects and a larger surrounding circle (25. [sent-105, score-0.521]
</p><p>60 Subjects were asked to match the depth of the unpaired target with that of the paired target using the keyboard (100 trials). [sent-107, score-0.914]
</p><p>61 The depth of the paired target was held ﬁxed across trials at 0. [sent-108, score-0.579]
</p><p>62 57 degrees horizontal disparity while the position of the surrounding circle was varied between 0. [sent-109, score-0.473]
</p><p>63 The depth of the unpaired target was selected randomly at the beginning of each trial to minimize any effects of the starting position. [sent-112, score-0.748]
</p><p>64 All objects were presented in gray-scale and the target was presented offcenter from the surrounding object to avoid confounding shape cues. [sent-113, score-0.374]
</p><p>65 The side on which the paired target and surrounding object appeared (left or right side of the screen) was also randomly chosen from trial to trial, and all objects were within the fusional limits for this task. [sent-114, score-0.431]
</p><p>66 When asked, subjects reported that diplopia occurred only when they drove the unpaired target too far in one direction or the other. [sent-115, score-0.45]
</p><p>67 Each trial consists of a matching task in which subjects control the depth of an unpaired circle (A, left). [sent-121, score-0.895]
</p><p>68 Subjects attempt to match the depth of this unpaired circle to the depth of a target circle which is surrounded by a larger object (A, right). [sent-122, score-1.262]
</p><p>69 To measure depth acuity, subjects also complete a two-alternative forced choice task (2AFC) using the same stimulus without the surrounding object. [sent-125, score-0.815]
</p><p>70 All subjects had normal or corrected to normal vision and normal stereo vision (as assessed by a depth acuity < 5 arcmin in the low uncertainty 2AFC task). [sent-128, score-0.906]
</p><p>71 4  Results  All subjects showed increased just-noticeable depth differences between the low and high uncertainty conditions. [sent-131, score-0.646]
</p><p>72 In the matching task, subjects were, on average, biased by the presence of the surrounding object. [sent-134, score-0.33]
</p><p>73 As the disparity of the surrounding object was increased and disparity cues suggested that s was closer than c, this bias increased. [sent-135, score-0.714]
</p><p>74 050 arcmin for the high uncertainty  Difference in perceived depth (arcmin)  B  Just noticeable depth difference (arcmin)  A  8  *  3. [sent-147, score-1.071]
</p><p>75 5 0  14  w ty gh ty Lo tain Hi rtain r ce ce Un Un  yc  12  yc  Across Subject Average N=7  8  Subject IV  10  6  6  4  4  2  2  0  0  −2  −2  −4  0. [sent-151, score-0.846]
</p><p>76 (A) Just noticeable depth differences for the two uncertainty conditions averaged across subjects. [sent-160, score-0.493]
</p><p>77 (B) and (C) show the difference between the perceived depth of the unpaired target and the paired target (the bias) as a function of the depth of the surrounding circle. [sent-161, score-1.475]
</p><p>78 Dots and error-bars denote subject responses, solid lines denote model ﬁts, and dotted lines denote the depth of the paired target, which was ﬁxed. [sent-163, score-0.553]
</p><p>79 However, we predict that for larger discrepancies this relevance term would come into play as subjects begin to ignore the surrounding object (as in Fig 2). [sent-168, score-0.43]
</p><p>80 Note that if the presence of a surrounding object had no effect subjects would be unbiased across depths of the occluded object. [sent-169, score-0.504]
</p><p>81 Two subjects (out of 7) did not show bias; however, both subjects had normal stereo vision and this behavior did not appear to be correlated with low or high depth acuity. [sent-170, score-0.825]
</p><p>82 Since subjects were allowed to free-view the stimulus, it is possible that some subjects were able to ignore the surrounding object completely. [sent-171, score-0.562]
</p><p>83 The rest of the subjects demonstrated bias (see Fig 4B for an example), but more data may be need to conclusively show differences between the two uncertainty conditions and causal inference effects. [sent-173, score-0.352]
</p><p>84 5  Discussion  The results presented above illustrate the importance of structural inference in depth perception. [sent-174, score-0.532]
</p><p>85 We have shown that potential occlusion can bias perceived depth, and a probabilistic model of the constraints accurately accounts for subjects’ perception during occlusion tasks with unpaired image points [7] as well as a novel task designed to probe the effects of structural inference. [sent-175, score-0.956]
</p><p>86 Non-linear cue combination can be explained by causal inference models where x1 and x2 are probabilistically equal. [sent-186, score-0.317]
</p><p>87 (C) In the model presented here, ordinal information introduces an asymmetry into cue combination. [sent-187, score-0.379]
</p><p>88 7  A number of studies have proposed probabilistic accounts of depth perception [1, 4, 12, 14], and a variety of cues, such as disparity, shading, and texture, can all be combined to estimate depth [4, 12]. [sent-190, score-0.954]
</p><p>89 However, accounting for structure in the visual scene and use of occlusion constraints is typically qualitative or limited to hard constraints where certain depth arrangements are strictly ruled out [6, 14]. [sent-191, score-0.715]
</p><p>90 The model presented here accounts for a range of depth perception effects including perception of both paired and unpaired image points. [sent-192, score-0.929]
</p><p>91 Importantly, this model of perception explains the effects of ordinal cues in a cohesive structural inference framework. [sent-193, score-0.444]
</p><p>92 More generally, ordinal information introduces asymmetry into cue combination. [sent-194, score-0.356]
</p><p>93 Classically, cue combination models assume a generative model in which two observations arise from the same hidden source. [sent-195, score-0.261]
</p><p>94 More recently, causal inference or cue conﬂict models have been developed that allow for the possibility of probabilistic equality [9, 11, 12]. [sent-197, score-0.306]
</p><p>95 The ordinal cue combination model thus increases the class of behaviors that can be modeled by cue combination and causal inference and should have applications for other modalities where ordinal and structural information is important. [sent-202, score-0.894]
</p><p>96 Measurement and modeling of depth cue combination: In defense of weak fusion. [sent-229, score-0.602]
</p><p>97 Ordinal conﬁgural cues combine with metric disparity in depth perception. [sent-237, score-0.739]
</p><p>98 da vinci stereopsis: Depth and subjective occluding contours from unpaired image points. [sent-248, score-0.285]
</p><p>99 Neither occlusion constraint nor binocular disparity accounts for the perceived depth in the sieve effect. [sent-255, score-0.913]
</p><p>100 Robust cue integration: A bayesian model and evidence from cue-conﬂict studies with stereoscopic and ﬁgure cues to slant. [sent-284, score-0.301]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('depth', 0.421), ('yc', 0.4), ('ys', 0.356), ('xc', 0.327), ('unpaired', 0.216), ('occlusion', 0.191), ('cue', 0.181), ('disparity', 0.173), ('subjects', 0.171), ('surrounding', 0.159), ('ordinal', 0.152), ('fig', 0.135), ('xs', 0.124), ('cues', 0.12), ('arcmin', 0.101), ('bar', 0.081), ('paired', 0.076), ('dxs', 0.076), ('nxc', 0.072), ('depths', 0.072), ('causal', 0.066), ('perception', 0.063), ('target', 0.063), ('object', 0.061), ('horizontal', 0.061), ('vertical', 0.061), ('perceived', 0.056), ('structural', 0.055), ('uncertainty', 0.054), ('invalid', 0.054), ('asked', 0.046), ('occluding', 0.046), ('objects', 0.045), ('rectangle', 0.045), ('occluded', 0.041), ('stimuli', 0.04), ('degrees', 0.04), ('circle', 0.04), ('relevance', 0.039), ('disparities', 0.038), ('nakayama', 0.038), ('combination', 0.037), ('subject', 0.035), ('central', 0.035), ('acuity', 0.035), ('ernst', 0.035), ('inference', 0.033), ('observer', 0.033), ('scene', 0.032), ('binocular', 0.031), ('sources', 0.029), ('jnd', 0.029), ('jnds', 0.029), ('keyboard', 0.029), ('knill', 0.029), ('northwestern', 0.029), ('nxs', 0.029), ('nys', 0.029), ('pedestal', 0.029), ('bias', 0.028), ('dots', 0.028), ('arc', 0.027), ('trial', 0.027), ('structures', 0.026), ('probabilistic', 0.026), ('eye', 0.026), ('hidden', 0.026), ('stimulus', 0.026), ('combine', 0.025), ('impression', 0.025), ('nyc', 0.025), ('visual', 0.025), ('humans', 0.024), ('vision', 0.024), ('accounts', 0.023), ('ty', 0.023), ('asymmetry', 0.023), ('shading', 0.023), ('erf', 0.023), ('rding', 0.023), ('image', 0.023), ('presented', 0.023), ('constraints', 0.023), ('un', 0.022), ('dotted', 0.021), ('effects', 0.021), ('observers', 0.02), ('task', 0.02), ('accurately', 0.02), ('valid', 0.02), ('positioned', 0.02), ('normal', 0.019), ('stereo', 0.019), ('trials', 0.019), ('forced', 0.018), ('nervous', 0.018), ('noticeable', 0.018), ('constraint', 0.018), ('ict', 0.017), ('observations', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="235-tfidf-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.13617913 <a title="235-tfidf-2" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>Author: Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</p><p>Abstract: We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, speciﬁed in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research ﬁeld of multiple-causes component extraction methods. 1</p><p>3 0.12430906 <a title="235-tfidf-3" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>Author: Lei Wu, Rong Jin, Steven C. Hoi, Jianke Zhu, Nenghai Yu</p><p>Abstract: Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a ﬁxed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a nonparametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of ﬁxed metric by implicitly deriving a local distance from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We also present an efﬁcient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efﬁcient for high dimensional data. 1</p><p>4 0.11520294 <a title="235-tfidf-4" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>Author: Lei Shi, Thomas L. Griffiths</p><p>Abstract: The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and the oblique effect. 1</p><p>5 0.08009275 <a title="235-tfidf-5" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>Author: Andrew Frank, Padhraic Smyth, Alexander T. Ihler</p><p>Abstract: Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions deﬁned on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the beneﬁts of these more advanced inference techniques to the continuous domain. 1</p><p>6 0.075522818 <a title="235-tfidf-6" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>7 0.070327029 <a title="235-tfidf-7" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>8 0.065550365 <a title="235-tfidf-8" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>9 0.064004935 <a title="235-tfidf-9" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>10 0.062781632 <a title="235-tfidf-10" href="./nips-2009-Dirichlet-Bernoulli_Alignment%3A_A_Generative_Model_for_Multi-Class_Multi-Label_Multi-Instance_Corpora.html">68 nips-2009-Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></p>
<p>11 0.0606337 <a title="235-tfidf-11" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>12 0.058068573 <a title="235-tfidf-12" href="./nips-2009-Measuring_Invariances_in_Deep_Networks.html">151 nips-2009-Measuring Invariances in Deep Networks</a></p>
<p>13 0.057899699 <a title="235-tfidf-13" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>14 0.056406517 <a title="235-tfidf-14" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>15 0.054716442 <a title="235-tfidf-15" href="./nips-2009-Help_or_Hinder%3A_Bayesian_Models_of_Social_Goal_Inference.html">107 nips-2009-Help or Hinder: Bayesian Models of Social Goal Inference</a></p>
<p>16 0.052196637 <a title="235-tfidf-16" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>17 0.051757213 <a title="235-tfidf-17" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>18 0.051122513 <a title="235-tfidf-18" href="./nips-2009-Subject_independent_EEG-based_BCI_decoding.html">237 nips-2009-Subject independent EEG-based BCI decoding</a></p>
<p>19 0.049616508 <a title="235-tfidf-19" href="./nips-2009-Structured_output_regression_for_detection_with_partial_truncation.html">236 nips-2009-Structured output regression for detection with partial truncation</a></p>
<p>20 0.046687115 <a title="235-tfidf-20" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, -0.101), (2, -0.001), (3, -0.013), (4, 0.007), (5, 0.057), (6, 0.013), (7, 0.038), (8, 0.022), (9, -0.069), (10, 0.029), (11, -0.054), (12, 0.066), (13, -0.035), (14, 0.097), (15, -0.019), (16, -0.01), (17, -0.019), (18, -0.021), (19, -0.04), (20, -0.081), (21, -0.067), (22, 0.002), (23, -0.159), (24, 0.118), (25, -0.06), (26, -0.072), (27, 0.078), (28, -0.149), (29, -0.098), (30, -0.061), (31, -0.041), (32, -0.111), (33, -0.166), (34, 0.083), (35, -0.015), (36, -0.09), (37, -0.059), (38, 0.067), (39, -0.094), (40, 0.039), (41, 0.009), (42, 0.087), (43, 0.06), (44, -0.016), (45, -0.091), (46, -0.084), (47, -0.013), (48, -0.111), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96367842 <a title="235-lsi-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.52493119 <a title="235-lsi-2" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>Author: Ed Vul, George Alvarez, Joshua B. Tenenbaum, Michael J. Black</p><p>Abstract: Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We ﬁnd that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle ﬁlter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a ﬂexible cognitive resource, which can be formalized as either memory or attention. 1</p><p>3 0.51936769 <a title="235-lsi-3" href="./nips-2009-Occlusive_Components_Analysis.html">175 nips-2009-Occlusive Components Analysis</a></p>
<p>Author: Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</p><p>Abstract: We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, speciﬁed in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research ﬁeld of multiple-causes component extraction methods. 1</p><p>4 0.43861276 <a title="235-lsi-4" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<p>Author: Samuel Gershman, Ed Vul, Joshua B. Tenenbaum</p><p>Abstract: While many perceptual and cognitive phenomena are well described in terms of Bayesian inference, the necessary computations are intractable at the scale of realworld tasks, and it remains unclear how the human mind approximates Bayesian computations algorithmically. We explore the proposal that for some tasks, humans use a form of Markov Chain Monte Carlo to approximate the posterior distribution over hidden variables. As a case study, we show how several phenomena of perceptual multistability can be explained as MCMC inference in simple graphical models for low-level vision. 1</p><p>5 0.42121458 <a title="235-lsi-5" href="./nips-2009-Particle-based_Variational_Inference_for_Continuous_Systems.html">187 nips-2009-Particle-based Variational Inference for Continuous Systems</a></p>
<p>Author: Andrew Frank, Padhraic Smyth, Alexander T. Ihler</p><p>Abstract: Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions deﬁned on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference. However, extending these methods to continuous-valued systems has lagged behind. While several methods have been developed to use belief propagation on systems with continuous values, recent advances for discrete variables have not as yet been incorporated. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to inference in continuous systems. The resulting algorithms behave similarly to their purely discrete counterparts, extending the beneﬁts of these more advanced inference techniques to the continuous domain. 1</p><p>6 0.3900595 <a title="235-lsi-6" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>7 0.38142225 <a title="235-lsi-7" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>8 0.37451991 <a title="235-lsi-8" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>9 0.37192556 <a title="235-lsi-9" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>10 0.33930627 <a title="235-lsi-10" href="./nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></p>
<p>11 0.33118832 <a title="235-lsi-11" href="./nips-2009-A_Biologically_Plausible_Model_for_Rapid_Natural_Scene_Identification.html">6 nips-2009-A Biologically Plausible Model for Rapid Natural Scene Identification</a></p>
<p>12 0.32395181 <a title="235-lsi-12" href="./nips-2009-Region-based_Segmentation_and_Object_Detection.html">201 nips-2009-Region-based Segmentation and Object Detection</a></p>
<p>13 0.31268698 <a title="235-lsi-13" href="./nips-2009-Bootstrapping_from_Game_Tree_Search.html">48 nips-2009-Bootstrapping from Game Tree Search</a></p>
<p>14 0.30990914 <a title="235-lsi-14" href="./nips-2009-DUOL%3A_A_Double_Updating_Approach_for_Online_Learning.html">63 nips-2009-DUOL: A Double Updating Approach for Online Learning</a></p>
<p>15 0.28677768 <a title="235-lsi-15" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>16 0.28460494 <a title="235-lsi-16" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>17 0.28162095 <a title="235-lsi-17" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>18 0.28161159 <a title="235-lsi-18" href="./nips-2009-Efficient_Bregman_Range_Search.html">74 nips-2009-Efficient Bregman Range Search</a></p>
<p>19 0.27237621 <a title="235-lsi-19" href="./nips-2009-Extending_Phase_Mechanism_to_Differential_Motion_Opponency_for_Motion_Pop-out.html">88 nips-2009-Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></p>
<p>20 0.27191383 <a title="235-lsi-20" href="./nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.015), (25, 0.102), (35, 0.045), (36, 0.04), (37, 0.402), (39, 0.079), (58, 0.049), (61, 0.013), (71, 0.055), (81, 0.017), (86, 0.058), (91, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8504622 <a title="235-lda-1" href="./nips-2009-Structural_inference_affects_depth_perception_in_the_context_of_potential_occlusion.html">235 nips-2009-Structural inference affects depth perception in the context of potential occlusion</a></p>
<p>Author: Ian Stevenson, Konrad Koerding</p><p>Abstract: In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target - in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then ﬁt the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We ﬁnd that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception. 1</p><p>2 0.62435162 <a title="235-lda-2" href="./nips-2009-Statistical_Models_of_Linear_and_Nonlinear_Contextual_Interactions_in_Early_Visual_Processing.html">231 nips-2009-Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></p>
<p>Author: Ruben Coen-cagli, Peter Dayan, Odelia Schwartz</p><p>Abstract: A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor–like receptive ﬁelds and divisive gain modulation from local surrounds; these have led to inﬂuential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non–linear correlations between the responses of spatially distributed Gaborlike receptive ﬁelds, which, when trained on an ensemble of natural scenes, uniﬁes a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Li’s (2002) hypothesis that V1 builds a saliency map, and ﬁts data on the tilt illusion. 1</p><p>3 0.61695856 <a title="235-lda-3" href="./nips-2009-Streaming_k-means_approximation.html">234 nips-2009-Streaming k-means approximation</a></p>
<p>Author: Nir Ailon, Ragesh Jaiswal, Claire Monteleoni</p><p>Abstract: We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are: a derivation of an extremely simple pseudo-approximation batch algorithm for k-means (based on the recent k-means++), in which the algorithm is allowed to output more than k centers, and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (ﬁtting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data reveal the practical utility of our method. 1</p><p>4 0.56404656 <a title="235-lda-4" href="./nips-2009-A_Generalized_Natural_Actor-Critic_Algorithm.html">12 nips-2009-A Generalized Natural Actor-Critic Algorithm</a></p>
<p>Author: Tetsuro Morimura, Eiji Uchibe, Junichiro Yoshimoto, Kenji Doya</p><p>Abstract: Policy gradient Reinforcement Learning (RL) algorithms have received substantial attention, seeking stochastic policies that maximize the average (or discounted cumulative) reward. In addition, extensions based on the concept of the Natural Gradient (NG) show promising learning efﬁciency because these regard metrics for the task. Though there are two candidate metrics, Kakade’s Fisher Information Matrix (FIM) for the policy (action) distribution and Morimura’s FIM for the stateaction joint distribution, but all RL algorithms with NG have followed Kakade’s approach. In this paper, we describe a generalized Natural Gradient (gNG) that linearly interpolates the two FIMs and propose an efﬁcient implementation for the gNG learning based on a theory of the estimating function, the generalized Natural Actor-Critic (gNAC) algorithm. The gNAC algorithm involves a near optimal auxiliary function to reduce the variance of the gNG estimates. Interestingly, the gNAC can be regarded as a natural extension of the current state-of-the-art NAC algorithm [1], as long as the interpolating parameter is appropriately selected. Numerical experiments showed that the proposed gNAC algorithm can estimate gNG efﬁciently and outperformed the NAC algorithm.</p><p>5 0.47976774 <a title="235-lda-5" href="./nips-2009-Accelerated_Gradient_Methods_for_Stochastic_Optimization_and_Online_Learning.html">22 nips-2009-Accelerated Gradient Methods for Stochastic Optimization and Online Learning</a></p>
<p>Author: Chonghai Hu, Weike Pan, James T. Kwok</p><p>Abstract: Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., ℓ1 -regularizer). Gradient methods, though highly scalable and easy to implement, are known to converge slowly. In this paper, we develop a novel accelerated gradient method for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic composite optimization with convex or strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple algorithm but with the best regret bounds currently known for these problems. 1</p><p>6 0.3840234 <a title="235-lda-6" href="./nips-2009-Neural_Implementation_of_Hierarchical_Bayesian_Inference_by_Importance_Sampling.html">162 nips-2009-Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></p>
<p>7 0.3790043 <a title="235-lda-7" href="./nips-2009-Learning_models_of_object_structure.html">133 nips-2009-Learning models of object structure</a></p>
<p>8 0.37269318 <a title="235-lda-8" href="./nips-2009-Segmenting_Scenes_by_Matching_Image_Composites.html">211 nips-2009-Segmenting Scenes by Matching Image Composites</a></p>
<p>9 0.36544266 <a title="235-lda-9" href="./nips-2009-Individuation%2C_Identification_and_Object_Discovery.html">115 nips-2009-Individuation, Identification and Object Discovery</a></p>
<p>10 0.36459637 <a title="235-lda-10" href="./nips-2009-Modeling_the_spacing_effect_in_sequential_category_learning.html">154 nips-2009-Modeling the spacing effect in sequential category learning</a></p>
<p>11 0.36415568 <a title="235-lda-11" href="./nips-2009-Modelling_Relational_Data_using_Bayesian_Clustered_Tensor_Factorization.html">155 nips-2009-Modelling Relational Data using Bayesian Clustered Tensor Factorization</a></p>
<p>12 0.36350647 <a title="235-lda-12" href="./nips-2009-Beyond_Categories%3A_The_Visual_Memex_Model_for_Reasoning_About_Object_Relationships.html">44 nips-2009-Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></p>
<p>13 0.36077669 <a title="235-lda-13" href="./nips-2009-Explaining_human_multiple_object_tracking_as_resource-constrained_approximate_inference_in_a_dynamic_probabilistic_model.html">85 nips-2009-Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></p>
<p>14 0.36075559 <a title="235-lda-14" href="./nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</a></p>
<p>15 0.36022252 <a title="235-lda-15" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>16 0.35554266 <a title="235-lda-16" href="./nips-2009-Learning_from_Neighboring_Strokes%3A_Combining_Appearance_and_Context_for_Multi-Domain_Sketch_Recognition.html">131 nips-2009-Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></p>
<p>17 0.35551867 <a title="235-lda-17" href="./nips-2009-Adaptive_Design_Optimization_in_Experiments_with_People.html">25 nips-2009-Adaptive Design Optimization in Experiments with People</a></p>
<p>18 0.35357833 <a title="235-lda-18" href="./nips-2009-Human_Rademacher_Complexity.html">112 nips-2009-Human Rademacher Complexity</a></p>
<p>19 0.35327348 <a title="235-lda-19" href="./nips-2009-A_Bayesian_Model_for_Simultaneous_Image_Clustering%2C_Annotation_and_Object_Segmentation.html">5 nips-2009-A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></p>
<p>20 0.35278365 <a title="235-lda-20" href="./nips-2009-Perceptual_Multistability_as_Markov_Chain_Monte_Carlo_Inference.html">188 nips-2009-Perceptual Multistability as Markov Chain Monte Carlo Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
