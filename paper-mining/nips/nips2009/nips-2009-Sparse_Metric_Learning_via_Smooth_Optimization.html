<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 nips-2009-Sparse Metric Learning via Smooth Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-223" href="#">nips2009-223</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 nips-2009-Sparse Metric Learning via Smooth Optimization</h1>
<br/><p>Source: <a title="nips-2009-223-pdf" href="http://papers.nips.cc/paper/3847-sparse-metric-learning-via-smooth-optimization.pdf">pdf</a></p><p>Author: Yiming Ying, Kaizhu Huang, Colin Campbell</p><p>Abstract: In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efﬁcient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a nondifferentiable loss function. Finally, we run experiments to validate the effectiveness and efﬁciency of our sparse metric learning model on various datasets.</p><p>Reference: <a title="nips-2009-223-reference" href="../nips2009_reference/nips-2009-Sparse_Metric_Learning_via_Smooth_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. [sent-2, score-0.408]
</p><p>2 The sparse representation involves a mixed-norm regularization which is non-convex. [sent-3, score-0.191]
</p><p>3 We then show that it can be equivalently formulated as a convex saddle (min-max) problem. [sent-4, score-0.233]
</p><p>4 From this saddle representation, we develop an efﬁcient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a nondifferentiable loss function. [sent-5, score-0.686]
</p><p>5 Finally, we run experiments to validate the effectiveness and efﬁciency of our sparse metric learning model on various datasets. [sent-6, score-0.338]
</p><p>6 1 Introduction For many machine learning algorithms, the choice of a distance metric has a direct impact on their success. [sent-7, score-0.36]
</p><p>7 Hence, choosing a good distance metric remains a challenging problem. [sent-8, score-0.36]
</p><p>8 There has been much work attempting to exploit a distance metric in many learning settings, e. [sent-9, score-0.36]
</p><p>9 These methods have successfully indicated that a good distance metric can signiﬁcantly improve the performance of k-nearest neighbor classiﬁcation and k-means clustering, for example. [sent-12, score-0.399]
</p><p>10 A good choice of a distance metric generally preserves the distance structure of the data: the distance between examples exhibiting similarity should be relatively smaller, in the transformed space, than between examples exhibiting dissimilarity. [sent-13, score-0.686]
</p><p>11 Since it is very common that the presented data is contaminated by noise, especially for high-dimensional datasets, a good distance metric should also be minimally inﬂuenced by noise. [sent-16, score-0.36]
</p><p>12 In this case, a low-rank distance matrix would produce a better generalization performance than non-sparse counterparts and provide a much faster and efﬁcient distance calculation for test samples. [sent-17, score-0.27]
</p><p>13 Hence, a good distance metric should also pursue dimension reduction during the learning process. [sent-18, score-0.408]
</p><p>14 In this paper we present a novel approach to learn a low-rank (sparse) distance matrix. [sent-19, score-0.121]
</p><p>15 We ﬁrst propose in Section 2 a novel metric learning model for estimating the linear transformation (equivalently distance matrix) that combines and retains the advantages of existing methods [8, 9, 12, 20, 22, 23, 25]. [sent-20, score-0.393]
</p><p>16 Our method can simultaneously conduct dimension reduction and learn a low-rank distance matrix. [sent-21, score-0.169]
</p><p>17 The sparse representation is realized by a mixed-norm regularization used in various learning settings [1, 18, 21]. [sent-22, score-0.211]
</p><p>18 We then show that this non-convex mixed-norm regularization framework is equivalent to a convex saddle (min-max) problem. [sent-23, score-0.281]
</p><p>19 Based on this equivalent representation, we develop, in Section 3, Nesterov’s smooth optimization approach [16, 17] for sparse metric learning using smoothing approximation techniques, although the learning model is based on a non-differentiable loss function. [sent-24, score-0.593]
</p><p>20 In Section 4, we demonstrate the effectiveness and efﬁciency of our sparse metric learning model with experiments on various datasets. [sent-25, score-0.338]
</p><p>21 , xd ) ∈ Rd , i i class label yi (not necessary binary) and let xij = xi − xj . [sent-39, score-0.433]
</p><p>22 Denote by xi = P xi for any i ∈ Nn and ˆ ˆ by x = {xi : i ∈ Nn } the transformed data matrix. [sent-41, score-0.158]
</p><p>23 The linear transformation matrix P induces a ˆ distance matrix M = P P which deﬁnes a distance between xi and xj given by dM (xi , xj ) = (xi − xj ) M (xi − xj ). [sent-42, score-0.827]
</p><p>24 Our sparse metric learning model is based on two principal hypotheses: 1) a good choice of distance matrix M should preserve the distance structure, i. [sent-43, score-0.628]
</p><p>25 the distance between similar examples should be relatively smaller than between dissimilar examples; 2) a good distance matrix should also be able to effectively remove noise leading to dimension reduction. [sent-45, score-0.336]
</p><p>26 Equivalently, xj − xk ) 2 ≥ xi − xj 2 + 1, ∀(xi , xj ) ∈ S and (xj , xk ) ∈ D. [sent-47, score-0.504]
</p><p>27 ˆ ˆ ˆ ˆ (1) For the second hypothesis, we use a sparse regularization to give a sparse solution. [sent-48, score-0.255]
</p><p>28 This regularization ranges from element-sparsity for variable selection to a low-rank matrix for dimension reduction [1, 2, 3, 13, 21]. [sent-49, score-0.133]
</p><p>29 xi = P xi = 0 which means that P = 0 has the effect of eleminating -th variable. [sent-53, score-0.128]
</p><p>30 Hence, we introduce an extra orthonormal transformation U ∈ O d and let xi = P U xi . [sent-74, score-0.181]
</p><p>31 Denote a set of triplets T by ˆ T = {τ = (i, j, k) : i, j, k ∈ Nn , (xi , xj ) ∈ S and (xj , xk ) ∈ D}. [sent-75, score-0.193]
</p><p>32 (2) By introducing slack variables ξ in constraints (1), we propose the following sparse (low-rank) distance matrix learning formulation: 2 min min τ ξτ + γ||W ||(2,1) d U ∈O d W ∈S+  s. [sent-76, score-0.314]
</p><p>33 1 + xij U W U xij ≤ xkj U W U xkj + ξτ , d ξτ ≥ 0, ∀τ = (i, j, k) ∈ T , and W ∈ S+ . [sent-78, score-1.08]
</p><p>34 A similar mixed (2, 1)-norm regularization was used in [1, 18] for multi-task learning and multi-class classiﬁcation to learn the sparse representation shared across different tasks or classes. [sent-80, score-0.191]
</p><p>35 1  Equivalent Saddle Representation  We now turn our attention to an equivalent saddle (min-max) representation for sparse metric learning (3) which is essential for developing optimization algorithms in the next section. [sent-82, score-0.606]
</p><p>36 To this end, we need the following lemma which develops and extends a similar version in multi-task learning [1, 2] to the case of learning a positive semi-deﬁnite distance matrix. [sent-83, score-0.149]
</p><p>37 Problem (3) is equivalent to the following convex optimization problem (1 + xij M xij − xkj M xkj )+ + γ(Tr(M ))2  min  M  0  (4)  τ =(i,j,k)∈T  Proof. [sent-85, score-1.257]
</p><p>38 xij M xij ≤ xkj M xkj + ξτ , d ξτ ≥ 0 ∀τ = (i, j, k) ∈ T , and M ∈ S+ . [sent-89, score-1.08]
</p><p>39 2 k λk Vki  2 2 k λk Vki 2 k λk Vki  From the above lemma, we are ready to present an equivalent saddle (min-max) representation of d problem (3). [sent-104, score-0.204]
</p><p>40 Problem (4) is equivalent to the following saddle representation uτ (xjk xjk − xij xij ), M − γ(Tr(M ))2 −  min max  u∈Q1 M ∈Q2  uτ  (7)  t∈T  τ =(i,j,k)∈T  Proof. [sent-109, score-0.958]
</p><p>41 By its deﬁnition, there holds γ(Tr(M ∗ ))2 ≤ τ ∈T (1 + xkj M xik − xkj M xkj )+ + γ(Tr(M ))2 for any M 0. [sent-111, score-0.867]
</p><p>42 Hence, problem (4) is identical to (1 + xij M xij − xkj M xkj )+ + γ(Tr(M ))2 . [sent-113, score-1.08]
</p><p>43 Consequently, the above equation can be written as minM ∈Q2 max0≤u≤1 τ ∈T uτ (1 + xkj M xik − xij M xij ) + γ(Tr(M ))2 . [sent-115, score-0.831]
</p><p>44 Combining τ ∈T uτ (−xij M xij + xjk M xjk ) − γ(Tr(M )) this with the fact that xjk M xjk − xij M xij = xjk xjk − xij xij , M completes the proof of the theorem. [sent-119, score-2.349]
</p><p>45 2  Related Work  There is a considerable amount of work on metric learning. [sent-121, score-0.239]
</p><p>46 In [9], an information-theoretic approach to metric learning (ITML) is developed which equivalently transforms the metric learning problem 3  to that of learning an optimal Gaussian distribution with respect to an relative entropy. [sent-122, score-0.512]
</p><p>47 The method of Relevant Component analysis (RCA)[7] attempts to ﬁnd a distance metric which can minimize the covariance matrix imposed by the equivalence constraints. [sent-123, score-0.388]
</p><p>48 In [25], a distance metric for k-means clustering is then learned to shrink the averaged distance within the similar set while enlarging the average distance within the dissimilar set simultaneously. [sent-124, score-0.647]
</p><p>49 All the above methods generally do not yield sparse solutions and only work within their special settings. [sent-125, score-0.12]
</p><p>50 There are many other metric learning approaches in either unsupervised or supervised learning setting, see [26] for a detailed review. [sent-127, score-0.239]
</p><p>51 We particularly mention the following work which is more related to our sparse metric learning model (3). [sent-128, score-0.338]
</p><p>52 • Large Margin Nearest Neighbor (LMNN) [23, 24]: LMNN aims to explore a large margin nearest neighbor classiﬁer by exploiting nearest neighbor samples as side information in the training set. [sent-129, score-0.178]
</p><p>53 Speciﬁcally, let Nk (x) denotes the k-nearest neighbor of sample x and deﬁne the similar set S = {(xi , xj ) : xi ∈ N (xj ), yi = yj } and D = {(xj , xk ) : xk ∈ N (xj ), yk = yj }. [sent-130, score-0.35]
</p><p>54 Then, recall that the triplet set T is given by equation (2), the framework LMNN can be rewritten as the following: min  M  0  (1 + xij M xij − xkj M xkj )+ + γTr(CM )  (9)  τ =(i,j,k)∈T  where the covariance matrix C over the similar set S is deﬁned by C = (xi ,xj )∈S (xi − xj )(xi − xj ) . [sent-131, score-1.393]
</p><p>55 From the above reformulation, we see that LMNN also involves a sparse regularization term Tr(CM ). [sent-132, score-0.156]
</p><p>56 However, LMCA controls the sparsity by directly specifying the dimensionality of the transformation matrix and it is an extended version of LMNN. [sent-135, score-0.091]
</p><p>57 The learned sparse model would not generate an appropriate low-ranked principal matrix M for metric learning. [sent-138, score-0.386]
</p><p>58 Such a restriction would only result in a sub-optimal solution, although the ﬁnal optimization is an efﬁcient linear programming problem. [sent-140, score-0.097]
</p><p>59 3  Smooth Optimization Algorithms  Nesterov [17, 16] developed an efﬁcient smooth optimization method for solving convex programming problems of the form minx∈Q f (x) where Q is a bounded closed convex set in a ﬁnitedimensional real vector space E. [sent-141, score-0.347]
</p><p>60 This smooth optimization usually requires f to be differentiable with Lipschitz continuous gradient and it has an optimal convergence rate of O(1/t2 ) for smooth problems where t is the iteration number. [sent-142, score-0.396]
</p><p>61 Unfortunately, we can not directly apply the smooth optimization method to problem (4) since the hinge loss there is not continuously differentiable. [sent-143, score-0.224]
</p><p>62 Below we show the smooth approximation method [17] can be approached through the saddle representation (7). [sent-144, score-0.319]
</p><p>63 Let E2 be the dual space of E2 with standard norm deﬁned, for any ∗ s ∈ E2 , by s ∗ = max{ s, x 2 : x 2 = 1}, where the scalar product ·, · 2 denotes the value 2 ∗ ∗ of s at x. [sent-152, score-0.09]
</p><p>64 (11)  Here, φ(u) is assumed to be continuously differentiable and convex with Lipschitz continuous graˆ dient and f (x) is convex and differentiable. [sent-164, score-0.139]
</p><p>65 The above min-max problem is usually not smooth and Nesterov [17] proposed a smoothing approximation approach to solve the above problem: min  u∈Q1  φµ (u) = φ(u) + max{ Au, x  2  ˆ − f (x) − µd2 (x) : x ∈ Q2 } . [sent-165, score-0.199]
</p><p>66 (12)  Here, d2 (·) is a continuous proxy-function, strongly convex on Q2 with some convexity parameter σ2 > 0 and µ > 0 is a small smoohting parameter. [sent-166, score-0.102]
</p><p>67 1 2 2 Hence, the proxy-function d2 can be regarded as a generalized Moreau-Yosida regularization term to smooth out the objective function. [sent-175, score-0.197]
</p><p>68 Hence, we can apply the optimal smooth optimization scheme [17, Section 3] to the smooth approximate problem (12). [sent-177, score-0.369]
</p><p>69 Below, we will apply this general scheme to solve the min-max representation (7) of the sparse metric learning problem (3), and hence solves the original problem (4). [sent-183, score-0.425]
</p><p>70 2 Smooth Optimization Approach for Sparse Metric Learning We now turn our attention to developing a smooth optimization approach for problem (4). [sent-185, score-0.204]
</p><p>71 Our main idea is to connect the saddle representation (7) in Theorem 1 with the special formulation (11). [sent-186, score-0.2]
</p><p>72 To this end, ﬁrstly let E1 = RT with standard Euclidean norm · 1 = · and E2 = S d with 2 Frobenius norm deﬁned, for any S ∈ S d , by S 2 = i,j∈Nd Sij . [sent-187, score-0.094]
</p><p>73 Consequently, the proxy-function d2 (·) is strongly convex on Q2 with convexity parameter σ2 = 2. [sent-190, score-0.102]
</p><p>74 Finally, for any τ = (i, j, k) ∈ T , let 5  Xτ = xjk xjk − xij xij . [sent-191, score-0.87]
</p><p>75 (14)  τ ∈T  With the above preparations, the saddle representation (7) exactly matches the special structure (11) which can be approximated by problem (12) with µ sufﬁciently small. [sent-194, score-0.2]
</p><p>76 Let the linear operator A be deﬁned as above, then A d  for any M ∈ S , M  2  1,2  ≤  2 2  Xτ  τ ∈T  1 2  where,  denotes the Frobenius norm of M . [sent-197, score-0.101]
</p><p>77 We now can adapt the smooth optimization [17, Section 3 and Theorem 3] to solve the smooth approximation formulation (12) for metric learning. [sent-200, score-0.583]
</p><p>78 The smooth optimization pseudo-code for problem (7) (equivalently problem (4)) is outlined in Table 1. [sent-204, score-0.204]
</p><p>79 The efﬁciency of Nesterov’s smooth optimization largely depends on Steps 2, 3, and 4 in Table 1. [sent-206, score-0.204]
</p><p>80 Problem (15) is equivalent to the following s∗ = arg max  λi si − γ( i∈Nd  i∈Nd  si )2 − µ  s2 : i i∈Nd  si ≤  T /γ, and si ≥ 0 ∀i ∈ Nd  (16)  i∈Nd  where λ = (λ1 , . [sent-211, score-0.174]
</p><p>81 Moreover, if we denotes the eigendecomposition t∈T ut Xt by t∈T ut Xt = U diag(λ)U with some U ∈ O d then the optimal solution of problem (15) is given by Mµ (u) = U diag(s∗ )U . [sent-215, score-0.129]
</p><p>82 As listed in Table 1, the complexity for each iteration mainly depends on the eigen-decomposition on t∈Nt ut Xt and the quadratic programming to solve problem (15) which has complexity O(d3 ). [sent-222, score-0.086]
</p><p>83 Hence, the overall iteration complexity of the smooth optimization approach for sparse metric learning is of the order O(d3 /ε) for ﬁnding an 1 ε-optimal solution. [sent-223, score-0.542]
</p><p>84 We also implemented the iterative sub-gradient descent algorithm [24] to solve the proposed framework (4) (called SMLgd) in order to evaluate the efﬁciency of the proposed smooth optimization algorithm SMLsm. [sent-228, score-0.204]
</p><p>85 We try to exploit all these methods to learn a good distance metric and a KNN classiﬁer is used to examine the performance of these different learned metrics. [sent-229, score-0.36]
</p><p>86 All the approaches except the Euclidean distance need to deﬁne a triplet set T before training. [sent-235, score-0.157]
</p><p>87 It is observed that our model outputs the most sparse metric. [sent-245, score-0.099]
</p><p>88 That is, our method directly learns both an accurate and sparse distance metric simultaneously. [sent-247, score-0.459]
</p><p>89 ITML and Euc do not generate a sparse metric at all. [sent-249, score-0.338]
</p><p>90 Finally, in order to examine the efﬁciency of the proposed smooth optimization algorithm, we plot the convergence graphs of SMLsm versus those of SMLgd in Figure 1(i)-(l). [sent-250, score-0.227]
</p><p>91 5  Conclusion  In this paper we proposed a novel regularization framework for learning a sparse (low-rank) distance matrix. [sent-254, score-0.277]
</p><p>92 This model was realized by a mixed-norm regularization term over a distance matrix which is non-convex. [sent-255, score-0.226]
</p><p>93 Using its special structure, it was shown to be equivalent to a convex min-max (saddle) representation involving a trace norm regularization. [sent-256, score-0.205]
</p><p>94 Depart from the saddle representation, we successfully developed an efﬁcient Nesterov’s ﬁrst-order optimization approach [16, 17] for our metric learning model. [sent-257, score-0.447]
</p><p>95 Experimental results on various datasets show that our sparse metric learning framework outperforms other state-of-the-art methods with higher accuracy and signiﬁcantly smaller dimensionality. [sent-258, score-0.338]
</p><p>96 Subﬁgures (a)-(d) present the average error rates; (e)-(h) plots the average dimensionality used in different methods; (i)-(l) give the convergence graph for the sub-gradient algorithm and the proposed smooth optimization algorithm. [sent-334, score-0.227]
</p><p>97 Learning a similarity metric discriminatively with application to face veriﬁcation. [sent-386, score-0.239]
</p><p>98 Distance metric learning for large margin nearest neighbour classiﬁcation. [sent-479, score-0.308]
</p><p>99 Fast solvers and efﬁcient implementations for distance metric learning. [sent-486, score-0.36]
</p><p>100 Distance metric learning with application to clustering with side information. [sent-493, score-0.239]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('smlsm', 0.398), ('smlgd', 0.378), ('xkj', 0.279), ('xij', 0.261), ('smllp', 0.259), ('metric', 0.239), ('euc', 0.199), ('lmnn', 0.194), ('itml', 0.176), ('xjk', 0.174), ('saddle', 0.144), ('smooth', 0.14), ('vki', 0.139), ('tr', 0.133), ('distance', 0.121), ('xj', 0.108), ('nesterov', 0.1), ('sparse', 0.099), ('au', 0.09), ('minu', 0.08), ('iris', 0.065), ('optimization', 0.064), ('xi', 0.064), ('vkj', 0.06), ('xk', 0.058), ('regularization', 0.057), ('wine', 0.057), ('dim', 0.057), ('epoch', 0.057), ('convex', 0.055), ('ut', 0.053), ('minv', 0.052), ('nn', 0.051), ('lipschitz', 0.048), ('norm', 0.047), ('dissimilar', 0.045), ('lmca', 0.04), ('neighbor', 0.039), ('margin', 0.038), ('diag', 0.037), ('rt', 0.037), ('triplet', 0.036), ('ionosphere', 0.036), ('representation', 0.035), ('bristol', 0.035), ('iono', 0.035), ('equivalently', 0.034), ('vk', 0.033), ('min', 0.033), ('transformation', 0.033), ('programming', 0.033), ('wd', 0.032), ('bal', 0.032), ('cm', 0.031), ('operator', 0.031), ('nearest', 0.031), ('sparsity', 0.03), ('xik', 0.03), ('transformed', 0.03), ('differentiable', 0.029), ('argyriou', 0.028), ('matrix', 0.028), ('lemma', 0.028), ('convexity', 0.028), ('triplets', 0.027), ('rennie', 0.027), ('exhibiting', 0.027), ('hence', 0.027), ('nips', 0.027), ('reduction', 0.027), ('ciency', 0.027), ('euclidean', 0.026), ('si', 0.026), ('smoothing', 0.026), ('enforce', 0.026), ('knn', 0.026), ('collapsing', 0.026), ('balance', 0.026), ('max', 0.025), ('equivalent', 0.025), ('scheme', 0.025), ('minx', 0.024), ('xt', 0.024), ('denotes', 0.023), ('convergence', 0.023), ('accelerate', 0.022), ('pd', 0.022), ('trace', 0.022), ('weinberger', 0.021), ('dimension', 0.021), ('theorem', 0.021), ('special', 0.021), ('realized', 0.02), ('arg', 0.02), ('principal', 0.02), ('dual', 0.02), ('orthonormal', 0.02), ('putting', 0.02), ('hinge', 0.02), ('strongly', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="223-tfidf-1" href="./nips-2009-Sparse_Metric_Learning_via_Smooth_Optimization.html">223 nips-2009-Sparse Metric Learning via Smooth Optimization</a></p>
<p>Author: Yiming Ying, Kaizhu Huang, Colin Campbell</p><p>Abstract: In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efﬁcient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a nondifferentiable loss function. Finally, we run experiments to validate the effectiveness and efﬁciency of our sparse metric learning model on various datasets.</p><p>2 0.30749479 <a title="223-tfidf-2" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>Author: Rong Jin, Shijun Wang, Yang Zhou</p><p>Abstract: In this paper, we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data. In addition, we present an efﬁcient online learning algorithm for regularized distance metric learning. Our empirical studies with data classiﬁcation and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efﬁcient and robust for high dimensional data.</p><p>3 0.12988304 <a title="223-tfidf-3" href="./nips-2009-Analysis_of_SVM_with_Indefinite_Kernels.html">33 nips-2009-Analysis of SVM with Indefinite Kernels</a></p>
<p>Author: Yiming Ying, Colin Campbell, Mark Girolami</p><p>Abstract: The recent introduction of indeﬁnite SVM by Luss and d’Aspremont [15] has effectively demonstrated SVM classiﬁcation with a non-positive semi-deﬁnite kernel (indeﬁnite kernel). This paper studies the properties of the objective function introduced there. In particular, we show that the objective function is continuously differentiable and its gradient can be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous. The main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indeﬁnite kernel matrix and the proxy positive semi-deﬁnite one. Our elementary result greatly facilitates the application of gradient-based algorithms. Based on our analysis, we further develop Nesterov’s smooth optimization approach [17, 18] for indeﬁnite SVM which has an optimal convergence rate for smooth problems. Experiments on various benchmark datasets validate our analysis and demonstrate the efﬁciency of our proposed algorithms.</p><p>4 0.11940141 <a title="223-tfidf-4" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton Hengel</p><p>Abstract: The learning of appropriate distance metrics is a critical problem in image classiﬁcation and retrieval. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a Mahalanobis distance metric. One of the primary difﬁculties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semideﬁnite. Semideﬁnite programming is sometimes used to enforce this constraint, but does not scale well. B OOST M ETRIC is instead based on a key observation that any positive semideﬁnite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. 1</p><p>5 0.11697632 <a title="223-tfidf-5" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>Author: Lei Wu, Rong Jin, Steven C. Hoi, Jianke Zhu, Nenghai Yu</p><p>Abstract: Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a ﬁxed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a nonparametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of ﬁxed metric by implicitly deriving a local distance from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We also present an efﬁcient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efﬁcient for high dimensional data. 1</p><p>6 0.096920088 <a title="223-tfidf-6" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>7 0.086930014 <a title="223-tfidf-7" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>8 0.083794534 <a title="223-tfidf-8" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>9 0.077791899 <a title="223-tfidf-9" href="./nips-2009-Factor_Modeling_for_Advertisement_Targeting.html">90 nips-2009-Factor Modeling for Advertisement Targeting</a></p>
<p>10 0.072977982 <a title="223-tfidf-10" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>11 0.070787936 <a title="223-tfidf-11" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>12 0.069478944 <a title="223-tfidf-12" href="./nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</a></p>
<p>13 0.069146372 <a title="223-tfidf-13" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>14 0.064214379 <a title="223-tfidf-14" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>15 0.062542491 <a title="223-tfidf-15" href="./nips-2009-Rank-Approximate_Nearest_Neighbor_Search%3A_Retaining_Meaning_and_Speed_in_High_Dimensions.html">198 nips-2009-Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions</a></p>
<p>16 0.061576787 <a title="223-tfidf-16" href="./nips-2009-Statistical_Analysis_of_Semi-Supervised_Learning%3A_The_Limit_of_Infinite_Unlabelled_Data.html">229 nips-2009-Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></p>
<p>17 0.059266675 <a title="223-tfidf-17" href="./nips-2009-Information-theoretic_lower_bounds_on_the_oracle_complexity_of_convex_optimization.html">116 nips-2009-Information-theoretic lower bounds on the oracle complexity of convex optimization</a></p>
<p>18 0.056326285 <a title="223-tfidf-18" href="./nips-2009-Semi-supervised_Regression_using_Hessian_energy_with_an_application_to_semi-supervised_dimensionality_reduction.html">214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</a></p>
<p>19 0.055784341 <a title="223-tfidf-19" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>20 0.055718143 <a title="223-tfidf-20" href="./nips-2009-Exponential_Family_Graph_Matching_and_Ranking.html">87 nips-2009-Exponential Family Graph Matching and Ranking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.183), (1, 0.156), (2, -0.015), (3, 0.074), (4, 0.014), (5, 0.01), (6, 0.0), (7, 0.018), (8, 0.046), (9, 0.084), (10, 0.079), (11, 0.003), (12, 0.115), (13, 0.077), (14, 0.005), (15, -0.209), (16, 0.119), (17, -0.07), (18, -0.054), (19, 0.004), (20, -0.165), (21, -0.114), (22, -0.071), (23, -0.009), (24, 0.04), (25, 0.094), (26, -0.032), (27, -0.178), (28, -0.025), (29, 0.024), (30, -0.005), (31, 0.085), (32, 0.137), (33, 0.005), (34, -0.147), (35, 0.055), (36, -0.009), (37, 0.021), (38, -0.119), (39, 0.014), (40, -0.133), (41, -0.055), (42, -0.105), (43, 0.083), (44, -0.094), (45, -0.003), (46, 0.02), (47, 0.039), (48, -0.055), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94386762 <a title="223-lsi-1" href="./nips-2009-Sparse_Metric_Learning_via_Smooth_Optimization.html">223 nips-2009-Sparse Metric Learning via Smooth Optimization</a></p>
<p>Author: Yiming Ying, Kaizhu Huang, Colin Campbell</p><p>Abstract: In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efﬁcient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a nondifferentiable loss function. Finally, we run experiments to validate the effectiveness and efﬁciency of our sparse metric learning model on various datasets.</p><p>2 0.84649521 <a title="223-lsi-2" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>Author: Rong Jin, Shijun Wang, Yang Zhou</p><p>Abstract: In this paper, we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data. In addition, we present an efﬁcient online learning algorithm for regularized distance metric learning. Our empirical studies with data classiﬁcation and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efﬁcient and robust for high dimensional data.</p><p>3 0.83175874 <a title="223-lsi-3" href="./nips-2009-Positive_Semidefinite_Metric_Learning_with_Boosting.html">191 nips-2009-Positive Semidefinite Metric Learning with Boosting</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton Hengel</p><p>Abstract: The learning of appropriate distance metrics is a critical problem in image classiﬁcation and retrieval. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a Mahalanobis distance metric. One of the primary difﬁculties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semideﬁnite. Semideﬁnite programming is sometimes used to enforce this constraint, but does not scale well. B OOST M ETRIC is instead based on a key observation that any positive semideﬁnite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. 1</p><p>4 0.59910089 <a title="223-lsi-4" href="./nips-2009-Learning_Bregman_Distance_Functions_and_Its_Application_for_Semi-Supervised_Clustering.html">126 nips-2009-Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></p>
<p>Author: Lei Wu, Rong Jin, Steven C. Hoi, Jianke Zhu, Nenghai Yu</p><p>Abstract: Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a ﬁxed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a nonparametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of ﬁxed metric by implicitly deriving a local distance from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We also present an efﬁcient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efﬁcient for high dimensional data. 1</p><p>5 0.52949351 <a title="223-lsi-5" href="./nips-2009-Analysis_of_SVM_with_Indefinite_Kernels.html">33 nips-2009-Analysis of SVM with Indefinite Kernels</a></p>
<p>Author: Yiming Ying, Colin Campbell, Mark Girolami</p><p>Abstract: The recent introduction of indeﬁnite SVM by Luss and d’Aspremont [15] has effectively demonstrated SVM classiﬁcation with a non-positive semi-deﬁnite kernel (indeﬁnite kernel). This paper studies the properties of the objective function introduced there. In particular, we show that the objective function is continuously differentiable and its gradient can be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous. The main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indeﬁnite kernel matrix and the proxy positive semi-deﬁnite one. Our elementary result greatly facilitates the application of gradient-based algorithms. Based on our analysis, we further develop Nesterov’s smooth optimization approach [17, 18] for indeﬁnite SVM which has an optimal convergence rate for smooth problems. Experiments on various benchmark datasets validate our analysis and demonstrate the efﬁciency of our proposed algorithms.</p><p>6 0.52527976 <a title="223-lsi-6" href="./nips-2009-An_Online_Algorithm_for_Large_Scale_Image_Similarity_Learning.html">32 nips-2009-An Online Algorithm for Large Scale Image Similarity Learning</a></p>
<p>7 0.4485096 <a title="223-lsi-7" href="./nips-2009-Efficient_Recovery_of_Jointly_Sparse_Vectors.html">79 nips-2009-Efficient Recovery of Jointly Sparse Vectors</a></p>
<p>8 0.43513963 <a title="223-lsi-8" href="./nips-2009-Factor_Modeling_for_Advertisement_Targeting.html">90 nips-2009-Factor Modeling for Advertisement Targeting</a></p>
<p>9 0.41208962 <a title="223-lsi-9" href="./nips-2009-Dual_Averaging_Method_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">73 nips-2009-Dual Averaging Method for Regularized Stochastic Learning and Online Optimization</a></p>
<p>10 0.41176978 <a title="223-lsi-10" href="./nips-2009-Robust_Nonparametric_Regression_with_Metric-Space_Valued_Output.html">207 nips-2009-Robust Nonparametric Regression with Metric-Space Valued Output</a></p>
<p>11 0.3987751 <a title="223-lsi-11" href="./nips-2009-Fast_Graph_Laplacian_Regularized_Kernel_Learning_via_Semidefinite%E2%80%93Quadratic%E2%80%93Linear_Programming.html">92 nips-2009-Fast Graph Laplacian Regularized Kernel Learning via Semidefinite–Quadratic–Linear Programming</a></p>
<p>12 0.39568308 <a title="223-lsi-12" href="./nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></p>
<p>13 0.38724363 <a title="223-lsi-13" href="./nips-2009-On_the_Convergence_of_the_Concave-Convex_Procedure.html">180 nips-2009-On the Convergence of the Concave-Convex Procedure</a></p>
<p>14 0.3864367 <a title="223-lsi-14" href="./nips-2009-Lower_bounds_on_minimax_rates_for_nonparametric_regression_with_additive_sparsity_and_smoothness.html">144 nips-2009-Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></p>
<p>15 0.3712683 <a title="223-lsi-15" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>16 0.36204994 <a title="223-lsi-16" href="./nips-2009-Fast%2C_smooth_and_adaptive_regression_in_metric_spaces.html">91 nips-2009-Fast, smooth and adaptive regression in metric spaces</a></p>
<p>17 0.3547588 <a title="223-lsi-17" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>18 0.34260058 <a title="223-lsi-18" href="./nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</a></p>
<p>19 0.34037012 <a title="223-lsi-19" href="./nips-2009-Distribution_Matching_for_Transduction.html">72 nips-2009-Distribution Matching for Transduction</a></p>
<p>20 0.33995593 <a title="223-lsi-20" href="./nips-2009-Manifold_Regularization_for_SIR_with_Rate_Root-n_Convergence.html">146 nips-2009-Manifold Regularization for SIR with Rate Root-n Convergence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(24, 0.034), (25, 0.03), (35, 0.044), (36, 0.075), (39, 0.036), (44, 0.027), (58, 0.522), (61, 0.046), (71, 0.027), (86, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97852051 <a title="223-lda-1" href="./nips-2009-Bayesian_estimation_of_orientation_preference_maps.html">43 nips-2009-Bayesian estimation of orientation preference maps</a></p>
<p>Author: Sebastian Gerwinn, Leonard White, Matthias Kaschube, Matthias Bethge, Jakob H. Macke</p><p>Abstract: Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial and temporal scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data. In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data. We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reﬂects known properties of OPMs, and a noise covariance adjusted to the data. The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements. By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations, pinwheel locations or pinwheel counts. Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and quantitative model comparisons. We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex. 1</p><p>2 0.97021526 <a title="223-lda-2" href="./nips-2009-Robust_Principal_Component_Analysis%3A_Exact_Recovery_of_Corrupted_Low-Rank_Matrices_via_Convex_Optimization.html">208 nips-2009-Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization</a></p>
<p>Author: John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, Yi Ma</p><p>Abstract: Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized “robust principal component analysis” problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efﬁciently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the ﬁrst proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision. 1</p><p>3 0.96307343 <a title="223-lda-3" href="./nips-2009-White_Functionals_for_Anomaly_Detection_in_Dynamical_Systems.html">257 nips-2009-White Functionals for Anomaly Detection in Dynamical Systems</a></p>
<p>Author: Marco Cuturi, Jean-philippe Vert, Alexandre D'aspremont</p><p>Abstract: We propose new methodologies to detect anomalies in discrete-time processes taking values in a probability space. These methods are based on the inference of functionals whose evaluations on successive states visited by the process are stationary and have low autocorrelations. Deviations from this behavior are used to ﬂag anomalies. The candidate functionals are estimated in a subspace of a reproducing kernel Hilbert space associated with the original probability space considered. We provide experimental results on simulated datasets which show that these techniques compare favorably with other algorithms.</p><p>same-paper 4 0.95765501 <a title="223-lda-4" href="./nips-2009-Sparse_Metric_Learning_via_Smooth_Optimization.html">223 nips-2009-Sparse Metric Learning via Smooth Optimization</a></p>
<p>Author: Yiming Ying, Kaizhu Huang, Colin Campbell</p><p>Abstract: In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efﬁcient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a nondifferentiable loss function. Finally, we run experiments to validate the effectiveness and efﬁciency of our sparse metric learning model on various datasets.</p><p>5 0.95467615 <a title="223-lda-5" href="./nips-2009-Directed_Regression.html">67 nips-2009-Directed Regression</a></p>
<p>Author: Yi-hao Kao, Benjamin V. Roy, Xiang Yan</p><p>Abstract: When used to guide decisions, linear regression analysis typically involves estimation of regression coefﬁcients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneﬁcial to account for the decision objective when computing regression coefﬁcients. Empirical optimization does so but sacriﬁces performance when features are well-chosen or training data are insufﬁcient. We propose directed regression, an efﬁcient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate signiﬁcant performance gains over either alternative. We also develop a theory that motivates the algorithm. 1</p><p>6 0.94087088 <a title="223-lda-6" href="./nips-2009-Ensemble_Nystrom_Method.html">81 nips-2009-Ensemble Nystrom Method</a></p>
<p>7 0.77480578 <a title="223-lda-7" href="./nips-2009-Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Compressed_Sensing.html">36 nips-2009-Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></p>
<p>8 0.77177984 <a title="223-lda-8" href="./nips-2009-Matrix_Completion_from_Noisy_Entries.html">147 nips-2009-Matrix Completion from Noisy Entries</a></p>
<p>9 0.75893903 <a title="223-lda-9" href="./nips-2009-On_Learning_Rotations.html">177 nips-2009-On Learning Rotations</a></p>
<p>10 0.72643924 <a title="223-lda-10" href="./nips-2009-The_Ordered_Residual_Kernel_for_Robust_Motion_Subspace_Clustering.html">243 nips-2009-The Ordered Residual Kernel for Robust Motion Subspace Clustering</a></p>
<p>11 0.72445142 <a title="223-lda-11" href="./nips-2009-Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data.html">254 nips-2009-Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></p>
<p>12 0.70316368 <a title="223-lda-12" href="./nips-2009-Regularized_Distance_Metric_Learning%3ATheory_and_Algorithm.html">202 nips-2009-Regularized Distance Metric Learning:Theory and Algorithm</a></p>
<p>13 0.70249629 <a title="223-lda-13" href="./nips-2009-Correlation_Coefficients_are_Insufficient_for_Analyzing_Spike_Count_Dependencies.html">62 nips-2009-Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></p>
<p>14 0.7000562 <a title="223-lda-14" href="./nips-2009-Sparse_and_Locally_Constant_Gaussian_Graphical_Models.html">224 nips-2009-Sparse and Locally Constant Gaussian Graphical Models</a></p>
<p>15 0.69880712 <a title="223-lda-15" href="./nips-2009-Speeding_up_Magnetic_Resonance_Image_Acquisition_by_Bayesian_Multi-Slice_Adaptive_Compressed_Sensing.html">228 nips-2009-Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></p>
<p>16 0.69852567 <a title="223-lda-16" href="./nips-2009-Accelerated_Gradient_Methods_for_Stochastic_Optimization_and_Online_Learning.html">22 nips-2009-Accelerated Gradient Methods for Stochastic Optimization and Online Learning</a></p>
<p>17 0.69685727 <a title="223-lda-17" href="./nips-2009-Convex_Relaxation_of_Mixture_Regression_with_Efficient_Algorithms.html">61 nips-2009-Convex Relaxation of Mixture Regression with Efficient Algorithms</a></p>
<p>18 0.69455576 <a title="223-lda-18" href="./nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</a></p>
<p>19 0.69239587 <a title="223-lda-19" href="./nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></p>
<p>20 0.69074374 <a title="223-lda-20" href="./nips-2009-Lattice_Regression.html">124 nips-2009-Lattice Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
