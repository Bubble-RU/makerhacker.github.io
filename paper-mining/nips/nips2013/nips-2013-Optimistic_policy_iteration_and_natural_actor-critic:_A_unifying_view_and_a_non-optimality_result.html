<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-239" href="#">nips2013-239</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</h1>
<br/><p>Source: <a title="nips-2013-239-pdf" href="http://papers.nips.cc/paper/5188-optimistic-policy-iteration-and-natural-actor-critic-a-unifying-view-and-a-non-optimality-result.pdf">pdf</a></p><p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>Reference: <a title="nips-2013-239-reference" href="../nips2013_reference/nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result  Paul Wagner Department of Information and Computer Science Aalto University FI-00076 Aalto, Finland paul. [sent-1, score-0.914]
</p><p>2 fi  Abstract Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. [sent-3, score-1.074]
</p><p>3 The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. [sent-5, score-1.566]
</p><p>4 As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. [sent-6, score-1.971]
</p><p>5 1  Introduction  We consider the reinforcement learning problem in which one attempts to ﬁnd an approximately optimal policy for controlling a stochastic nonlinear dynamical system. [sent-8, score-0.889]
</p><p>6 Here the sampling policy changes during the learning process in a manner that depends on the main policy being optimized. [sent-10, score-1.544]
</p><p>7 The majority of these methods are often categorized into greedy value function methods (critic-only) and value-based policy gradient methods (actor-critic) [e. [sent-15, score-0.981]
</p><p>8 Within this interactive setting, the policy gradient approach has better convergence guarantees, with the strongest case being for Monte Carlo evaluation with ‘compatible’ value function approximation. [sent-18, score-1.029]
</p><p>9 1) to a local optimum can be established for arbitrary differentiable policy classes under mild assumptions [22, 13, 19]. [sent-21, score-0.904]
</p><p>10 1  value function methods can fail to converge to any single policy and instead become trapped in sustained policy oscillation or policy chattering, which is currently a poorly understood phenomenon [6, 7]. [sent-29, score-2.622]
</p><p>11 This applies to both non-optimistic and optimistic policy iteration (value iteration being a special case of the latter). [sent-30, score-1.124]
</p><p>12 A hard-greedy policy is a discontinuous function of its parameters, which has been identiﬁed as a key source of problems [18, 10, 17, 22]. [sent-41, score-0.772]
</p><p>13 20], it is known that continuously stochastic policies can also re-gain convergence: both non-optimistic and optimistic soft-greedy approximate policy iteration using, for example, the Gibbs/Boltzmann policy class, is known to converge with enough softness, ‘enough’ being problem-speciﬁc. [sent-43, score-1.976]
</p><p>14 We share the belief of Bertsekas [5, 6], expressed in the context of the policy oscillation phenomenon, that a better understanding of these issues “has the potential to alter in fundamental ways our thinking about approximate DP. [sent-48, score-0.98]
</p><p>15 ” In this paper, we provide insight into the convergence behavior and optimality of the generalized optimistic form of the greedy value function methodology by reﬂecting it against the policy gradient approach. [sent-49, score-1.319]
</p><p>16 These issues revolve mainly around the greedy methodology, while at the same time, solid convergence results exist for the policy gradient methodology; connecting these methodologies more ﬁrmly might well lead to a fuller understanding of both. [sent-51, score-1.058]
</p><p>17 First, we show that natural actor-critic methods from the policy gradient side are, in fact, a limiting special case of optimistic policy iteration (Sec. [sent-53, score-1.979]
</p><p>18 Second, we show that while having the potential to avoid policy oscillation and chattering, a substantial subset of soft-greedy value function approaches can never converge to an optimal policy, except in a certain pathological case (Sec. [sent-55, score-1.052]
</p><p>19 A policy π(a|s, θk ) ∈ Π is a stochastic mapping from states to actions, parameterized by θk ∈ Θ. [sent-66, score-0.824]
</p><p>20 π J(θk ) ∈ Π denotes the corresponding policy gradient in the selected policy space. [sent-69, score-1.634]
</p><p>21 We deﬁne the policy distance πu − πv as some p-norm of the action probability ¯ differences ( s a |πu (a|s) − πv (a|s)|p )1/p . [sent-70, score-0.807]
</p><p>22 Action value functions Q(s, a, wk ) and Q(s, a, wk ), ˆ ˆ parameterized by wk , are estimators of the γ-discounted cumulative reward t γ t E[r(St , At )|S0 = ˆ s, A0 = a, π(θk )] for some (s, a) when following some policy π(θk ). [sent-71, score-1.684]
</p><p>23 The state value function V (s, wk ) is an estimator of such cumulative reward that follows some s. [sent-72, score-0.32]
</p><p>24 2  We focus on the Gibbs (Boltzmann) policy class with a linear combination of basis functions φ: eθk φ(s,a)  π(a|s, θk ) =  b  eθk φ(s,b)  . [sent-74, score-0.772]
</p><p>25 (1)  We shall use the term ‘semi-uniformly stochastic policy’ for referring to a policy for which π(a|s) = cs ∨ π(a|s) = 0, ∀s, a, ∀s ∃cs ∈ [0, 1]. [sent-75, score-0.824]
</p><p>26 Note that both the uniformly stochastic policy and all deterministic policies are special cases of semi-uniformly stochastic policies. [sent-76, score-0.964]
</p><p>27 We consider both advantage values [see 22, 19] ¯ Qk (s, a, wk ) = wk ˆ ˆ  φ(s, a) −  π(b|s, θk )φ(s, b)  (2)  b  and absolute action values Qk (s, a, wk ) = wk φ(s, a) . [sent-78, score-1.219]
</p><p>28 We focus on optimistic policy iteration, which contains both non-optimistic policy iteration and value iteration as special cases, and on the policy gradient counterparts of these. [sent-80, score-2.782]
</p><p>29 In the general form of optimistic approximate policy iteration (e. [sent-81, score-1.064]
</p><p>30 3]), a value function parameter vector w is gradually interpolated toward the most recent evaluation w: ˆ wk+1 = wk + κk (wk − wk ) , ˆ  κk ∈ (0, 1] . [sent-85, score-0.817]
</p><p>31 (4)  Non-optimistic policy iteration is obtained with κk = 1, ∀k and ‘complete’ evaluations wk (see ˆ below). [sent-86, score-1.151]
</p><p>32 The corresponding Gibbs soft-greedy policy is obtained by combining (1) and a temperature (softness) parameter τ with θk+1 = wk+1 /τk ,  τk ∈ (0, ∞) . [sent-87, score-0.906]
</p><p>33 In optimistic policy iteration, policy improvement is based on an incomplete evaluation. [sent-89, score-1.752]
</p><p>34 With shallow evaluation, the current value function parameter vector wk is required for look-ahead truncation when computing wk+1 . [sent-95, score-0.32]
</p><p>35 Inaccurate (noisy) evaluaˆ tion necessitates additional caution in the policy improvement process and is the usual motivation for using (4) with κ < 1. [sent-96, score-0.794]
</p><p>36 It is well known that greedy policy iteration can be non-convergent under approximations [4]. [sent-97, score-0.93]
</p><p>37 The widely used projected equation approach can manifest convergence behavior that is complex and not well understood, including bounded but potentially severe sustained policy oscillations [6, 7] (see the extended version for further details). [sent-98, score-0.938]
</p><p>38 A novel explanation to the phenomenon in the non-optimistic case was recently proposed in [24, 25], where policy oscillation was re-cast as sustained overshooting over an attractive stochastic policy. [sent-102, score-1.091]
</p><p>39 With value-based policy gradient methods, using (1) together with either (2) or (3) fulﬁlls the ‘compatibility condition’ [22, 13]. [sent-106, score-0.862]
</p><p>40 With (2), the value function parameter vector wk becomes the natural gradient estimate for the evaluated ˆ policy π(θk ), leading to natural actor-critic algorithms [11, 19], for which ηk = wk . [sent-107, score-1.546]
</p><p>41 ˆ 3  (7)  For policy gradient learning with a ‘compatible’ value function and Monte Carlo evaluation, convergence w. [sent-108, score-0.956]
</p><p>42 3  Forgetful natural actor-critic  In this section, we show that an important subset of natural actor-critic algorithms is a limiting special case of optimistic policy iteration. [sent-112, score-1.068]
</p><p>43 A related connection was recently shown in [24, 25], where a modiﬁed form of the natural actor-critic algorithm by Peters & Schaal [19] was shown to correspond to non-optimistic policy iteration. [sent-113, score-0.806]
</p><p>44 We consider the Gibbs policy class from (1) and the linear-in-parameters advantage function from (2), which form a ‘compatible’ actor-critic setup. [sent-115, score-0.772]
</p><p>45 In the following, we show that this algorithm is, within the discussed context, equivalent to the general form of optimistic policy iteration in (4) and (5), with the following translation of the parameterization: κk κk τk = , or αk = . [sent-121, score-1.041]
</p><p>46 (9) αk τk Taking the forgetting factor κ in (8) toward zero leads back toward the original natural actor-critic algorithm, with the implication that the original algorithm is a limiting special case of optimistic policy iteration. [sent-122, score-1.394]
</p><p>47 For the case of deep policy evaluation (Section 2), the natural actor-critic algorithm for the Gibbs policy class ((6), (7), (1), (2)) is a limiting special case of Gibbs soft-greedy optimistic policy iteration ((4), (5), (1), (2)). [sent-124, score-2.715]
</p><p>48 The update rule for Gibbs soft-greedy optimistic policy iteration is given in (4) and (5). [sent-126, score-1.041]
</p><p>49 By moving the temperature to scale w (assume w0 to be scaled accordingly), we obtain ˆ wk+1 θk+1  = wk + κk (wk /τk − wk ) ˆ = wk+1 ,  (10)  again with κk ∈ (0, 1], τk ∈ (0, ∞). [sent-127, score-0.748]
</p><p>50 Such a re-formulation effectively re-scales w and is possible only with deep policy evaluation (cf. [sent-128, score-0.826]
</p><p>51 Section 2), with which the non-scaled w is not needed by the policy evaluation process. [sent-129, score-0.826]
</p><p>52 ˆ  (11)  Finally, we open up the last term and encapsulate κ/τ into α: θk+1 = θk + κk (wk /τk ) − κk θk ˆ = θk + αk wk − κk θk , ˆ  (12) (13)  with αk = κk /τk . [sent-131, score-0.296]
</p><p>53 First, it becomes apparent that the implicit effective step size in optimistic policy iteration is, in fact, α = κ/τ , i. [sent-135, score-1.062]
</p><p>54 If the interpolation factor κ is held ﬁxed, a low temperature, which can lead to policy 4  oscillation, equals a long effective step size. [sent-138, score-0.868]
</p><p>55 This agrees with the interpretation of policy oscillation as overshooting in [24, 25]. [sent-139, score-0.945]
</p><p>56 Second, we see that the interpolation scheme in optimistic policy iteration, while originally introduced for the sake of countering an inaccurate value function estimate, actually goes in the direction of the policy gradient methodology. [sent-146, score-1.919]
</p><p>57 Smooth interpolation between policy gradient and greedy value function learning turns out to be possible by simply adjusting the interpolation factor κ while treating the temperature τ as an inverse of the step size (we return to provide an interpretation of the role of κ at a later point). [sent-147, score-1.27]
</p><p>58 This connection also allows the convergence results from the policy gradient literature to be brought in (see Section 2): convergence w. [sent-149, score-1.002]
</p><p>59 1, under standard assumptions from the referred literature, to an optimal solution is established in the limit for this class of approximate optimistic policy iteration as the interpolation factor κ is taken toward zero and the step size requirements are inversely enforced on the temperature τ . [sent-151, score-1.494]
</p><p>60 4  Systematic non-optimality of soft-greedy methods  For greedy value function methods, using the hard-greedy policy class trivially prevents convergence to other than deterministic policies. [sent-156, score-0.963]
</p><p>61 Furthermore, the proximity of an attractive stochastic policy can prevent convergence altogether and trap the process in oscillation (cf. [sent-157, score-1.091]
</p><p>62 The Gibbs soft-greedy policy class, on the other hand, can represent stochastic policies, ﬁxed points do exist [10, 17], and convergence toward some policy is guaranteed with sufﬁcient softness [18, 14]. [sent-159, score-1.895]
</p><p>63 While convergence toward deterministic optimal decisions is trivially lost as soon as any softness is introduced (τ → 0, and assuming a bounded value function), one might hope that convergence toward stochastic optimal decisions could still occur in some cases. [sent-160, score-0.662]
</p><p>64 Unfortunately, as we show in the following, this is not the case: in the presence of any softness, this approach can never converge toward any optimal policy (i. [sent-161, score-0.968]
</p><p>65 At this point, we wish to make clear that we are not arguing against the practical value of the greedy value function methodology in (interactively) approximated problems; the methodology has some clear merits, and the sub-optimality and oscillations could well be negligible in a given task. [sent-164, score-0.295]
</p><p>66 Now, for Gibbs soft-greedy policy iteration ((1), (4) and (5)) using a linear-in-parameters value function approximator ((2) or (3)), including optimistic and non-optimistic variants (any κ in (4)), there cannot exist a ﬁxed point at an optimum, except for the uniformly stochastic policy. [sent-170, score-1.202]
</p><p>67 A ﬁxed point of the update rule (4) must satisfy wk = wk , ˆ  (14)  i. [sent-174, score-0.617]
</p><p>68 , at a ﬁxed point, the policy evaluation step wk := eval(π(wk /τk )) for the current parameter ˆ vector must yield the same parameter vector as its result: eval (π (wk /τk )) = wk . [sent-176, score-1.47]
</p><p>69 (15)  By applying (14) and (7), we have wk = wk = ηk = G(θk )−1 ˆ  θ J(θk )  ,  (16)  which shows that the ﬁxed-point policy π(wk /τk ) in (15) is deﬁned solely by its own (scaled) performance gradient. [sent-177, score-1.392]
</p><p>70 For an optimal policy and an unbiased estimator, this parameter gradient must, by deﬁnition, map to the zero policy gradient, i. [sent-178, score-1.654]
</p><p>71 Consequently, an optimal policy at a ﬁxed point is deﬁned solely by the zero policy gradient, making the policy equal to π(0), which is the uniformly stochastic policy. [sent-181, score-2.417]
</p><p>72 Assume a smooth policy gradient ﬁeld ( π J(πu ) − π J(πv ) → 0 as πu − πv → 0) and τ → 0. [sent-185, score-0.862]
</p><p>73 First, the policy distance between a ﬁxed point policy π f and an optimal policy π cannot be vanishingly small ( π f − π < ), except if the optimal policy π is a semi-uniformly stochastic policy. [sent-186, score-3.279]
</p><p>74 Second, for bounded returns (γ → 1 and r(s, a) → ±∞, ∀s, a), the policy distance between a ﬁxed point policy π f and an optimal policy π cannot be vanishingly small ( π f − π < ), except if the optimal policy π is the uniformly stochastic policy. [sent-187, score-3.321]
</p><p>75 For a policy π = π(wk /τk ) that is vanishingly close to an optimum, an unbiased ¯ parameter gradient ηk must, assuming a smooth gradient ﬁeld, map to a policy gradient that is vanishingly close to zero, i. [sent-189, score-2.06]
</p><p>76 (17)  If π is also a ﬁxed point, then, by (16), we can substitute both wk and ηk in (17) with wk : ¯ ˆ π(wk /τk + αwk ) − π(wk /τk ) < , ˆ ˆ ˆ ⇔ π ((1/τk + α)wk ) − π((1/τk )wk ) < , ˆ ˆ  ∀α > 0, α → ∞ ∀α > 0, α → ∞ . [sent-192, score-0.592]
</p><p>77 (18)  We now see that π is deﬁned solely by a temperature-scaled version of a vanishingly small policy ¯ gradient, and that the condition in (17) is equivalent to stating that any ﬁnite decrease of the temperature must not have a non-vanishing effect on π . [sent-193, score-1.072]
</p><p>78 As only semi-uniformly stochastic policies are ¯ invariant to such temperature decreases, it follows that π must be vanishingly close to such a policy. [sent-194, score-0.369]
</p><p>79 Consequently, for τ → 0, the ˆ uniformly stochastic policy π(0) becomes the only semi-uniformly stochastic policy that the Gibbs policy class in (1) can approach, with the implication that π must be vanishingly close to the uni¯ formly stochastic policy. [sent-196, score-2.654]
</p><p>80 To interpret the preceding theorems, we observe that the gist of them is that, assuming a wellbehaved gradient ﬁeld, the closer the evaluated policy is to an optimum, the closer the target point of the next greedy update will be to the origin (in policy parameter space). [sent-198, score-1.8]
</p><p>81 At a ﬁxed point, the policy parameter vector must equal the target point of the next update, causing convergence to or toward a policy that is exactly optimal but not at the origin to be a contradiction (Theorem 2). [sent-199, score-1.837]
</p><p>82 Convergence to or toward a policy that is vanishingly close to an optimum is also impossible, except if the optimum is (semi-)uniformly stochastic (Theorem 3). [sent-200, score-1.296]
</p><p>83 In practical terms, Theorem 2 states that even if the task at hand and the chosen hyperparameters would allow convergence to some policy in a ﬁnite number of iterations, the resulting policy can 6  never contain optimal decisions, except for uniformly stochastic ones. [sent-201, score-1.74]
</p><p>84 If convergence is to occur, then the limiting policy must reside “between” the origin and an optimum, i. [sent-203, score-0.981]
</p><p>85 Perkins & Precup [18] report negative convergence results for non-optimistic iteration (κ = 1) with a too low τ , while for optimistic iteration (κ < 1), Melo et al. [sent-208, score-0.422]
</p><p>86 For the Gibbs policy class, deterministic policies reside at inﬁnity in some direction in the parameter space, with two implications for the Markovian case. [sent-217, score-0.86]
</p><p>87 Consequently, the value function, being a correction toward an optimum, never vanishes toward a ‘neutral’ state. [sent-219, score-0.345]
</p><p>88 Second, when moving toward an optimum at inﬁnity, how can the value function / natural gradient (encoded by w = η) stay non-zero and continue to properly represent action values while the correˆ sponding policy gradient π J(θ) must approach zero at the same time? [sent-222, score-1.332]
</p><p>89 We then recall that the curvature of the Gibbs policy class turns into a plateau at inﬁnity, onto which the policy becomes pushed when moving toward a deterministic optimum. [sent-224, score-1.758]
</p><p>90 5  Common ground  Figure 1 shows a map of relevant variants of optimistic policy iteration, parameterized as in (4). [sent-226, score-1.016]
</p><p>91 For a continuously soft-greedy policy class (toward right on the map), convergence can be established with enough softness [18, 14]. [sent-228, score-0.984]
</p><p>92 The main value of Theorem 1 is in bringing the greedy value function and policy gradient methodologies closer to each other. [sent-231, score-1.036]
</p><p>93 5  θ (left) − θ (right)  Figure 1: The hyperparameter space of the general form of (approximate) optimistic policy iteration in (4), with known convergence and optimality properties (see text for assumptions). [sent-250, score-1.141]
</p><p>94 Figure 2: Empirical illustration of the behavior of optimistic policy iteration ((1), (2), (4) and (5), with tabular φ) in the proximity of a stochastic optimum. [sent-264, score-1.144]
</p><p>95 The uniformly stochastic policy is denoted by a dashed red line. [sent-268, score-0.845]
</p><p>96 In this respect, the policy improvement parameter κ in NAC(κ) can be seen (inversely) as a dual in spirit to the policy evaluation parameter λ in TD(λ)-style algorithms. [sent-270, score-1.62]
</p><p>97 On the policy evaluation side, having λ = 0 obtains variance reduction by assuming and exploiting Markovianity of the problem, while λ = 1 obtains unbiased estimates also for non-Markovian problems. [sent-271, score-0.826]
</p><p>98 Approximate policy iteration: A survey and some new methods. [sent-308, score-0.772]
</p><p>99 Least squares policy evaluation algorithms with linear function c approximation. [sent-371, score-0.826]
</p><p>100 A reinterpretation of the policy oscillation phenomenon in approximate policy iteration. [sent-428, score-1.739]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.772), ('wk', 0.296), ('optimistic', 0.186), ('toward', 0.147), ('oscillation', 0.146), ('temperature', 0.134), ('vanishingly', 0.113), ('nac', 0.095), ('bertsekas', 0.093), ('optimum', 0.093), ('gradient', 0.09), ('iteration', 0.083), ('softness', 0.082), ('perkins', 0.075), ('interpolation', 0.075), ('greedy', 0.075), ('methodology', 0.072), ('convergence', 0.07), ('sustained', 0.068), ('reinforcement', 0.065), ('precup', 0.063), ('chattering', 0.062), ('melo', 0.055), ('evaluation', 0.054), ('stochastic', 0.052), ('gibbs', 0.051), ('origin', 0.051), ('markovian', 0.049), ('policies', 0.045), ('forgetting', 0.043), ('limiting', 0.042), ('established', 0.039), ('variants', 0.038), ('wagner', 0.038), ('pathological', 0.035), ('schaal', 0.035), ('inversely', 0.035), ('action', 0.035), ('natural', 0.034), ('permit', 0.034), ('convergent', 0.033), ('methodologies', 0.031), ('markovianity', 0.031), ('pendrith', 0.031), ('undershoot', 0.031), ('optimality', 0.03), ('nity', 0.028), ('observable', 0.028), ('oscillations', 0.028), ('solely', 0.028), ('dynamic', 0.028), ('eval', 0.027), ('aalto', 0.027), ('overshooting', 0.027), ('never', 0.027), ('theorems', 0.027), ('except', 0.026), ('st', 0.026), ('proximity', 0.026), ('sutton', 0.026), ('phenomenon', 0.026), ('peters', 0.026), ('trap', 0.025), ('tabular', 0.025), ('forgetful', 0.025), ('finland', 0.025), ('unifying', 0.025), ('must', 0.025), ('adjusting', 0.025), ('decisions', 0.024), ('value', 0.024), ('interactively', 0.024), ('permits', 0.024), ('approximate', 0.023), ('implication', 0.023), ('compatible', 0.023), ('curvature', 0.023), ('optima', 0.022), ('moving', 0.022), ('improvement', 0.022), ('converge', 0.022), ('deterministic', 0.022), ('reside', 0.021), ('effective', 0.021), ('uniformly', 0.021), ('returns', 0.021), ('continuously', 0.021), ('tsitsiklis', 0.021), ('map', 0.02), ('vanish', 0.02), ('categorized', 0.02), ('trapped', 0.02), ('closer', 0.02), ('issues', 0.02), ('decreasing', 0.02), ('context', 0.019), ('td', 0.019), ('strongest', 0.019), ('carlo', 0.019), ('consequently', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="239-tfidf-1" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>2 0.58250296 <a title="239-tfidf-2" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>3 0.46420044 <a title="239-tfidf-3" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>4 0.36592856 <a title="239-tfidf-4" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>5 0.3634313 <a title="239-tfidf-5" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>Author: Philip S. Thomas, William C. Dabney, Stephen Giguere, Sridhar Mahadevan</p><p>Abstract: Natural actor-critics form a popular class of policy search algorithms for ﬁnding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability—their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural ActorCritics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent. 1</p><p>6 0.35074466 <a title="239-tfidf-6" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>7 0.31039557 <a title="239-tfidf-7" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>8 0.30904114 <a title="239-tfidf-8" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>9 0.27452716 <a title="239-tfidf-9" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>10 0.27122521 <a title="239-tfidf-10" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>11 0.26790538 <a title="239-tfidf-11" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>12 0.2398711 <a title="239-tfidf-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.21611799 <a title="239-tfidf-13" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>14 0.21398129 <a title="239-tfidf-14" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>15 0.20823631 <a title="239-tfidf-15" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>16 0.20629725 <a title="239-tfidf-16" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>17 0.20188348 <a title="239-tfidf-17" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>18 0.18817884 <a title="239-tfidf-18" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>19 0.18405393 <a title="239-tfidf-19" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>20 0.1804584 <a title="239-tfidf-20" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.259), (1, -0.525), (2, -0.261), (3, 0.28), (4, -0.042), (5, 0.069), (6, -0.203), (7, 0.131), (8, 0.099), (9, 0.075), (10, 0.033), (11, 0.036), (12, -0.034), (13, -0.012), (14, 0.096), (15, -0.084), (16, -0.025), (17, -0.059), (18, -0.089), (19, 0.079), (20, 0.061), (21, -0.033), (22, 0.009), (23, 0.008), (24, -0.004), (25, -0.008), (26, 0.031), (27, 0.046), (28, 0.012), (29, -0.025), (30, -0.024), (31, -0.034), (32, 0.051), (33, -0.077), (34, 0.003), (35, 0.025), (36, 0.066), (37, -0.031), (38, 0.029), (39, 0.044), (40, -0.002), (41, -0.027), (42, 0.023), (43, 0.029), (44, 0.017), (45, -0.026), (46, 0.015), (47, 0.033), (48, -0.021), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99024022 <a title="239-lsi-1" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>2 0.96130294 <a title="239-lsi-2" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>3 0.86375302 <a title="239-lsi-3" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>Author: Aswin Raghavan, Roni Khardon, Alan Fern, Prasad Tadepalli</p><p>Abstract: This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions. Our ﬁrst contribution is a symbolic implementation of Modiﬁed Policy Iteration (MPI) for factored actions that views policy evaluation as policy-constrained value iteration (VI). Unfortunately, a na¨ve approach ı to enforce policy constraints can lead to large memory requirements, sometimes making symbolic MPI worse than VI. We address this through our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent algorithm lying between VI and MPI, that applies policy constraints if it does not increase the size of the value function representation, and otherwise performs VI backups. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signiﬁcantly improved scalability over state-of-the-art symbolic planners. 1</p><p>4 0.8627488 <a title="239-lsi-4" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>Author: Philip S. Thomas, William C. Dabney, Stephen Giguere, Sridhar Mahadevan</p><p>Abstract: Natural actor-critics form a popular class of policy search algorithms for ﬁnding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability—their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural ActorCritics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent. 1</p><p>5 0.84689939 <a title="239-lsi-5" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><p>6 0.84597719 <a title="239-lsi-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.80234039 <a title="239-lsi-7" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>8 0.73662251 <a title="239-lsi-8" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>9 0.73252439 <a title="239-lsi-9" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>10 0.68004405 <a title="239-lsi-10" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>11 0.67102212 <a title="239-lsi-11" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>12 0.65144229 <a title="239-lsi-12" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>13 0.63724452 <a title="239-lsi-13" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>14 0.57628709 <a title="239-lsi-14" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>15 0.55583149 <a title="239-lsi-15" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>16 0.5454812 <a title="239-lsi-16" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>17 0.52900994 <a title="239-lsi-17" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>18 0.51438928 <a title="239-lsi-18" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>19 0.46536285 <a title="239-lsi-19" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>20 0.45933872 <a title="239-lsi-20" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.038), (33, 0.158), (34, 0.176), (41, 0.085), (49, 0.022), (56, 0.116), (64, 0.129), (70, 0.029), (85, 0.039), (89, 0.033), (93, 0.037), (95, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95645022 <a title="239-lda-1" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>same-paper 2 0.92333788 <a title="239-lda-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.91058487 <a title="239-lda-3" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>Author: Xiaojin Zhu</p><p>Abstract: What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for ﬁnding the optimal teaching set. Our algorithm optimizes the aggregate sufﬁcient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. 1</p><p>4 0.88303775 <a title="239-lda-4" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>5 0.87863743 <a title="239-lda-5" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>Author: Ryosuke Matsushita, Toshiyuki Tanaka</p><p>Abstract: We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efﬁcient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by reformulating it as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd’s K-means algorithm. 1</p><p>6 0.87842208 <a title="239-lda-6" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>7 0.87172157 <a title="239-lda-7" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>8 0.87042844 <a title="239-lda-8" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>9 0.8692739 <a title="239-lda-9" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>10 0.86912906 <a title="239-lda-10" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>11 0.86791593 <a title="239-lda-11" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>12 0.86749953 <a title="239-lda-12" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>13 0.86706823 <a title="239-lda-13" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>14 0.86538893 <a title="239-lda-14" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>15 0.86410433 <a title="239-lda-15" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>16 0.86361074 <a title="239-lda-16" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>17 0.86359274 <a title="239-lda-17" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>18 0.863226 <a title="239-lda-18" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>19 0.86292863 <a title="239-lda-19" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>20 0.86262387 <a title="239-lda-20" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
