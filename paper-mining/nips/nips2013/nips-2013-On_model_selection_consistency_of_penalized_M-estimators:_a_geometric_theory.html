<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-219" href="#">nips2013-219</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</h1>
<br/><p>Source: <a title="nips-2013-219-pdf" href="http://papers.nips.cc/paper/5175-on-model-selection-consistency-of-penalized-m-estimators-a-geometric-theory.pdf">pdf</a></p><p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>Reference: <a title="nips-2013-219-reference" href="../nips2013_reference/nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On model selection consistency of M-estimators with geometrically decomposable penalties  Jason D. [sent-1, score-1.471]
</p><p>2 can be expressed as a sum of support functions over convex sets. [sent-9, score-0.145]
</p><p>3 We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. [sent-10, score-2.012]
</p><p>4 In machine learning, signal processing, and high-dimensional statistics, this principle motivates the use of sparsity inducing penalties for model selection and signal recovery from incomplete/noisy measurements. [sent-13, score-0.487]
</p><p>5 1) p θ∈R  where (n) is a convex, twice continuously differentiable loss function, ρ is a penalty function, and S ⊆ Rp is a subspace. [sent-15, score-0.111]
</p><p>6 can be expressed as a sum of support functions over convex sets. [sent-18, score-0.145]
</p><p>7 We describe this notion of decomposable in Section 2 and then develop a general framework for analyzing the consistency and model selection consistency of M-estimators with geometrically decomposable penalties. [sent-19, score-1.927]
</p><p>8 When specialized to various statistical models, our framework yields some known and some new model selection consistency results. [sent-20, score-0.382]
</p><p>9 This paper is organized as follows: First, we review existing work on consistency and model selection consistency of penalized M-estimators. [sent-21, score-0.768]
</p><p>10 Then, in Section 2, we describe the notion of geometrically decomposable and give some examples of geometrically decomposable penalties. [sent-22, score-1.747]
</p><p>11 In Section 3, we generalize the notion of irrepresentable to geometrically decomposable penalties and state our main result (Theorem 3. [sent-23, score-1.349]
</p><p>12 We prove our main result in the Supplementary Material and develop a converse result concerning the necessity of the irrepresentable condition in the Supplementary Material. [sent-25, score-0.25]
</p><p>13 In Section 4, we use our main result to derive consistency and model selection consistency results for the generalized lasso (total variation) and maximum likelihood estimation in exponential families. [sent-26, score-1.131]
</p><p>14 1  Consistency of penalized M-estimators  The consistency of penalized M-estimators has been studied extensively. [sent-28, score-0.532]
</p><p>15 The three most wellstudied problems are (i) the lasso [2, 26], (ii) generalized linear models (GLM) with the lasso penalty [10], and (iii) inverse covariance estimators with sparsity inducing penalties (equivalent to sparse maximum likelihood estimation for a Gaussian graphical model) [21, 20]. [sent-29, score-1.3]
</p><p>16 There are also consistency results for M-estimators with group and structured variants of the lasso penalty [1, 7]. [sent-30, score-0.766]
</p><p>17 [17] proposes a uniﬁed framework for establishing consistency and convergence rates for M-estimators with penalties ρ that are decomposable with respect to a pair of subspaces ¯ M, M : ¯ ρ(x + y) = ρ(x) + ρ(y), for all x ∈ M, y ∈ M ⊥ . [sent-32, score-0.928]
</p><p>18 Many commonly used penalties such as the lasso, group lasso, and nuclear norm are decomposable in this sense. [sent-33, score-0.796]
</p><p>19 prove a general result that establishes the consistency of M-estimators with decomposable penalties. [sent-35, score-0.641]
</p><p>20 Using their framework, they derive consistency results for special cases like sparse and group sparse regression. [sent-36, score-0.318]
</p><p>21 [17], but we focus on establishing the more stringent result of model selection consistency rather than consistency. [sent-38, score-0.423]
</p><p>22 The model selection consistency of penalized M-estimators has also been extensively studied. [sent-40, score-0.528]
</p><p>23 The most commonly studied problems are (i) the lasso [30, 26], (ii) GLM’s with the lasso penalty [4, 19, 28], (iii) covariance estimation [15, 12, 20] and (more generally) structure learning [6, 14]. [sent-41, score-0.833]
</p><p>24 There are also general results concerning M-estimators with sparsity inducing penalties [29, 16, 11, 22, 8, 18, 24]. [sent-42, score-0.345]
</p><p>25 Despite the extensive work on model selection consistency, to our knowledge, this is the ﬁrst work to establish a general framework for model selection consistency for penalized M-estimators. [sent-43, score-0.691]
</p><p>26 2  Geometrically decomposable penalties  Let C ⊂ Rp be a closed convex set. [sent-44, score-0.689]
</p><p>27 The support function is a supremum of linear functions, hence the subdifferential consists of the linear functions that attain the supremum: ∂hC (x) = {y ∈ C | y T x = hC (x)}. [sent-51, score-0.102]
</p><p>28 We use this property to express penalty functions as sums of support functions. [sent-55, score-0.246]
</p><p>29 if ρ is a norm and the dual norm ball can be expressed as a (Minkowski) sum of convex sets C1 , . [sent-58, score-0.286]
</p><p>30 If a penalty ρ can be expressed as ρ(θ) = hA (θ) + hI (θ) + hS ⊥ (θ), (2. [sent-62, score-0.139]
</p><p>31 2) where A and I are closed convex sets and S is a subspace, then we say ρ is a geometrically decomposable penalty. [sent-63, score-0.908]
</p><p>32 , Ck to express ρ in geometrically decomposable form (2. [sent-69, score-0.87]
</p><p>33 In many cases of interest, A + I is a norm ball and hA+I = hA + hI is the dual norm. [sent-71, score-0.122]
</p><p>34 In our analysis, we assume 1 Given the extensive work on consistency of penalized M-estimators, our review and referencing is necessarily incomplete. [sent-72, score-0.386]
</p><p>35 if we know the sparsity pattern of the unknown parameter vector, then A should span the subspace of all vectors with the correct sparsity pattern. [sent-84, score-0.229]
</p><p>36 The third term enforces a subspace constraint θ ∈ S because the support function of a subspace is the (convex) indicator function of the orthogonal complement: 0 x∈S ∞ otherwise. [sent-85, score-0.211]
</p><p>37 We give three examples of penalized M-estimators with geometrically decomposable penalties, i. [sent-91, score-0.989]
</p><p>38 We also compare our notion of geometrically decomposable to two other notions of decomposable penalties by Negahban et al. [sent-95, score-1.582]
</p><p>39 1  The lasso and group lasso penalties  Two geometrically decomposable penalties are the lasso and group lasso penalties. [sent-98, score-2.839]
</p><p>40 We can decompose the lasso penalty component-wise to obtain θ 1 = hB∞,A (θ) + hB∞,I (θ), where hB∞,A and hB∞,I are support functions of the sets B∞,A = θ ∈ Rp | θ  ∞  ≤ 1 and θI = 0  ∞  ≤ 1 and θA = 0 . [sent-103, score-0.546]
</p><p>41 p  B∞,I = θ ∈ R | θ  If the groups do not overlap, then we can also decompose the group lasso penalty group-wise (A and I are now sets of groups) to obtain θg  2  = hB(2,∞),A (θ) + hB(2,∞),I (θ). [sent-104, score-0.585]
</p><p>42 g∈G  hB(2,∞),A and hB(2,∞),I are support functions of the sets B(2,∞),A = θ ∈ Rp | max θg  2  ≤ 1 and θg = 0, g ∈ A  B(2,∞),I = θ ∈ Rp | max θg  2  ≤ 1 and θg = 0, g ∈ I . [sent-105, score-0.098]
</p><p>43 2  The generalized lasso penalty  Another geometrically decomposable penalty is the generalized lasso penalty [23]. [sent-108, score-2.004]
</p><p>44 We can express the generalized lasso penalty in decomposable form: Dθ  1  = hDT B∞,A (θ) + hDT B∞,I (θ). [sent-113, score-0.953]
</p><p>45 4)  hDT B∞,A and hDT B∞,I are support functions of the sets T DT B∞,A = {x ∈ Rp | x = DA y, y T  p  D B∞,I = {x ∈ R | x =  T DI y,  y  ∞  ≤ 1}  (2. [sent-115, score-0.098]
</p><p>46 6)  We can also formulate any generalized lasso penalized M-estimator as a linearly constrained, lasso penalized M-estimator. [sent-118, score-1.043]
</p><p>47 After a change of variables, a generalized lasso penalized M-estimator is equivalent to minimize (n) (D† θ + γ) + λ θ 1 , subject to γ ∈ N (D), θ∈Rk ,γ∈Rp  where N (D) is the nullspace of D. [sent-119, score-0.589]
</p><p>48 The lasso penalty can then be decomposed component-wise to obtain θ 1 = hB∞,A (θ) + hB∞,I (θ). [sent-120, score-0.448]
</p><p>49 We enforce the subspace constraint θ ∈ N (D) with the support function of R(D)⊥ . [sent-121, score-0.156]
</p><p>50 There are many interesting applications of the generalized lasso in signal processing and statistical learning. [sent-123, score-0.414]
</p><p>51 3  “Hybrid” penalties  A large class of geometrically decomposable penalties are so-called “hybrid” penalties: inﬁmal convolutions of penalties to promote solutions that are sums of simple components, e. [sent-126, score-1.642]
</p><p>52 If the constituent simple penalties are geometrically decomposable, then the resulting hybrid penalty is also geometrically decomposable. [sent-129, score-1.291]
</p><p>53 For example, let ρ1 and ρ2 be geometrically decomposable penalties, i. [sent-130, score-0.843]
</p><p>54 7)  This is an M-estimator with a geometrically decomposable penalty: minimize 2p θ∈R  (n)  (θ1 + θ2 ) + λ(hA (θ) + hI (θ) + hS ⊥ (θ)). [sent-134, score-0.872]
</p><p>55 hA , hI and hS ⊥ are support functions of the sets A = {(θ1 , θ2 ) | θ1 ∈ A1 ⊂ Rp , θ2 ∈ A2 ⊂ Rp } I = {(θ1 , θ2 ) | θ1 ∈ I1 ⊂ Rp , θ2 ∈ I2 ⊂ Rp } S = {(θ1 , θ2 ) | θ1 ∈ S1 ⊂ Rp , θ2 ∈ S2 ⊂ Rp }. [sent-135, score-0.098]
</p><p>56 There are many interesting applications of the hybrid penalties in signal processing and statistical 2 learning. [sent-136, score-0.296]
</p><p>57 Two examples are the huber function, ρ(θ) = inf θ=γ1 +γ2 γ1 1 + γ2 2 , and the multitask group regularizer, ρ(Θ) = inf Θ=B+S B 1,∞ + S 1 . [sent-137, score-0.176]
</p><p>58 See [27] for recent work on model selection consistency in hybrid penalties. [sent-138, score-0.432]
</p><p>59 3  Main result  We assume the unknown parameter vector θ is contained in the model subspace M := span(I)⊥ ∩ S, 4  (3. [sent-139, score-0.123]
</p><p>60 x  Further, we use κ(ρ) to denote the compatibility constant between a semi-norm ρ and the over the model subspace: κ(ρ) := sup {ρ(x) | x x  Finally, we choose a norm · the penalty. [sent-143, score-0.156]
</p><p>61 This norm is usually the dual norm to  ε  Before we state our main result, we state our assumptions on the problem. [sent-146, score-0.168]
</p><p>62 This assumption requires the loss function to be curved along certain directions in the model subspace and is very similar to Negahban et al. [sent-156, score-0.101]
</p><p>63 ’s notion of restricted strong convexity [17] and Buhlmann and van de Geer’s notion of compatibility [3]. [sent-157, score-0.212]
</p><p>64 To obtain model selection consistency results, we must ﬁrst generalize the key notion of irrepresentable to geometrically decomposable penalties. [sent-164, score-1.485]
</p><p>65 Hence the irrepresentable condition requires any z ∈ M ⊥ to be decomposable into uI + uS ⊥ , where uI ∈ relint(I) and uS ⊥ ∈ S ⊥ . [sent-171, score-0.628]
</p><p>66 Intuitively, τ quantiﬁes how ¯ large the irrepresentable term can be compared to the error norm. [sent-181, score-0.199]
</p><p>67 The irrepresentable condition is a standard assumption for model selection consistency and has been shown to be almost necessary for sign consistency of the lasso [30, 26]. [sent-182, score-1.186]
</p><p>68 Intuitively, the irrepresentable condition requires the active predictors to be not overly dependent on the inactive predictors. [sent-183, score-0.303]
</p><p>69 In Supplementary Material, we show our (generalized) irrepresentable condition is also necessary for model selection consistency with some geometrically decomposable penalties. [sent-184, score-1.452]
</p><p>70 If we select λ such that λ> and λ < min  2¯ τ τ  (n)  (θ )   2 m  τ L 2¯κ( · )(2κ(hA )+ τ κ( · τ ε τ ¯ mr  2κ(hA )+ τ κ( · ∗ ) , ε τ ¯  then the penalized M-estimator is unique, consistent (in the i. [sent-190, score-0.227]
</p><p>71 ˆ θ−θ  ≤ 2  2 m  κ(hA ) +  ε  τ 2¯ κ( τ  2  2 ∗ ε)  )  norm), and model selection consistent,  ∗  · ε ) λ,  ˆ 2. [sent-194, score-0.142]
</p><p>72 To use this result to derive consistency and model selection consistency results for a statistical model, we must ﬁrst verify Assumptions (3. [sent-200, score-0.622]
</p><p>73 Then, we must choose an error norm · ε and select λ such that λ> and λ < min  2¯ τ τ  (n)   2 m  (θ )  ε  τ L 2¯κ( · )(2κ(hA )+ τ κ( · τ ε τ ¯ mr  ∗ τ 2κ(hA )+ τ κ( · ε ) ¯  2 ∗ ε)  )  with high probability. [sent-203, score-0.118]
</p><p>74 In Section 4, we use this theorem to derive consistency and model selection consistency results for the generalized lasso and penalized likelihood estimation for exponential families. [sent-204, score-1.277]
</p><p>75 4 to establish the consistency and model selection consistency of the generalized lasso and a group lasso penalized maximum likelihood estimator in the high-dimensional setting. [sent-206, score-1.684]
</p><p>76 1  The generalized lasso  Consider the linear model y = X T θ + , where X ∈ Rn×p is the design matrix, and θ ∈ √ p R are unknown regression parameters. [sent-211, score-0.456]
</p><p>77 6  We seek an estimate of θ with the generalized lasso: minimize p θ∈R  1 y − Xθ 2n  2 2  + λ Dθ  1  ,  (4. [sent-217, score-0.128]
</p><p>78 The generalized lasso penalty is geometrically decomposable: Dθ  1  = hDT B∞,A (θ) + hDT B∞,I (θ). [sent-219, score-0.967]
</p><p>79 hDT B∞,A and hDT B∞,I are support functions of the sets DT B∞,A = {x ∈ Rp | x = DT y, yI = 0, y T  p  ∞  ≤ 1}  ∞  ≤ 1}. [sent-220, score-0.098]
</p><p>80 The model subspace is the set T span(DT B∞,I )⊥ = R(DI )⊥ = N (DI ),  where I is a subset of the row indices of D. [sent-224, score-0.101]
</p><p>81 The compatibility constants κ( 1 ), κ(hA ) are κ( 1 ) = sup { x x  κ(hA ) = sup x  √ ¯ If we select λ > 2 2σ τ τ  1  | x  2  ≤ 1, x ∈ N (DI )}  hDT B∞,A (x) | x  log p n ,  2  ≤ 1, x ∈ M ≤ DA  then there exists c such that Pr λ ≥  2¯ τ τ  2  |A|. [sent-225, score-0.116]
</p><p>82 4 are satisﬁed with probability at least 1 − 2 exp(−cλ2 n), and we deduce the generalized lasso is consistent and model selection consistent. [sent-228, score-0.59]
</p><p>83 If we select λ > √ ¯ 2 2σ τ log p then, with probability at least 1 − 2 exp −cλ2 n , the solution to the generalized τ n lasso is unique, consistent, and model selection consistent, i. [sent-235, score-0.576]
</p><p>84 We seek a maximum likelihood estimate (MLE) of the unknown parameters: minimize p θ∈R  where  (n) ML  (n) ML (θ)  +λ θ  2,1  , subject to θ ∈ S. [sent-250, score-0.108]
</p><p>85 2)  and θ  2,1  is the group lasso penalty θ  2,1  =  θg  2  . [sent-252, score-0.526]
</p><p>86 g∈G  It is also straightforward to change the maximum likelihood estimator to the more computationally tractable pseudolikelihood estimator [13, 6], the neighborhood selection procedure [15], and include covariates [13]. [sent-253, score-0.257]
</p><p>87 Thus estimating the parameters of exponential families is equivalent to learning undirected graphical models, a problem of interest in many application areas such as bioinformatics. [sent-256, score-0.117]
</p><p>88 Conclusion  We proposed the notion of geometrically decomposable and generalized the irrepresentable condition to geometrically decomposable penalties. [sent-275, score-2.051]
</p><p>89 This notion of decomposability also allows us to enforce linear constraints. [sent-278, score-0.12]
</p><p>90 We developed a general framework for establishing the model selection consistency of M-estimators with geometrically decomposable penalties. [sent-279, score-1.266]
</p><p>91 Our main result gives deterministic conditions on the problem that guarantee consistency and model selection consistency; in this sense, it extends the work of [17] from estimation consistency to model selection consistency. [sent-280, score-0.784]
</p><p>92 We combine our main result with probabilistic analysis to establish the consistency and model selection consistency of the generalized lasso and group lasso penalized maximum likelihood estimators. [sent-281, score-1.653]
</p><p>93 Consistency of the group lasso and multiple kernel learning. [sent-292, score-0.415]
</p><p>94 Honest variable selection in linear and logistic regression models via 1 and 1 + 2 penalization. [sent-315, score-0.122]
</p><p>95 Asymptotic properties of the joint neighborhood selection method for estimating categorical markov networks. [sent-333, score-0.16]
</p><p>96 Learning exponential families in high-dimensions: Strong convexity and sparsity. [sent-371, score-0.098]
</p><p>97 On the asymptotic properties of the group lasso estimator for linear models. [sent-420, score-0.446]
</p><p>98 A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. [sent-432, score-0.401]
</p><p>99 High-dimensional ising model selection using 1 regularized logistic regression. [sent-452, score-0.142]
</p><p>100 Model selection and estimation in regression with grouped variables. [sent-537, score-0.142]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('geometrically', 0.442), ('decomposable', 0.401), ('lasso', 0.337), ('penalties', 0.246), ('consistency', 0.24), ('hb', 0.207), ('hdt', 0.203), ('irrepresentable', 0.199), ('ha', 0.184), ('penalized', 0.146), ('rp', 0.125), ('selection', 0.122), ('pm', 0.115), ('penalty', 0.111), ('hc', 0.098), ('negahban', 0.093), ('subspace', 0.081), ('group', 0.078), ('generalized', 0.077), ('maxg', 0.073), ('norm', 0.071), ('relint', 0.068), ('notion', 0.061), ('hs', 0.058), ('span', 0.058), ('ravikumar', 0.052), ('hybrid', 0.05), ('inf', 0.049), ('support', 0.049), ('convex', 0.042), ('da', 0.042), ('inducing', 0.042), ('establishing', 0.041), ('ui', 0.041), ('exponential', 0.04), ('minkowski', 0.04), ('br', 0.039), ('neighborhood', 0.038), ('hi', 0.038), ('hck', 0.037), ('groups', 0.036), ('likelihood', 0.035), ('di', 0.035), ('consistent', 0.034), ('dt', 0.034), ('compatibility', 0.034), ('sparsity', 0.034), ('convexity', 0.034), ('wainwright', 0.033), ('levina', 0.033), ('geer', 0.033), ('decomposability', 0.033), ('graphical', 0.033), ('sums', 0.033), ('ml', 0.032), ('sup', 0.031), ('estimator', 0.031), ('notions', 0.031), ('mal', 0.03), ('minimize', 0.029), ('expressed', 0.028), ('glm', 0.028), ('overly', 0.028), ('bickel', 0.028), ('promote', 0.028), ('pr', 0.028), ('covariance', 0.028), ('condition', 0.028), ('supremum', 0.027), ('mr', 0.027), ('express', 0.027), ('inactive', 0.027), ('dual', 0.026), ('functions', 0.026), ('enforce', 0.026), ('arxiv', 0.025), ('obozinski', 0.025), ('ball', 0.025), ('overlap', 0.025), ('families', 0.024), ('sublinear', 0.024), ('supplementary', 0.023), ('recovery', 0.023), ('sets', 0.023), ('concerning', 0.023), ('unknown', 0.022), ('van', 0.022), ('seek', 0.022), ('graduate', 0.021), ('establish', 0.021), ('origin', 0.021), ('lee', 0.021), ('predictors', 0.021), ('cand', 0.021), ('estimation', 0.02), ('select', 0.02), ('model', 0.02), ('areas', 0.02), ('engineering', 0.02), ('kolar', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="219-tfidf-1" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>2 0.22770503 <a title="219-tfidf-2" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar</p><p>Abstract: We provide a uniﬁed framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M -estimators that minimize the sum of any loss function, and an instance of what we call a “hybrid” regularization, that is the inﬁmal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures. 1</p><p>3 0.21542297 <a title="219-tfidf-3" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>4 0.19208738 <a title="219-tfidf-4" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>5 0.17750818 <a title="219-tfidf-5" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>6 0.17476028 <a title="219-tfidf-6" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>7 0.15985127 <a title="219-tfidf-7" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>8 0.12127583 <a title="219-tfidf-8" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>9 0.11734093 <a title="219-tfidf-9" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>10 0.087314546 <a title="219-tfidf-10" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>11 0.087230109 <a title="219-tfidf-11" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>12 0.085521564 <a title="219-tfidf-12" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>13 0.077356711 <a title="219-tfidf-13" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>14 0.076870002 <a title="219-tfidf-14" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>15 0.07091096 <a title="219-tfidf-15" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>16 0.063443936 <a title="219-tfidf-16" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>17 0.061534759 <a title="219-tfidf-17" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>18 0.060286183 <a title="219-tfidf-18" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>19 0.059891045 <a title="219-tfidf-19" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>20 0.058946032 <a title="219-tfidf-20" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.067), (2, 0.115), (3, 0.09), (4, -0.035), (5, 0.084), (6, -0.072), (7, 0.037), (8, -0.196), (9, -0.013), (10, 0.125), (11, -0.014), (12, -0.104), (13, -0.18), (14, -0.159), (15, -0.1), (16, 0.116), (17, -0.1), (18, 0.027), (19, -0.094), (20, 0.037), (21, 0.076), (22, -0.037), (23, 0.06), (24, 0.077), (25, 0.065), (26, -0.026), (27, 0.008), (28, -0.02), (29, 0.108), (30, 0.056), (31, -0.036), (32, 0.075), (33, -0.106), (34, -0.058), (35, 0.121), (36, -0.007), (37, -0.004), (38, -0.004), (39, -0.076), (40, -0.029), (41, 0.08), (42, -0.021), (43, 0.008), (44, 0.012), (45, -0.025), (46, -0.021), (47, 0.059), (48, 0.048), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97373593 <a title="219-lsi-1" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>2 0.8562237 <a title="219-lsi-2" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>3 0.85382229 <a title="219-lsi-3" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>4 0.82734269 <a title="219-lsi-4" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>5 0.72792625 <a title="219-lsi-5" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>6 0.66521078 <a title="219-lsi-6" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>7 0.63400984 <a title="219-lsi-7" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>8 0.60932344 <a title="219-lsi-8" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>9 0.59623748 <a title="219-lsi-9" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>10 0.56389463 <a title="219-lsi-10" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>11 0.56117254 <a title="219-lsi-11" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>12 0.534733 <a title="219-lsi-12" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>13 0.47853425 <a title="219-lsi-13" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>14 0.46129671 <a title="219-lsi-14" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>15 0.41009456 <a title="219-lsi-15" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>16 0.40936515 <a title="219-lsi-16" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>17 0.39851755 <a title="219-lsi-17" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>18 0.39312381 <a title="219-lsi-18" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>19 0.36887342 <a title="219-lsi-19" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<p>20 0.3463327 <a title="219-lsi-20" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.022), (19, 0.011), (33, 0.14), (34, 0.518), (49, 0.021), (56, 0.085), (70, 0.018), (85, 0.013), (89, 0.051), (93, 0.016), (95, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98638111 <a title="219-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.98308736 <a title="219-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.97532272 <a title="219-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.97509265 <a title="219-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.96864033 <a title="219-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.96540672 <a title="219-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.95553988 <a title="219-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.95094246 <a title="219-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>same-paper 9 0.94692546 <a title="219-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.93733716 <a title="219-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.84213042 <a title="219-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.82703054 <a title="219-lda-12" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>13 0.82284379 <a title="219-lda-13" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>14 0.79891288 <a title="219-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.79743379 <a title="219-lda-15" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>16 0.79156232 <a title="219-lda-16" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>17 0.79144418 <a title="219-lda-17" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>18 0.78976542 <a title="219-lda-18" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<p>19 0.78927594 <a title="219-lda-19" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>20 0.78407466 <a title="219-lda-20" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
