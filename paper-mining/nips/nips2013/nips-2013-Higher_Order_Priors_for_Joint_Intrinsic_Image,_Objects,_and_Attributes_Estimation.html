<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-138" href="#">nips2013-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</h1>
<br/><p>Source: <a title="nips-2013-138-pdf" href="http://papers.nips.cc/paper/5198-higher-order-priors-for-joint-intrinsic-image-objects-and-attributes-estimation.pdf">pdf</a></p><p>Author: Vibhav Vineet, Carsten Rother, Philip Torr</p><p>Abstract: Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reﬂectance and illumination from a single image, and object class segmentation separately. While these two problems are mutually informative, in the past not many papers have addressed this topic. In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. In this way, our uniﬁed framework is able to capture the correlations between intrinsic properties (reﬂectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. For example, our model is able to enforce the condition that if a set of pixels take same object label, e.g. table, most likely those pixels would receive similar reﬂectance values. We cast the problem in an energy minimization framework and demonstrate the qualitative and quantitative improvement in the overall accuracy on the NYU and Pascal datasets. 1</p><p>Reference: <a title="nips-2013-138-reference" href="../nips2013_reference/nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reﬂectance and illumination from a single image, and object class segmentation separately. [sent-11, score-0.805]
</p><p>2 In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. [sent-13, score-0.658]
</p><p>3 In this way, our uniﬁed framework is able to capture the correlations between intrinsic properties (reﬂectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. [sent-14, score-0.382]
</p><p>4 For example, our model is able to enforce the condition that if a set of pixels take same object label, e. [sent-15, score-0.275]
</p><p>5 1  Introduction  Recovering scene properties (shape, illumination, reﬂectance) that led to the generation of an image has been one of the fundamental problems in computer vision. [sent-19, score-0.201]
</p><p>6 Over the years, many decomposition methods have been proposed [5, 16, 17], but most of them focussed on recovering a reﬂectance image and a shading1 image without explicitly modelling illumination or shape. [sent-21, score-0.372]
</p><p>7 But in the recent years a breakthrough in the research on intrinsic images came with the works of Barron and Malik [1-4] who presented an algorithm that jointly estimated the reﬂectance, the illumination and the shape. [sent-22, score-0.456]
</p><p>8 Further, recognition of objects and their material attributes is central to our understanding of the world. [sent-24, score-0.322]
</p><p>9 A great deal of work has been devoted to estimating the objects and their attributes in the scene: Shotton et. [sent-25, score-0.318]
</p><p>10 [9] propose approaches to estimate the object labels at the pixel level. [sent-29, score-0.268]
</p><p>11 [23] deﬁne and estimate the attributes at the pixel, object and scene levels. [sent-34, score-0.453]
</p><p>12 Some of these attributes are material properties such as woollen, metallic, shiny, and some are structural properties such as rectangular, spherical. [sent-35, score-0.301]
</p><p>13 While these methods for estimating the intrinsic images, objects and attributes have separately been successful in generating good results on laboratory and real-world datasets, they fail to capture the strong correlation existing between these properties. [sent-36, score-0.519]
</p><p>14 Knowledge about the objects and attributes in the image can provide strong prior information about the intrinsic properties. [sent-37, score-0.574]
</p><p>15 For example, if a set of pixels takes the same object label, e. [sent-38, score-0.242]
</p><p>16 Thus recovering the objects and their attributes can help reduce the ambiguities present in the world leading to better estimation of the reﬂectance and other intrinsic properties. [sent-41, score-0.526]
</p><p>17 1 shading is the product of some shape and some illumination model which includes effects such as shadows, indirect lighting etc. [sent-42, score-0.449]
</p><p>18 Additionally such a decomposition might be useful for per-pixel object and attribute segmentation tasks. [sent-44, score-0.545]
</p><p>19 For example, using reﬂectance (illumination invariant) should improve the results-when estimating per-pixel object and attribute labels [24]. [sent-45, score-0.533]
</p><p>20 Moreover if a set of pixels have similar reﬂectance values, they are more likely to have the same object and attribute class. [sent-46, score-0.513]
</p><p>21 Some of the previous research has looked at the correlation of objects and intrinsic properties by propagating results from one step to the next. [sent-47, score-0.351]
</p><p>22 [14] use knowledge of the objects present in the scene to better separate the illumination from the reﬂectance images. [sent-58, score-0.361]
</p><p>23 Joint estimation of the intrinsic images, objects and attributes can be used to overcome these issues. [sent-60, score-0.493]
</p><p>24 For instance, in the context of joint object recognition and depth estimation such positive synergy effects have been shown in e. [sent-61, score-0.435]
</p><p>25 In this work, our main contribution is to explore such synergy effects existing between the intrinsic properties, objects and material attributes present in a scene (see Fig. [sent-64, score-0.635]
</p><p>26 Given an image, our algorithm jointly estimates the intrinsic properties such as reﬂectance, shading and depth maps, along with per-pixel object and attribute labels. [sent-66, score-1.108]
</p><p>27 We demonstrate the potential of our approach on the aNYU and aPascal datasets, which are extended versions of the NYU [25] and Pascal [26] datasets with perpixel attribute labels. [sent-69, score-0.271]
</p><p>28 We evaluate both the qualitative and quantitative improvements for the object and attribute labelling, and qualitative improvement for the intrinsic images estimation. [sent-70, score-0.902]
</p><p>29 2  Problem Formulation  Our goal is to jointly estimate the intrinsic properties of the image, i. [sent-75, score-0.27]
</p><p>30 reﬂectance, shape and illumination, along with estimating the objects and attributes at the pixel level, given an image ¯ ¯ ¯ ¯ array C = (C1 . [sent-77, score-0.491]
</p><p>31 We ﬁrst brieﬂy describe the SIRFS (shape, illumination and reﬂectance from shading) model [2] for estimating the intrinsic properties for a single given object, and then a CRF model for estimating objects, and attributes [12]. [sent-85, score-0.658]
</p><p>32 1 SIRFS model for a single, given object mask We build on the SIRFS model [2] for estimating the intrinsic properties of an image. [sent-87, score-0.461]
</p><p>33 They formulate the problem of recovering the shape, illumination and reﬂectance as an energy minimization problem given an image. [sent-88, score-0.252]
</p><p>34 ZV ) be the reﬂectance, and depth maps respectively, where Ri ∈ R3 and Zi ∈ R3 , and the illumination L be a 27-dimensional vector of spherical harmonics [10]. [sent-95, score-0.375]
</p><p>35 Further, let S(Z, L) be a function that generates a shading image given the depth map Z and the illumination L. [sent-96, score-0.634]
</p><p>36 The SIRFS model then minimizes the energy minimizeR,Z,L E SIRFS subject to  = E R (R) + E Z (Z) + E L (L) ¯ C = R · S(Z, L) 2  (1)  where ”·” represents componentwise multiplication, and E R (R), E Z (Z) and E L (L) are the costs for the reﬂectance, depth and illumination respectively. [sent-100, score-0.383]
</p><p>37 The SIRFS model is limited to estimating the intrinsic properties for a single object mask within an image. [sent-102, score-0.461]
</p><p>38 The recently proposed Scene-SIRFS model [4] proposes an approach to recover the intrinsic properties of whole image by embedding a mixture of shapes in a soft segmentation of the scene. [sent-103, score-0.408]
</p><p>39 The main difference to Scene-SIRFS is that we perform joint optimization over the object (and attributes) labelling and intrinsic image properties per-pixel. [sent-106, score-0.639]
</p><p>40 2 Multilabel Object and Attribute Model The problem of estimating the per-pixel objects and attributes labels can also be formulated in a CRF framework [12]. [sent-108, score-0.363]
</p><p>41 AV ) be the object and attribute variables associated with all V pixels, where each object variable Oi takes one out of K discrete labels such as table, monitor, or ﬂoor. [sent-115, score-0.698]
</p><p>42 Each attribute variable Ai takes a label from the power set of the M attribute labels, for example the subset of attribute labels can be Ai = {red, shiny, wet}. [sent-116, score-0.858]
</p><p>43 Efﬁcient inference is performed by ﬁrst representing each attributes subset Ai by M binary attribute variables Am ∈ i {0, 1}, meaning that Am = 1 if the ith pixel takes the mth attribute and it is absent when Am = 0. [sent-117, score-0.759]
</p><p>44 i i Under this assumption, the most likely solution for the objects and the attributes correspond to minimizing the following energy function E OA (O, A) =  ψi,m (Am )+ i  ψi (Oi ) + m i∈V  i∈V  ψij (Am , Am ) (2) i j  ψij (Oi , Oj )+ i < 1 is a constant. [sent-118, score-0.334]
</p><p>45 This term allows variables within a segment to take different reﬂectance c values if the pixels in that segment take different object labels. [sent-119, score-0.314]
</p><p>46 Similarly we enforce higher order consistency over the object labeling in a clique c ∈ S. [sent-121, score-0.224]
</p><p>47 Essentially, this quality measurement allows the pixels within a segment to take different object labels, if the variation in the reﬂectance terms within the segment is above a threshold. [sent-128, score-0.314]
</p><p>48 To summarize, these two higher order terms enforce the cost of inconsistency within the object and reﬂectance labels. [sent-129, score-0.224]
</p><p>49 3 Reﬂectance, Attributes term Similarly we deﬁne the term E RA (R, A) which enforces a higher order consistency between reﬂectance and attribute variables. [sent-131, score-0.271]
</p><p>50 Such higher order consistency takes the following form: E RA (R, A) =  c πa,m ψ(Rc ) + m  c∈S  c πr ψ(Am ) c  (10)  c∈S  c c where πa,m ψ(Rc ) and πr ψ(Am ) are the higher order terms deﬁned over the reﬂectance image and c the attribute image corresponding to the mth attribute respectively. [sent-132, score-0.704]
</p><p>51 We ﬁrst introduce a set of duplicate variables for the reﬂectance (R1 , R2 , R3 ), objects (O1 , O2 ), and attributes (A1 , A2 ) and a set of new equality constraints to enforce the consistency on these duplicate variables. [sent-136, score-0.369]
</p><p>52 The dual variable dependent terms add θo O1 to the object unary potential ψi (O1 ) and θa A1 to the attribute unary potential ψi (A1 ). [sent-145, score-0.59]
</p><p>53 Let ψ (O1 ) and ψ (A1 ) be the updated object and attribute unary potentials. [sent-146, score-0.509]
</p><p>54 2, Q takes the form as Qi (Oi , Ai ) = QO (Oi ) m QA (Ai 1 ), where QO is a multi-class m i i,m i A distribution over the object variable, and Qi,m is a binary distribution over {0,1}. [sent-152, score-0.191]
</p><p>55 With this, the mean-ﬁeld updates for the object variables take the following form: 1 QO (Oi = l) = i  1 1 exp{−ψi (Oi ) − O Zi l ∈1. [sent-153, score-0.191]
</p><p>56 The updates for the attribute variables also take similar form (refer to the supplementary material). [sent-158, score-0.271]
</p><p>57 8) and dual variable dependent terms, we follow a simple co-ordinate descent strategy to update the reﬂectance and the object (and attribute) variables iteratively. [sent-161, score-0.246]
</p><p>58 The unary potentials for the objects and attributes are learnt using a modiﬁed TextonBoost model of Ladicky et. [sent-176, score-0.339]
</p><p>59 5  Experiments  We demonstrate our joint estimation approach on both the per-pixel object and attribute labelling tasks, and estimation of the intrinsic properties of the images. [sent-179, score-0.829]
</p><p>60 For the object and attribute labelling tasks, we conduct experiments on the NYU 2 [25] and Pascal [26] datasets both quantitatively and qualitatively. [sent-180, score-0.54]
</p><p>61 To this end, we annotate the NYU 2 and the Pascal datasets with per-pixel attribute labels. [sent-181, score-0.297]
</p><p>62 Additionally we also evaluate our approach in estimating better intrinsic properties of the images though qualitatively only, since it is extremely difﬁcult to generate the ground truths for the intrinsic properties, e. [sent-184, score-0.523]
</p><p>63 We compare our intrinsic properties results against the model of Barron and Malik2 [2, 4], Gehler et. [sent-187, score-0.244]
</p><p>64 Further, only visually we also show how our approach is able to recover better smooth and de-noised depth maps compared to the raw depth provided by the Kinect [25]. [sent-190, score-0.362]
</p><p>65 Further, the dataset consists of per-pixel object and attribute labels (see Fig. [sent-196, score-0.536]
</p><p>66 We select 15 object and 8 attribute classes that have sufﬁcient number of instances to train the unary classiﬁer responses. [sent-198, score-0.509]
</p><p>67 The object labels corresponds to some indoor object classes as ﬂoor, wall, . [sent-199, score-0.454]
</p><p>68 and attribute labels corresponds to material properties of the objects as wooden, painted, . [sent-201, score-0.496]
</p><p>69 Further, since this dataset has depth from the Kinect depths, we use them to initialize the depth maps Z for both our joint estimation approach and the Barron and Malik models [2-4]. [sent-205, score-0.436]
</p><p>70 5 % improvement in the 2 We extended the SIRFS [2] model to our Scene-SIRFS using a mixture of reﬂectance and depth maps, and a single illumination model. [sent-212, score-0.374]
</p><p>71 These mixtures of reﬂectance and depth maps were embedded in the soft segmentation of the scene generated using the approach of Felzenswalb et. [sent-213, score-0.358]
</p><p>72 25  (b) Attribute Accuracy  Table 1: Quantitative results on aNYU 2 dataset for both the object segmentation (a), and attributes segmentation (b) tasks. [sent-231, score-0.571]
</p><p>73 The importance of our joint estimation for intrinsic images, objects and attributes is conﬁrmed by the better performance of our algorithm compared to the graph-cuts based (AHCRF) method [9] and mean-ﬁeld based approach [12] for both the tasks. [sent-233, score-0.538]
</p><p>74 average I/U over the model of [12] for the object class segmentation . [sent-240, score-0.274]
</p><p>75 6 % in the overall accuracy and average I/U over the model of [12] for the per-pixel attribute labelling task. [sent-245, score-0.349]
</p><p>76 These quantitative improvement suggests that our model is able to improve the object and attribute labelling using the intrinsic properties information. [sent-246, score-0.857]
</p><p>77 Qualitatively also we observe an improvement in the output of both the object and attribute segmentation tasks as shown in Fig. [sent-247, score-0.578]
</p><p>78 Further, we show the qualitative improvement in the results of the intrinsic properties in the Fig. [sent-249, score-0.334]
</p><p>79 As shown our joint approach helps to recover better depth map compared to the noisy kinect depth maps; justifying the uniﬁcation of reconstruction and objects and attributes based recognition tasks. [sent-251, score-0.724]
</p><p>80 Further, our reﬂectance and shading images visually look much better than the models of Retinex [17] and Gehler et. [sent-252, score-0.264]
</p><p>81 2 aPascal dataset We also show experiments on aPascal dataset, our extended Pascal dataset with per-pixel attribute labels. [sent-256, score-0.329]
</p><p>82 We select a subset of 517 images with the per-pixel object labels from the Pascal dataset and annotate it with 7 material attribute labels at the pixel level. [sent-257, score-0.721]
</p><p>83 Further for the Pascal dataset we do not have any initial depth estimate. [sent-262, score-0.193]
</p><p>84 Thus, we start with a depth map where each point in the space is given same constant depth value. [sent-263, score-0.328]
</p><p>85 7  (b) Attribute Accuracy  Table 2: Quantitative results on aPascal dataset for both the object segmentation (a), and attributes segmentation (b) tasks. [sent-283, score-0.571]
</p><p>86 The importance of our joint estimation for intrinsic images, objects and attributes is conﬁrmed by the better performance of our algorithm compared to the graph-cuts based (AHCRF) method [9] and mean-ﬁeld based approach [12] for both the tasks. [sent-285, score-0.538]
</p><p>87 attribute labelling tasks respectively over the model of [12]. [sent-288, score-0.349]
</p><p>88 From left to right: input image, reﬂectance, depth images, ground truth, output from [9] (AHCRF), output from [12], our output for the object segmentation. [sent-292, score-0.355]
</p><p>89 )  6  Discussion and Conclusion  In this work, we have explored the synergy effects between intrinsic properties of an images, and the objects and attributes present in the scene. [sent-295, score-0.571]
</p><p>90 We cast the problem in a joint energy minimization framework; thus our model is able to encode the strong correlations between intrinsic properties (reﬂectance, shape,illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. [sent-296, score-0.469]
</p><p>91 We achieve both the qualitative and quantitative improvements for the object and attribute labeling, and qualitative improvement for the intrinsic images estimation. [sent-299, score-0.902]
</p><p>92 Future directions include further exploration of the possibilities of integrating priors based on the structural attributes such as slanted, cylindrical to the joint intrinsic properties, objects and attributes model. [sent-300, score-0.723]
</p><p>93 For instance, knowledge that the object is slanted would provide a prior for the depth and distribution of the surface normals. [sent-301, score-0.384]
</p><p>94 Further, the possibility of incorporating a mixture of illumination models to better model the illumination in a natural scene remains another future direction. [sent-302, score-0.431]
</p><p>95 (2012) Shape, albedo, and illumination from a single image of an unknown object. [sent-312, score-0.258]
</p><p>96 (2012) High-frequency shape and albedo from shading using natural image statistics. [sent-326, score-0.382]
</p><p>97 (2011) Recovering intrinsic images with a global sparsity prior on reﬂectance. [sent-340, score-0.253]
</p><p>98 (2012) Joint optimization for object class segmentation and dense stereo reconstruction. [sent-372, score-0.274]
</p><p>99 , (2009) Associative hierarchical CRFs for object class image segmentation. [sent-381, score-0.272]
</p><p>100 (1970) Shape from shading: a method for obtaining the shape of a smooth opaque object from one view. [sent-436, score-0.251]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ectance', 0.627), ('attribute', 0.271), ('shading', 0.212), ('intrinsic', 0.201), ('object', 0.191), ('attributes', 0.185), ('illumination', 0.177), ('depth', 0.164), ('ahcrf', 0.145), ('barron', 0.145), ('nyu', 0.116), ('sirfs', 0.116), ('re', 0.11), ('objects', 0.107), ('torr', 0.102), ('malik', 0.091), ('anyu', 0.087), ('segmentation', 0.083), ('oi', 0.082), ('image', 0.081), ('labelling', 0.078), ('oa', 0.077), ('scene', 0.077), ('apascal', 0.073), ('pascal', 0.07), ('ladicky', 0.064), ('shape', 0.06), ('kinect', 0.059), ('densecrf', 0.058), ('intr', 0.058), ('oveall', 0.058), ('qo', 0.058), ('retinex', 0.058), ('shiny', 0.058), ('qualitative', 0.057), ('normals', 0.055), ('images', 0.052), ('pixels', 0.051), ('plastic', 0.047), ('wooden', 0.047), ('gehler', 0.047), ('unary', 0.047), ('labels', 0.045), ('joint', 0.045), ('adelson', 0.044), ('glass', 0.044), ('lightness', 0.044), ('rgbd', 0.044), ('vineet', 0.044), ('properties', 0.043), ('master', 0.043), ('energy', 0.042), ('quantitative', 0.04), ('oc', 0.038), ('kohli', 0.036), ('segment', 0.036), ('synergy', 0.035), ('oj', 0.035), ('dual', 0.034), ('maps', 0.034), ('rc', 0.034), ('recovering', 0.033), ('rother', 0.033), ('corr', 0.033), ('enforce', 0.033), ('improvement', 0.033), ('pixel', 0.032), ('labelled', 0.032), ('materials', 0.031), ('material', 0.03), ('tp', 0.029), ('albedo', 0.029), ('brookes', 0.029), ('metallic', 0.029), ('osadchy', 0.029), ('painted', 0.029), ('shadows', 0.029), ('slanted', 0.029), ('sturgess', 0.029), ('warrell', 0.029), ('weijer', 0.029), ('dataset', 0.029), ('eld', 0.028), ('indoor', 0.027), ('ra', 0.026), ('jointly', 0.026), ('estimating', 0.026), ('barrow', 0.026), ('felzenswalb', 0.026), ('annotate', 0.026), ('florence', 0.026), ('textonboost', 0.026), ('cvpr', 0.025), ('granada', 0.024), ('lo', 0.022), ('shotton', 0.022), ('lazebnik', 0.022), ('skin', 0.022), ('duplicate', 0.022), ('strategy', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="138-tfidf-1" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>Author: Vibhav Vineet, Carsten Rother, Philip Torr</p><p>Abstract: Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reﬂectance and illumination from a single image, and object class segmentation separately. While these two problems are mutually informative, in the past not many papers have addressed this topic. In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. In this way, our uniﬁed framework is able to capture the correlations between intrinsic properties (reﬂectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. For example, our model is able to enforce the condition that if a set of pixels take same object label, e.g. table, most likely those pixels would receive similar reﬂectance values. We cast the problem in an energy minimization framework and demonstrate the qualitative and quantitative improvement in the overall accuracy on the NYU and Pascal datasets. 1</p><p>2 0.13056384 <a title="138-tfidf-2" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>Author: Marcus Rohrbach, Sandra Ebert, Bernt Schiele</p><p>Abstract: Category models for objects or activities typically rely on supervised learning requiring sufﬁciently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expertspeciﬁed information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More speciﬁcally we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classiﬁcation and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets. 1</p><p>3 0.10843979 <a title="138-tfidf-3" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>4 0.077031679 <a title="138-tfidf-4" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>5 0.074466303 <a title="138-tfidf-5" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>6 0.071876697 <a title="138-tfidf-6" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>7 0.070427269 <a title="138-tfidf-7" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>8 0.069046363 <a title="138-tfidf-8" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>9 0.068559624 <a title="138-tfidf-9" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>10 0.062773474 <a title="138-tfidf-10" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>11 0.060019355 <a title="138-tfidf-11" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>12 0.059855148 <a title="138-tfidf-12" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>13 0.059597209 <a title="138-tfidf-13" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>14 0.05821339 <a title="138-tfidf-14" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>15 0.057537895 <a title="138-tfidf-15" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>16 0.057389129 <a title="138-tfidf-16" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>17 0.057316191 <a title="138-tfidf-17" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>18 0.056828093 <a title="138-tfidf-18" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>19 0.053101555 <a title="138-tfidf-19" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>20 0.050112311 <a title="138-tfidf-20" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.104), (1, 0.054), (2, -0.08), (3, -0.039), (4, 0.086), (5, -0.042), (6, -0.028), (7, -0.003), (8, -0.056), (9, 0.026), (10, -0.101), (11, 0.0), (12, 0.052), (13, -0.003), (14, -0.045), (15, 0.023), (16, -0.05), (17, -0.08), (18, -0.017), (19, 0.043), (20, 0.007), (21, 0.036), (22, -0.004), (23, 0.014), (24, -0.067), (25, 0.049), (26, 0.098), (27, -0.006), (28, 0.022), (29, 0.04), (30, 0.012), (31, 0.058), (32, -0.025), (33, 0.024), (34, -0.048), (35, 0.049), (36, 0.045), (37, -0.072), (38, -0.012), (39, -0.016), (40, -0.014), (41, -0.014), (42, 0.02), (43, 0.008), (44, 0.071), (45, 0.007), (46, 0.129), (47, 0.084), (48, 0.023), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95261347 <a title="138-lsi-1" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>Author: Vibhav Vineet, Carsten Rother, Philip Torr</p><p>Abstract: Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reﬂectance and illumination from a single image, and object class segmentation separately. While these two problems are mutually informative, in the past not many papers have addressed this topic. In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. In this way, our uniﬁed framework is able to capture the correlations between intrinsic properties (reﬂectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. For example, our model is able to enforce the condition that if a set of pixels take same object label, e.g. table, most likely those pixels would receive similar reﬂectance values. We cast the problem in an energy minimization framework and demonstrate the qualitative and quantitative improvement in the overall accuracy on the NYU and Pascal datasets. 1</p><p>2 0.70420992 <a title="138-lsi-2" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>Author: Chen-Ping Yu, Wen-Yu Hua, Dimitris Samaras, Greg Zelinsky</p><p>Abstract: Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects deﬁned as groupings of superpixels that are similar in intensity, color, and gradient orientation features. We introduce a novel parametric method of clustering superpixels by modeling mixture of Weibulls on Earth Mover’s Distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception. We validated this model using a new 90-image dataset of real world scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (Spearman’s ρ = 0.8038, p < 0.001), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth. We conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features. 1</p><p>3 0.69423664 <a title="138-lsi-3" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>Author: Marcus Rohrbach, Sandra Ebert, Bernt Schiele</p><p>Abstract: Category models for objects or activities typically rely on supervised learning requiring sufﬁciently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expertspeciﬁed information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More speciﬁcally we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classiﬁcation and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets. 1</p><p>4 0.6837709 <a title="138-lsi-4" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>5 0.68159676 <a title="138-lsi-5" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>6 0.64939946 <a title="138-lsi-6" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>7 0.63737988 <a title="138-lsi-7" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>8 0.59932381 <a title="138-lsi-8" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>9 0.58332962 <a title="138-lsi-9" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>10 0.56361246 <a title="138-lsi-10" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>11 0.55554098 <a title="138-lsi-11" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>12 0.54779208 <a title="138-lsi-12" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>13 0.54254484 <a title="138-lsi-13" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>14 0.53219169 <a title="138-lsi-14" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>15 0.528606 <a title="138-lsi-15" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>16 0.52107733 <a title="138-lsi-16" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>17 0.51048732 <a title="138-lsi-17" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>18 0.49774787 <a title="138-lsi-18" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>19 0.4942393 <a title="138-lsi-19" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>20 0.4932979 <a title="138-lsi-20" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.023), (33, 0.136), (34, 0.09), (41, 0.014), (43, 0.307), (46, 0.027), (49, 0.065), (56, 0.061), (70, 0.054), (85, 0.014), (89, 0.016), (93, 0.045), (95, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74530196 <a title="138-lda-1" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>Author: Vibhav Vineet, Carsten Rother, Philip Torr</p><p>Abstract: Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reﬂectance and illumination from a single image, and object class segmentation separately. While these two problems are mutually informative, in the past not many papers have addressed this topic. In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. In this way, our uniﬁed framework is able to capture the correlations between intrinsic properties (reﬂectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. For example, our model is able to enforce the condition that if a set of pixels take same object label, e.g. table, most likely those pixels would receive similar reﬂectance values. We cast the problem in an energy minimization framework and demonstrate the qualitative and quantitative improvement in the overall accuracy on the NYU and Pascal datasets. 1</p><p>2 0.70726532 <a title="138-lda-2" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>Author: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, Jeff Dean</p><p>Abstract: The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for ﬁnding phrases in text, and show that learning good vector representations for millions of phrases is possible.</p><p>3 0.65243125 <a title="138-lda-3" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>Author: Daniel S. Levine, Jonathan P. How</p><p>Abstract: We consider the sensor selection problem on multivariate Gaussian distributions where only a subset of latent variables is of inferential interest. For pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efﬁciently from the output of message passing algorithms. We integrate these decompositions into a computationally efﬁcient greedy selector where the computational expense of quantiﬁcation can be distributed across nodes in the network. Experimental results demonstrate the comparative efﬁciency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for any distribution with nuisances. 1</p><p>4 0.53555501 <a title="138-lda-4" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>5 0.53263354 <a title="138-lda-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.5306589 <a title="138-lda-6" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>7 0.52735353 <a title="138-lda-7" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>8 0.52457446 <a title="138-lda-8" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>9 0.52419829 <a title="138-lda-9" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>10 0.52383405 <a title="138-lda-10" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>11 0.52311987 <a title="138-lda-11" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>12 0.52217865 <a title="138-lda-12" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>13 0.52143162 <a title="138-lda-13" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>14 0.52075183 <a title="138-lda-14" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>15 0.52017963 <a title="138-lda-15" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>16 0.52007943 <a title="138-lda-16" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>17 0.51781631 <a title="138-lda-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.51700181 <a title="138-lda-18" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>19 0.51686966 <a title="138-lda-19" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>20 0.51582897 <a title="138-lda-20" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
