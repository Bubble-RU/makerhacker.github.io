<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-6" href="#">nips2013-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</h1>
<br/><p>Source: <a title="nips-2013-6-pdf" href="http://papers.nips.cc/paper/5051-a-determinantal-point-process-latent-variable-model-for-inhibition-in-neural-spiking-data.pdf">pdf</a></p><p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>Reference: <a title="nips-2013-6-reference" href="../nips2013_reference/nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. [sent-9, score-0.572]
</p><p>2 We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. [sent-11, score-0.906]
</p><p>3 The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. [sent-13, score-0.822]
</p><p>4 Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. [sent-14, score-1.084]
</p><p>5 1  Introduction  Statistical models of neural spike recordings have greatly facilitated the study of both intra-neuron spiking behavior and the interaction between populations of neurons. [sent-15, score-0.664]
</p><p>6 Point processes in particular are popular for modeling neural spiking behavior as they provide statistical distributions over temporal sequences of spikes and help to reveal the complexities underlying a series of noisy measured action potentials (see, e. [sent-17, score-0.572]
</p><p>7 Signiﬁcant effort has been focused on addressing the inadequacies of the standard homogenous Poisson process to model the highly non-stationary stimulus-dependent spiking behavior of neurons. [sent-20, score-0.434]
</p><p>8 The generalized linear model (GLM) is a widely accepted extension for which the instantaneous spiking probability can be conditioned on spiking history or some external covariate. [sent-21, score-0.822]
</p><p>9 (2008) demonstrated how the incorporation of simple pairwise connections into the GLM can capture correlated spiking activity and result in a superior model of physiological data. [sent-24, score-0.483]
</p><p>10 In this paper, we develop a point process over spikes from collections of neurons that explicitly models anti-correlation to capture the inhibitive and competitive relationships known to exist between neurons throughout the brain. [sent-27, score-1.034]
</p><p>11 1  Although the incorporation of pairwise inhibition in statistical models is challenging, we demonstrate how complex nonlinear pairwise inhibition between neurons can be modeled explicitly and tractably using a determinantal point process (DPP). [sent-29, score-0.93]
</p><p>12 The Poisson spike rate of each neuron is used to model individual spiking behavior, while pairwise inhibition is introduced to model competition between neurons. [sent-32, score-0.955]
</p><p>13 Given neural spiking data from a collection of neurons and corresponding stimuli, we learn a latent embedding of neurons such that nearby neurons in the latent space inhibit one another as enforced by a DPP over the kernel between latent embeddings. [sent-35, score-2.108]
</p><p>14 Not only does this overcome a modeling shortcoming of standard point processes applied to spiking data but it provides an interpretable model for studying the inhibitive and competitive properties of sets of neurons. [sent-36, score-0.453]
</p><p>15 We demonstrate how divisive normalization is easily incorporated into our model and a learned periodic modulation of individual neuron spiking is added to model the inﬂuence on individual neurons of periodic phenomena such as theta or gamma rhythms. [sent-37, score-1.925]
</p><p>16 The model is empirically validated in Section 4, ﬁrst on three simulated examples to show the inﬂuence of its various components and then using spike recordings from a collection of neurons in the hippocampus of an awake behaving rat. [sent-38, score-0.761]
</p><p>17 We show that the model learns a latent embedding of neurons that is consistent with the previously observed inhibitory relationship between interneurons and pyramidal cells. [sent-39, score-0.959]
</p><p>18 The inferred periodic component of approximately 4 Hz is precisely the frequency of the theta rhythm observed in these data and its learned inﬂuence on individual neurons is again consistent with the dichotomy of neurons. [sent-40, score-1.175]
</p><p>19 1  Background Generalized Linear Models for Neuron Spiking  A standard starting point for modeling single neuron spiking data is the homogenous Poisson process, for which the instantaneous probability of spiking is determined by a scalar rate or intensity parameter. [sent-42, score-1.02]
</p><p>20 Paninski (2004) showed that one can analyze recorded spike data by ﬁnding the maximum likelihood estimate of the parameters of the GLM, and thereby study the dependence of the spiking on external input. [sent-47, score-0.622]
</p><p>21 (2005) extended this to analyze the dependence of a neuron’s spiking behavior on its past spiking history, ensemble activity and stimuli. [sent-49, score-0.835]
</p><p>22 (2008) demonstrated that the model of individual neuron spiking activity was signiﬁcantly improved by including coupling ﬁlters from other neurons with correlated spiking activity in the GLM. [sent-51, score-1.456]
</p><p>23 Although it is prevalent in the literature, there are fundamental limitations to the GLM’s ability to model real neural spiking patterns. [sent-52, score-0.4]
</p><p>24 The GLM can not model the joint probability of multiple neurons spiking simultaneously and thus lacks a direct dependence between the spiking of multiple neurons. [sent-53, score-1.168]
</p><p>25 Instead, the coupled GLM relies on an assumption that pairs of neurons are conditionally independent given the previous time step. [sent-54, score-0.402]
</p><p>26 , 2003), suggests that one can better predict the spiking of an individual neuron by taking into account the simultaneous spiking of other neurons. [sent-56, score-0.988]
</p><p>27 This new model enables a rich set of interactions between neurons and enables them to be embedded in an easily-visualized latent space. [sent-58, score-0.564]
</p><p>28 1  Methods Modeling inter-Neuron Inhibition with Determinantal Point Processes  We are interested in modelling the spikes on N neurons during an interval of time T . [sent-76, score-0.499]
</p><p>29 At a high level, we use an L-ensemble determinantal point process to model which neurons spike in time t via a subset St ⊂ S: |KSt | Pr(St | {yn }N ) = . [sent-83, score-0.758]
</p><p>30 The kernel function, governed by hyperparameters θ, measures the degree of dependence between two neurons as a function of their latent vectors. [sent-85, score-0.595]
</p><p>31 In our empirical analysis we choose a kernel function that measures this dependence based on the Euclidean distance between latent vectors such that neurons that are closer in the latent space will inhibit each other more. [sent-86, score-0.78]
</p><p>32 Thus in the case of independent neurons with Poisson spiking we can write KS as a diagonal matrix where the diagonal entries are the individual Poisson intensity parameters, KS = diag(λ1 , λ2 , · · · , λN ). [sent-89, score-0.898]
</p><p>33 Following Zou and Adams (2012), we express the marginal preference for a neuron ﬁring over others, thus including the neuron in the subset S, with a “prior kernel” that modulates the covariance. [sent-92, score-0.476]
</p><p>34 Assuming that kθ (y, y) = 1, this kernel has the form (2) [KS ]n,n = kθ (yn , yn )δ λn λn , where n, n ∈ S and λn is the intensity measure of the Poisson process for the individual spiking behavior of neuron n. [sent-93, score-0.856]
</p><p>35 We denote the stimulus at time t by a vector xt ∈ RK and neuron-speciﬁc weights as wn ∈ RK , leading to instantaneous rates: λ(t) = exp{xT wn }. [sent-95, score-0.559]
</p><p>36 (3) n t This leads to a stimulus dependent kernel for the DPP L-ensemble: 1 T (t) x (wn + wn ) . [sent-96, score-0.387]
</p><p>37 [KS ]n,n = kθ (yn , yn ) δ exp (4) 2 t 3  (t)  It is convenient to denote the diagonal matrix Π(t) = diag( λ1 , (t)  (t)  λ2 , · · · ,  (t)  λN ), as well as  the St -restricted submatrix ΠSt , where St indexes the rows of Π corresponding to the subset of neurons that spiked at time t. [sent-97, score-0.513]
</p><p>38 The latent embeddings yn and weights wn can now be learned so that the appropriate balance is found between stimulus dependence and inhibition due to, e. [sent-102, score-0.83]
</p><p>39 The cubic scaling of determinants in this model will not be a realistic limiting factor until it is possible to simultaneously record from tens of thousands of neurons simultaneously. [sent-112, score-0.402]
</p><p>40 3  Gain and Contrast Normalization  There is increasing evidence that neural responses are normalized or scaled by a common factor such as the summed activations across a pool of neurons (Carandini and Heeger, 2012). [sent-116, score-0.434]
</p><p>41 Such normalization can be captured in our model through scaling the individual neuron spiking rates by a stimulus-dependent multiplicative constant νt > 0: (t)  Pr(St | {wn , yn }N , xt , θ, νt ) = n=1  (t)  |νt δΠSt KSt ΠSt | (t)  (t)  |νt δΠS KS ΠS + IN |  ,  (6)  where νt = exp{xT wν }. [sent-119, score-0.769]
</p><p>42 4  Modeling the Inﬂuence of Periodic Phenomena  Neuronal spiking is known to be heavily inﬂuenced by periodic phenomena. [sent-122, score-0.607]
</p><p>43 3 we apply the model to the spiking of neurons in the hippocampus of behaving rats. [sent-124, score-0.891]
</p><p>44 (1999) observe that the theta rhythm plays a signiﬁcant role in determining the spiking behavior of the neurons in these data, with neurons spiking in phase with the 4 Hz periodic signal. [sent-126, score-2.093]
</p><p>45 Thus, the ﬁring patterns of neurons that ﬁre in phase can be expected to be highly correlated while those which ﬁre out of phase will be strongly anti-correlated. [sent-127, score-0.402]
</p><p>46 Note that if desired one can easily manipulate Equation 7 to have each of the neurons modulated by an individual frequency, ai , and offset bi . [sent-129, score-0.467]
</p><p>47 Alternatively, we can create a mixture of J periodic components, modeling for example the inﬂuence of the theta and gamma rhythms, by adding a sum over components,   J   λ(t) = exp xT wn + ρjn sin(fj t + ϕj ) (8) n  t  j=1  4  2  2  1. [sent-130, score-0.553]
</p><p>48 5 0  2  4 6 8 Order in 1D Retina  10  (a) Sliding Bar  12  −2 0  2  4 6 8 Order in 1D Retina  (b) Random Spiking  10  12  0 0  2  4 6 8 Order in 1D Retina  10  12  (c) Gain Control  Figure 1: Results of the simulated moving bar experiment (1a) compared to independent spiking behavior (1b). [sent-140, score-0.522]
</p><p>49 Note that in 1a the model puts neighboring neurons within the unit length scale while it puts others at least one length scale apart. [sent-141, score-0.434]
</p><p>50 1c demonstrates the weights, wν , of the gain component learned if up to 5x random gain is added to the stimulus at retina locations 6-12. [sent-142, score-0.533]
</p><p>51 We ﬁrst evaluate the model on a set of simulated experiments to examine its ability to capture inhibition in the latent variables while learning the stimulus weights and gain normalization. [sent-144, score-0.626]
</p><p>52 1  Simulated Moving Bar  We ﬁrst consider an example simulated problem where twelve neurons are conﬁgured in order along a one dimensional retinotopic map and evaluate the ability of the DPP to learn latent representations that reﬂect their inhibitive properties. [sent-148, score-0.601]
</p><p>53 Each neuron has a receptive ﬁeld of a single pixel and the neurons are stimulated by a three pixel wide moving bar. [sent-149, score-0.678]
</p><p>54 Of the three neighboring neurons exposed to the bar, all receive high spike intensity but due to neural inhibition, only the middle one spikes. [sent-151, score-0.671]
</p><p>55 A small amount of random background stimulus is added as well, causing some neurons to spike without being stimulated by the moving bar. [sent-152, score-0.843]
</p><p>56 We train the DPP speciﬁed above on the resulting spike trains, using the stimulus of each neuron as the Poisson intensity measure and visualize the one-dimensional latent representation, y, for each neuron. [sent-153, score-0.748]
</p><p>57 This is compared to the case where all neurons receive random stimulus and spike randomly and independently when the stimulus is above a threshold. [sent-154, score-0.992]
</p><p>58 The resulting learned latent values for the neurons are displayed in Figure 1. [sent-155, score-0.583]
</p><p>59 We see in Figure 1a that the DPP prefers neighboring neurons to be close in the latent space, because they compete when the moving bar stimulates them. [sent-156, score-0.635]
</p><p>60 To demonstrate the effect of the gain and contrast normalization we now add random gain of up to 5x to the stimulus only at retina locations 6-12 and retrain the model while learning the gain component. [sent-157, score-0.548]
</p><p>61 2  Digits Data  Now we use a second simulated experiment to examine the ability of the model to capture structure encoding inhibitory interactions in the latent representation while learning the stimulus dependent probability of spiking from data. [sent-160, score-0.962]
</p><p>62 In the data, each of the thirty neurons is specialized to one digit class, with three neurons per digit. [sent-165, score-0.839]
</p><p>63 When a digit is presented, two neurons ﬁre among the three: one that ﬁres with probability one, and one of the remaining two ﬁres with uniform probability. [sent-166, score-0.437]
</p><p>64 Thus, we expect three neurons to have strong probability of ﬁring when the stimulus contains their preferred digit; however, one of the neurons does not spike due to competition with another neuron. [sent-167, score-1.182]
</p><p>65 We expect the model to learn this inhibition by moving the neurons close together in the latent space. [sent-168, score-0.672]
</p><p>66 Examining the learned stimulus weights and latent embeddings, shown in Figures 2a and 2b respectively, we see that this is indeed the case. [sent-169, score-0.422]
</p><p>67 A visualization of the neuron speciﬁc weights wn (2a) and latent embedding (2b) learned by the DPP. [sent-172, score-0.622]
</p><p>68 In (2b) each blue number indicates the position of the neuron that always ﬁres for that speciﬁc digit, and the red and green numbers indicate the neurons that respond to that digit but inhibit each other. [sent-173, score-0.717]
</p><p>69 We observe in (2b) that inhibitory pairs of neurons, the red and green pairs, are placed extremely close to each other in the DPP’s learned latent space while neurons that spike simultaneously (the blue and either red or green) are distant. [sent-174, score-0.902]
</p><p>70 The coupled GLM can not model this scenario well because both neurons of the inhibitory pair receive strong stimulus but there is no indication from past spiking behavior which neuron will spike. [sent-176, score-1.384]
</p><p>71 Figure 3b shows the (normalized) weights, wn learned from the stimulus feature vectors, which consist of concatenated (t) location and orientation bins, to each neuron’s Poisson spike rate λn . [sent-184, score-0.571]
</p><p>72 An interesting observation is that the two highest frequency neurons, interneurons, have little dependence on any particular stimulus and are strongly anti-correlated with a large group of low frequency pyramidal cells. [sent-185, score-0.479]
</p><p>73 3c shows the weights, wν to the gain control, ν, and 3d shows a visualization of the stimulus weights for a single neuron n = 3 organized by location and orientation bins. [sent-186, score-0.568]
</p><p>74 In 3a and 3b the neurons are ordered by their ﬁring rates. [sent-187, score-0.402]
</p><p>75 pairs of neurons, both will simultaneously receive strong stimulus but the conditional independence assumption will not hold; past spiking behavior can not indicate that only one can spike. [sent-189, score-0.616]
</p><p>76 The data consist of spikes recorded from 31 neurons across four shanks during open ﬁeld tasks as well as the syncronized positions of two LEDs on the rat’s head. [sent-194, score-0.53]
</p><p>77 2  (a) Latent embedding of neurons  Spike Rate (Hz)  6c  Spike Rate (Hz)  0. [sent-216, score-0.438]
</p><p>78 2  1  (b) Latent embedding of neurons (zoomed)  Figure 4: A visualization of the two dimensional latent embeddings, yn , learned for each neuron. [sent-224, score-0.738]
</p><p>79 The color of the dots represents the empirical spiking rate of the neuron, the number indicates the depth of the neuron according to its position along the shank - from 0 (shallow) to 7 (deep) - and the letter denotes which of four distinct shanks the neurons spiking was read from. [sent-227, score-1.382]
</p><p>80 We observe that the higher frequency interneurons are placed distant from each other but in a conﬁguration such that they inhibit the low frequency pyramidal cells. [sent-228, score-0.412]
</p><p>81 In 5a, the neurons share a single learned periodic frequency and offset but each learn an individual scaling factor ρn and 5b shows the average inﬂuence of the two component mixture on the high and low spike rate neurons. [sent-251, score-1.009]
</p><p>82 In 5a the neurons are colored by ﬁring rate from light (high) to dark (low). [sent-254, score-0.402]
</p><p>83 Note that the model learns a frequency that is consistent with the approximately 4 Hz theta rhythm and there is a dichotomy in the learned amplitudes, ρ, that is consistent with the inﬂuence of the theta rhythm on pyramidal cells and interneurons. [sent-255, score-0.916]
</p><p>84 Interneurons are known to inhibit pyramidal cells, so we expect interesting inhibitory interactions and anti-correlated spiking between the pyramidal cells. [sent-263, score-0.866]
</p><p>85 In our qualitative analysis we visualize the the data by the ﬁring rates of the neurons to see if the model learns this dichotomy. [sent-264, score-0.427]
</p><p>86 Figure 3 shows the kernel matrix KS corresponding to the latent embeddings in Figure 4 and the stimulus and gain control weights learned by the model. [sent-266, score-0.591]
</p><p>87 In Figure 5 we see the periodic components learned for individual neurons on the hippocampal data according to Equation 7 when the frequency term f and offset ϕ are shared across neurons. [sent-268, score-0.87]
</p><p>88 However, the scaling terms ρn are learned for each neuron, so the neurons can each determine the inﬂuence of the periodic component on their spiking behavior. [sent-269, score-1.112]
</p><p>89 (1999) that interneurons and pyramidal cells are modulated by the theta rhythm at different amplitudes. [sent-291, score-0.57]
</p><p>90 A spike is predicted if it increases the likelihood under the model and the accuracy is averaged over all neurons and time slices in the validation set. [sent-299, score-0.652]
</p><p>91 We compare GLMs with the periodic component, gain, stimulus and coupling ﬁlters to our DPP with the latent component. [sent-300, score-0.569]
</p><p>92 The models did not differ signiﬁcantly in the correct prediction of when neurons would not spike - i. [sent-301, score-0.568]
</p><p>93 As an example scenario, in a one-of-N neuron ﬁring case the GLM may prefer to predict that nothing ﬁres (rather than incorrectly predict multiple spikes) whereas the DPP can actually condition on the behavior of the other neurons to determine which neuron ﬁred. [sent-308, score-0.864]
</p><p>94 5  Conclusion  In this paper we presented a novel model for neural spiking data from populations of neurons that is designed to capture the inhibitory interactions between neurons. [sent-309, score-1.056]
</p><p>95 The model is able to accurately capture the known interaction between a dichotomy of neurons and the learned frequency component reﬂects the true modulation of these neurons by the theta rhythm. [sent-312, score-1.295]
</p><p>96 A deﬁning feature of the DPP is an ability to model inhibitory relationships in a neural population; excitatory connections between neurons are modeled as through the lack of inhibition. [sent-314, score-0.612]
</p><p>97 Thus, neurons are not inﬂuenced by their own or others’ spiking history. [sent-317, score-0.77]
</p><p>98 Finally, we see from Table 1 that the gain modulation and periodic component are essential to model the hippocampal data. [sent-320, score-0.485]
</p><p>99 An interesting alternative to the periodic modulation of individual neuron spiking probabilities would be to have the latent representation of neurons itself be modulated by a periodic component. [sent-321, score-1.702]
</p><p>100 A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects. [sent-372, score-0.863]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurons', 0.402), ('spiking', 0.368), ('dpp', 0.246), ('periodic', 0.239), ('ks', 0.227), ('neuron', 0.213), ('stimulus', 0.212), ('theta', 0.184), ('spike', 0.166), ('determinantal', 0.16), ('inhibitory', 0.153), ('csicsvari', 0.135), ('wn', 0.13), ('glm', 0.125), ('inhibition', 0.119), ('latent', 0.118), ('pyramidal', 0.117), ('interneurons', 0.108), ('spikes', 0.097), ('rhythm', 0.094), ('yn', 0.086), ('gain', 0.081), ('hippocampus', 0.074), ('poisson', 0.071), ('inhibit', 0.067), ('ring', 0.067), ('hippocampal', 0.067), ('rat', 0.067), ('learned', 0.063), ('frequency', 0.06), ('hz', 0.058), ('modulation', 0.058), ('st', 0.057), ('retina', 0.056), ('dichotomy', 0.054), ('kulesza', 0.051), ('pairwise', 0.05), ('modulates', 0.05), ('bar', 0.05), ('behaving', 0.047), ('inhibitive', 0.046), ('mizuseki', 0.046), ('kernel', 0.045), ('divisive', 0.044), ('interactions', 0.044), ('embeddings', 0.043), ('glms', 0.043), ('uence', 0.042), ('cells', 0.041), ('res', 0.041), ('kst', 0.04), ('component', 0.04), ('processes', 0.039), ('intensity', 0.039), ('individual', 0.039), ('truccolo', 0.037), ('recordings', 0.037), ('normalization', 0.037), ('behavior', 0.036), ('embedding', 0.036), ('ben', 0.035), ('digit', 0.035), ('simulated', 0.035), ('moving', 0.033), ('activity', 0.033), ('visualization', 0.033), ('instantaneous', 0.032), ('capture', 0.032), ('neural', 0.032), ('neighboring', 0.032), ('bins', 0.031), ('buzs', 0.031), ('hirase', 0.031), ('shanks', 0.031), ('zoomed', 0.031), ('process', 0.03), ('dependence', 0.03), ('stimulated', 0.03), ('et', 0.029), ('external', 0.029), ('likelihood', 0.029), ('weights', 0.029), ('slices', 0.028), ('validation', 0.027), ('paninski', 0.027), ('affandi', 0.027), ('chornoboy', 0.027), ('pillow', 0.027), ('xt', 0.026), ('modulated', 0.026), ('determinant', 0.025), ('schneidman', 0.025), ('diagonal', 0.025), ('interacting', 0.025), ('elegant', 0.025), ('gram', 0.025), ('populations', 0.025), ('learns', 0.025), ('relationships', 0.025), ('history', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="6-tfidf-1" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>2 0.31905791 <a title="6-tfidf-2" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>3 0.30165681 <a title="6-tfidf-3" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>4 0.26692826 <a title="6-tfidf-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.25300887 <a title="6-tfidf-5" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>6 0.22694342 <a title="6-tfidf-6" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>7 0.19808584 <a title="6-tfidf-7" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>8 0.19032356 <a title="6-tfidf-8" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>9 0.18991442 <a title="6-tfidf-9" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>10 0.18439488 <a title="6-tfidf-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.18083927 <a title="6-tfidf-11" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>12 0.16839267 <a title="6-tfidf-12" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>13 0.16368197 <a title="6-tfidf-13" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>14 0.15462418 <a title="6-tfidf-14" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>15 0.15350072 <a title="6-tfidf-15" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>16 0.14833125 <a title="6-tfidf-16" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>17 0.14337701 <a title="6-tfidf-17" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>18 0.13395652 <a title="6-tfidf-18" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>19 0.11997119 <a title="6-tfidf-19" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>20 0.11690907 <a title="6-tfidf-20" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.133), (2, -0.169), (3, -0.121), (4, -0.469), (5, -0.063), (6, -0.021), (7, -0.1), (8, 0.02), (9, 0.086), (10, -0.036), (11, -0.022), (12, -0.049), (13, -0.1), (14, 0.16), (15, 0.007), (16, 0.035), (17, -0.081), (18, 0.225), (19, 0.049), (20, 0.033), (21, -0.022), (22, 0.016), (23, -0.094), (24, 0.025), (25, -0.122), (26, -0.036), (27, 0.031), (28, 0.079), (29, -0.007), (30, -0.077), (31, -0.091), (32, -0.078), (33, 0.013), (34, 0.014), (35, 0.02), (36, -0.004), (37, -0.042), (38, -0.006), (39, -0.017), (40, -0.095), (41, -0.035), (42, 0.064), (43, 0.012), (44, 0.001), (45, -0.027), (46, -0.029), (47, 0.014), (48, 0.05), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95943218 <a title="6-lsi-1" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>2 0.78452116 <a title="6-lsi-2" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>3 0.76208371 <a title="6-lsi-3" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>4 0.69793034 <a title="6-lsi-4" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>5 0.67659551 <a title="6-lsi-5" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive ﬁelds in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identiﬁcation of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identiﬁed. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identiﬁcation algorithms. 1</p><p>6 0.64163935 <a title="6-lsi-6" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>7 0.56692171 <a title="6-lsi-7" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>8 0.56603312 <a title="6-lsi-8" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>9 0.56378925 <a title="6-lsi-9" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>10 0.50184894 <a title="6-lsi-10" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>11 0.50148618 <a title="6-lsi-11" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>12 0.49638009 <a title="6-lsi-12" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>13 0.49376434 <a title="6-lsi-13" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>14 0.44987217 <a title="6-lsi-14" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>15 0.44502082 <a title="6-lsi-15" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>16 0.44093412 <a title="6-lsi-16" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>17 0.43287557 <a title="6-lsi-17" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>18 0.43173459 <a title="6-lsi-18" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>19 0.43097439 <a title="6-lsi-19" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>20 0.43090525 <a title="6-lsi-20" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.045), (33, 0.093), (34, 0.078), (41, 0.019), (49, 0.485), (56, 0.058), (70, 0.04), (85, 0.019), (89, 0.04), (93, 0.027), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88048738 <a title="6-lda-1" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>Author: Tuan A. Nguyen, Subbarao Kambhampati, Minh Do</p><p>Abstract: Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we ﬁrst introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of ﬁnding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner. 1</p><p>same-paper 2 0.87556708 <a title="6-lda-2" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>3 0.82704914 <a title="6-lda-3" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>Author: Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan</p><p>Abstract: Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a “relevance topic model” for jointly learning meaningful mid-level representations upon bagof-words (BoW) video representations and a classiﬁer with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectiﬁed linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efﬁcient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classiﬁcation accuracy, particularly in the case of a very small number of labeled training videos. 1</p><p>4 0.79706562 <a title="6-lda-4" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>5 0.74067682 <a title="6-lda-5" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>6 0.71642685 <a title="6-lda-6" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>7 0.69519389 <a title="6-lda-7" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>8 0.63665658 <a title="6-lda-8" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>9 0.61620039 <a title="6-lda-9" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>10 0.60423005 <a title="6-lda-10" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>11 0.54785162 <a title="6-lda-11" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>12 0.50981164 <a title="6-lda-12" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>13 0.50619876 <a title="6-lda-13" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>14 0.49168128 <a title="6-lda-14" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>15 0.45674375 <a title="6-lda-15" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>16 0.45538872 <a title="6-lda-16" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>17 0.45535553 <a title="6-lda-17" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>18 0.45397148 <a title="6-lda-18" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>19 0.45173946 <a title="6-lda-19" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>20 0.44815728 <a title="6-lda-20" href="./nips-2013-Lexical_and_Hierarchical_Topic_Regression.html">174 nips-2013-Lexical and Hierarchical Topic Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
