<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-49" href="#">nips2013-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</h1>
<br/><p>Source: <a title="nips-2013-49-pdf" href="http://papers.nips.cc/paper/4890-bayesian-inference-and-online-experimental-design-for-mapping-neural-microcircuits.pdf">pdf</a></p><p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>Reference: <a title="nips-2013-49-reference" href="../nips2013_reference/nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. [sent-8, score-0.59]
</p><p>2 In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. [sent-9, score-0.382]
</p><p>3 The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. [sent-10, score-0.801]
</p><p>4 We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. [sent-11, score-0.292]
</p><p>5 Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. [sent-12, score-1.371]
</p><p>6 1  Introduction  A major goal of neuroscience is the mapping of neural microcircuits at the scale of hundreds to thousands of neurons [1]. [sent-13, score-0.448]
</p><p>7 By mapping, we speciﬁcally mean determining which neurons synapse onto each other and with what weight. [sent-14, score-0.284]
</p><p>8 In this paper, we speciﬁcally address the mapping experiment in which a set of putative presynaptic neurons are optically stimulated while an electrophysiological trace is recorded from a designated postsynaptic neuron. [sent-16, score-0.91]
</p><p>9 For example, while it has been shown that multiple neurons can be stimulated simultaneously [4, 5], successful mapping experiments have thus far only stimulated a single neuron per trial which increases experimental time [2, 3, 6]. [sent-19, score-0.966]
</p><p>10 Stimulating multiple neurons simultaneously and with high accuracy requires well-tuned hardware, and even then some level of stimulus uncertainty may remain. [sent-20, score-0.421]
</p><p>11 In this paper, we address these issues by developing a procedure for sparse Bayesian inference and information-based experimental design which can reconstruct neural microcircuits accurately and quickly despite the issues listed above. [sent-24, score-0.372]
</p><p>12 , N , the experimenter stimulates R of K possible presynaptic neurons. [sent-30, score-0.239]
</p><p>13 We represent the chosen set of neurons for each trial with the binary vector zn ∈ {0, 1}K , which has a one in each of the the R entries corresponding to the stimulated neurons on that trial. [sent-31, score-1.11]
</p><p>14 One of the difﬁculties of optical stimulation lies in the experimenter’s inability to stimulate a speciﬁc neuron without possibly failing to stimulate the target neuron or engaging other nearby neurons. [sent-32, score-0.936]
</p><p>15 In general, this is a result of the fact that optical excitation does not stimulate a single point in space but rather has a point spread function that is dependent on the hardware and the biological tissue. [sent-33, score-0.374]
</p><p>16 To complicate matters further, each neuron has a different rheobase (a measure of how much current is needed to generate an action potential) and expression level of the optogenetic protein. [sent-34, score-0.299]
</p><p>17 While some work has shown that it may be possible to stimulate exact sets of neurons, this setup requires very speciﬁc hardware and ﬁne tuning [4, 5]. [sent-35, score-0.276]
</p><p>18 In addition, even if a neuron ﬁres, there is some probability that synaptic transmission will not occur. [sent-36, score-0.524]
</p><p>19 Because these events are difﬁcult or impossible to observe, we model this uncertainty by introducing a second binary vector xn ∈ {0, 1}K denoting the neurons that actually release neurotransmitter in trial n. [sent-37, score-0.564]
</p><p>20 The conditional distribution of xn given zn can be chosen by the experimenter to match their hardware settings and understanding of synaptic transmission rates in their preparation. [sent-38, score-0.753]
</p><p>21 2  Sparse connectivity  Numerous studies have collected data to estimate both connection probabilities and synaptic weight distributions as a function of distance and cell identity [2, 3, 6, 7, 8, 9, 10, 11, 12]. [sent-40, score-0.597]
</p><p>22 Generally, the data show that connectivity is sparse and that most synaptic weights are small with a heavy tail of strong connections. [sent-41, score-0.45]
</p><p>23 To capture the sparsity of neural connectivity, we place a “spike-and-slab” prior on the synaptic weights wk [13, 14, 15], for each presynaptic neuron k = 1, . [sent-42, score-0.843]
</p><p>24 Note that we do not need to restrict the “slab” distributions (the conditional distributions of wk given that wk is nonzero) to the traditional Gaussian choice, and in fact each weight can have its own parameters. [sent-46, score-0.239]
</p><p>25 3  Postsynaptic response  In our model a subthreshold response is measured from a designated postsynaptic neuron. [sent-49, score-0.419]
</p><p>26 The postsynaptic response for each synaptic event in a given trial can be modeled using an appropriate template function fk (·) for each presynaptic neuron k. [sent-51, score-1.26]
</p><p>27 For this paper we use an alpha function to model the shape of each neuron’s contribution to the postsynaptic current, parameterized by time constants τk which deﬁne the rise and decay time. [sent-52, score-0.257]
</p><p>28 As with the synaptic weight priors, the template functions could be designed based on the cells’ identities. [sent-53, score-0.309]
</p><p>29 The onset of each postsynaptic 1 A cell’s identity can be general such as excitatory or inhibitory, or more speciﬁc such as VIP- or PVinterneurons. [sent-54, score-0.379]
</p><p>30 These identities can be identiﬁed by driving the optogenetic channel with a particular promotor unique to that cell type or by coexpressing markers for various cell types along with the optogenetic channel. [sent-55, score-0.337]
</p><p>31 2  Presynaptic weights  Location of presynaptic neurons and stimuli  Weight  1 0 − 1 0  20  40  10 Current [pA]  Neuron k  60  80  100  Postsynaptic current trace  0 − 10 − 20 − 30 0  50  100 Time [samples]  150  200  Figure 1: A schematic of the model experiment. [sent-56, score-0.655]
</p><p>32 The left ﬁgure shows the relative location of 100 presynaptic neurons; inhibitory neurons are shown in yellow, and excitatory neurons in purple. [sent-57, score-0.903]
</p><p>33 Neurons marked with a black outline have a nonzero connectivity to the postsynaptic neuron (shown as a blue star, in the center). [sent-58, score-0.653]
</p><p>34 The true connectivity weights are shown on the upper right, with blue vertical lines marking the ﬁve neurons which were actually ﬁred as a result of this stimulus. [sent-60, score-0.506]
</p><p>35 The resulting time series postsynaptic current trace is shown in the bottom right. [sent-61, score-0.299]
</p><p>36 The connected neurons which ﬁred are circled in red, the triangle and star marking their weights and corresponding postsynaptic events in the plots at right. [sent-62, score-0.627]
</p><p>37 response may be jittered such that each event starts at some time dnk after t = 0, where the delays could be conditionally distributed on the parameters of the stimulation and cells. [sent-63, score-0.363]
</p><p>38 To infer the marginal distribution of the synaptic weights, one can use standard Bayesian methods such as Gibbs sampling or variational inference, both of which are discussed below. [sent-69, score-0.502]
</p><p>39 An example set of neurons and connectivity weights, along with the set of stimuli and postsynaptic current trace for a single trial, is shown in Figure 1. [sent-70, score-0.838]
</p><p>40 1  Charge as synaptic strength  To reduce the space over which we perform inference, we collapse the variables wk and τk into a single variable ck = t wk fk (t − dnk , τk ) which quantiﬁes the charge transfer during the synaptic event and can be used to deﬁne the strength of a connection. [sent-77, score-1.108]
</p><p>41 p(y|X, c) =  (3)  n  We found that na¨ve MCMC sampling over the posterior of w, τ , γ, X, and D insufﬁciently exı plored the support and inference was unsuccessful. [sent-80, score-0.227]
</p><p>42 We approximate the prior over c as a spike-and-slab with Gaussian slabs where the slabs could be truncated if the cells’ excitatory or inhibitory identity is known. [sent-87, score-0.394]
</p><p>43 Each xnk can be sampled by computing the odds ratio, and following [15] we draw each ck , γk from the joint distribution p(ck , γk |Z, y, X, {cj , γj |j = k}) by sampling ﬁrst γk from p(γk |Z, y, X, {cj |j = k}), then p(ck |Z, y, X, {cj , |j = k}, γk ). [sent-88, score-0.3]
</p><p>44 This means that we must be able to perform inference of the posterior as well as choose the next stimulus extremely quickly. [sent-92, score-0.326]
</p><p>45 To achieve this decrease in runtime, we approximate the posterior distribution of c and γ using a variational approach [16]. [sent-94, score-0.314]
</p><p>46 The use of variational inference for spike-and-slab regression models has been explored in [17, 18], and we follow their methods with some minor changes. [sent-95, score-0.271]
</p><p>47 As is the case with fully-factorized variational distributions, updating the posterior involves an iterative algorithm which cycles through the parameters for each factor. [sent-102, score-0.314]
</p><p>48 4  Therefore, since the product of a spike-and-slab and a Gaussian is still a spike-and-slab, if we stimulate only one neuron at each trial then this posterior is also spike-and-slab, and the variational approximation becomes exact in this limit. [sent-105, score-0.957]
</p><p>49 We Monte Carlo approximate this integral in a manner similar to the approach used for integrating over the hyperparameters in [17]; however, here we further approximate by sampling over potential stimuli xnk from p(xnk = 1|zn ). [sent-107, score-0.24]
</p><p>50 In practice we will see this approximation sufﬁces for experimental design, with the overall variational approach performing nearly as well for posterior weight reconstruction as Gibbs sampling from the true posterior. [sent-108, score-0.488]
</p><p>51 4  Optimal experimental design  The preparations needed to perform these type of experiments tend to be short-lived, and indeed, the very act of collecting data — that is, stimulating and probing cells — can compromise the health of the preparation further. [sent-109, score-0.338]
</p><p>52 We are thus strongly motivated to optimize the experimental design: to choose the optimal subset of neurons zn to stimulate at each trial to minimize N , the overall number of trials required for good inference. [sent-112, score-1.075]
</p><p>53 , (zn−1 , yn−1 )} are ﬁxed and yn is dependent on the stimulus zn , our problem is reduced to choosing the optimal next stimulus, denoted zn , in expectation over yn , (7) zn = arg max Eyn |zn [I(θ; D)] = arg min Eyn |zn [H(θ|D)] . [sent-120, score-1.121]
</p><p>54 zn  zn  5  Experimental design procedure  The optimization described in Section 4 entails performing a combinatorial optimization over zn , where for each zn we consider an expectation over all possible yn . [sent-121, score-1.16]
</p><p>55 1  Computing the objective function  The variational posterior distribution of ck , γk can be used to characterize our general objective function described in Section 4. [sent-125, score-0.493]
</p><p>56 We deﬁne the cost function J to be the right-hand side of Equation 7, J ≡ Eyn |zn [H(c, γ|D)] (8) such that the optimal next stimulus zn can be found by minimizing J. [sent-126, score-0.408]
</p><p>57 (10) k,n 2 k  5  Here, we have introduced additional notation, using αk,n , µk,n , and sk,n to refer to the parameters of the variational posterior distribution given the data through trial n. [sent-129, score-0.556]
</p><p>58 Intuitively, we see that equation 10 represents a balance between minimizing the sparsity pattern entropy H[γk ] of each neuron and minimizing the weight entropy H[ck |γk = 1] proportional to the probability αk that the presynaptic neuron is connected. [sent-130, score-0.847]
</p><p>59 In algorithm behavior, we see when the probability that a neuron is connected increases, we spend time stimulating it to reduce the uncertainty in the corresponding nonzero slab distribution. [sent-132, score-0.514]
</p><p>60 For any particular candidate zn , this can be Monte Carlo approximated by ﬁrst sampling yn from the posterior distribution p(yn |zn , c, Dn−1 ), where c is drawn from the variational posterior inferred at trial n − 1. [sent-134, score-1.051]
</p><p>61 Each sampled yn may be used to estimate the variational parameters αk,n and sk,n with which we evaluate H[ck , γk ]; we average over these evaluations of the entropy from each sample to compute an estimate of J in Eq. [sent-135, score-0.393]
</p><p>62 Once we have chosen zn , we execute the actual trial and run the variational inference procedure on the full data to obtain the updated variational posterior parameters αk,n , µk,n , and sk,n which are needed for optimization. [sent-137, score-1.066]
</p><p>63 Once the experiment has concluded, Gibbs sampling can be run, though we found only a limited gain when comparing Gibbs sampling to variational inference. [sent-138, score-0.274]
</p><p>64 It is not feasible to evaluate the right-hand side of equation 10 for every zn because as K grows there is a combinatorial explosion of possible stimuli. [sent-141, score-0.239]
</p><p>65 To avoid an exhaustive search over possible zn , we adopt a greedy approach for choosing which R of the K locations to stimulate. [sent-142, score-0.239]
</p><p>66 First we rank the K neurons based on an ˜n approximation of the objective function. [sent-143, score-0.284]
</p><p>67 To do this, we propose K hypothetical stimuli, zk , each all zeros except the k th entry equal to 1 — that is, we examine only the K stimuli which represent ∗ ˜n stimulating a single location. [sent-144, score-0.315]
</p><p>68 We then set znk = 1 for the R neurons corresponding to the zk which give the smallest values for the objective function and all other entries of z∗ to zero. [sent-145, score-0.334]
</p><p>69 We found that n the neurons selected by a brute force approach are most likely to be the neurons that the greedy selection process chooses (see Figure 1 in the Appendix). [sent-146, score-0.598]
</p><p>70 For each of ˜n the K proposed stimuli zk , to approximate the expected entropy we must compute the variational ˜ ˜ posterior for M samples of [X1:n−1 xn ] and L samples of yn (where xn is the random variable corresponding to p(˜ n |˜n )). [sent-148, score-0.754]
</p><p>71 Therefore we run the variational inference procedure on the full data x z on the order of O(M KL) times at each trial. [sent-149, score-0.271]
</p><p>72 As the system size grows, running the variational inference procedure this many times becomes intractable because the number of iterations needed to converge the coordinate ascent algorithm is dependent on the correlations between the rows of X. [sent-150, score-0.338]
</p><p>73 Note that the stronger dependence here is on R; when R = 1 the variational parameter updates become exact and independent across the neurons, and therefore no coordinate ascent is necessary and the runtime becomes linear in K. [sent-152, score-0.234]
</p><p>74 We therefore take one last measure to speed up the optimization process by implementing an online Bayesian approach to updating the variational posterior (in the stimulus selection phase only). [sent-153, score-0.511]
</p><p>75 Since the variational posterior of ck and γk takes the same form as the prior distribution, we can use the posterior from trial n − 1 as the prior at trial n, allowing us to effectively summarize the previous data. [sent-154, score-1.175]
</p><p>76 In this online setting, when we stimulate only one neuron, only the parameters of that speciﬁc ˜ n ˜n neuron change. [sent-155, score-0.431]
</p><p>77 If during optimization we temporarily assume that xk = zk , this results in explicit updates for each variational parameter, with no coordinate ascent iterations required. [sent-156, score-0.284]
</p><p>78 The combined accelerations described in this section result in a speed up of several orders of magnitude which allows the full inference and optimization procedure to be run in real time, running at approximately one second per trial in our computing environment for K = 500, R = 8. [sent-158, score-0.315]
</p><p>79 We chose to parallelize over M which distributes the sampling of X and the running of variational inference for each sample. [sent-160, score-0.309]
</p><p>80 The heavy red and blue lines indicate the results when running the Gibbs sampler at that point in the experiment, and the thinner magenta and cyan lines indicate the results from variational inference. [sent-182, score-0.23]
</p><p>81 6  Experiments and results  We ran our inference and optimal experimental design algorithm on data sets generated from the model described in Section 2. [sent-187, score-0.261]
</p><p>82 Baseline results are shown in Figure 2, over a range of values for stimulations per trial R and baseline postsynaptic noise levels ν. [sent-189, score-0.499]
</p><p>83 The results here use an informative prior, where we assume the excitatory or inhibitory identity is known, and we set individual prior connectivity probabilities for each neuron based on that neuron’s identity and distance from the postsynaptic cell. [sent-190, score-0.911]
</p><p>84 We choose to let X be unobserved and let the stimuli Z produce Gaussian ellipsoids which excite neurons that are located nearby. [sent-191, score-0.403]
</p><p>85 The optimal procedure was able to achieve equivalent reconstruction quality as a random stimulation paradigm in signiﬁcantly fewer trials when the number of stimuli per trial and response noise were in an experimentally realistic range (R = 4 and ν = 2. [sent-194, score-0.685]
</p><p>86 As the the number of stimuli per trial R increases, we start to see improved weight estimates and faster convergence but a decrease in the relative beneﬁt of optimal design; the random approach “catches up” to the optimal approach as R becomes large. [sent-197, score-0.468]
</p><p>87 4 0  200  400 trial, n  600  800  0  200  400 trial, n  600  800  Figure 3: The results of inference and optimal design (A) with a single spike-andslab prior for all connections (prior connection probability of . [sent-209, score-0.281]
</p><p>88 ) Finally, we see that we are still able to recover the synaptic strengths when we use a more general prior as in Figure 3A where we placed a single spike-and-slab prior across all the connections. [sent-216, score-0.348]
</p><p>89 Since we assumed the cells’ identities were unknown, we used a zero-centered Gaussian for the slab and a prior connection probability of . [sent-217, score-0.215]
</p><p>90 While we allow for stimulus uncertainty, it will likely soon be possible to stimulate multiple neurons with high accuracy. [sent-219, score-0.594]
</p><p>91 The algorithms proposed by [23] are based on computing a maximum a posteriori (MAP) estimate of the weights w; note that to pursue the optimal Bayesian experimental design methods proposed here, it is necessary to compute (or approximate) the full posterior distribution, not just the MAP estimate. [sent-222, score-0.352]
</p><p>92 ) In the simulated experiments of [23], stimulating roughly 30 of 500 neurons per trial is found to be optimal; extrapolating from Fig. [sent-226, score-0.672]
</p><p>93 First, the implementation of an inference algorithm which performs well on the full model such that we can recover the synaptic weights, the time constants, and the delays would allow us to avoid compressing the responses to scalar values and recover more information about the system. [sent-231, score-0.408]
</p><p>94 Also, it may be necessary to improve the noise model as we currently assume that there are no spontaneous synaptic events which will confound the determination of each connection’s strength. [sent-232, score-0.266]
</p><p>95 Nadal, “What can we learn from synaptic weight distributions? [sent-291, score-0.309]
</p><p>96 Yuste, “Stereotyped position of local synaptic targets in neocortex,” Science, vol. [sent-307, score-0.266]
</p><p>97 Reyes, “Spatial proﬁle of excitatory and inhibitory synaptic connectivity in mouse primary auditory cortex,” The Journal of Neuroscience, vol. [sent-315, score-0.575]
</p><p>98 Markram, “A synaptic organizing principle for cortical neuronal groups,” Proceedings of the National Academy of Sciences, vol. [sent-323, score-0.266]
</p><p>99 Chklovskii, “Highly nonrandom features of o o synaptic connectivity in local cortical circuits. [sent-334, score-0.402]
</p><p>100 Stephens, “Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies,” Bayesian Analysis, vol. [sent-368, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurons', 0.284), ('synaptic', 0.266), ('postsynaptic', 0.257), ('trial', 0.242), ('zn', 0.239), ('neuron', 0.228), ('variational', 0.198), ('ck', 0.179), ('stimulate', 0.173), ('presynaptic', 0.162), ('stimulating', 0.146), ('stimulus', 0.137), ('connectivity', 0.136), ('stimulation', 0.134), ('stimuli', 0.119), ('nre', 0.118), ('posterior', 0.116), ('yuste', 0.115), ('slab', 0.108), ('hardware', 0.103), ('design', 0.102), ('yn', 0.102), ('wk', 0.098), ('dnk', 0.094), ('eyn', 0.094), ('entropy', 0.093), ('inhibitory', 0.089), ('excitatory', 0.084), ('microcircuits', 0.083), ('xnk', 0.083), ('cell', 0.081), ('gibbs', 0.077), ('experimenter', 0.077), ('inference', 0.073), ('optogenetic', 0.071), ('slabs', 0.071), ('delays', 0.069), ('subthreshold', 0.062), ('stimulated', 0.061), ('neocortex', 0.054), ('experimental', 0.054), ('trials', 0.051), ('zk', 0.05), ('weights', 0.048), ('hirtz', 0.047), ('realistically', 0.047), ('spines', 0.047), ('ynt', 0.047), ('neuroscience', 0.045), ('weight', 0.043), ('trace', 0.042), ('meth', 0.042), ('prior', 0.041), ('sensing', 0.04), ('bayesian', 0.04), ('reconstruction', 0.039), ('fk', 0.039), ('marking', 0.038), ('neocortical', 0.038), ('quartiles', 0.038), ('sampling', 0.038), ('identity', 0.038), ('columbia', 0.038), ('xn', 0.038), ('cj', 0.037), ('cells', 0.036), ('charge', 0.036), ('putative', 0.036), ('ascent', 0.036), ('mapping', 0.036), ('biological', 0.035), ('brooks', 0.034), ('response', 0.034), ('realistic', 0.034), ('circuits', 0.034), ('connection', 0.033), ('chklovskii', 0.033), ('identities', 0.033), ('grossman', 0.033), ('packer', 0.033), ('event', 0.032), ('nonzero', 0.032), ('optimal', 0.032), ('nat', 0.032), ('physiology', 0.032), ('cyan', 0.032), ('dendritic', 0.032), ('designated', 0.032), ('excitation', 0.032), ('dependent', 0.031), ('ak', 0.031), ('ny', 0.031), ('liam', 0.03), ('army', 0.03), ('entropies', 0.03), ('july', 0.03), ('issues', 0.03), ('selection', 0.03), ('online', 0.03), ('transmission', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="49-tfidf-1" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>2 0.27663532 <a title="49-tfidf-2" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>3 0.25557739 <a title="49-tfidf-3" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>4 0.25300887 <a title="49-tfidf-4" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>5 0.21885498 <a title="49-tfidf-5" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>6 0.18413562 <a title="49-tfidf-6" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>7 0.16776522 <a title="49-tfidf-7" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>8 0.16746035 <a title="49-tfidf-8" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>9 0.15884729 <a title="49-tfidf-9" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>10 0.14879288 <a title="49-tfidf-10" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>11 0.1485471 <a title="49-tfidf-11" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>12 0.14235538 <a title="49-tfidf-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.13171151 <a title="49-tfidf-13" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>14 0.12367879 <a title="49-tfidf-14" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>15 0.11048979 <a title="49-tfidf-15" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>16 0.10969258 <a title="49-tfidf-16" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>17 0.10028106 <a title="49-tfidf-17" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>18 0.099060684 <a title="49-tfidf-18" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>19 0.098507538 <a title="49-tfidf-19" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>20 0.097387135 <a title="49-tfidf-20" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.1), (2, -0.163), (3, -0.09), (4, -0.39), (5, 0.025), (6, 0.028), (7, -0.092), (8, 0.116), (9, 0.089), (10, 0.05), (11, 0.121), (12, 0.01), (13, 0.001), (14, 0.003), (15, -0.062), (16, -0.049), (17, -0.045), (18, -0.029), (19, -0.064), (20, 0.002), (21, -0.02), (22, 0.002), (23, 0.143), (24, -0.076), (25, 0.093), (26, 0.021), (27, -0.013), (28, 0.003), (29, 0.048), (30, 0.007), (31, 0.03), (32, -0.005), (33, -0.011), (34, 0.035), (35, -0.02), (36, 0.031), (37, 0.008), (38, -0.052), (39, 0.044), (40, -0.006), (41, -0.04), (42, 0.06), (43, 0.029), (44, 0.044), (45, 0.035), (46, -0.037), (47, 0.071), (48, 0.025), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93817848 <a title="49-lsi-1" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>2 0.80315423 <a title="49-lsi-2" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>3 0.76578921 <a title="49-lsi-3" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>4 0.73455191 <a title="49-lsi-4" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>5 0.69249189 <a title="49-lsi-5" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>6 0.67982602 <a title="49-lsi-6" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>7 0.64705414 <a title="49-lsi-7" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>8 0.64462686 <a title="49-lsi-8" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>9 0.63756669 <a title="49-lsi-9" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>10 0.61004907 <a title="49-lsi-10" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>11 0.59862632 <a title="49-lsi-11" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>12 0.59112954 <a title="49-lsi-12" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>13 0.55489653 <a title="49-lsi-13" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>14 0.55269849 <a title="49-lsi-14" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>15 0.5359717 <a title="49-lsi-15" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>16 0.53328323 <a title="49-lsi-16" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>17 0.51213014 <a title="49-lsi-17" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>18 0.46367034 <a title="49-lsi-18" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>19 0.46273252 <a title="49-lsi-19" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>20 0.45090726 <a title="49-lsi-20" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.047), (33, 0.136), (34, 0.111), (41, 0.036), (49, 0.071), (56, 0.079), (70, 0.083), (85, 0.03), (86, 0.214), (89, 0.069), (93, 0.027), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81033027 <a title="49-lda-1" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>2 0.79656613 <a title="49-lda-2" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>3 0.78796226 <a title="49-lda-3" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>Author: Remi Gribonval, Pierre Machart</p><p>Abstract: There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efﬁciency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization. 1</p><p>4 0.74174786 <a title="49-lda-4" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>5 0.71604347 <a title="49-lda-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.71536309 <a title="49-lda-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.70366371 <a title="49-lda-7" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>8 0.70266104 <a title="49-lda-8" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>9 0.69969165 <a title="49-lda-9" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>10 0.6983223 <a title="49-lda-10" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>11 0.69573694 <a title="49-lda-11" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>12 0.69414073 <a title="49-lda-12" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>13 0.69166905 <a title="49-lda-13" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>14 0.69142854 <a title="49-lda-14" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>15 0.69114244 <a title="49-lda-15" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>16 0.68992633 <a title="49-lda-16" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>17 0.68896347 <a title="49-lda-17" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>18 0.68798947 <a title="49-lda-18" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>19 0.68753177 <a title="49-lda-19" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>20 0.68623531 <a title="49-lda-20" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
