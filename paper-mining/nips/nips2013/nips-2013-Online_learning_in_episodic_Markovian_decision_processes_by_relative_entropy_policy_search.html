<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-235" href="#">nips2013-235</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</h1>
<br/><p>Source: <a title="nips-2013-235-pdf" href="http://papers.nips.cc/paper/4974-online-learning-in-episodic-markovian-decision-processes-by-relative-entropy-policy-search.pdf">pdf</a></p><p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>Reference: <a title="nips-2013-235-reference" href="../nips2013_reference/nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. [sent-6, score-0.338]
</p><p>2 The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. [sent-7, score-0.494]
</p><p>3 We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. [sent-8, score-0.447]
</p><p>4 1  Introduction  In this paper, we study the problem of online learning in a class of ﬁnite non-stationary episodic Markov decision processes. [sent-11, score-0.288]
</p><p>5 The learning problem that we consider can be formalized as a sequential interaction between a learner (often called agent) and an environment, where the interaction between the two entities proceeds in episodes. [sent-12, score-0.265]
</p><p>6 Every episode consists of multiple time steps: In every time step of an episode, a learner has to choose one of its available actions after observing some part of the current state of the environment. [sent-13, score-0.443]
</p><p>7 The chosen action inﬂuences the observable state of the environment in a stochastic fashion and imposes some loss on the learner. [sent-14, score-0.222]
</p><p>8 However, the entire state (be it observed or not) also inﬂuences the loss. [sent-15, score-0.057]
</p><p>9 The goal of the learner is to minimize its total (non-discounted) loss that it suffers. [sent-16, score-0.243]
</p><p>10 In this work, we assume that the unobserved part of the state evolves autonomously from the observed part of the state or the actions chosen by the learner, thus corresponding to a state sequence generated by an oblivious adversary such as nature. [sent-17, score-0.22]
</p><p>11 As usual for such learning problems, we set our goal as minimizing the regret deﬁned as the difference between the total loss suffered by the learner and the total loss of the best stationary state-feedback policy. [sent-19, score-0.512]
</p><p>12 This setting fuses two important paradigms of learning theory: online learning [5] and reinforcement learning [21, 22]. [sent-20, score-0.122]
</p><p>13 The learning problem outlined above can be formalized as an online learning problem where the actions of the learner correspond to choosing policies in a known Markovian decision process where the loss function changes arbitrarily between episodes. [sent-21, score-0.507]
</p><p>14 In their variant of the problem, the learner faces a continuing MDP task where all policies are assumed to generate a unique stationary distribution over the state space and losses can change arbitrarily between consecutive time steps. [sent-28, score-0.463]
</p><p>15 Assuming that the learner observes the complete loss function after each time step (that is, assuming full information feedback), they propose an algorithm called MDP-E and show that its regret is O(τ 2 T log |A|), where τ > 0 is an upper bound on the mixing time of any policy. [sent-29, score-0.484]
</p><p>16 The core idea of MDP-E is the observation that the regret of the global decision problem can be decomposed into regrets of simpler decision problems deﬁned in each state. [sent-30, score-0.338]
</p><p>17 [23] consider the same setting and propose an algorithm that guarantees o(T ) regret under bandit feedback where the learner only observes the losses that it actually suffers, but not the whole loss function. [sent-32, score-0.669]
</p><p>18 [16] propose an algorithm that is shown to enjoy an O(T 2/3 ) bound on the regret in the bandit setting, given some further assumptions concerning the transition structure of the underlying MDP. [sent-35, score-0.354]
</p><p>19 [14], who consider online learning in episodic MDPs where the state space has a layered (or loop-free) structure and every policy visits every state with a positive probability of at least α > 0. [sent-38, score-0.556]
</p><p>20 [9], and is sufﬁcient to prove a regret bound of O(L2 T |A| log |A|/α) in the bandit case and O(L2 T log |A|) in the full information case. [sent-40, score-0.358]
</p><p>21 In this paper, we present a learning algorithm that directly aims to minimize the global regret of the algorithm instead of trying to minimize the local regrets in a decomposed problem. [sent-41, score-0.194]
</p><p>22 [17] and Kakade [11] point out, good performance of policy search algorithms requires that the information loss between the consecutive policies selected by the algorithm is bounded, so that policies are only modiﬁed in small steps. [sent-45, score-0.38]
</p><p>23 Accordingly, REPS aims to select policies that minimize the expected loss while guaranteeing that the state-action distributions generated by the policies stay close in terms of Kullback–Leibler divergence. [sent-46, score-0.205]
</p><p>24 [6] point out that REPS is closely related to a number of previously known probabilistic policy search methods. [sent-48, score-0.175]
</p><p>25 We propose a variant of REPS called online REPS or O-REPS and analyze it using fundamental results concerning the PPA family. [sent-50, score-0.132]
</p><p>26 Our analysis improves all previous results concerning online learning in episodic MDPs: we show that the expected regret of O-REPS is bounded by 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting. [sent-51, score-0.554]
</p><p>27 , x, a) and indeﬁnite sums over states and actions are to be understood as sums over the entire state and action spaces. [sent-57, score-0.146]
</p><p>28 We will assume that M satisﬁes the following assumptions: • The state space X can be decomposed into non-intersecting layers, i. [sent-61, score-0.08]
</p><p>29 The interaction between the learner and the environment is described on Figure 1. [sent-69, score-0.293]
</p><p>30 The interaction of an agent and the Markovian environment proceeds in episodes, where in each episode the agent starts in state x0 and moves forward across the consecutive layers until it reaches state xL . [sent-70, score-0.517]
</p><p>31 1 We T assume that the environment selects a sequence of loss functions { t }t=1 and the losses only change between episodes. [sent-71, score-0.199]
</p><p>32 Furthermore, we assume that the learner only observes the losses that it suffers in each individual state-action pair that it visits, in other words, we consider bandit feedback. [sent-72, score-0.446]
</p><p>33 2 Parameters: Markovian environment M = {X , A, P }; For all episodes t = 1, 2, . [sent-73, score-0.116]
</p><p>34 The environment chooses the loss function  t  : X × A → [0, 1]. [sent-77, score-0.125]
</p><p>35 , L − 1, repeat (a) (b) (c) (d)  The learner observes xl (t) ∈ Xl . [sent-84, score-0.423]
</p><p>36 Based on its previous observations (and randomness), the learner selects al (t). [sent-85, score-0.263]
</p><p>37 The learner suffers and observes loss t (xl (t), al (t)). [sent-86, score-0.36]
</p><p>38 The environment draws the new state xl+1 (t) ∼ P (·|xl (t), al (t)). [sent-87, score-0.175]
</p><p>39 Figure 1: The protocol of online learning in episodic MDPs. [sent-88, score-0.216]
</p><p>40 To this end, we deﬁne the concept of (stochastic stationary) policies: A policy is deﬁned as a mapping π : A × X → [0, 1], where π(a|x) gives the probability of selecting action a in state x. [sent-90, score-0.272]
</p><p>41 The expected total loss of a policy π is deﬁned as T  L−1 t (xk , ak )  LT (π) = E  P, π ,  t=1 k=0  where the notation E [ ·| P, π] is used to emphasize that the random variables xk and ak are generated by executing π in the MDP speciﬁed by the transition function P . [sent-91, score-0.561]
</p><p>42 Denote the total expected T L−1 loss suffered by the learner as LT = t=1 k=0 E [ t (xk (t), ak (t))| P ], where the expectation is taken over the internal randomization of the learner and the random transitions of the Markovian environment. [sent-92, score-0.558]
</p><p>43 Using these notations, we deﬁne the learner’s goal as minimizing the (total expected) regret deﬁned as RT = LT − min LT (π), π  where the minimum is taken over the complete set of stochastic stationary policies. [sent-93, score-0.177]
</p><p>44 It is easy to see that the occupancy measure of any policy π satisﬁes q π (x, a) = a  P (x|x , a )q π (x , a ),  (1)  x ∈Xk(x)−1 a  1 Such MDPs naturally arise in episodic decision tasks where some notion of time is present in the state description. [sent-95, score-0.681]
</p><p>45 2 In the literature of online combinatorial optimization, this feedback scheme is often called semi-bandit feedback, see Audibert et al. [sent-96, score-0.208]
</p><p>46 3  for all x ∈ X \ {x0 , xl }, with q π (x0 , a) = π(a|x0 ) for all a ∈ A. [sent-99, score-0.184]
</p><p>47 The set of all occupancy measures satisfying the above equality in the MDP M will be denoted as ∆(M ). [sent-100, score-0.281]
</p><p>48 The policy π is said to generate the occupancy measure q ∈ ∆(M ) if q(x, a) b q(x, b)  π(a|x) =  holds for all (x, a) ∈ X × A. [sent-101, score-0.426]
</p><p>49 It is clear that there exists a unique generating policy for all measures in ∆(M ) and vice versa. [sent-102, score-0.205]
</p><p>50 In what follows, we will redeﬁne the task of the learner from having to select individual actions ak (t) to having to select occupancy measures qt ∈ ∆(M ) in each episode t. [sent-104, score-1.361]
</p><p>51 Using this notation, we can reformulate our original problem as an instance of online linear optimization with decision space ∆(M ). [sent-106, score-0.184]
</p><p>52 Assuming that the learner selects occupancy measure qt in episode t, the regret can be rewritten as T  qt − q,  RT = max E q∈∆(M )  3  . [sent-107, score-2.032]
</p><p>53 O-REPS is an instance of online linear optimization methods usually referred to as Follow-the-Regularized-Leader (FTRL), Online Stochastic Mirror Descent (OSMD) or the Proximal Point Algorithm (PPA)—see, e. [sent-109, score-0.112]
</p><p>54 First, deﬁne D (q q ) as the unnormalized Kullback– Leibler divergence between two occupancy measures q and q : D (q q ) =  q(x, a) log x,a  q(x, a) − q (x, a)  (q(x, a) − q (x, a)) . [sent-115, score-0.335]
</p><p>55 x,a  Furthermore, let R(q) deﬁne the unnormalized negative entropy of the occupancy measure q: q(x, a) log q(x, a) −  R(q) = x,a  q(x, a). [sent-116, score-0.356]
</p><p>56 In the ﬁrst episode, O-REPS chooses the uniform policy with π1 (a|x) = 1/|A| for all x and a, and we let q1 = q π1 . [sent-118, score-0.175]
</p><p>57 4 Then, the algorithm proceeds recursively: After observing ut = (x0 (t), a0 (t), t (x0 (t), a0 (t)), . [sent-119, score-0.111]
</p><p>58 , xL−1 (t), aL−1 (t), t (xL−1 (t), aL−1 (t)), xL (t)) in episode t, we deﬁne the loss estimates ˆt as ˆt =  t (x, a) I {(x, a) ∈ ut } , qt (x, a)  where we used the notation (x, a) ∈ ut to indicate that the state-action pair (x, a) was observed during episode t. [sent-122, score-1.155]
</p><p>59 After episode t, O-REPS selects the occupancy measure that solves the optimization problem qt+1 = arg min η q, ˆt + D(q||qt ) . [sent-123, score-0.466]
</p><p>60 4  In episode t, our algorithm follows the policy πt = π qt . [sent-125, score-0.958]
</p><p>61 , ut ), we clearly have that qt (x, a) = P [ (x, a) ∈ ut | Ut−1 ], so ˆt (x, a) is an unbiased estimate of t (x, a) for all (x, a) such that qt (x, a) > 0: E ˆt (x, a) Ut−1 =  t (x, a) P [ (x, a) ∈ ut | Ut−1 ] = qt (x, a)  t (x, a). [sent-129, score-2.184]
</p><p>62 (4)  We now proceed to explain how the policy update step (3) can be implemented efﬁciently. [sent-130, score-0.175]
</p><p>63 6]) that performing this optimization can be reformulated o as ﬁrst solving the unconstrained optimization problem ˜ qt+1 = arg min η q, ˆt + D(q||qt ) q  and then projecting the result to ∆(M ) as ˜ qt+1 = arg min D (q qt+1 ) . [sent-135, score-0.088]
</p><p>64 q∈∆(M ) ˆ  ˜ The ﬁrst step can be simply carried out by setting qt+1 (x, a) = qt (x, a)e−η t (x,a) . [sent-136, score-0.639]
</p><p>65 Let t > 1 and deﬁne the function qt (x, a)eδ(x,a|v,  Zt (v, k) =  ˆt )  . [sent-144, score-0.639]
</p><p>66 x∈Xk ,a∈A  The update step (3) can be performed as qt+1 (x, a) =  v qt (x, a)eδ(x,a|ˆ t , Zt (ˆ t , k(x)) v  ˆt )  ,  where L  ˆ vt = arg min v  ln Zt (v, k). [sent-145, score-0.711]
</p><p>67 (6)  k=0  Minimizing the expression on the right-hand side of Equation (6) is an unconstrained convex optimization problem (see Boyd and Vandenberghe [4] and the comments of Peters et al. [sent-146, score-0.078]
</p><p>68 It is important to note that since q1 (x, a) > 0 holds for all (x, a) pairs, qt (x, a) is also positive for all t > 0 by the multiplicative update rule, so Equation 4 holds for all state-action pairs (x, a) in all time steps. [sent-148, score-0.639]
</p><p>69 ˜ Substituting the formula for qt+1 (x, a), we get qt+1 (x, a) = qt (x, a)e−λk(x) +δ(x,a|v,  ˆt )  . [sent-161, score-0.639]
</p><p>70 , L − 1 that qt (x, a)e−λk +δ(x,a|v, x∈Xk  ˆt )  = 1,  a  yielding e−λk = 1/Zt (v, k), which leaves us with computing the value of v at the optimum. [sent-165, score-0.639]
</p><p>71 4  Analysis  The next theorem states our main result concerning the regret of O-REPS under bandit feedback. [sent-168, score-0.313]
</p><p>72 After proving the theorem, we also present the regret bound for O-REPS when used in a full information setting where the learner gets to observe t after each episode t. [sent-172, score-0.501]
</p><p>73 Assuming bandit feedback, the total expected regret of O-REPS satisﬁes RT ≤ η|X ||A|T +  L log  |X ||A| L  η  . [sent-174, score-0.302]
</p><p>74 |X ||A|  In particular, setting η =  L  log L T |X ||A|  yields  RT ≤ 2  L|X ||A|T log  6  |X ||A| . [sent-175, score-0.062]
</p><p>75 1]), we have T  T  qt − q, ˆt ≤  ˜ qt − qt+1 , ˆt +  t=1  t=1  D (q q1 ) . [sent-181, score-1.278]
</p><p>76 η  (7)  ˜ Using the exact form of qt+1 and the fact that ex ≥ 1 + x, we get that ˜ qt+1 (x, a) ≥ qt (x, a) − ηqt (x, a) ˆt (x, a) and thus T  T  ˜ qt − qt+1 , ˆt ≤ η  qt (x, a) ˆ2 (x, a) t t=1 x,a  t=1  T  ≤η  qt (x, a) t=1 x,a  t (x, a)  qt (x, a)  T  ˆt (x, a) ≤ η  ˆt (x, a). [sent-182, score-3.195]
</p><p>77 t=1 x,a  Combining this with (7), we get T  T  qt − q, ˆt ≤ η t=1 x,a  t=1  ˆt (x, a) + D (q q1 ) . [sent-183, score-0.639]
</p><p>78 E t=1 x,a  It also follows from Equation (4) that E notice that  q, ˆt  = q,  t  and E  qt , ˆt  L−1  D (q q1 ) ≤R(q) − R(q1 ) ≤  q1 (x, a) log a  k=0 x∈Xk  = E [ qt ,  t  ]. [sent-186, score-1.309]
</p><p>79 Finally,  1 q1 (x, a)  (since R(q) ≤ 0) L−1  ≤  log |Xk ||A| ≤ L log k=0  |X ||A| , L  where we used the trivial upper bound on the entropy of distributions and Jensen’s inequality in the last step. [sent-187, score-0.113]
</p><p>80 Assuming full feedback, the total expected regret of O-REPS satisﬁes RT ≤ ηLT + log  In particular, setting η =  |X ||A| L  T  |X ||A| L  L log  η  . [sent-190, score-0.226]
</p><p>81 7  5  Conclusions and future work  Comparison with previous results We ﬁrst compare our regret bounds with previous results from the literature. [sent-193, score-0.139]
</p><p>82 First, our guarantees for the full information case trade off a factor of L present in the bounds of Neu et al. [sent-194, score-0.081]
</p><p>83 More importantly, our bounds trade off a factor of L3/2 /α in the bandit case to a factor of |X |. [sent-196, score-0.132]
</p><p>84 This improvement is particularly remarkable considering that we do not need to assume that α > 0, that is, we drop the rather unnatural assumption that every stationary policy has to visit every state with positive probability. [sent-197, score-0.27]
</p><p>85 In particular, dropping this assumption enables our algorithm to work in deterministic loop-free MDPs, that is, to solve the online shortest path problem (see, e. [sent-198, score-0.158]
</p><p>86 In the shortest path setting, O-REPS provides an alternative implementation to the Component Hedge algorithm analyzed by Koolen et al. [sent-201, score-0.124]
</p><p>87 [2], Component Hedge achieves the analog of our bounds in the bandit case as well. [sent-204, score-0.132]
</p><p>88 [16] who also use policy updates of the form πt+1 (a|x) ∝ πt (a|x) exp(−η t (x, a) − x P (x |x, a)vt (x )). [sent-207, score-0.175]
</p><p>89 [9], it is possible to √ show that O-REPS attains a regret of O( τ T ) in the unichain setting with full information feedback, improving their bound by a factor of τ 3/2 under the same assumptions. [sent-210, score-0.227]
</p><p>90 Another important direction of future work is to extend our results to the case of unichain MDPs with bandit feedback and the setting where the transition probabilities of the underlying MDP is unknown (see Neu et al. [sent-212, score-0.354]
</p><p>91 [2], it is straightforward to construct an MDP consisting of |X |/L chains of L consecutive bandit problems each with |A| actions such that no algorithm can achieve smaller regret than 0. [sent-215, score-0.369]
</p><p>92 These results suggest that our bounds cannot be signiﬁcantly improved in general, however, ﬁnding an appropriate problem-dependent lower bound remains an interesting open problem in the much broader ﬁeld of online linear optimization. [sent-218, score-0.09]
</p><p>93 aim to solve the optimization problem qt+1 = arg minq∈∆(M ) q, ˆt subject to the constraint D (q qt ) ≤ ε for some ε > 0. [sent-223, score-0.683]
</p><p>94 This is to be contrasted with the following property of the occupancy measures generated by O-REPS (proved in the supplementary material): Lemma 1. [sent-224, score-0.281]
</p><p>95 t  In particular, if the losses are estimated by bounded sample averages as done by Peters et al. [sent-226, score-0.103]
</p><p>96 While this is not the exact same property as desired by REPS, both inequalities imply that the occupancy measures stay close to each other in the 1-norm sense by Pinsker’s inequality. [sent-228, score-0.306]
</p><p>97 Competing in the dark: An efﬁcient algorithm for bandit linear optimization. [sent-238, score-0.132]
</p><p>98 The on-line shortest path probo a lem under partial monitoring. [sent-305, score-0.068]
</p><p>99 The adversarial stochastic shortest path o a problem with unknown transition probabilities. [sent-333, score-0.109]
</p><p>100 Online Markov decision proo a cesses under bandit feedback. [sent-340, score-0.204]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qt', 0.639), ('occupancy', 0.251), ('reps', 0.251), ('learner', 0.193), ('xl', 0.184), ('neu', 0.176), ('policy', 0.175), ('xk', 0.16), ('episode', 0.144), ('regret', 0.139), ('peters', 0.138), ('bandit', 0.132), ('episodic', 0.126), ('online', 0.09), ('ut', 0.089), ('lt', 0.085), ('mdp', 0.079), ('environment', 0.075), ('decision', 0.072), ('markovian', 0.066), ('gy', 0.064), ('unichain', 0.063), ('zimin', 0.063), ('mdps', 0.063), ('feedback', 0.062), ('rgy', 0.061), ('audibert', 0.058), ('state', 0.057), ('et', 0.056), ('ppa', 0.055), ('gergely', 0.055), ('szepesv', 0.055), ('ak', 0.055), ('policies', 0.053), ('entropy', 0.051), ('loss', 0.05), ('actions', 0.049), ('consecutive', 0.049), ('losses', 0.047), ('observes', 0.046), ('zt', 0.043), ('al', 0.043), ('rt', 0.043), ('suffered', 0.042), ('martinet', 0.042), ('shortest', 0.042), ('concerning', 0.042), ('transition', 0.041), ('episodes', 0.041), ('action', 0.04), ('stationary', 0.038), ('kakade', 0.037), ('budapest', 0.037), ('ri', 0.035), ('koolen', 0.034), ('lugosi', 0.033), ('leibler', 0.032), ('kullback', 0.032), ('regrets', 0.032), ('reinforcement', 0.032), ('layers', 0.032), ('log', 0.031), ('puterman', 0.03), ('measures', 0.03), ('alexander', 0.029), ('formalization', 0.029), ('hedge', 0.029), ('vandenberghe', 0.029), ('suffers', 0.028), ('markov', 0.028), ('agent', 0.028), ('hazan', 0.027), ('selects', 0.027), ('proximal', 0.027), ('path', 0.026), ('layered', 0.026), ('continuing', 0.026), ('bart', 0.026), ('vt', 0.026), ('visits', 0.025), ('stay', 0.025), ('rakhlin', 0.025), ('transitions', 0.025), ('interaction', 0.025), ('full', 0.025), ('executing', 0.025), ('guaranteeing', 0.024), ('ln', 0.024), ('decomposed', 0.023), ('uences', 0.023), ('programme', 0.023), ('unnormalized', 0.023), ('proceeds', 0.022), ('colt', 0.022), ('arg', 0.022), ('bellman', 0.022), ('dekel', 0.022), ('mansour', 0.022), ('optimization', 0.022), ('inria', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="235-tfidf-1" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>2 0.30755737 <a title="235-tfidf-2" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>3 0.24139625 <a title="235-tfidf-3" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>Author: Navid Zolghadr, Gabor Bartok, Russell Greiner, András György, Csaba Szepesvari</p><p>Abstract: This paper introduces the online probing problem: In each round, the learner is able to purchase the values of a subset of feature values. After the learner uses this information to come up with a prediction for the given round, he then has the option of paying to see the loss function that he is evaluated against. Either way, the learner pays for both the errors of his predictions and also whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features. We consider two variations of this problem, depending on whether the learner can observe the label for free or not. We provide algorithms and upper and lower bounds on the regret for both variants. We show that a positive cost for observing the label signiﬁcantly increases the regret of the problem. 1</p><p>4 0.23869917 <a title="235-tfidf-4" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>5 0.2342755 <a title="235-tfidf-5" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>6 0.18566148 <a title="235-tfidf-6" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>7 0.17268811 <a title="235-tfidf-7" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>8 0.16697223 <a title="235-tfidf-8" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>9 0.16415147 <a title="235-tfidf-9" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>10 0.15304202 <a title="235-tfidf-10" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>11 0.15122584 <a title="235-tfidf-11" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>12 0.14051682 <a title="235-tfidf-12" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>13 0.13492982 <a title="235-tfidf-13" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>14 0.12016901 <a title="235-tfidf-14" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>15 0.1150832 <a title="235-tfidf-15" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>16 0.11312157 <a title="235-tfidf-16" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>17 0.11268414 <a title="235-tfidf-17" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>18 0.10773587 <a title="235-tfidf-18" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>19 0.10731881 <a title="235-tfidf-19" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>20 0.10706975 <a title="235-tfidf-20" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, -0.317), (2, 0.049), (3, -0.015), (4, -0.014), (5, -0.068), (6, -0.004), (7, 0.012), (8, -0.001), (9, -0.035), (10, -0.015), (11, -0.037), (12, 0.074), (13, 0.027), (14, 0.102), (15, -0.085), (16, 0.014), (17, -0.055), (18, 0.007), (19, -0.033), (20, -0.09), (21, -0.03), (22, -0.014), (23, 0.014), (24, 0.078), (25, 0.05), (26, -0.073), (27, -0.04), (28, 0.015), (29, 0.06), (30, -0.048), (31, 0.168), (32, -0.03), (33, 0.098), (34, -0.161), (35, -0.037), (36, -0.122), (37, -0.073), (38, -0.018), (39, -0.033), (40, -0.062), (41, 0.086), (42, -0.034), (43, -0.003), (44, 0.164), (45, 0.046), (46, -0.06), (47, -0.057), (48, -0.041), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9587459 <a title="235-lsi-1" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>2 0.8050319 <a title="235-lsi-2" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>3 0.75185603 <a title="235-lsi-3" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>4 0.71078062 <a title="235-lsi-4" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>5 0.70057356 <a title="235-lsi-5" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>Author: Asrar Ahmed, Pradeep Varakantham, Yossiri Adulyasak, Patrick Jaillet</p><p>Abstract: In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.</p><p>6 0.6745916 <a title="235-lsi-6" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>7 0.64990938 <a title="235-lsi-7" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>8 0.5383718 <a title="235-lsi-8" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>9 0.52110803 <a title="235-lsi-9" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>10 0.51944888 <a title="235-lsi-10" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>11 0.51535213 <a title="235-lsi-11" href="./nips-2013-Learning_Prices_for_Repeated_Auctions_with_Strategic_Buyers.html">159 nips-2013-Learning Prices for Repeated Auctions with Strategic Buyers</a></p>
<p>12 0.50903207 <a title="235-lsi-12" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>13 0.50463116 <a title="235-lsi-13" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>14 0.47934264 <a title="235-lsi-14" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>15 0.47225583 <a title="235-lsi-15" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>16 0.46497756 <a title="235-lsi-16" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>17 0.46128413 <a title="235-lsi-17" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>18 0.4612262 <a title="235-lsi-18" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>19 0.45591474 <a title="235-lsi-19" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>20 0.45358598 <a title="235-lsi-20" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.225), (2, 0.023), (16, 0.025), (33, 0.108), (34, 0.094), (36, 0.015), (41, 0.058), (49, 0.022), (56, 0.176), (70, 0.044), (85, 0.06), (89, 0.028), (93, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90501338 <a title="235-lda-1" href="./nips-2013-Estimation_Bias_in_Multi-Armed_Bandit_Algorithms_for_Search_Advertising.html">112 nips-2013-Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising</a></p>
<p>Author: Min Xu, Tao Qin, Tie-Yan Liu</p><p>Abstract: In search advertising, the search engine needs to select the most proﬁtable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit (MAB) problem. In this paper, we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and “estimation of the largest mean” (ELM) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with beneﬁts to both the search engine and the advertisers. 1</p><p>same-paper 2 0.84451973 <a title="235-lda-2" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>3 0.82677674 <a title="235-lda-3" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>Author: Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund</p><p>Abstract: We consider a situation in which we see samples Xn ∈ Rd drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give ﬁnite-sample convergence rates for both. 1</p><p>4 0.81266117 <a title="235-lda-4" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>Author: Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram</p><p>Abstract: Hypergraphs allow one to encode higher-order relationships in data and are thus a very ﬂexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs. 1</p><p>5 0.74345464 <a title="235-lda-5" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>Author: Tianbao Yang</p><p>Abstract: We present and study a distributed optimization algorithm by employing a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between computation and communication. We verify our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances. 1</p><p>6 0.73176223 <a title="235-lda-6" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>7 0.73079985 <a title="235-lda-7" href="./nips-2013-Learning_Prices_for_Repeated_Auctions_with_Strategic_Buyers.html">159 nips-2013-Learning Prices for Repeated Auctions with Strategic Buyers</a></p>
<p>8 0.72317457 <a title="235-lda-8" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>9 0.72053546 <a title="235-lda-9" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>10 0.71796972 <a title="235-lda-10" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>11 0.71613026 <a title="235-lda-11" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>12 0.71408218 <a title="235-lda-12" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>13 0.71218842 <a title="235-lda-13" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>14 0.71077806 <a title="235-lda-14" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>15 0.71052164 <a title="235-lda-15" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>16 0.70982403 <a title="235-lda-16" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>17 0.70936847 <a title="235-lda-17" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>18 0.70854664 <a title="235-lda-18" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>19 0.70829874 <a title="235-lda-19" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>20 0.70794994 <a title="235-lda-20" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
