<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-103" href="#">nips2013-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</h1>
<br/><p>Source: <a title="nips-2013-103-pdf" href="http://papers.nips.cc/paper/4972-efficient-exploration-and-value-function-generalization-in-deterministic-systems.pdf">pdf</a></p><p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>Reference: <a title="nips-2013-103-reference" href="../nips2013_reference/nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. [sent-3, score-1.048]
</p><p>2 We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. [sent-4, score-0.525]
</p><p>3 We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. [sent-5, score-0.248]
</p><p>4 1  Introduction  A growing body of work on efﬁcient reinforcement learning provides algorithms with guarantees on sample and computational efﬁciency [13, 6, 2, 22, 4, 9]. [sent-6, score-0.383]
</p><p>5 This literature highlights the point that an effective exploration scheme is critical to the design of any efﬁcient reinforcement learning algorithm. [sent-7, score-0.429]
</p><p>6 In particular, popular exploration schemes such as ✏-greedy, Boltzmann, and knowledge gradient can require learning times that grow exponentially in the number of states and/or the planning horizon. [sent-8, score-0.228]
</p><p>7 The aforementioned literature focuses on tabula rasa learning; that is, algorithms aim to learn with little or no prior knowledge about transition probabilities and rewards. [sent-9, score-0.4]
</p><p>8 Such algorithms require learning times that grow at least linearly with the number of states. [sent-10, score-0.037]
</p><p>9 Despite the valuable insights that have been generated through their design and analysis, these algorithms are of limited practical import because state spaces in most contexts of practical interest are enormous. [sent-11, score-0.318]
</p><p>10 There is a need for algorithms that generalize from past experience in order to learn how to make effective decisions in reasonable time. [sent-12, score-0.117]
</p><p>11 There has been much work on reinforcement learning algorithms that generalize (see, e. [sent-13, score-0.293]
</p><p>12 Most of these algorithms do not come with statistical or computational efﬁciency guarantees, though there are a few noteworthy exceptions, which we now discuss. [sent-16, score-0.05]
</p><p>13 Though interesting results have been produced in this line of work, each entails quite restrictive assumptions or does not make strong guarantees. [sent-18, score-0.036]
</p><p>14 An algorithm is proposed in [12] that ﬁts a factored model to observed data and makes decisions based on the ﬁtted model. [sent-20, score-0.098]
</p><p>15 The authors establish a sample complexity bound that is polynomial in the number of model parameters rather than the number of states, but the algorithm is computationally intractable because of the difﬁculty of solving factored MDPs. [sent-21, score-0.164]
</p><p>16 Though this result is theoretically interesting, for most model classes of interest, the ✏-covering-number is enormous since it typically grows exponentially in the number of free parameters. [sent-23, score-0.134]
</p><p>17 Another recent paper [17] establishes a regret bound for an algorithm that applies to problems with continuous state spaces and H¨ lder-continuous rewards and transition kernels. [sent-24, score-0.437]
</p><p>18 o Though the results represent an interesting contribution to the literature, a couple features of the regret bound weaken its practical implications. [sent-25, score-0.335]
</p><p>19 First, regret grows linearly with the H¨ lder constant o of the transition kernel, which for most contexts of practical relevance grows exponentially in the number of state variables. [sent-26, score-0.629]
</p><p>20 Second, the dependence on time becomes arbitrarily close to linear as the dimension of the state space grows. [sent-27, score-0.108]
</p><p>21 Reinforcement learning in linear systems with quadratic cost is treated in [1]. [sent-28, score-0.038]
</p><p>22 The method proposed is shown to realize regret that grows with the square root of time. [sent-29, score-0.199]
</p><p>23 Here, there are efﬁciency guarantees that scale gracefully with the number of state variables, but only under sparsity and other technical assumptions. [sent-32, score-0.221]
</p><p>24 The most popular approach to generalization in the applied reinforcement learning literature involves ﬁtting parameterized value functions. [sent-33, score-0.361]
</p><p>25 Such approaches relate closely to supervised learning in that they learn functions from state to value, though a difference is that value is inﬂuenced by action and observed only through delayed feedback. [sent-34, score-0.399]
</p><p>26 One advantage over model learning approaches is that, given a ﬁtted value function, decisions can be made without solving a potentially intractable control problem. [sent-35, score-0.053]
</p><p>27 We see this as a promising direction, though there currently is a lack of theoretical results that provide attractive bounds on learning time with value function generalization. [sent-36, score-0.05]
</p><p>28 A relevant paper along this research line is [15], which studies the efﬁcient reinforcement learning with value function generalization in the KWIK framework (see [16]), and reduces the efﬁcient reinforcement learning problem to the efﬁcient KWIK online regression problem. [sent-37, score-0.621]
</p><p>29 Thus, though the result of [15] is interesting, it does not provide a provably efﬁcient algorithm. [sent-39, score-0.083]
</p><p>30 An important challenge that remains is to couple exploration and value function generalization in a provably effective way, and in particular, to establish sample and computational efﬁciency guarantees that scale gracefully with the planning horizon and model complexity. [sent-40, score-0.549]
</p><p>31 To start with a simple context, we restrict our attention to deterministic systems that evolve over ﬁnite time horizons, and we consider episodic learning, in which an agent repeatedly interacts with the same system. [sent-42, score-0.278]
</p><p>32 As a solution to the problem, we propose optimistic constraint propagation (OCP), a computationally efﬁcient reinforcement learning algorithm designed to synthesize efﬁcient exploration and value function generalization. [sent-43, score-0.749]
</p><p>33 We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes. [sent-44, score-0.406]
</p><p>34 Here, dimE denotes the eluder dimension, which quantiﬁes complexity of the hypothesis class. [sent-45, score-0.227]
</p><p>35 A corollary of this result is that regret is bounded by a function that is constant over time and linear in the problem horizon and eluder dimension. [sent-46, score-0.368]
</p><p>36 To put our aforementioned result in perspective, it is useful to relate it to other lines of work. [sent-47, score-0.104]
</p><p>37 Consider ﬁrst the broad area of reinforcement learning algorithms that ﬁt value functions, such as SARSA [19]. [sent-48, score-0.293]
</p><p>38 Even with the most commonly used sort of hypothesis class Q, which is made up of linear combinations of ﬁxed basis functions, and even when the hypothesis class contains the true value function Q⇤ , there are no guarantees that these algorithms will efﬁciently learn to make near-optimal decisions. [sent-49, score-0.347]
</p><p>39 On the other hand, our result implies that OCP attains near-optimal performance in time that scales linearly with the number of basis functions. [sent-50, score-0.044]
</p><p>40 Now consider the more specialized context of a deterministic linear system with quadratic cost and a ﬁnite time horizon. [sent-51, score-0.162]
</p><p>41 The analysis of [1] can be leveraged to produce regret bounds that scale exponentially in the number of state variables. [sent-52, score-0.309]
</p><p>42 On the other hand, using a hypothesis space Q consisting of quadratic functions of state-action pairs, the results of this paper show that OCP behaves near optimally within time that scales quadratically in the number of state and action variables. [sent-53, score-0.457]
</p><p>43 We also establish efﬁciency and asymptotic performance guarantees that apply to agnostic reinforcement learning, where Q⇤ does not necessarily lie in Q. [sent-54, score-0.544]
</p><p>44 In particular, we consider the case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. [sent-55, score-0.065]
</p><p>45 Our results here add to the literature on agnostic reinforcement learning with such a hypothesis class [21, 25, 7, 26]. [sent-56, score-0.541]
</p><p>46 Prior work in 2  this area has produced interesting algorithms and insights, as well as bounds on performance loss associated with potential limits of convergence, but no convergence or efﬁciency guarantees. [sent-57, score-0.036]
</p><p>47 2  Reinforcement Learning in Deterministic Systems  In this paper, we consider an episodic reinforcement learning (RL) problem in which an agent repeatedly interacts with a discrete-time ﬁnite-horizon deterministic system, and refer to each interaction as an episode. [sent-58, score-0.571]
</p><p>48 The system is identiﬁed by a sextuple M = (S, A, H, F, R, S), where S is the state space, A is the action space, H is the horizon, F is a system function, R is a reward function and S is a sequence of states. [sent-59, score-0.541]
</p><p>49 If action a 2 A is selected while the system is in state x 2 S at period t = 0, 1, · · · , H 1, a reward of Rt (x, a) is realized; furthermore, if t < H 1, the state transitions to Ft (x, a). [sent-60, score-0.72]
</p><p>50 Each episode terminates at period H 1, and then a new episode begins. [sent-61, score-0.75]
</p><p>51 The initial state of episode j is the jth element of S. [sent-62, score-0.481]
</p><p>52 To represent the history of actions and observations over multiple episodes, we will often index variables by both episode and period. [sent-63, score-0.387]
</p><p>53 For example, xj,t and aj,t denote the state and action at period t of episode j, where j = 0, 1, · · · and t = 0, 1, · · · , H 1. [sent-64, score-0.75]
</p><p>54 To count the total number of steps since the agent started learning, we say period t in episode j is time jH + t. [sent-65, score-0.499]
</p><p>55 , µH 1 ) is a sequence of functions, each mapping S to A. [sent-69, score-0.044]
</p><p>56 PH 1 For each policy µ, deﬁne a value function Vtµ (x) = ⌧ =t R⌧ (x⌧ , a⌧ ), where xt = x, x⌧ +1 = F⌧ (x⌧ , a⌧ ), and a⌧ = µ⌧ (x⌧ ). [sent-70, score-0.063]
</p><p>57 A ⇤ policy µ⇤ is said to be optimal if V µ = V ⇤ . [sent-72, score-0.063]
</p><p>58 Note that this restriction incurs no loss of generality when the action space is ﬁnite. [sent-74, score-0.203]
</p><p>59 Then, a policy µ⇤ is optimal if H µ⇤ (x) 2 arg maxa2A Q⇤ (x, a) for all (x, t). [sent-76, score-0.063]
</p><p>60 In each episode, the algorithm realizes reward R(j) = t=0 Rt (xj,t , aj,t ). [sent-78, score-0.12]
</p><p>61 Note that R(j)  V0⇤ (xj,0 ) for each jth episode. [sent-79, score-0.062]
</p><p>62 One way to quantify performance of a reinforcement learning algorithm is in terms of the number of episodes JL for which R(j) < V0⇤ (xj,0 ) ✏, where ✏ 0 is a pre-speciﬁed performance loss threshold. [sent-80, score-0.468]
</p><p>63 If the reward function R is bounded, with |Rt (x, a)|  R for all (x, a, t), then this also implies a bound on regret over episodes exPbT /Hc 1 ⇤ perienced prior to time T , deﬁned by Regret(T ) = (V0 (xj,0 ) R(j) ). [sent-81, score-0.482]
</p><p>64 Speciﬁcally, it takes as input the state space S, the action space A, the horizon H, and a hypothesis class Q of candidates for Q⇤ . [sent-84, score-0.557]
</p><p>65 The algorithm maintains a sequence of subsets of Q and a sequence of scalar “upper bounds”, which summarize constraints that past experience suggests for ruling out hypotheses. [sent-85, score-0.213]
</p><p>66 Each constraint in this sequence is speciﬁed by a state x 2 S, an action a 2 A, a period t = 0, . [sent-86, score-0.563]
</p><p>67 , C|C| ) of such constraints and upper bounds U = (U1 , . [sent-94, score-0.098]
</p><p>68 , U|C| ), a set QC is deﬁned constructively by Algorithm 1. [sent-97, score-0.037]
</p><p>69 Note that if the constraints do not conﬂict then QC = C1 \ · · · \ C|C| . [sent-98, score-0.061]
</p><p>70 When constraints do conﬂict, priority is assigned ﬁrst based on upper bound, with smaller upper bound preferred, and then, in the event of ties in upper bound, based on position in the sequence, with more recent experience preferred. [sent-99, score-0.27]
</p><p>71 then QC QC \ C ⌧ end if end for if {u0 2 U : u0 > u} = ? [sent-101, score-0.082]
</p><p>72 then return QC end if u min{u0 2 U : u0 > u} end while OCP, presented below as Algorithm 2, at each time t computes for the current state xj,t and each action a the greatest state-action value Qt (xj,t , a) among functions in QC and selects an action that attains the maximum. [sent-102, score-0.699]
</p><p>73 In other words, an action is chosen based on the most optimistic feasible outcome subject to constraints. [sent-103, score-0.334]
</p><p>74 The subsequent reward and state transition give rise to a new constraint that is used to update C. [sent-104, score-0.325]
</p><p>75 Note that the update of C is postponed until one episode is completed. [sent-105, score-0.345]
</p><p>76 In the agnostic case, where Q⇤ may not lie in Q, new constraints can be inconsistent with previous constraints, in which case selected previous constraints are relaxed as determined by Algorithm 1. [sent-108, score-0.235]
</p><p>77 Let us brieﬂy discuss several contexts of practical relevance and/or theoretical interest in which OCP can be applied. [sent-109, score-0.138]
</p><p>78 With ﬁnite state and action spaces, Q⇤ can be represented as a vector, and without special prior knowledge, it is natural to let Q = <|S|·|A|·H . [sent-111, score-0.348]
</p><p>79 Consider the aforementioned example, but suppose that we have prior knowledge that Q⇤ lies in a particular polytope. [sent-113, score-0.142]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ocp', 0.406), ('episode', 0.311), ('qc', 0.31), ('reinforcement', 0.293), ('action', 0.203), ('episodes', 0.175), ('dime', 0.159), ('regret', 0.15), ('qt', 0.14), ('kwik', 0.135), ('optimistic', 0.131), ('period', 0.128), ('rt', 0.123), ('eluder', 0.119), ('hypothesis', 0.108), ('state', 0.108), ('exploration', 0.103), ('horizon', 0.099), ('rasa', 0.09), ('tabula', 0.09), ('reward', 0.086), ('establish', 0.085), ('constraint', 0.08), ('actions', 0.076), ('propagation', 0.076), ('ciency', 0.075), ('vt', 0.075), ('deterministic', 0.074), ('agnostic', 0.068), ('aforementioned', 0.066), ('synthesize', 0.066), ('experience', 0.064), ('contexts', 0.063), ('policy', 0.063), ('jth', 0.062), ('constraints', 0.061), ('episodic', 0.06), ('gracefully', 0.06), ('ict', 0.06), ('agent', 0.06), ('selects', 0.059), ('rewards', 0.055), ('decisions', 0.053), ('guarantees', 0.053), ('exponentially', 0.051), ('interacts', 0.051), ('transition', 0.051), ('system', 0.05), ('though', 0.05), ('grows', 0.049), ('lie', 0.045), ('factored', 0.045), ('therein', 0.045), ('sequence', 0.044), ('couple', 0.044), ('tted', 0.044), ('attains', 0.044), ('relevance', 0.041), ('end', 0.041), ('wen', 0.04), ('zhengwen', 0.04), ('import', 0.04), ('appended', 0.04), ('ble', 0.04), ('jh', 0.04), ('spaces', 0.039), ('lies', 0.039), ('class', 0.039), ('ft', 0.039), ('quadratic', 0.038), ('relate', 0.038), ('upper', 0.037), ('prior', 0.037), ('bvr', 0.037), ('weaken', 0.037), ('constructively', 0.037), ('nitehorizon', 0.037), ('sarsa', 0.037), ('transitions', 0.037), ('grow', 0.037), ('body', 0.037), ('planning', 0.037), ('interesting', 0.036), ('generalization', 0.035), ('enormous', 0.034), ('propagates', 0.034), ('postponed', 0.034), ('realizes', 0.034), ('horizons', 0.034), ('bound', 0.034), ('practical', 0.034), ('literature', 0.033), ('span', 0.033), ('repeatedly', 0.033), ('provably', 0.033), ('stanford', 0.033), ('focuses', 0.033), ('lder', 0.033), ('ph', 0.033), ('disjoint', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="103-tfidf-1" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>2 0.27406028 <a title="103-tfidf-2" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><p>3 0.23943385 <a title="103-tfidf-3" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>Author: Ian Osband, Dan Russo, Benjamin Van Roy</p><p>Abstract: Most provably-eﬃcient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for eﬃcient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally eﬃcient and allows an √ agent to encode prior knowledge ˜ in a natural way. We establish an O(τ S AT ) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the ﬁrst for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL signiﬁcantly outperforms existing algorithms with similar regret bounds. 1</p><p>4 0.23869917 <a title="103-tfidf-4" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>5 0.21412706 <a title="103-tfidf-5" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>6 0.16672313 <a title="103-tfidf-6" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>7 0.16283727 <a title="103-tfidf-7" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>8 0.1536918 <a title="103-tfidf-8" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>9 0.14203168 <a title="103-tfidf-9" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>10 0.13716681 <a title="103-tfidf-10" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>11 0.13312626 <a title="103-tfidf-11" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>12 0.13273847 <a title="103-tfidf-12" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>13 0.12130663 <a title="103-tfidf-13" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>14 0.12110037 <a title="103-tfidf-14" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>15 0.11948133 <a title="103-tfidf-15" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>16 0.11881201 <a title="103-tfidf-16" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>17 0.11537497 <a title="103-tfidf-17" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>18 0.11397549 <a title="103-tfidf-18" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>19 0.10889183 <a title="103-tfidf-19" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>20 0.10858692 <a title="103-tfidf-20" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, -0.29), (2, 0.032), (3, -0.037), (4, -0.017), (5, -0.093), (6, 0.076), (7, -0.065), (8, -0.078), (9, -0.017), (10, 0.009), (11, 0.045), (12, 0.032), (13, -0.042), (14, -0.005), (15, 0.048), (16, 0.055), (17, -0.038), (18, 0.045), (19, -0.113), (20, -0.1), (21, 0.041), (22, -0.015), (23, 0.026), (24, 0.074), (25, 0.049), (26, -0.095), (27, -0.145), (28, -0.029), (29, 0.055), (30, 0.002), (31, -0.003), (32, -0.049), (33, 0.125), (34, -0.117), (35, -0.073), (36, -0.091), (37, 0.036), (38, 0.03), (39, -0.036), (40, 0.032), (41, -0.022), (42, -0.025), (43, 0.066), (44, 0.043), (45, 0.02), (46, -0.056), (47, -0.036), (48, -0.052), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95203757 <a title="103-lsi-1" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>2 0.86021596 <a title="103-lsi-2" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>Author: Ian Osband, Dan Russo, Benjamin Van Roy</p><p>Abstract: Most provably-eﬃcient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for eﬃcient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally eﬃcient and allows an √ agent to encode prior knowledge ˜ in a natural way. We establish an O(τ S AT ) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the ﬁrst for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL signiﬁcantly outperforms existing algorithms with similar regret bounds. 1</p><p>3 0.78028661 <a title="103-lsi-3" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>Author: Asrar Ahmed, Pradeep Varakantham, Yossiri Adulyasak, Patrick Jaillet</p><p>Abstract: In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.</p><p>4 0.77557868 <a title="103-lsi-4" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><p>5 0.7615782 <a title="103-lsi-5" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>6 0.73934406 <a title="103-lsi-6" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>7 0.71973783 <a title="103-lsi-7" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>8 0.66126209 <a title="103-lsi-8" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>9 0.64443797 <a title="103-lsi-9" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>10 0.63361168 <a title="103-lsi-10" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>11 0.62591189 <a title="103-lsi-11" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>12 0.54886073 <a title="103-lsi-12" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>13 0.54228473 <a title="103-lsi-13" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>14 0.52481979 <a title="103-lsi-14" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>15 0.5085488 <a title="103-lsi-15" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>16 0.50417292 <a title="103-lsi-16" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>17 0.5009371 <a title="103-lsi-17" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>18 0.48808873 <a title="103-lsi-18" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>19 0.48748761 <a title="103-lsi-19" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>20 0.47334722 <a title="103-lsi-20" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.017), (13, 0.172), (16, 0.028), (33, 0.098), (34, 0.108), (41, 0.053), (49, 0.04), (56, 0.271), (70, 0.023), (85, 0.043), (89, 0.024), (93, 0.028), (95, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91812474 <a title="103-lda-1" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>Author: Rishabh K. Iyer, Stefanie Jegelka, Jeff A. Bilmes</p><p>Abstract: We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. 1</p><p>same-paper 2 0.91702366 <a title="103-lda-2" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><p>3 0.8706798 <a title="103-lda-3" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>4 0.86478472 <a title="103-lda-4" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>Author: Josh S. Merel, Roy Fox, Tony Jebara, Liam Paninski</p><p>Abstract: In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user’s neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modiﬁed decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.</p><p>5 0.85992903 <a title="103-lda-5" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>6 0.85750353 <a title="103-lda-6" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>7 0.85531354 <a title="103-lda-7" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>8 0.85482222 <a title="103-lda-8" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>9 0.85041404 <a title="103-lda-9" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>10 0.85038531 <a title="103-lda-10" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>11 0.84915888 <a title="103-lda-11" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>12 0.84605604 <a title="103-lda-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.83676046 <a title="103-lda-13" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>14 0.83537209 <a title="103-lda-14" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>15 0.83314013 <a title="103-lda-15" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>16 0.82893008 <a title="103-lda-16" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>17 0.82693946 <a title="103-lda-17" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>18 0.82680005 <a title="103-lda-18" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>19 0.8259384 <a title="103-lda-19" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>20 0.82548821 <a title="103-lda-20" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
