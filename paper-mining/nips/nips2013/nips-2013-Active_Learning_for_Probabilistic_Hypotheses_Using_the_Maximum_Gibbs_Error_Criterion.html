<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-23" href="#">nips2013-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</h1>
<br/><p>Source: <a title="nips-2013-23-pdf" href="http://papers.nips.cc/paper/4958-active-learning-for-probabilistic-hypotheses-using-the-maximum-gibbs-error-criterion.pdf">pdf</a></p><p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>Reference: <a title="nips-2013-23-reference" href="../nips2013_reference/nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. [sent-8, score-0.817]
</p><p>2 Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. [sent-9, score-0.91]
</p><p>3 We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. [sent-10, score-0.748]
</p><p>4 In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. [sent-11, score-0.559]
</p><p>5 For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. [sent-12, score-0.324]
</p><p>6 Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. [sent-13, score-0.518]
</p><p>7 1  Introduction  In pool-based active learning [1], we select training data from a ﬁnite set (called a pool) of unlabeled examples and aim to obtain good performance on the set by asking for as few labels as possible. [sent-14, score-0.412]
</p><p>8 We formulate the objective as a maximum coverage objective with a ﬁxed budget: with a budget of k queries, we aim to select k examples such that the policy Gibbs error is maximal. [sent-19, score-0.697]
</p><p>9 The policy Gibbs error of a policy is the expected error rate of a Gibbs classiﬁer1 on the set adaptively selected by the policy. [sent-20, score-0.996]
</p><p>10 The policy Gibbs error is a lower bound of the policy entropy, a generalization of the Shannon entropy to general (both adaptive and non-adaptive) policies. [sent-21, score-1.106]
</p><p>11 Figure 1: An example of a non-adaptive policy tree (left) and an adaptive policy tree (right). [sent-32, score-1.022]
</p><p>12 the policy Gibbs error reduces to the Gibbs error for sets, which is a special case of a measure of uncertainty called the Tsallis entropy [4]. [sent-33, score-0.651]
</p><p>13 By maximizing policy Gibbs error, we hope to maximize the policy entropy, whose maximality implies the minimality of the posterior label entropy of the remaining unlabeled examples in the pool. [sent-34, score-1.355]
</p><p>14 Besides, by maximizing policy Gibbs error, we also aim to obtain a small expected error of a posterior Gibbs classiﬁer (which samples a hypothesis from the posterior instead of the prior for labeling). [sent-35, score-0.762]
</p><p>15 Maximizing policy Gibbs error is hard, and we propose a greedy criterion, the maximum Gibbs error criterion (maxGEC), to solve it. [sent-37, score-0.718]
</p><p>16 We investigate this criterion in three settings: the non-adaptive setting, the adaptive setting and batch setting (also called batch mode setting) [5]. [sent-39, score-0.792]
</p><p>17 In the adaptive setting, the examples are labeled as soon as they are selected, and the new information is used to select the next example. [sent-41, score-0.261]
</p><p>18 In the batch setting, we select a batch of examples, query their labels and proceed to select the next batch taking into account the labels. [sent-42, score-0.962]
</p><p>19 In all these settings, we prove that maxGEC is near-optimal compared to the best policy that has maximal policy Gibbs error in the setting. [sent-43, score-0.883]
</p><p>20 We provide an approximation for maxGEC in the non-adaptive and batch settings with Bayesian transductive Naive Bayes model. [sent-46, score-0.391]
</p><p>21 Finally, we conduct pool-based active learning experiments using maxGEC for a named entity recognition task with conditional random ﬁelds and a text classiﬁcation task with Bayesian transductive Naive Bayes. [sent-47, score-0.408]
</p><p>22 A pool-based active learning algorithm is a policy for choosing training examples from a pool X ⊆ X . [sent-55, score-0.7]
</p><p>23 = 2  During the learning process, each time the learner selects an unlabeled example, its label will be revealed to the learner. [sent-61, score-0.28]
</p><p>24 A policy for choosing training examples is a mapping from a set of labeled examples to an unlabeled example to be queried. [sent-62, score-0.725]
</p><p>25 This can be represented by a policy tree, where a node represents the next example to be queried, and each edge from the node corresponds to a possible label. [sent-63, score-0.416]
</p><p>26 A full policy tree for a pool X is a policy tree of height |X|. [sent-66, score-1.005]
</p><p>27 A partial policy tree is a subtree of a full policy tree with the same root. [sent-67, score-0.932]
</p><p>28 Our query criterion gives a method to build a full policy tree one level at a time. [sent-69, score-0.586]
</p><p>29 The main building block is the probability distribution pπ [·] over all possible paths from the root to the leaves for any (full 0 or partial) policy tree π. [sent-70, score-0.487]
</p><p>30 This distribution over paths is induced from the uncertainty in the ﬁxed labeling y∗ for X: since y∗ is drawn randomly from p0 [y∗ ; X], the path ρ followed from the root to a leaf of the policy tree during the execution of π is also a random variable. [sent-71, score-0.583]
</p><p>31 = 0  3  Maximum Gibbs Error Criterion for Active Learning  A commonly used objective for active learning in the non-adaptive setting is to choose k training examples such that their Shannon entropy is maximal, as this reduces uncertainty in the later stage. [sent-75, score-0.422]
</p><p>32 Formally, the policy entropy of a policy π is H(π) def Eρ∼pπ [ − ln pπ [ρ] ]. [sent-77, score-1.037]
</p><p>33 = 0 0 From this deﬁnition, policy entropy is the Shannon entropy of the paths in the policy. [sent-78, score-0.703]
</p><p>34 The policy entropy reduces to the Shannon entropy on a set of examples when the policy is non-adaptive. [sent-79, score-1.18]
</p><p>35 The following result gives a formal statement that maximizing policy entropy minimizes the uncertainty on the label of the remaining unlabeled examples in the pool. [sent-80, score-0.849]
</p><p>36 Suppose a path ρ has been observed, the labels of the remaining examples in X \ xρ follow the distribution pρ [ · ; X \ xρ ], where pρ is the posterior obtained after observing (xρ , yρ ). [sent-81, score-0.259]
</p><p>37 The entropy of this distribution will be denoted by G(ρ) and will be called the posterior label entropy of the remaining examples given ρ. [sent-82, score-0.517]
</p><p>38 The posterior label entropy of a policy π is deﬁned as G(π) = Eρ∼pπ G(ρ). [sent-84, score-0.718]
</p><p>39 For any k ≥ 1, if a policy π in Πk maximizes H(π), then π minimizes the posterior label entropy G(π). [sent-86, score-0.718]
</p><p>40 The usual maximum Shannon entropy criterion, which selects the next example x maximizing Ey∼pD [y;x] [− ln pD [y; x]] where D is the previously observed labeled examples, can be thought of as a greedy heuristic for building a policy π maximizing H(π). [sent-90, score-0.833]
</p><p>41 In this paper, we introduce a new objective for active learning: the policy Gibbs error. [sent-92, score-0.595]
</p><p>42 This new objective is a lower bound of the policy entropy and there are near-optimal greedy algorithms to optimize it. [sent-93, score-0.658]
</p><p>43 Intuitively, the policy Gibbs error of a policy π is the expected probability for a Gibbs classiﬁer to make an error on the set adaptively selected by π. [sent-94, score-0.996]
</p><p>44 Formally, we deﬁne the policy Gibbs error of a policy π as V (π) def Eρ∼pπ [ 1 − pπ [ρ] ], (1) = 0 0 In the above equation, 1 − pπ [ρ] is the probability that a Gibbs classiﬁer makes an error on the 0 selected set along the path ρ. [sent-95, score-1.024]
</p><p>45 Theorem 2 below, which is straightforward from the inequality x ≥ 1 + ln x, states that the policy Gibbs error is a lower bound of the policy entropy. [sent-96, score-0.906]
</p><p>46 For any (full or partial) policy π, we have V (π) ≤ H(π). [sent-98, score-0.416]
</p><p>47 3  Given a budget of k queries, our proposed objective is to ﬁnd π ∗ = arg maxπ∈Πk V (π), the height k policy with maximum policy Gibbs error. [sent-99, score-0.97]
</p><p>48 By maximizing V (π), we hope to maximize the policy entropy H(π), and thus minimize the uncertainty in the remaining examples. [sent-100, score-0.586]
</p><p>49 Using this objective, we propose greedy algorithms for active learning that are provably near-optimal for probabilistic hypotheses. [sent-102, score-0.261]
</p><p>50 We will consider the non-adaptive, adaptive and batch settings. [sent-103, score-0.367]
</p><p>51 1  The Non-adaptive Setting  In the non-adaptive setting, the policy π ignores the observed labels: it never updates the posterior. [sent-105, score-0.416]
</p><p>52 The Gibbs error of a non-adaptive policy π is simply V (π) = Ey∼p0 [ · ;xπ ] [1 − p0 [y; xπ ]]. [sent-109, score-0.467]
</p><p>53 Thus, the optimal non-adaptive policy selects a set S of examples maximizing its Gibbs error, which is deﬁned by p0 (S) def 1 − y p0 [y; S]2 . [sent-110, score-0.62]
</p><p>54 The Gibbs error is a special case of the Tsallis entropy used in nonextensive statistical mechanics [4] and is known to be monotone submodular [7]. [sent-112, score-0.329]
</p><p>55 Given a budget of k ≥ 1 queries, let πn be the non-adaptive policy in Πk selecting ∗ examples using Equation (2), and let πn be the non-adaptive policy in Πk with the maximum policy ∗ Gibbs error. [sent-116, score-1.422]
</p><p>56 2  The Adaptive Setting  In the adaptive setting, a policy takes into account the observed labels when choosing the next example. [sent-119, score-0.559]
</p><p>57 The adaptive setting is the most common setting for active learning. [sent-121, score-0.296]
</p><p>58 1, pD (x) is in fact the Gibbs error of a 1-step policy with g g respect to the prior pD . [sent-126, score-0.494]
</p><p>59 Thus, we call this greedy criterion the adaptive maximum Gibbs error criterion (maxGEC). [sent-127, score-0.433]
</p><p>60 Theorem 4 below states that maxGEC is near-optimal compared to the best adaptive policy with respect to the objective in Equation (1). [sent-130, score-0.535]
</p><p>61 Given a budget of k ≥ 1 queries, let π maxGEC be the adaptive policy in Πk selecting examples using maxGEC and π ∗ be the adaptive policy in Πk with the maximum policy Gibbs error. [sent-132, score-1.602]
</p><p>62 For deterministic hypotheses, we show that maxGEC is equivalent to maximizing the version space reduction objective, which is known to be adaptive monotone submodular [2]. [sent-136, score-0.258]
</p><p>63 4  Algorithm 1 Batch maxGEC for Bayesian Batch Active Learning Input: Unlabeled pool X, prior p0 , number of iterations k, and batch size s. [sent-138, score-0.356]
</p><p>64 3  The Batch Setting  In the batch setting [5], we query the labels of s (instead of 1) examples each time, and we do this for a given number of k iterations. [sent-140, score-0.468]
</p><p>65 After each iteration, we query the labeling of the selected batch and update the posterior based on this labeling. [sent-141, score-0.532]
</p><p>66 The new posterior can be used to select the next batch of examples. [sent-142, score-0.392]
</p><p>67 A non-adaptive policy can be seen as a batch policy that selects only one batch. [sent-143, score-1.145]
</p><p>68 Algorithm 1 describes a greedy algorithm for this setting which we call the batch maxGEC algorithm. [sent-144, score-0.385]
</p><p>69 At iteration i of the algorithm with the posterior pi , the batch S is ﬁrst initialized to be empty, then s examples are greedily chosen one at a time using the criterion x∗ = arg max x  pi g (S  ∪ {x}). [sent-145, score-0.642]
</p><p>70 Query-labels(S) returns the true labeling yS of S and Posterior-update(pi , S, yS ) returns the new posterior obtained from the prior pi after observing yS . [sent-148, score-0.286]
</p><p>71 The following theorem states that batch maxGEC is near optimal compared to the best batch policy with respect to the objective in Equation (1). [sent-149, score-0.999]
</p><p>72 The proof also makes use of the reduction to deterministic hypotheses and the adaptive submodularity of version space reduction. [sent-151, score-0.282]
</p><p>73 Given a budget of k batches of size s, let πb be the batch policy selecting k ∗ batches using batch maxGEC and πb be the batch policy selecting k batches with maximum policy maxGEC ∗ Gibbs error. [sent-153, score-2.33]
</p><p>74 This theorem has a different bounding constant than those in Theorems 3 and 4 because it uses two levels of approximation to compute the batch policy: at each iteration, it approximates the optimal batch by greedily choosing one example at a time using equation (4) (1st approximation). [sent-155, score-0.575]
</p><p>75 Then it uses these chosen batches to approximate the optimal batch policy (2nd approximation). [sent-156, score-0.737]
</p><p>76 In contrast, the fully adaptive case has batch size 1 and only needs the 2nd approximation, while the non-adaptive case chooses 1 batch and only needs the 1st approximation. [sent-157, score-0.664]
</p><p>77 In non-adaptive and batch settings, our algorithms need to sum over all labelings of the previously selected examples in a batch to choose the next example. [sent-158, score-0.713]
</p><p>78 For example, if there is a small number of annotators and labeling one example takes a long time, we may want to select a batch size that matches the number of annotators. [sent-161, score-0.426]
</p><p>79 In this case, the annotators can label the examples concurrently while we can make use of the labels as soon as they are available. [sent-162, score-0.263]
</p><p>80 It would take a longer time to label a larger batch and we cannot use the labels until all the examples in the batch are labeled. [sent-163, score-0.768]
</p><p>81 4  Computing maxGEC  We now discuss how to compute maxGEC and batch maxGEC for some probabilistic models. [sent-164, score-0.308]
</p><p>82 Input: Selected unlabeled examples S, current unlabeled example x, current posterior pc . [sent-168, score-0.461]
</p><p>83 After observing the labeled examples D = {(xj , yj )}t , we can obtain the posterior j=1 t  pD [λ] = p0 [λ|D] ∝  1 exp Zλ (xj ) j=1  m  λi Fi (yj , xj ) exp − i=1  1 2  m  i=1  λi σ  2  . [sent-184, score-0.295]
</p><p>84 For active learning, we need to estimate the Gibbs error in Equation (3) from the posterior pD . [sent-185, score-0.291]
</p><p>85 2 Batch maxGEC for Bayesian Transductive Naive Bayes We discuss an algorithm to approximate batch maxGEC for non-adaptive and batch active learning with Bayesian transductive Naive Bayes. [sent-198, score-0.818]
</p><p>86 In transductive setting, we work with the conditional prior pc [θ] = p0 [θ|X; T ]. [sent-210, score-0.265]
</p><p>87 For a set D = (T, yT ) of 0 labeled examples where T ⊆ X is the set of unlabeled examples and yT is the labeling of T , the c conditional posterior is pD [θ] = p0 [θ|X; T ; D] = pD [θ|(X \ T ) ∪ T ], where pD [θ] = p0 [θ|D] is the Dirichlet posterior of the non-transductive model. [sent-211, score-0.624]
</p><p>88 To implement the batch maxGEC algorithm, we need to estimate the Gibbs error in Equation (4) from the conditional posterior. [sent-212, score-0.367]
</p><p>89 For each unlabeled example x ∈ S, we need to estimate: / 2  (pc D  1−  2  [h(S) = yS ∧ h(x) = y]) = 1 − EyS  yS ,y  6  y  (pc [h(S) = yS ∧ h(x) = y]) D pc [yS ; S] D  ,  Table 1: AUC of different learning algorithms with batch size s = 10. [sent-214, score-0.464]
</p><p>90 First, we sample M label vectors y(X\T )∪T of the remaining unlabeled examples from pc using Gibbs sampling. [sent-288, score-0.348]
</p><p>91 1: the ﬁrst version approximates Gibbs error by using only the MAP hypothesis (maxGEC-MAP) and the second version approximates Gibbs error by using 50 hypotheses sampled from the posterior (maxGEC-50). [sent-299, score-0.324]
</p><p>92 To implement the least conﬁdence algorithm, we sample M label vectors as in Algorithm 2 and use them to estimate the label distribution for each unlabeled example. [sent-331, score-0.26]
</p><p>93 We run the algorithms on 7 binary tasks from the 20Newsgroups dataset [13] with batch size s = 10, 20, 30 and report the areas under the accuracy curve (AUC) for the case s = 10 in Table 1. [sent-333, score-0.277]
</p><p>94 From the results, maxGEC obtains the best AUC scores on 4/7 tasks for each batch size and also the best average AUC scores. [sent-336, score-0.277]
</p><p>95 For non-adaptive active learning, greedy optimization of the Shannon entropy guarantees near optimal performance due to the submodularity of the entropy [2]. [sent-343, score-0.561]
</p><p>96 However, this has not been shown to extend to adaptive active learning, where each example is labeled as soon as it is selected, and the labeled examples are exploited in selecting the next example to label. [sent-344, score-0.456]
</p><p>97 The adaptive submodularity of version space reduction was also applied to the batch setting to prove the near-optimality of a batch greedy algorithm that maximizes the average version space reduction for each selected batch [3]. [sent-349, score-1.175]
</p><p>98 The maxGEC and batch maxGEC algorithms that we proposed in this paper can be seen as generalizations of these version space reduction algorithms to the noisy setting. [sent-350, score-0.297]
</p><p>99 7  Conclusion  We considered a new objective function for Bayesian active learning: the policy Gibbs error. [sent-357, score-0.595]
</p><p>100 The algorithm has near-optimality guarantees in the non-adaptive, adaptive and batch settings. [sent-359, score-0.367]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maxgec', 0.564), ('policy', 0.416), ('batch', 0.277), ('gibbs', 0.257), ('ys', 0.19), ('pd', 0.186), ('active', 0.15), ('entropy', 0.133), ('transductive', 0.114), ('shannon', 0.102), ('unlabeled', 0.102), ('labeling', 0.096), ('criterion', 0.092), ('posterior', 0.09), ('adaptive', 0.09), ('pc', 0.085), ('crf', 0.084), ('examples', 0.082), ('hypotheses', 0.081), ('greedy', 0.08), ('label', 0.079), ('leastconf', 0.078), ('passive', 0.074), ('submodularity', 0.065), ('naive', 0.064), ('learner', 0.063), ('segent', 0.063), ('bayesian', 0.061), ('submodular', 0.053), ('labels', 0.053), ('pool', 0.052), ('hypothesis', 0.051), ('error', 0.051), ('tree', 0.05), ('def', 0.049), ('bayes', 0.049), ('auc', 0.049), ('tsallis', 0.047), ('yj', 0.046), ('batches', 0.044), ('labeled', 0.043), ('fi', 0.043), ('entity', 0.042), ('longest', 0.042), ('selected', 0.041), ('classi', 0.04), ('conditional', 0.039), ('pi', 0.039), ('ner', 0.038), ('maximizing', 0.037), ('budget', 0.037), ('er', 0.037), ('labelings', 0.036), ('selects', 0.036), ('observing', 0.034), ('text', 0.033), ('monotone', 0.032), ('queries', 0.032), ('lc', 0.031), ('summation', 0.031), ('cuong', 0.031), ('hai', 0.031), ('logfisher', 0.031), ('logpass', 0.031), ('nonextensive', 0.031), ('npass', 0.031), ('tpass', 0.031), ('probabilistic', 0.031), ('named', 0.03), ('objective', 0.029), ('mechanics', 0.029), ('query', 0.028), ('nonadaptive', 0.028), ('annotators', 0.028), ('burr', 0.028), ('dso', 0.028), ('leong', 0.028), ('viet', 0.028), ('setting', 0.028), ('maximum', 0.028), ('prior', 0.027), ('selecting', 0.027), ('deterministic', 0.026), ('select', 0.025), ('document', 0.024), ('golovin', 0.024), ('wee', 0.024), ('arg', 0.023), ('nan', 0.023), ('ln', 0.023), ('dence', 0.022), ('andreas', 0.022), ('paths', 0.021), ('adaptively', 0.021), ('height', 0.021), ('soon', 0.021), ('equation', 0.021), ('chooses', 0.02), ('mccallum', 0.02), ('reduction', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="23-tfidf-1" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>2 0.35074466 <a title="23-tfidf-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.29270989 <a title="23-tfidf-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>4 0.25272939 <a title="23-tfidf-4" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>5 0.21025352 <a title="23-tfidf-5" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>Author: David J. Weiss, Ben Taskar</p><p>Abstract: Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Signiﬁcant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to ﬁnetune feature extraction to each input at run-time. We address the key challenge of learning to control ﬁne-grained feature extraction adaptively, exploiting nonhomogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efﬁcient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate signiﬁcant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task. 1</p><p>6 0.20907198 <a title="23-tfidf-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.2016518 <a title="23-tfidf-7" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>8 0.18856917 <a title="23-tfidf-8" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>9 0.1768343 <a title="23-tfidf-9" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>10 0.16469049 <a title="23-tfidf-10" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>11 0.16108908 <a title="23-tfidf-11" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>12 0.15228337 <a title="23-tfidf-12" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>13 0.14469631 <a title="23-tfidf-13" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>14 0.14269026 <a title="23-tfidf-14" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>15 0.1367517 <a title="23-tfidf-15" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>16 0.12903444 <a title="23-tfidf-16" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>17 0.12801975 <a title="23-tfidf-17" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>18 0.12640193 <a title="23-tfidf-18" href="./nips-2013-Conditional_Random_Fields_via_Univariate_Exponential_Families.html">67 nips-2013-Conditional Random Fields via Univariate Exponential Families</a></p>
<p>19 0.12574968 <a title="23-tfidf-19" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>20 0.11380485 <a title="23-tfidf-20" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.249), (1, -0.261), (2, -0.182), (3, 0.143), (4, 0.056), (5, 0.138), (6, -0.082), (7, -0.006), (8, 0.082), (9, 0.056), (10, 0.003), (11, 0.01), (12, -0.11), (13, 0.087), (14, 0.113), (15, -0.267), (16, -0.141), (17, 0.016), (18, 0.023), (19, 0.082), (20, -0.088), (21, 0.004), (22, 0.046), (23, -0.095), (24, 0.059), (25, 0.082), (26, 0.01), (27, 0.061), (28, 0.012), (29, 0.068), (30, 0.112), (31, -0.02), (32, 0.03), (33, -0.126), (34, 0.017), (35, -0.017), (36, 0.088), (37, -0.052), (38, 0.061), (39, 0.029), (40, 0.076), (41, -0.062), (42, 0.05), (43, -0.021), (44, 0.045), (45, -0.059), (46, 0.017), (47, 0.023), (48, -0.003), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97438031 <a title="23-lsi-1" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>2 0.7201106 <a title="23-lsi-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.68872601 <a title="23-lsi-3" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>4 0.68323022 <a title="23-lsi-4" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>5 0.6810472 <a title="23-lsi-5" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><p>6 0.60770696 <a title="23-lsi-6" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>7 0.59506446 <a title="23-lsi-7" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>8 0.57958621 <a title="23-lsi-8" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>9 0.57578284 <a title="23-lsi-9" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>10 0.56652451 <a title="23-lsi-10" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>11 0.554039 <a title="23-lsi-11" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>12 0.52979922 <a title="23-lsi-12" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>13 0.52539468 <a title="23-lsi-13" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>14 0.52529597 <a title="23-lsi-14" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>15 0.46958783 <a title="23-lsi-15" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>16 0.46586928 <a title="23-lsi-16" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>17 0.4658193 <a title="23-lsi-17" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>18 0.45150054 <a title="23-lsi-18" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>19 0.45139015 <a title="23-lsi-19" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>20 0.44650486 <a title="23-lsi-20" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.033), (16, 0.034), (33, 0.115), (34, 0.135), (41, 0.023), (49, 0.039), (56, 0.141), (70, 0.033), (85, 0.024), (86, 0.212), (89, 0.015), (93, 0.052), (95, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83388257 <a title="23-lda-1" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>2 0.82498747 <a title="23-lda-2" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>Author: Remi Gribonval, Pierre Machart</p><p>Abstract: There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efﬁciency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization. 1</p><p>3 0.80416131 <a title="23-lda-3" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>4 0.80139929 <a title="23-lda-4" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>same-paper 5 0.78692919 <a title="23-lda-5" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>6 0.73099709 <a title="23-lda-6" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>7 0.73050463 <a title="23-lda-7" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>8 0.72779083 <a title="23-lda-8" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>9 0.72726822 <a title="23-lda-9" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>10 0.72709095 <a title="23-lda-10" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>11 0.72502291 <a title="23-lda-11" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>12 0.72297442 <a title="23-lda-12" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>13 0.72296458 <a title="23-lda-13" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>14 0.72265112 <a title="23-lda-14" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>15 0.72171724 <a title="23-lda-15" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>16 0.71934205 <a title="23-lda-16" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>17 0.71916223 <a title="23-lda-17" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>18 0.71829051 <a title="23-lda-18" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>19 0.71673328 <a title="23-lda-19" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>20 0.71625543 <a title="23-lda-20" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
