<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-290" href="#">nips2013-290</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</h1>
<br/><p>Source: <a title="nips-2013-290-pdf" href="http://papers.nips.cc/paper/4889-scoring-workers-in-crowdsourcing-how-many-control-questions-are-enough.pdf">pdf</a></p><p>Author: Qiang Liu, Alex Ihler, Mark Steyvers</p><p>Abstract: We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. 1</p><p>Reference: <a title="nips-2013-290-reference" href="../nips2013_reference/nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. [sent-15, score-1.346]
</p><p>2 This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. [sent-16, score-3.826]
</p><p>3 Unfortunately, it is not always obvious how best to combine the crowd, because the (often anonymous) workers have unknown and diverse levels of expertise, and potentially systematic biases across the crowd. [sent-24, score-0.611]
</p><p>4 Na¨ve consensus methods which simply take uniform averages or ı the majority answer of the workers have been known to perform poorly. [sent-25, score-0.641]
</p><p>5 One direct way to address this problem is to score workers using their past performance on similar problems. [sent-27, score-0.501]
</p><p>6 An alternative is the idea behind reCAPTCHA: “seed” some control items with known answers into the assigned tasks (without telling workers which are control items), score the workers using these control items, and weight their answers accordingly on the unknown target items. [sent-29, score-2.259]
</p><p>7 CrowdFlower, for example, provides interfaces and tools to allow requesters to explicitly specify and analyze a set of control items (sometimes called “gold data”). [sent-31, score-0.731]
</p><p>8 The reCAPTCHA example is a particularly simple case, where workers answer exactly one control and one target item. [sent-32, score-0.761]
</p><p>9 However, in general crowdsourcing, the workers may answer hundreds of questions, raising the question of how many control items should be used. [sent-33, score-1.259]
</p><p>10 There is a clear tradeoff: having workers answer more control items gives better estimates of their performance and any 1  potential systematic bias, but leaves fewer resources for the target items that are of direct interest. [sent-34, score-2.021]
</p><p>11 However, using few control items gives poor estimates of workers’ performance and their biases, also leading to bad results. [sent-35, score-0.731]
</p><p>12 A deep understanding of the value of control items may provide useful guidance for crowdsourcing practitioners. [sent-36, score-0.881]
</p><p>13 On the other hand, a line of research has studied more advanced consensus methods that are able to simultaneously estimate the workers’ performance and items’ answers without any ground truth on the items, by building joint statistical models of the workers and labels (e. [sent-37, score-0.841]
</p><p>14 The basic idea is to score the workers by their agreement with other workers, assuming that the majority of workers are correct. [sent-44, score-1.002]
</p><p>15 Perhaps surprisingly, the worker reliabilities estimated by these “unsupervised” consensus methods can be almost as good as those estimated when the true labels of all the items are known, and are much better than self-evaluated worker reliability (Romney et al. [sent-45, score-1.129]
</p><p>16 Control items can also be incorporated into these methods: but how much can we expect them to improve results, or does an “unsupervised” method sufﬁce? [sent-48, score-0.592]
</p><p>17 The goal of this paper is to study the value of control items, and provide practical guidance on how many control items are enough under different scenarios. [sent-49, score-0.888]
</p><p>18 Important practical issues such as the impact of model misspeciﬁcation, systematic biases and heteroscedasticity are also highlighted on real datasets. [sent-53, score-0.171]
</p><p>19 In addition, we have a set C of control (or training) items whose true T labels µ∗ = {µ∗ i ∈ C} are known. [sent-55, score-0.796]
</p><p>20 We denote the set of workers by W; each worker j is associated i C ∗ with a parameter νj that characterizes their expertise, bias, any other relevant features. [sent-56, score-0.637]
</p><p>21 Denote by nt the number of target items and m the workers. [sent-59, score-0.858]
</p><p>22 t c Let ∂i be the set of workers assigned to item i, and ∂j (and ∂j ) the set of target (and control) items labeled by worker j. [sent-60, score-1.41]
</p><p>23 The assignment relationship between the workers and the target items can be represented by a bipartite graph Gt = (T , W, Et ), where there is an edge (ij) ∈ Et iff item i is assigned to worker j. [sent-61, score-1.526]
</p><p>24 Let ri be the number of workers assigned to the i-th target item, and let t j (and c ) be the number of target (and control) items assigned to the j-th worker. [sent-62, score-1.372]
</p><p>25 j  Denote by xij the label we collect from worker j for item i. [sent-64, score-0.392]
</p><p>26 In general, we can assume that xij is a ∗ random variable drawn from a probabilistic distribution p(xij µ∗ , νj ). [sent-65, score-0.218]
</p><p>27 The computational question i ∗ is then to construct an estimator µT of the true labels µT based on the crowdsourced labels {xij }, ˆ such that the expected mean square error (MSE) on the target items, E[ µT − µ∗ 2 ], is minimized. [sent-66, score-0.448]
</p><p>28 We focus on a class of simple Gaussian models on the labels xij : xij = µ∗ + b∗ + ξij , i j  ξij ∼ N (0, σ ∗2 ),  (1)  where µ∗ is the quantity of interest of item i, b∗ is the bias of worker j, and σ ∗2 is the variance. [sent-68, score-0.716]
</p><p>29 It captures heterogeneous biases across workers that are commonly observed in practice, for example in workers’ judgments on probabilities and prices, and which can have signiﬁcant effects on the estimate accuracy. [sent-73, score-0.56]
</p><p>30 Note that the biases are not identiﬁable solely from the crowdsourced labels {xij }, making it is necessary to add some control items or other information when decoding the answers. [sent-75, score-0.908]
</p><p>31 2  An extension of model (1) is to introduce heteroscedasticity, allowing different workers to have ∗ ∗2 different level of Gaussian noise: that is, xij = µ∗ + b∗ + σj ξij , where ξij ∼ N (0, 1) and σj is i j a variance parameter of worker j. [sent-76, score-0.872]
</p><p>32 We will also consider another special case, xij = µ∗ + σj ξij , j which assumes the workers all have zero bias but different variances (the variance-only model). [sent-78, score-0.778]
</p><p>33 1  Consensus Algorithms With Partial Ground Truth  Control items with known true labels can be used to estimate workers’ parameters, and hence improve the estimation accuracy. [sent-81, score-0.657]
</p><p>34 Two-stage Estimator: the workers’ parameters are ﬁrst estimated using the control items, and are then used to predict the target items. [sent-84, score-0.25]
</p><p>35 That is, Scoring workers:  νj = arg max ∑ log p(xij µ∗ , νj ), ˆ i νj  Predicting target items:  for all j ∈ W,  (2)  for all i ∈ T ,  (3)  c i∈∂j  ˆ µi = arg max ∑ log p(xij µi , νj ), ˆ µi  j∈∂i  where we use the maximum likelihood estimator as a general procedure for estimating parameters. [sent-85, score-0.265]
</p><p>36 Joint Estimator: we directly maximize the joint likelihood of the crowdsourced labels {xij } of both target and control items, with µC of the control items clamped to the true values µ∗ . [sent-86, score-1.191]
</p><p>37 Compared to the two-stage estimator, the joint estimator estimates the workers’ parameters based on both the control items and the target items, even though their true labels are unknown. [sent-88, score-1.17]
</p><p>38 This is because the labels ∗ xij provide information on µ∗ through the model assumption p(xij µ∗ , νj ). [sent-89, score-0.285]
</p><p>39 Therefore, the joint i i estimator may be much more efﬁcient than the two-stage estimator when the model assumptions are satisﬁed, but may perform poorly if the model is misspeciﬁed. [sent-90, score-0.487]
</p><p>40 We now consider the central question: assuming each worker answers items (we refer as the budget), including k control items and − k target items, what is the optimal choice of k to minimize the expected MSE? [sent-92, score-1.647]
</p><p>41 To be concrete, here we assume all the workers (items) are assigned to the same number of randomly selected items (workers), and hence the assignment graph Gt is a random semi-regular bipartite graph, which can be generated by the conﬁguration model (e. [sent-93, score-1.26]
</p><p>42 Obviously, the optimal number of control items should depend on their usage in the subsequent consensus method. [sent-98, score-0.885]
</p><p>43 We will show that the two-stage and joint estimators exploit control items in fundamentally different ways, and yield very different optimal values of k. [sent-99, score-0.881]
</p><p>44 Roughly speaking, the √ √ optimal k should scale as O( ) when using a two-stage estimator, compared to O( / nt ) when using joint estimators. [sent-100, score-0.322]
</p><p>45 For the bias-only model with xij = µ∗ + b∗ + ξij , where ξij are i. [sent-108, score-0.22]
</p><p>46 noise drawn i j from N (0, σ ∗2 ), the expected mean square error (MSE) of the two-stage estimator in (2)-(3) is σ ∗2 1 ˆ (1 + ). [sent-111, score-0.187]
</p><p>47 The solution of two-stage estimator has a simple linear form under the bias-only model, 1 1 µi = ∑ (xij − ˆj ), ˆj = ∑ (xij − µ∗ ), for ∀i ∈ T , ∀j ∈ W. [sent-115, score-0.171]
</p><p>48 ˆ b b i r j∈∂i k i∈∂ c j  Since the xij are Gaussian, the µi are also Gaussian. [sent-116, score-0.202]
</p><p>49 (5) shows that the MSE is inversely proportional to the number r of workers per target item, while the number k of control items per workers only reﬁnes the multiplicative constant. [sent-122, score-1.843]
</p><p>50 Therefore, the resources assigned to the control items are much less “useful” than those assigned directly to the target items, suggesting the optimal k should be much less than the budget . [sent-123, score-1.067]
</p><p>51 In addition, if the budget grows to inﬁnity, the optimal k should also grow to inﬁnity, otherwise the multiplicative constant is strictly larger than one, which is suboptimal. [sent-127, score-0.17]
</p><p>52 Letting k take continuous values, the optimal k to minimize (6) is k ∗ = a + a2 − a, which achieves a minimum MSE of nt ⋅ σ 2 /( − 2k ∗ ). [sent-160, score-0.213]
</p><p>53 For comparison, the MSE would be nt ⋅ σ 2 /( − k ∗ ) ˜ ˜ m m if the worker parameters were known exactly. [sent-161, score-0.308]
</p><p>54 2  Optimal k for Joint Estimator  The two-stage estimator is easy to analyze in that its accuracy is independent of the structure of the bipartite assignment graph beyond the degree r and k. [sent-165, score-0.303]
</p><p>55 For the bias-only model with xij = µ∗ + b∗ + ξij , where ξij are i. [sent-174, score-0.22]
</p><p>56 The solution of the joint estimator on t 1 1 ∗ −1 ¯ ¯ the bias-only model is µT = µT + B zT , where zi = ˆ ∑ (ξij − ξj ), and ξj = c t ∑ ξij ri j∈∂i j + j i′ ∈∂ c ∪∂ t and ξij = xij − µ∗ − b∗ for ∀i ∈ T . [sent-180, score-0.525]
</p><p>57 Observe that tr((I −  −k  nt  W )−1 ) = ∑(1 −  −k  i=1  λi )−1 ≤  k  +  nt − 1 . [sent-187, score-0.344]
</p><p>58 1 − −k λ2  (9)  Therefore, the joint estimator performs better when λ2 is small, i. [sent-188, score-0.28]
</p><p>59 Intuitively, better connectivity “couples” the items and workers more tightly together, making it easier not to make mistakes during inference. [sent-191, score-1.111]
</p><p>60 Assume At is a random regular bipartite graph, and nt = m. [sent-205, score-0.258]
</p><p>61 We have that E[ ∑ µi − µ∗ 2 /nt ] = ˆ i i∈T  1 σ ∗2 nt − 1 [ ], (1 + O( )) + − k nt nt k  (10)  → ∞, the optimal k that minimizes (10) is k ∗ =  with probability one as nt → ∞. [sent-206, score-0.729]
</p><p>62 If in addition √ √ ⌈ 2 /nt + 2 /nt 2 + 1/4 − /nt − 1/2⌉ ≈ / nt . [sent-207, score-0.172]
</p><p>63 Perhaps surprisingly, the optimal k of the joint estimator scales linearly w. [sent-212, score-0.321]
</p><p>64 However, since usually ≤ nt , we have √ √ / nt ≤ , that is, the joint estimator requires fewer control items than the two-stage estimator. [sent-216, score-1.373]
</p><p>65 In addition, the optimal k for the joint estimator also decreases as the total number nt of target items increases. [sent-218, score-1.179]
</p><p>66 Because nt is usually quite large in practice, the number of control items is usually very small. [sent-219, score-0.903]
</p><p>67 In particular, as nt → ∞, we have k ∗ = 1, that is, there is no need for control items beyond ﬁxing the unidentiﬁability issue of the biases. [sent-220, score-0.903]
</p><p>68 The joint estimator on general models is more involved to analyze, but it is still possible to give an rough estimate by analyzing the Fisher information matrix of the likelihood. [sent-222, score-0.28]
</p><p>69 The price dataset consists of 80 household items collected from stores like Amazon and Costco, whose prices are estimated by 155 undergraduate students at UC Irvine. [sent-240, score-0.737]
</p><p>70 For all the experiments, we ﬁrst construct the set of target items and control items by randomly partitioning items, and then randomly assign each worker with k control items and −k target items, for varying values of and k. [sent-245, score-2.394]
</p><p>71 Figure 1(a) shows the empirical MSE of the two-stage estimator when varying the number k of control items. [sent-250, score-0.347]
</p><p>72 The MSE of the joint estimator in Figure 1(b) follows a similar trend, but the gain by using control items is less signiﬁcant (the left parts of the curves are ﬂatter). [sent-252, score-1.011]
</p><p>73 This is because the joint estimator leverages the labels on the target items (whose true values are unknown), and relies less on the control items. [sent-253, score-1.17]
</p><p>74 In particular, as the number nt of √ target items increases, the optimal value of k for the joint estimator decreases with a rate of 1/ nt (see Figure 1(d)), but that of the two-stage estimator stays the same. [sent-254, score-1.522]
</p><p>75 Overall, the empirical optimal k of the two-stage and joint estimator aligns closely with our theoretical prediction (Figure 1(c)-(d)). [sent-255, score-0.382]
</p><p>76 The optimal k of the two-stage estimator aligns closely to a with a = 3, matching √ the asymptotic result in Theorem 3. [sent-257, score-0.285]
</p><p>77 2, while that of the joint estimator scales like the line a/nt with a ≈ 3, matching our hypothesis in Section 3. [sent-258, score-0.28]
</p><p>78 nt  Figure 1: Results of the bias-only model on data simulated from the same model. [sent-272, score-0.224]
</p><p>79 (a)-(b) The MSE of the two-stage and joint estimators with varying and k and ﬁxed nt = 100. [sent-273, score-0.297]
</p><p>80 (c) The optimal k with varying , but ﬁxed nt = 100. [sent-275, score-0.229]
</p><p>81 (d) The optimal k with varying nt , but ﬁxed = 50. [sent-276, score-0.229]
</p><p>82 Figure 2(b)-(c) shows the results when the data are simulated from a bias-variance model with non-zero biases, but we use the variance-only model (with zero bias) in the consensus algorithm. [sent-282, score-0.183]
</p><p>83 We see in Figure 2(b) that the optimal k of the two-stage estimator still aligns closely to our theoretical prediction, but that of the joint estimator is much larger than one would expect (almost half of the budget ). [sent-283, score-0.645]
</p><p>84 In addition, the MSE of the joint estimator in this case is signiﬁcantly worse than that of the two-stage estimator (see Figure 2(c)), which is not expected if the model assumption holds. [sent-284, score-0.469]
</p><p>85 Therefore, the joint estimator seems to be more sensitive to model misspeciﬁcation than the two-stage estimator, suggesting that caution should be taken when it is applied in practice. [sent-285, score-0.298]
</p><p>86 Interestingly, the two real datasets have opposite properties in terms of the importance of bias and heteroskedasticity (see Figure 4): In the price dataset, all the workers tend to underestimate the prices of the products, i. [sent-289, score-0.695]
</p><p>87 In both cases, the full bias-variance model works best if budget is large, but is not necessarily best if the budget is small and over-ﬁtting is an issue. [sent-293, score-0.244]
</p><p>88 However, our results highlight several issues and provide insights and rules of thumb that can help crowdsourcing practitioners make their √ own decisions. [sent-295, score-0.175]
</p><p>89 In particular, we show that the optimal number of control items should be O( ) for √ the two-stage estimator and O( / nt ) for the joint estimator. [sent-296, score-1.224]
</p><p>90 Because the number nt of target items is usually large in practice, it is reasonable to recommend using a minimal number of control items, just enough to ﬁx potential unidentiﬁability issues, assuming the model assumptions hold well. [sent-297, score-1.015]
</p><p>91 However, the joint estimator may require signiﬁcantly more control items if model misspeciﬁcation exists; in this case one might better switch to the more robust two-stage estimator, or search for better models. [sent-298, score-1.029]
</p><p>92 The control items can also be used to do model selection, an issue which deserves further discussion in the future. [sent-299, score-0.749]
</p><p>93 5  0 0  100  = 40  1  20  20  40 60 Budget  (a) Bias-variance Model  80  100  2  3  5  8  13  22  36  60  100  k (# of control items)  (b)-(c) Model Misspeciﬁcation  Figure 2: (a) Results of the bias-variance model on data simulated from the same model. [sent-305, score-0.191]
</p><p>94 (b)-(c) Results when the data are simulated from the bias-variance model with non-zero biases, but we use the variance-only model (with zero bias) in the consensus algorithm. [sent-306, score-0.183]
</p><p>95 With this model misspeciﬁcation, the joint estimator requires signiﬁcantly more control items than one would expect (almost half of the budget ), and performs worse than the two-stage estimator. [sent-307, score-1.142]
</p><p>96 21  =7  MSE  MSE  Price Dataset  10  =7  Two-stage (empirical) √ Two-stage ( )  4 2  8  6 1  2  3  5  8  13  6 1  22  k (# of control items)  2  3  5  8  13  22  0  k (# of control items)  (d) Two-stage Estimator  10  20 30 Budget  40  (f) Optimal k vs. [sent-315, score-0.278]
</p><p>97 (c) and (f) The empirically and theoretically optimal k as the budget varies. [sent-318, score-0.17]
</p><p>98 Here we ﬁx nt = 50 for price dataset and nt = 200 for NFL dataset. [sent-319, score-0.41]
</p><p>99 The workers in the price dataset has systematic bias, and the bias-only model works better than the variance-only model, while the workers in NFL dataset have no bias but different individual variances, and the variance-only model is better than bias-only. [sent-330, score-1.219]
</p><p>100 In both datasets, the full bias-variance model works best if the budget is large, but is not necessarily best if the budget is small when over-ﬁtting is an issue. [sent-331, score-0.244]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('items', 0.592), ('workers', 0.501), ('mse', 0.231), ('xij', 0.202), ('nt', 0.172), ('estimator', 0.171), ('control', 0.139), ('worker', 0.136), ('crowdsourcing', 0.132), ('nfl', 0.12), ('consensus', 0.113), ('budget', 0.113), ('joint', 0.109), ('misspeci', 0.097), ('target', 0.094), ('labels', 0.065), ('ij', 0.063), ('prices', 0.062), ('biases', 0.059), ('bipartite', 0.058), ('bias', 0.057), ('item', 0.054), ('crowdsourced', 0.053), ('expertise', 0.053), ('answers', 0.053), ('karger', 0.049), ('gt', 0.046), ('recaptcha', 0.045), ('unidenti', 0.045), ('wisdom', 0.045), ('price', 0.044), ('optimal', 0.041), ('lc', 0.04), ('spreads', 0.04), ('aligns', 0.04), ('lt', 0.038), ('rt', 0.036), ('systematic', 0.036), ('simulated', 0.034), ('assigned', 0.033), ('graph', 0.033), ('asymptotic', 0.033), ('tr', 0.032), ('forecasting', 0.031), ('dawid', 0.03), ('hoory', 0.03), ('massey', 0.03), ('reliabilities', 0.03), ('romney', 0.03), ('irvine', 0.029), ('regular', 0.028), ('answer', 0.027), ('crowd', 0.026), ('sheng', 0.026), ('whitehill', 0.026), ('crowds', 0.026), ('assignment', 0.025), ('ri', 0.025), ('heteroscedasticity', 0.024), ('thumb', 0.024), ('expander', 0.024), ('qiang', 0.024), ('et', 0.023), ('byproduct', 0.023), ('steyvers', 0.023), ('dataset', 0.022), ('resources', 0.022), ('graphs', 0.021), ('empirical', 0.021), ('nity', 0.021), ('scoring', 0.02), ('fisher', 0.02), ('issues', 0.019), ('ihler', 0.019), ('connectivity', 0.018), ('model', 0.018), ('variances', 0.018), ('guidance', 0.018), ('participants', 0.018), ('fewer', 0.018), ('estimated', 0.017), ('raises', 0.017), ('trend', 0.017), ('drawn', 0.016), ('datasets', 0.016), ('surprisingly', 0.016), ('theoretically', 0.016), ('varying', 0.016), ('california', 0.016), ('multiplicative', 0.016), ('jacob', 0.016), ('ii', 0.016), ('degree', 0.016), ('stage', 0.015), ('amazon', 0.015), ('adjacency', 0.015), ('games', 0.015), ('unknown', 0.015), ('real', 0.015), ('variance', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="290-tfidf-1" href="./nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F.html">290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</a></p>
<p>Author: Qiang Liu, Alex Ihler, Mark Steyvers</p><p>Abstract: We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. 1</p><p>2 0.21297601 <a title="290-tfidf-2" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>3 0.15040776 <a title="290-tfidf-3" href="./nips-2013-More_Effective_Distributed_ML_via_a_Stale_Synchronous_Parallel_Parameter_Server.html">198 nips-2013-More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</a></p>
<p>Author: Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Greg Ganger, Eric Xing</p><p>Abstract: We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model’s values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This signiﬁcantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes. 1</p><p>4 0.13364838 <a title="290-tfidf-4" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>5 0.076234564 <a title="290-tfidf-5" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>6 0.065168567 <a title="290-tfidf-6" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>7 0.063339733 <a title="290-tfidf-7" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>8 0.057669062 <a title="290-tfidf-8" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>9 0.057028163 <a title="290-tfidf-9" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>10 0.055174254 <a title="290-tfidf-10" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>11 0.053547136 <a title="290-tfidf-11" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>12 0.053030979 <a title="290-tfidf-12" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>13 0.0490565 <a title="290-tfidf-13" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>14 0.044892266 <a title="290-tfidf-14" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>15 0.043039374 <a title="290-tfidf-15" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>16 0.042856447 <a title="290-tfidf-16" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>17 0.039854977 <a title="290-tfidf-17" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>18 0.039124165 <a title="290-tfidf-18" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>19 0.039122216 <a title="290-tfidf-19" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>20 0.038371533 <a title="290-tfidf-20" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.005), (2, 0.026), (3, 0.007), (4, 0.016), (5, 0.019), (6, -0.024), (7, -0.028), (8, -0.025), (9, -0.003), (10, 0.048), (11, -0.051), (12, -0.007), (13, -0.038), (14, -0.064), (15, -0.046), (16, 0.026), (17, -0.009), (18, -0.024), (19, -0.019), (20, 0.003), (21, -0.101), (22, -0.024), (23, -0.043), (24, -0.026), (25, -0.002), (26, -0.006), (27, -0.075), (28, -0.02), (29, -0.098), (30, 0.011), (31, 0.036), (32, -0.051), (33, -0.064), (34, 0.015), (35, -0.081), (36, -0.016), (37, 0.001), (38, -0.067), (39, 0.102), (40, -0.085), (41, -0.042), (42, 0.033), (43, -0.099), (44, -0.062), (45, 0.072), (46, -0.081), (47, 0.066), (48, -0.149), (49, 0.193)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93740171 <a title="290-lsi-1" href="./nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F.html">290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</a></p>
<p>Author: Qiang Liu, Alex Ihler, Mark Steyvers</p><p>Abstract: We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. 1</p><p>2 0.5970456 <a title="290-lsi-2" href="./nips-2013-More_Effective_Distributed_ML_via_a_Stale_Synchronous_Parallel_Parameter_Server.html">198 nips-2013-More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</a></p>
<p>Author: Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Greg Ganger, Eric Xing</p><p>Abstract: We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model’s values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This signiﬁcantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes. 1</p><p>3 0.49575073 <a title="290-lsi-3" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>4 0.49263784 <a title="290-lsi-4" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>Author: Hossein Azari Soufiani, William Chen, David C. Parkes, Lirong Xia</p><p>Abstract: In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efﬁciency. 1</p><p>5 0.45920041 <a title="290-lsi-5" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>6 0.45815247 <a title="290-lsi-6" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>7 0.45547515 <a title="290-lsi-7" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<p>8 0.40607798 <a title="290-lsi-8" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>9 0.39497063 <a title="290-lsi-9" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>10 0.39351776 <a title="290-lsi-10" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>11 0.39006311 <a title="290-lsi-11" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>12 0.39000306 <a title="290-lsi-12" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>13 0.38525328 <a title="290-lsi-13" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>14 0.3801223 <a title="290-lsi-14" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>15 0.37537155 <a title="290-lsi-15" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>16 0.36936954 <a title="290-lsi-16" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>17 0.36483639 <a title="290-lsi-17" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>18 0.35983667 <a title="290-lsi-18" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>19 0.35861647 <a title="290-lsi-19" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>20 0.35803011 <a title="290-lsi-20" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (16, 0.03), (33, 0.105), (34, 0.075), (37, 0.29), (41, 0.022), (49, 0.028), (56, 0.123), (70, 0.037), (85, 0.042), (89, 0.067), (93, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72123879 <a title="290-lda-1" href="./nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F.html">290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</a></p>
<p>Author: Qiang Liu, Alex Ihler, Mark Steyvers</p><p>Abstract: We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. 1</p><p>2 0.69869614 <a title="290-lda-2" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>Author: Yanshuai Cao, Marcus A. Brubaker, David Fleet, Aaron Hertzmann</p><p>Abstract: We propose an efﬁcient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case. 1</p><p>3 0.62401515 <a title="290-lda-3" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Joseph Keshet, Tommi Jaakkola</p><p>Abstract: In this work we develop efﬁcient methods for learning random MAP predictors for structured label problems. In particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods. We show that any smooth posterior distribution would sufﬁce to deﬁne a smooth PAC-Bayesian risk bound suitable for gradient methods. In addition, we relate the posterior distributions to computational properties of the MAP predictors. We suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized MAP predictors such as graph-cuts. We also describe label-augmented posterior models that can use efﬁcient MAP approximations, such as those arising from linear program relaxations. 1</p><p>4 0.62156659 <a title="290-lda-4" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>5 0.56430793 <a title="290-lda-5" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>6 0.56261766 <a title="290-lda-6" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>7 0.56063068 <a title="290-lda-7" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>8 0.55908412 <a title="290-lda-8" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>9 0.55777997 <a title="290-lda-9" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>10 0.55522221 <a title="290-lda-10" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>11 0.55481225 <a title="290-lda-11" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>12 0.55461383 <a title="290-lda-12" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>13 0.55441701 <a title="290-lda-13" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>14 0.55382591 <a title="290-lda-14" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>15 0.55251491 <a title="290-lda-15" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>16 0.55231076 <a title="290-lda-16" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>17 0.55193114 <a title="290-lda-17" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>18 0.55189264 <a title="290-lda-18" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>19 0.55189204 <a title="290-lda-19" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>20 0.55140203 <a title="290-lda-20" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
