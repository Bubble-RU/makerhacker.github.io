<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-145" href="#">nips2013-145</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</h1>
<br/><p>Source: <a title="nips-2013-145-pdf" href="http://papers.nips.cc/paper/5089-it-is-all-in-the-noise-efficient-multi-task-gaussian-process-inference-with-structured-residuals.pdf">pdf</a></p><p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><p>Reference: <a title="nips-2013-145-reference" href="../nips2013_reference/nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is all in the noise: Efﬁcient multi-task Gaussian process inference with structured residuals  Christoph Lippert Microsoft Research Los Angeles, USA lippert@microsoft. [sent-1, score-0.233]
</p><p>2 uk  Abstract Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. [sent-10, score-0.215]
</p><p>3 We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. [sent-11, score-0.362]
</p><p>4 The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. [sent-12, score-0.422]
</p><p>5 On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. [sent-13, score-0.368]
</p><p>6 1  Introduction  Multi-task Gaussian process (GP) models are widely used to couple related tasks or functions for joint regression. [sent-14, score-0.242]
</p><p>7 This coupling is achieved by designing a structured covariance function, yielding a prior on vector-valued functions. [sent-15, score-0.393]
</p><p>8 If the outputs yn,t are fully observed, with one training example per sample and task, the resulting covariance matrix between the latent factors can be written as a Kronecker product between the sample covariance matrix and the task covariance matrix (e. [sent-17, score-1.277]
</p><p>9 More complex multi-task covariance structures can be derived from generalizations of this product structure, for example via convolution of multiple features, e. [sent-20, score-0.314]
</p><p>10 In [3], a parameterized covariance over the tasks is used, assuming that task-relevant features are observed. [sent-23, score-0.532]
</p><p>11 The authors of [4] couple the latent features over the tasks exploiting a dependency in neural population activity over time. [sent-24, score-0.296]
</p><p>12 1  Work proposing this type of multi-task GP regression builds on Bonilla and Williams [1], who have emphasized that the power of Kronecker covariance models for GP models (Eqn. [sent-26, score-0.314]
</p><p>13 In fact, in the limit of noise-free training observations, the coupling of tasks for predictions is lost in the predictive model, reducing to ordinary GP regressors for each individual task. [sent-28, score-0.309]
</p><p>14 Most multi task GP models build on a simple independent noise model, an assumption that is mainly routed in computational convenience. [sent-29, score-0.271]
</p><p>15 In this paper, we account for residual noise structure by modeling the signal and the noise covariance matrix as two separate Kronecker products. [sent-31, score-0.823]
</p><p>16 The structured noise covariance is independent of the inputs but instead allows to capture residual correlation between tasks due to latent causes; moreover, the model is simple and extends the widely used product covariance structure. [sent-32, score-1.16]
</p><p>17 In geostatistics [8], linear coregionalization models have been introduced to allow for more complicated covariance structures: the signal covariance matrix is modeled as a sum of Kronecker products and the noise covariance as a single Kronecker product. [sent-34, score-1.415]
</p><p>18 The mixing coefﬁcients are dependent on the input signal and control the signal and noise correlation simultaneously. [sent-36, score-0.493]
</p><p>19 First, we show that unobserved regressors or causal processes inevitably lead to correlated residual, motivating the need to account for structured noise (Section 2). [sent-38, score-0.502]
</p><p>20 This extension of the multi task GP model allows for more accurate estimation of the task-task relationships, thereby improving the performance for out-of-sample predictions. [sent-39, score-0.146]
</p><p>21 The proposed implementation handles closed form marginal likelihoods and parameter gradients for matrix-variate normal models with a covariance structure represented by the sum of two Kronecker products. [sent-41, score-0.388]
</p><p>22 These operations can be implemented at marginal extra computational cost compared to models that ignore residual task correlations (Section 3). [sent-42, score-0.201]
</p><p>23 2  Multi-task Gaussian processes with structured noise  Let Y ∈ RN ×T denote the N × T output training matrix for N samples and T tasks. [sent-45, score-0.332]
</p><p>24 A column of this matrix corresponds to a particular task t is denoted as yt , and vecY = y1 . [sent-46, score-0.138]
</p><p>25 Multivariate linear model equivalence The multi-task Gaussian process regression model with structured noise can be derived from the perspective of a linear multivariate generative model. [sent-52, score-0.299]
</p><p>26 For a particular task t, the outputs are determined by a linear function of the training inputs across F features S = {s1 , . [sent-53, score-0.236]
</p><p>27 (2)  f =1  Multi-task sharing is achieved by specifying a multivariate normal prior across tasks, both for the regression weights wf,t and the noise variances ψ t : F  N  N (wf | 0, CT T )  p(W ) =  N (ψ n | 0, ΣT T ) . [sent-57, score-0.205]
</p><p>28 In the following, we will refer to a Gaussian process model with this type of sum of Kronecker products covariance structure as GP-kronsum1 . [sent-60, score-0.481]
</p><p>29 As common to any kernel method, the linear covariance R can be replaced with any positive semi-deﬁnite covariance function. [sent-61, score-0.678]
</p><p>30 Predictive distribution In a GP-kronsum model, predictions for unseen test instances can be carried out by using the standard Gaussian process framework [10]: p(vecY∗ |R∗ , Y) = N (vecY∗ | vec M∗ , V∗ ) . [sent-62, score-0.258]
</p><p>31 ∗  (4)  ∗  Here, M denotes the mean prediction and V is the predictive covariance. [sent-63, score-0.134]
</p><p>32 Design of multi-task covariance function In practice, neither the form of C nor the form of Σ is known a priori and hence needs to be inferred from data, ﬁtting a set of corresponding covariance parameters θC and θΣ . [sent-65, score-0.628]
</p><p>33 If the number of tasks T is large, learning a free-form covariance matrix is prone to overﬁtting, as the number of free parameters grows quadratically with T . [sent-66, score-0.505]
</p><p>34 Task cancellation when the task covariance matrices are equal A notable form of the predictive distribution (4) arises for the special case C = Σ, that is the task covariance matrix of signal and noise are identical. [sent-68, score-1.196]
</p><p>35 the beneﬁts from joint modeling are lost: vec M∗ = vec R∗ ∗ N (RN N + IN N )−1 YN T N  (5)  In this case, the predictions depend on the sample covariance, but not on the task covariance. [sent-71, score-0.499]
</p><p>36 Thus, the GP-kronsum model is most useful when the task covariances on observed features and on noise reﬂect two independent sharing structures. [sent-72, score-0.408]
</p><p>37 3  Efﬁcient Inference  In general, efﬁcient inference can be carried out for Gaussian models with a sum covariance of two arbitrary Kronecker products p(vecY | C, R, Σ) = N (vecY | 0, CT T ⊗ RN N + ΣT T ⊗ ΩN N ) . [sent-73, score-0.425]
</p><p>38 (6)  The key idea is to ﬁrst consider a suitable data transformation that leads to a diagonalization of all covariance matrices and second to exploit Kronecker tricks whenever possible. [sent-74, score-0.314]
</p><p>39 Borrowing ideas from [11], we can ﬁrst bring the covariance matrix in a more amenable form by factoring out the structured noise: 1 the covariance is deﬁned as the sum of two Kronecker products and not as the classical Kronecker sum C ⊕ R = C ⊗ I + I ⊗ R. [sent-76, score-0.897]
</p><p>40 ˜ of the transformed covariance K: NT L=− ln(2π) − 2 NT =− ln(2π) − 2 1  The log model likelihood (Eqn. [sent-80, score-0.363]
</p><p>41 (6)) can be expressed in terms 1 ln|K| − 2 1 ˜ ln|K| − 2  1 vecY K−1 vecY 2 1 1 ˜ ˜ ˜ |SΣ ⊗ SΩ | − vecY K−1 vecY, 2 2  1  1  (8)  1  − − − − ˜ where vecY = SΣ 2 UΣ ⊗ SΩ 2 UΩ vecY = vec SΩ 2 UT YUΣ SΣ 2 Ω  is the projected output. [sent-81, score-0.202]
</p><p>42 Except for the additional term |SΣ ⊗ SΩ |, resulting from the transformation, the log likelihood has the exactly same form as for multi-task GP regression with iid noise [1, 5]. [sent-82, score-0.216]
</p><p>43 Analogous gradients can be derived for ˜ ˜ ˜ the task covariance parameters θC and θΣ . [sent-85, score-0.409]
</p><p>44 (4)) can be efﬁciently evaluated vec M∗ = vec  −1  ˆ ˜ UR YUC ˜  R∗ UΩ SΩ 2  −1  SΣ 2 UΣ C  . [sent-89, score-0.404]
</p><p>45 (11)  Gradient-based parameter inference The closed-form expression of the marginal likelihood (Eqn. [sent-90, score-0.149]
</p><p>46 (9)) and gradients with respect to covariance parameters (Eqn. [sent-91, score-0.314]
</p><p>47 The empirical runtime savings over the naive approach are explored in Section 4. [sent-96, score-0.154]
</p><p>48 Unless stated otherwise, in the multi-task models the relationship between tasks was parameterized as xx + σ 2 I, the sum of a rank-1 matrix and a constant diagonal component. [sent-108, score-0.227]
</p><p>49 Finally, we measured the predictive performance of the different methods via the averaged square of Pearson’s correlation coefﬁcient r2 between the true and the predicted output, averaged over tasks. [sent-110, score-0.134]
</p><p>50 The squared correlation coefﬁcient is commonly used in statistical genetics to evaluate the performance of different predictors [13]. [sent-111, score-0.186]
</p><p>51 1  Simulations  First, we considered simulated experiments to explore the runtime behavior and to ﬁnd out if there are settings in which GP-kronsum performs better than existing methods. [sent-113, score-0.149]
</p><p>52 (3)) using a linear kernel for the sample covariance matrix R and rank-1 matrices for the task covariances C and Σ. [sent-118, score-0.579]
</p><p>53 The runtime of this model was assessed for a single likelihood optimization on an AMD Opteron Processor 6,378 using a single core (2. [sent-119, score-0.154]
</p><p>54 In the experiments, we used a standard linear kernel on the features of the samples as sample covariance while learning the task covariances. [sent-122, score-0.529]
</p><p>55 While our algorithm can handle 256 samples/256 tasks with ease, the naive implementation failed to process more than 32 samples/32 tasks. [sent-125, score-0.253]
</p><p>56 Unobserved causal process induces structured noise A common source of structured residuals are unobserved causal processes that are not captured via the inputs. [sent-126, score-0.791]
</p><p>57 For one of the processes, we assumed that the causal features Xobs were observed, whereas for the second process the causal features Xhidden were hidden and independent of the observed measurements. [sent-128, score-0.553]
</p><p>58 The effect of the hidden features was simulated analogously. [sent-131, score-0.231]
</p><p>59 Note that the best possible explained variance for the default setting is 45%, as the causal signal is split up equally between the observed and the hidden process. [sent-155, score-0.422]
</p><p>60 The number of observed features was set to 200, as well as the number of hidden features. [sent-157, score-0.187]
</p><p>61 First, we considered the impact of variation in signal strength µsignal (Figure 2a), where the overall signal was divided up equally between the observed and hidden signal. [sent-159, score-0.499]
</p><p>62 Both GP-single and GPkronsum performed better as the overall signal strength increased. [sent-160, score-0.197]
</p><p>63 Second, we explored the ability of the different methods to cope with an underlying hidden process (Figure 2b). [sent-162, score-0.173]
</p><p>64 In the absence of a hidden process (µhidden = 0), GP-kronprod and GP-kronsum had very similar performances, as both methods leverage the shared signal of the observed process, thereby outperforming the single-task GPs. [sent-163, score-0.359]
</p><p>65 However, as the magnitude of the hidden signal increases, GP-kronprod falsely explains the task correlation completely by the covariance term representing the observed process which leads to loss of predictive power. [sent-164, score-0.864]
</p><p>66 Last, we examined the ability of different methods to exploit the relatedness between the tasks (Figure 2c). [sent-165, score-0.254]
</p><p>67 GP-kronprod suffered from the same limitations as previously described, because the correlation between tasks in the hidden process increases synchronously with the correlation in the observed process as µcommon increases. [sent-167, score-0.521]
</p><p>68 GP-pool was consistently outperformed by all competitors as two of its main assumptions are heavily violated: samples of different tasks do not share the same signal and the residuals are neither independent of each other, nor do they have the same noise level. [sent-169, score-0.554]
</p><p>69 In summary, the proposed model is robust to a range of different settings and clearly outperforms its competitors when the tasks are related to each other and not all causal processes are observed. [sent-170, score-0.353]
</p><p>70 2  Applications to phenotype prediction  As a real world application we considered phenotype prediction in statistical genetics. [sent-172, score-0.328]
</p><p>71 The aim of these experiments was to demonstrate the relevance of unobserved causes in real world prediction problems and hence warrant greater attention. [sent-173, score-0.136]
</p><p>72 Gene expression prediction in yeast We considered gene expression levels from a yeast genetics study [14]. [sent-174, score-0.675]
</p><p>73 The dataset comprised of gene expression levels of 5, 493 genes and 2, 956 SNPs (features), measured for 109 yeast crosses. [sent-175, score-0.383]
</p><p>74 In this experiment, we treated the condition information as a hidden factor instead of regressing it out, which is analogous to the hidden process in the simulation experiments. [sent-177, score-0.326]
</p><p>75 We normalized all features and all tasks to zero mean and unit variance. [sent-179, score-0.218]
</p><p>76 From left to right: (a) Evaluation for varying signal strength. [sent-183, score-0.148]
</p><p>77 (c) Evaluation for different strength of relatedness between the tasks. [sent-185, score-0.155]
</p><p>78 (a) Empirical  (b) Signal  (c) Noise  Figure 3: Fitted task covariance matrices for gene expression levels in yeast. [sent-187, score-0.636]
</p><p>79 From left to right: (a) Empirical covariance matrix of the gene expression levels. [sent-188, score-0.539]
</p><p>80 The ordering of the tasks was determined using hierarchical clustering on the empirical covariance matrix. [sent-191, score-0.462]
</p><p>81 discarded genes with low signal (< 10% of the variance) or were close to noise free (> 90% of the variance), reducing the number of genes to 123, which we considered as tasks in our experiment. [sent-192, score-0.533]
</p><p>82 The signal strength was estimated by a univariate GP model. [sent-193, score-0.197]
</p><p>83 Figure 3 shows the empirical covariance and the learnt task covariances by GP-kronsum. [sent-195, score-0.56]
</p><p>84 Both learnt covariances are highly structured, demonstrating that the assumption of iid noise in the GP-kronprod model is violated in this dataset. [sent-196, score-0.318]
</p><p>85 While the signal task covariance matrix reﬂects genetic signals that are shared between the gene expression levels, the noise covariance matrix mainly captures the mean shift between the two conditions the gene expression levels were measured in (Figure 4). [sent-197, score-1.529]
</p><p>86 As a second dataset, we considered a genome-wide association study in Arabidopsis thaliana [15] to assess the prediction of developmental phenotypes from genomic data. [sent-213, score-0.36]
</p><p>87 As different tasks, we considered the phenotypes ﬂowering period duration, life cycle period, maturation period and reproduction period. [sent-215, score-0.612]
</p><p>88 The SNPs in Arabidopsis thaliana are binary and we discarded features with a frequency of less 7  XC  XSigma  Figure 4: Correlation between the mean difference of the two conditions and the latent factors on the yeast dataset. [sent-217, score-0.341]
</p><p>89 Shown is the strength of the latent factor of the signal (left) and the noise (right) task covariance matrix as a function of the mean difference between the two environmental conditions. [sent-218, score-0.814]
</p><p>90 Since the causal processes in Arabidopsis thaliana are complex, we allowed the rank of the signal and noise matrix to vary between 1 and 3. [sent-223, score-0.617]
</p><p>91 We considered the average squared correlation coefﬁcient on the holdout fraction of the training data to select the model for prediction on the test dataset. [sent-225, score-0.179]
</p><p>92 Notably, for GP-kronprod, the selected task complexity was rank(C) = 3, whereas GP-kronsum selected a simpler structure for the signal task covariance (rank(C) = 1) and chose a more complex noise covariance, rank(Σ) = 2. [sent-226, score-0.777]
</p><p>93 For the phenotype life cycle period, the noise estimates of the univariate GP model were close to zero, and hence all methods, except of GP-pool, performed equally well since the measurements of the other phenotypes do not provide additional information. [sent-229, score-0.501]
</p><p>94 GP-pool GP-single GP-kronprod GP-kronsum  Flowering period duration 0. [sent-232, score-0.153]
</p><p>95 0033  Table 1: Predictive performance of the different methods on the Arabidopsis thaliana dataset. [sent-264, score-0.131]
</p><p>96 5  Discussion and conclusions  Multi-task Gaussian process models are a widely used tool in many application domains, ranging from the prediction of user preferences in collaborative ﬁltering to the prediction of phenotypes in computational biology. [sent-266, score-0.357]
</p><p>97 Many of these prediction tasks are complex and important causal features may remain unobserved or are not modeled. [sent-267, score-0.474]
</p><p>98 We here propose the GP-kronsum model, which allows to efﬁciently model data where the noise is dependent between tasks by building on a sum of Kronecker products covariance. [sent-269, score-0.384]
</p><p>99 In applications to statistical genetics, we have demonstrated (1) the advantages of the dependent noise model over an independent noise model, as well as (2) the feasibility of applying larger data sets by the efﬁcient learning algorithm. [sent-270, score-0.25]
</p><p>100 Genome-wide association study of 107 phenotypes in Arabidopsis thaliana inbred lines. [sent-349, score-0.288]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vecy', 0.365), ('covariance', 0.314), ('kronecker', 0.266), ('vec', 0.202), ('arabidopsis', 0.157), ('phenotypes', 0.157), ('signal', 0.148), ('tasks', 0.148), ('thaliana', 0.131), ('ur', 0.127), ('noise', 0.125), ('gp', 0.122), ('gene', 0.12), ('causal', 0.12), ('hidden', 0.117), ('period', 0.116), ('genetics', 0.114), ('relatedness', 0.106), ('runtime', 0.105), ('snps', 0.104), ('wcommon', 0.104), ('yuc', 0.104), ('yeast', 0.1), ('residuals', 0.098), ('task', 0.095), ('ct', 0.095), ('phenotype', 0.092), ('structured', 0.079), ('lippert', 0.078), ('rcommon', 0.078), ('covariances', 0.077), ('products', 0.075), ('learnt', 0.074), ('sc', 0.074), ('correlation', 0.072), ('prediction', 0.072), ('features', 0.07), ('maturation', 0.069), ('residual', 0.068), ('reproduction', 0.064), ('regressors', 0.064), ('unobserved', 0.064), ('expression', 0.062), ('predictive', 0.062), ('gaussian', 0.061), ('ln', 0.06), ('bonilla', 0.06), ('sr', 0.059), ('genes', 0.056), ('process', 0.056), ('neil', 0.054), ('life', 0.052), ('gpkronsum', 0.052), ('owering', 0.052), ('rakitsch', 0.052), ('vecyn', 0.052), ('xobs', 0.052), ('ycommon', 0.052), ('ynoise', 0.052), ('multi', 0.051), ('processes', 0.05), ('kernel', 0.05), ('strength', 0.049), ('naive', 0.049), ('likelihood', 0.049), ('bingen', 0.048), ('oliver', 0.046), ('yobs', 0.046), ('coregionalization', 0.046), ('institutes', 0.046), ('prematurely', 0.046), ('levels', 0.045), ('simulated', 0.044), ('matrix', 0.043), ('edwin', 0.042), ('alfried', 0.042), ('krupp', 0.042), ('iid', 0.042), ('sharing', 0.041), ('latent', 0.04), ('sf', 0.04), ('karsten', 0.04), ('biology', 0.039), ('multivariate', 0.039), ('rn', 0.038), ('cycle', 0.038), ('shared', 0.038), ('marginal', 0.038), ('couple', 0.038), ('equally', 0.037), ('duration', 0.037), ('outputs', 0.036), ('sum', 0.036), ('simulation', 0.036), ('training', 0.035), ('germany', 0.035), ('christopher', 0.035), ('competitors', 0.035), ('planck', 0.034), ('christoph', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="145-tfidf-1" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><p>2 0.12611765 <a title="145-tfidf-2" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>Author: Kevin Swersky, Jasper Snoek, Ryan P. Adams</p><p>Abstract: Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost. 1</p><p>3 0.10848574 <a title="145-tfidf-3" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>4 0.10814615 <a title="145-tfidf-4" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>5 0.093726926 <a title="145-tfidf-5" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>Author: Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain</p><p>Abstract: We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2 ) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2 ). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) samplecomplexity – the ﬁrst algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data. 1</p><p>6 0.090795688 <a title="145-tfidf-6" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>7 0.090656705 <a title="145-tfidf-7" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>8 0.088451393 <a title="145-tfidf-8" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>9 0.084051982 <a title="145-tfidf-9" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<p>10 0.083021536 <a title="145-tfidf-10" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>11 0.082799874 <a title="145-tfidf-11" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>12 0.081011824 <a title="145-tfidf-12" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>13 0.080764085 <a title="145-tfidf-13" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>14 0.078184135 <a title="145-tfidf-14" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>15 0.076715119 <a title="145-tfidf-15" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>16 0.072839558 <a title="145-tfidf-16" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>17 0.072748959 <a title="145-tfidf-17" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>18 0.072671771 <a title="145-tfidf-18" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>19 0.071553901 <a title="145-tfidf-19" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>20 0.070767321 <a title="145-tfidf-20" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.081), (2, -0.007), (3, 0.02), (4, -0.04), (5, 0.016), (6, 0.039), (7, 0.066), (8, -0.024), (9, -0.062), (10, -0.048), (11, -0.069), (12, -0.106), (13, -0.016), (14, -0.037), (15, 0.048), (16, -0.056), (17, 0.076), (18, -0.047), (19, -0.085), (20, 0.031), (21, -0.037), (22, -0.048), (23, -0.0), (24, 0.09), (25, 0.027), (26, -0.015), (27, 0.075), (28, -0.056), (29, -0.031), (30, -0.022), (31, 0.025), (32, -0.014), (33, 0.093), (34, -0.01), (35, 0.031), (36, -0.046), (37, -0.089), (38, 0.108), (39, 0.001), (40, -0.033), (41, -0.117), (42, 0.075), (43, 0.012), (44, 0.013), (45, 0.054), (46, 0.004), (47, -0.021), (48, -0.051), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95094287 <a title="145-lsi-1" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><p>2 0.689735 <a title="145-lsi-2" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>Author: Kevin Swersky, Jasper Snoek, Ryan P. Adams</p><p>Abstract: Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost. 1</p><p>3 0.67687422 <a title="145-lsi-3" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<p>Author: Daniel Bartz, Klaus-Robert Müller</p><p>Abstract: Analytic shrinkage is a statistical technique that offers a fast alternative to crossvalidation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage –orthogonal complement shrinkage– which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of ﬁnance, spoken letter and optical character recognition, and neuroscience. 1</p><p>4 0.63580841 <a title="145-lsi-4" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>5 0.63490564 <a title="145-lsi-5" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>Author: Yanshuai Cao, Marcus A. Brubaker, David Fleet, Aaron Hertzmann</p><p>Abstract: We propose an efﬁcient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case. 1</p><p>6 0.62249362 <a title="145-lsi-6" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>7 0.61464059 <a title="145-lsi-7" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>8 0.59580344 <a title="145-lsi-8" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>9 0.59282851 <a title="145-lsi-9" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>10 0.58949935 <a title="145-lsi-10" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>11 0.57579243 <a title="145-lsi-11" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>12 0.56898791 <a title="145-lsi-12" href="./nips-2013-Gaussian_Process_Conditional_Copulas_with_Applications_to_Financial_Time_Series.html">126 nips-2013-Gaussian Process Conditional Copulas with Applications to Financial Time Series</a></p>
<p>13 0.5491001 <a title="145-lsi-13" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>14 0.54380995 <a title="145-lsi-14" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>15 0.53928286 <a title="145-lsi-15" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>16 0.53926611 <a title="145-lsi-16" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>17 0.52746427 <a title="145-lsi-17" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>18 0.51712698 <a title="145-lsi-18" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>19 0.51580089 <a title="145-lsi-19" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>20 0.5109129 <a title="145-lsi-20" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.481), (33, 0.102), (34, 0.082), (41, 0.036), (49, 0.021), (56, 0.072), (70, 0.026), (85, 0.033), (89, 0.031), (93, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93202627 <a title="145-lda-1" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>Author: Abbas Edalat</p><p>Abstract: We solve the mean ﬁeld equations for a stochastic Hopﬁeld network with temperature (noise) in the presence of strong, i.e., multiply stored, patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the ﬁrst time a rigorous solution of the mean ﬁled equations for the standard Hopﬁeld model and is in contrast to the mathematically unjustiﬁable replica technique that has been used hitherto for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when the sum of the squares of degrees of the patterns is negligible compared to the network size. In the case of a single strong pattern, when the ratio of the number of all stored pattens and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean ﬁeld and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justiﬁcation for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy. 1 Introduction: Multiply learned patterns in Hopﬁeld networks The Hopﬁeld network as a model of associative memory and unsupervised learning was introduced in [23] and has been intensively studied from a wide range of viewpoints in the past thirty years. However, properties of a strong pattern, as a pattern that has been multiply stored or learned in these networks, have only been examined very recently, a surprising delay given that repetition of an activity is the basis of learning by the Hebbian rule and long term potentiation. In particular, while the storage capacity of a Hopﬁeld network with certain correlated patterns has been tackled [13, 25], the storage capacity of a Hopﬁeld network in the presence of strong as well as random patterns has not been hitherto addressed. The notion of a strong pattern of a Hopﬁeld network has been proposed in [15] to model attachment types and behavioural prototypes in developmental psychology and psychotherapy. This suggestion has been motivated by reviewing the pioneering work of Bowlby [9] in attachment theory and highlighting how a number of academic biologists, psychiatrists, psychologists, sociologists and neuroscientists have consistently regarded Hopﬁeld-like artiﬁcial neural networks as suitable tools to model cognitive and behavioural constructs as patterns that are deeply and repeatedly learned by individuals [11, 22, 24, 30, 29, 10]. A number of mathematical properties of strong patterns in Hopﬁeld networks, which give rise to strong attractors, have been derived in [15]. These show in particular that strong attractors are strongly stable; a series of experiments have also been carried out which conﬁrm the mathematical 1 results and also indicate that a strong pattern stored in the network can be retrieved even in the presence of a large number of simple patterns, far exceeding the well-known maximum load parameter or storage capacity of the Hopﬁeld network with random patterns (αc ≈ 0.138). In this paper, we consider strong patterns in stochastic Hopﬁeld model with temperature, which accounts for various types of noise in the network. In these networks, the updating rule is probabilistic and depend on the temperature. Since analytical solution of such a system is not possible in general, one strives to obtain the average behaviour of the network when the input to each node, the so-called ﬁeld at the node, is replaced with its mean. This is the basis of mean ﬁeld theory for these networks. Due to the close connection between the Hopﬁeld network and the Ising model in ferromagnetism [1, 8], the mean ﬁeld approach for the Hopﬁeld network and its variations has been tackled using the replica method, starting with the pioneering work of Amit, Gutfreund and Sompolinsky [3, 2, 4, 19, 31, 1, 13]. Although this method has been widely used in the theory of spin glasses in statistical physics [26, 16] its mathematical justiﬁcation has proved to be elusive as we will discuss in the next section; see for example [20, page 264], [14, page 27], and [7, page 9]. In [17] and independently in [27], an alternative technique to the replica method for solving the mean ﬁeld equations has been proposed which is reproduced and characterised as heuristic in [20, section 2.5] since it relies on a number of assumptions that are not later justiﬁed and uses a number of mathematical steps that are not validated. Here, we use the basic idea of the above heuristic to develop a veriﬁable mathematical framework with provable results grounded on elements of probability theory, with which we assume the reader is familiar. This technique allows us to solve the mean ﬁeld equations for the Hopﬁeld network in the presence of strong patterns and use the results to study, ﬁrst, the stability of these patterns in the presence of temperature (noise) and, second, the storage capacity of the network with a single strong pattern at temperature zero. We show that the critical temperature for the stability of a strong pattern is equal to its degree (i.e., its multiplicity) when the ratio of the sum of the squares of degrees of the patterns to the network size tends to zero when the latter tends to inﬁnity. In the case that there is only one strong pattern present with its degree small compared to the number of patterns and the latter is a ﬁxed multiple of the number of nodes, we ﬁnd the distribution of the overlap of the mean ﬁeld and the patterns when the strong pattern is being retrieved. We use these distributions to prove that the storage capacity for retrieving a strong pattern exceeds that for a simple pattern by a multiplicative factor equal to the square of the degree of the strong attractor. This result matches the ﬁnding in [15] regarding the capacity of a network to recall strong patterns as mentioned above. Our results therefore show that strong patterns are robust and persistent in the network memory as attachment types and behavioural prototypes are in the human memory system. In this paper, we will several times use Lyapunov’s theorem in probability which provides a simple sufﬁcient condition to generalise the Central Limit theorem when we deal with independent but not necessarily identically distributed random variables. We require a general form of this theorem kn as follows. Let Yn = N i=1 Yni , for n ∈ I , be a triangular array of random variables such that for each n, the random variables Yni , for 1 ≤ i ≤ kn are independent with E(Yni ) = 0 2 2 and E(Yni ) = σni , where E(X) stands for the expected value of the random variable X. Let kn 2 2 sn = i=1 σni . We use the notation X ∼ Y when the two random variables X and Y have the same distribution (for large n if either or both of them depend on n). Theorem 1.1 (Lyapunov’s theorem [6, page 368]) If for some δ > 0, we have the condition: 1 E(|Yn |2+δ |) → 0 s2+δ n d d as n → ∞ then s1 Yn −→ N (0, 1) as n → ∞ where −→ denotes convergence in distribution, and we denote n by N (a, σ 2 ) the normal distribution with mean a and variance σ 2 . Thus, for large n we have Yn ∼ N (0, s2 ). n 2 2 Mean ﬁeld theory We consider a Hopﬁeld network with N neurons i = 1, . . . , N with values Si = ±1 and follow the notations in [20]. As in [15], we assume patterns can be multiply stored and the degree of a pattern is deﬁned as its multiplicity. The total number of patterns, counting their multiplicity, is denoted by p and we assume there are n patterns ξ 1 , . . . , ξ n with degrees d1 , . . . , dn ≥ 1 respectively and that n the remaining p − k=1 dk ≥ 0 patterns are simple, i.e., each has degree one. Note that by our assumptions there are precisely n p0 = p + n − dk k=1 distinct patterns, which we assume are independent and identically distributed with equal probability of taking value ±1 for each node. More generally, for any non-negative integer k ∈ I , we let N p0 dk . µ pk = µ=1 p µ µ 0 1 We use the generalized Hebbian rule for the synaptic couplings: wij = N µ=1 dµ ξi ξj for i = j with wii = 0 for 1 ≤ i, j ≤ N . As in the standard stochastic Hopﬁeld model [20], we use Glauber dynamics [18] for the stochastic updating rule with pseudo-temperature T > 0, which accounts for various types of noise in the network, and assume zero bias in the local ﬁeld. Putting β = 1/T (i.e., with the Boltzmann constant kB = 1) and letting fβ (h) = 1/(1 + exp(−2βh)), the stochastic updating rule at time t is given by: N Pr(Si (t + 1) = ±1) = fβ (±hi (t)), where hi (t) = wij Sj (t), (1) j=1 is the local ﬁeld at i at time t. The updating is implemented asynchronously in a random way. The energy of the network in the conﬁguration S = (Si )N is given by i=1 N 1 Si Sj wij . H(S) = − 2 i,j=1 For large N , this speciﬁes a complex system, with an underlying state space of dimension 2N , which in general cannot be solved exactly. However, mean ﬁeld theory has proved very useful in studying Hopﬁeld networks. The average updated value of Si (t + 1) in Equation (1) is Si (t + 1) = 1/(1 + e−2βhi (t) ) − 1/(1 + e2βhi (t) ) = tanh(βhi (t)), (2) where . . . denotes taking average with respect to the probability distribution in the updating rule in Equation (1). The stationary solution for the mean ﬁeld thus satisﬁes: Si = tanh(βhi ) , (3) The average overlap of pattern ξ µ with the mean ﬁeld at the nodes of the network is given by: mν = 1 N N ν ξi Si (4) i=1 The replica technique for solving the mean ﬁeld problem, used in the case p/N = α > 0 as N → ∞, seeks to obtain the average of the overlaps in Equation (4) by evaluating the partition function of the system, namely, Z = TrS exp(−βH(S)), where the trace TrS stands for taking sum over all possible conﬁgurations S = (Si )N . As it i=1 is generally the case in statistical physics, once the partition function of the system is obtained, 3 all required physical quantities can in principle be computed. However, in this case, the partition function is very difﬁcult to compute since it entails computing the average log Z of log Z, where . . . indicates averaging over the random distribution of the stored patterns ξ µ . To overcome this problem, the identity Zk − 1 log Z = lim k→0 k is used to reduce the problem to ﬁnding the average Z k of Z k , which is then computed for positive integer values of k. For such k, we have: Z k = TrS 1 TrS 2 . . . TrS k exp(−β(H(S 1 ) + H(S 1 ) + . . . + H(S k ))), where for each i = 1, . . . , k the super-scripted conﬁguration S i is a replica of the conﬁguration state. In computing the trace over each replica, various parameters are obtained and the replica symmetry condition assumes that these parameters are independent of the particular replica under consideration. Apart from this assumption, there are two basic mathematical problems with the technique which makes it unjustiﬁable [20, page 264]. Firstly, the positive integer k above is eventually treated as a real number near zero without any mathematical justiﬁcation. Secondly, the order of taking limits, in particular the order of taking the two limits k → 0 and N → ∞, are several times interchanged again without any mathematical justiﬁcation. Here, we develop a mathematically rigorous method for solving the mean ﬁeld problem, i.e., computing the average of the overlaps in Equation (4) in the case of p/N = α > 0 as N → ∞. Our method turns the basic idea of the heuristic presented in [17] and reproduced in [20] for solving the mean ﬁeld equation into a mathematically veriﬁable formalism, which for the standard Hopﬁeld network with random stored patterns gives the same result as the replica method, assuming replica symmetry. In the presence of strong patterns we obtain a set of new results as explained in the next two sections. The mean ﬁeld equation is obtained from Equation (3) by approximating the right hand side of N this equation by the value of tanh at the mean ﬁeld hi = j=1 wij Sj , ignoring the sum N j=1 wij (Sj − Sj ) for large N [17, page 32]: Si = tanh(β hi ) = tanh β N N j=1 p0 µ=1 µ µ dµ ξi ξj Sj . (5) Equation (5) gives the mean ﬁeld equation for the Hopﬁeld network with n possible strong patterns n ξ µ (1 ≤ µ ≤ n) and p − µ=1 dµ simple patterns ξ µ with n + 1 ≤ µ ≤ p0 . As in the standard Hopﬁeld model, where all patterns are simple, we have two cases to deal with. However, we now have to account for the presence of strong attractors and our two cases will be as follows: (i) In the p0 ﬁrst case we assume p2 := µ=1 d2 = o(N ), which includes the simpler case p2 N when p2 µ is ﬁxed and independent of N . (ii) In the second case we assume we have a single strong attractor with the load parameter p/N = α > 0. 3 Stability of strong patterns with noise: p2 = o(N ) The case of constant p and N → ∞ is usually referred to as α = 0 in the standard Hopﬁeld model. Here, we need to consider the sum of degrees of all stored patterns (and not just the number of patterns) compared to N . We solve the mean ﬁeld equation with T > 0 by using a method similar in spirit to [20, page 33] for the standard Hopﬁeld model, but in our case strong patterns induce a sequence of independent but non-identically distributed random variables in the crosstalk term, where the Central Limit Theorem cannot be used; we show however that Lyapunov’s theorem (Theorem (1.1) can be invoked. In retrieving pattern ξ 1 , we look for a solution of the mean ﬁled 1 equation of the form: Si = mξi , where m > 0 is a constant. Using Equation (5) and separating 1 the contribution of ξ in the argument of tanh, we obtain:  1 mξi = tanh    mβ  1 d1 ξi + N 4 µ µ 1 dµ ξi ξj ξj  . j=i,µ>1 (6) For each N , µ > 1 and j = i, let dµ µ µ 1 (7) ξ ξ ξ . N i j j 2 This gives (p0 − 1)(N − 1) independent random variables with E(YN µj ) = 0, E(YN µj ) = d2 /N 2 , µ 3 3 3 and E(|YN µj |) = dµ /N . We have: YN µj = s2 := N 2 E(YN µj ) = µ>1,j=i 1 N −1 d2 ∼ N 2 µ>1 µ N d2 . µ (8) µ>1 Thus, as N → ∞, we have: 1 s3 N 3 E(|YN µj |) ∼ √ µ>1,j=i µ>1 N( d3 µ µ>1 d2 )3/2 µ → 0. (9) as N → ∞ since for positive numbers dµ we always have µ>1 d3 < ( µ>1 d2 )3/2 . Thus the µ µ Lyapunov condition is satisﬁed for δ = 1. By Lyapunov’s theorem we deduce: 1 N µ µ 1 dµ ξi ξj ξj ∼ N d2 /N µ 0, (10) µ>1 µ>1,j=i Since we also have p2 = o(N ), it follows that we can ignore the second term, i.e., the crosstalk term, in the argument of tanh in Equation (6) as N → ∞; we thus obtain: m = tanh βd1 m. (11) To examine the ﬁxed points of the Equation (11), we let d = d1 for convenience and put x = βdm = dm/T , so that tanh x = T x/d; see Figure 1. It follows that Tc = d is the critical temperature. If T < d then there is a non-zero (non-trivial) solution for m, whereas for T > d we only have the trivial solution. For d = 1 our solution is that of the standard Hopﬁeld network as in [20, page 34]. (d < T) y>x y = x ( d = T) y = tanh x y</p><p>same-paper 2 0.88025391 <a title="145-lda-2" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><p>3 0.85130638 <a title="145-lda-3" href="./nips-2013-Pass-efficient_unsupervised_feature_selection.html">245 nips-2013-Pass-efficient unsupervised feature selection</a></p>
<p>Author: Crystal Maung, Haim Schweitzer</p><p>Abstract: The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm, a modiﬁcation of the classical pivoted QR algorithm of Businger and Golub, that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the ﬁnal selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm, and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of several orders of magnitude over the classical algorithm. These results appear to be competitive with recently proposed randomized algorithms in terms of pass efﬁciency and run time. On the other hand, the randomized algorithms may produce more accurate features, at the cost of small probability of failure. 1</p><p>4 0.8000465 <a title="145-lda-4" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>5 0.73914391 <a title="145-lda-5" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>Author: Francesca Petralia, Joshua T. Vogelstein, David Dunson</p><p>Abstract: Nonparametric estimation of the conditional distribution of a response given highdimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change ﬂexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efﬁciently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. 1</p><p>6 0.73121965 <a title="145-lda-6" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>7 0.53640604 <a title="145-lda-7" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>8 0.5257448 <a title="145-lda-8" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>9 0.51632971 <a title="145-lda-9" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>10 0.51384854 <a title="145-lda-10" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>11 0.50116313 <a title="145-lda-11" href="./nips-2013-Bayesian_Hierarchical_Community_Discovery.html">47 nips-2013-Bayesian Hierarchical Community Discovery</a></p>
<p>12 0.50100476 <a title="145-lda-12" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>13 0.49597266 <a title="145-lda-13" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>14 0.48849031 <a title="145-lda-14" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>15 0.48630759 <a title="145-lda-15" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>16 0.48296079 <a title="145-lda-16" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>17 0.48043039 <a title="145-lda-17" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>18 0.47706196 <a title="145-lda-18" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>19 0.47681814 <a title="145-lda-19" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>20 0.47440189 <a title="145-lda-20" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
