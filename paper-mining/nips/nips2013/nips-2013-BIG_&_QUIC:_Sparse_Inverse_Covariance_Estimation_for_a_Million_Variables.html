<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-45" href="#">nips2013-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</h1>
<br/><p>Source: <a title="nips-2013-45-pdf" href="http://papers.nips.cc/paper/4923-big-quic-sparse-inverse-covariance-estimation-for-a-million-variables.pdf">pdf</a></p><p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>Reference: <a title="nips-2013-45-reference" href="../nips2013_reference/nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. [sent-8, score-0.172]
</p><p>2 In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. [sent-11, score-0.129]
</p><p>3 Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. [sent-13, score-0.27]
</p><p>4 An important problem is that of recovering the covariance matrix (or its inverse) of this distribution, given the n samples, in a high-dimensional regime where n p. [sent-19, score-0.084]
</p><p>5 Complementary techniques such as exact covariance thresholding [13, 19], and the divide and conquer approach of [8], have also been proposed to speed up the solvers. [sent-23, score-0.084]
</p><p>6 However the caveat with these estimators is that they are not guaranteed to yield a positive-deﬁnite covariance matrix, and typically yield less accurate parameters. [sent-26, score-0.082]
</p><p>7 What if we want to solve the M -estimator in (1) with a million variables? [sent-27, score-0.095]
</p><p>8 Note that the number of parameters in (1) is quadratic in the number of variables, so that for a million variables, we would 1  have a trillion parameters. [sent-28, score-0.098]
</p><p>9 Here we ask the following ambitious but simple question: can we solve the M -estimator in (1) with a million variables using a single machine with bounded memory? [sent-31, score-0.095]
</p><p>10 Our method can solve one million dimensional problems with 1000 billion variables using a single machine with 32 cores and 32G memory. [sent-34, score-0.171]
</p><p>11 The key bottleneck with QUIC stems from the memory required to store the gradient W = X −1 of the iterates X, which is a dense p × p matrix, and the computation of the log-determinant function of a p × p matrix. [sent-36, score-0.236]
</p><p>12 A starting point to reduce the memory footprint is to use sparse representations for the iterates X and compute the elements of the empirical covariance matrix S on demand from the sample data points. [sent-37, score-0.288]
</p><p>13 In addition we also have to avoid the storage of the dense matrix X −1 and perform intermediate computations involving functions of such dense matrices on demand. [sent-38, score-0.127]
</p><p>14 Our ﬁrst is to carry out the coordinate descent computations in a blockwise manner, and by selecting the blocks very carefully using an automated clustering scheme, we not only leverage sparsity of the iterates, but help cache computations suitably. [sent-41, score-0.411]
</p><p>15 Lastly, since the Hessian computation is a key bottleneck in the second-order method, we compute it inexactly. [sent-43, score-0.083]
</p><p>16 We show that even with these modiﬁcations and inexact computations, we can still guarantee not only convergence of our overall procedure, but can easily control the degree of approximation of Hessian to achieve super-linear or even quadratic convergence rates. [sent-44, score-0.148]
</p><p>17 Inspite of our low-memory footprint, these innovations allow us to beat the state of the art DC-QUIC algorithm (which has no memory limits) in computational complexity even on medium-size problems of a few thousand variables. [sent-45, score-0.095]
</p><p>18 Finally, we show how to parallelize our method in a multicore shared memory system. [sent-46, score-0.093]
</p><p>19 In Section 2, we brieﬂy review the QUIC algorithm and outline the difﬁculties of scaling QUIC to million dimensional data. [sent-48, score-0.103]
</p><p>20 2  Difﬁculties in scaling QUIC to million dimensional data  Our proposed algorithm is based on the framework of QUIC [9]; which is a state of the art procedure for solving (1), based on a second-order optimization method. [sent-51, score-0.103]
</p><p>21 QUIC is a second-order method that iteratively solves for a generalized Newton direction using coordinate descent; and then descends using this generalized Newton direction and line-search. [sent-54, score-0.14]
</p><p>22 To leverage the sparsity of the solution, the variables are partitioned into Sf ixed and Sf ree sets: Xij ∈ Sf ixed if | ij g(X)| ≤ λij , and Xij = 0, Xij ∈ Sf ree otherwise. [sent-55, score-0.218]
</p><p>23 (2) Only the free set Sf ree is updated at each Newton iteration, reducing the number of variables to be updated to m = |Sf ree |, which is comparable to X ∗ 0 , the sparsity of the solution. [sent-56, score-0.164]
</p><p>24 When Xt is sparse, the Newton direction computation (3) can be solved 2  efﬁciently by coordinate descent [9]. [sent-60, score-0.198]
</p><p>25 The obvious implementation calls for the computation and −1 2 T storage of Wt = Xt ; using this to compute a = Wij + Wii Wjj , b = Sij − Wij + wi Dwj , and c = Xij + Dij . [sent-61, score-0.144]
</p><p>26 Armed with these quantities, the coordinate descent update for variable Dij takes the form: Dij ← Dij − c + S(c − b/a, λij /a), (5) where S(z, r) = sign(z) max{|z| − r, 0} is the soft-thresholding function. [sent-62, score-0.207]
</p><p>27 T The key computational bottleneck here is in computing the terms wi Dwj , which take O(p2 ) time when implemented naively. [sent-63, score-0.121]
</p><p>28 However, this is not a strategy we can use when dealing with very large data sets: storing the p by p dense matrices U and W in memory would be prohibitive. [sent-65, score-0.1]
</p><p>29 Our key innovation to address this is a novel block coordinate descent scheme, detailed in Section 3. [sent-67, score-0.249]
</p><p>30 1, that also uses clustering to strike a balance between memory use and computational cost while exploiting sparsity. [sent-68, score-0.108]
</p><p>31 (6) The key computational bottleneck is checking positive deﬁniteness (typically by computing the smallest eigenvalue), and the computation of the determinant of a sparse matrix with dimension that can reach a million. [sent-76, score-0.132]
</p><p>32 The computation only uses memory proportional to the number of nonzeros in the iterate. [sent-81, score-0.153]
</p><p>33 Many other difﬁculties arise when dealing with large sparse matrices in the sparse inverse covairance problem. [sent-82, score-0.138]
</p><p>34 We assume that the iterates Xt have m nonzero elements, and that each iterate is stored in memory using a sparse format. [sent-86, score-0.186]
</p><p>35 We denote the size of the free set by s and observe that it is usually very small and just a constant factor larger than m∗ , the number of nonzeros in the ﬁnal solution [9]. [sent-87, score-0.098]
</p><p>36 Also, the sample covariance matrix is stored in its factor form S = Y Y T , where Y is the normalized sample matrix. [sent-88, score-0.121]
</p><p>37 We now discuss a crucial element of B IG QUIC, our novel block coordinate descent scheme for solving each subproblem (3). [sent-89, score-0.297]
</p><p>38 1  Block Coordinate Descent method  The most expensive step during the coordinate descent update for Dij is the computation of T wi Dwj , where wi is the i-th column of W = X −1 ; see (5). [sent-91, score-0.414]
</p><p>39 Note that wi is the solution of the linear system Xwi = ei . [sent-93, score-0.091]
</p><p>40 We thus use the conjugate gradient method (CG) to compute wi , leveraging the fact that X is a positive deﬁnite matrix. [sent-94, score-0.119]
</p><p>41 This solver requires only matrix vector products, which can be efﬁciently implemented for the sparse matrix X. [sent-95, score-0.104]
</p><p>42 A single step of coordinate descent requires the solution of two linear systems Xwi = ei and Xwj = ej which yield the vectors wi , wj , and we can then compute T wi Dwj . [sent-98, score-0.383]
</p><p>43 The time complexity for each update would require O(mT +s) operations, and the overall complexity will be O(msT +s2 ) for one full sweep through the entire matrix. [sent-99, score-0.08]
</p><p>44 Our Approach: Block Coordinate Descent with memory cache scheme. [sent-101, score-0.129]
</p><p>45 In the following we present a block coordinate descent scheme that can accelerate the update procedure by storing and 3  reusing more results of the intermediate computations. [sent-102, score-0.306]
</p><p>46 The resulting increased memory use and speedup is controlled by the number of blocks employed, that we denote by k. [sent-103, score-0.173]
</p><p>47 In order to update Dij , we need both wi and wj ; if either one is not directly available, we have to recompute it by CG and we call this a “cache miss”. [sent-105, score-0.125]
</p><p>48 A good update sequence can minimize the cache miss rate. [sent-106, score-0.122]
</p><p>49 While it is hard to ﬁnd the optimal sequence in general, we successfully applied a block by block update sequence with a careful clustering scheme, where the number of cache misses is sufﬁciently small. [sent-107, score-0.307]
</p><p>50 Assume we pick k such that we can store p/k columns of W (p2 /k elements) in memory. [sent-108, score-0.085]
</p><p>51 We divide matrix D into k × k blocks accordingly. [sent-116, score-0.088]
</p><p>52 Within each block we run Tinner sweeps over variables within that block, and in the outer iteration we sweep through all the blocks Touter times. [sent-117, score-0.183]
</p><p>53 To update the variables in the block (Sz , Sq ) of D, we ﬁrst compute WSz and WSq by CG and store it in memory, meaning that there is no cache miss during the within-block coordinate updates. [sent-120, score-0.388]
</p><p>54 With Usq = DWSq maintained, the update for Dij can be T computed by wi uj when i ∈ Sz and j ∈ Sq . [sent-121, score-0.154]
</p><p>55 The above coordinate update computations cost only O(p/k) operations because we only update a subset of the columns. [sent-123, score-0.218]
</p><p>56 Before running coordinate descent for the block we compute and store Pij = (wi )Tzq (uj )Szq for all (i, j) ¯¯ S ¯¯ T in the free set of the current block, where Szq = {i | i ∈ Sz and i ∈ Sq }. [sent-126, score-0.369]
</p><p>57 The term wi uj for / / ¯¯ T T T updating Dij can then be computed by wi uj = Pij + wSz uSz + wSq uSq . [sent-127, score-0.24]
</p><p>58 With this trick, each coordinate descent step within the block only takes O(p/k) time, and we only need to store USz ,Sq , which only requires O(p2 /k 2 ) memory. [sent-128, score-0.301]
</p><p>59 Computing Pij takes O(p) time for each i, j, so if we update each coordinate Tinner times within a block, the time complexity is O(p + Tinner p/k) and the amortized cost per coordinate update is only O(p/Tinner + p/k). [sent-129, score-0.288]
</p><p>60 Since all of them share {wi | i ∈ Sz }, we ﬁrst compute them and store in memory. [sent-139, score-0.08]
</p><p>61 When updating an off-diagonal block (Sz , Sq ), if the free sets are dense, we need to compute and store {wi | i ∈ Sq }. [sent-140, score-0.196]
</p><p>62 So totally each block of W will be computed k times. [sent-141, score-0.076]
</p><p>63 Assume the nonzeros in X is close to the size of free set (m ≈ s), then each coordinate update costs O(kpT ) ﬂops. [sent-143, score-0.242]
</p><p>64 We now show that a careful selection of the blocks using a clustering scheme can lead to dramatic speedup for block coordinate descent. [sent-145, score-0.374]
</p><p>65 When updating variables in the block (Sz , Sq ), we would need the column wj only if some variable in {Dij | i ∈ Sz } lies in the free set. [sent-146, score-0.116]
</p><p>66 The number of columns to be computed in one sweep is then given by p + z=q |B(Sz , Sq )|. [sent-150, score-0.079]
</p><p>67 This suggests the use of graph clustering algorithms, such as METIS [10] or Graclus [5] which minimize the right hand side. [sent-157, score-0.074]
</p><p>68 Compared to block coordinate descent with random partition, which generally needs to compute 228483 × 20 columns, the clustering resulted in the computation of 228483 + 8697 columns, thus achieved an almost 20 times speedup. [sent-161, score-0.34]
</p><p>69 2  Line Search  The line search step requires an efﬁcient and scalable procedure that computes log det(A) and checks the positive deﬁniteness of a sparse matrix A. [sent-165, score-0.077]
</p><p>70 We note that computing log det(A) for a large sparse matrix A for which we only have a matrix-vector multiplication subroutine available is an interesting subproblem on its own and we expect that numerous other applications may beneﬁt from the approach presented below. [sent-167, score-0.102]
</p><p>71 In summary, the time needed to compute the columns of W in block coordinate descent, O((p + |B|)mT Touter ), dominates the time complexity, which underscores the importance of minimizing the number of boundary nodes |B| via our clustering scheme. [sent-189, score-0.321]
</p><p>72 3 Run graph clustering algorithm based on absolute values on free set. [sent-195, score-0.114]
</p><p>73 The only sequential part is the coordinate update in step 11, but note, (see Section 3. [sent-220, score-0.144]
</p><p>74 Second, we show that by a careful control of the error in the Hessian computation, B IG QUIC can still achieve a quadratic rate of convergence in terms of Newton iterations. [sent-224, score-0.081]
</p><p>75 In the B IG QUIC algorithm, we compute wi in two places. [sent-227, score-0.119]
</p><p>76 At the ﬁrst glance they are equivalent and can be computed simultaneously, but it turns out that by carefully analysing the difference between two types of wi , we can achieve much faster convergence, as discussed below. [sent-230, score-0.091]
</p><p>77 The key observation is that we only require the gradient Wij for all (i, j) ∈ Sf ree to conduct coordinate descent updates. [sent-231, score-0.235]
</p><p>78 Since the free set is very sparse and can ﬁt in memory, those Wij only T need to be computed once and stored in memory. [sent-232, score-0.127]
</p><p>79 On the other hand, the computation of wi Dwj corresponds to the Hessian computation, and we need two columns for each coordinate update, which has to be computed repeatedly. [sent-233, score-0.259]
</p><p>80 On the other hand, wi for the Hessian has to be computed repeatedly, so we do ˆ ˆ ˆ not want to spend too much time to compute each of them accurately. [sent-237, score-0.119]
</p><p>81 We ﬁrst introduce the following notion of minimum norm subgradient to measure the optimality of X: gradS f (X) = ij  ij g(X) + sign(Xij )λij sign( ij g(X)) max(| ij g(X)| − λij , 0)  if Xij = 0, if Xij = 0. [sent-246, score-0.16]
</p><p>82 The following theorem then shows that if we compute Hessian more and more accurately, B IG QUIC will have a super-linear or even quadratic convergence rate. [sent-247, score-0.085]
</p><p>83 Theorem 2 suggests that we can achieve super-linear, or even quadratic convergence rate by a careful control of the approximated Hessian ˆ ˆ Ht . [sent-254, score-0.081]
</p><p>84 In all the experiments, B IG QUIC can solve larger problems than QUIC even with a single core, and using 32 cores B IG QUIC can solve million dimensional data in one day. [sent-259, score-0.197]
</p><p>85 Suppose the residual is bi = X wi − ei for each i = 1, . [sent-261, score-0.091]
</p><p>86 In all of our experiments, number of nonzeros during the optimization phase do not exceed 5 X ∗ 0 in intermediate steps, therefore we can always store the sparse representation of Xt in memory. [sent-293, score-0.16]
</p><p>87 For B IG QUIC, we set blocks k to be the smallest number such that p/k columns of W can ﬁt into 32G memory. [sent-294, score-0.094]
</p><p>88 We can see that B IG QUIC can solve one million dimensional chain graphs and random graphs in one day, and handle the full fMRI dataset in about 5 hours. [sent-297, score-0.129]
</p><p>89 Figure 3 shows the speedup under a multicore shared memory machine. [sent-301, score-0.135]
</p><p>90 B IG QUIC can achieve about 14 times speedup using 16 cores, and 20 times speedup when using 32 cores. [sent-302, score-0.084]
</p><p>91 org) comprised 36 resting fMRI sessions collected across different days using whole-brain multiband EPI acquisition, each lasting 10 minutes (TR=1. [sent-307, score-0.079]
</p><p>92 Degree was computed for each vertex; high-degree regions were primarily found in gray matter regions, suggesting that the method successfully identiﬁed plausible functional connections (see left panel of Figure 4). [sent-321, score-0.094]
</p><p>93 Modularity-based clustering [1] was applied to the graph, resulting in 60 modules that exceeded the threshold size of 100 vertices. [sent-323, score-0.079]
</p><p>94 A number of neurobiologically plausible resting-state networks were identiﬁed, including “default mode” and sensorimotor networks (right panel of Figure 4). [sent-324, score-0.094]
</p><p>95 For both neurally plausible and artifactual modules, the modules detected by B IG QUIC are similar to those identiﬁed using independent components analysis on the same dataset, without the need for the extensive dimensionality reduction (without statistical guarantees) inherent in such techniques. [sent-328, score-0.075]
</p><p>96 Right panel: Left-hemisphere surface renderings of two network modules obtained through graph clustering. [sent-332, score-0.077]
</p><p>97 Top panel shows a sensorimotor network, bottom panel shows medial prefrontal, posterior cingulate, and lateral temporoparietal regions characteristic of the “default mode” generally observed during the resting state. [sent-333, score-0.169]
</p><p>98 A constrained 1 minimization approach to sparse precision matrix estimation. [sent-361, score-0.077]
</p><p>99 An inexact interior point method for 1 -reguarlized sparse covariance selection. [sent-438, score-0.142]
</p><p>100 Learning sparse Gaussian Markov networks using a greedy coordinate ascent approach. [sent-474, score-0.16]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('quic', 0.671), ('ig', 0.47), ('sq', 0.181), ('sz', 0.143), ('dij', 0.112), ('coordinate', 0.11), ('newton', 0.105), ('xt', 0.104), ('fmri', 0.098), ('wi', 0.091), ('hessian', 0.085), ('sf', 0.081), ('block', 0.076), ('dwj', 0.076), ('grads', 0.076), ('memory', 0.07), ('million', 0.069), ('descent', 0.063), ('ree', 0.062), ('blocks', 0.061), ('xij', 0.061), ('det', 0.06), ('cache', 0.059), ('nonzeros', 0.058), ('covariance', 0.057), ('store', 0.052), ('sparse', 0.05), ('resting', 0.049), ('sweep', 0.046), ('tinner', 0.046), ('touter', 0.046), ('usq', 0.046), ('wsq', 0.046), ('speedup', 0.042), ('cores', 0.042), ('modules', 0.041), ('free', 0.04), ('ij', 0.04), ('computations', 0.04), ('cg', 0.039), ('ht', 0.039), ('inverse', 0.038), ('clustering', 0.038), ('stored', 0.037), ('bt', 0.036), ('boundary', 0.036), ('pij', 0.036), ('graph', 0.036), ('panel', 0.035), ('inexact', 0.035), ('update', 0.034), ('plausible', 0.034), ('dimensional', 0.034), ('columns', 0.033), ('dhillon', 0.032), ('voxels', 0.031), ('descends', 0.03), ('multiband', 0.03), ('scheinberg', 0.03), ('scrubbing', 0.03), ('szq', 0.03), ('ujt', 0.03), ('usz', 0.03), ('wsz', 0.03), ('xwi', 0.03), ('appendix', 0.03), ('bottleneck', 0.03), ('dense', 0.03), ('uj', 0.029), ('iterates', 0.029), ('quadratic', 0.029), ('miss', 0.029), ('compute', 0.028), ('hsieh', 0.028), ('niteness', 0.028), ('convergence', 0.028), ('degree', 0.028), ('wij', 0.028), ('matrix', 0.027), ('conquer', 0.027), ('epi', 0.027), ('footprint', 0.027), ('gmrf', 0.027), ('ixed', 0.027), ('metis', 0.027), ('poldrack', 0.027), ('sustik', 0.027), ('solve', 0.026), ('regions', 0.025), ('innovations', 0.025), ('computation', 0.025), ('caveat', 0.025), ('sensorimotor', 0.025), ('subproblem', 0.025), ('careful', 0.024), ('mle', 0.024), ('acknowledges', 0.024), ('scheme', 0.023), ('uit', 0.023), ('multicore', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="45-tfidf-1" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>2 0.17223902 <a title="45-tfidf-2" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>Author: Huahua Wang, Arindam Banerjee, Cho-Jui Hsieh, Pradeep Ravikumar, Inderjit Dhillon</p><p>Abstract: We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in columnblocks and only involves elementwise operations and parallel matrix multiplications. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efﬁciency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores. 1</p><p>3 0.079390712 <a title="45-tfidf-3" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>Author: Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain</p><p>Abstract: We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2 ) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2 ). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) samplecomplexity – the ﬁrst algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data. 1</p><p>4 0.068716556 <a title="45-tfidf-4" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>Author: Maria-Florina Balcan, Vitaly Feldman</p><p>Abstract: We describe a framework for designing efﬁcient active learning algorithms that are tolerant to random classiﬁcation noise and differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of ﬁltered random examples. It builds on the powerful statistical query framework of Kearns [30]. We show that any efﬁcient active statistical learning algorithm can be automatically converted to an efﬁcient active learning algorithm which is tolerant to random classiﬁcation noise as well as other forms of “uncorrelated” noise. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efﬁciently actively learned in our framework. These results combined with our generic conversion lead to the ﬁrst computationally-efﬁcient algorithms for actively learning some of these concept classes in the presence of random classiﬁcation noise that provide exponential improvement in the dependence on the error over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efﬁcient active differentially-private algorithms. This leads to the ﬁrst differentially-private active learning algorithms with exponential label savings over the passive case. 1</p><p>5 0.067647837 <a title="45-tfidf-5" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>Author: Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, Guillermo Sapiro</p><p>Abstract: Graph matching is a challenging problem with very important applications in a wide range of ﬁelds, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efﬁciently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. 1</p><p>6 0.067286864 <a title="45-tfidf-6" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>7 0.064952418 <a title="45-tfidf-7" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>8 0.06398122 <a title="45-tfidf-8" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>9 0.062057249 <a title="45-tfidf-9" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>10 0.060450561 <a title="45-tfidf-10" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>11 0.056320284 <a title="45-tfidf-11" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>12 0.051857952 <a title="45-tfidf-12" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>13 0.049903154 <a title="45-tfidf-13" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>14 0.049708128 <a title="45-tfidf-14" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>15 0.049675893 <a title="45-tfidf-15" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>16 0.049664732 <a title="45-tfidf-16" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>17 0.049414441 <a title="45-tfidf-17" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>18 0.048704118 <a title="45-tfidf-18" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>19 0.047335941 <a title="45-tfidf-19" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>20 0.046486136 <a title="45-tfidf-20" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.036), (2, 0.05), (3, 0.002), (4, -0.017), (5, 0.056), (6, -0.045), (7, 0.037), (8, 0.015), (9, -0.043), (10, -0.018), (11, -0.061), (12, 0.055), (13, -0.046), (14, -0.079), (15, 0.023), (16, -0.049), (17, 0.017), (18, -0.008), (19, 0.027), (20, 0.022), (21, -0.05), (22, 0.038), (23, -0.023), (24, 0.015), (25, -0.029), (26, 0.002), (27, -0.029), (28, -0.091), (29, -0.023), (30, 0.045), (31, 0.036), (32, 0.01), (33, 0.069), (34, 0.006), (35, 0.01), (36, -0.005), (37, 0.056), (38, 0.032), (39, 0.013), (40, 0.09), (41, -0.037), (42, -0.008), (43, 0.114), (44, -0.105), (45, -0.024), (46, -0.081), (47, -0.018), (48, 0.021), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91415995 <a title="45-lsi-1" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>2 0.77761847 <a title="45-lsi-2" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>Author: Huahua Wang, Arindam Banerjee, Cho-Jui Hsieh, Pradeep Ravikumar, Inderjit Dhillon</p><p>Abstract: We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in columnblocks and only involves elementwise operations and parallel matrix multiplications. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efﬁciency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores. 1</p><p>3 0.61158127 <a title="45-lsi-3" href="./nips-2013-More_Effective_Distributed_ML_via_a_Stale_Synchronous_Parallel_Parameter_Server.html">198 nips-2013-More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</a></p>
<p>Author: Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Greg Ganger, Eric Xing</p><p>Abstract: We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model’s values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This signiﬁcantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes. 1</p><p>4 0.61005867 <a title="45-lsi-4" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>Author: Fajwel Fogel, Rodolphe Jenatton, Francis Bach, Alexandre D'Aspremont</p><p>Abstract: Seriation seeks to reconstruct a linear order between variables using unsorted similarity information. It has direct applications in archeology and shotgun gene sequencing for example. We prove the equivalence between the seriation and the combinatorial 2-SUM problem (a quadratic minimization problem over permutations) over a class of similarity matrices. The seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-SUM problem to improve the robustness of solutions in a noisy setting. This relaxation also allows us to impose additional structural constraints on the solution, to solve semi-supervised seriation problems. We present numerical experiments on archeological data, Markov chains and gene sequences. 1</p><p>5 0.55804121 <a title="45-lsi-5" href="./nips-2013-On_Algorithms_for_Sparse_Multi-factor_NMF.html">214 nips-2013-On Algorithms for Sparse Multi-factor NMF</a></p>
<p>Author: Siwei Lyu, Xin Wang</p><p>Abstract: Nonnegative matrix factorization (NMF) is a popular data analysis method, the objective of which is to approximate a matrix with all nonnegative components into the product of two nonnegative matrices. In this work, we describe a new simple and efﬁcient algorithm for multi-factor nonnegative matrix factorization (mfNMF) problem that generalizes the original NMF problem to more than two factors. Furthermore, we extend the mfNMF algorithm to incorporate a regularizer based on the Dirichlet distribution to encourage the sparsity of the components of the obtained factors. Our sparse mfNMF algorithm affords a closed form and an intuitive interpretation, and is more efﬁcient in comparison with previous works that use ﬁx point iterations. We demonstrate the effectiveness and efﬁciency of our algorithms on both synthetic and real data sets. 1</p><p>6 0.53100634 <a title="45-lsi-6" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>7 0.52996266 <a title="45-lsi-7" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>8 0.52014345 <a title="45-lsi-8" href="./nips-2013-Accelerated_Mini-Batch_Stochastic_Dual_Coordinate_Ascent.html">19 nips-2013-Accelerated Mini-Batch Stochastic Dual Coordinate Ascent</a></p>
<p>9 0.51892787 <a title="45-lsi-9" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>10 0.51883465 <a title="45-lsi-10" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>11 0.50728375 <a title="45-lsi-11" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<p>12 0.50043762 <a title="45-lsi-12" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>13 0.49438176 <a title="45-lsi-13" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>14 0.490585 <a title="45-lsi-14" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>15 0.49051216 <a title="45-lsi-15" href="./nips-2013-Solving_inverse_problem_of_Markov_chain_with_partial_observations.html">299 nips-2013-Solving inverse problem of Markov chain with partial observations</a></p>
<p>16 0.48581487 <a title="45-lsi-16" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>17 0.47773877 <a title="45-lsi-17" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>18 0.47446913 <a title="45-lsi-18" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>19 0.47416374 <a title="45-lsi-19" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>20 0.47410008 <a title="45-lsi-20" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.057), (33, 0.112), (34, 0.088), (41, 0.041), (49, 0.038), (56, 0.082), (70, 0.032), (74, 0.222), (85, 0.058), (89, 0.044), (93, 0.089), (95, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.77584141 <a title="45-lda-1" href="./nips-2013-Global_Solver_and_Its_Efficient_Approximation_for_Variational_Bayesian_Low-rank_Subspace_Clustering.html">133 nips-2013-Global Solver and Its Efficient Approximation for Variational Bayesian Low-rank Subspace Clustering</a></p>
<p>Author: Shinichi Nakajima, Akiko Takeda, S. Derin Babacan, Masashi Sugiyama, Ichiro Takeuchi</p><p>Abstract: When a probabilistic model and its prior are given, Bayesian learning offers inference with automatic parameter tuning. However, Bayesian learning is often obstructed by computational difﬁculty: the rigorous Bayesian learning is intractable in many models, and its variational Bayesian (VB) approximation is prone to suffer from local minima. In this paper, we overcome this difﬁculty for low-rank subspace clustering (LRSC) by providing an exact global solver and its efﬁcient approximation. LRSC extracts a low-dimensional structure of data by embedding samples into the union of low-dimensional subspaces, and its variational Bayesian variant has shown good performance. We ﬁrst prove a key property that the VBLRSC model is highly redundant. Thanks to this property, the optimization problem of VB-LRSC can be separated into small subproblems, each of which has only a small number of unknown variables. Our exact global solver relies on another key property that the stationary condition of each subproblem consists of a set of polynomial equations, which is solvable with the homotopy method. For further computational efﬁciency, we also propose an efﬁcient approximate variant, of which the stationary condition can be written as a polynomial equation with a single variable. Experimental results show the usefulness of our approach. 1</p><p>2 0.76850319 <a title="45-lda-2" href="./nips-2013-Translating_Embeddings_for_Modeling_Multi-relational_Data.html">336 nips-2013-Translating Embeddings for Modeling Multi-relational Data</a></p>
<p>Author: Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko</p><p>Abstract: We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples. 1</p><p>same-paper 3 0.75936317 <a title="45-lda-3" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>4 0.72136348 <a title="45-lda-4" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<p>Author: Ping Li, Gennady Samorodnitsk, John Hopcroft</p><p>Abstract: The method of stable random projections is useful for efﬁciently approximating the lα distance (0 < α ≤ 2) in high dimension and it is naturally suitable for data streams. In this paper, we propose to use only the signs of the projected data and we analyze the probability of collision (i.e., when the two signs differ). Interestingly, when α = 1 (i.e., Cauchy random projections), we show that the probability of collision can be accurately approximated as functions of the chi-square (χ2 ) similarity. In text and vision applications, the χ2 similarity is a popular measure when the features are generated from histograms (which are a typical example of data streams). Experiments conﬁrm that the proposed method is promising for large-scale learning applications. The full paper is available at arXiv:1308.1009. There are many future research problems. For example, when α → 0, the collision probability is a function of the resemblance (of the binary-quantized data). This provides an effective mechanism for resemblance estimation in data streams. 1</p><p>5 0.71803159 <a title="45-lda-5" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>6 0.70384818 <a title="45-lda-6" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>7 0.6568768 <a title="45-lda-7" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>8 0.64999056 <a title="45-lda-8" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>9 0.64295238 <a title="45-lda-9" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>10 0.6422984 <a title="45-lda-10" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>11 0.6401974 <a title="45-lda-11" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>12 0.63997948 <a title="45-lda-12" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>13 0.63970757 <a title="45-lda-13" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>14 0.63969743 <a title="45-lda-14" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>15 0.63843733 <a title="45-lda-15" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>16 0.63807464 <a title="45-lda-16" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>17 0.63759756 <a title="45-lda-17" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<p>18 0.63735813 <a title="45-lda-18" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>19 0.63703018 <a title="45-lda-19" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>20 0.63671511 <a title="45-lda-20" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
