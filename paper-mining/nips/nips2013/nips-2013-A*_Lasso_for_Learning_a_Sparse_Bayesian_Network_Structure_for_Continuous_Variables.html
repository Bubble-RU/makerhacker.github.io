<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-3" href="#">nips2013-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</h1>
<br/><p>Source: <a title="nips-2013-3-pdf" href="http://papers.nips.cc/paper/5174-a-lasso-for-learning-a-sparse-bayesian-network-structure-for-continuous-variables.pdf">pdf</a></p><p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>Reference: <a title="nips-2013-3-reference" href="../nips2013_reference/nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. [sent-6, score-0.292]
</p><p>2 Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. [sent-7, score-0.297]
</p><p>3 Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. [sent-8, score-0.157]
</p><p>4 In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. [sent-9, score-0.408]
</p><p>5 We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. [sent-11, score-0.325]
</p><p>6 However, learning a Bayesian network structure from data has been known to be an NP-hard problem [1] because of the constraint that the network structure has to be a directed acyclic graph (DAG). [sent-14, score-0.218]
</p><p>7 Approximate methods based on heuristic search are more computationally efﬁcient, but they recover a suboptimal structure. [sent-16, score-0.155]
</p><p>8 Many of the existing algorithms are based on scoring each candidate graph and ﬁnding a graph with the best score, where the score decomposes for each variable given its parents in a DAG. [sent-18, score-0.203]
</p><p>9 , MDL [9], BIC [14], and BDe [4]), most of these algorithms, whether exact methods or heuristic search techniques, have a two-stage learning process. [sent-21, score-0.155]
</p><p>10 Then, Stage 2 employs various algorithms to search for the best-scoring network structure that satisﬁes the DAG constraint by limiting the search space to the candidate parent sets from Stage 1. [sent-23, score-0.355]
</p><p>11 For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed. [sent-25, score-0.237]
</p><p>12 For continuous variables, L1 -regularized Markov blanket (L1MB) [13] was proposed as a two-stage method that uses lasso to select candidate parents for each variable in Stage 1 and performs heuristic search for DAG structure and variable ordering in Stage 2. [sent-27, score-0.605]
</p><p>13 Although a two-stage approach can reduce the search space by pruning candidate parent sets in Stage 1, Huang et al. [sent-28, score-0.178]
</p><p>14 [5] observed that applying lasso in Stage 1 as in L1MB is likely to miss the true parents in a high-dimensional setting, thereby limiting the quality of the solution in Stage 2. [sent-29, score-0.311]
</p><p>15 They proposed the sparse Bayesian network (SBN) algorithm that formulates the problem of Bayesian network structure learning as a singlestage optimization problem and transforms it into a lasso-type optimization to obtain an approximate solution. [sent-30, score-0.155]
</p><p>16 Then, they applied a heuristic search to reﬁne the solution as a post-processing step. [sent-31, score-0.155]
</p><p>17 Our method is a single-stage algorithm that ﬁnds the optimal network structure with a sparse set of parents while ensuring the DAG constraint is satisﬁed. [sent-33, score-0.18]
</p><p>18 While previous approaches based on DP required identifying the exponential number of candidate parent sets and their scores for each variable in Stage 1 before applying DP in Stage 2 [7, 15], our approach effectively combines the score computation in Stage 1 within Stage 2 via lasso optimization. [sent-35, score-0.335]
</p><p>19 Then, we present A* lasso which signiﬁcantly prunes the search space of DP by incorporating the A* search algorithm [12], while guaranteeing the optimality of the solution. [sent-36, score-0.411]
</p><p>20 Since in practice, A* search can still be expensive compared to heuristic methods, we explore heuristic schemes that further limit the search space of A* lasso. [sent-37, score-0.324]
</p><p>21 We demonstrate in our experiments that this heuristic approach can substantially improve the computation time without signiﬁcantly compromising the quality of the solution, especially on large Bayesian networks. [sent-38, score-0.153]
</p><p>22 , vp }, where each node vj is associated with a random variable Xj [8]. [sent-42, score-0.239]
</p><p>23 , Xp ) = p j=1 p(Xj |Pa(Xj )), where p(Xj |Pa(Xj )) is the conditional probability distribution for Xj given its parents Pa(Xj ) with directed edges from each node in Pa(Xj ) to Xj in G. [sent-46, score-0.144]
</p><p>24 G ∈ DAG,  1  (1)  j=1  where x−j represents all columns of X excluding xj , assuming all other variables are candidate parents of node vj . [sent-59, score-0.379]
</p><p>25 Given the estimate of βj ’s, the set of parents for node vj can be found as the support of βj , S(βj ) = {vi |βji = 0}. [sent-60, score-0.291]
</p><p>26 We notice that if the acyclicity constraint is ignored, Equation (1) decomposes into individual lasso estimations for each node: LassoScore(vj |V \vj ) = min βj  xj − x−j βj 2  2 2  +λ  βj  1,  where V \vj represents the set of all nodes in V excluding vj . [sent-62, score-0.496]
</p><p>27 The above lasso optimization problem can be solved efﬁciently with the shooting algorithm [3]. [sent-63, score-0.233]
</p><p>28 1  A* Lasso for Bayesian Network Structure Learning Dynamic Programming with Lasso  The problem of learning a Bayesian network structure that satisﬁes the constraint of no directed cycles can be cast as that of learning an optimal ordering of variables [8]. [sent-66, score-0.255]
</p><p>29 Once the optimal variable ordering is given, the constraint of no directed cycles can be trivially enforced by constraining the parents of each variable in the local conditional probability distribution to be a subset of the nodes that precede the V V given node in the ordering. [sent-67, score-0.362]
</p><p>30 , π|V | ] denote an V ordering of the nodes in V , where πj indicates the node v ∈ V in the jth position of the ordering, and ΠV vj denote the set of nodes in V that precede node vj in ordering ΠV . [sent-71, score-0.709]
</p><p>31 {}  {υ1}  {υ2}  {υ3}  {υ1,υ2}  {υ1,υ3}  {υ2,υ3}  {υ1,υ2,υ3}  Figure 1: Search space of Algorithms based on DP have been developed to learn the optimal variable ordering for three variable ordering for Bayesian networks [16]. [sent-72, score-0.272]
</p><p>32 based on the observation that the score of the optimal ordering of the full set of nodes V can be decomposed into (a) the optimal score for the ﬁrst node in the ordering, given a choice of the ﬁrst node and (b) the score of the optimal ordering of the nodes excluding the ﬁrst node. [sent-74, score-0.49]
</p><p>33 The optimal variable ordering can be constructed by recursively applying this decomposition to select the ﬁrst node in the ordering and to ﬁnd the optimal ordering of the set of remaining nodes U ⊂ V . [sent-75, score-0.393]
</p><p>34 In order to obtain BestScore(vj |V \U ) in Equations (2) and (3), for the case of discrete variables, many previous approaches enumerated all possible subsets of V as candidate sets of parents for node vj to precompute BestScore(vj |V \U ) in Stage 1 before applying DP in Stage 2 [7, 15]. [sent-77, score-0.323]
</p><p>35 In this paper, we consider the high-dimensional setting and present a single-stage method that applies lasso to obtain BestScore(vj |V \U ) within DP as follows: BestScore(vj |V \U )  = LassoScore(vj |V \U ) =  min  βj ,S(βj )⊆V \U  xj − x−j βj  2 2  +λ  βj  1  . [sent-79, score-0.25]
</p><p>36 The constraint S(βj ) ⊆ V \U in the above lasso optimization can be trivially maintained by setting the βjk for vk ∈ U to 0 and optimizing only for the other βjk ’s. [sent-80, score-0.292]
</p><p>37 When applying the recursion in Equations (2) and (3), DP takes advantage of the overlapping subproblems to prune the search space of orderings, since the problem of computing OptScore(U ) for U ⊆ V can appear as a subproblem of scoring orderings of any larger subsets of V that contain U . [sent-81, score-0.151]
</p><p>38 The problem of ﬁnding the optimal variable ordering can be viewed as that of ﬁnding the shortest path from the start state to the goal state in a search space given as a subset lattice. [sent-82, score-0.335]
</p><p>39 The search space consists of a set of states, each of which is associated with one of the 2|V | possible subsets of nodes in V . [sent-83, score-0.132]
</p><p>40 A valid move in this search space is deﬁned from a state for subset Qs to another state for subset Qs , only if Qs contains one additional node to Qs . [sent-85, score-0.238]
</p><p>41 Each move to the next state corresponds to adding a node at the end of the ordering of the nodes in the previous state. [sent-86, score-0.227]
</p><p>42 Each path from the start state to the goal state gives one 3  possible ordering of nodes. [sent-88, score-0.19]
</p><p>43 Figure 1 illustrates the search space, where each state is associated with a Qs . [sent-89, score-0.121]
</p><p>44 DP ﬁnds the shortest path from the start state to the goal state that corresponds to the optimal variable ordering by considering all possible paths in this search space and visiting all 2|V | states. [sent-90, score-0.335]
</p><p>45 On the other hand, a greedy algorithm is computationally efﬁcient because it explores a single variable ordering by greedily selecting the most promising next state based on BestScore(v|Qs ), but it returns a suboptimal solution. [sent-94, score-0.143]
</p><p>46 In this paper, we propose A* lasso that incorporates the A* search algorithm [12] to construct the optimal variable ordering in the search space of the subset lattice. [sent-95, score-0.504]
</p><p>47 Although the exact future cost is not known until A* search constructs the full path by reaching the goal state, a reasonable estimate of the future cost can be obtained by ignoring the directed acyclicity constraint. [sent-98, score-0.178]
</p><p>48 It is well-known that A* search is guaranteed to ﬁnd the shortest path if the heuristic function h(Qs ) is admissible [12], meaning that h(Qs ) is always an underestimate of the true cost of reaching the goal state. [sent-99, score-0.237]
</p><p>49 While exploring the search space, A* search algorithm assigns a score f (Qs ) to each state and its corresponding subset Qs of variables for which the ordering has been determined. [sent-101, score-0.339]
</p><p>50 A* search algorithm computes this score f (Qs ) as the sum of the cost g(Qs ) that has been incurred so far to reach the current state from the start state and an estimate of the cost h(Qs ) that will be incurred to reach the goal state from the current state: f (Qs ) = g(Qs ) + h(Qs ). [sent-102, score-0.335]
</p><p>51 When the search space is a graph where multiple paths can reach the same state, we can further improve efﬁciency if the heuristic function has the property of consistency in addition to admissibility. [sent-104, score-0.203]
</p><p>52 A consistent heuristic always satisﬁes h(Qs ) ≤ h(Qs ) + LassoScore(vk |Qs ), where LassoScore(vk |Qs ) is the cost of moving from state Qs to state Qs with {vk } = Qs \Qs . [sent-105, score-0.165]
</p><p>53 Consistency ensures that the ﬁrst path found by A* search to reach the given state is always the shortest path to that state [12]. [sent-106, score-0.247]
</p><p>54 This allows us to prune the search when we reach the same state via a different path later in the search. [sent-107, score-0.181]
</p><p>55 pop(); if h(Qs ) = 0 then Return ΠV ← ΠQs ; end foreach v ∈ V \Qs do Qs ← Qs ∪ {v}; if Qs ∈ CLOSED then / Compute LassoScore(v|Qs ) with lasso shooting algorithm; g(Qs ) ← g(Qs ) + LassoScore(v|Qs ); h(Qs ) ← h(Qs ) − LassoScore(v|V \v); f (Qs ) ← g(Qs ) + h(Qs ); ΠQs ← [ΠQs , v]; OPEN. [sent-113, score-0.233]
</p><p>56 insert(L = (Qs , f (Qs ), g(Qs ), ΠQs )); CLOSED ← CLOSED ∪{Qs }; end end end  Algorithm 1: A* lasso for learning Bayesian network structure  where LassoScore(vk |Qs ) is the true cost of moving from state Qs to Qs . [sent-114, score-0.34]
</p><p>57 The inequality above holds because vk has fewer parents to choose from in LassoScore(vk |Qs ) than in LassoScore(vk |V \vk ). [sent-115, score-0.133]
</p><p>58 In practice, the OPEN list can be implemented with a priority queue and the CLOSED list can be implemented with a hash table. [sent-118, score-0.225]
</p><p>59 The OPEN list is a priority queue that maintains all the intermediate results (Qs , f (Qs ), g(Qs ), ΠQs )’s for a partial construction of the variable ordering up to Qs at the frontier of the search, sorted according to the score f (Qs ). [sent-119, score-0.351]
</p><p>60 During search, A* lasso pops from the OPEN list the partial construction of ordering with the lowest score f (Qs ), visits the successor states by adding another node to the ordering ΠQs , and queues the results onto the OPEN list. [sent-120, score-0.536]
</p><p>61 Any state that has been popped by A* lasso is placed in the CLOSED list. [sent-121, score-0.247]
</p><p>62 The states that have been placed in the CLOSED list are not considered again, even if A* search reaches these states through different paths later in the search. [sent-122, score-0.16]
</p><p>63 The full algorithm for A* lasso is given in Algorithm 1. [sent-123, score-0.208]
</p><p>64 As in DP with lasso, A* lasso is a singlestage algorithm that solves lasso within A* search. [sent-124, score-0.435]
</p><p>65 Every time A* lasso moves from state Qs to s the next state Qs in the search space, LassoScore(vj |ΠQvj ) for {vj } = Qs \Qs is computed with the shooting algorithm and added to g(Qs ) to obtain g(Qs ). [sent-125, score-0.393]
</p><p>66 The heuristic score h(Qs ) can be precomputed as LassoScore(vj |V \vj ) for all vj ∈ V for a simple look-up during A* search. [sent-126, score-0.281]
</p><p>67 3  Heuristic Schemes for A* Lasso to Improve Scalability  Although A* lasso substantially prunes the search space compared to DP, it is not sufﬁciently efﬁcient for large graphs, because it still considers a large number of states in the exponentially large search space. [sent-128, score-0.454]
</p><p>68 One simple strategy for further pruning the search space would be to limit the size of the priority queue in the OPEN list, forcing A* lasso to discard less promising intermediate results ﬁrst. [sent-129, score-0.509]
</p><p>69 In this case, limiting the queue size to one is equivalent to a greedy algorithm with a scoring function in Equation (4). [sent-130, score-0.189]
</p><p>70 In our experiments, we found that such a naive strategy substantially reduced the quality of solutions because the best-scoring intermediate results tend to be the results at the early stage of the exploration. [sent-131, score-0.131]
</p><p>71 They are at the shallow part of the search space near the start state because the admissible heuristic underestimates the true cost. [sent-132, score-0.233]
</p><p>72 Instead, given a limited queue size, we propose to distribute the intermediate results to be discarded across different depths/layers of the search space. [sent-133, score-0.245]
</p><p>73 For example, given the depth of the search space 5  Table 1: Comparison of computation time of different methods Dataset (Nodes) DP A* lasso A* Qlimit 1000 A* Qlimit 200 A* Qlimit 100 A* Qlimit 5 L1MB SBN Dsep (6) 0. [sent-134, score-0.321]
</p><p>74 41  Table 2: A* lasso computation time under different edge strengths βj ’s Dataset (Nodes) Dsep (6) Asia (8) Bowling (9) Inversetree (11) Rain (14) Cloud (16 ) Funnel (18) Galaxy (20)  (1. [sent-235, score-0.246]
</p><p>75 In our experiments, we found that this heuristic scheme substantially improves the computation time of A* lasso with a small reduction in the quality of the solution. [sent-261, score-0.333]
</p><p>76 We created several small networks under 20 nodes and obtained the structure of several benchmark networks between 27 and 56 nodes from the Bayesian Network Repository (the left-most column in Table 1). [sent-265, score-0.185]
</p><p>77 Given the Bayesian network structures, we set the parameters βj for each conditional probability distribution of node vj such that βjk ∼ ±U nif orm[l, u] for predetermined values for u and l if node vk is a parent of node vj and βjk = 0 otherwise. [sent-267, score-0.691]
</p><p>78 We compare our method to several other methods including DP with lasso for an exact method, L1MB for heuristic search, and SBN for an optimization-based approximate method. [sent-270, score-0.281]
</p><p>79 For L1MB, we increased the authors’ recommended number of evaluations 2500 to 10 000 in Stage 2 heuristic search for all networks except the two larger networks of around 300 nodes (Alarm 2 and Hailﬁnder 2), where we used two different settings of 50 000 and 100 000 evaluations. [sent-272, score-0.263]
</p><p>80 We also evaluated A* lasso with the heuristic scheme with the queue sizes of 5, 100, 200, and 1000. [sent-273, score-0.426]
</p><p>81 DP, A* lasso, and A* lasso with a limited queue size require a selection of the regularization parameter λ with cross-validation. [sent-274, score-0.353]
</p><p>82 For DP, A* lasso, and A* lasso with limited queue sizes, we also record the number of states visited in the search space in parentheses in Table 1. [sent-319, score-0.49]
</p><p>83 It is clear that A* lasso visits signiﬁcantly fewer states than DP, visiting about 10% of the number of states in DP for the funnel and galaxy networks. [sent-326, score-0.307]
</p><p>84 We were unable to obtain the computation time for A* lasso and DP for some of the larger graphs in Table 1 as they required signiﬁcantly more time. [sent-327, score-0.225]
</p><p>85 Limiting the size of the queue in A* lasso reduces both the computation time and the number of states visited. [sent-328, score-0.394]
</p><p>86 For smaller graphs, we do not report the computation time for A* lasso with limited queue size, since it is identical to the full A* lasso. [sent-329, score-0.37]
</p><p>87 We notice that the computation time for A* lasso with a small queue of 5 or 100 is comparable to that of L1MB and SBN. [sent-330, score-0.37]
</p><p>88 In general, we found that the extent of pruning of the search space by A* lasso compared to DP depends on the strengths of edges (βj values) in the true model. [sent-331, score-0.347]
</p><p>89 We applied DP and A* lasso to datasets of 200 samples generated from each of the networks under each of the three settings for the true edge strengths, ±U nif orm[1. [sent-332, score-0.29]
</p><p>90 As can be seen from the computation time and the number of states visited by DP and A* lasso in Table 2, as the strengths of edges increase, the number of states visited by A* lasso and the computation time tend to decrease. [sent-337, score-0.553]
</p><p>91 The results in Table 2 indicate that the efﬁciency of A* lasso is affected by the signal-to-noise ratio. [sent-338, score-0.208]
</p><p>92 We applied A* lasso with different queue limits of 5, 100, and 200. [sent-344, score-0.353]
</p><p>93 Our method obtains lower prediction errors than the other methods, even with the smaller queue sizes. [sent-346, score-0.176]
</p><p>94 The A*−Qlim=1000 15 skeleton of a Bayesian network is deﬁned as the edge connectivities ignoring edge directions and a v-structure is deﬁned as the 10 local graph structure over three variables, with two variables point5 1 2 3 4 5 6 7 8 9 ing to the other variables (i. [sent-354, score-0.149]
</p><p>95 We evaluated the Network performance of the different methods by comparing the estimated network structure with the true network structure in terms of skele- Figure 4: Prediction errors for benchmark Bayesian netton and v-structures and computing the precision and recall. [sent-357, score-0.246]
</p><p>96 For the benchmark Bayesian networks, we used A* lasso with different queue sizes, including 100, 200, and 1000, whereas for the two large networks (Alarm 2 and Hailﬁnder 2) that require more computation time, we used A* lasso with queue size of 5 and 100. [sent-365, score-0.778]
</p><p>97 We ﬁnd that although increasing the size of queues in A* lasso generally improves the performance, even with smaller queue sizes, A* lasso outperforms L1MB and SBN in most of the networks. [sent-367, score-0.576]
</p><p>98 This demonstrates that while limiting the queue size in A* lasso will not guarantee the optimality of the solution, it still reduces the computation time of A* lasso dramatically without substantially compromising the quality of the solution. [sent-369, score-0.66]
</p><p>99 We ﬁnd that the prediction errors of A* lasso are consistently lower even with a limited queue size. [sent-371, score-0.384]
</p><p>100 Conclusions  In this paper, we considered the problem of learning a Bayesian network structure and proposed A* lasso that guarantees the optimality of the solution while reducing the computational time of the well-known exact methods based on DP. [sent-373, score-0.287]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qs', 0.648), ('qlim', 0.513), ('sbn', 0.209), ('lasso', 0.208), ('lassoscore', 0.18), ('vj', 0.174), ('queue', 0.145), ('dp', 0.095), ('ordering', 0.088), ('bestscore', 0.085), ('search', 0.082), ('stage', 0.078), ('heuristic', 0.073), ('alarm', 0.073), ('bayesian', 0.071), ('parents', 0.068), ('hail', 0.066), ('nder', 0.066), ('vk', 0.065), ('network', 0.057), ('precision', 0.054), ('dag', 0.052), ('node', 0.049), ('nif', 0.046), ('orm', 0.046), ('xj', 0.042), ('state', 0.039), ('barley', 0.038), ('hailfinder', 0.038), ('mildew', 0.038), ('optscore', 0.038), ('qlimit', 0.038), ('nodes', 0.036), ('jk', 0.036), ('networks', 0.036), ('score', 0.034), ('recall', 0.034), ('insurance', 0.034), ('candidate', 0.032), ('list', 0.03), ('compromising', 0.028), ('funnel', 0.028), ('skeleton', 0.028), ('pa', 0.028), ('parent', 0.028), ('directed', 0.027), ('prunes', 0.025), ('shooting', 0.025), ('scoring', 0.025), ('admissible', 0.025), ('states', 0.024), ('path', 0.024), ('galaxy', 0.023), ('water', 0.023), ('closed', 0.022), ('pruning', 0.022), ('structure', 0.022), ('stock', 0.021), ('strengths', 0.021), ('reach', 0.02), ('priority', 0.02), ('substantially', 0.019), ('dsep', 0.019), ('inversetree', 0.019), ('qvj', 0.019), ('singlestage', 0.019), ('tsamardinos', 0.019), ('shortest', 0.019), ('benchmark', 0.019), ('constraint', 0.019), ('limiting', 0.019), ('intermediate', 0.018), ('computation', 0.017), ('open', 0.017), ('incurred', 0.017), ('visited', 0.017), ('acyclicity', 0.017), ('constantin', 0.017), ('mdl', 0.017), ('quality', 0.016), ('variable', 0.016), ('prune', 0.016), ('prediction', 0.016), ('bowling', 0.015), ('companies', 0.015), ('ioannis', 0.015), ('jing', 0.015), ('precede', 0.015), ('queues', 0.015), ('skeletons', 0.015), ('move', 0.015), ('errors', 0.015), ('rain', 0.015), ('cycles', 0.014), ('space', 0.014), ('recursion', 0.014), ('graph', 0.014), ('cost', 0.014), ('variables', 0.014), ('optimal', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="3-tfidf-1" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>2 0.10471921 <a title="3-tfidf-2" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>3 0.080871306 <a title="3-tfidf-3" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>4 0.077356711 <a title="3-tfidf-4" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>5 0.075298473 <a title="3-tfidf-5" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>6 0.063203506 <a title="3-tfidf-6" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>7 0.055512898 <a title="3-tfidf-7" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>8 0.049907766 <a title="3-tfidf-8" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>9 0.049883213 <a title="3-tfidf-9" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>10 0.049536757 <a title="3-tfidf-10" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>11 0.046938352 <a title="3-tfidf-11" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>12 0.045178253 <a title="3-tfidf-12" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>13 0.041787662 <a title="3-tfidf-13" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>14 0.041361105 <a title="3-tfidf-14" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>15 0.039438989 <a title="3-tfidf-15" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>16 0.038686313 <a title="3-tfidf-16" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>17 0.038211353 <a title="3-tfidf-17" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>18 0.038204696 <a title="3-tfidf-18" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>19 0.038044952 <a title="3-tfidf-19" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>20 0.036343917 <a title="3-tfidf-20" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.091), (1, 0.021), (2, -0.01), (3, 0.015), (4, 0.011), (5, 0.054), (6, 0.023), (7, -0.022), (8, -0.043), (9, -0.009), (10, 0.087), (11, -0.065), (12, 0.032), (13, -0.097), (14, -0.092), (15, -0.032), (16, 0.047), (17, -0.047), (18, -0.014), (19, -0.032), (20, 0.108), (21, 0.019), (22, -0.06), (23, 0.043), (24, 0.05), (25, 0.045), (26, -0.004), (27, 0.042), (28, 0.029), (29, 0.004), (30, 0.02), (31, 0.021), (32, 0.022), (33, -0.021), (34, -0.051), (35, 0.041), (36, -0.013), (37, 0.05), (38, -0.05), (39, -0.058), (40, -0.064), (41, -0.006), (42, 0.052), (43, 0.011), (44, -0.047), (45, -0.026), (46, 0.018), (47, -0.018), (48, -0.006), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93090183 <a title="3-lsi-1" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>2 0.67102343 <a title="3-lsi-2" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>3 0.61374307 <a title="3-lsi-3" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>4 0.60689825 <a title="3-lsi-4" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>5 0.60335463 <a title="3-lsi-5" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>6 0.58706808 <a title="3-lsi-6" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>7 0.52433896 <a title="3-lsi-7" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>8 0.48091474 <a title="3-lsi-8" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>9 0.46318489 <a title="3-lsi-9" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>10 0.44416267 <a title="3-lsi-10" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>11 0.43911442 <a title="3-lsi-11" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>12 0.42978334 <a title="3-lsi-12" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>13 0.41947871 <a title="3-lsi-13" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>14 0.40271506 <a title="3-lsi-14" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>15 0.39587933 <a title="3-lsi-15" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>16 0.38009796 <a title="3-lsi-16" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>17 0.37404057 <a title="3-lsi-17" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>18 0.3731617 <a title="3-lsi-18" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>19 0.36940113 <a title="3-lsi-19" href="./nips-2013-Bayesian_Hierarchical_Community_Discovery.html">47 nips-2013-Bayesian Hierarchical Community Discovery</a></p>
<p>20 0.36192927 <a title="3-lsi-20" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.022), (33, 0.079), (34, 0.091), (36, 0.013), (41, 0.014), (56, 0.527), (70, 0.035), (85, 0.028), (89, 0.012), (93, 0.039), (99, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9909237 <a title="3-lda-1" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>Author: Samory Kpotufe, Vikas Garg</p><p>Abstract: We present the ﬁrst result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown H¨ lder-continuity of the regression function at x. The result holds with o high probability simultaneously at all points x in a general metric space X of unknown structure. 1</p><p>2 0.98533452 <a title="3-lda-2" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>3 0.97188908 <a title="3-lda-3" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>4 0.96669459 <a title="3-lda-4" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>Author: Tzu-Kuo Huang, Jeff Schneider</p><p>Abstract: Learning dynamic models from observed data has been a central issue in many scientiﬁc studies or engineering tasks. The usual setting is that data are collected sequentially from trajectories of some dynamical system operation. In quite a few modern scientiﬁc modeling tasks, however, it turns out that reliable sequential data are rather difﬁcult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer’s, or certain biological processes. Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze. Inspired by recent advances in spectral learning methods, we propose to study this problem from a different perspective: moment matching and spectral decomposition. Under that framework, we identify reasonable assumptions on the generative process of non-sequence data, and propose learning algorithms based on the tensor decomposition method [2] to provably recover ﬁrstorder Markov models and hidden Markov models. To the best of our knowledge, this is the ﬁrst formal guarantee on learning from non-sequence data. Preliminary simulation results conﬁrm our theoretical ﬁndings. 1</p><p>5 0.96097451 <a title="3-lda-5" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><p>6 0.95359123 <a title="3-lda-6" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>same-paper 7 0.9489612 <a title="3-lda-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.93181598 <a title="3-lda-8" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>9 0.92515552 <a title="3-lda-9" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>10 0.89214563 <a title="3-lda-10" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>11 0.88550007 <a title="3-lda-11" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>12 0.87215686 <a title="3-lda-12" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>13 0.86605853 <a title="3-lda-13" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>14 0.85320693 <a title="3-lda-14" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>15 0.84964579 <a title="3-lda-15" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>16 0.84947807 <a title="3-lda-16" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>17 0.84835279 <a title="3-lda-17" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>18 0.84729278 <a title="3-lda-18" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>19 0.83982772 <a title="3-lda-19" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>20 0.83575761 <a title="3-lda-20" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
