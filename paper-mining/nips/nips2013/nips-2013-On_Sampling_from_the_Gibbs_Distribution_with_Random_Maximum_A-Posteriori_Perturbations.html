<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-218" href="#">nips2013-218</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</h1>
<br/><p>Source: <a title="nips-2013-218-pdf" href="http://papers.nips.cc/paper/4962-on-sampling-from-the-gibbs-distribution-with-random-maximum-a-posteriori-perturbations.pdf">pdf</a></p><p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><p>Reference: <a title="nips-2013-218-reference" href="../nips2013_reference/nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. [sent-2, score-0.589]
</p><p>2 Our approach also leads to new ways to derive lower bounds on partition functions. [sent-3, score-0.42]
</p><p>3 The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. [sent-5, score-0.417]
</p><p>4 Each structure corresponds to an assignment of values to random variables and the likelihood of an assignment is based on deﬁning potential functions in a Gibbs distribution. [sent-9, score-0.248]
</p><p>5 Usually, it is feasible to ﬁnd only the most likely or maximum a-posteriori (MAP) assignment (structure) rather than sampling from the full Gibbs distribution. [sent-10, score-0.21]
</p><p>6 Speciﬁcally, we aim to draw either approximate or unbiased samples from Gibbs distributions by introducing low dimensional perturbations in the potential functions and solving the corresponding MAP assignments. [sent-14, score-0.661]
</p><p>7 Connections between random MAP perturbations and Gibbs distributions have been explored before. [sent-15, score-0.317]
</p><p>8 [5] augmented these results by providing bounds on the partition function in terms of random MAP perturbations. [sent-17, score-0.344]
</p><p>9 In this work we build on these results to construct an efﬁcient sampler for the Gibbs distribution, also deriving new lower bounds on the partition function. [sent-18, score-0.5]
</p><p>10 In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [3, 25]. [sent-20, score-0.376]
</p><p>11 ,xn  The normalization constant Z is called the partition function. [sent-40, score-0.26]
</p><p>12 The feasibility of using the distribution for prediction, including sampling from it, is inherently tied to the ability to evaluate the partition function, i. [sent-41, score-0.52]
</p><p>13 Our approach is based on representations of the Gibbs distribution and the partition function using extreme value statistics of linearly perturbed potential functions. [sent-54, score-0.402]
</p><p>14 ) random perturbations are applied for every assignment x ∈ X. [sent-59, score-0.452]
</p><p>15 Speciﬁcally, when the random perturbations follow the Gumbel distribution (cf. [sent-60, score-0.352]
</p><p>16 ˆ x∈X  The max-stability of the Gumbel distribution provides a straight forward approach to generate unbiased samples from the Gibbs distribution as well as to approximate the partition function by a sample mean of random MAP perturbation. [sent-69, score-0.578]
</p><p>17 , m independent predictions maxx {θ(x) + γj (x)}, then every maximal argument is an unbiased sample from the Gibbs distribution. [sent-73, score-0.368]
</p><p>18 Moreover, the randomized MAP predictions maxx {θ(x) + γj (x)} are independent and follow the Gumbel distribution, whose variance is π 2 /6. [sent-74, score-0.231]
</p><p>19 In our work we propose to investigate low dimensional random perturbations as the main tool to efﬁciently (approximate) sampling from the Gibbs distribution. [sent-81, score-0.497]
</p><p>20 3  Probable approximate samples from the Gibbs distribution  Sampling from the Gibbs distribution is inherently tied to estimating the partition function. [sent-82, score-0.526]
</p><p>21 Markov properties that simplify the distribution also decompose the computation of the partition function. [sent-83, score-0.295]
</p><p>22 2  For example, assume a graphical model with potential functions associated with subsets of variables α ⊂ {1, . [sent-84, score-0.199]
</p><p>23 This separation implies that the partition function can be computed in lower dimensional pieces Z=  exp(θα (xα )) xβ α∈A  xα \xβ  As a result, the computation is exponential only in the size of the subsets α ∈ A. [sent-89, score-0.499]
</p><p>24 Thus, we can also estimate the partition function with lower dimensional random MAP perturbations, Eγ [maxxα \xβ {θα (xα ) + γα (xα )}]. [sent-90, score-0.394]
</p><p>25 The random perturbation are now required only for each assignment of values to the variables within the subsets α ∈ A rather than the set of all variables. [sent-91, score-0.213]
</p><p>26 We approximate such partition functions with low dimensional perturbations and their averages. [sent-92, score-0.691]
</p><p>27 Since xβ is ﬁxed for every α ∈ A the maximizations are done independently across subsets in x \ xβ , where x is the concatenation of ˆ ˆ all xα , and ˆ mα  max  α∈A  xα \xβ ˆ  mα  θα (xα,jα ) + γα,jα (xα,jα ) = max x\xβ ˆ  jα =1  θα (xα,jα ) + jα =1  α∈A  γα,jα (xα,jα ) . [sent-107, score-0.193]
</p><p>28 For notational simplicity we describe our approximate sampling scheme for pairwise interactions α = (i, j) although it holds for general graphical models without cycles: Theorem 2. [sent-111, score-0.297]
</p><p>29 Let θ(x) = i∈V θi (xi ) + i,j∈E θi,j (xi , xj ) be a graphical model withˆ out cycles, and let p(x) be the Gibbs distribution deﬁned in Equation (1). [sent-112, score-0.204]
</p><p>30 , xn,kn )/ i mi , and γi,j (xi , xj ) = ˆ γi,j,ki ,kj (xi,ki , xj,kj )/mi mj ki =1 ki ,kj =1 where each perturbation is independent and distributed according to the Gumbel distribution with zero mean. [sent-116, score-0.403]
</p><p>31 Using graph separation (or equivalently the Markov property) it sufﬁces to approximate the partial partition function over the disjoint subtrees Tr , Ts that originate from r, s respectively. [sent-120, score-0.412]
</p><p>32 Our approximated sampling procedure expands the graphical model, creating layers of the original graph, while connecting edges between vertices in the different layers if an edge exists in the original graph. [sent-123, score-0.247]
</p><p>33 This construction preserves the structure of the original graph, in particular, whenever the original graph has no cycles, the expanded graph does not have cycles as well. [sent-125, score-0.23]
</p><p>34 4  Unbiased sampling using sequential bounds on the partition function  In the following we describe how to use random MAP perturbations to generate unbiased samples from the Gibbs distribution. [sent-127, score-0.975]
</p><p>35 Sampling from the Gibbs distribution is inherently tied to estimating the partition function. [sent-128, score-0.398]
</p><p>36 Assume we could have compute the partition function exactly, then we could have sample from the Gibbs distribution sequentially: for every dimension we sample xi with probability which is proportional to xi+1 ,. [sent-129, score-0.427]
</p><p>37 Unfortunately, approximations to the partition function, as described in Section 3, cannot provide a sequential procedure that would generate unbiased samples from the full Gibbs distribution. [sent-133, score-0.452]
</p><p>38 Instead, we construct a family of self-reducible upper bounds which imitate the partition function behavior, namely bound the summation over its exponentiations. [sent-134, score-0.458]
</p><p>39 ,xn  i=j+1  In particular, for j = n holds  xn  γi (xi )} i=j  exp(θ(x)) = exp Eγn (xn ) maxxj ,. [sent-153, score-0.352]
</p><p>40 Proof: The result is an application of the expectation-optimization interpretation of the partition function in Theorem 1. [sent-157, score-0.26]
</p><p>41 , xn ) according to the Gibbs distribution P Algorithm 1 outputs x Algorithm 1 accepts = p(x). [sent-211, score-0.207]
</p><p>42 , xn ) without rejecting is n  j=1  exp Eγ  max {θ(x) +  xj+1 ,. [sent-215, score-0.244]
</p><p>43 Interestingly, this average is the quality of the partition upper bound presented in [5]. [sent-235, score-0.342]
</p><p>44 To augment this result we investigate in the next section efﬁciently computable lower bounds to the partition function, that are based on random MAP perturbations. [sent-236, score-0.42]
</p><p>45 These lower bounds provide a way to efﬁciently determine the computational complexity for sampling from the Gibbs distribution for a given potential function. [sent-237, score-0.389]
</p><p>46 5  Lower bounds on the partition function  The realization of the partition function as expectation-optimization pair in Theorem 1 provides efﬁciently computable lower bounds on the partition function. [sent-238, score-1.024]
</p><p>47 In the following we present two lower bounds that are derived along these lines, the ﬁrst holds in expectation and the second holds in probability. [sent-240, score-0.268]
</p><p>48 Then ∀α ∈ A  log Z ≥ Eγ max θ(x) + γα (xα )  In particular, log Z ≥ Eγ maxx θ(x) +  x  1 |A|  α∈A  γα (xα )  . [sent-247, score-0.195]
</p><p>49 The 1 second result is attained while averaging these lower bounds log Z ≥ α∈A |A| Eγ [maxx {θ(x) + γα (xα )}], and by moving the summation inside the maximization operation. [sent-254, score-0.264]
</p><p>50 We suggest to recursively use Lemma 1 to lower bound the partition function with a single MAP operation in probability. [sent-257, score-0.385]
</p><p>51 We create multiple copies of xi , namely xi,ki for ki = 1, . [sent-263, score-0.208]
</p><p>52 , mi , and deﬁne the extended potential function mi ˆ θ(x) = mi . [sent-266, score-0.279]
</p><p>53 , xn,kn )/ mi γi,ki (xi,ki )/mi where each perturbation is independent and distributed according to the ki =1 n Gumbel distribution with zero mean. [sent-270, score-0.233]
</p><p>54 Middle: estimating our unbiased sampling procedure complexity on spin glass models of varying sizes. [sent-272, score-0.556]
</p><p>55 Right: Comparing our approximate sampling procedure on attractive models with high signal. [sent-273, score-0.248]
</p><p>56 Our result holds for every potential function, thus the statistics in each recursion hold uniformly for every x with probability at least 1 − π 2 |dom(θ)|/6mi 2 . [sent-275, score-0.22]
</p><p>57 We then move the averages inside the maximization operation, thus lower bounding the n−approximation of the partition function. [sent-276, score-0.408]
</p><p>58 The probable lower bound that we provide does not assume graph separations thus the statistical guarantees are worse than the ones presented in the approximation scheme of Theorem 2. [sent-277, score-0.335]
</p><p>59 Also, since we are seeking for lower bound, we are able relax our optimization requirements and thus to use vertex based random perturbations γi (xi ). [sent-278, score-0.393]
</p><p>60 6  Experiments  We evaluated our approach on spin glass models θ(x) = i∈V θi xi + (i,j)∈E θi,j xi xj . [sent-280, score-0.587]
</p><p>61 Each spin has a local ﬁeld parameter θi , sampled uniformly from [−1, 1]. [sent-282, score-0.21]
</p><p>62 We begin by evaluating our lower bounds, presented in Section 5, on 10 × 10 spin glass models. [sent-286, score-0.389]
</p><p>63 We evaluated these lower bounds while perturbing the local potentials with γi (xi ). [sent-288, score-0.263]
</p><p>64 We evaluate the probable bound by expanding the model to 1000 × 1000 grids, ignoring the discrepancy . [sent-290, score-0.197]
</p><p>65 For both the expected lower bound and the probable lower bound we used graph-cuts to compute the random MAP perturbations. [sent-291, score-0.354]
</p><p>66 We compared these bounds to the different forms of structured mean-ﬁeld, taking the one that performed best: standard structured mean-ﬁeld that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14]. [sent-292, score-0.206]
</p><p>67 We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [20, 18]. [sent-293, score-0.386]
</p><p>68 We computed the error in estimating the logarithm of the partition function, averaged over 10 spin glass models, see Figure 1. [sent-294, score-0.573]
</p><p>69 One can see that the probable bound is the tightest when considering the medium and high coupling domain, which is traditionally hard for all methods. [sent-295, score-0.228]
</p><p>70 The expected lower bound is signiﬁcantly worse for the low coupling regime, in which many conﬁgurations need to be taken into account. [sent-298, score-0.2]
</p><p>71 Focusing on spin glass models with strong local ﬁeld potentials, it is well know that one cannot produce unbiased samples from the Gibbs distributions in polynomial time [3]. [sent-301, score-0.509]
</p><p>72 Theorem 3 connects 6  Image + annotation  MAP solution  Average of 20 samples  Error estimates  Figure 2: Example image with the boundary annotation (left) and the error estimates obtained using our method (right). [sent-302, score-0.222]
</p><p>73 the computational complexity of our unbiased sampling procedure to the gap between the logarithm of the partition function and its upper bound in [5]. [sent-304, score-0.585]
</p><p>74 We use our probable lower bound to estimate this gap on large grids, for which we cannot compute the partition function exactly. [sent-305, score-0.489]
</p><p>75 Sampling from the Gibbs distribution in spin glass models with non-zero local ﬁeld potentials is computationally hard [7, 3]. [sent-307, score-0.451]
</p><p>76 The approximate sampling technique in Theorem 3 suggests a method to overcome this difﬁculty by efﬁciently sampling from a distribution that approximates the Gibbs distribution on its marginal probabilities. [sent-308, score-0.37]
</p><p>77 Both Gibbs sampling and the Metropolis algorithm perform similarly (we omit the Gibbs sampler performance for clarity). [sent-314, score-0.202]
</p><p>78 Although these algorithms as well as the Swendsen-Wang algorithm directly sample from the Gibbs distribution, they typically require exponential running time to succeed on spin glass models. [sent-315, score-0.313]
</p><p>79 Although we omit from the plots for clarity, our approximate sampling marginal probabilities compare to those of the sum-product belief propagation and the tree re-weighted belief propagation [22]. [sent-317, score-0.565]
</p><p>80 Nevertheless, our sampling scheme also provides a probability notion, which lacks in the belief propagation type algorithms. [sent-318, score-0.278]
</p><p>81 Surprisingly, the approximate sampler that uses pairwise perturbations performs (slightly) worse than the approximate sampler that only use local perturbations. [sent-319, score-0.627]
</p><p>82 Although this is not explained by our current theory, it is an encouraging observation, since approximate sampler that uses random MAP predictions with local perturbations is orders of magnitude faster. [sent-320, score-0.538]
</p><p>83 Lastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [22] or max-marginal probabilities [10], that only generate probabilities over small subsets of variables. [sent-321, score-0.396]
</p><p>84 On a dataset of 10 images which we carefully annotated to obtain pixel accurate boundaries we ﬁnd that random MAP perturbations produce signiﬁcantly more accurate estimates of boundary error compared to a single MAP solution. [sent-328, score-0.442]
</p><p>85 On average the error estimates obtained using random MAP perturbations is off by 1. [sent-329, score-0.317]
</p><p>86 The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [12, 17, 21, 5]. [sent-339, score-0.647]
</p><p>87 [17] show that random MAP predictors with low dimensional perturbations share similar statistics as the Gibbs distribution. [sent-342, score-0.411]
</p><p>88 In our work we formally relate random MAP perturbations and the Gibbs distribution. [sent-345, score-0.317]
</p><p>89 We also show how to use the statistics of random MAP perturbations to generate unbiased samples from the Gibbs distribution. [sent-347, score-0.509]
</p><p>90 Our suggested samplers for the Gibbs distribution are based on low dimensional representation of the partition function, [5]. [sent-350, score-0.417]
</p><p>91 Lower bounds for the partition function have been extensively developed in the recent years within the context of variational methods. [sent-354, score-0.38]
</p><p>92 Surprisingly, [20, 18] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions. [sent-357, score-0.613]
</p><p>93 Following [17, 21], we showed here that one can take advantage of efﬁcient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function. [sent-362, score-0.36]
</p><p>94 Since MAP predictions are not affected by ragged energy landscapes, our approach excels in the “high-signal high-coupling” regime. [sent-363, score-0.262]
</p><p>95 As a by-product to our approach we constructed lower bounds to the partition functions, which are both tighter and faster than the previous approaches in the ”high-signal high-coupling” regime. [sent-364, score-0.42]
</p><p>96 Our approach is based on random MAP perturbations that estimate the partition functions with expectation. [sent-365, score-0.577]
</p><p>97 The computational complexity of our approximate sampling procedure is determined by the perturbations dimension. [sent-368, score-0.495]
</p><p>98 On the partition function and random maximum a-posteriori perturbations. [sent-396, score-0.26]
</p><p>99 Loop series and Bethe variational bounds in attractive graphical models. [sent-488, score-0.255]
</p><p>100 A new class of upper bounds on the log partition function. [sent-505, score-0.377]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gibbs', 0.432), ('perturbations', 0.317), ('gumbel', 0.278), ('partition', 0.26), ('map', 0.242), ('spin', 0.172), ('maxx', 0.153), ('glass', 0.141), ('maxxj', 0.131), ('sampling', 0.122), ('unbiased', 0.121), ('ragged', 0.105), ('xj', 0.104), ('probable', 0.104), ('assignment', 0.088), ('accepts', 0.088), ('xi', 0.085), ('bounds', 0.084), ('xn', 0.084), ('exp', 0.083), ('cycles', 0.08), ('sampler', 0.08), ('propagation', 0.079), ('belief', 0.077), ('lower', 0.076), ('coupling', 0.075), ('boundary', 0.073), ('potential', 0.072), ('attractive', 0.07), ('mi', 0.069), ('xr', 0.068), ('ki', 0.066), ('graphical', 0.065), ('potentials', 0.065), ('landscapes', 0.064), ('samplers', 0.064), ('tied', 0.064), ('perturbation', 0.063), ('subsets', 0.062), ('excels', 0.06), ('dimensional', 0.058), ('copies', 0.057), ('approximate', 0.056), ('annotation', 0.056), ('holds', 0.054), ('jaakkola', 0.054), ('graph', 0.053), ('separations', 0.053), ('boundaries', 0.052), ('energy', 0.05), ('guration', 0.05), ('bound', 0.049), ('dom', 0.048), ('predictions', 0.047), ('every', 0.047), ('eld', 0.046), ('hazan', 0.046), ('discrepancy', 0.044), ('pj', 0.044), ('whenever', 0.044), ('papandreou', 0.043), ('herding', 0.043), ('maji', 0.043), ('meltzer', 0.043), ('separation', 0.043), ('tree', 0.042), ('maximization', 0.042), ('max', 0.042), ('perturb', 0.04), ('keshet', 0.04), ('structured', 0.04), ('inherently', 0.039), ('local', 0.038), ('lemma', 0.037), ('samples', 0.037), ('determinantal', 0.037), ('bethe', 0.037), ('tamir', 0.037), ('predictors', 0.036), ('relaxations', 0.036), ('variational', 0.036), ('rejecting', 0.035), ('extreme', 0.035), ('distribution', 0.035), ('metropolis', 0.034), ('generate', 0.034), ('corollary', 0.034), ('probabilities', 0.033), ('upper', 0.033), ('rough', 0.032), ('grids', 0.032), ('summation', 0.032), ('randomized', 0.031), ('inside', 0.03), ('con', 0.03), ('layers', 0.03), ('assignments', 0.03), ('theorem', 0.03), ('reject', 0.03), ('inference', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="218-tfidf-1" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><p>2 0.21445614 <a title="218-tfidf-2" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>Author: Matthew Johnson, James Saunderson, Alan Willsky</p><p>Abstract: Sampling inference methods are computationally difﬁcult to scale for many models in part because global dependencies can reduce opportunities for parallel computation. Without strict conditional independence structure among variables, standard Gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong. Empirical work has shown that some models can be sampled effectively by going “Hogwild” and simply running Gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood. As a step towards such an understanding, we study the Hogwild Gibbs sampling strategy in the context of Gaussian distributions. We develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra. In particular, we show that if the Gaussian precision matrix is generalized diagonally dominant, then any Hogwild Gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean. 1</p><p>3 0.21046454 <a title="218-tfidf-3" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>Author: Justin Domke, Xianghang Liu</p><p>Abstract: Inference in general Ising models is difﬁcult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We ﬁnd that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.</p><p>4 0.17727166 <a title="218-tfidf-4" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<p>Author: Jie Liu, David Page</p><p>Abstract: In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneﬁcial to group the parameters for more efﬁcient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with “stripped” Beta approximation (Gibbs SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs SBA’s performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs SBA also generalize better than the models learned by MLE on real-world Senate voting data. 1</p><p>5 0.14469631 <a title="218-tfidf-5" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>6 0.13391717 <a title="218-tfidf-6" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>7 0.13369296 <a title="218-tfidf-7" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>8 0.13279079 <a title="218-tfidf-8" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>9 0.11532406 <a title="218-tfidf-9" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>10 0.10482301 <a title="218-tfidf-10" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>11 0.087048151 <a title="218-tfidf-11" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>12 0.085671745 <a title="218-tfidf-12" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>13 0.084660865 <a title="218-tfidf-13" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>14 0.084649198 <a title="218-tfidf-14" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>15 0.082747661 <a title="218-tfidf-15" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>16 0.082463019 <a title="218-tfidf-16" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>17 0.080683425 <a title="218-tfidf-17" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>18 0.080373779 <a title="218-tfidf-18" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>19 0.079114884 <a title="218-tfidf-19" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>20 0.077249661 <a title="218-tfidf-20" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.048), (2, -0.026), (3, 0.02), (4, 0.046), (5, 0.184), (6, 0.139), (7, -0.044), (8, 0.08), (9, -0.029), (10, 0.066), (11, 0.004), (12, 0.035), (13, 0.035), (14, 0.073), (15, 0.013), (16, -0.14), (17, -0.085), (18, 0.002), (19, 0.144), (20, -0.099), (21, 0.022), (22, 0.064), (23, 0.031), (24, -0.006), (25, 0.122), (26, 0.007), (27, -0.018), (28, -0.004), (29, 0.199), (30, 0.085), (31, -0.089), (32, 0.067), (33, -0.027), (34, 0.086), (35, -0.115), (36, 0.031), (37, -0.07), (38, 0.134), (39, -0.066), (40, 0.05), (41, -0.041), (42, 0.002), (43, 0.071), (44, 0.074), (45, -0.012), (46, 0.082), (47, 0.019), (48, -0.028), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96643966 <a title="218-lsi-1" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><p>2 0.8590942 <a title="218-lsi-2" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>Author: Justin Domke, Xianghang Liu</p><p>Abstract: Inference in general Ising models is difﬁcult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We ﬁnd that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.</p><p>3 0.84446311 <a title="218-lsi-3" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>Author: Matthew Johnson, James Saunderson, Alan Willsky</p><p>Abstract: Sampling inference methods are computationally difﬁcult to scale for many models in part because global dependencies can reduce opportunities for parallel computation. Without strict conditional independence structure among variables, standard Gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong. Empirical work has shown that some models can be sampled effectively by going “Hogwild” and simply running Gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood. As a step towards such an understanding, we study the Hogwild Gibbs sampling strategy in the context of Gaussian distributions. We develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra. In particular, we show that if the Gaussian precision matrix is generalized diagonally dominant, then any Hogwild Gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean. 1</p><p>4 0.78762168 <a title="218-lsi-4" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<p>Author: Jie Liu, David Page</p><p>Abstract: In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneﬁcial to group the parameters for more efﬁcient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with “stripped” Beta approximation (Gibbs SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs SBA’s performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs SBA also generalize better than the models learned by MLE on real-world Senate voting data. 1</p><p>5 0.72768807 <a title="218-lsi-5" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>Author: Andreas Stuhlmüller, Jacob Taylor, Noah Goodman</p><p>Abstract: We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model’s joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly ﬁnd a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efﬁciency of this sampler for a variety of parameter regimes and Bayes nets. 1</p><p>6 0.7186752 <a title="218-lsi-6" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>7 0.69448978 <a title="218-lsi-7" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>8 0.60865134 <a title="218-lsi-8" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>9 0.56281167 <a title="218-lsi-9" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>10 0.54822868 <a title="218-lsi-10" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>11 0.54057819 <a title="218-lsi-11" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>12 0.53486598 <a title="218-lsi-12" href="./nips-2013-Annealing_between_distributions_by_averaging_moments.html">36 nips-2013-Annealing between distributions by averaging moments</a></p>
<p>13 0.53073215 <a title="218-lsi-13" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>14 0.52128571 <a title="218-lsi-14" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>15 0.51327908 <a title="218-lsi-15" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>16 0.50639659 <a title="218-lsi-16" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>17 0.50452191 <a title="218-lsi-17" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>18 0.4940044 <a title="218-lsi-18" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>19 0.46621332 <a title="218-lsi-19" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>20 0.46218434 <a title="218-lsi-20" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (16, 0.075), (33, 0.175), (34, 0.12), (41, 0.056), (45, 0.159), (49, 0.033), (56, 0.124), (70, 0.044), (85, 0.042), (89, 0.037), (93, 0.042), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92439425 <a title="218-lda-1" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>Author: Nan Du, Le Song, Manuel Gomez-Rodriguez, Hongyuan Zha</p><p>Abstract: If a piece of information is released from a media site, can we predict whether it may spread to one million web pages, in a month ? This inﬂuence estimation problem is very challenging since both the time-sensitive nature of the task and the requirement of scalability need to be addressed simultaneously. In this paper, we propose a randomized algorithm for inﬂuence estimation in continuous-time diffusion networks. Our algorithm can estimate the inﬂuence of every node in a network with |V| nodes and |E| edges to an accuracy of using n = O(1/ 2 ) randomizations and up to logarithmic factors O(n|E|+n|V|) computations. When used as a subroutine in a greedy inﬂuence maximization approach, our proposed algorithm is guaranteed to ﬁnd a set of C nodes with the inﬂuence of at least (1 − 1/e) OPT −2C , where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed algorithm can easily scale up to networks of millions of nodes while signiﬁcantly improves over previous state-of-the-arts in terms of the accuracy of the estimated inﬂuence and the quality of the selected nodes in maximizing the inﬂuence. 1</p><p>2 0.92100632 <a title="218-lda-2" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>Author: Haim Avron, Vikas Sindhwani, David Woodruff</p><p>Abstract: Motivated by the desire to extend fast randomized techniques to nonlinear lp regression, we consider a class of structured regression problems. These problems involve Vandermonde matrices which arise naturally in various statistical modeling settings, including classical polynomial ﬁtting problems, additive models and approximations to recently developed randomized techniques for scalable kernel methods. We show that this structure can be exploited to further accelerate the solution of the regression problem, achieving running times that are faster than “input sparsity”. We present empirical results conﬁrming both the practical value of our modeling framework, as well as speedup beneﬁts of randomized regression. 1</p><p>same-paper 3 0.88700294 <a title="218-lda-3" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><p>4 0.84350133 <a title="218-lda-4" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>Author: Raif Rustamov, Leonidas Guibas</p><p>Abstract: An increasing number of applications require processing of signals deﬁned on weighted graphs. While wavelets provide a ﬂexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less ﬂexible – they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is conﬁrmed via experiments both on synthetic and real data. 1</p><p>5 0.84107435 <a title="218-lda-5" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>6 0.83854139 <a title="218-lda-6" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>7 0.83750272 <a title="218-lda-7" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>8 0.83742958 <a title="218-lda-8" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>9 0.83701736 <a title="218-lda-9" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>10 0.83663183 <a title="218-lda-10" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>11 0.83579981 <a title="218-lda-11" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>12 0.83502656 <a title="218-lda-12" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>13 0.83473176 <a title="218-lda-13" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>14 0.83442503 <a title="218-lda-14" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>15 0.83433902 <a title="218-lda-15" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>16 0.83400589 <a title="218-lda-16" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>17 0.83400393 <a title="218-lda-17" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>18 0.83382219 <a title="218-lda-18" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>19 0.83259284 <a title="218-lda-19" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>20 0.83245611 <a title="218-lda-20" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
