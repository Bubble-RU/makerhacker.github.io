<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-267" href="#">nips2013-267</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</h1>
<br/><p>Source: <a title="nips-2013-267-pdf" href="http://papers.nips.cc/paper/5057-recurrent-networks-of-coupled-winner-take-all-oscillators-for-solving-constraint-satisfaction-problems.pdf">pdf</a></p><p>Author: Hesham Mostafa, Lorenz. K. Mueller, Giacomo Indiveri</p><p>Abstract: We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. External evidence, or input to the network, can force variables to speciﬁc values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. 1</p><p>Reference: <a title="nips-2013-267-reference" href="../nips2013_reference/nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems  ¨ Hesham Mostafa, Lorenz K. [sent-1, score-0.334]
</p><p>2 ch  Abstract We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. [sent-4, score-0.425]
</p><p>3 Constraints over the variables are encoded in the network connectivity. [sent-6, score-0.312]
</p><p>4 Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. [sent-7, score-0.704]
</p><p>5 If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. [sent-8, score-0.522]
</p><p>6 When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. [sent-10, score-0.781]
</p><p>7 Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. [sent-11, score-0.354]
</p><p>8 The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. [sent-12, score-0.699]
</p><p>9 1  Introduction  The brain is able to integrate noisy and partial information from both sensory inputs and internal states to construct a consistent interpretation of the actual state of the environment. [sent-13, score-0.327]
</p><p>10 One possible approach is to formulate a stochastic neural network that samples from a probability distribution in which the correct solutions have higher 1  probability [2]. [sent-20, score-0.3]
</p><p>11 However, the stochastic network will continuously explore the solution space and will not stabilize at fully consistent solutions. [sent-21, score-0.391]
</p><p>12 An alternative deterministic dynamical systems approach for solving combinatorial optimization problems is to formulate a quadratic cost function for the problem and construct a Hopﬁeld network whose Lyapunov function is this cost function [4]. [sent-24, score-0.319]
</p><p>13 The recurrent neural network we propose does not need a noise source to carry out the search process. [sent-27, score-0.411]
</p><p>14 The network is cortically inspired as it is composed of coupled Winner-Take-All (WTA) circuits. [sent-30, score-0.36]
</p><p>15 The WTA circuit is a possible cortical circuit motif [10] as its dynamics can explain the ampliﬁcation of genico-cortical inputs that was observed in intracellular recordings in cat visual cortex [11]. [sent-31, score-0.305]
</p><p>16 In addition to elucidating possible computational mechanisms in the brain, implementing “usable computation” with the dynamics of a neural network holds a number of advantages over conventional digital computation, including massive parallelism and fault tolerance. [sent-32, score-0.372]
</p><p>17 For example, the network proposed could be implemented using low-power analog current-mode WTA circuits [13], or by appropriately coupling silicon neurons in neuromorphic Very Large Scale Integration (VLSI) chips [14]. [sent-34, score-0.611]
</p><p>18 In the next section we describe the architecture of the proposed network and the models that we use for the network elements. [sent-35, score-0.559]
</p><p>19 Section 3 contains simulation results showing how the proposed network architecture solves a number of max-CSPs with binary variables. [sent-36, score-0.297]
</p><p>20 We discuss the network dynamics in Section 4 and present our conclusions in Section 5. [sent-37, score-0.372]
</p><p>21 2  Network Architecture  The basic building block of the proposed network is the WTA circuit in which multiple excitatory populations are competing through a common inhibitory population as shown in Fig. [sent-38, score-0.818]
</p><p>22 When the excitatory populations of the WTA network receive inputs of different amplitudes, their activity will increase and be ampliﬁed due to the recurrent excitatory connections. [sent-40, score-1.001]
</p><p>23 This will in turn activate the inhibitory population which will suppress activity in the excitatory populations until an equilibrium is reached. [sent-41, score-0.68]
</p><p>24 Typically, the excitatory population that receives the strongest external input is the only one that remains active (the network has selected a winner). [sent-42, score-0.831]
</p><p>25 By properly tuning the connection strengths, it is possible to conﬁgure the network so that it settles into a stable state of activity (or an attractor) that persists after input removal [15]. [sent-43, score-0.628]
</p><p>26 1 is a good approximation of the steady state average ﬁring rate in a population of integrate and ﬁre neurons receiving noisy, uncorrelated inputs [16]. [sent-48, score-0.323]
</p><p>27 For a step increase in mean input, the actual average ﬁring rate in a population settles into a steady state after a number of transient modes have died out [17] but in eq. [sent-49, score-0.321]
</p><p>28 2  D  network activity after stimulus removal  WA  network activity during stimulus presentation  WB  C  A  B  input stimulus excitatory population  ﬁxed connection plastic connection  inhibitory population  (a)  (b)  80  Weight  Rate(Hz)  0. [sent-51, score-1.667]
</p><p>29 (b) Three coupled WTA circuits form the network representation of a single binary variable. [sent-55, score-0.404]
</p><p>30 (c) Simulation results of the network in (b) showing activity in the four excitatory populations. [sent-58, score-0.603]
</p><p>31 Shaded rectangles indicate the time intervals in which the state of the oscillator can be changed by external input. [sent-59, score-0.285]
</p><p>32 The plastic connections in the proposed network obey a learning rule analogous to the BienenstockCooper-Munro (BCM) rule [18]: w(t) = Ku(t) ˙  (wmax − w(t))[v(t) − vth ]+ (w(t) − wmin )[vth − v(t)]− + τdep τpot  (2)  where [x]+ = max(0, x), and [x]− = min(0, x). [sent-65, score-0.417]
</p><p>33 1a are computationally useful as they enable the network to disambiguate the inputs to the excitatory populations by making a categorical choice based on the relative strengths of these inputs. [sent-71, score-0.632]
</p><p>34 Point attractor dominated dynamics promote noise robustness at the expense of reduced input sensitivity: external input has to be large to move the network state out of the basin of attraction of one point attractor, and into the basin of attraction of another. [sent-72, score-0.781]
</p><p>35 As a consequence, persistent activity can no longer appear in a single WTA stage if there is no input. [sent-77, score-0.313]
</p><p>36 As a consequence, a bump of activity continuously jumps from one WTA stage to the next. [sent-80, score-0.313]
</p><p>37 Since the stages are connected in a loop, the network will exhibit oscillatory activity. [sent-81, score-0.438]
</p><p>38 There are two stable limit cycles that the network trajectory can follow. [sent-82, score-0.465]
</p><p>39 The limit cycle chosen by the network depends on the outcome of the winner selection process in the bottom WTA stage. [sent-83, score-0.625]
</p><p>40 The limit cycles are stable as the weak coupling between the stages leaves the signal restoration properties of the destroyed attractors intact allowing activity in each WTA stage to be restored to a point close to that of the destroyed attractor. [sent-84, score-0.799]
</p><p>41 In the absence of external input, the dynamics of the winner selection process in the bottom stage will favor the population that receives the stronger projection weight from D. [sent-86, score-0.82]
</p><p>42 1b can represent one binary variable whose value is encoded in the identity of the winning population in the bottom WTA stage, which determines the limit cycle the network follows. [sent-90, score-0.771]
</p><p>43 More than two values can be encoded by increasing the number of excitatory populations in the bottom WTA stage. [sent-92, score-0.343]
</p><p>44 This is expressed by a limit cycle in which populations B,C, and D are periodically activated. [sent-96, score-0.303]
</p><p>45 During the winner selection process in the bottom WTA stage, the WTA circuit is very sensitive to external input, which can bias the competition towards a particular limit cycle. [sent-97, score-0.571]
</p><p>46 e, activity in the winning population has ramped up to a high level, the WTA circuit is relatively insensitive to external input. [sent-99, score-0.665]
</p><p>47 The ﬁrst external input to population A arrives after the winner, B, has already been selected so it is ineffective. [sent-102, score-0.408]
</p><p>48 A second external input having the same strength and duration as the ﬁrst input arrives during the winner selection phase and biases the competition towards A. [sent-103, score-0.563]
</p><p>49 2 causes WA to potentiate and WB to depress so that activity in the network continues to follow the new limit cycle even after the input is removed. [sent-105, score-0.636]
</p><p>50 Pair-wise constraints can be implemented by coupling the excitatory populations of the bottom WTA stages of two variables. [sent-109, score-0.612]
</p><p>51 Variable X1 will maximally affect X2 when the activity peak in the bottom WTA stage of X1 coincides with the winner selection interval of X2 and vice versa. [sent-113, score-0.557]
</p><p>52 2a is not related to the constraint, but it is there to prevent coupled variables in large networks from phase locking. [sent-115, score-0.284]
</p><p>53 We deﬁne the zero phase point of a variable as the point at which activity in the winning excitatory population in the bottom WTA stage reaches a peak and we assume the phase changes linearly during an oscillation period (from one peak to the next). [sent-117, score-1.112]
</p><p>54 X1 gradually begins to lead X2 until at a particular phase difference, input from X1 is able to bias the competition in X2 so that the B population in X2 wins even though the A population is receiving a stronger projection from the D population in X2. [sent-122, score-0.672]
</p><p>55 Only the bottom WTA stages of the four variables and the inter-variable connections coupling the bottom WTA stages are shown. [sent-128, score-0.422]
</p><p>56 The only states, or oscillatory modes, of X1, X2, and X3 that are stable under arbitrary phase relations with the intermediate variable are the states which satisfy the constraint X1 XOR X2 = X3. [sent-131, score-0.506]
</p><p>57 These irregular phase relations enable the network to search for the optimal solution of a max-CSP. [sent-133, score-0.515]
</p><p>58 The phase differences also determine which of the variables in a violated constraint changes in order to satisfy the constraint (see Fig. [sent-135, score-0.446]
</p><p>59 The irregular phase relations result in a continuous perturbation of the strengths of the different constraints by modulating the effective network connectivity embodying these constraints. [sent-137, score-0.646]
</p><p>60 The transiently dominant unsatisﬁed constraints will reassign the values of the variables in their domain and pull the network out of the local optimum. [sent-140, score-0.397]
</p><p>61 The network thus searches for optimal solutions by effectively perturbing the underlying max-CSP. [sent-141, score-0.3]
</p><p>62 Under this search scheme, states that satisfy all constraints are dynamically stable since any perturbation of the strengths of the constraints deﬁning the max-CSP will result in a constraints conﬁguration that reinforces the current fully consistent state of the network. [sent-142, score-0.692]
</p><p>63 In principle, if some variables/oscillators phase-lock, then the weights of the constraint(s) among these variables will not change anymore, which will impact the ability of the network to ﬁnd good solutions. [sent-143, score-0.312]
</p><p>64 1  Network Behavior in the Presence of a Fully Consistent Variable Assignment  We simulated a recurrent neuronal network that represents a CSP that has ten binary variables and nine tertiary constraints (see Fig. [sent-146, score-0.658]
</p><p>65 Each variable is represented by the network in Fig. [sent-148, score-0.318]
</p><p>66 Each tertiary constraint is implemented by introducing an intermediate variable and using a coupling scheme similar to the one in Fig. [sent-150, score-0.322]
</p><p>67 The network always converges to one of the optimal variable assignments. [sent-155, score-0.318]
</p><p>68 (c) Evolution of network state in a sample trial. [sent-161, score-0.335]
</p><p>69 The top plot shows the number of constraints violated by the variable assignment decoded from the network state. [sent-162, score-0.564]
</p><p>70 The bottom plot shows the Hamming distance between the decoded variable assignment to each of the two fully consistent solutions. [sent-163, score-0.308]
</p><p>71 The search resumes to ﬁnd a fully consistent variable assignment that is compatible with the external input. [sent-165, score-0.5]
</p><p>72 3c shows how the network state evolves in a sample trial. [sent-169, score-0.335]
</p><p>73 Due to the continuous perturbation of the weights caused by the irregular phase relations between the variables/oscillators, the network sometimes takes steps that lead to the violation of more constraints. [sent-170, score-0.458]
</p><p>74 We model the arrival of external evidence by activating an additional variable/oscillator that has only one state, or limit cycle, and which is coupled to one of the original problem variables. [sent-172, score-0.37]
</p><p>75 3d shows that the network is able to take the external evidence into account by searching for, and ﬁnally settling into, the only remaining fully consistent state that accommodates the external evidence. [sent-176, score-0.933]
</p><p>76 2  Network Behavior in the Absence of Fully Consistent Variable Assignments  As shown in the previous section, if a fully consistent solution exists, the network state will end up in that solution and stay there. [sent-178, score-0.464]
</p><p>77 If no such solution exists, the network will never settle into one variable assignment, but will keep exploring possible assignments and will spend more time in solutions that satisfy more constraints. [sent-179, score-0.393]
</p><p>78 Red squares are data points (the time the network spent in one particular state), a blue star is the average time spent in states of equal energy and the green line is an exponential ﬁt to the blue stars. [sent-194, score-0.41]
</p><p>79 Let E(s) be a function that maps a network state s to the number of constraints it violates; this is analogous to an energy function and we will refer to E(s) as the energy of state s. [sent-204, score-0.641]
</p><p>80 4a, we observe that the average time the network spends in states with energy E is t(E) = c1 exp(−c2 E) as can be seen in Fig. [sent-206, score-0.452]
</p><p>81 The network spends almost equal times in complementary states that have low energy. [sent-208, score-0.378]
</p><p>82 Complementary states are maximally different but the network is able to traverse the space of intervening states, which can have higher energy, in order to visit the complementary states almost equally often. [sent-209, score-0.485]
</p><p>83 We expect the network to spend less time in less consistent states; the higher the number of violated constraints, the more rapidly the variable values change because there are more possible phase relations that can emphasize a violated constraint. [sent-210, score-0.78]
</p><p>84 For example, the network can never go into states where all constraints are violated even though they have ﬁnite energies. [sent-213, score-0.536]
</p><p>85 One possible functional role for oscillatory activity is that it rhythmically modulates the sensitivity of neuronal circuits to external inﬂuence [22, 23]. [sent-225, score-0.68]
</p><p>86 We have used the idea of oscillatory modulation of sensitivity to construct multi-stable neural oscillators whose state, or limit cycle, can be changed by external inputs only in narrow, periodically recurring temporal windows. [sent-227, score-0.483]
</p><p>87 External input to the network can be interpreted as an additional constraint that immediately affects the search for maximally consistent states. [sent-229, score-0.566]
</p><p>88 Continuous reformulation of the problem, by adding new constraints, is problematic for any approach that works by having an initial exploratory phase that slowly morphs into a greedy search for optimal solutions, as the exploratory phase has to be restarted after a change in the problem. [sent-230, score-0.359]
</p><p>89 5 where the network spends less time in a state that violates more constraints. [sent-235, score-0.377]
</p><p>90 The size of the proposed network grows linearly with the number of variables in the problem. [sent-236, score-0.312]
</p><p>91 We used the network to solve a graph coloring problem with 17 nodes and 4 colors (each oscillator/variable representing a node had 4 possible stable limit cycles). [sent-239, score-0.356]
</p><p>92 5  Conclusions and Future Work  By combining two basic dynamical mechanisms observed in many brain areas, oscillation and competition, we constructed a recurrent neuronal network that can solve constraint satisfaction problems. [sent-243, score-0.757]
</p><p>93 The proposed network deterministically searches for optimal solutions by modulating the effective network connectivity through oscillations. [sent-244, score-0.562]
</p><p>94 The network can take into account partial external evidence that constrains the values of some variables and extrapolate from this partial evidence to reach states that are maximally consistent with the external evidence and the internal constraints. [sent-246, score-1.125]
</p><p>95 For sample problems, we have shown empirically that the network searches for, and settles into, a state that satisﬁes all constraints if there is one, otherwise it explores the space of highly consistent states with a stronger bias towards states that satisfy more constraints. [sent-247, score-0.692]
</p><p>96 An analytic framework for understanding the search scheme employed by the network is a topic for future work. [sent-248, score-0.319]
</p><p>97 The proposed network exploits its temporal dynamics and analog properties to solve a class of computationally intensive problems. [sent-249, score-0.372]
</p><p>98 The WTA modules making up the network can be efﬁciently implemented using neuromorphic VLSI circuits [26]. [sent-250, score-0.503]
</p><p>99 The results presented in this work encourage the design of neuromorphic circuits and components that implement the full network in order to solve constraint satisfaction problems in compact and ultra-low power VLSI systems. [sent-251, score-0.64]
</p><p>100 Chaotic simulated annealing by a neural network model with transient chaos. [sent-277, score-0.296]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wta', 0.534), ('network', 0.262), ('external', 0.212), ('activity', 0.18), ('excitatory', 0.161), ('population', 0.161), ('stage', 0.133), ('winner', 0.127), ('neuromorphic', 0.12), ('violated', 0.115), ('dynamics', 0.11), ('phase', 0.109), ('cycles', 0.109), ('cycle', 0.105), ('populations', 0.105), ('oscillatory', 0.104), ('xor', 0.104), ('neuronal', 0.101), ('recurrent', 0.092), ('satisfaction', 0.089), ('constraint', 0.086), ('consistent', 0.086), ('constraints', 0.085), ('circuits', 0.083), ('bottom', 0.077), ('energy', 0.074), ('coupling', 0.074), ('states', 0.074), ('hz', 0.073), ('inhibitory', 0.073), ('state', 0.073), ('stages', 0.072), ('oscillation', 0.07), ('vlsi', 0.068), ('tertiary', 0.068), ('networks', 0.066), ('strengths', 0.064), ('plastic', 0.063), ('wb', 0.063), ('attractors', 0.059), ('coupled', 0.059), ('csps', 0.058), ('vth', 0.058), ('dynamical', 0.057), ('search', 0.057), ('winning', 0.056), ('circuit', 0.056), ('wa', 0.056), ('variable', 0.056), ('attractor', 0.054), ('internal', 0.054), ('limit', 0.054), ('plasticity', 0.05), ('irregular', 0.05), ('variables', 0.05), ('hamming', 0.049), ('steady', 0.049), ('microcircuit', 0.047), ('oscillations', 0.047), ('assignment', 0.046), ('evidence', 0.045), ('competition', 0.045), ('douglas', 0.045), ('depression', 0.045), ('potentiation', 0.045), ('csp', 0.045), ('stimulus', 0.043), ('fully', 0.043), ('cortical', 0.043), ('ring', 0.043), ('spends', 0.042), ('exploratory', 0.042), ('maximally', 0.04), ('stable', 0.04), ('inputs', 0.04), ('cortically', 0.039), ('destroyed', 0.039), ('embodying', 0.039), ('entrainment', 0.039), ('indiveri', 0.039), ('maurizio', 0.039), ('periodically', 0.039), ('ramps', 0.039), ('rodney', 0.039), ('shuts', 0.039), ('solutions', 0.038), ('implemented', 0.038), ('settles', 0.038), ('relations', 0.037), ('assignments', 0.037), ('hop', 0.036), ('behavior', 0.036), ('architecture', 0.035), ('equally', 0.035), ('input', 0.035), ('annealing', 0.034), ('silicon', 0.034), ('dep', 0.034), ('wmin', 0.034), ('oscillators', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="267-tfidf-1" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>Author: Hesham Mostafa, Lorenz. K. Mueller, Giacomo Indiveri</p><p>Abstract: We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. External evidence, or input to the network, can force variables to speciﬁc values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. 1</p><p>2 0.17897303 <a title="267-tfidf-2" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>3 0.17312349 <a title="267-tfidf-3" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>4 0.16921237 <a title="267-tfidf-4" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>5 0.16883615 <a title="267-tfidf-5" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>6 0.15409486 <a title="267-tfidf-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.14241254 <a title="267-tfidf-7" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>8 0.1201665 <a title="267-tfidf-8" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>9 0.097556531 <a title="267-tfidf-9" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>10 0.097387135 <a title="267-tfidf-10" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>11 0.09558022 <a title="267-tfidf-11" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>12 0.086823747 <a title="267-tfidf-12" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>13 0.0865383 <a title="267-tfidf-13" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>14 0.084484249 <a title="267-tfidf-14" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>15 0.083339117 <a title="267-tfidf-15" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>16 0.080236621 <a title="267-tfidf-16" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>17 0.071018539 <a title="267-tfidf-17" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>18 0.068122201 <a title="267-tfidf-18" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>19 0.066493921 <a title="267-tfidf-19" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>20 0.066374712 <a title="267-tfidf-20" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.068), (2, -0.127), (3, -0.075), (4, -0.206), (5, -0.048), (6, -0.034), (7, -0.089), (8, 0.022), (9, -0.024), (10, 0.137), (11, -0.037), (12, 0.097), (13, 0.039), (14, -0.033), (15, 0.034), (16, -0.045), (17, 0.024), (18, 0.013), (19, -0.029), (20, 0.04), (21, -0.063), (22, 0.025), (23, 0.14), (24, 0.001), (25, 0.063), (26, -0.021), (27, 0.045), (28, -0.025), (29, -0.025), (30, 0.001), (31, -0.084), (32, 0.014), (33, 0.006), (34, -0.046), (35, -0.041), (36, 0.027), (37, -0.007), (38, 0.039), (39, -0.03), (40, -0.055), (41, 0.016), (42, -0.131), (43, 0.033), (44, 0.056), (45, 0.016), (46, 0.042), (47, -0.106), (48, -0.036), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96407068 <a title="267-lsi-1" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>Author: Hesham Mostafa, Lorenz. K. Mueller, Giacomo Indiveri</p><p>Abstract: We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. External evidence, or input to the network, can force variables to speciﬁc values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. 1</p><p>2 0.8483429 <a title="267-lsi-2" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>Author: Wenhao Zhang, Si Wu</p><p>Abstract: Psychophysical experiments have demonstrated that the brain integrates information from multiple sensory cues in a near Bayesian optimal manner. The present study proposes a novel mechanism to achieve this. We consider two reciprocally connected networks, mimicking the integration of heading direction information between the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas. Each network serves as a local estimator and receives an independent cue, either the visual or the vestibular, as direct input for the external stimulus. We ﬁnd that positive reciprocal interactions can improve the decoding accuracy of each individual network as if it implements Bayesian inference from two cues. Our model successfully explains the experimental ﬁnding that both MSTd and VIP achieve Bayesian multisensory integration, though each of them only receives a single cue as direct external input. Our result suggests that the brain may implement optimal information integration distributively at each local estimator through the reciprocal connections between cortical regions. 1</p><p>3 0.81076419 <a title="267-lsi-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.77123141 <a title="267-lsi-4" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>Author: Abbas Edalat</p><p>Abstract: We solve the mean ﬁeld equations for a stochastic Hopﬁeld network with temperature (noise) in the presence of strong, i.e., multiply stored, patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the ﬁrst time a rigorous solution of the mean ﬁled equations for the standard Hopﬁeld model and is in contrast to the mathematically unjustiﬁable replica technique that has been used hitherto for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when the sum of the squares of degrees of the patterns is negligible compared to the network size. In the case of a single strong pattern, when the ratio of the number of all stored pattens and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean ﬁeld and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justiﬁcation for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy. 1 Introduction: Multiply learned patterns in Hopﬁeld networks The Hopﬁeld network as a model of associative memory and unsupervised learning was introduced in [23] and has been intensively studied from a wide range of viewpoints in the past thirty years. However, properties of a strong pattern, as a pattern that has been multiply stored or learned in these networks, have only been examined very recently, a surprising delay given that repetition of an activity is the basis of learning by the Hebbian rule and long term potentiation. In particular, while the storage capacity of a Hopﬁeld network with certain correlated patterns has been tackled [13, 25], the storage capacity of a Hopﬁeld network in the presence of strong as well as random patterns has not been hitherto addressed. The notion of a strong pattern of a Hopﬁeld network has been proposed in [15] to model attachment types and behavioural prototypes in developmental psychology and psychotherapy. This suggestion has been motivated by reviewing the pioneering work of Bowlby [9] in attachment theory and highlighting how a number of academic biologists, psychiatrists, psychologists, sociologists and neuroscientists have consistently regarded Hopﬁeld-like artiﬁcial neural networks as suitable tools to model cognitive and behavioural constructs as patterns that are deeply and repeatedly learned by individuals [11, 22, 24, 30, 29, 10]. A number of mathematical properties of strong patterns in Hopﬁeld networks, which give rise to strong attractors, have been derived in [15]. These show in particular that strong attractors are strongly stable; a series of experiments have also been carried out which conﬁrm the mathematical 1 results and also indicate that a strong pattern stored in the network can be retrieved even in the presence of a large number of simple patterns, far exceeding the well-known maximum load parameter or storage capacity of the Hopﬁeld network with random patterns (αc ≈ 0.138). In this paper, we consider strong patterns in stochastic Hopﬁeld model with temperature, which accounts for various types of noise in the network. In these networks, the updating rule is probabilistic and depend on the temperature. Since analytical solution of such a system is not possible in general, one strives to obtain the average behaviour of the network when the input to each node, the so-called ﬁeld at the node, is replaced with its mean. This is the basis of mean ﬁeld theory for these networks. Due to the close connection between the Hopﬁeld network and the Ising model in ferromagnetism [1, 8], the mean ﬁeld approach for the Hopﬁeld network and its variations has been tackled using the replica method, starting with the pioneering work of Amit, Gutfreund and Sompolinsky [3, 2, 4, 19, 31, 1, 13]. Although this method has been widely used in the theory of spin glasses in statistical physics [26, 16] its mathematical justiﬁcation has proved to be elusive as we will discuss in the next section; see for example [20, page 264], [14, page 27], and [7, page 9]. In [17] and independently in [27], an alternative technique to the replica method for solving the mean ﬁeld equations has been proposed which is reproduced and characterised as heuristic in [20, section 2.5] since it relies on a number of assumptions that are not later justiﬁed and uses a number of mathematical steps that are not validated. Here, we use the basic idea of the above heuristic to develop a veriﬁable mathematical framework with provable results grounded on elements of probability theory, with which we assume the reader is familiar. This technique allows us to solve the mean ﬁeld equations for the Hopﬁeld network in the presence of strong patterns and use the results to study, ﬁrst, the stability of these patterns in the presence of temperature (noise) and, second, the storage capacity of the network with a single strong pattern at temperature zero. We show that the critical temperature for the stability of a strong pattern is equal to its degree (i.e., its multiplicity) when the ratio of the sum of the squares of degrees of the patterns to the network size tends to zero when the latter tends to inﬁnity. In the case that there is only one strong pattern present with its degree small compared to the number of patterns and the latter is a ﬁxed multiple of the number of nodes, we ﬁnd the distribution of the overlap of the mean ﬁeld and the patterns when the strong pattern is being retrieved. We use these distributions to prove that the storage capacity for retrieving a strong pattern exceeds that for a simple pattern by a multiplicative factor equal to the square of the degree of the strong attractor. This result matches the ﬁnding in [15] regarding the capacity of a network to recall strong patterns as mentioned above. Our results therefore show that strong patterns are robust and persistent in the network memory as attachment types and behavioural prototypes are in the human memory system. In this paper, we will several times use Lyapunov’s theorem in probability which provides a simple sufﬁcient condition to generalise the Central Limit theorem when we deal with independent but not necessarily identically distributed random variables. We require a general form of this theorem kn as follows. Let Yn = N i=1 Yni , for n ∈ I , be a triangular array of random variables such that for each n, the random variables Yni , for 1 ≤ i ≤ kn are independent with E(Yni ) = 0 2 2 and E(Yni ) = σni , where E(X) stands for the expected value of the random variable X. Let kn 2 2 sn = i=1 σni . We use the notation X ∼ Y when the two random variables X and Y have the same distribution (for large n if either or both of them depend on n). Theorem 1.1 (Lyapunov’s theorem [6, page 368]) If for some δ > 0, we have the condition: 1 E(|Yn |2+δ |) → 0 s2+δ n d d as n → ∞ then s1 Yn −→ N (0, 1) as n → ∞ where −→ denotes convergence in distribution, and we denote n by N (a, σ 2 ) the normal distribution with mean a and variance σ 2 . Thus, for large n we have Yn ∼ N (0, s2 ). n 2 2 Mean ﬁeld theory We consider a Hopﬁeld network with N neurons i = 1, . . . , N with values Si = ±1 and follow the notations in [20]. As in [15], we assume patterns can be multiply stored and the degree of a pattern is deﬁned as its multiplicity. The total number of patterns, counting their multiplicity, is denoted by p and we assume there are n patterns ξ 1 , . . . , ξ n with degrees d1 , . . . , dn ≥ 1 respectively and that n the remaining p − k=1 dk ≥ 0 patterns are simple, i.e., each has degree one. Note that by our assumptions there are precisely n p0 = p + n − dk k=1 distinct patterns, which we assume are independent and identically distributed with equal probability of taking value ±1 for each node. More generally, for any non-negative integer k ∈ I , we let N p0 dk . µ pk = µ=1 p µ µ 0 1 We use the generalized Hebbian rule for the synaptic couplings: wij = N µ=1 dµ ξi ξj for i = j with wii = 0 for 1 ≤ i, j ≤ N . As in the standard stochastic Hopﬁeld model [20], we use Glauber dynamics [18] for the stochastic updating rule with pseudo-temperature T > 0, which accounts for various types of noise in the network, and assume zero bias in the local ﬁeld. Putting β = 1/T (i.e., with the Boltzmann constant kB = 1) and letting fβ (h) = 1/(1 + exp(−2βh)), the stochastic updating rule at time t is given by: N Pr(Si (t + 1) = ±1) = fβ (±hi (t)), where hi (t) = wij Sj (t), (1) j=1 is the local ﬁeld at i at time t. The updating is implemented asynchronously in a random way. The energy of the network in the conﬁguration S = (Si )N is given by i=1 N 1 Si Sj wij . H(S) = − 2 i,j=1 For large N , this speciﬁes a complex system, with an underlying state space of dimension 2N , which in general cannot be solved exactly. However, mean ﬁeld theory has proved very useful in studying Hopﬁeld networks. The average updated value of Si (t + 1) in Equation (1) is Si (t + 1) = 1/(1 + e−2βhi (t) ) − 1/(1 + e2βhi (t) ) = tanh(βhi (t)), (2) where . . . denotes taking average with respect to the probability distribution in the updating rule in Equation (1). The stationary solution for the mean ﬁeld thus satisﬁes: Si = tanh(βhi ) , (3) The average overlap of pattern ξ µ with the mean ﬁeld at the nodes of the network is given by: mν = 1 N N ν ξi Si (4) i=1 The replica technique for solving the mean ﬁeld problem, used in the case p/N = α > 0 as N → ∞, seeks to obtain the average of the overlaps in Equation (4) by evaluating the partition function of the system, namely, Z = TrS exp(−βH(S)), where the trace TrS stands for taking sum over all possible conﬁgurations S = (Si )N . As it i=1 is generally the case in statistical physics, once the partition function of the system is obtained, 3 all required physical quantities can in principle be computed. However, in this case, the partition function is very difﬁcult to compute since it entails computing the average log Z of log Z, where . . . indicates averaging over the random distribution of the stored patterns ξ µ . To overcome this problem, the identity Zk − 1 log Z = lim k→0 k is used to reduce the problem to ﬁnding the average Z k of Z k , which is then computed for positive integer values of k. For such k, we have: Z k = TrS 1 TrS 2 . . . TrS k exp(−β(H(S 1 ) + H(S 1 ) + . . . + H(S k ))), where for each i = 1, . . . , k the super-scripted conﬁguration S i is a replica of the conﬁguration state. In computing the trace over each replica, various parameters are obtained and the replica symmetry condition assumes that these parameters are independent of the particular replica under consideration. Apart from this assumption, there are two basic mathematical problems with the technique which makes it unjustiﬁable [20, page 264]. Firstly, the positive integer k above is eventually treated as a real number near zero without any mathematical justiﬁcation. Secondly, the order of taking limits, in particular the order of taking the two limits k → 0 and N → ∞, are several times interchanged again without any mathematical justiﬁcation. Here, we develop a mathematically rigorous method for solving the mean ﬁeld problem, i.e., computing the average of the overlaps in Equation (4) in the case of p/N = α > 0 as N → ∞. Our method turns the basic idea of the heuristic presented in [17] and reproduced in [20] for solving the mean ﬁeld equation into a mathematically veriﬁable formalism, which for the standard Hopﬁeld network with random stored patterns gives the same result as the replica method, assuming replica symmetry. In the presence of strong patterns we obtain a set of new results as explained in the next two sections. The mean ﬁeld equation is obtained from Equation (3) by approximating the right hand side of N this equation by the value of tanh at the mean ﬁeld hi = j=1 wij Sj , ignoring the sum N j=1 wij (Sj − Sj ) for large N [17, page 32]: Si = tanh(β hi ) = tanh β N N j=1 p0 µ=1 µ µ dµ ξi ξj Sj . (5) Equation (5) gives the mean ﬁeld equation for the Hopﬁeld network with n possible strong patterns n ξ µ (1 ≤ µ ≤ n) and p − µ=1 dµ simple patterns ξ µ with n + 1 ≤ µ ≤ p0 . As in the standard Hopﬁeld model, where all patterns are simple, we have two cases to deal with. However, we now have to account for the presence of strong attractors and our two cases will be as follows: (i) In the p0 ﬁrst case we assume p2 := µ=1 d2 = o(N ), which includes the simpler case p2 N when p2 µ is ﬁxed and independent of N . (ii) In the second case we assume we have a single strong attractor with the load parameter p/N = α > 0. 3 Stability of strong patterns with noise: p2 = o(N ) The case of constant p and N → ∞ is usually referred to as α = 0 in the standard Hopﬁeld model. Here, we need to consider the sum of degrees of all stored patterns (and not just the number of patterns) compared to N . We solve the mean ﬁeld equation with T > 0 by using a method similar in spirit to [20, page 33] for the standard Hopﬁeld model, but in our case strong patterns induce a sequence of independent but non-identically distributed random variables in the crosstalk term, where the Central Limit Theorem cannot be used; we show however that Lyapunov’s theorem (Theorem (1.1) can be invoked. In retrieving pattern ξ 1 , we look for a solution of the mean ﬁled 1 equation of the form: Si = mξi , where m > 0 is a constant. Using Equation (5) and separating 1 the contribution of ξ in the argument of tanh, we obtain:  1 mξi = tanh    mβ  1 d1 ξi + N 4 µ µ 1 dµ ξi ξj ξj  . j=i,µ>1 (6) For each N , µ > 1 and j = i, let dµ µ µ 1 (7) ξ ξ ξ . N i j j 2 This gives (p0 − 1)(N − 1) independent random variables with E(YN µj ) = 0, E(YN µj ) = d2 /N 2 , µ 3 3 3 and E(|YN µj |) = dµ /N . We have: YN µj = s2 := N 2 E(YN µj ) = µ>1,j=i 1 N −1 d2 ∼ N 2 µ>1 µ N d2 . µ (8) µ>1 Thus, as N → ∞, we have: 1 s3 N 3 E(|YN µj |) ∼ √ µ>1,j=i µ>1 N( d3 µ µ>1 d2 )3/2 µ → 0. (9) as N → ∞ since for positive numbers dµ we always have µ>1 d3 < ( µ>1 d2 )3/2 . Thus the µ µ Lyapunov condition is satisﬁed for δ = 1. By Lyapunov’s theorem we deduce: 1 N µ µ 1 dµ ξi ξj ξj ∼ N d2 /N µ 0, (10) µ>1 µ>1,j=i Since we also have p2 = o(N ), it follows that we can ignore the second term, i.e., the crosstalk term, in the argument of tanh in Equation (6) as N → ∞; we thus obtain: m = tanh βd1 m. (11) To examine the ﬁxed points of the Equation (11), we let d = d1 for convenience and put x = βdm = dm/T , so that tanh x = T x/d; see Figure 1. It follows that Tc = d is the critical temperature. If T < d then there is a non-zero (non-trivial) solution for m, whereas for T > d we only have the trivial solution. For d = 1 our solution is that of the standard Hopﬁeld network as in [20, page 34]. (d < T) y>x y = x ( d = T) y = tanh x y</p><p>5 0.73096442 <a title="267-lsi-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.70534074 <a title="267-lsi-6" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>7 0.65956283 <a title="267-lsi-7" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>8 0.65930885 <a title="267-lsi-8" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>9 0.56825995 <a title="267-lsi-9" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>10 0.56805843 <a title="267-lsi-10" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>11 0.53646386 <a title="267-lsi-11" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>12 0.51649964 <a title="267-lsi-12" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>13 0.51502341 <a title="267-lsi-13" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>14 0.50685215 <a title="267-lsi-14" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>15 0.4859682 <a title="267-lsi-15" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>16 0.4804953 <a title="267-lsi-16" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>17 0.47910219 <a title="267-lsi-17" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>18 0.4707799 <a title="267-lsi-18" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>19 0.4501777 <a title="267-lsi-19" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>20 0.44440463 <a title="267-lsi-20" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.033), (33, 0.073), (34, 0.111), (41, 0.022), (49, 0.047), (56, 0.077), (70, 0.486), (85, 0.023), (89, 0.015), (93, 0.025), (95, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90986937 <a title="267-lda-1" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>Author: Hesham Mostafa, Lorenz. K. Mueller, Giacomo Indiveri</p><p>Abstract: We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. External evidence, or input to the network, can force variables to speciﬁc values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. 1</p><p>2 0.81042773 <a title="267-lda-2" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>3 0.8019861 <a title="267-lda-3" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>4 0.79932821 <a title="267-lda-4" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>5 0.78367501 <a title="267-lda-5" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>Author: Shaodan Zhai, Tian Xia, Ming Tan, Shaojun Wang</p><p>Abstract: We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classiﬁer of weak classiﬁers through directly minimizing empirical classiﬁcation error over labeled training examples; once the training classiﬁcation error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classiﬁers to maximize any targeted arbitrarily deﬁned margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n′ th order bottom sample margin. 1</p><p>6 0.71767336 <a title="267-lda-6" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>7 0.70067525 <a title="267-lda-7" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>8 0.57710582 <a title="267-lda-8" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>9 0.5703221 <a title="267-lda-9" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>10 0.56340438 <a title="267-lda-10" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>11 0.53352487 <a title="267-lda-11" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>12 0.4980219 <a title="267-lda-12" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>13 0.49208048 <a title="267-lda-13" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>14 0.49073878 <a title="267-lda-14" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>15 0.4769229 <a title="267-lda-15" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>16 0.46965209 <a title="267-lda-16" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>17 0.46227795 <a title="267-lda-17" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>18 0.45232749 <a title="267-lda-18" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>19 0.44713748 <a title="267-lda-19" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>20 0.4463098 <a title="267-lda-20" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
