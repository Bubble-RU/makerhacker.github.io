<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-356" href="#">nips2013-356</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</h1>
<br/><p>Source: <a title="nips-2013-356-pdf" href="http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf">pdf</a></p><p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>Reference: <a title="nips-2013-356-reference" href="../nips2013_reference/nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. [sent-8, score-0.802]
</p><p>2 Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. [sent-11, score-0.474]
</p><p>3 Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. [sent-12, score-1.682]
</p><p>4 We then use novelty detection methods to differentiate unseen classes from seen classes. [sent-13, score-1.163]
</p><p>5 We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. [sent-14, score-1.195]
</p><p>6 1  Introduction  The ability to classify instances of an unseen visual class, called zero-shot learning, is useful in several situations. [sent-15, score-0.669]
</p><p>7 In this work, we show how to make use of the vast amount of knowledge about the visual world available in natural language to classify unseen objects. [sent-17, score-0.693]
</p><p>8 We attempt to model people’s ability to identify unseen objects even if the only knowledge about that object came from reading about it. [sent-18, score-0.572]
</p><p>9 We introduce a zero-shot model that can predict both seen and unseen classes. [sent-20, score-0.704]
</p><p>10 For instance, without ever seeing a cat image, it can determine whether an image shows a cat or a known category from the training set such as a dog or a horse. [sent-21, score-0.721]
</p><p>11 First, images are mapped into a semantic space of words that is learned by a neural network model [15]. [sent-25, score-0.65]
</p><p>12 By learning an image mapping into this space, the word vectors get implicitly grounded by the visual modality, allowing us to give prototypical instances for various words. [sent-27, score-0.549]
</p><p>13 Second, because classiﬁers prefer to assign test images into classes for which they have seen training examples, the model incorporates novelty detection which determines whether a new image is on the manifold of known categories. [sent-28, score-1.056]
</p><p>14 Otherwise, images are assigned to a class based on the likelihood of being an unseen category. [sent-30, score-0.82]
</p><p>15 The ﬁrst strategy prefers high accuracy for unseen classes, the second for seen classes. [sent-32, score-0.744]
</p><p>16 Unlike previous work on zero-shot learning which can only predict intermediate features or differentiate between various zero-shot classes [21, 27], our joint model can achieve both state of the art accuracy on known classes as well as reasonable performance on unseen classes. [sent-33, score-1.23]
</p><p>17 Furthermore, compared to related work on knowledge transfer [21, 28] we do not require manually deﬁned semantic 1  Manifold of known classes truck horse  auto  dog  Tra  ini  ng im a  New test image from unknown class  cat  ge  s  Figure 1: Overview of our cross-modal zero-shot model. [sent-34, score-1.288]
</p><p>18 We ﬁrst map each new testing image into a lower dimensional semantic word vector space. [sent-35, score-0.679]
</p><p>19 If the image is ‘novel’, meaning not on the manifold, we classify it with the help of unsupervised semantic word vectors. [sent-37, score-0.79]
</p><p>20 In this example, the unseen classes are truck and cat. [sent-38, score-0.942]
</p><p>21 or visual attributes for the zero-shot classes, allowing us to use state-of-the-art unsupervised and unaligned image features instead along with unsupervised and unaligned language corpora. [sent-39, score-0.51]
</p><p>22 They are able to predict semantic features even for words for which they have not seen scans and experiment with differentiating between several zero-shot classes. [sent-46, score-0.51]
</p><p>23 However, they do not classify new test instances into both seen and unseen classes. [sent-47, score-0.724]
</p><p>24 [21] construct a set of binary attributes for the image classes that convey various visual characteristics, such as “furry” and “paws” for bears and “wings” and “ﬂies” for birds. [sent-50, score-0.502]
</p><p>25 [21, 10] were two of the ﬁrst to use well-designed visual attributes of unseen classes to classify them. [sent-61, score-0.952]
</p><p>26 Similar to our work, they use unsupervised large text corpora to 2  learn semantic word representations. [sent-73, score-0.631]
</p><p>27 Unless otherwise mentioned, all word vectors are initialized with pre-trained d = 50-dimensional word vectors from the unsupervised model of Huang et al. [sent-82, score-0.64]
</p><p>28 Using free Wikipedia text, their model learns word vectors by predicting how likely it is for each word to occur in its context. [sent-84, score-0.536]
</p><p>29 Their model uses both local context in the window around each word and global document contex, thus capturing distributional syntactic and semantic information. [sent-85, score-0.614]
</p><p>30 4  Projecting Images into Semantic Word Spaces  In order to learn semantic relationships and class membership of images we project the image feature vectors into the d-dimensional, semantic word space F . [sent-90, score-1.288]
</p><p>31 Some of the classes y in this set will have available training data, others will be zero-shot classes without any training data. [sent-92, score-0.604]
</p><p>32 We deﬁne the former as the seen classes Ys and the latter as the unseen classes Yu . [sent-93, score-1.175]
</p><p>33 Let W = Ws ∪ Wu be the set of word vectors in Rd for both seen and unseen visual classes, respectively. [sent-94, score-1.041]
</p><p>34 All training images x(i) ∈ Xy of a seen class y ∈ Ys are mapped to the word vector wy corresponding to the class name. [sent-95, score-0.97]
</p><p>35 By projecting images into the word vector space, we implicitly extend the semantics with a visual grounding, allowing us to query the space, for instance for prototypical visual instances of a word. [sent-100, score-0.632]
</p><p>36 2 shows a visualization of the 50-dimensional semantic space with word vectors and images of both seen and unseen classes. [sent-102, score-1.522]
</p><p>37 We can observe that most classes are tightly clustered around their corresponding word vector while the zero-shot classes (cat and truck for this mapping) do not have close-by vectors. [sent-105, score-0.896]
</p><p>38 However, the images of the two zero-shot classes are close to semantically similar classes (such as in the case of cat, which is close to dog and horse but is far away from car or ship). [sent-106, score-0.935]
</p><p>39 This observation motivated the idea for ﬁrst detecting images of unseen classes and then classifying them to the zero-shot word vectors. [sent-107, score-1.271]
</p><p>40 In general, we want to predict p(y|x), the conditional probability for both seen and unseen classes y ∈ Ys ∪ Yu given an image from the test set x ∈ Xt . [sent-109, score-1.096]
</p><p>41 To achieve this we will employ the semantic vectors to which these images have been mapped to f ∈ Ft . [sent-110, score-0.659]
</p><p>42 Word vector locations are highlighted and mapped image locations are shown both for images for which this mapping has been trained and unseen images. [sent-112, score-1.053]
</p><p>43 Let Xs be the set of all feature vectors for training images of seen classes and Fs their corresponding semantic vectors. [sent-115, score-0.997]
</p><p>44 We predict a class y for a new input image x and its mapped semantic vector f via: P (y|V, x, Xs , Fs , W, θ)P (V |x, Xs , Fs , W, θ). [sent-117, score-0.603]
</p><p>45 p(y|x, Xs , Fs , W, θ) = V ∈{s,u}  Marginalizing out the novelty variable V allows us to ﬁrst distinguish between seen and unseen classes. [sent-118, score-0.855]
</p><p>46 The seen image classiﬁer can be a state of the art softmax classiﬁer while the unseen classiﬁer can be a simple Gaussian discriminator. [sent-120, score-0.888]
</p><p>47 1 Strategies for Novelty Detection We now consider two strategies for predicting whether an image is of a seen or unseen class. [sent-122, score-0.817]
</p><p>48 The term P (V = u|x, Xs , Fs , W, θ) is the probability of an image being in an unseen class. [sent-123, score-0.685]
</p><p>49 An image from an unseen class will not be very close to the existing training images but will still be roughly in the same semantic region. [sent-124, score-1.304]
</p><p>50 For instance, cat images are closest to dogs even though they are not as close to the dog word vector as most dog images are. [sent-125, score-1.124]
</p><p>51 Hence, at test time, we can use outlier detection methods to determine whether an image is in a seen or unseen class. [sent-126, score-0.996]
</p><p>52 Both are computed on the manifold of training images that were mapped to the semantic word space. [sent-128, score-0.965]
</p><p>53 The mapped points of seen classes are used to obtain this marginal. [sent-131, score-0.478]
</p><p>54 The Gaussian of each class is parameterized by the corresponding semantic word vector wy for its mean and a covariance matrix Σy that is estimated from all the mapped training points with that label. [sent-133, score-0.851]
</p><p>55 For a new image x, the outlier detector then becomes the indicator function that is 1 if the marginal probability is below a certain threshold Ty for all the classes: P (V = u|f, Xs , W, θ) := 1{∀y ∈ Ys : P (f |Fy , wy ) < Ty } We provide an experimental analysis for various thresholds T below. [sent-135, score-0.457]
</p><p>56 The thresholds are selected to make at least some fraction of the vectors from training images above threshold, that is, to be classiﬁed as a seen class. [sent-136, score-0.485]
</p><p>57 Then, we can obtain the conditional class probability using a weighted combination of classiﬁers for both seen and unseen classes (described below). [sent-140, score-0.971]
</p><p>58 2 shows that many unseen images are not technically outliers of the complete data manifold. [sent-142, score-0.774]
</p><p>59 This probability can now be used to weigh the seen and unseen classiﬁers by the appropriate amount given our belief about the outlierness of a new test image. [sent-155, score-0.675]
</p><p>60 For the zero-shot case where V = u we assume an isometric Gaussian distribution around each of the novel class word vectors and assign classes based on their likelihood. [sent-161, score-0.653]
</p><p>61 For word vectors, we use a set of 50-dimensional word vectors from the Huang dataset [15] that correspond to each CIFAR category. [sent-165, score-0.536]
</p><p>62 In this section we ﬁrst analyze the classiﬁcation performance for seen classes and unseen classes separately. [sent-168, score-1.175]
</p><p>63 Then, we combine images from the two types of classes, and discuss the trade-offs involved in our two unseen class detection strategies. [sent-169, score-0.864]
</p><p>64 1 Seen and Unseen Classes Separately First, we evaluate the classiﬁcation accuracy when presented only with images from classes that have been used in training. [sent-174, score-0.55]
</p><p>65 8  (c) Comparison  (b) LoOP model  1  unseen classes  0. [sent-192, score-0.793]
</p><p>66 8  Fraction of points classified as unseen  1  0  0  0. [sent-208, score-0.577]
</p><p>67 8  1  Fraction unseen/outlier threshold  Figure 4: Comparison of accuracies for images from previously seen and unseen categories when unseen images are detected under the (a) Gaussian threshold model, (b) LoOP model. [sent-217, score-1.958]
</p><p>68 classes excluding cat and truck, which closely matches the SVM-based classiﬁcation results in the original Coates and Ng paper [6] that used all 10 classes. [sent-221, score-0.429]
</p><p>69 In this case, the classiﬁcation is based on isometric Gaussians which amounts to simply comparing distances between word vectors of unseen classes and an image mapped into semantic space. [sent-223, score-1.678]
</p><p>70 For instance, when cat and dog are taken out from training, the resulting zero-shot classiﬁcation does not work well because none of the other 8 categories is similar enough to both images to learn a good semantic distinction. [sent-225, score-0.892]
</p><p>71 On the other hand, if cat and truck are taken out, then the cat vectors can be mapped to the word space thanks to similarities to dogs and trucks can be distinguished thanks to car, yielding better performance. [sent-226, score-0.92]
</p><p>72 We compare the performance when each image is passed through either of the two novelty detectors which decide with a certain probability (in the second scenario) whether an image belongs to a class that was used in training. [sent-245, score-0.514]
</p><p>73 Depending on this choice, the image is either passed through the softmax classiﬁer for seen category images, or assigned to the class of the nearest semantic word vector for unseen category images. [sent-246, score-1.616]
</p><p>74 4 shows the accuracies for test images for different choices made by the two scenarios for novelty detection. [sent-248, score-0.513]
</p><p>75 The test set includes an equal number of images from each category, with 8 categories having been seen before, and 2 being new. [sent-249, score-0.451]
</p><p>76 Firstly, at the left extreme of the curve, the Gaussian unseen image detector treats all of the images as unseen, and the LoOP model takes the probability threshold for an image being unseen to be 0. [sent-251, score-1.633]
</p><p>77 At this point, with all unseen images in the test set being treated as such, we achieve the highest accuracies, at 90% for this zero-shot pair. [sent-252, score-0.774]
</p><p>78 Similarly, at the other extreme of the curve, all images are classiﬁed as belonging to a seen category, and hence the softmax classiﬁer for seen images gives the best possible accuracy for these images. [sent-253, score-0.841]
</p><p>79 6  Between the extremes, the curves for unseen image accuracies and seen image accuracies fall and rise at different rates. [sent-254, score-1.211]
</p><p>80 Since the Gaussian model is liberal in designating an image as belonging to an unseen category, it treats more of the images as unseen, and hence we continue to get high unseen class accuracies along the curve. [sent-255, score-1.656]
</p><p>81 The LoOP model, which tries to detect whether an image could be regarded as an outlier for each class, does not assign very high outlier probabilities to zero-shot images due to a large number of them being spread on inside the manifold of seen images (see Fig. [sent-256, score-1.055]
</p><p>82 Hence, the LoOP model can be used in scenarios where one does not want to degrade the high performance on classes from the training set but allow for the possibility of unseen classes. [sent-259, score-0.845]
</p><p>83 4 (c) that since most images in the test set belong to previously seen categories, the LoOP model, which is conservative in assigning the unseen label, gives better overall accuracies than the Gaussian model. [sent-261, score-1.058]
</p><p>84 In general, we can choose an acceptable threshold for seen class accuracy and achieve a corresponding unseen class accuracy. [sent-262, score-0.868]
</p><p>85 For example, at 70% seen class accuracy in the Gaussian model, unseen classes can be classiﬁed with accuracies of between 30% to 15%, depending on the class. [sent-263, score-1.166]
</p><p>86 3 Combining predictions for seen and unseen classes The ﬁnal step in our experiments is to perform the full Bayesian pipeline as deﬁned by Equation 2. [sent-266, score-0.976]
</p><p>87 We train each binary attribute classiﬁer separately, and use the trained classiﬁers to construct attribute labels for unseen classes. [sent-276, score-0.68]
</p><p>88 In the mapped space, we observe that of the 100 images assigned the highest probability of being an outlier, 12% of those images are false positives. [sent-292, score-0.558]
</p><p>89 This is intuitively explained by the fact that the mapping function gathers extra semantic information from the word vectors it is trained on, and images are able to cluster better around these assumed Gaussian centroids. [sent-294, score-0.851]
</p><p>90 In the original space, there is no semantic information, and the Gaussian centroids need to be inferred from among the images themselves, which are not truly representative of the center of the image space for their classes. [sent-295, score-0.663]
</p><p>91 8 Fraction of points classified as seen  1  Figure 5: Comparison of accuracies for images from previously seen and unseen categories for the modiﬁed CIFAR-100 dataset, after training the semantic mapping with a one-layer network and two-layer network. [sent-303, score-1.669]
</p><p>92 When all images are labeled as zero shot, the peak accuracy for the 6 unseen classes is 52. [sent-312, score-1.093]
</p><p>93 Because of the large semantic space corresponding to 100 classes, the proximity of an image to its appropriate class vector is dependent on the quality of the mapping into semantic space. [sent-315, score-0.809]
</p><p>94 9  Neighbors of cat Neighbors of truck  Accuracy  We would like zero-shot images to be classi0. [sent-324, score-0.559]
</p><p>95 8 ﬁed correctly when there are a large number of unseen categories to choose from. [sent-325, score-0.631]
</p><p>96 6 correct unseen classes we create a set of distractor words. [sent-328, score-0.89]
</p><p>97 For the zero-shot class cat and 0 10 20 30 40 Number of distractor words truck, the nearest neighbors distractors include rabbit, kitten and mouse, among others. [sent-337, score-0.445]
</p><p>98 This is consistent with our expectation that a certain number of closely-related semantic neighbors would distract the classiﬁer; however, beyond that limited set, other categories would be further away in semantic space and would not affect classiﬁcation accuracy. [sent-346, score-0.718]
</p><p>99 7  Conclusion  We introduced a novel model for jointly doing standard and zero-shot classiﬁcation based on deep learned word and image representations. [sent-347, score-0.437]
</p><p>100 If the task was only to differentiate between various zero-shot classes we could obtain accuracies of up to 90% with a fully unsupervised model. [sent-349, score-0.476]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unseen', 0.543), ('semantic', 0.29), ('classes', 0.25), ('word', 0.247), ('images', 0.231), ('cat', 0.179), ('novelty', 0.156), ('truck', 0.149), ('image', 0.142), ('outlier', 0.135), ('seen', 0.132), ('accuracies', 0.126), ('wy', 0.12), ('fs', 0.112), ('dog', 0.104), ('distractor', 0.097), ('mapped', 0.096), ('classi', 0.095), ('categories', 0.088), ('loop', 0.083), ('lampert', 0.078), ('visual', 0.077), ('distributional', 0.077), ('lof', 0.075), ('shot', 0.069), ('accuracy', 0.069), ('deer', 0.068), ('isometric', 0.068), ('pdist', 0.068), ('category', 0.065), ('unsupervised', 0.062), ('ys', 0.059), ('er', 0.058), ('attribute', 0.056), ('ship', 0.056), ('xs', 0.053), ('training', 0.052), ('pipeline', 0.051), ('neighbors', 0.05), ('classify', 0.049), ('manifold', 0.049), ('deep', 0.048), ('fy', 0.048), ('softmax', 0.046), ('class', 0.046), ('auto', 0.045), ('detection', 0.044), ('horse', 0.043), ('vectors', 0.042), ('unaligned', 0.042), ('mapping', 0.041), ('socher', 0.04), ('coates', 0.04), ('nearest', 0.04), ('transfer', 0.04), ('differentiate', 0.038), ('layer', 0.037), ('visualization', 0.037), ('baroni', 0.034), ('bruni', 0.034), ('classified', 0.034), ('attributes', 0.033), ('multimodal', 0.033), ('words', 0.033), ('representations', 0.033), ('semantically', 0.032), ('manning', 0.032), ('threshold', 0.032), ('text', 0.032), ('xy', 0.03), ('sentiment', 0.03), ('frog', 0.03), ('airplane', 0.03), ('erf', 0.03), ('cvpr', 0.03), ('acl', 0.03), ('predict', 0.029), ('object', 0.029), ('detectors', 0.028), ('thresholds', 0.028), ('dogs', 0.028), ('palatucci', 0.028), ('automobile', 0.026), ('afrl', 0.026), ('conservative', 0.026), ('gaussian', 0.026), ('domain', 0.026), ('linguistics', 0.026), ('features', 0.026), ('car', 0.025), ('train', 0.025), ('ers', 0.025), ('art', 0.025), ('liberal', 0.025), ('nouns', 0.025), ('grounding', 0.025), ('distinguish', 0.024), ('language', 0.024), ('salakhutdinov', 0.024), ('embeddings', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="356-tfidf-1" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>2 0.25632194 <a title="356-tfidf-2" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>Author: Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, Tomas Mikolov</p><p>Abstract: Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model. 1</p><p>3 0.2428896 <a title="356-tfidf-3" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>Author: Marcus Rohrbach, Sandra Ebert, Bernt Schiele</p><p>Abstract: Category models for objects or activities typically rely on supervised learning requiring sufﬁciently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expertspeciﬁed information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More speciﬁcally we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classiﬁcation and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets. 1</p><p>4 0.19271715 <a title="356-tfidf-4" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: High capacity classiﬁers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classiﬁcation performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classiﬁer’s parameters. We show that the performance of deep neural networks can be improved by applying these priors to the weights in the last layer. Our method combines the strength of discriminatively trained deep neural networks, which typically require large amounts of training data, with tree-based priors, making deep neural networks work well on infrequent classes as well. We also propose an algorithm for learning the underlying tree structure. Starting from an initial pre-speciﬁed tree, this algorithm modiﬁes the tree to make it more pertinent to the task being solved, for example, removing semantic relationships in favour of visual ones for an image classiﬁcation task. Our method achieves state-of-the-art classiﬁcation results on the CIFAR-100 image data set and the MIR Flickr image-text data set. 1</p><p>5 0.18542719 <a title="356-tfidf-5" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>Author: Andriy Mnih, Koray Kavukcuoglu</p><p>Abstract: Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and ﬁnd that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones. 1</p><p>6 0.1836019 <a title="356-tfidf-6" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>7 0.16079763 <a title="356-tfidf-7" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>8 0.1494313 <a title="356-tfidf-8" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>9 0.13896562 <a title="356-tfidf-9" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>10 0.10171566 <a title="356-tfidf-10" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>11 0.09962976 <a title="356-tfidf-11" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>12 0.09781982 <a title="356-tfidf-12" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>13 0.092353679 <a title="356-tfidf-13" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>14 0.091922142 <a title="356-tfidf-14" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>15 0.091064282 <a title="356-tfidf-15" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>16 0.090640977 <a title="356-tfidf-16" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>17 0.090638302 <a title="356-tfidf-17" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>18 0.090258293 <a title="356-tfidf-18" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>19 0.088709764 <a title="356-tfidf-19" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>20 0.087695912 <a title="356-tfidf-20" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.108), (2, -0.185), (3, -0.116), (4, 0.204), (5, -0.157), (6, -0.043), (7, 0.021), (8, -0.081), (9, 0.061), (10, -0.168), (11, 0.023), (12, -0.051), (13, -0.025), (14, -0.035), (15, -0.077), (16, 0.139), (17, 0.013), (18, 0.01), (19, 0.158), (20, -0.014), (21, -0.032), (22, -0.023), (23, -0.024), (24, 0.008), (25, 0.189), (26, 0.044), (27, 0.006), (28, 0.007), (29, 0.039), (30, -0.066), (31, -0.071), (32, -0.022), (33, 0.085), (34, -0.022), (35, 0.035), (36, 0.063), (37, -0.004), (38, -0.057), (39, 0.069), (40, 0.012), (41, 0.023), (42, 0.027), (43, 0.083), (44, 0.036), (45, 0.026), (46, 0.076), (47, 0.031), (48, -0.001), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9774555 <a title="356-lsi-1" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>2 0.89257246 <a title="356-lsi-2" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>Author: Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, Tomas Mikolov</p><p>Abstract: Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model. 1</p><p>3 0.84780973 <a title="356-lsi-3" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>Author: Marcus Rohrbach, Sandra Ebert, Bernt Schiele</p><p>Abstract: Category models for objects or activities typically rely on supervised learning requiring sufﬁciently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expertspeciﬁed information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More speciﬁcally we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classiﬁcation and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets. 1</p><p>4 0.72299379 <a title="356-lsi-4" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>5 0.70549935 <a title="356-lsi-5" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>Author: Andriy Mnih, Koray Kavukcuoglu</p><p>Abstract: Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and ﬁnd that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones. 1</p><p>6 0.69955647 <a title="356-lsi-6" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>7 0.63323408 <a title="356-lsi-7" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>8 0.63061297 <a title="356-lsi-8" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>9 0.62218183 <a title="356-lsi-9" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>10 0.61885029 <a title="356-lsi-10" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>11 0.61396748 <a title="356-lsi-11" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>12 0.61216331 <a title="356-lsi-12" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>13 0.57909912 <a title="356-lsi-13" href="./nips-2013-A_Novel_Two-Step_Method_for_Cross_Language_Representation_Learning.html">12 nips-2013-A Novel Two-Step Method for Cross Language Representation Learning</a></p>
<p>14 0.57057959 <a title="356-lsi-14" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>15 0.56651515 <a title="356-lsi-15" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>16 0.55977589 <a title="356-lsi-16" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>17 0.55320138 <a title="356-lsi-17" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>18 0.54849839 <a title="356-lsi-18" href="./nips-2013-Translating_Embeddings_for_Modeling_Multi-relational_Data.html">336 nips-2013-Translating Embeddings for Modeling Multi-relational Data</a></p>
<p>19 0.54686749 <a title="356-lsi-19" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>20 0.54335171 <a title="356-lsi-20" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.031), (33, 0.255), (34, 0.077), (41, 0.022), (49, 0.033), (56, 0.056), (70, 0.048), (85, 0.04), (89, 0.034), (93, 0.085), (95, 0.01), (98, 0.209)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86727124 <a title="356-lda-1" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>2 0.81561512 <a title="356-lda-2" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>Author: James L. Sharpnack, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: The detection of anomalous activity in graphs is a statistical problem that arises in many applications, such as network surveillance, disease outbreak detection, and activity monitoring in social networks. Beyond its wide applicability, graph structured anomaly detection serves as a case study in the difﬁculty of balancing computational complexity with statistical power. In this work, we develop from ﬁrst principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in Gaussian noise. Because this test is computationally infeasible, we provide a relaxation, called the Lov´ sz extended scan statistic (LESS) that uses submodularity to approximate the a intractable generalized likelihood ratio. We demonstrate a connection between LESS and maximum a-posteriori inference in Markov random ﬁelds, which provides us with a poly-time algorithm for LESS. Using electrical network theory, we are able to control type 1 error for LESS and prove conditions under which LESS is risk consistent. Finally, we consider speciﬁc graph models, the torus, knearest neighbor graphs, and ǫ-random graphs. We show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds. 1</p><p>3 0.80011314 <a title="356-lda-3" href="./nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</a></p>
<p>Author: Yu Zhang</p><p>Abstract: All the existing multi-task local learning methods are deﬁned on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multitask classiﬁcation and regression problems based on heterogeneous neighborhood which is deﬁned on data points from all tasks. Speciﬁcally, we extend the knearest-neighbor classiﬁer by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-speciﬁc. By deﬁning a regularizer to enforce the task-speciﬁc weight matrix to approach a symmetric one, a regularized objective function is proposed and an efﬁcient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classiﬁcation case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods. 1</p><p>4 0.79353124 <a title="356-lda-4" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>Author: Michiel Hermans, Benjamin Schrauwen</p><p>Abstract: Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difﬁcult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales. 1</p><p>5 0.78951234 <a title="356-lda-5" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>6 0.78767133 <a title="356-lda-6" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>7 0.78385651 <a title="356-lda-7" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>8 0.78249794 <a title="356-lda-8" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>9 0.78130633 <a title="356-lda-9" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>10 0.78035718 <a title="356-lda-10" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>11 0.77988851 <a title="356-lda-11" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>12 0.77742201 <a title="356-lda-12" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>13 0.77713317 <a title="356-lda-13" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>14 0.77708489 <a title="356-lda-14" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>15 0.77708024 <a title="356-lda-15" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>16 0.77534831 <a title="356-lda-16" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>17 0.77514982 <a title="356-lda-17" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>18 0.77411288 <a title="356-lda-18" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>19 0.77382338 <a title="356-lda-19" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>20 0.77359205 <a title="356-lda-20" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
