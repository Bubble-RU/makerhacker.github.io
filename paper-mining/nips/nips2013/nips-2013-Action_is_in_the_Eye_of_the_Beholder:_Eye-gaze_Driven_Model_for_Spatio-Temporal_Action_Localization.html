<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-22" href="#">nips2013-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</h1>
<br/><p>Source: <a title="nips-2013-22-pdf" href="http://papers.nips.cc/paper/5197-action-is-in-the-eye-of-the-beholder-eye-gaze-driven-model-for-spatio-temporal-action-localization.pdf">pdf</a></p><p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>Reference: <a title="nips-2013-22-reference" href="../nips2013_reference/nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. [sent-6, score-0.378]
</p><p>2 Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. [sent-8, score-1.199]
</p><p>3 This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. [sent-9, score-0.391]
</p><p>4 In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. [sent-11, score-0.458]
</p><p>5 1  Introduction  Structured prediction models for action recognition and localization are emerging as prominent alternatives to more traditional holistic bag-of-words (BoW) representations. [sent-12, score-0.644]
</p><p>6 More recently, [9] and [18, 19] propose ﬁgure-centric approaches that can track an actor by searching over the space of spatio-temporal paths in video [19] and by incorporating person detection into the formulation [9]. [sent-15, score-0.397]
</p><p>7 However, all successful localization methods, to date, require spatial annotations in the form of partial poses [13], bounding boxes [9, 19] or pixel level segmentations [7] for learning. [sent-16, score-0.516]
</p><p>8 , grass may be highly discriminative for “kicking” action because in the training set most instances come from soccer, but clearly “kicking” can occur on nearly any surface). [sent-21, score-0.395]
</p><p>9 It has been shown that traditional BoW models computed over the salient regions of the video result in superior performance, compared to dense sampling of descriptors. [sent-23, score-0.414]
</p><p>10 ) which often fall outside the region of the action (and can confuse classiﬁers). [sent-28, score-0.461]
</p><p>11 To avoid reliance on spatial annotation of actors we utilize human gaze data (collected by having observers view corresponding videos [11]) as weak supervision in learning1 . [sent-30, score-0.959]
</p><p>12 By design, gaze gives perceptually semantic interest regions, however, while semantic, gaze, much like bottom-up saliency, is not necessarily discriminative. [sent-32, score-0.712]
</p><p>13 During learning, this objective is encouraged in the latent Structural SVM [26] formulation through a real-valued loss that penalizes misclassiﬁcation and, for correctly classiﬁed instances, misalignment with salient regions induced by the gaze. [sent-36, score-0.452]
</p><p>14 In addition to classiﬁcation and localization, we show that our model can provide top-down actionspeciﬁc saliency by predicting distribution over gaze conditioned on the action label and inferred spatio-temporal path. [sent-37, score-1.217]
</p><p>15 Having less (annotation) information available at training time, our model shows state-of-the art classiﬁcation and localization accuracy on the UCF-Sports dataset and is the ﬁrst, to our knowledge, to propose top-down saliency for action classiﬁcation task. [sent-38, score-0.798]
</p><p>16 The most prominent action recognition models to date utilize visual BoW representations [10, 22] and extensions [8, 15]. [sent-41, score-0.395]
</p><p>17 Recent work in action recognition [11, 21] look at bottom-up saliency as a way to sparsify descriptors and to bias BoW representations towards more salient portions of the video. [sent-44, score-0.704]
</p><p>18 In [11] and [21] multiple subjects were tasked with viewing videos while their gaze was recorded. [sent-45, score-0.741]
</p><p>19 A saliency model is then trained to predict the gaze and is used to either prune or weight the descriptors. [sent-46, score-0.87]
</p><p>20 In contrast, our model is designed with spatio-temporal localization in mind and uses gaze data as weak supervision during learning. [sent-48, score-0.973]
</p><p>21 In [16] and [17] authors use “objectness” saliency operator and person detector as weak supervision respectively, however, in both cases the saliency is bottom-up and task independent. [sent-49, score-0.572]
</p><p>22 The top-down discriminative saliency, based on distribution of gaze, in our approach, allows our model to focus on perceptually salient regions that are also discriminative. [sent-50, score-0.394]
</p><p>23 Similar in spirit, in [5] gaze and action labels are simultaneously inferred in ego-centric action recognition setting. [sent-51, score-1.335]
</p><p>24 However, the assumption of single ﬁxed axis aligned volumetric representation is limiting and only applicable 1  We assume no gaze data is available for test videos. [sent-58, score-0.686]
</p><p>25 Term φ(x, h) captures information about context (all the video excluding regions deﬁned by latent variables h); terms ψ(x, hi ) capture information about latent regions. [sent-61, score-0.934]
</p><p>26 Inferred latent regions should be discriminative and match high density regions of eye gaze data. [sent-62, score-1.424]
</p><p>27 In (b) ground truth eye gaze density, computed from ﬁxations of multiple subjects, is overlaid over images from sequences of 3 different action classes (see Sect. [sent-63, score-1.104]
</p><p>28 We generalize Max-Path idea by allowing multiple smooth paths and context within a latent max-margin structured output learning. [sent-74, score-0.368]
</p><p>29 Similarly, [2] relies on person detection and distributed partial pose representation, in the form of poselets, to build a spatio-temporal graph for action recognition and localization. [sent-78, score-0.421]
</p><p>30 To this end, we propose a model that has the ability to localize temporally and spatially discriminative regions of the video and encode the context in which these regions occur. [sent-85, score-0.776]
</p><p>31 The output of the model indicates the absence or presence of a particular action in the video sequence while simultaneously extracting the most discriminative and perceptually salient spatio-temporal video regions. [sent-86, score-0.854]
</p><p>32 During the training phase, the selection of these regions is implicitly driven by eye gaze ﬁxations collected by a sample of viewers. [sent-87, score-0.957]
</p><p>33 As a consequence, our model is able to perform top-down video saliency detection conditioned on the performed action and localized action region. [sent-88, score-1.039]
</p><p>34 , hiK } and hik ∈ ∅∪{(lj , tj , rj , bj )Te s } j=T denotes the left, top, right and bottom coordinates of spatio-temporal paths of bounding boxes that are deﬁned from frame Ts up to Te . [sent-102, score-0.406]
</p><p>35 The latent variables h specify the spatio-temporal regions ∗ selected by our model. [sent-103, score-0.382]
</p><p>36 Whereas, the corresponding feature map of videos where the action of interest is present is being decomposed into two components: a) the latent regions and b) context regions. [sent-106, score-0.817]
</p><p>37 As a consequence, the scoring function is written: K T F (x, y = 1, h; w) = wT Ψ(x, y = 1, h) = w0 φ(x, h) +  T wk ψ(x, hk ) + b  (2)  k=1  where K is the number of latent regions of the action model and b is the bias term. [sent-107, score-0.744]
</p><p>38 T Latent regions potential wk ψ(x, hk ): This potential function measures the compatibility of latent spatio-temporal region hk with the action model. [sent-110, score-0.907]
</p><p>39 More speciﬁcally, ψ(x, hk ) returns the sum of normalized BoW histograms extracted from the bounding box deﬁned by the latent variable hk = (lj , tj , rj , bj )Te s at each corresponding frame. [sent-111, score-0.528]
</p><p>40 j=T T Context potential w0 φ(x, h): We deﬁne context as the entire video sequence excluding the latent regions; our aim is to capture any information that is not directly produced by the appearance and motion of the actor. [sent-112, score-0.532]
</p><p>41 The characteristics of the context are encoded in φ(x, h) as a sum of normalized BoW histograms at each frame of the video excluding the regions indicated by latent variables h. [sent-113, score-0.72]
</p><p>42 Many action recognition scoring functions recently proposed [9, 12, 16] include the response of a global BoW statistical representation of the entire video. [sent-114, score-0.462]
</p><p>43 First, the visual information that corresponds to the latent region of interest implicitly gets to be counted twice. [sent-116, score-0.404]
</p><p>44 2  Inference  Given the model parameters w and an unseen video x our goal is to infer the binary action label y ∗ as well as the location of latent regions h∗ (Eq. [sent-118, score-0.846]
</p><p>45 The search space over even a single spatio-temporal path (non-smooth) of variable size bounding boxes in a video sequence of width M , height N and length T is exponential: O(M N )2T . [sent-123, score-0.389]
</p><p>46 These constraints allows the inference of the optimal latent variables for a single region using dynamic programming, similarly to Max-Path algorithm proposed by Tran and Yuan [18]. [sent-126, score-0.375]
</p><p>47 Algorithm 1 summarizes the process of dynamic programming considering both the context and the latent region contributions. [sent-127, score-0.448]
</p><p>48 Incorporating temporal ordering constraints between the K latent regions leads to a polynomial time algorithm. [sent-136, score-0.417]
</p><p>49 More speciﬁcally, the optimal scoring function can be inferred by enumerating all potential end locations of each latent region and executing independently Algorithm 1 at each interval in O(M N T K ). [sent-137, score-0.464]
</p><p>50 In our experimental validation a model with 2 latent regions proved to be sufﬁciently expressive. [sent-139, score-0.382]
</p><p>51 3 Learning Framework Identifying the spatio-temporal regions of the video sequences that will enable our model to detect human action is a challenging optimization problem. [sent-140, score-0.698]
</p><p>52 In our training formulation we adopt the large-margin latent structured output learning [26], however we also introduce a loss function that weakly supervises the selection of latent variables based on human gaze information. [sent-144, score-1.172]
</p><p>53 We transform these measurements using kernel density estimation with Gaussian kernel (with bandwidth set to the visual angle span of 2◦ ) T 1 to a probability density function of gaze gi = {gi , . [sent-152, score-0.8]
</p><p>54 The loss function is deﬁned as follows: ∆(yi , gi , yi , hi ) = ˆ ˆ  K 1 ˆ 1 − K k=1 δ(gi , hik ) 1 1 − 2 (yi yi + 1) ˆ  if yi = yi = 1, ˆ otherwise. [sent-157, score-0.351]
</p><p>55 The parameter r ik regulates the minimum amount of eye gaze “mass” that should be enclosed by each bounding box. [sent-159, score-0.902]
</p><p>56 4  Gaze Prediction  Our model is based on the core assumption that a subset of perceptually salient regions of a video, encoded by the gaze map, share discriminative idiosyncrasies useful for human action classiﬁcation. [sent-161, score-1.391]
</p><p>57 Assuming our assumption holds in practice, we can use selected latent regions for prediction of top-down saliency within the latent region. [sent-164, score-0.838]
</p><p>58 We do so by regressing the amount of eye gaze (probability density map over gaze) on a ﬁxed grid, inside each bounding box of the latent regions, by conditioning on low level features that construct the feature map ψi and the action label. [sent-165, score-1.475]
</p><p>59 In this way the latent regions select consistent salient portions of videos using top-down knowledge about the action, and image content modulates the saliency prediction within that region. [sent-166, score-0.829]
</p><p>60 Given the training data gaze g and the corresponding inferred latent variables h, we learn a linear regression, per action class, that maps augmented feature representation of the extracted bounding boxes, of each latent region, to a coarse description of the corresponding gaze distribution. [sent-167, score-2.199]
</p><p>61 Similarly, the human gaze is summarized by a 16 dimension vector by accumulating gaze density at each cell over a 4 × 4 grid. [sent-169, score-1.37]
</p><p>62 For visualization, we further smooth the predictions to obtain a continuous and smooth gaze density over the latent regions. [sent-170, score-0.919]
</p><p>63 The dataset contains 150 videos extracted from broadcast television channels and includes 10 different action classes. [sent-174, score-0.362]
</p><p>64 The dataset includes annotation of action classes as well as bounding boxes around the person of interest (which we ignore for training but use to measure localization performance). [sent-175, score-0.819]
</p><p>65 We employ the eye gaze data made available by Mathe and Sminchisescu [11]. [sent-178, score-0.763]
</p><p>66 The eye gaze data are represented with a probability density function (Sect. [sent-180, score-0.787]
</p><p>67 3) modulates importance of gaze localization within the latent region. [sent-190, score-1.13]
</p><p>68 The Region and Region+Context models with two latent regions demonstrate superior performance compared to Raptis et al. [sent-234, score-0.382]
</p><p>69 Our model with 1 latent region performs slightly worse then model of Raptis et al. [sent-236, score-0.375]
</p><p>70 Further, we can clearly see that having 2 latent regions is beneﬁcial, and improves the classiﬁcation performance by roughly 4%. [sent-238, score-0.382]
</p><p>71 We perform action localization by following the evaluation procedure of [9, 19] and estimate how well inferred latent regions capture the human5 performing the action. [sent-241, score-1.006]
</p><p>72 Given a video, for each frame we compute the overlap score between the latent region and the ground truth bounding box of the human. [sent-242, score-0.726]
</p><p>73 The total localization score per video is computed as an average of T 1 the overlap scores of the frames: T j=1 O(bj , bj ). [sent-244, score-0.55]
</p><p>74 Note, since our latent regions may not span k gt the entire video, instead of dividing by the number of frames T , we divide by the total length of the inferred latent regions. [sent-245, score-0.667]
</p><p>75 To be consistent with the literature [9, 19], we calculate the localization score of each test video given its ground truth action label. [sent-246, score-0.801]
</p><p>76 It is clear that our model with Context achieves considerably better localization than without (Region) especially with two latent regions. [sent-248, score-0.465]
</p><p>77 This can be explained by the fact that in UCF-Sports background tends to be discriminative for classiﬁcation; hence without proper context a latent region is likely to drift to the background (which reduces localization score). [sent-249, score-0.828]
</p><p>78 Context in our model models the background and leaves the latent regions free to select perceptually salient regions of the video. [sent-250, score-0.716]
</p><p>79 [9] (despite [9] having person detections and actor annotations 5  Note that by deﬁnition the task of localizing a human is unnatural for our model since it captures perceptually salient ﬁxed sized discriminate regions for action classiﬁcation, not human localization. [sent-252, score-0.977]
</p><p>80 3 for visual comparison between annotated person regions and our inferred discriminative salient latent regions. [sent-254, score-0.66]
</p><p>81 31  Table 2: Average amount of gaze (left): Table shows fraction of ground truth gaze captured by the latent region(s) on test videos; context improves the performance. [sent-271, score-1.614]
</p><p>82 Figure 3: Localization and gaze prediction: First row: groundtruth gaze and person bounding box, second row: predicted gaze and extent of the latent region in the frame. [sent-299, score-2.458]
</p><p>83 Gaze localization and prediction: Since our model is driven by eye-gaze, we also measure how much gaze our latent regions can actually capture on the test set and whether we can predict eyegaze saliency maps for the inferred latent regions. [sent-307, score-1.79]
</p><p>84 Evaluation of the gaze localization is performed in a similar fashion to the evaluation of action localization described earlier. [sent-308, score-1.451]
</p><p>85 We estimate amount of gaze that falls into each bounding box of the latent region, and then average the gaze amount over the length of all the latent regions of the video. [sent-309, score-2.062]
</p><p>86 Thus, each video has a gaze localization score sg ∈ [0, 1]. [sent-310, score-1.103]
</p><p>87 Table 2 (left) summarizes average gaze localization for different variants of our model. [sent-311, score-0.9]
</p><p>88 Noteworthy, we are able to capture around 60% of gaze by latent regions when modeling context. [sent-312, score-1.045]
</p><p>89 Qualitative results of the gaze prediction are illustrated in Fig. [sent-315, score-0.664]
</p><p>90 We also evaluate performance of bottom-up gaze prediction [11] within inferred latent regions. [sent-318, score-0.925]
</p><p>91 Still, we can observe that for both approaches the full model (Region+Context) is more consistent with gaze prediction. [sent-320, score-0.643]
</p><p>92 6  Conclusion  We propose a novel weakly-supervised structured learning approach for recognition and spatiotemporal localization of actions in video. [sent-321, score-0.378]
</p><p>93 Instead we rely on gaze data for weak supervision, by incorporating it into our structured loss. [sent-324, score-0.735]
</p><p>94 Further, we show how our model can be used to predict top-down saliency in the form of gaze density maps. [sent-325, score-0.894]
</p><p>95 In the future, we plan to explore the beneﬁts of searching over region scale and focus on more complex spatio-temporal relationships between latent regions. [sent-326, score-0.375]
</p><p>96 Discriminative ﬁgure-centric models for joint action localization and recognition. [sent-383, score-0.551]
</p><p>97 Dynamic eye movement datasets and learnt saliency models for visual action recognition. [sent-392, score-0.67]
</p><p>98 Similarity constrained latent support vector machine: An application to weakly supervised action classiﬁcation. [sent-422, score-0.502]
</p><p>99 Space-variant descriptor sampling for action recognition based on saliency and eye movements. [sent-450, score-0.692]
</p><p>100 Hidden part models for human action recognition: Probabilistic vs. [sent-464, score-0.354]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gaze', 0.643), ('action', 0.294), ('localization', 0.257), ('saliency', 0.227), ('latent', 0.208), ('regions', 0.174), ('video', 0.17), ('region', 0.167), ('bow', 0.155), ('eye', 0.12), ('bounding', 0.109), ('actor', 0.089), ('raptis', 0.082), ('discriminative', 0.081), ('gi', 0.08), ('localize', 0.078), ('box', 0.077), ('context', 0.073), ('hik', 0.072), ('salient', 0.07), ('bj', 0.07), ('perceptually', 0.069), ('videos', 0.068), ('frame', 0.065), ('annotations', 0.064), ('human', 0.06), ('boxes', 0.058), ('shapovalova', 0.055), ('inferred', 0.053), ('tran', 0.053), ('hi', 0.051), ('recognition', 0.051), ('lan', 0.047), ('person', 0.045), ('supervision', 0.044), ('portions', 0.039), ('yuan', 0.039), ('yi', 0.037), ('actions', 0.037), ('diving', 0.036), ('textured', 0.036), ('annotation', 0.036), ('scoring', 0.036), ('response', 0.036), ('cvpr', 0.036), ('temporal', 0.035), ('classi', 0.035), ('baselines', 0.035), ('svm', 0.033), ('structured', 0.033), ('score', 0.033), ('hk', 0.032), ('paths', 0.032), ('detection', 0.031), ('te', 0.031), ('search', 0.031), ('motion', 0.031), ('excluding', 0.03), ('eccv', 0.03), ('subjects', 0.03), ('incorporating', 0.03), ('localizing', 0.03), ('ik', 0.03), ('visual', 0.029), ('weak', 0.029), ('spatial', 0.028), ('actors', 0.027), ('axcp', 0.027), ('cannons', 0.027), ('orward', 0.027), ('temporally', 0.026), ('discriminant', 0.025), ('truth', 0.025), ('frames', 0.024), ('mathe', 0.024), ('kicking', 0.024), ('observers', 0.024), ('density', 0.024), ('global', 0.024), ('camera', 0.024), ('localized', 0.023), ('descriptors', 0.023), ('ground', 0.022), ('modulates', 0.022), ('riding', 0.022), ('ath', 0.022), ('unnatural', 0.022), ('yx', 0.022), ('aligned', 0.022), ('smooth', 0.022), ('prominent', 0.021), ('prediction', 0.021), ('xations', 0.021), ('representation', 0.021), ('path', 0.021), ('background', 0.021), ('pami', 0.02), ('overlap', 0.02), ('training', 0.02), ('capture', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="22-tfidf-1" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>2 0.29438442 <a title="22-tfidf-2" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>3 0.15233055 <a title="22-tfidf-3" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>4 0.12910487 <a title="22-tfidf-4" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>5 0.11346996 <a title="22-tfidf-5" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>Author: Özlem Aslan, Hao Cheng, Xinhua Zhang, Dale Schuurmans</p><p>Abstract: Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction. Unfortunately, such models are difﬁcult to train because inference over latent variables must be performed concurrently with parameter optimization—creating a highly non-convex problem. Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training. Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer. The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics. 1</p><p>6 0.11288264 <a title="22-tfidf-6" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>7 0.10837596 <a title="22-tfidf-7" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>8 0.10339033 <a title="22-tfidf-8" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>9 0.099165514 <a title="22-tfidf-9" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>10 0.096736364 <a title="22-tfidf-10" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>11 0.088986903 <a title="22-tfidf-11" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>12 0.083597496 <a title="22-tfidf-12" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>13 0.081486382 <a title="22-tfidf-13" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>14 0.07947515 <a title="22-tfidf-14" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>15 0.077224426 <a title="22-tfidf-15" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>16 0.073565178 <a title="22-tfidf-16" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>17 0.072379105 <a title="22-tfidf-17" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>18 0.069444537 <a title="22-tfidf-18" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>19 0.068616956 <a title="22-tfidf-19" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>20 0.067151017 <a title="22-tfidf-20" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.009), (2, -0.114), (3, -0.051), (4, 0.065), (5, -0.095), (6, 0.019), (7, 0.001), (8, -0.019), (9, 0.069), (10, -0.126), (11, 0.004), (12, 0.011), (13, -0.063), (14, -0.068), (15, 0.068), (16, -0.008), (17, -0.099), (18, 0.033), (19, -0.15), (20, -0.054), (21, 0.137), (22, 0.045), (23, 0.017), (24, 0.026), (25, -0.122), (26, -0.011), (27, -0.079), (28, -0.091), (29, 0.033), (30, -0.023), (31, -0.206), (32, 0.017), (33, -0.035), (34, 0.103), (35, -0.053), (36, -0.23), (37, 0.074), (38, 0.016), (39, 0.017), (40, 0.133), (41, 0.037), (42, 0.146), (43, -0.141), (44, -0.045), (45, -0.064), (46, -0.036), (47, -0.049), (48, -0.04), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96046805 <a title="22-lsi-1" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>2 0.77979231 <a title="22-lsi-2" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>3 0.65463185 <a title="22-lsi-3" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>4 0.56355757 <a title="22-lsi-4" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>Author: Tuan A. Nguyen, Subbarao Kambhampati, Minh Do</p><p>Abstract: Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we ﬁrst introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of ﬁnding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner. 1</p><p>5 0.49285579 <a title="22-lsi-5" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>6 0.45684659 <a title="22-lsi-6" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>7 0.45322993 <a title="22-lsi-7" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>8 0.43972993 <a title="22-lsi-8" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>9 0.43886945 <a title="22-lsi-9" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>10 0.43751225 <a title="22-lsi-10" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>11 0.43207452 <a title="22-lsi-11" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>12 0.43168512 <a title="22-lsi-12" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>13 0.41761878 <a title="22-lsi-13" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>14 0.40599072 <a title="22-lsi-14" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>15 0.38981509 <a title="22-lsi-15" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>16 0.37588683 <a title="22-lsi-16" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>17 0.37262982 <a title="22-lsi-17" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>18 0.36755487 <a title="22-lsi-18" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>19 0.36272928 <a title="22-lsi-19" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>20 0.35772035 <a title="22-lsi-20" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.027), (33, 0.144), (34, 0.096), (41, 0.038), (49, 0.08), (56, 0.089), (70, 0.098), (75, 0.042), (79, 0.125), (85, 0.024), (89, 0.032), (93, 0.088), (96, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89593107 <a title="22-lda-1" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>2 0.85118353 <a title="22-lda-2" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>3 0.83612221 <a title="22-lda-3" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>4 0.83562762 <a title="22-lda-4" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>5 0.833924 <a title="22-lda-5" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>6 0.82265306 <a title="22-lda-6" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>7 0.82230049 <a title="22-lda-7" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>8 0.81449378 <a title="22-lda-8" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>9 0.81442261 <a title="22-lda-9" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>10 0.81131762 <a title="22-lda-10" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>11 0.80945528 <a title="22-lda-11" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>12 0.80907094 <a title="22-lda-12" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>13 0.80737221 <a title="22-lda-13" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>14 0.80710709 <a title="22-lda-14" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>15 0.80664349 <a title="22-lda-15" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>16 0.8044008 <a title="22-lda-16" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>17 0.80423814 <a title="22-lda-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.8035894 <a title="22-lda-18" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>19 0.80343598 <a title="22-lda-19" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>20 0.80308646 <a title="22-lda-20" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
