<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-310" href="#">nips2013-310</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</h1>
<br/><p>Source: <a title="nips-2013-310-pdf" href="http://papers.nips.cc/paper/5079-statistical-analysis-of-coupled-time-series-with-kernel-cross-spectral-density-operators.pdf">pdf</a></p><p>Author: Michel Besserve, Nikos K. Logothetis, Bernhard Schölkopf</p><p>Abstract: Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive deﬁnite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. This similarity measure enables us to identify different types of interactions in electrophysiological neural time series. 1</p><p>Reference: <a title="nips-2013-310-reference" href="../nips2013_reference/nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators._reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators. [sent-1, score-0.204]
</p><p>2 This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. [sent-15, score-0.272]
</p><p>3 assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. [sent-19, score-0.157]
</p><p>4 1  Introduction  Complex dynamical systems can often be observed by monitoring time series of one or more variables. [sent-21, score-0.244]
</p><p>5 Finding and characterizing dependencies between several of these time series is key to understand the underlying mechanisms of these systems. [sent-22, score-0.298]
</p><p>6 Especially, studying the relationships between time series of arbitrary objects such as texts or graphs within a general framework is largely unaddressed. [sent-25, score-0.204]
</p><p>7 It has been shown that when using a characteristic kernel for the mapping [9], the properties of RKHS operators are related to statistical independence between input variables and allow testing for it in a principled way with the Hilbert-Schmidt Independence Criterion (HSIC) test [11]. [sent-31, score-0.552]
</p><p>8 It was recently suggested that generalizing the concept of cross-spectral density to Reproducible Kernel Hilbert Spaces (RKHS) could help formulate nonlinear dependency measures for time series [2]. [sent-41, score-0.304]
</p><p>9 In this paper, after recalling the concept of kernel spectral density operator, we characterize its statistical properties. [sent-43, score-0.229]
</p><p>10 In particular, we deﬁne independence tests based on this concept as well as a similarity measure to compare different types of couplings. [sent-44, score-0.229]
</p><p>11 We use these tests in section 4 to compute the statistical dependencies between simulated time series of various types of objects, as well as recordings of neural activity in the visual cortex of non-human primates. [sent-45, score-0.449]
</p><p>12 We show that our technique reliably detects complex interactions and provides a characterization of these interactions in the frequency domain. [sent-46, score-0.208]
</p><p>13 As a consequence, the cross-covariance can be seen as an operator in L(Hj , Hi ), the Hilbert space of linear Hilbert-Schmidt operators from Hj to Hi (isomorphic to ∗ Hi ⊗ Hj ). [sent-58, score-0.164]
</p><p>14 If the time series are assumed to be k-order Markovian, then results of the classical HSIC can be generalized for a structured HSIC using universal kernels based on the state vectors (x1 (t), . [sent-61, score-0.25]
</p><p>15 Kernel Cross-Spectral Density operator Consider a bivariate discrete time random process on X1 × X2 : {(X1 (t), X2 (t))}t∈Z . [sent-70, score-0.192]
</p><p>16 Under mild assumptions, it is a Hilbert-Schmidt operator deﬁned for all normalized frequencies ν ∈ [0 ; 1] as: S12 (ν) =  C12 (k)z −k , for z = e2πiν . [sent-72, score-0.152]
</p><p>17 3  Statistical properties of KCSD  Measuring independence with the KCSD One interesting characteristic of the KCSD is given by the following theorem [2]: Theorem 1. [sent-75, score-0.258]
</p><p>18 While this theorem states that KCSD can be used to test pairwise independence between time series, it does not imply independence between arbitrary sets of random variables taken from each time series in general. [sent-80, score-0.705]
</p><p>19 However, if the joint probability distribution of the time series is encoded by a Directed Acyclic Graph (DAG), the following Theorem shows that independence in this broader sense is achieved under mild assumptions. [sent-81, score-0.377]
</p><p>20 If the joint probability distribution of time series is encoded by a DAG with no confounder under the Markov property and faithfulness assumption, pairwise independence between time series implies the mutual independence relationship {X1 (t)}t∈Z ⊥ {X2 (t)}t∈Z . [sent-83, score-0.843]
</p><p>21 The proof uses the fact that the faithfulness and Markov property assumptions provide an equivalence between the independence of two sets of random variables and the d-separation of the corresponding sets of nodes in the DAG (see [17]). [sent-85, score-0.261]
</p><p>22 We start by assuming pairwise independence between the time series. [sent-86, score-0.245]
</p><p>23 Since this holds for all t and t , there is no path linking the nodes of each time series and we have {X1 (t)}t∈Z ⊥ {X2 (t)}t∈Z according to the Markov property (any joint probability distribution on ⊥ the nodes will factorize in two terms, one for each time series). [sent-91, score-0.278]
</p><p>24 As a consequence, the use of KCSD to test for independence is justiﬁed under the widely used faithfulness and Markov assumptions of graphical models. [sent-92, score-0.316]
</p><p>25 As a comparison, the structured HSIC proposed in [22] is theoretically able to capture all dependencies within the range of k samples by assuming k-order Markovian time series. [sent-93, score-0.164]
</p><p>26 Fourth order kernel cumulant operator Statistical properties of KCSD require assumptions regarding the higher order statistics of the time series. [sent-94, score-0.395]
</p><p>27 The quadricu∗ ∗ mulant operator K1234 is a linear operator in the Hilbert space L(H1 ⊗ H2 , H1 ⊗ H2 ), such that ∗ ∗ κ(f1 (X1 ),f2 (X2 ),f3 (X3 ),f4 (X4 )) = f1 ⊗f2 ,K1234 f3 ⊗f4 , for arbitrary elements fi . [sent-98, score-0.208]
</p><p>28 Properties of the windowed Fourier transform are related to the regularity of the tapering window. [sent-105, score-0.17]
</p><p>29 [A property of bounded variation functions] Let w be a bounded function of bounded +∞ +∞ 2 variation then for all k, ≤ C|k| t=−∞ wT (t + k)w(t) − t=−∞ wT (t) Using this assumption, the above periodogram estimate is asymptotically unbiased as shown in the following theorem Theorem 5. [sent-109, score-0.258]
</p><p>30 However, the squared Hilbert-Schmidt norm of PT (ν) is an asymptotically biased estimator of the 12 population KCSD squared norm according to the following theorem. [sent-113, score-0.321]
</p><p>31 This estimate requires speciﬁc bias estimation techniques to develop an independence test, we will call it the biased estimate of the KCSD squared norm. [sent-116, score-0.383]
</p><p>32 Assume assumptions of Theorem 5 hold for two independent samples of bivariate time series{(X1 (t), X2 (t))}t=. [sent-120, score-0.156]
</p><p>33 Interestingly, this estimate of the scalar product between KCSD operators is unbiased. [sent-134, score-0.148]
</p><p>34 This comes from the assumption that the two bivariate series are independent. [sent-135, score-0.219]
</p><p>35 This provides a new opportunity to estimate the Hilbert-Schmidt norm as well, in case two independent samples of the same bivariate series are available. [sent-136, score-0.344]
</p><p>36 Assume assumptions of Theorem 5 hold for the bivariate time series ˜ ˜ {(X1 (t), X2 (t))}t∈Z and assume {(X1 (t), X2 (t))}t∈Z an independent copy of the same time series, T ˜ providing the periodogram estimates P12 (ν) and PT (ν), respectively. [sent-138, score-0.428]
</p><p>37 12 Then  ˜ lim E PT (ν), PT (ν) 12 12  T →+∞  HS  = S12 (ν)  2 , HS  ν≡0  (mod 1/2)  In many experimental settings, such as in neuroscience, it is possible to measure the same time series in several independent trials. [sent-139, score-0.232]
</p><p>38 We will call this estimate the unbiased estimate of the KCSD squared norm. [sent-141, score-0.209]
</p><p>39 These estimate can be computed efﬁciently for T equispaced frequency samples using the fast Fourier transform of the centered kernel matrices of the two time series. [sent-142, score-0.453]
</p><p>40 In general, the choice of the kernel is a trade-off between the capacity to capture complex dependencies (a characteristic kernel being better in this respect), and the convergence rate of the estimate (simpler kernels related to lower order statistics usually require less samples). [sent-143, score-0.613]
</p><p>41 Unless otherwise stated, the Gaussian RBF kernel with bandwidth 2 parameter σ, k(x, y) = exp( x − y /2σ 2 ), will be used as a characteristic kernel for vector spaces. [sent-145, score-0.467]
</p><p>42 The biased and unbiased squared norm estimates can be trivially retrieved from the above expression. [sent-151, score-0.317]
</p><p>43 Shufﬂing independence tests According to Theorem 1, pairwise independence between time series requires the cross-spectral density operator to be zero for all frequencies. [sent-152, score-0.76]
</p><p>44 We can thus test independence by testing whether the Hilbert-Schmidt norm of the operator vanishes for each frequency. [sent-153, score-0.384]
</p><p>45 We rely on Theorem 6 and Corollary 8 to compute biased and unbiased estimates of this norm. [sent-154, score-0.209]
</p><p>46 To achieve this, we generate a distribution of the Hilbert-Schmidt norm statistics under the null hypothesis by cutting the time interval in non-overlapping blocks and matching the blocks of each time series in pairs at random. [sent-155, score-0.389]
</p><p>47 5  0  5  number of dependencies detected (%)  number of dependencies detected (%)  Detection probability for biased kcsd 1  linear rbf σ=. [sent-166, score-1.148]
</p><p>48 2  0  2  10  number of samples  3  10  10  15  20  frequency (Hz)  25  30  Detection probability for unbiased kcsd 1  −0. [sent-175, score-0.884]
</p><p>49 8  0  0  100 error rate (%)  −2 −4  biased unbiased  0. [sent-180, score-0.209]
</p><p>50 4) type II (C=2)  60 40 20  number of samples  3  10  0  block hsic  hsic  linear kcsd  kcsd  Figure 1: Results for the phase-amplitude coupling system. [sent-184, score-1.946]
</p><p>51 Bottom-left: performance of the biased kcsd test as a function of number of samples. [sent-188, score-0.84]
</p><p>52 Bottom-middle: performance of the unbiased kcsd test as a function of number of samples. [sent-189, score-0.839]
</p><p>53 Bottom-right: Rate of type I and type II errors for several independence tests. [sent-190, score-0.269]
</p><p>54 4  Experiments  In the following, we validate the performance of our test, called kcsd, on several datasets in the biased and unbiased case. [sent-191, score-0.209]
</p><p>55 There is no general time series analysis tool in the literature to compare with our approach on all these datasets. [sent-192, score-0.204]
</p><p>56 So our main source of comparison will be the HSIC test of independence (assuming data is i. [sent-193, score-0.228]
</p><p>57 For vector data, one can compare the performance of our approach with a linear dependency measure: we do this by implementing our test using a linear kernel (instead of an RBF kernel), and we call it linear kscd. [sent-198, score-0.31]
</p><p>58 Finally, we use the alternative approach of structured HSIC [22] by cutting the time series in time windows (using the same approach as our independence test) and considering each of them as a single multivariate sample. [sent-199, score-0.464]
</p><p>59 The p-value for all independence tests will be set to 5%. [sent-202, score-0.229]
</p><p>60 Phase amplitude coupling We ﬁrst simulate a non-linear dependency between two time series by generating two oscillations at frequencies f1 and f2 , and introducing a modulation of the amplitude of the second oscillation by the phase of the ﬁrst one. [sent-203, score-0.402]
</p><p>61 We used a Gaussian RBF kernel to compute nonlinear dependencies between the two time series after standardizing each of them (divide them by their standard deviation). [sent-212, score-0.49]
</p><p>62 1) for a linear and a Gaussian RBF kernel (with σ = 1) respectively. [sent-214, score-0.192]
</p><p>63 The bias of the ﬁrst estimate appears clearly in both cases at the two power picks of the signals for the biased estimate. [sent-215, score-0.169]
</p><p>64 The observed negative values are also a direct consequence of the unbiased property of our estimate (Corollary 8). [sent-217, score-0.165]
</p><p>65 The inﬂuence of the bandwidth parameter of the kernel was studied in the case of weakly coupled time series (C = . [sent-218, score-0.422]
</p><p>66 5  state 3 state 2 state 1  type I error type II error  80 60 40 20 0  block hsic  hsic  kcsd  Figure 2: Markov chain dynamical system. [sent-243, score-1.256]
</p><p>67 Bottom left: the biased and unbiased KCSD norm estimates in the frequency domain. [sent-246, score-0.35]
</p><p>68 Bottom right: type I and type II errors for hsic and kcsd tests the inﬂuence of this parameter on the number of samples required to actually reject the null hypothesis and detect the dependency for biased and unbiased estimates respectively. [sent-247, score-1.424]
</p><p>69 5) was an optimal strategy, and that the test relying on the unbiased estimate outperformed the biased estimate. [sent-249, score-0.296]
</p><p>70 The coupling parameter C was further varied to test the performance of independence tests both in case the null hypothesis of independence is true (C=0), and when it should be rejected (C = . [sent-251, score-0.603]
</p><p>71 The bottom-right panel of Figure 1 reports these errors for several independence tests. [sent-254, score-0.202]
</p><p>72 We generate a symbolic time series x2 using the alphabet S = [1, 2, 3], controlled by a scalar time series x1 . [sent-258, score-0.449]
</p><p>73 The coupling is achieved by modulating across time the transition probabilities of the Markov transition matrix generating the symbolic time series x2 using the current value of the scalar time series x1 . [sent-259, score-0.643]
</p><p>74 In order to measure the dependency between these two time series, we use a k-spectrum kernel [14] for x2 and a RBF kernel for x1 . [sent-265, score-0.491]
</p><p>75 counting occurrences of single symbols was less efﬁcient) and we computed the kernel between words of 3 successive symbols of the time series. [sent-268, score-0.236]
</p><p>76 We used an RBF kernel with σ = 1, decimated the signals by a factor 2 and signals were cut in time windows of 100 samples. [sent-269, score-0.343]
</p><p>77 The biased and unbiased estimates of the KCSD norm are represented at the bottom-left of Figure 2 and show a clear peak at the modulating frequency (1Hz). [sent-270, score-0.376]
</p><p>78 The independence test results shown at the bottom-right of Figure 2 illustrate again the superiority of KCSD for type II error, whereas type I error stays in an acceptable range. [sent-271, score-0.324]
</p><p>79 7  Figure 3: Left: Experimental setup of LFP recordings in anesthetized monkey during visual stimulation with a movie. [sent-272, score-0.146]
</p><p>80 Right: Proportion of detected dependencies for the unbiased kcsd test of interactions between Gamma band and wide band LFP for different kernels. [sent-273, score-1.236]
</p><p>81 Neural data: local ﬁeld potentials from monkey visual cortex We analyzed dependencies between local ﬁeld potential (LFP) time series recorded in the primary visual cortex of one anesthetized monkey during visual stimulation by a commercial movie (see Figure 3 for a scheme of the experiment). [sent-274, score-0.617]
</p><p>82 Here we investigate this interplay by extracting LFP activity in two frequency bands within the same electrode and quantify the non-linear interactions between them with our approach. [sent-276, score-0.201]
</p><p>83 LFPs were ﬁltered into two frequency bands: 1/ a wide band ranging from 1 to 100Hz which contains a rich variety of rhythms and 2/ a high gamma band ranging from 60 to 100Hz which as been shown to play a role in the processing of visual information. [sent-277, score-0.366]
</p><p>84 Both of these time series were sampled at 1000Hz. [sent-278, score-0.204]
</p><p>85 Using non-overlapping time windows of 1s points, we computed the Hilbert-Schmidt norm of the KCSD operator between gamma and large band time series originating from the same electrode. [sent-279, score-0.603]
</p><p>86 We observe a highly reliable detection of interactions in the gamma band, using either a linear or non-linear kernel. [sent-282, score-0.149]
</p><p>87 This is due to the fact that the Gamma band LFP is a ﬁltered version of the wide band LFP, making these signals highly correlated in the Gamma band. [sent-283, score-0.234]
</p><p>88 This characteristic illustrates the non-linear interaction between the high frequency gamma rhythm and other lower frequencies of the brain electrical activity, which has been reported in other studies [21]. [sent-286, score-0.249]
</p><p>89 This also shows the interpretability of our approach as a test of non-linear dependency in the frequency domain. [sent-287, score-0.192]
</p><p>90 5  Conclusion  An independence test for time series based on the concept of Kernel Cross Spectral Density estimation was introduced in this paper. [sent-288, score-0.432]
</p><p>91 First, it allows quantiﬁcation of non-linear interactions for time series living in vector spaces. [sent-290, score-0.271]
</p><p>92 Moreover, it can measure dependencies between more complex objects, including sequences in an arbitrary alphabet, or graphs, as long as an appropriate positive deﬁnite kernel can be deﬁned in the space of each time series. [sent-291, score-0.33]
</p><p>93 The space of KCSD operators constitutes a very general framework to analyze dependencies in multivariate and highly structured dynamical systems. [sent-293, score-0.209]
</p><p>94 Following [13, 18], our independence test can further be combined to recent developments in kernel time series prediction techniques [20] to deﬁne general and reliable multivariate causal inference techniques. [sent-294, score-0.662]
</p><p>95 Finding dependencies between frequencies o with the kernel cross-spectral density. [sent-307, score-0.349]
</p><p>96 Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. [sent-334, score-0.235]
</p><p>97 The spectrum kernel: a string kernel for SVM protein classiﬁcation. [sent-397, score-0.192]
</p><p>98 Causal inference on time series using structural equation models. [sent-420, score-0.204]
</p><p>99 Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality. [sent-434, score-0.192]
</p><p>100 Frequency-band coupling in surface EEG reﬂects spiking activity in monkey visual cortex. [sent-440, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kcsd', 0.68), ('xc', 0.22), ('hsic', 0.22), ('kernel', 0.192), ('independence', 0.173), ('series', 0.16), ('hs', 0.122), ('lfp', 0.117), ('pt', 0.11), ('rbf', 0.107), ('biased', 0.105), ('unbiased', 0.104), ('band', 0.101), ('periodogram', 0.094), ('dependencies', 0.094), ('operator', 0.089), ('wt', 0.086), ('fukumizu', 0.079), ('hilbert', 0.078), ('operators', 0.075), ('frequency', 0.074), ('coupling', 0.072), ('interactions', 0.067), ('norm', 0.067), ('windowed', 0.067), ('dependency', 0.063), ('frequencies', 0.063), ('faithfulness', 0.061), ('fourier', 0.06), ('bivariate', 0.059), ('quadricumulant', 0.057), ('tapering', 0.057), ('characteristic', 0.057), ('tests', 0.056), ('test', 0.055), ('gamma', 0.055), ('hi', 0.055), ('gretton', 0.055), ('rkhs', 0.052), ('monkey', 0.051), ('type', 0.048), ('sch', 0.048), ('mpi', 0.047), ('null', 0.046), ('kernels', 0.046), ('transform', 0.046), ('mod', 0.046), ('cij', 0.045), ('time', 0.044), ('reproducing', 0.043), ('cumulant', 0.043), ('windows', 0.043), ('dag', 0.042), ('squared', 0.041), ('janzing', 0.041), ('scalar', 0.041), ('tr', 0.04), ('hj', 0.04), ('dynamical', 0.04), ('centered', 0.039), ('causal', 0.038), ('besserve', 0.038), ('density', 0.037), ('mapped', 0.035), ('visual', 0.035), ('markov', 0.035), ('kij', 0.034), ('bingen', 0.034), ('detected', 0.034), ('activity', 0.034), ('logothetis', 0.033), ('lkopf', 0.033), ('signals', 0.032), ('estimate', 0.032), ('fj', 0.031), ('stimulation', 0.031), ('hz', 0.031), ('cov', 0.03), ('linking', 0.03), ('window', 0.03), ('fi', 0.03), ('panel', 0.029), ('anesthetized', 0.029), ('consequence', 0.029), ('hypothesis', 0.028), ('pairwise', 0.028), ('theorem', 0.028), ('lim', 0.028), ('suitability', 0.027), ('spaces', 0.027), ('detection', 0.027), ('assumptions', 0.027), ('ii', 0.026), ('transition', 0.026), ('bandwidth', 0.026), ('cortex', 0.026), ('samples', 0.026), ('modulating', 0.026), ('arrow', 0.026), ('bands', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="310-tfidf-1" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>Author: Michel Besserve, Nikos K. Logothetis, Bernhard Schölkopf</p><p>Abstract: Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive deﬁnite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. This similarity measure enables us to identify different types of interactions in electrophysiological neural time series. 1</p><p>2 0.20990098 <a title="310-tfidf-2" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>Author: Fabian Sinz, Anna Stockl, January Grewe, January Benda</p><p>Abstract: We present a novel non-parametric method for ﬁnding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest. 1</p><p>3 0.15720347 <a title="310-tfidf-3" href="./nips-2013-A_Kernel_Test_for_Three-Variable_Interactions.html">9 nips-2013-A Kernel Test for Three-Variable Interactions</a></p>
<p>Author: Dino Sejdinovic, Arthur Gretton, Wicher Bergsma</p><p>Abstract: We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak inﬂuence on a third dependent variable, but their combined effect has a strong inﬂuence. This makes the Lancaster test especially suited to ﬁnding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.</p><p>4 0.15571344 <a title="310-tfidf-4" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>Author: Wojciech Zaremba, Arthur Gretton, Matthew Blaschko</p><p>Abstract: A family of maximum mean discrepancy (MMD) kernel two-sample tests is introduced. Members of the test family are called Block-tests or B-tests, since the test statistic is an average over MMDs computed on subsets of the samples. The choice of block size allows control over the tradeoff between test power and computation time. In this respect, the B-test family combines favorable properties of previously proposed MMD two-sample tests: B-tests are more powerful than a linear time test where blocks are just pairs of samples, yet they are more computationally efﬁcient than a quadratic time test where a single large block incorporating all the samples is used to compute a U-statistic. A further important advantage of the B-tests is their asymptotically Normal null distribution: this is by contrast with the U-statistic, which is degenerate under the null hypothesis, and for which estimates of the null distribution are computationally demanding. Recent results on kernel selection for hypothesis testing transfer seamlessly to the B-tests, yielding a means to optimize test power via kernel choice. 1</p><p>5 0.14067881 <a title="310-tfidf-5" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>Author: Qichao Que, Mikhail Belkin</p><p>Abstract: q We address the problem of estimating the ratio p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration often referred to as importance sampling in statistical inference. It is also closely related to the problem of covariate shift in transfer learning. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, known as the Fredholm problem of the ﬁrst kind. This formulation, combined with the techniques of regularization leads to a principled framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is ﬂexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities deﬁned on Rd and smooth d-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difﬁcult problem. It turns out that in the density ratio estimation setting, when samples from both distributions are available, simple completely unsupervised model selection methods are available. We call this mechanism CD-CV for Cross-Density Cross-Validation. We show encouraging experimental results including applications to classiﬁcation within the covariate shift framework. 1</p><p>6 0.12649775 <a title="310-tfidf-6" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>7 0.11175736 <a title="310-tfidf-7" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>8 0.10165843 <a title="310-tfidf-8" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>9 0.088557638 <a title="310-tfidf-9" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>10 0.078496099 <a title="310-tfidf-10" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>11 0.078183368 <a title="310-tfidf-11" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>12 0.073410951 <a title="310-tfidf-12" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>13 0.068858713 <a title="310-tfidf-13" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>14 0.068249144 <a title="310-tfidf-14" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>15 0.067309134 <a title="310-tfidf-15" href="./nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</a></p>
<p>16 0.065723814 <a title="310-tfidf-16" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>17 0.062938422 <a title="310-tfidf-17" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>18 0.062780991 <a title="310-tfidf-18" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>19 0.061530244 <a title="310-tfidf-19" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>20 0.061255969 <a title="310-tfidf-20" href="./nips-2013-Adaptive_Market_Making_via_Online_Learning.html">26 nips-2013-Adaptive Market Making via Online Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.061), (2, 0.049), (3, 0.002), (4, -0.041), (5, 0.005), (6, 0.004), (7, 0.008), (8, -0.029), (9, 0.058), (10, -0.053), (11, -0.108), (12, -0.071), (13, -0.148), (14, 0.23), (15, 0.169), (16, 0.019), (17, 0.096), (18, -0.134), (19, -0.089), (20, 0.01), (21, -0.029), (22, 0.041), (23, -0.024), (24, -0.02), (25, 0.023), (26, 0.054), (27, 0.019), (28, 0.031), (29, 0.016), (30, 0.118), (31, 0.005), (32, 0.018), (33, -0.016), (34, -0.035), (35, -0.091), (36, 0.018), (37, 0.101), (38, 0.0), (39, 0.019), (40, -0.055), (41, 0.117), (42, -0.034), (43, 0.006), (44, -0.032), (45, -0.016), (46, 0.076), (47, -0.055), (48, -0.006), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95530689 <a title="310-lsi-1" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>Author: Michel Besserve, Nikos K. Logothetis, Bernhard Schölkopf</p><p>Abstract: Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive deﬁnite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. This similarity measure enables us to identify different types of interactions in electrophysiological neural time series. 1</p><p>2 0.85481083 <a title="310-lsi-2" href="./nips-2013-A_Kernel_Test_for_Three-Variable_Interactions.html">9 nips-2013-A Kernel Test for Three-Variable Interactions</a></p>
<p>Author: Dino Sejdinovic, Arthur Gretton, Wicher Bergsma</p><p>Abstract: We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak inﬂuence on a third dependent variable, but their combined effect has a strong inﬂuence. This makes the Lancaster test especially suited to ﬁnding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.</p><p>3 0.83157116 <a title="310-lsi-3" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>Author: Wojciech Zaremba, Arthur Gretton, Matthew Blaschko</p><p>Abstract: A family of maximum mean discrepancy (MMD) kernel two-sample tests is introduced. Members of the test family are called Block-tests or B-tests, since the test statistic is an average over MMDs computed on subsets of the samples. The choice of block size allows control over the tradeoff between test power and computation time. In this respect, the B-test family combines favorable properties of previously proposed MMD two-sample tests: B-tests are more powerful than a linear time test where blocks are just pairs of samples, yet they are more computationally efﬁcient than a quadratic time test where a single large block incorporating all the samples is used to compute a U-statistic. A further important advantage of the B-tests is their asymptotically Normal null distribution: this is by contrast with the U-statistic, which is degenerate under the null hypothesis, and for which estimates of the null distribution are computationally demanding. Recent results on kernel selection for hypothesis testing transfer seamlessly to the B-tests, yielding a means to optimize test power via kernel choice. 1</p><p>4 0.73364496 <a title="310-lsi-4" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>Author: Fabian Sinz, Anna Stockl, January Grewe, January Benda</p><p>Abstract: We present a novel non-parametric method for ﬁnding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest. 1</p><p>5 0.69147509 <a title="310-lsi-5" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>Author: Corinna Cortes, Marius Kloft, Mehryar Mohri</p><p>Abstract: We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby beneﬁt from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efﬁcient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classiﬁcation tasks. 1</p><p>6 0.69039744 <a title="310-lsi-6" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>7 0.68764734 <a title="310-lsi-7" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>8 0.63915581 <a title="310-lsi-8" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>9 0.55084777 <a title="310-lsi-9" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>10 0.53731751 <a title="310-lsi-10" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>11 0.49162087 <a title="310-lsi-11" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>12 0.42734805 <a title="310-lsi-12" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>13 0.41084707 <a title="310-lsi-13" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>14 0.40646029 <a title="310-lsi-14" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>15 0.39948291 <a title="310-lsi-15" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>16 0.38552877 <a title="310-lsi-16" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>17 0.37141931 <a title="310-lsi-17" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>18 0.37014207 <a title="310-lsi-18" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>19 0.35412207 <a title="310-lsi-19" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>20 0.34946755 <a title="310-lsi-20" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.176), (16, 0.033), (19, 0.034), (33, 0.17), (34, 0.114), (36, 0.016), (41, 0.062), (49, 0.039), (56, 0.1), (70, 0.051), (85, 0.025), (89, 0.063), (93, 0.019), (95, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9331162 <a title="310-lda-1" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>Author: Evan W. Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: Shannon’s entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difﬁcult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefﬁcient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to ﬂexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We deﬁne two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efﬁcient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.</p><p>2 0.92624295 <a title="310-lda-2" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>Author: Alfredo Kalaitzis, Ricardo Silva</p><p>Abstract: Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parameterization of joint distributions. Among other properties, copulas provide a recipe for combining ﬂexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efﬁcient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size. 1 Contribution There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6]. More recently, the idea of copula modeling [16] has been combined with such standard building blocks. Our contribution is a novel algorithm for efﬁcient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [17]. A copula is a continuous cumulative distribution function (CDF) with uniformly distributed univariate marginals in the unit interval [0, 1]. It complements graphical models and other formalisms that provide a modular parameterization of joint distributions. The core idea is simple and given by the following observation: suppose we are given a (say) bivariate CDF F (y1 , y2 ) with marginals −1 −1 F1 (y1 ) and F2 (y2 ). This CDF can then be rewritten as F (F1 (F1 (y1 )), F2 (F2 (y2 ))). The func−1 −1 tion C(·, ·) given by F (F1 (·), F2 (·)) is a copula. For discrete distributions, this decomposition is not unique but still well-deﬁned [16]. Copulas have found numerous applications in statistics and machine learning since they provide a way of constructing ﬂexible multivariate distributions by mix-and-matching different copulas with different univariate marginals. For instance, one can combine ﬂexible univariate marginals Fi (·) with useful but more constrained high-dimensional copulas. We will not further motivate the use of copula models, which has been discussed at length in recent 1 machine learning publications and conference workshops, and for which comprehensive textbooks exist [e.g., 9]. For a recent discussion on the applications of copulas from a machine learning perspective, [4] provides an overview. [10] is an early reference in machine learning. The core idea dates back at least to the 1950s [16]. In the discrete case, copulas can be difﬁcult to apply: transforming a copula CDF into a probability mass function (PMF) is computationally intractable in general. For the continuous case, a common ˆ trick goes as follows: transform variables by deﬁning ai ≡ Fi (yi ) for an estimate of Fi (·) and then ﬁt a copula density c(·, . . . , ·) to the resulting ai [e.g. 9]. It is not hard to check this breaks down in the discrete case [7]. An alternative is to represent the CDF to PMF transformation for each data point by a continuous integral on a bounded space. Sampling methods can then be used. This trick has allowed many applications of the Gaussian copula to discrete domains. Readers familiar with probit models will recognize the similarities to models where an underlying latent Gaussian ﬁeld is discretized into observable integers as in Gaussian process classiﬁers and ordinal regression [18]. Such models can be indirectly interpreted as special cases of the Gaussian copula. In what follows, we describe in Section 2 the Gaussian copula and the general framework for constructing Bayesian estimators of Gaussian copulas by [7], the extended rank likelihood framework. This framework entails computational issues which are discussed. A recent general approach for MCMC in constrained Gaussian ﬁelds by [17] can in principle be directly applied to this problem as a blackbox, but at a cost that scales quadratically in sample size and as such it is not practical in general. Our key contribution is given in Section 4. An application experiment on the Bayesian Gaussian copula factor model is performed in Section 5. Conclusions are discussed in the ﬁnal section. 2 Gaussian copulas and the extended rank likelihood It is not hard to see that any multivariate Gaussian copula is fully deﬁned by a correlation matrix C, since marginal distributions have no free parameters. In practice, the following equivalent generative model is used to deﬁne a sample U according to a Gaussian copula GC(C): 1. Sample Z from a zero mean Gaussian with covariance matrix C 2. For each Zj , set Uj = Φ(zj ), where Φ(·) is the CDF of the standard Gaussian It is clear that each Uj follows a uniform distribution in [0, 1]. To obtain a model for variables {y1 , y2 , . . . , yp } with marginal distributions Fj (·) and copula GC(C), one can add the deterministic (n) (1) (1) (2) step yj = Fj−1 (uj ). Now, given n samples of observed data Y ≡ {y1 , . . . , yp , y1 , . . . , yp }, one is interested on inferring C via a Bayesian approach and the posterior distribution p(C, θF | Y) ∝ pGC (Y | C, θF )π(C, θF ) where π(·) is a prior distribution, θF are marginal parameters for each Fj (·), which in general might need to be marginalized since they will be unknown, and pGC (·) is the PMF of a (here discrete) distribution with a Gaussian copula and marginals given by θF . Let Z be the underlying latent Gaussians of the corresponding copula for dataset Y. Although Y is a deterministic function of Z, this mapping is not invertible due to the discreteness of the distribution: each marginal Fj (·) has jumps. Instead, the reverse mapping only enforces the constraints where (i ) (i ) (i ) (i ) yj 1 < yj 2 implies zj 1 < zj 2 . Based on this observation, [7] considers the event Z ∈ D(y), where D(y) is the set of values of Z in Rn×p obeying those constraints, that is (k) (k) D(y) ≡ Z ∈ Rn×p : max zj s.t. yj (i) < yj (i) (k) (i) (k) < zj < min zj s.t. yj < yj . Since {Y = y} ⇒ Z(y) ∈ D(y), we have pGC (Y | C, θF ) = pGC (Z ∈ D(y), Y | C, θF ) = pN (Z ∈ D(y) | C) × pGC (Y| Z ∈ D(y), C, θF ), (1) the ﬁrst factor of the last line being that of a zero-mean a Gaussian density function marginalized over D(y). 2 The extended rank likelihood is deﬁned by the ﬁrst factor of (1). With this likelihood, inference for C is given simply by marginalizing p(C, Z | Y) ∝ I(Z ∈ D(y)) pN (Z| C) π(C), (2) the ﬁrst factor of the right-hand side being the usual binary indicator function. Strictly speaking, this is not a fully Bayesian method since partial information on the marginals is ignored. Nevertheless, it is possible to show that under some mild conditions there is information in the extended rank likelihood to consistently estimate C [13]. It has two important properties: ﬁrst, in many applications where marginal distributions are nuisance parameters, this sidesteps any major assumptions about the shape of {Fi (·)} – applications include learning the degree of dependence among variables (e.g., to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.g., [5]); second, MCMC inference in the extended rank likelihood is conceptually simpler than with the joint likelihood, since dropping marginal models will remove complicated entanglements between C and θF . Therefore, even if θF is necessary (when, for instance, predicting missing values of Y), an estimate of C can be computed separately and will not depend on the choice of estimator for {Fi (·)}. The standard model with a full correlation matrix C can be further reﬁned to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others. We explore the latter case in section 5. An off-the-shelf algorithm for sampling from (2) is full Gibbs sampling: ﬁrst, given Z, the (full or structured) correlation matrix C can be sampled by standard methods. More to the point, sampling (i) Z is straightforward if for each variable j and data point i we sample Zj conditioned on all other variables. The corresponding distribution is an univariate truncated Gaussian. This is the approach used originally by Hoff. However, mixing can be severely compromised by the sampling of Z, and that is where novel sampling methods can facilitate inference. 3 Exact HMC for truncated Gaussian distributions (i) Hoff’s algorithm modiﬁes the positions of all Zj associated with a particular discrete value of Yj , conditioned on the remaining points. As the number of data points increases, the spread of the hard (i) boundaries on Zj , given by data points of Zj associated with other levels of Yj , increases. This (i) reduces the space in which variables Zj can move at a time. To improve the mixing, we aim to sample from the joint Gaussian distribution of all latent variables (i) Zj , i = 1 . . . n , conditioned on other columns of the data, such that the constraints between them are satisﬁed and thus the ordering in the observation level is conserved. Standard Gibbs approaches for sampling from truncated Gaussians reduce the problem to sampling from univariate truncated Gaussians. Even though each step is computationally simple, mixing can be slow when strong correlations are induced by very tight truncation bounds. In the following, we brieﬂy describe the methodology recently introduced by [17] that deals with the problem of sampling from log p(x) ∝ − 1 x Mx + r x , where x, r ∈ Rn and M is positive 2 deﬁnite, with linear constraints of the form fj x ≤ gj , where fj ∈ Rn , j = 1 . . . m, is the normal vector to some linear boundary in the sample space. Later in this section we shall describe how this framework can be applied to the Gaussian copula extended rank likelihood model. More importantly, the observed rank statistics impose only linear constraints of the form xi1 ≤ xi2 . We shall describe how this special structure can be exploited to reduce the runtime complexity of the constrained sampler from O(n2 ) (in the number of observations) to O(n) in practice. 3.1 Hamiltonian Monte Carlo for the Gaussian distribution Hamiltonian Monte Carlo (HMC) [15] is a MCMC method that extends the sampling space with auxiliary variables so that (ideally) deterministic moves in the joint space brings the sampler to 3 potentially far places in the original variable space. Deterministic moves cannot in general be done, but this is possible in the Gaussian case. The form of the Hamiltonian for the general d-dimensional Gaussian case with mean µ and precision matrix M is: 1 1 H = x Mx − r x + s M−1 s , (3) 2 2 where M is also known in the present context as the mass matrix, r = Mµ and s is the velocity. Both x and s are Gaussian distributed so this Hamiltonian can be seen (up to a constant) as the negative log of the product of two independent Gaussian random variables. The physical interpretation is that of a sum of potential and kinetic energy terms, where the total energy of the system is conserved. In a system where this Hamiltonian function is constant, we can exactly compute its evolution through the pair of differential equations: ˙ x= sH = M−1 s , ˙ s=− xH = −Mx + r . (4) These are solved exactly by x(t) = µ + a sin(t) + b cos(t) , where a and b can be identiﬁed at initial conditions (t = 0) : ˙ a = x(0) = M−1 s , b = x(0) − µ . (5) Therefore, the exact HMC algorithm can be summarised as follows: • Initialise the allowed travel time T and some initial position x0 . • Repeat for HMC samples k = 1 . . . N 1. Sample sk ∼ N (0, M) 2. Use sk and xk to update a and b and store the new position at the end of the trajectory xk+1 = x(T ) as an HMC sample. It can be easily shown that the Markov chain of sampled positions has the desired equilibrium distribution N µ, M−1 [17]. 3.2 Sampling with linear constraints Sampling from multivariate Gaussians does not require any method as sophisticated as HMC, but the plot thickens when the target distribution is truncated by linear constraints of the form Fx ≤ g . Here, F ∈ Rm×n is a constraint matrix whose every row is the normal vector to a linear boundary in the sample space. This is equivalent to sampling from a Gaussian that is conﬁned in the (not necessarily bounded) convex polyhedron {x : Fx ≤ g}. In general, to remain within the boundaries of each wall, once a new velocity has been sampled one must compute all possible collision times with the walls. The smallest of all collision times signiﬁes the wall that the particle should bounce from at that collision time. Figure 1 illustrates the concept with two simple examples on 2 and 3 dimensions. The collision times can be computed analytically and their equations can be found in the supplementary material. We also point the reader to [17] for a more detailed discussion of this implementation. Once the wall to be hit has been found, then position and velocity at impact time are computed and the velocity is reﬂected about the boundary normal1 . The constrained HMC sampler is summarized follows: • Initialise the allowed travel time T and some initial position x0 . • Repeat for HMC samples k = 1 . . . N 1. Sample sk ∼ N (0, M) 2. Use sk and xk to update a and b . 1 Also equivalent to transforming the velocity with a Householder reﬂection matrix about the bounding hyperplane. 4 1 2 3 4 1 2 3 4 Figure 1: Left: Trajectories of the ﬁrst 40 iterations of the exact HMC sampler on a 2D truncated Gaussian. A reﬂection of the velocity can clearly be seen when the particle meets wall #2 . Here, the constraint matrix F is a 4 × 2 matrix. Center: The same example after 40000 samples. The coloring of each sample indicates its density value. Right: The anatomy of a 3D Gaussian. The walls are now planes and in this case F is a 2 × 3 matrix. Figure best seen in color. 3. Reset remaining travel time Tleft ← T . Until no travel time is left or no walls can be reached (no solutions exist), do: (a) Compute impact times with all walls and pick the smallest one, th (if a solution exists). (b) Compute v(th ) and reﬂect it about the hyperplane fh . This is the updated velocity after impact. The updated position is x(th ) . (c) Tleft ← Tleft − th 4. Store the new position at the end of the trajectory xk+1 as an HMC sample. In general, all walls are candidates for impact, so the runtime of the sampler is linear in m , the number of constraints. This means that the computational load is concentrated in step 3(a). Another consideration is that of the allocated travel time T . Depending on the shape of the bounding polyhedron and the number of walls, a very large travel time can induce many more bounces thus requiring more computations per sample. On the other hand, a very small travel time explores the distribution more locally so the mixing of the chain can suffer. What constitutes a given travel time “large” or “small” is relative to the dimensionality, the number of constraints and the structure of the constraints. Due to the nature of our problem, the number of constraints, when explicitly expressed as linear functions, is O(n2 ) . Clearly, this restricts any direct application of the HMC framework for Gaussian copula estimation to small-sample (n) datasets. More importantly, we show how to exploit the structure of the constraints to reduce the number of candidate walls (prior to each bounce) to O(n) . 4 HMC for the Gaussian Copula extended rank likelihood model Given some discrete data Y ∈ Rn×p , the task is to infer the correlation matrix of the underlying Gaussian copula. Hoff’s sampling algorithm proceeds by alternating between sampling the continu(i) (i) ous latent representation Zj of each Yj , for i = 1 . . . n, j = 1 . . . p , and sampling a covariance matrix from an inverse-Wishart distribution conditioned on the sampled matrix Z ∈ Rn×p , which is then renormalized as a correlation matrix. From here on, we use matrix notation for the samples, as opposed to the random variables – with (i) Zi,j replacing Zj , Z:,j being a column of Z, and Z:,\j being the submatrix of Z without the j-th column. In a similar vein to Hoff’s sampling algorithm, we replace the successive sampling of each Zi,j conditioned on Zi,\j (a conditional univariate truncated Gaussian) with the simultaneous sampling of Z:,j conditioned on Z:,\j . This is done through an HMC step from a conditional multivariate truncated Gaussian. The added beneﬁt of this HMC step over the standard Gibbs approach, is that of a handle for regulating the trade-off between exploration and runtime via the allocated travel time T . Larger travel times potentially allow for larger moves in the sample space, but it comes at a cost as explained in the sequel. 5 4.1 The Hough envelope algorithm The special structure of constraints. Recall that the number of constraints is quadratic in the dimension of the distribution. This is because every Z sample must satisfy the conditions of the event Z ∈ D(y) of the extended rank likelihood (see Section 2). In other words, for any column Z:,j , all entries are organised into a partition L(j) of |L(j) | levels, the number of unique values observed for the discrete or ordinal variable Y (j) . Thereby, for any two adjacent levels lk , lk+1 ∈ L(j) and any pair i1 ∈ lk , i2 ∈ lk+1 , it must be true that Zli ,j < Zli+1 ,j . Equivalently, a constraint f exists where fi1 = 1, fi2 = −1 and g = 0 . It is easy to see that O(n2 ) of such constraints are induced by the order statistics of the j-th variable. To deal with this boundary explosion, we developed the Hough Envelope algorithm to search efﬁciently, within all pairs in {Z:,j }, in practically linear time. Recall in HMC (section 3.2) that the trajectory of the particle, x(t), is decomposed as xi (t) = ai sin(t) + bi cos(t) + µi , (6) and there are n such functions, grouped into a partition of levels as described above. The Hough envelope2 is found for every pair of adjacent levels. We illustrate this with an example of 10 dimensions and two levels in Figure 2, without loss of generalization to any number of levels or dimensions. Assume we represent trajectories for points in level lk with blue curves, and points in lk+1 with red curves. Assuming we start with a valid state, at time t = 0 all red curves are above all blue curves. The goal is to ﬁnd the smallest t where a blue curve meets a red curve. This will be our collision time where a bounce will be necessary. 5 3 1 2 Figure 2: The trajectories xj (t) of each component are sinusoid functions. The right-most green dot signiﬁes the wall and the time th of the earliest bounce, where the ﬁrst inter-level pair (that is, any two components respectively from the blue and red level) becomes equal, in this case the constraint activated being xblue2 = xred2 . 4 4 5 1 2 3 0.2 0.4 0.6 t 0.8 1 1.2 1.4 1. First we ﬁnd the largest component bluemax of the blue level at t = 0. This takes O(n) time. Clearly, this will be the largest component until its sinusoid intersects that of any other component. 2. To ﬁnd the next largest component, compute the roots of xbluemax (t) − xi (t) = 0 for all components and pick the smallest (earliest) one (represented by a green dot). This also takes O(n) time. 3. Repeat this procedure until a red sinusoid crosses the highest running blue sinusoid. When this happens, the time of earliest bounce and its constraint are found. In the worst-case scenario, n such repetitions have to be made, but in practice we can safely assume an ﬁxed upper bound h on the number of blue crossings before a inter-level crossing occurs. In experiments, we found h << n, no more than 10 in simulations with hundreds of thousands of curves. Thus, this search strategy takes O(n) time in practice to complete, mirroring the analysis of other output-sensitive algorithms such as the gift wrapping algorithm for computing convex hulls [8]. Our HMC sampling approach is summarized in Algorithm 1. 2 The name is inspired from the fact that each xi (t) is the sinusoid representation, in angle-distance space, of all lines that pass from the (ai , bi ) point in a − b space. A representation known in image processing as the Hough transform [3]. 6 Algorithm 1 HMC for GCERL # Notation: T MN (µ, C, F) is a truncated multivariate normal with location vector µ, scale matrix C and constraints encoded by F and g = 0 . # IW(df, V0 ) is an inverse-Wishart prior with degrees of freedom df and scale matrix V0 . Input: Y ∈ Rn×p , allocated travel time T , a starting Z and variable covariance V ∈ Rp×p , df = p + 2, V0 = df Ip and chain size N . Generate constraints F(j) from Y:,j , for j = 1 . . . p . for samples k = 1 . . . N do # Resample Z as follows: for variables j = 1 . . . p do −1 −1 2 Compute parameters: σj = Vjj − Vj,\j V\j,\j V\j,j , µj = Z:,\j V\j,\j V\j,j . 2 Get one sample Z:,j ∼ T MN µj , σj I, F(j) efﬁciently by using the Hough Envelope algorithm, see section 4.1. end for Resample V ∼ IW(df + n, V0 + Z Z) . Compute correlation matrix C, s.t. Ci,j = Vi,j / Vi,i Vj,j and store sample, C(k) ← C . end for 5 An application on the Bayesian Gausian copula factor model In this section we describe an experiment that highlights the beneﬁts of our HMC treatment, compared to a state-of-the-art parameter expansion (PX) sampling scheme. During this experiment we ask the important question: “How do the two schemes compare when we exploit the full-advantage of the HMC machinery to jointly sample parameters and the augmented data Z, in a model of latent variables and structured correlations?” We argue that under such circumstances the superior convergence speed and mixing of HMC undeniably compensate for its computational overhead. Experimental setup In this section we provide results from an application on the Gaussian copula latent factor model of [13] (Hoff’s model [7] for low-rank structured correlation matrices). We modify the parameter expansion (PX) algorithm used by [13] by replacing two of its Gibbs steps with a single HMC step. We show a much faster convergence to the true mode with considerable support on its vicinity. We show that unlike the HMC, the PX algorithm falls short of properly exploring the posterior in any reasonable ﬁnite amount of time, even for small models, even for small samples. Worse, PX fails in ways one cannot easily detect. Namely, we sample each row of the factor loadings matrix Λ jointly with the corresponding column of the augmented data matrix Z, conditioning on the higher-order latent factors. This step is analogous to Pakman and Paninski’s [17, sec.3.1] use of HMC in the context of a binary probit model (the extension to many levels in the discrete marginal is straightforward with direct application of the constraint matrix F and the Hough envelope algorithm). The sampling of the higher level latent factors remains identical to [13]. Our scheme involves no parameter expansion. We do however interweave the Gibbs step for the Z matrix similarly to Hoff. This has the added beneﬁt of exploring the Z sample space within their current boundaries, complementing the joint (λ, z) sampling which moves the boundaries jointly. The value of such ”interweaving” schemes has been addressed in [19]. Results We perform simulations of 10000 iterations, n = 1000 observations (rows of Y), travel time π/2 for HMC with the setups listed in the following table, along with the elapsed times of each sampling scheme. These experiments were run on Intel COREi7 desktops with 4 cores and 8GB of RAM. Both methods were parallelized across the observed variables (p). Figure p (vars) k (latent factors) M (ordinal levels) elapsed (mins): HMC PX 3(a) : 20 5 2 115 8 3(b) : 10 3 2 80 6 10 3 5 203 16 3(c) : Many functionals of the loadings matrix Λ can be assessed. We focus on reconstructing the true (low-rank) correlation matrix of the Gaussian copula. In particular, we summarize the algorithm’s 7 outcome with the root mean squared error (RMSE) of the differences between entries of the ground-truth correlation matrix and the implied correlation matrix at each iteration of a MCMC scheme (so the following plots looks like a time-series of 10000 timepoints), see Figures 3(a), 3(b) and 3(c) . (a) (b) (c) Figure 3: Reconstruction (RMSE per iteration) of the low-rank structured correlation matrix of the Gaussian copula and its histogram (along the left side). (a) Simulation setup: 20 variables, 5 factors, 5 levels. HMC (blue) reaches a better mode faster (in iterations/CPU-time) than PX (red). Even more importantly the RMSE posterior samples of PX are concentrated in a much smaller region compared to HMC, even after 10000 iterations. This illustrates that PX poorly explores the true distribution. (b) Simulation setup: 10 vars, 3 factors, 2 levels. We observe behaviors similar to Figure 3(a). Note that the histogram counts RMSEs after the burn-in period of PX (iteration #500). (c) Simulation setup: 10 vars, 3 factors, 5 levels. We observe behaviors similar to Figures 3(a) and 3(b) but with a thinner tail for HMC. Note that the histogram counts RMSEs after the burn-in period of PX (iteration #2000). Main message HMC reaches a better mode faster (iterations/CPUtime). Even more importantly the RMSE posterior samples of PX are concentrated in a much smaller region compared to HMC, even after 10000 iterations. This illustrates that PX poorly explores the true distribution. As an analogous situation we refer to the top and bottom panels of Figure 14 of Radford Neal’s slice sampler paper [14]. If there was no comparison against HMC, there would be no evidence from the PX plot alone that the algorithm is performing poorly. This mirrors Radford Neal’s statement opening Section 8 of his paper: “a wrong answer is obtained without any obvious indication that something is amiss”. The concentration on the posterior mode of PX in these simulations is misleading of the truth. PX might seen a bit simpler to implement, but it seems one cannot avoid using complex algorithms for complex models. We urge practitioners to revisit their past work with this model to ﬁnd out by how much credible intervals of functionals of interest have been overconﬁdent. Whether trivially or severely, our algorithm offers the ﬁrst principled approach for checking this out. 6 Conclusion Sampling large random vectors simultaneously in order to improve mixing is in general a very hard problem, and this is why clever methods such as HMC or elliptical slice sampling [12] are necessary. We expect that the method here developed is useful not only for those with data analysis problems within the large family of Gaussian copula extended rank likelihood models, but the method itself and its behaviour might provide some new insights on MCMC sampling in constrained spaces in general. Another direction of future work consists of exploring methods for elliptical copulas, and related possible extensions of general HMC for non-Gaussian copula models. Acknowledgements The quality of this work has beneﬁted largely from comments by our anonymous reviewers and useful discussions with Simon Byrne and Vassilios Stathopoulos. Research was supported by EPSRC grant EP/J013293/1. 8 References [1] Y. Bishop, S. Fienberg, and P. Holland. Discrete Multivariate Analysis: Theory and Practice. MIT Press, 1975. [2] A. Dobra and A. Lenkoski. Copula Gaussian graphical models and their application to modeling functional disability data. Annals of Applied Statistics, 5:969–993, 2011. [3] R. O. Duda and P. E. Hart. Use of the Hough transformation to detect lines and curves in pictures. Communications of the ACM, 15(1):11–15, 1972. [4] G. Elidan. Copulas and machine learning. Proceedings of the Copulae in Mathematical and Quantitative Finance workshop, to appear, 2013. [5] F. Han and H. Liu. Semiparametric principal component analysis. Advances in Neural Information Processing Systems, 25:171–179, 2012. [6] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. [7] P. Hoff. Extending the rank likelihood for semiparametric copula estimation. Annals of Applied Statistics, 1:265–283, 2007. [8] R. Jarvis. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane. Information Processing Letters, 2(1):18–21, 1973. [9] H. Joe. Multivariate Models and Dependence Concepts. Chapman-Hall, 1997. [10] S. Kirshner. Learning with tree-averaged densities and distributions. Neural Information Processing Systems, 2007. [11] S. Lauritzen. Graphical Models. Oxford University Press, 1996. [12] I. Murray, R. Adams, and D. MacKay. Elliptical slice sampling. JMLR Workshop and Conference Proceedings: AISTATS 2010, 9:541–548, 2010. [13] J. Murray, D. Dunson, L. Carin, and J. Lucas. Bayesian Gaussian copula factor models for mixed data. Journal of the American Statistical Association, to appear, 2013. [14] R. Neal. Slice sampling. The Annals of Statistics, 31:705–767, 2003. [15] R. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, pages 113–162, 2010. [16] R. Nelsen. An Introduction to Copulas. Springer-Verlag, 2007. [17] A. Pakman and L. Paninski. Exact Hamiltonian Monte Carlo for truncated multivariate Gaussians. arXiv:1208.4118, 2012. [18] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. [19] Y. Yu and X. L. Meng. To center or not to center: That is not the question — An ancillaritysufﬁciency interweaving strategy (ASIS) for boosting MCMC efﬁciency. Journal of Computational and Graphical Statistics, 20(3):531–570, 2011. 9</p><p>3 0.91701847 <a title="310-lda-3" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>Author: Brendan McMahan, Jacob Abernethy</p><p>Abstract: We design and analyze minimax-optimal algorithms for online linear optimization games where the player’s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player’s and the adversary’s optimal strategy. We show how these objects can be computed efﬁciently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game. 1</p><p>4 0.91102916 <a title="310-lda-4" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>Author: Liam C. MacDermed, Charles Isbell</p><p>Abstract: We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard inﬁnite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the ﬁrst tractable value iteration method for DecPOMDPs that outperforms existing algorithms. 1</p><p>same-paper 5 0.88822979 <a title="310-lda-5" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>Author: Michel Besserve, Nikos K. Logothetis, Bernhard Schölkopf</p><p>Abstract: Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive deﬁnite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. This similarity measure enables us to identify different types of interactions in electrophysiological neural time series. 1</p><p>6 0.88059938 <a title="310-lda-6" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>7 0.87809598 <a title="310-lda-7" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>8 0.83187306 <a title="310-lda-8" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>9 0.83185554 <a title="310-lda-9" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>10 0.81916422 <a title="310-lda-10" href="./nips-2013-Spike_train_entropy-rate_estimation_using_hierarchical_Dirichlet_process_priors.html">308 nips-2013-Spike train entropy-rate estimation using hierarchical Dirichlet process priors</a></p>
<p>11 0.81674027 <a title="310-lda-11" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>12 0.81595975 <a title="310-lda-12" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>13 0.81027931 <a title="310-lda-13" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>14 0.80910659 <a title="310-lda-14" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>15 0.8044104 <a title="310-lda-15" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>16 0.80318815 <a title="310-lda-16" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>17 0.80001557 <a title="310-lda-17" href="./nips-2013-Gaussian_Process_Conditional_Copulas_with_Applications_to_Financial_Time_Series.html">126 nips-2013-Gaussian Process Conditional Copulas with Applications to Financial Time Series</a></p>
<p>18 0.79877561 <a title="310-lda-18" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>19 0.79872024 <a title="310-lda-19" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>20 0.79867035 <a title="310-lda-20" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
