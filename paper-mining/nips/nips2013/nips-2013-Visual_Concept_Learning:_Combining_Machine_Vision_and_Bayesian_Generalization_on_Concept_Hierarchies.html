<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-349" href="#">nips2013-349</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</h1>
<br/><p>Source: <a title="nips-2013-349-pdf" href="http://papers.nips.cc/paper/5205-visual-concept-learning-combining-machine-vision-and-bayesian-generalization-on-concept-hierarchies.pdf">pdf</a></p><p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>Reference: <a title="nips-2013-349-reference" href="../nips2013_reference/nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. [sent-4, score-0.7]
</p><p>2 Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. [sent-5, score-0.848]
</p><p>3 Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. [sent-6, score-0.28]
</p><p>4 We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. [sent-7, score-0.838]
</p><p>5 We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. [sent-9, score-0.422]
</p><p>6 This task is quite different from that faced by a human child trying to learn a new word, where the child is provided with multiple positive examples and has to generalize appropriately. [sent-12, score-0.377]
</p><p>7 Even young children are able to learn novel visual concepts from very few positive examples [3], something that still poses a challenge for machine vision systems. [sent-13, score-0.671]
</p><p>8 In this paper, we deﬁne a new challenge task for computer vision – visual concept learning – and provide a ﬁrst account of a system that can learn visual concepts from a small number of positive examples. [sent-14, score-1.255]
</p><p>9 In our visual concept learning task, a few example images from a visual concept are given and the system has to indicate whether a new image is or is not an instance of the target concept. [sent-15, score-1.515]
</p><p>10 A key aspect of this task is determining the degree to which the concept should be generalized [21] when multiple concepts are logically consistent with the given examples. [sent-16, score-0.644]
</p><p>11 For example, consider the concepts represented by examples in Figure 1 (a-c) respectively, and the task of predicting whether new images (d-e) belong to them or not. [sent-17, score-0.55]
</p><p>12 The ground truth from human annotators reveals that the level of generalization varies according to the conceptual diversity, with greater diversity leading to broader generalization. [sent-18, score-0.692]
</p><p>13 In the examples shown in Figure 1, people might identify the concepts as (a) Dalmatians, (b) all dogs, and (c) all animals, but not generalize beyond these levels although no 1  (a)  (b)  (c)  (d)  (e)  Figure 1: Visual concept learning. [sent-19, score-0.809]
</p><p>14 Even without negative data, people are able to learn these concepts: (a) Dalmatians, (b) dogs and (c) animals. [sent-21, score-0.262]
</p><p>15 Bayesian models of generalization [1, 18, 21] account for these phenomena, determining the scope of a novel concept (e. [sent-25, score-0.501]
</p><p>16 , does the concept refer to Dalmatians, all dogs, or all animals? [sent-27, score-0.356]
</p><p>17 However, these models were developed by cognitive scientists interested in analyzing human cognition, and require examples to be manually labeled as belonging to a particular leaf node in a conceptual hierarchy. [sent-29, score-0.806]
</p><p>18 This is reasonable if one is asking whether proposed psychological models explain human behavior, but prevents the models from being used to automatically solve visual concept learning problems for a robot or intelligent agent. [sent-30, score-0.926]
</p><p>19 We bring these two threads of research together, using machine vision systems to assign novel images locations within a conceptual hierarchy and a Bayesian generalization model to determine how to generalize from these examples. [sent-31, score-0.575]
</p><p>20 This results in a system that comes closer to human performance than state-of-the-art machine vision baselines. [sent-32, score-0.418]
</p><p>21 2  Background  In machine vision, scant attention has been given to the problem of learning a visual concept from a few positive examples as we have deﬁned it. [sent-35, score-0.655]
</p><p>22 However, this work did not address the problem of determining the level of abstraction within the hierarchy at which to make generalizations, which is a key aspect of the visual concept learning problem. [sent-42, score-0.757]
</p><p>23 [5] proposed predicting object labels only to a granularity that the classiﬁer is conﬁdent with, but their goal was minimizing structured loss rather than mimicking human generalization. [sent-44, score-0.374]
</p><p>24 Existing models from cognitive science mainly focus on understanding human generalization judgments within fairly restricted domains. [sent-45, score-0.53]
</p><p>25 Tenenbaum and colleagues [18, 20] proposed mathematical abstractions for the concept learning problem, building on previous work on models of generalization by Shepard [17]. [sent-46, score-0.501]
</p><p>26 [1] conducted experiments 2  with human participants that provided support for this Bayesian generalization framework. [sent-48, score-0.571]
</p><p>27 Xu and Tenenbaum [21] showed participants one or more positive examples of a novel word (e. [sent-49, score-0.348]
</p><p>28 If the positive examples were three Dalmatians, people might be asked whether other Dalmatians, dogs, and animals are Feps, along with other objects such as vegetables and vehicles. [sent-54, score-0.37]
</p><p>29 Linking such generalization models to direct perceptual input is necessary in order to be able to use this approach to learn visual concepts directly from images. [sent-58, score-0.702]
</p><p>30 To our knowledge, no existing dataset accurately captures the task we refer to as visual concept learning: to learn a novel word from a small set of positive examples like humans do. [sent-61, score-0.836]
</p><p>31 1  Test Procedure  In our test procedure, an agent is shown n example images (n = 5 in our dataset) sampled from a node (may be leaf nodes or intermediate nodes) from the ImageNet synset tree, and is then asked whether other new images sampled from ImageNet belong to the concept or not. [sent-64, score-1.204]
</p><p>32 The scores that the agent gives are then compared against human ground truth that we collect, and we use precisionrecall curves to evaluate the performance. [sent-65, score-0.396]
</p><p>33 2 that using this approach fails to explain how people learn visual concepts. [sent-68, score-0.293]
</p><p>34 Human performance in the above task exhibits much more sophisticated concept learning behaviors than simply identifying the node itself, and the latter differs signiﬁcantly from what we observe from human participants. [sent-69, score-0.835]
</p><p>35 These make our visual concept learning essentially different and richer than a conventional classiﬁcation problem. [sent-73, score-0.654]
</p><p>36 2  Automatic Generation of Examples and Queries  Large-scale experimentation requires an efﬁcient scheme to generate test data across varying levels of a concept hierarchy. [sent-75, score-0.454]
</p><p>37 To this end, we developed a fully-automated procedure for constructing a large-scale dataset suitable for a challenge problem focused on visual concept learning. [sent-76, score-0.658]
</p><p>38 We used the ImageNet LSVRC [2] 2010 data as the basis for automatically constructing a hierarchicallyorganized set of concepts at four different levels of abstraction. [sent-77, score-0.307]
</p><p>39 We had two goals in constructing the dataset: to cover concepts at various levels of abstraction (from subordinate concepts to superordinate concepts, such as from Dalmatian to living things), and to ﬁnd query images that comprehensively test human generalization behavior. [sent-78, score-1.296]
</p><p>40 To generate concepts at various levels of abstraction, we use all the nodes in the ImageNet hierarchy as concept candidates, starting from the leaf node classes as the most speciﬁc level concept. [sent-80, score-1.212]
</p><p>41 We then generate three more levels of increasingly broad concepts along the path from the leaf to the root for each leaf node in the hierarchy. [sent-81, score-0.861]
</p><p>42 Examples from such concepts are then shown to human participants to obtain human generalization judgements, which will serve as the ground truth. [sent-82, score-1.105]
</p><p>43 (a) example images sampled from the four levels for blueberry, and (b) the histogram for the subtree sizes of different levels of concepts (x axis in log scale). [sent-84, score-0.73]
</p><p>44 L1 , L2 , L3 which correspond to three intermediate nodes along the path from the leaf node to the root. [sent-85, score-0.388]
</p><p>45 We choose the three nodes that maximize the combined information gain across these levels: C(L1···3 ) =  3 i=0  log(|Li+1 | − |Li |) − log |Li+1 |,  (1)  where |Li | is the number of leaf nodes under the subtree rooted at Li , and L4 is the whole taxonomy tree. [sent-86, score-0.42]
</p><p>46 For each concept, the ﬁve images shown to participants as examples of that concept were randomly sampled from ﬁve different leaf node categories from the corresponding subtree in the ILSVRC 2010 test images. [sent-90, score-1.268]
</p><p>47 To obtain the ground truth (the concepts people perceive when given the set of examples), we then randomly sample twenty query images, and ask human participants whether each of these query images belong to the concept given by the example images. [sent-92, score-1.679]
</p><p>48 A total of 20 images are randomly sampled as follows: three each from the L0 , L1 , L2 and L3 subtrees, and eight images outside L3 . [sent-93, score-0.384]
</p><p>49 We explicitly made sure that the leaf node classes of the query images were different from those of the examples if possible, and no duplicates exist among the 20 queries. [sent-95, score-0.783]
</p><p>50 Note that we always sampled the example and query images from the ILSVRC 2010 test images, allowing us to subsequently train our machine vision models with the training and validation images from the ILSVRC dataset while keeping those in the visual concept learning dataset as novel test images. [sent-96, score-1.317]
</p><p>51 3  Collecting Human Judgements  We created 4,000 identical concepts (four for each leaf node) using the protocol above, and recruited participants online through Amazon Mechanical Turk (AMT, http://www. [sent-98, score-0.58]
</p><p>52 For each concept, an AMT HIT (a single task presented to the human participants) is formed with ﬁve example images and twenty query images, and the participants were asked whether each query belongs to the concept represented by the examples. [sent-101, score-1.381]
</p><p>53 4  Visually-Grounded Bayesian Concept Learning  In this section, we describe an end-to-end framework which combines Bayesian word learning models and visual classiﬁers, and is able to perform concept learning with perceptual inputs. [sent-111, score-0.81]
</p><p>54 1  Bayesian Concept Learning  Prior work on concept learning [21] addressed the problem of generalization from examples using a Bayesian framework: given a set of N examples (images in our case) X = {x1 , x2 , . [sent-113, score-0.653]
</p><p>55 Given a speciﬁc hypothesis h, the probability Pnew (xnew |h) that a new instance belongs to it is 1 if xnew is in the set, and 0 otherwise, and P (h|X ) is the posterior probability of a hypothesis h given the examples X . [sent-118, score-0.41]
</p><p>56 For example, it is known that people favor basic level object categories such as dogs over subcategories (such as Dalmatians) or supercategories (such as animals). [sent-126, score-0.41]
</p><p>57 Performing Bayesian inference with a complex perceptual input such as images is thus still a challenge. [sent-131, score-0.317]
</p><p>58 To this end, we utilize the state-of-the-art image classiﬁers and classify each image into the set of leaf node classes given in the ImageNet hierarchy, and then build a hypothesis space on top of the classiﬁer outputs. [sent-132, score-0.688]
</p><p>59 Speciﬁcally, we construct the hypothesis space over the image labels using the ImageNet hierarchy, with each subtree rooted at a node serving as a possible hypothesis. [sent-133, score-0.497]
</p><p>60 The hypothesis sizes are then computed as the number of leaf node classes under the corresponding node, e. [sent-134, score-0.502]
</p><p>61 , the node “animal” would have a larger size than the node “dogs”. [sent-136, score-0.288]
</p><p>62 The large number of images collected by ImageNet allows us to train classiﬁers from images to the leaf node labels, which we will describe shortly. [sent-137, score-0.733]
</p><p>63 Thus, the use of the confusion matrix incorporates the visual ambiguity into the word learning framework by providing an unbiased estimation of the true leaf node label for an image. [sent-140, score-0.754]
</p><p>64 The prior probability of a hypothesis was deﬁned to be an Erlang distribution, P (h) ∝ (|h|/σ 2 ) exp{−|h|/σ}, which is a standard prior over sizes in Bayesian models of generalization [17, 19]. [sent-141, score-0.261]
</p><p>65 The parameter σ is set to 200 according to [1] in order to ﬁt human cognition, which favors basic level hypotheses [15]. [sent-142, score-0.376]
</p><p>66 2 million images categorized into the 1,000 leaf node classes, and followed the pipeline in [11] to obtain feature vectors to represent the images. [sent-146, score-0.592]
</p><p>67 Algorithms using similar approaches have reported competitive performance in image classiﬁcation on a large number of classes (on the scale of tens of thousands) [10, 9], which provides reassurance about the possibility of using state-of-the-art classiﬁcation models in visual concept learning. [sent-157, score-0.709]
</p><p>68 To obtain the confusion matrix A of the classiﬁers, we note that the validation data alone does not sufﬁce to provide a dense estimation of the full confusion matrix, because there is a large number of entries (1 million) but very few validation images (50K). [sent-158, score-0.422]
</p><p>69 5  Experiments  In this section, we describe the experimental protocol adopted to compare our system with human performance and compare our system against various baseline algorithms. [sent-164, score-0.396]
</p><p>70 To the best of our knowledge, there are no existing vision models that explicitly handles our concept learning task. [sent-166, score-0.474]
</p><p>71 Thus, we compare our vision baseg Bayes generalization algorithm (denoted by VG) described in the previous section against the following baselines, which are reasonable extensions of existing vision or cognitive science models: 1. [sent-167, score-0.457]
</p><p>72 Naive vision approach (NV): this uses a nearest neighbor approach by computing the score of a query as its distance to the closest example image, using GIST features [12]. [sent-168, score-0.292]
</p><p>73 The human results are shown as the red crosses, and the non-perceptual Bayesian word learning model (NB) is shown as magenta dashed lines. [sent-195, score-0.366]
</p><p>74 The score of a query image is then computed as the probability that it belongs to this subtree. [sent-205, score-0.299]
</p><p>75 , by taking the ground-truth leaf labels for the test images. [sent-210, score-0.252]
</p><p>76 This is not practical in actual applications, but evaluating NP helps understand how a perceptual component contributes to modeling human behavior. [sent-211, score-0.385]
</p><p>77 Conventional vision approaches that build upon image classiﬁers work better than simple image features (such as GIST), which is sensible given that object categories provide relatively more semantics than simple features. [sent-214, score-0.403]
</p><p>78 However, all the baselines still have performances far from human’s, because they miss the key mechanism for inferring the “width” of the latent concept represented by a set of images (instead of a single image as conventional approaches assume). [sent-215, score-0.716]
</p><p>79 In contrast, adopting the size principle and the Bayesian generalization framework allows us to perform much better, obtaining an increase of about 10% in average precision and F1 scores, closer to the human performance than other visual baselines. [sent-216, score-0.686]
</p><p>80 It is also interesting to note that all visual models yield higher precision values in the low-recall region (top left of Figure 3) than the NP model, which does not use perceptual input and has a lower starting precision. [sent-220, score-0.406]
</p><p>81 This suggests that perceptual signals do play an important role in human generalization behaviors, and should not be left out of the pipeline as previous Bayesian word learning methods do. [sent-221, score-0.687]
</p><p>82 , the proportion of query images from level Li that are predicted positive, given examples from level Lj . [sent-226, score-0.495]
</p><p>83 0  human ground truth  Generalization Probability  L0 L1 L2 L3 L4  0. [sent-253, score-0.396]
</p><p>84 0  L0  L1  L2  L3  (c) PM baseline (d) IC oracle Figure 4: Per-level generalization predictions from various methods, where the horizontal axis shows four levels at which examples were provided (L0 to L3 ). [sent-275, score-0.51]
</p><p>85 At each level, ﬁve bars show the proportion of queries form levels L0 to L4 that are labeled as instances of the concept by each method. [sent-276, score-0.489]
</p><p>86 human judgments (vertical axis), with the red line showing a linear regression ﬁt. [sent-278, score-0.309]
</p><p>87 Generalization Probability  human oracle We show in Figures 4 and 5 the per-level generalization 1. [sent-279, score-0.466]
</p><p>88 In addition, for queries of the same level, its generalization score peaks when examples from the 0. [sent-285, score-0.301]
</p><p>89 The NP model tends to give Figure 5: Per-level generalization from more extreme predictions (either very low or very high), possibly due to the fact that it assumes perfect recogni- human participants. [sent-288, score-0.443]
</p><p>90 tion, while visual inputs are actually difﬁcult to precisely classify even for a human being. [sent-289, score-0.483]
</p><p>91 The conventional vision baseline does not utilize the size principle to model human concept learning, and as a result shows very similar behavior with different level of examples. [sent-290, score-0.914]
</p><p>92 Our method exhibits a good correlation with the human results, although it has a smaller generalization probability for L0 queries, possibly because current visual models are still not completely accurate in identifying leaf node classes [5]. [sent-291, score-1.048]
</p><p>93 To do so, Figure 44(d) plots the results of an image classiﬁcation (IC) oracle that predicts yes for an image within the ground-truth ImageNet node that the current examples were sampled from and no otherwise. [sent-293, score-0.467]
</p><p>94 Note that the IC oracle never generalizes beyond the level from which the examples are drawn, and thus, exhibits very different generalization results compared to the human participants in our experiment. [sent-294, score-0.791]
</p><p>95 Thus, visual concept learning poses more realistic and challenging problems for computer vision studies. [sent-295, score-0.697]
</p><p>96 6  Conclusions  We proposed a new task for machine vision – visual concept learning – and presented the ﬁrst system capable of approaching human performance on this problem. [sent-296, score-1.038]
</p><p>97 By linking research on object classiﬁcation in machine vision and Bayesian generalization in cognitive science, we were able to deﬁne a system that could infer the appropriate scope of generalization for a novel concept directly from a set of images. [sent-297, score-0.947]
</p><p>98 This system outperforms baselines that draw on previous approaches in both machine vision and cognitive science, coming closer to human performance than any of these approaches. [sent-298, score-0.494]
</p><p>99 However, there is still signiﬁcant room to improve performance on this task, and we present our visual concept learning dataset as the basis for a new challenge problem for machine vision, going beyond assigning labels to individual objects. [sent-299, score-0.705]
</p><p>100 Learning to share visual appearance for multiclass object detection. [sent-421, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('concept', 0.356), ('human', 0.26), ('visual', 0.223), ('dalmatians', 0.215), ('concepts', 0.209), ('leaf', 0.205), ('dogs', 0.192), ('images', 0.192), ('participants', 0.166), ('generalization', 0.145), ('node', 0.144), ('imagenet', 0.134), ('query', 0.129), ('perceptual', 0.125), ('classi', 0.122), ('vision', 0.118), ('hypothesis', 0.116), ('np', 0.114), ('word', 0.106), ('ilsvrc', 0.099), ('levels', 0.098), ('subtree', 0.097), ('ers', 0.095), ('image', 0.093), ('animals', 0.089), ('bayesian', 0.077), ('pm', 0.077), ('confusion', 0.076), ('pnew', 0.076), ('examples', 0.076), ('cognitive', 0.076), ('conventional', 0.075), ('hierarchy', 0.075), ('er', 0.073), ('truth', 0.071), ('xnew', 0.07), ('people', 0.07), ('ic', 0.067), ('hypotheses', 0.067), ('object', 0.067), ('ground', 0.065), ('amt', 0.065), ('blueberry', 0.065), ('dalmatian', 0.065), ('feps', 0.065), ('pexample', 0.065), ('oracle', 0.061), ('objects', 0.059), ('precision', 0.058), ('annotators', 0.057), ('baseline', 0.056), ('psychological', 0.055), ('abstraction', 0.054), ('pipeline', 0.051), ('tenenbaum', 0.05), ('judgments', 0.049), ('hb', 0.049), ('level', 0.049), ('hit', 0.049), ('hc', 0.047), ('hedging', 0.047), ('labels', 0.047), ('deng', 0.046), ('score', 0.045), ('vg', 0.045), ('challenge', 0.045), ('conceptual', 0.045), ('asked', 0.044), ('prototype', 0.043), ('austerweil', 0.043), ('bets', 0.043), ('xquery', 0.043), ('nv', 0.042), ('li', 0.041), ('task', 0.041), ('taxonomy', 0.04), ('system', 0.04), ('nodes', 0.039), ('validation', 0.039), ('predictions', 0.038), ('edible', 0.038), ('fruit', 0.038), ('logically', 0.038), ('loo', 0.038), ('cvpr', 0.037), ('classes', 0.037), ('axis', 0.036), ('queries', 0.035), ('dept', 0.035), ('ynew', 0.035), ('dataset', 0.034), ('ap', 0.034), ('exhibits', 0.034), ('transfer', 0.033), ('judgements', 0.033), ('participant', 0.033), ('whether', 0.032), ('categories', 0.032), ('belongs', 0.032), ('abbott', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="349-tfidf-1" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>2 0.2218167 <a title="349-tfidf-2" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>Author: Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, Tomas Mikolov</p><p>Abstract: Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model. 1</p><p>3 0.19966991 <a title="349-tfidf-3" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>4 0.1836019 <a title="349-tfidf-4" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>5 0.17530511 <a title="349-tfidf-5" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>Author: Rohit Babbar, Ioannis Partalas, Eric Gaussier, Massih-Reza Amini</p><p>Abstract: We study in this paper ﬂat and hierarchical classiﬁcation strategies in the context of large-scale taxonomies. To this end, we ﬁrst propose a multiclass, hierarchical data dependent bound on the generalization error of classiﬁers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of ﬂat and hierarchical classiﬁers. We then introduce another type of bound targeting the approximation error of a family of classiﬁers, and derive from it features used in a meta-classiﬁer to decide which nodes to prune (or ﬂatten) in a large-scale taxonomy. We ﬁnally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies. 1</p><p>6 0.15336826 <a title="349-tfidf-6" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>7 0.15192895 <a title="349-tfidf-7" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>8 0.14459194 <a title="349-tfidf-8" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>9 0.13235329 <a title="349-tfidf-9" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>10 0.12887976 <a title="349-tfidf-10" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>11 0.12762573 <a title="349-tfidf-11" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>12 0.12141729 <a title="349-tfidf-12" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>13 0.10554569 <a title="349-tfidf-13" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>14 0.10231553 <a title="349-tfidf-14" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>15 0.097596586 <a title="349-tfidf-15" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>16 0.095573984 <a title="349-tfidf-16" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>17 0.091469109 <a title="349-tfidf-17" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>18 0.091238871 <a title="349-tfidf-18" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>19 0.090161175 <a title="349-tfidf-19" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>20 0.083257392 <a title="349-tfidf-20" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.22), (1, 0.084), (2, -0.193), (3, -0.109), (4, 0.191), (5, -0.094), (6, -0.021), (7, -0.032), (8, -0.088), (9, 0.148), (10, -0.188), (11, -0.026), (12, 0.0), (13, -0.034), (14, -0.11), (15, -0.057), (16, -0.039), (17, 0.001), (18, -0.007), (19, -0.009), (20, 0.051), (21, 0.008), (22, -0.058), (23, -0.018), (24, -0.011), (25, 0.132), (26, 0.055), (27, 0.036), (28, -0.038), (29, -0.01), (30, -0.069), (31, 0.005), (32, -0.095), (33, -0.084), (34, 0.004), (35, -0.019), (36, 0.031), (37, 0.064), (38, 0.007), (39, 0.015), (40, 0.01), (41, 0.109), (42, 0.0), (43, 0.067), (44, 0.023), (45, 0.043), (46, 0.03), (47, -0.025), (48, 0.051), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98140621 <a title="349-lsi-1" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>2 0.84391433 <a title="349-lsi-2" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>3 0.77381682 <a title="349-lsi-3" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>4 0.76709962 <a title="349-lsi-4" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>Author: Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, Tomas Mikolov</p><p>Abstract: Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model. 1</p><p>5 0.75807166 <a title="349-lsi-5" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>Author: Marcus Rohrbach, Sandra Ebert, Bernt Schiele</p><p>Abstract: Category models for objects or activities typically rely on supervised learning requiring sufﬁciently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expertspeciﬁed information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More speciﬁcally we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classiﬁcation and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets. 1</p><p>6 0.74455988 <a title="349-lsi-6" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>7 0.71713656 <a title="349-lsi-7" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>8 0.70132303 <a title="349-lsi-8" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>9 0.70083529 <a title="349-lsi-9" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>10 0.66899449 <a title="349-lsi-10" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>11 0.6663211 <a title="349-lsi-11" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>12 0.65915674 <a title="349-lsi-12" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>13 0.64984834 <a title="349-lsi-13" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>14 0.63636708 <a title="349-lsi-14" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>15 0.6263147 <a title="349-lsi-15" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>16 0.59434867 <a title="349-lsi-16" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>17 0.56261164 <a title="349-lsi-17" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>18 0.55849367 <a title="349-lsi-18" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>19 0.55603278 <a title="349-lsi-19" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>20 0.53847128 <a title="349-lsi-20" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.185), (16, 0.032), (33, 0.226), (34, 0.103), (41, 0.033), (49, 0.033), (56, 0.091), (70, 0.065), (85, 0.037), (89, 0.037), (93, 0.052), (95, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92051286 <a title="349-lda-1" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>Author: Matthew Lawlor, Steven W. Zucker</p><p>Abstract: Association ﬁeld models have attempted to explain human contour grouping performance, and to explain the mean frequency of long-range horizontal connections across cortical columns in V1. However, association ﬁelds only depend on the pairwise statistics of edges in natural scenes. We develop a spectral test of the sufﬁciency of pairwise statistics and show there is signiﬁcant higher order structure. An analysis using a probabilistic spectral embedding reveals curvature-dependent components. 1</p><p>same-paper 2 0.89000881 <a title="349-lda-2" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>3 0.88674819 <a title="349-lda-3" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>Author: Edoardo M. Airoldi, Thiago B. Costa, Stanley H. Chan</p><p>Abstract: Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that deﬁnes an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efﬁcient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches inﬁnity.</p><p>4 0.82542795 <a title="349-lda-4" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>5 0.8227331 <a title="349-lda-5" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>Author: Carl Doersch, Abhinav Gupta, Alexei A. Efros</p><p>Abstract: Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical “visual words”, but lower than full-blown semantic objects. Several approaches [5, 6, 12, 23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difﬁcult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classiﬁcation, demonstrating state-of-the-art performance on the MIT Scene-67 dataset. 1</p><p>6 0.82002628 <a title="349-lda-6" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>7 0.81991303 <a title="349-lda-7" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>8 0.81872433 <a title="349-lda-8" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>9 0.81867617 <a title="349-lda-9" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>10 0.8185426 <a title="349-lda-10" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>11 0.8166827 <a title="349-lda-11" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>12 0.81586903 <a title="349-lda-12" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>13 0.81539738 <a title="349-lda-13" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>14 0.81505942 <a title="349-lda-14" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>15 0.81483018 <a title="349-lda-15" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>16 0.81384569 <a title="349-lda-16" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>17 0.81358951 <a title="349-lda-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.81298143 <a title="349-lda-18" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>19 0.81239688 <a title="349-lda-19" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>20 0.81217569 <a title="349-lda-20" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
