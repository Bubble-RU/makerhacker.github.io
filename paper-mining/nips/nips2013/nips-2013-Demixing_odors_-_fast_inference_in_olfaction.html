<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2013-Demixing odors - fast inference in olfaction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-86" href="#">nips2013-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 nips-2013-Demixing odors - fast inference in olfaction</h1>
<br/><p>Source: <a title="nips-2013-86-pdf" href="http://papers.nips.cc/paper/4876-demixing-odors-fast-inference-in-olfaction.pdf">pdf</a></p><p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>Reference: <a title="nips-2013-86-reference" href="../nips2013_reference/nips-2013-Demixing_odors_-_fast_inference_in_olfaction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Demixing odors — fast inference in olfaction  ´ Agnieszka Grabska-Barwinska Gatsby Computational Neuroscience Unit UCL agnieszka@gatsby. [sent-1, score-0.889]
</p><p>2 ch  Abstract The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. [sent-13, score-1.27]
</p><p>3 Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. [sent-18, score-1.703]
</p><p>4 After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. [sent-19, score-0.843]
</p><p>5 If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. [sent-23, score-0.296]
</p><p>6 For the olfactory system, the input spikes come from a few hundred different types of olfactory receptor neurons, and the problem is to infer which odors caused them. [sent-25, score-1.492]
</p><p>7 As there are more than 10,000 possible odors, and more than one can be present at a time, the search space for mixtures of odors is combinatorially large. [sent-26, score-0.85]
</p><p>8 Nevertheless, olfactory processing is fast: organisms can typically determine what odors are present in a few hundred ms. [sent-27, score-1.133]
</p><p>9 Since our focus is on inference, not learning: we assume that the olfactory system has learned both the statistics of odors in the world and the mapping from those odors to olfactory receptor neuron activity. [sent-29, score-2.368]
</p><p>10 We begin by introducing a generative model for spikes in a population of olfactory receptor neurons. [sent-38, score-0.407]
</p><p>11 We simulate those equations, and ﬁnd that both the variational and sampling approaches work well, and require less than 100 ms to converge to a reasonable solution. [sent-41, score-0.177]
</p><p>12 It is known that each odor, by itself, activates a different subset of the olfactory receptor neurons; typically on the order of 10%-30% [2]. [sent-47, score-0.386]
</p><p>13 Here we assume, for simplicity, that activation is linear, for which the activity of odorant receptor neuron i, denoted ri is linearly related to the concentrations, cj of the various odors which are present in a given olfactory scene, plus some background rate, r0 . [sent-48, score-1.636]
</p><p>14 Assuming Poisson noise, the response distribution has the form ri  r0 + P (r|c) =  j  wij cj  ri ! [sent-49, score-0.383]
</p><p>15 i  In a nutshell, ri is Poisson with mean r0 +  j  e−  r0 +  j  wij cj  . [sent-50, score-0.327]
</p><p>16 With this prior, there is a ﬁnite probability that the concentration of any particular odor is zero. [sent-54, score-0.289]
</p><p>17 This prior is much more realistic than a smooth one, as it allows only a small number of odors (out of ∼10,000) to be present in any given olfactory scene. [sent-55, score-1.151]
</p><p>18 It is modeled by introducing a binary variable, sj , which is 1 if odor j is present and 0 otherwise. [sent-56, score-0.403]
</p><p>19 For simplicity we assume that odors are independent and statistically homogeneous, (1 − sj )δ(cj ) + sj Γ(cj |α1 , β1 )  (2. [sent-57, score-1.161]
</p><p>20 1  Inference Variational inference  Because of the delta-function in the prior, performing efﬁcient variational inference in our model is difﬁcult. [sent-61, score-0.167]
</p><p>21 2a)) prior on c is (1 − sj )Γ(cj |α0 , β0 ) + sj Γ(cj |α1 , β1 ) . [sent-65, score-0.357]
</p><p>22 1)  The approximate prior allows absent odors to have nonzero concentration. [sent-67, score-0.9]
</p><p>23 We can partially compensate for that by setting the background ﬁring rate, r0 to zero, and choosing α0 and β0 such that the effective background ﬁring rate (due to the small concentration when sj = 0) is equal to r0 ; see Sec. [sent-68, score-0.245]
</p><p>24 This distribution, denoted Q(c, s|r),was set to Q(c|s, r)Q(s|r) where (1 − sj )Γ(cj |α0j , β0j ) + sj Γ(cj |α1j , β1j )  Q(c|s, r) =  (3. [sent-71, score-0.332]
</p><p>25 To simplify those equations, we set α1 to α0 + 1, resulting in ri wij Fj (λj , α0j ) k=1 wik Fk (λk , α0k )  α0j = α0 + i  Lj ≡ log  λj = L0j + log(α0j /α0 ) + α0j log(β0j /β1j ) 1 − λj  (3. [sent-75, score-0.209]
</p><p>26 The remaining two parameters, β0j and β1j , are ﬁxed by our choice of weights and priors: β0j = β0 + i wij and β1j = β1 + i wij . [sent-80, score-0.182]
</p><p>27 3), τρ τα  dρi = ri − ρi dt  wij Fj (λj , α0j )  dα0j = α0 + Fj (λj , α0j ) dt  τλ  (3. [sent-85, score-0.223]
</p><p>28 3d) might raise some concerns: (i) ρ and α are reciprocally and symmetrically connected; (ii) there are multiplicative interactions between F (λj , α0j ) and ρ; and (iii) the neurons need to compute nontrivial nonlinearities, such as logarithm, exponent and a mixture of digamma functions. [sent-96, score-0.176]
</p><p>29 However: (i) reciprocal and symmetric connectivity exists in the early olfactory processing system [4, 5, 6]; (ii) although multiplicative interactions are in general not easy for neurons, the divisive normalization (Eq. [sent-97, score-0.371]
</p><p>30 5)) has been observed in the olfactory bulb [7], and (iii) the nonlinearities in our algorithms are not extreme (the logarithm is deﬁned only on the positive range (α0j > α0 , Eq. [sent-99, score-0.399]
</p><p>31 To sample efﬁciently from our model, we introduce a new set of variables, cj , ˜ cj = cj sj . [sent-108, score-0.706]
</p><p>32 6)  When written in terms of cj rather than cj , the likelihood becomes ˜ (r0 +  P (r|˜, s) = c  j  wij cj sj )ri ˜  ri ! [sent-110, score-0.853]
</p><p>33 7)  Because the value of cj is unconstrained when sj = 0, we have complete freedom in choosing ˜ P (˜j |sj = 0), the piece of the prior corresponding to the absence of odor j. [sent-113, score-0.608]
</p><p>34 It is convenient to set it c to the same prior we use when sj = 1, which is Γ(˜j |α1 , β1 ). [sent-114, score-0.191]
</p><p>35 Note that this set of manipulations does not change the model: the likelihood doesn’t change, since by deﬁnition cj sj = cj ; when sj = 1, cj is drawn ˜ ˜ from the correct prior; and when sj = 0, cj does not appear in the likelihood. [sent-120, score-1.232]
</p><p>36 The former is standard, τc  d˜j c ∂ log P (˜, s|r) c α1 − 1 = + ξ(t) = − β1 + sj dt ∂˜j c cj ˜  wij i  r0 +  ri − 1 + ξ(t) ˜ k wik ck sk (3. [sent-122, score-0.625]
</p><p>37 This can be done by discretizing time into steps of length dt, and computing the update probability for each odor on each time step. [sent-125, score-0.25]
</p><p>38 This is a valid Gibbs sampler only in the limit dt → 0, where no more than one odor can be updated per time step that’s the limit of interest here. [sent-126, score-0.294]
</p><p>39 The update rule is T (sj |˜, s, r) = ν0 dtP (sj |˜, s, r) + (1 − ν0 dt) ∆(sj − sj ) c c  (3. [sent-127, score-0.166]
</p><p>40 10)  where sj ≡ sj (t + dt), s and ˜ should be evaluated at time t, and ∆(s) is the Kronecker delta: c ∆(s) = 1 if s = 0 and 0 otherwise. [sent-128, score-0.332]
</p><p>41 Computing P (sj = 1|˜, s, r) is straightforward, and we ﬁnd that c P (sj = 1|˜, s, r) = c Φj = log  π + 1−π  1 1 + exp[−Φj ] ri log i  r0 +  k=j  r0 +  wik ck sk + wij cj ˜ ˜ k=j  wik ck sk ˜  − cj wij ˜  . [sent-130, score-0.786]
</p><p>42 Thus, as with the variational approach, we expect a biophysical model to introduce approximations, and, therefore — as with the variational algorithm — degrade slightly the quality of the inference. [sent-138, score-0.196]
</p><p>43 For both algorithms, the odors were generated from the true prior, Eq. [sent-152, score-0.829]
</p><p>44 We modeled a small olfactory system, with 40 olfactory receptor types (compared to approximately 350 in humans and 1000 in mice [8]). [sent-155, score-0.682]
</p><p>45 To keep the ratio of identiﬁable odors to receptor types similar to the one in humans [8], we assumed 400 possible odors, with 3 odors expected to be present in the scene (π = 3/400). [sent-156, score-1.786]
</p><p>46 If an odor was present, its concentration was drawn from a Gamma distribution with α1 = 1. [sent-157, score-0.276]
</p><p>47 Our remaining parameter, β0 , was set to ensure that, for the variational algorithm, the absent odors (those with sj = 0) contributed a background ﬁring rate of r0 on average. [sent-171, score-1.152]
</p><p>48 This average background rate is given by j wij cj = pc Nodors α0 /β0 . [sent-172, score-0.307]
</p><p>49 Figure 2 shows how the inference process evolves over time for a typical set of odors and concentrations. [sent-182, score-0.867]
</p><p>50 The top panel shows concentration, with variational inference on the left (where we plot the mean of the posterior distribution over concentration, (1 − λj )α0j (t)/β0j (t) + λj α1j (t)/β1j (t); see Eq. [sent-183, score-0.187]
</p><p>51 2)) and sampling on the right (where we plot cj , the output of our Langevin sampler; see ˜ Eq. [sent-185, score-0.228]
</p><p>52 The three colored lines correspond to the odors that  Variational  Sampling 150  100  100 c(t)  Concentrations  150  50  50 0  0  400 300  −2  odors  Log−probabilities  0  −4 −6 0  200 100  0. [sent-188, score-1.68]
</p><p>53 2 we plot the log-probability that each of the odors is present, λj (t). [sent-204, score-0.846]
</p><p>54 The present odors quickly approach probabilities of 1; the absent odors all have probabilities below 10−4 within about 200 ms. [sent-205, score-1.742]
</p><p>55 The bottom right panel shows samples from sj for all the odors, with dots denoting present odors (sj (t) = 1) and blanks absent odors (sj (t) = 0). [sent-206, score-1.892]
</p><p>56 Beyond about 500 ms, the true odors (the colored lines at the bottom) are on continuously, and for the odors that were not present, sj is still occasionally 1, but relatively rarely. [sent-207, score-1.846]
</p><p>57 3 we show the time course of the probability of odors when between 1 and 5 odors were presented. [sent-209, score-1.671]
</p><p>58 Therefore, we plot the probability of the most likely non-presented odor (red); the average probability of the non-presented odors (green), and the probability of guessing the correct odors via simple template matching (dashed; see Fig. [sent-215, score-2.012]
</p><p>59 Although odors are inferred relatively rapidly (they exceed template matching within 20 ms), there were almost always false positives. [sent-217, score-0.921]
</p><p>60 Even with just one odor present, both algorithms consistently report the existence of another odor (red). [sent-218, score-0.474]
</p><p>61 This problem diminishes with time if fewer odors are presented than the expected three, but it persists for more complex mixtures. [sent-219, score-0.829]
</p><p>62 The false positives are in fact consistent with human behavior: humans have difﬁculty correctly identify more than one odor in a mixture, with the most common problem being false positives [9]. [sent-220, score-0.308]
</p><p>63 4, we show the log-probability, L (left), and probability, λ (right), averaged across 400 scenes containing 3 odors (see Supplementary Fig. [sent-223, score-0.829]
</p><p>64 The probability of absent odors drops from log(3/400) ≈ e−5 (the prior) to e−12 (the ﬁnal inferred probability). [sent-225, score-0.92]
</p><p>65 For the variational approach, this represents a drop in activity of 7 log units, comparable to the increase of about 5 log units for the present odors (whose probability is inferred to be near 1). [sent-226, score-1.118]
</p><p>66 Thus, for the variational algorithm the average activity associated with the absent odors exhibits a large drop, whereas for the sampling based approach the average activity associated with the absent odors starts small and stays small. [sent-228, score-2.008]
</p><p>67 5  Discussion  We introduced two algorithms for inferring odors from the activity of the odorant receptor neurons. [sent-229, score-1.073]
</p><p>68 The two algorithms performed with striking similarity: they both inferred odors within about 100 ms and they both had about the same accuracy. [sent-232, score-0.916]
</p><p>69 4c), for variational inference the log probability of concentration and presence/absence are related to the dynamical variables via log Q(cj ) ∼ α1j log cj − β1j cj (5. [sent-237, score-0.624]
</p><p>70 If we interpret α0j and Lj as ﬁring rates, then these equations correspond to a linear probabilistic population code [10]: the log probability inferred by the approximate algorithm is linear in ﬁring rate, with a parameter-dependent offset (the term −β1j cj in Eq. [sent-240, score-0.289]
</p><p>71 For the sampling-based algorithm, on the other hand, activity generates samples from the posterior; an average of those samples codes for the probability of an odor being present. [sent-243, score-0.348]
</p><p>72 Thus, if the olfactory system uses variational inference, activity should code for log probability, whereas if it uses sampling, activity should code for probability. [sent-244, score-0.544]
</p><p>73 5  0  4 odors  1  4 odors  1        2 odors  1  0  0. [sent-250, score-2.487]
</p><p>74 5  0 0  1 odor  1        Sampling  1 odor  1  20  40  60  80  0. [sent-254, score-0.474]
</p><p>75 Shaded areas represent 25th–75th percentile of values across 400 olfactory scenes. [sent-257, score-0.308]
</p><p>76 (Template matching ﬁnds odors (the j’s) that maximize the dot product between the activity, ri , and the weights, wij , associated, 1/2  2 2 with odor j; that is, it chooses j’s that maximize i ri wij / . [sent-261, score-1.375]
</p><p>77 The number of i ri i wij odors chosen by template matching was set to the number of odors presented. [sent-262, score-1.852]
</p><p>78 7  Variational  Sampling  3 odors  −5  λ  L  3 odors  1  0  0. [sent-265, score-1.658]
</p><p>79 For the variational algorithm, the activity of the neurons codes for log probability (relative to some background to keep ﬁring rates non-negative). [sent-268, score-0.302]
</p><p>80 For this algorithm, the drop in probability of the non-presented odors from about e−5 to e−12 corresponds to a large drop in ﬁring rate. [sent-269, score-0.92]
</p><p>81 For the sampling based algorithm, on the other hand, activity codes for probability, and there is almost no drop in activity. [sent-270, score-0.168]
</p><p>82 One is to note that for the variational algorithm there is a large drop in the average activity of the neurons coding for the non-present odors (Fig. [sent-272, score-1.103]
</p><p>83 Unfortunately, it is not clear where exactly one needs to stick the electrode to record the trace of the olfactory inference. [sent-282, score-0.292]
</p><p>84 A good place to start would be the olfactory bulb, where odor representations have been studied extensively [12, 13, 14]. [sent-283, score-0.514]
</p><p>85 We should also point out that although the olfactory bulb is a likely location for at least part of our two inference algorithms, both are sufﬁciently complicated that they may need to be performed by higher cortical structures, such as the anterior piriform cortex, [18, 19]. [sent-288, score-0.413]
</p><p>86 For instance, the generative model was very simple: we assumed that concentrations added linearly, that weights were binary (so that each odor activated a subset of the olfactory receptor neurons at a ﬁnite value, and did not activate the rest at all), and that noise was Poisson. [sent-291, score-0.731]
</p><p>87 And we considered priors such that all odors were independent. [sent-293, score-0.842]
</p><p>88 This too is unlikely to be true – for instance, the set of odors one expects in a restaurant are very different than the ones one expects in a toxic waste dump, consistent with the fact that responses in the olfactory bulb are modulated by task-relevant behavior [20]. [sent-294, score-1.203]
</p><p>89 We have also focused solely on inference: we assumed that the network knew perfectly both the mapping from odors to odorant receptor neurons and the priors. [sent-296, score-1.062]
</p><p>90 Finally, the neurons in our network had to implement relatively complicated nonlinearities: logs, exponents, and digamma and quadratic functions, and neurons had to be reciprocally connected. [sent-298, score-0.182]
</p><p>91 Dense representation of natural odorants in the mouse olfactory bulb. [sent-319, score-0.277]
</p><p>92 Theoretical reconstruction of ﬁeld potentials and dendrodendritic synaptic interactions in olfactory bulb. [sent-330, score-0.291]
</p><p>93 Sparse incomplete representations: a potential role of olfactory granule cells. [sent-341, score-0.277]
</p><p>94 The capacity of humans to identify odors in mixtures. [sent-358, score-0.848]
</p><p>95 Spatio-temporal dynamics of odor representations in the mammalian olfactory bulb. [sent-398, score-0.532]
</p><p>96 Robust odor coding via inhalation-coupled transient activity in the mammalian olfactory bulb. [sent-401, score-0.619]
</p><p>97 Modeling the olfactory bulb and its neural oscillatory processings. [sent-407, score-0.346]
</p><p>98 Sparse distributed representation of odors in a large-scale olfactory bulb circuit. [sent-419, score-1.175]
</p><p>99 A model of olfactory adaptation and sensitivity enhancement in the olfactory bulb. [sent-425, score-0.554]
</p><p>100 Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity. [sent-431, score-0.317]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('odors', 0.829), ('olfactory', 0.277), ('odor', 0.237), ('cj', 0.18), ('sj', 0.166), ('receptor', 0.109), ('wij', 0.091), ('variational', 0.091), ('bulb', 0.069), ('activity', 0.068), ('odorant', 0.067), ('neurons', 0.057), ('ri', 0.056), ('ms', 0.055), ('concentrations', 0.051), ('absent', 0.046), ('drop', 0.039), ('wik', 0.039), ('concentration', 0.039), ('dt', 0.038), ('inference', 0.038), ('nonlinearities', 0.037), ('ring', 0.034), ('ppm', 0.034), ('divisive', 0.033), ('template', 0.032), ('inferred', 0.032), ('percentile', 0.031), ('fj', 0.031), ('sampling', 0.031), ('codes', 0.03), ('neuron', 0.03), ('reciprocally', 0.03), ('lj', 0.028), ('coffee', 0.027), ('langevin', 0.027), ('organisms', 0.027), ('apr', 0.026), ('prior', 0.025), ('digamma', 0.024), ('log', 0.023), ('nov', 0.022), ('agnieszka', 0.022), ('bacon', 0.022), ('cybern', 0.022), ('naoshige', 0.022), ('nodors', 0.022), ('olfaction', 0.022), ('shepherd', 0.022), ('lines', 0.022), ('panel', 0.022), ('mixtures', 0.021), ('population', 0.021), ('realistic', 0.02), ('background', 0.02), ('equations', 0.02), ('spike', 0.019), ('humans', 0.019), ('sampler', 0.019), ('posterior', 0.019), ('probabilities', 0.019), ('gamma', 0.019), ('coding', 0.019), ('mammalian', 0.018), ('raise', 0.018), ('sk', 0.018), ('jeff', 0.017), ('beck', 0.017), ('symmetrically', 0.017), ('slab', 0.017), ('latham', 0.017), ('plot', 0.017), ('behavioral', 0.017), ('system', 0.017), ('pc', 0.016), ('ucl', 0.016), ('logarithm', 0.016), ('nontrivial', 0.016), ('dashed', 0.016), ('collapses', 0.016), ('reciprocal', 0.016), ('matching', 0.015), ('biol', 0.015), ('electrode', 0.015), ('cortical', 0.015), ('expects', 0.014), ('alexandre', 0.014), ('complicated', 0.014), ('interactions', 0.014), ('dynamical', 0.014), ('ck', 0.014), ('correct', 0.014), ('connectivity', 0.014), ('degrade', 0.014), ('delta', 0.013), ('predictions', 0.013), ('priors', 0.013), ('probability', 0.013), ('false', 0.013), ('positives', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="86-tfidf-1" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>2 0.08277759 <a title="86-tfidf-2" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>3 0.068907581 <a title="86-tfidf-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.061579105 <a title="86-tfidf-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.058214653 <a title="86-tfidf-5" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>6 0.056437198 <a title="86-tfidf-6" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>7 0.054279484 <a title="86-tfidf-7" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>8 0.050740335 <a title="86-tfidf-8" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>9 0.049717676 <a title="86-tfidf-9" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>10 0.040760696 <a title="86-tfidf-10" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>11 0.038380891 <a title="86-tfidf-11" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>12 0.037400201 <a title="86-tfidf-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.035599779 <a title="86-tfidf-13" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>14 0.035499237 <a title="86-tfidf-14" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>15 0.03438827 <a title="86-tfidf-15" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>16 0.033994131 <a title="86-tfidf-16" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>17 0.033691227 <a title="86-tfidf-17" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>18 0.031770036 <a title="86-tfidf-18" href="./nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</a></p>
<p>19 0.031582166 <a title="86-tfidf-19" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>20 0.030826241 <a title="86-tfidf-20" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.082), (1, 0.036), (2, -0.057), (3, -0.036), (4, -0.097), (5, 0.012), (6, 0.026), (7, -0.029), (8, 0.036), (9, 0.004), (10, 0.004), (11, 0.036), (12, 0.028), (13, 0.013), (14, -0.001), (15, -0.014), (16, -0.005), (17, -0.005), (18, -0.003), (19, -0.024), (20, -0.001), (21, -0.012), (22, -0.018), (23, 0.038), (24, -0.031), (25, -0.009), (26, 0.041), (27, -0.014), (28, 0.016), (29, 0.032), (30, -0.031), (31, 0.022), (32, 0.016), (33, 0.046), (34, -0.0), (35, -0.008), (36, 0.061), (37, -0.012), (38, -0.06), (39, -0.021), (40, -0.071), (41, 0.02), (42, -0.023), (43, 0.062), (44, -0.005), (45, -0.022), (46, -0.011), (47, 0.032), (48, 0.033), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87706423 <a title="86-lsi-1" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>2 0.70829225 <a title="86-lsi-2" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>3 0.64382839 <a title="86-lsi-3" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>4 0.5810619 <a title="86-lsi-4" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>5 0.57015061 <a title="86-lsi-5" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>6 0.52694792 <a title="86-lsi-6" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>7 0.51839697 <a title="86-lsi-7" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>8 0.51638567 <a title="86-lsi-8" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>9 0.50701743 <a title="86-lsi-9" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>10 0.473638 <a title="86-lsi-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.46183729 <a title="86-lsi-11" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>12 0.46007463 <a title="86-lsi-12" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>13 0.45918226 <a title="86-lsi-13" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>14 0.42567104 <a title="86-lsi-14" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>15 0.42450419 <a title="86-lsi-15" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>16 0.42264533 <a title="86-lsi-16" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>17 0.41993919 <a title="86-lsi-17" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>18 0.41795602 <a title="86-lsi-18" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>19 0.40577519 <a title="86-lsi-19" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>20 0.39762205 <a title="86-lsi-20" href="./nips-2013-Online_Variational_Approximations_to_non-Exponential_Family_Change_Point_Models%3A_With_Application_to_Radar_Tracking.html">234 nips-2013-Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (16, 0.062), (33, 0.082), (34, 0.136), (41, 0.037), (49, 0.052), (51, 0.281), (56, 0.065), (70, 0.055), (85, 0.02), (89, 0.02), (93, 0.026), (95, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73738432 <a title="86-lda-1" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>2 0.56762433 <a title="86-lda-2" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>3 0.55807418 <a title="86-lda-3" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>4 0.54612732 <a title="86-lda-4" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>Author: Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour</p><p>Abstract: We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir [14]. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph (which must be accessible before selecting an action). In the undirected case, we show that the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efﬁcient manner. 1</p><p>5 0.53879368 <a title="86-lda-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.53815883 <a title="86-lda-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.53581262 <a title="86-lda-7" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>8 0.53525966 <a title="86-lda-8" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>9 0.53502899 <a title="86-lda-9" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>10 0.53305119 <a title="86-lda-10" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>11 0.53295749 <a title="86-lda-11" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>12 0.5299381 <a title="86-lda-12" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>13 0.52970225 <a title="86-lda-13" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>14 0.52861595 <a title="86-lda-14" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>15 0.52808881 <a title="86-lda-15" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>16 0.5277614 <a title="86-lda-16" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>17 0.52776116 <a title="86-lda-17" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>18 0.52661377 <a title="86-lda-18" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>19 0.52655703 <a title="86-lda-19" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>20 0.52621764 <a title="86-lda-20" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
