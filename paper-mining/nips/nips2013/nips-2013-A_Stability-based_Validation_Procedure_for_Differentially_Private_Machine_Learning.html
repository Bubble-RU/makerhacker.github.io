<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-14" href="#">nips2013-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</h1>
<br/><p>Source: <a title="nips-2013-14-pdf" href="http://papers.nips.cc/paper/5014-a-stability-based-validation-procedure-for-differentially-private-machine-learning.pdf">pdf</a></p><p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>Reference: <a title="nips-2013-14-reference" href="../nips2013_reference/nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. [sent-4, score-0.923]
</p><p>2 In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. [sent-6, score-1.737]
</p><p>3 The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. [sent-7, score-0.765]
</p><p>4 We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. [sent-8, score-1.284]
</p><p>5 The emerging standard for privacy-preserving computation for the past few years is differential privacy [7]. [sent-10, score-0.544]
</p><p>6 The value α is called the privacy budget, and measures the level of privacy risk allowed. [sent-13, score-0.902]
</p><p>7 As more noise is needed to achieve lower α,the price of higher privacy is reduced utility or accuracy. [sent-14, score-0.451]
</p><p>8 1  A major barrier to achieving end-to-end differential privacy in practical machine-learning applications is the absence of an effective procedure for differentially private parameter-tuning. [sent-20, score-1.534]
</p><p>9 Currently, parameter-tuning with differential privacy is done in two ways. [sent-22, score-0.544]
</p><p>10 However re-using the data leads to a degradation in the privacy guarantees, and thus to maintain the privacy budget α, for each training, we need to use a privacy budget that shrinks polynomially with the number of parameter values. [sent-24, score-1.47]
</p><p>11 Both solutions are highly sub-optimal, particularly, if a large number of parameter values are involved – the ﬁrst due to the lower privacy budget, and the second due to less data. [sent-26, score-0.478]
</p><p>12 Thus the challenge is to design a differentially private validation procedure that uses the data and the privacy budget effectively, but can still do parameter-tuning. [sent-27, score-1.72]
</p><p>13 In this paper, we show that it is indeed possible to do effective parameter-tuning with differential privacy in a fairly general setting, provided the training algorithm and the performance measure used to evaluate its output on the validation data together obey a certain stability condition. [sent-29, score-1.043]
</p><p>14 The second condition is fairly standard, and our key insight is in characterizing the ﬁrst condition and showing that it can help in differentially private parameter tuning. [sent-31, score-1.017]
</p><p>15 We next design a generic differentially private training and validation procedure that provides endto-end privacy provided this stability condition holds. [sent-32, score-1.933]
</p><p>16 The training set size and the privacy budget used by our training algorithms are independent of k, the number of parameter values, and the accuracy of our validation procedure degrades only logarithmically with k. [sent-33, score-0.999]
</p><p>17 We apply our generic procedure to two fundamental tasks in machine-learning and statistics – training a linear classiﬁer using regularized convex optimization, and building a histogram density estimator. [sent-34, score-0.341]
</p><p>18 We prove that existing differentially private algorithms for these problems obey our notion of stability with respect to standard validation performance measures, and we show how to combine them to provide end-to-end differentially private solutions for these tasks. [sent-35, score-2.243]
</p><p>19 In particular, our application to linear classiﬁcation is based on existing differentially private procedures for regularized convex optimization due to [4], and our application to histogram density estimation is based on the algorithm variant due to [19]. [sent-36, score-1.151]
</p><p>20 In our experiments, even for a moderate value of k, our procedure outperformed existing differentially private solutions for parameter tuning, and achieved performance only slightly worse than knowing the best parameter to use ahead of time. [sent-38, score-1.066]
</p><p>21 In particular, our case study on linear classiﬁcation is based on existing differentially private procedures for regularized convex optimization, which were proposed by [4], and extended by [23, 18, 15]. [sent-42, score-1.007]
</p><p>22 There has also been a large body of work on differentially private histogram construction in the statistics, algorithms and database literature [7, 19, 27, 28, 20, 29, 14]. [sent-43, score-1.002]
</p><p>23 While the problem of differentially private parameter tuning has been mentioned in several works, to the best of our knowledge, an efﬁcient systematic solution has been elusive. [sent-45, score-0.963]
</p><p>24 [28]  2  mentions ﬁnding a good bin size for a histogram using differentially private validation procedure as an open problem. [sent-48, score-1.35]
</p><p>25 We adopt differential privacy as our notion of privacy. [sent-51, score-0.544]
</p><p>26 Deﬁnition 1 A (randomized) algorithm A whose output lies in a domain S is said to be (α, δ)differentially private if for all measurable S ⊆ S, for all datasets D and D that differ in the value of a single individual, it is the case that: Pr(A(D) ∈ S) ≤ eα Pr(A(D ) ∈ S) + δ. [sent-52, score-0.612]
</p><p>27 An algorithm is said to be α-differentially private if δ = 0. [sent-53, score-0.523]
</p><p>28 Here α and δ are privacy parameters where lower α and δ imply higher privacy. [sent-54, score-0.451]
</p><p>29 Differential privacy has been shown to have many desirable properties, such as robustness to side information [7] and resistance to composition attacks [11]. [sent-55, score-0.516]
</p><p>30 An important property of differential privacy is that the privacy guarantees degrade gracefully if the same sensitive data is used in multiple private computations. [sent-56, score-1.56]
</p><p>31 In particular, if we apply an αdifferentially private procedure k times on the same data, the result is kα-differential private as well as (α , δ)-differentially private for α = kα(eα − 1) + 2k log(1/δ)α [7, 8]. [sent-57, score-1.623]
</p><p>32 These privacy composition results are the basis of existing differentially private parameter tuning procedures. [sent-58, score-1.456]
</p><p>33 Typical (non-private) machine learning algorithms have one or more undetermined parameters, and standard practice is to run the machine learning algorithm for a number of different parameter values on a training set, and evaluate the outputs on a separate held-out validation dataset. [sent-60, score-0.375]
</p><p>34 Our goal in this paper is to design a differentially private version of this procedure which uses the privacy budget efﬁciently. [sent-63, score-1.486]
</p><p>35 The full validation process thus has two components – a training procedure, and a validation score which evaluates how good the training procedure is. [sent-64, score-0.771]
</p><p>36 We assume that training and validation data are drawn from a domain X , and the result of the differentially private training algorithm lies in a domain C. [sent-65, score-1.358]
</p><p>37 A differentially private training procedure is a randomized algorithm, which takes as input a (sensitive) training dataset, a parameter (of the training procedure), and a privacy parameter α and outputs an element of C; the procedure is expected to be α-differentially private. [sent-68, score-1.851]
</p><p>38 Observe that any differentially private algorithm can be represented as such a tuple. [sent-71, score-0.936]
</p><p>39 , xn ∈ [0, 1], an α-differentially private approximation to the sample mean x is x + ¯ ¯ 1 αn Z where Z is drawn from the standard Laplace distribution. [sent-75, score-0.558]
</p><p>40 A validation score is a function q : C × X m → R which takes an object h in C and a validation dataset V , and outputs a score which reﬂects the quality of h with respect to V . [sent-81, score-0.61]
</p><p>41 3  Stability and Generic Validation Procedure  We now introduce and discuss our notion of stability, and provide a generic validation procedure that uses the privacy budget efﬁciently when this notion of stability holds. [sent-84, score-0.921]
</p><p>42 Deﬁnition 2 ((β1 , β2 , δ)-Stability) A validation score q is said to be (β1 , β2 , δ)-stable with respect to a training procedure T = (G, F ), a privacy parameter α, and a parameter set Θ if the following holds. [sent-85, score-0.948]
</p><p>43 m Condition (1), the training stability condition, bounds the change in the validation score q, when one person’s private data in the training set T changes, and the validation set V as well as the value of the random variable R remains the same. [sent-90, score-1.377]
</p><p>44 Our validation procedure critically relies on this condition, and our main contribution in this paper is to identify and exploit it to provide a validation procedure that uses the privacy budget efﬁciently. [sent-91, score-1.072]
</p><p>45 As F (T, θ, α, R) is a deterministic function, Condition (2), the validation stability condition, bounds the change in q when one person’s private data in the validation set V changes, and the output of the training procedure remains the same. [sent-92, score-1.31]
</p><p>46 First, observe that Condition (1) is a property of the differentially private training algorithm (in addition to q and the non-private quantity being approximated). [sent-95, score-1.049]
</p><p>47 Even if all else remains the same, different differentially private approximations to the same non-private quantity will have different values of β1 . [sent-96, score-0.936]
</p><p>48 Second, Condition (1) does not always hold for small β1 as an immediate consequence of differential privacy of the training procedure. [sent-97, score-0.638]
</p><p>49 1, we present an example of a problem and two α-differentially private training algorithms which approximately optimize the same function; the ﬁrst algorithm is based on exponential mechanism, and the second on a maximum of Laplace random variables mechanism. [sent-100, score-0.617]
</p><p>50 We show that while both provide α-differential privacy guarantees, the ﬁrst algorithm does not satisfy training stability for β1 = o(n) and small enough δ while the second one ensures training stability for β1 = 1 and δ = 0. [sent-101, score-0.913]
</p><p>51 In Section 4, we present two case studies of commonly used differentially private algorithms where Conditions (1) and (2) hold for constant β1 and β2 . [sent-102, score-0.936]
</p><p>52 When the (β1 , β2 , δ)-stability condition holds, we can design an end-to-end differentially private parameter tuning algorithm, which is shown in Algorithm 2. [sent-103, score-0.99]
</p><p>53 , θk }, training procedure T = (G, F ), validation score q,  2: 3: 4: 5: 6: 7:  training set T , validation set V , stability parameters β1 and β2 , training privacy parameter α1 , validation privacy parameter α2 . [sent-110, score-2.192]
</p><p>54 Algorithm 1 takes as input a training procedure T , a parameter list Θ, a validation score q, training and validation datasets T and V , and privacy parameters α1 and α2 . [sent-119, score-1.269]
</p><p>55 It runs the training procedure T on the same training set T with privacy budget α1 for each parameter in Θ to generate outputs h1 , h2 , . [sent-120, score-0.785]
</p><p>56 , and then uses an α2 -differentially private procedure to select the index i∗ such that the validation score q(hi∗ , V ) is (approximately) maximum. [sent-123, score-0.872]
</p><p>57 , θk }, training procedure T = (G, F ), validation score q,  training set T , validation set V , stability parameters β1 and β2 , training privacy parameter α1 , validation privacy parameter α2 . [sent-129, score-2.192]
</p><p>58 1  Performance Guarantees  Theorem 1 shows that Algorithm 1 is (α2 , δ)-differentially private, and Theorem 2 shows privacy guarantees on Algorithm 2. [sent-134, score-0.473]
</p><p>59 Theorem 1 (Privacy Guarantees for Validation Procedure) If the validation score q is δ (β1 , β2 , k )-stable with respect to the training procedure T , the privacy parameter α1 and the parameter set Θ, then, Algorithm 1 guarantees (α2 , δ)-differential privacy. [sent-137, score-0.97]
</p><p>60 , hk be the output of the differentially private training procedure in Step (3) of Algorithm 1. [sent-143, score-1.118]
</p><p>61 α2  4  Case Studies  We next show that Algorithm 2 may be applied to design end-to-end differentially private training and validation procedures for two fundamental statistical and machine-learning tasks – training a linear classiﬁer, and building a histogram density estimator. [sent-145, score-1.524]
</p><p>62 In each case, we use existing differentially private algorithms and validation scores for these tasks. [sent-146, score-1.17]
</p><p>63 We show that the validation score satisﬁes the (β1 , β2 , δ)-stability property with respect to the training procedure for small values of β1 and 5  β2 , and thus we can apply in Algorithm 2 with a small value of β to obtain end-to-end differential privacy. [sent-147, score-0.536]
</p><p>64 [4] present two algorithms for computing differentially private approximations to these regularized convex optimization problems for ﬁxed λ: output perturbation and objective perturbation. [sent-160, score-1.06]
</p><p>65 Thus we can use Algorithm 2 along with this training procedure 6  and any L-Lipschitz validation score to get an end-to-end differentially private algorithm for linear classiﬁcation. [sent-180, score-1.379]
</p><p>66 Theorem 4 (Stability of differentially private linear classiﬁers) Let Λ = {λ1 , . [sent-181, score-0.936]
</p><p>67 The Methods Each method takes input (α, Θ, T, V ), where α denotes the allowed differential privacy, T is a training set, V is a validation set, and Θ = {θ1 , . [sent-192, score-0.421]
</p><p>68 01, average negative ramp loss used as validation score q, and with α1 = α2 = α/2. [sent-198, score-0.352]
</p><p>69 The four other methods work by performing the following 4 steps: (1) for each θi ∈ Θ, train a differentially private classiﬁer fi = oplr (αi , θi , Ti ), (2) determine the number of errors ei each fi makes on validation set V , (3) randomly choose i∗ from {1, 2, . [sent-199, score-1.266]
</p><p>70 The three other alternative methods are differentially private which we state in the following theorem. [sent-209, score-0.936]
</p><p>71 Random is α differentially private even if T and V are not disjoint, in which case alphaSplit and dataSplit are 2α-differentially private. [sent-211, score-0.936]
</p><p>72 By Theorems 2 and 5, all methods except Control produce 7  a (α, δ)-differentially private classiﬁer. [sent-225, score-0.523]
</p><p>73 Figure 1: A summary of 10 times 10-fold cross-validation experiments for different privacy levels α. [sent-272, score-0.451]
</p><p>74 Results Figure 1 summarizes classiﬁer performances and regularizer choices for the different values of the privacy parameter α, aggregated over all cross-validation runs. [sent-276, score-0.508]
</p><p>75 For MSE on the other hand, Stability outperformed the differentially private alternatives in all experiments. [sent-281, score-0.958]
</p><p>76 Boosting the accuracy of differentially private histograms through consistency. [sent-371, score-0.936]
</p><p>77 Differentially private projected histograms: Construction and use for prediction. [sent-443, score-0.523]
</p><p>78 1  Appendix An Example to Show Training Stability is not a Direct Consequence of Differential Privacy  We now present an example to illustrate that training stability is a property of the training algorithm and not a direct consequence of differential privacy. [sent-459, score-0.418]
</p><p>79 We present a problem and two α-differentially private training algorithms which approximately optimize the same function; the ﬁrst algorithm is based on exponential mechanism, and the second on a maximum of Laplace random variables mechanism. [sent-460, score-0.617]
</p><p>80 We show that while both provide α-differential privacy guarantees, the ﬁrst algorithm does not satisfy training stability while the second one does. [sent-461, score-0.682]
</p><p>81 Given a sensitive dataset D, the private training procedure A outputs a tuple (i∗ , t1 , . [sent-467, score-0.735]
</p><p>82 , tl ), where i∗ is the output of the α/2-differentially private exponential mechanism [21] run to approximately maximize f (D, i), and each ti is equal to 2l f (D, i) plus an independent Laplace random variable with standard deviation αn . [sent-470, score-0.651]
</p><p>83 Case Study: Histogram Density Estimation  Our second case study is developing an end-to-end differentially private solution for histogrambased density estimation. [sent-513, score-1.014]
</p><p>84 In a histogram density estimator, we divide the range of the data into equal-sized bins of width h; if ni out of n of the input 1/h ni ˆ ˆ samples lie in bin i, then f is the density function: f (x) = i=1 hn · 1(x ∈ Bin i). [sent-518, score-0.484]
</p><p>85 However, the choice of h is usually data-dependent, and in practice, the optimal h is often determined by building a histogram density estimator for a few different values of h, and selecting the one which has the best performance on held-out validation data. [sent-521, score-0.406]
</p><p>86 Given n samples and a bin size h, several works, including [7, 19, 27, 28, 20, 29, 14] have shown different ways of constructing and sampling from differentially private histograms. [sent-535, score-0.996]
</p><p>87 Algorithm 5 presents a variant of a differentially private histogram density estimator due to [19] in our framework. [sent-537, score-1.108]
</p><p>88 Deﬁne: ni = j=1 1(xj ∈ Ii ), and let ni = max 0, ni + h 5: end for 1/h ni ˜ ˆ 6: Let n = i ni . [sent-547, score-0.505]
</p><p>89 The following theorem shows stability guarantees on the differentially private histogram density estimator described in Algorithm 5. [sent-549, score-1.298]
</p><p>90 Our proofs involve ideas similar to those in the analysis of the multiplicative weights update method for answering a set of linear queries in a differentially private manner [13]. [sent-557, score-0.936]
</p><p>91 Let A(D) denote the output of Algorithm 1 when the input is a sensitive dataset D = (T, V ), where T is the training part and V is the validation part. [sent-558, score-0.382]
</p><p>92 P ROOF :(Of Theorem 2) The proof of Theorem 2 follows from privacy composition; Theorem 1 ensures that Step (2) of Algorithm 2 is (α2 , δ)-differentially private; moreover the training procedure T is α1 -differentially private. [sent-631, score-0.599]
</p><p>93 We observe that for ﬁxed λ, α and R, F (T, λ, α, R) − F (T , λ, α, R) = w∗ (T ) − w∗ (T ) When the training sets are T and T , the objective functions in the regularized convex optimization 1 problems are both λ-strongly convex, and they differ by n ( (w, xn , yn )− (w, xn , yn )). [sent-640, score-0.341]
</p><p>94 , zm } be a validation dataset, and let V be a validation dataset that differs from V in a single sample (zm vs zm ). [sent-672, score-0.538]
</p><p>95 We ﬁx a bin size h, a value of α, and a sequence R, and for these ﬁxed values, we use the notation ni and ni to denote the ˜ ˜ value of ni in Algorithm 5 when the inputs are T and T respectively. [sent-680, score-0.393]
</p><p>96 From standard composition of privacy it follows that (a) in alphaSplit is α-differentially private. [sent-728, score-0.493]
</p><p>97 Since a single change in V can change the number of errors any ﬁxed classiﬁer can make by at most 1 = ∆, we get that task (b) is α-differentially private for = α/2. [sent-730, score-0.523]
</p><p>98 If T and V are not disjoint, by standard composition of privacy we get that both alphaSplit and dataSplit yield 2α-differential privacy. [sent-732, score-0.493]
</p><p>99 We can therefore simulate Random by ﬁrst choosing i∗ uniformly at random, and then computing fi at α-differential privacy, which by standard privacy composition is α-differentially private. [sent-736, score-0.513]
</p><p>100 0  alpha  q Stability  alphaSplit  dataSplit  Random  Control  Figure 2: A summary of 10 times 10-fold cross-validation selection of regularizer index i into Θ for different privacy levels α. [sent-750, score-0.501]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('private', 0.523), ('privacy', 0.451), ('differentially', 0.413), ('validation', 0.234), ('pr', 0.2), ('stability', 0.137), ('alphasplit', 0.113), ('rj', 0.106), ('ni', 0.101), ('training', 0.094), ('datasplit', 0.093), ('differential', 0.093), ('roof', 0.092), ('zj', 0.084), ('supj', 0.084), ('gmax', 0.082), ('density', 0.078), ('zi', 0.077), ('nb', 0.073), ('ri', 0.072), ('laplace', 0.07), ('histogram', 0.066), ('score', 0.061), ('bin', 0.06), ('hi', 0.056), ('differ', 0.055), ('magic', 0.054), ('procedure', 0.054), ('adult', 0.053), ('ti', 0.05), ('auc', 0.048), ('classi', 0.048), ('labelled', 0.045), ('budget', 0.045), ('ga', 0.043), ('composition', 0.042), ('perturbation', 0.041), ('fa', 0.04), ('na', 0.039), ('ramp', 0.036), ('xn', 0.035), ('dj', 0.035), ('zm', 0.035), ('output', 0.034), ('equation', 0.034), ('zk', 0.033), ('theorem', 0.031), ('etz', 0.031), ('hmin', 0.031), ('oplr', 0.031), ('inputs', 0.03), ('regularizer', 0.03), ('er', 0.029), ('event', 0.028), ('estimator', 0.028), ('regularized', 0.028), ('yn', 0.027), ('parameter', 0.027), ('apriori', 0.027), ('argminw', 0.027), ('lemma', 0.027), ('condition', 0.027), ('rl', 0.027), ('rd', 0.025), ('ei', 0.025), ('person', 0.024), ('tuple', 0.024), ('chaudhuri', 0.024), ('yi', 0.023), ('mechanism', 0.023), ('side', 0.023), ('logistic', 0.023), ('dwork', 0.022), ('guarantees', 0.022), ('procedures', 0.022), ('outperformed', 0.022), ('pi', 0.022), ('disjoint', 0.022), ('tl', 0.021), ('dx', 0.021), ('ers', 0.021), ('holds', 0.021), ('hinge', 0.021), ('convex', 0.021), ('loss', 0.021), ('cryptographically', 0.021), ('indiciate', 0.021), ('ise', 0.021), ('jitter', 0.021), ('sensitive', 0.02), ('fi', 0.02), ('list', 0.02), ('outputs', 0.02), ('mcsherry', 0.02), ('explosion', 0.02), ('alpha', 0.02), ('mse', 0.02), ('zs', 0.019), ('observe', 0.019), ('dk', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="14-tfidf-1" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>2 0.59100688 <a title="14-tfidf-2" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>Author: John Duchi, Martin J. Wainwright, Michael Jordan</p><p>Abstract: We provide a detailed study of the estimation of probability distributions— discrete and continuous—in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental trade-offs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efﬁciency continuum. One of the consequences of our results is that Warner’s classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents. 1</p><p>3 0.44331995 <a title="14-tfidf-3" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>4 0.27167112 <a title="14-tfidf-4" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>5 0.14003403 <a title="14-tfidf-5" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>Author: Krzysztof M. Choromanski, Tony Jebara, Kui Tang</p><p>Abstract: The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of k-anonymity to the b-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results conﬁrm improved utility on benchmark and social data-sets.</p><p>6 0.10657256 <a title="14-tfidf-6" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>7 0.065733723 <a title="14-tfidf-7" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>8 0.064713798 <a title="14-tfidf-8" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>9 0.06410981 <a title="14-tfidf-9" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>10 0.06375891 <a title="14-tfidf-10" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>11 0.058882874 <a title="14-tfidf-11" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>12 0.050724249 <a title="14-tfidf-12" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>13 0.050664969 <a title="14-tfidf-13" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>14 0.050561391 <a title="14-tfidf-14" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>15 0.047375515 <a title="14-tfidf-15" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>16 0.04702713 <a title="14-tfidf-16" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>17 0.046054199 <a title="14-tfidf-17" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>18 0.045209229 <a title="14-tfidf-18" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>19 0.044838402 <a title="14-tfidf-19" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>20 0.042807668 <a title="14-tfidf-20" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, 0.022), (2, 0.105), (3, -0.113), (4, 0.077), (5, 0.059), (6, -0.021), (7, -0.004), (8, 0.064), (9, 0.458), (10, 0.24), (11, -0.078), (12, -0.244), (13, 0.349), (14, -0.072), (15, 0.167), (16, 0.137), (17, -0.061), (18, 0.019), (19, -0.044), (20, 0.164), (21, -0.06), (22, 0.095), (23, -0.052), (24, 0.021), (25, 0.037), (26, 0.131), (27, -0.03), (28, 0.027), (29, 0.034), (30, -0.06), (31, 0.014), (32, -0.029), (33, 0.009), (34, 0.022), (35, -0.018), (36, -0.047), (37, -0.057), (38, -0.01), (39, -0.008), (40, -0.02), (41, 0.034), (42, 0.067), (43, -0.014), (44, 0.003), (45, -0.028), (46, 0.038), (47, -0.042), (48, -0.027), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93922335 <a title="14-lsi-1" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>2 0.88215041 <a title="14-lsi-2" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>Author: John Duchi, Martin J. Wainwright, Michael Jordan</p><p>Abstract: We provide a detailed study of the estimation of probability distributions— discrete and continuous—in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental trade-offs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efﬁciency continuum. One of the consequences of our results is that Warner’s classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents. 1</p><p>3 0.75351298 <a title="14-lsi-3" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>4 0.57756442 <a title="14-lsi-4" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>5 0.5692783 <a title="14-lsi-5" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>Author: Krzysztof M. Choromanski, Tony Jebara, Kui Tang</p><p>Abstract: The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of k-anonymity to the b-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results conﬁrm improved utility on benchmark and social data-sets.</p><p>6 0.28754875 <a title="14-lsi-6" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>7 0.26819733 <a title="14-lsi-7" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>8 0.25776955 <a title="14-lsi-8" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>9 0.25226751 <a title="14-lsi-9" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>10 0.23758315 <a title="14-lsi-10" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>11 0.22804342 <a title="14-lsi-11" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>12 0.22213495 <a title="14-lsi-12" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>13 0.21938342 <a title="14-lsi-13" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>14 0.20930293 <a title="14-lsi-14" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>15 0.20265666 <a title="14-lsi-15" href="./nips-2013-Estimation%2C_Optimization%2C_and_Parallelism_when_Data_is_Sparse.html">111 nips-2013-Estimation, Optimization, and Parallelism when Data is Sparse</a></p>
<p>16 0.2015937 <a title="14-lsi-16" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>17 0.20096287 <a title="14-lsi-17" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>18 0.20082293 <a title="14-lsi-18" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>19 0.2002811 <a title="14-lsi-19" href="./nips-2013-Parametric_Task_Learning.html">244 nips-2013-Parametric Task Learning</a></p>
<p>20 0.19996282 <a title="14-lsi-20" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (16, 0.055), (33, 0.115), (34, 0.114), (41, 0.013), (46, 0.01), (49, 0.041), (56, 0.097), (70, 0.064), (76, 0.232), (85, 0.04), (89, 0.041), (93, 0.041), (95, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82910401 <a title="14-lda-1" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>Author: Mahito Sugiyama, Karsten Borgwardt</p><p>Abstract: Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efﬁciency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search. 1</p><p>2 0.79624623 <a title="14-lda-2" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>Author: Yifei Ma, Roman Garnett, Jeff Schneider</p><p>Abstract: A common classiﬁer for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes, equivalent to the harmonic predictor on Gaussian random ﬁelds (GRFs). For active learning on GRFs, the commonly used V-optimality criterion queries nodes that reduce the L2 (regression) loss. V-optimality satisﬁes a submodularity property showing that greedy reduction produces a (1 − 1/e) globally optimal solution. However, L2 loss may not characterise the true nature of 0/1 loss in classiﬁcation problems and thus may not be the best choice for active learning. We consider a new criterion we call Σ-optimality, which queries the node that minimizes the sum of the elements in the predictive covariance. Σ-optimality directly optimizes the risk of the surveying problem, which is to determine the proportion of nodes belonging to one class. In this paper we extend submodularity guarantees from V-optimality to Σ-optimality using properties speciﬁc to GRFs. We further show that GRFs satisfy the suppressor-free condition in addition to the conditional independence inherited from Markov random ﬁelds. We test Σoptimality on real-world graphs with both synthetic and real data and show that it outperforms V-optimality and other related methods on classiﬁcation. 1</p><p>same-paper 3 0.78280663 <a title="14-lda-3" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>4 0.77293628 <a title="14-lda-4" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<p>Author: Kohei Hayashi, Ryohei Fujimaki</p><p>Abstract: This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models (LFMs). FAB inference has not been applicable to models, including LFMs, without a speciﬁc condition on the Hessian matrix of a complete loglikelihood, which is required to derive a “factorized information criterion” (FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models. FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identiﬁability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efﬁciency. 1</p><p>5 0.75311446 <a title="14-lda-5" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>Author: Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles Isbell, Andrea L. Thomaz</p><p>Abstract: A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.</p><p>6 0.67542851 <a title="14-lda-6" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>7 0.67432272 <a title="14-lda-7" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>8 0.66239727 <a title="14-lda-8" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>9 0.658948 <a title="14-lda-9" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>10 0.65748602 <a title="14-lda-10" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>11 0.65563542 <a title="14-lda-11" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>12 0.65524727 <a title="14-lda-12" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>13 0.65255386 <a title="14-lda-13" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>14 0.65254861 <a title="14-lda-14" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>15 0.652031 <a title="14-lda-15" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>16 0.65142083 <a title="14-lda-16" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>17 0.65127337 <a title="14-lda-17" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>18 0.64930362 <a title="14-lda-18" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>19 0.64899707 <a title="14-lda-19" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>20 0.6483314 <a title="14-lda-20" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
