<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-116" href="#">nips2013-116</a> knowledge-graph by maker-knowledge-mining</p><h1>116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</h1>
<br/><p>Source: <a title="nips-2013-116-pdf" href="http://papers.nips.cc/paper/5136-fantope-projection-and-selection-a-near-optimal-convex-relaxation-of-sparse-pca.pdf">pdf</a></p><p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>Reference: <a title="nips-2013-116-reference" href="../nips2013_reference/nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fantop', 0.403), ('princip', 0.336), ('subspac', 0.311), ('pca', 0.232), ('adm', 0.218), ('kendal', 0.203), ('dspca', 0.202), ('tau', 0.195), ('fps', 0.178), ('eigenvalu', 0.163), ('spars', 0.121), ('semid', 0.115), ('minimax', 0.114), ('eigenvect', 0.108), ('pf', 0.102), ('transellipt', 0.101), ('xjj', 0.101), ('jj', 0.1), ('thresholding', 0.098), ('corol', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="116-tfidf-1" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>2 0.27112302 <a title="116-tfidf-2" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>3 0.2378267 <a title="116-tfidf-3" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>4 0.23468569 <a title="116-tfidf-4" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>5 0.22286184 <a title="116-tfidf-5" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>6 0.21773767 <a title="116-tfidf-6" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>7 0.21627033 <a title="116-tfidf-7" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>8 0.19385983 <a title="116-tfidf-8" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>9 0.16706169 <a title="116-tfidf-9" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>10 0.16042653 <a title="116-tfidf-10" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>11 0.14211613 <a title="116-tfidf-11" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>12 0.13910484 <a title="116-tfidf-12" href="./nips-2013-Robust_Sparse_Principal_Component_Regression_under_the_High_Dimensional_Elliptical_Model.html">283 nips-2013-Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model</a></p>
<p>13 0.1372833 <a title="116-tfidf-13" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>14 0.13047947 <a title="116-tfidf-14" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>15 0.12967655 <a title="116-tfidf-15" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>16 0.12471118 <a title="116-tfidf-16" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>17 0.11831351 <a title="116-tfidf-17" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>18 0.1105968 <a title="116-tfidf-18" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>19 0.10886102 <a title="116-tfidf-19" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>20 0.1066907 <a title="116-tfidf-20" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.126), (2, 0.081), (3, 0.044), (4, 0.116), (5, 0.245), (6, -0.142), (7, 0.012), (8, -0.103), (9, 0.246), (10, -0.149), (11, 0.007), (12, 0.102), (13, 0.154), (14, -0.031), (15, -0.01), (16, -0.131), (17, -0.006), (18, 0.072), (19, -0.029), (20, 0.029), (21, -0.063), (22, -0.026), (23, 0.055), (24, 0.121), (25, -0.015), (26, 0.078), (27, -0.039), (28, 0.066), (29, 0.008), (30, -0.032), (31, 0.001), (32, -0.021), (33, 0.076), (34, 0.045), (35, -0.099), (36, -0.046), (37, -0.027), (38, 0.065), (39, -0.006), (40, -0.013), (41, 0.037), (42, 0.026), (43, 0.001), (44, -0.023), (45, -0.02), (46, 0.069), (47, 0.006), (48, -0.055), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92739576 <a title="116-lsi-1" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>2 0.79418892 <a title="116-lsi-2" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>3 0.75781566 <a title="116-lsi-3" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>4 0.74644363 <a title="116-lsi-4" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>5 0.71373242 <a title="116-lsi-5" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>6 0.69643605 <a title="116-lsi-6" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>7 0.67438692 <a title="116-lsi-7" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>8 0.6540001 <a title="116-lsi-8" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>9 0.63844198 <a title="116-lsi-9" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>10 0.63537884 <a title="116-lsi-10" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>11 0.58284491 <a title="116-lsi-11" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>12 0.57398927 <a title="116-lsi-12" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>13 0.54975802 <a title="116-lsi-13" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>14 0.54792118 <a title="116-lsi-14" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>15 0.53933269 <a title="116-lsi-15" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>16 0.53429168 <a title="116-lsi-16" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>17 0.52636987 <a title="116-lsi-17" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>18 0.51657945 <a title="116-lsi-18" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>19 0.50970352 <a title="116-lsi-19" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>20 0.50681585 <a title="116-lsi-20" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.171), (14, 0.01), (20, 0.176), (25, 0.17), (37, 0.068), (70, 0.053), (80, 0.075), (86, 0.11), (87, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91016322 <a title="116-lda-1" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>2 0.89637959 <a title="116-lda-2" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>same-paper 3 0.89025146 <a title="116-lda-3" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>4 0.85469115 <a title="116-lda-4" href="./nips-2013-Estimation%2C_Optimization%2C_and_Parallelism_when_Data_is_Sparse.html">111 nips-2013-Estimation, Optimization, and Parallelism when Data is Sparse</a></p>
<p>5 0.85287672 <a title="116-lda-5" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>6 0.84949166 <a title="116-lda-6" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>7 0.84899622 <a title="116-lda-7" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>8 0.84855181 <a title="116-lda-8" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>9 0.84715682 <a title="116-lda-9" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>10 0.84595054 <a title="116-lda-10" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>11 0.84572309 <a title="116-lda-11" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>12 0.8450948 <a title="116-lda-12" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>13 0.84503388 <a title="116-lda-13" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>14 0.84440434 <a title="116-lda-14" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>15 0.84440392 <a title="116-lda-15" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>16 0.84403312 <a title="116-lda-16" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>17 0.84318334 <a title="116-lda-17" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>18 0.84281588 <a title="116-lda-18" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>19 0.84255481 <a title="116-lda-19" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>20 0.84203345 <a title="116-lda-20" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
