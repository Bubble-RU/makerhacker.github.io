<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-18" href="#">nips2013-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</h1>
<br/><p>Source: <a title="nips-2013-18-pdf" href="http://papers.nips.cc/paper/4880-a-simple-example-of-dirichlet-process-mixture-inconsistency-for-the-number-of-components.pdf">pdf</a></p><p>Author: Jeffrey W. Miller, Matthew T. Harrison</p><p>Abstract: For data assumed to come from a ﬁnite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of clusters — that is, the posterior on the number of components represented in the observed data. However, it turns out that this posterior is not consistent — it does not concentrate at the true number of components. In this note, we give an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a “mixture” with one standard normal component. Further, we show that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster converges (in probability) to 0. 1</p><p>Reference: <a title="nips-2013-18-reference" href="../nips2013_reference/nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A simple example of Dirichlet process mixture inconsistency for the number of components  Jeffrey W. [sent-1, score-0.519]
</p><p>2 Miller Division of Applied Mathematics Brown University Providence, RI 02912 jeffrey miller@brown. [sent-2, score-0.045]
</p><p>3 Harrison Division of Applied Mathematics Brown University Providence, RI 02912 matthew harrison@brown. [sent-4, score-0.033]
</p><p>4 edu  Abstract For data assumed to come from a ﬁnite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. [sent-5, score-0.423]
</p><p>5 The typical approach is to use the posterior distribution on the number of clusters — that is, the posterior on the number of components represented in the observed data. [sent-6, score-0.58]
</p><p>6 However, it turns out that this posterior is not consistent — it does not concentrate at the true number of components. [sent-7, score-0.208]
</p><p>7 In this note, we give an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a “mixture” with one standard normal component. [sent-8, score-0.679]
</p><p>8 Further, we show that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster converges (in probability) to 0. [sent-9, score-0.17]
</p><p>9 1  Introduction  It is well-known that Dirichlet process mixtures (DPMs) of normals are consistent for the density — that is, given data from a sufﬁciently regular density p0 the posterior converges to the point mass at p0 (see [1] for details and references). [sent-10, score-0.588]
</p><p>10 However, it is easy to see that this does not necessarily imply consistency for the number of components, since for example, a good estimate of the density might include superﬂuous components having vanishingly small weight. [sent-11, score-0.292]
</p><p>11 Of course, if the data-generating process very closely resembles the DPM model, then it is ﬁne to use this posterior for inferences about the number of clusters (but beware of misspeciﬁcation; see Section 2). [sent-13, score-0.396]
</p><p>12 However, in the examples cited, the authors evaluated the performance of their methods on data simulated from a ﬁxed ﬁnite number of components or populations, suggesting that they found this to be more realistic than a DPM for their applications. [sent-14, score-0.151]
</p><p>13 Therefore, it is important to understand the behavior of this posterior when the data comes from a ﬁnite mixture — in particular, does it concentrate at the true number of components? [sent-15, score-0.331]
</p><p>14 In this note, we give a simple example in which a DPM is applied to data from a ﬁnite mixture and the posterior distribution on the number of clusters does not concentrate at the true number of components. [sent-16, score-0.498]
</p><p>15 In fact, DPMs exhibit this type of inconsistency under very general conditions [7] — however, the aim of this note is brevity and clarity. [sent-17, score-0.234]
</p><p>16 To that end, we focus our attention on a special case that is as 1  Figure 1: Prior (red x) and estimated posterior (blue o) of the number of clusters in the observed 2 data, for a univariate normal DPM on n i. [sent-18, score-0.497]
</p><p>17 5 2 The DPM had concentration parameter α = 1 and a Normal–Gamma base measure on the mean and precision: N (µ | 0, 1/cλ)Gamma(λ | a, b) with a = 1, b = 0. [sent-22, score-0.032]
</p><p>18 Estimates were made using a collapsed Gibbs sampler, with 104 burn-in sweeps and 105 sample sweeps; traceplots and running averages were used as convergence diagnostics. [sent-25, score-0.055]
</p><p>19 simple as possible: a “standard normal DPM”, that is, a DPM using univariate normal components of unit variance, with a standard normal base measure (prior on component means). [sent-27, score-0.691]
</p><p>20 In Section 2, we address several pertinent questions and consider some suggestive experimental evidence. [sent-29, score-0.056]
</p><p>21 In Section 4, we give an elementary proof of inconsistency in the case of a standard normal DPM on data from one component, and in Section 5, we show that on standard normal data, a standard normal DPM is in fact severely inconsistent. [sent-31, score-0.675]
</p><p>22 2  Discussion  It should be emphasized that these results do not diminish, in any way, the utility of Dirichlet process mixtures as a ﬂexible prior on densities, i. [sent-32, score-0.18]
</p><p>23 In addition to their widespread success in empirical studies, DPMs are backed by theoretical guarantees showing that in many cases the posterior on the density concentrates at the true density at the minimax-optimal rate, up to a logarithmic factor (see [1] and references therein). [sent-35, score-0.387]
</p><p>24 [8, 9], among others) have empirically observed that the DPM posterior on the number of clusters tends to overestimate the number of components, in the sense that it tends to put its mass on a range of values greater or equal to the true number. [sent-38, score-0.34]
</p><p>25 Figure 1 illustrates this effect for univariate normals, and similar experiments with different families of component distributions yield similar results. [sent-39, score-0.099]
</p><p>26 It is natural to think that this overestimation is due to the fact that the prior on the number of clusters diverges as n → ∞, at a log n rate. [sent-41, score-0.24]
</p><p>27 However, this does not seem to be the main issue — rather, the problem is that DPMs strongly prefer having some tiny clusters and will introduce extra clusters even when they are not needed (see [7] for an intuitive explanation of why this is the case). [sent-42, score-0.419]
</p><p>28 2  In fact, many researchers have observed the presence of tiny extra clusters (e. [sent-43, score-0.297]
</p><p>29 [8, 9]), but the reason for this has not previously been well understood, often being incorrectly attributed to the difﬁculty of detecting components with small weight. [sent-45, score-0.151]
</p><p>30 These tiny extra clusters are rather inconvenient, especially in clustering applications, and are often dealt with in an ad hoc way by simply removing them. [sent-46, score-0.283]
</p><p>31 It might be possible to consistently estimate the number of components in this way, but this remains an open question. [sent-47, score-0.151]
</p><p>32 A more natural solution is the following: if the number of components is unknown, put a prior on the number of components. [sent-48, score-0.194]
</p><p>33 For example, draw the number of components s from a probability mass function p(s) on {1, 2, . [sent-49, score-0.23]
</p><p>34 } with p(s) > 0 for all s, draw mixing weights π = (π1 , . [sent-52, score-0.037]
</p><p>35 , πs ) (given s), draw component parameters θ1 , . [sent-55, score-0.084]
</p><p>36 (given s and π) from an appropriate prior, and draw X1 , X2 , . [sent-61, score-0.037]
</p><p>37 Under certain conditions, the posterior on the density has been shown to concentrate at the true density at the minimax-optimal rate, up to a logarithmic factor, for any sufﬁciently regular true density [14]. [sent-69, score-0.547]
</p><p>38 Subject to a modiﬁcation of this sort, it can be shown (see [10]) that under very general conditions, when the data is from a ﬁnite mixture of the chosen family, such models are (a. [sent-71, score-0.123]
</p><p>39 ) consistent for the number of components, the mixing weights, the component parameters, and the density. [sent-73, score-0.047]
</p><p>40 However, as a practical matter, when dealing with real-world data, one would not expect to ﬁnd data coming exactly from a ﬁnite mixture of a known family (except, perhaps, in rare circumstances). [sent-75, score-0.155]
</p><p>41 Unfortunately, even for a model as in the preceding paragraph, the posterior on the number of components will typically be highly sensitive to misspeciﬁcation, and it seems likely that in order to obtain robust estimators, the problem itself may need to be reformulated. [sent-76, score-0.282]
</p><p>42 We urge researchers interested in the number of components to be wary of this robustness issue, and to think carefully about whether they really need to estimate the number of components, or whether some other measure of heterogeneity will sufﬁce. [sent-77, score-0.265]
</p><p>43 3  Setup  In this section, we deﬁne the Dirichlet process mixture model under consideration. [sent-78, score-0.171]
</p><p>44 1  Dirichlet process mixture model  The DPM model was introduced by Ferguson [16] and Lo [17] for the purpose of Bayesian density estimation, and was made practical through the efforts of several authors (see [18] and references therein). [sent-80, score-0.284]
</p><p>45 The core of the DPM is the so-called Chinese restaurant process (CRP), which deﬁnes a certain probability distribution on partitions. [sent-82, score-0.048]
</p><p>46 , n}, let At (n) denote the set of all ordered partitions (A1 , . [sent-89, score-0.063]
</p><p>47 i=1  The CRP with concentration parameter α > 0 deﬁnes a probability mass function on A(n) = n t=1 At (n) by setting p(A) =  t  αt  (|Ai | − 1)! [sent-106, score-0.074]
</p><p>48 (It is more common to see this distribution deﬁned in terms of unordered partitions {A1 , . [sent-110, score-0.063]
</p><p>49 does not appear in the denominator — however, for our purposes it is more convenient to use the distribution on ordered partitions (A1 , . [sent-114, score-0.1]
</p><p>50 This does not affect the prior or posterior on t. [sent-118, score-0.174]
</p><p>51 ) 3  Consider the hierarchical model p(A, t) = p(A) =  αt  t  (|Ai | − 1)! [sent-119, score-0.04]
</p><p>52 1)  t  p(θ1:t | A, t) =  π(θi ), and i=1 t  p(x1:n | θ1:t , A, t) =  pθi (xj ), i=1 j∈Ai  where π(θ) is a prior on component parameters θ ∈ Θ, and {pθ : θ ∈ Θ} is a parametrized family of distributions on x ∈ X for the components. [sent-122, score-0.09]
</p><p>53 This hierarchical model is referred to as a Dirichlet process mixture (DPM) model. [sent-131, score-0.211]
</p><p>54 The prior on the number of clusters t under this model is pn (t) = A∈At (n) p(A, t). [sent-132, score-0.21]
</p><p>55 We use Tn (rather than T ) to denote the random variable representing the number of clusters, as a reminder that its distribution depends on n. [sent-133, score-0.028]
</p><p>56 Note that we distinguish between the terms “component” and ∞ “cluster”: a component is part of a mixture distribution (e. [sent-134, score-0.17]
</p><p>57 a mixture i=1 πi pθi has components pθ1 , pθ2 , . [sent-136, score-0.274]
</p><p>58 ), while a cluster is the set of indices of data points coming from a given component (e. [sent-139, score-0.079]
</p><p>59 , n}, we denote xS = (xj : j ∈ S) and let m(xS ) denote the single-cluster marginal of xS , m(xS ) =  pθ (xj ) π(θ) dθ. [sent-149, score-0.028]
</p><p>60 2 2π  and  It is a straightforward calculation to show that the single-cluster marginal is then m(x1:n ) = √  1 1 1 p0 (x1:n ) exp 2n+1 n+1  n  2  xj  ,  (3. [sent-153, score-0.188]
</p><p>61 When pθ (x) and π(θ) are as above, we refer to the resulting DPM as a standard normal DPM. [sent-155, score-0.147]
</p><p>62 4  4  Simple example of inconsistency  In this section, we prove the following result, exhibiting a simple example in which a DPM is inconsistent for the number of components: even when the true number of components is 1 (e. [sent-156, score-0.348]
</p><p>63 N (µ, 1) data), the posterior probability of Tn = 1 does not converge to 1. [sent-158, score-0.131]
</p><p>64 from any distribution with E|Xi | < ∞, then with probability 1, under the standard normal DPM with α = 1 as deﬁned above, p(Tn = 1 | X1:n ) does not converge to 1 as n → ∞. [sent-172, score-0.147]
</p><p>65 , xn ∈ R, A ∈ A2 (n), and ai = |Ai | for i = 1, 2. [sent-181, score-0.12]
</p><p>66 n Deﬁne sn = j=1 xj and sAi = j∈Ai xj for i = 1, 2. [sent-182, score-0.199]
</p><p>67 4 and noting that 1/(n + 1) ≤ 1/(n + 2) + 1/n2 , we have √ 1 s2 1 s2 m(x1:n ) 1 s2 n n n ≤ exp exp . [sent-184, score-0.152]
</p><p>68 n+1 = exp p0 (x1:n ) 2n+1 2n+2 2 n2 1 The second factor equals exp( 2 x2 ), where xn = n  a1 + 1 sA1 sn 2 ≤ n+2 n + 2 a1 + 1 and thus, the ﬁrst factor is less or equal to exp  n j=1  1 n 2  +  xj . [sent-185, score-0.298]
</p><p>69 1, with α = 1), and step (d) follows since there are n partitions A ∈ A2 (n) such that |A1 | = 1. [sent-202, score-0.063]
</p><p>70 with µ = EXj ﬁnite, then by the law of large numbers, X n = n 1 j=1 Xj → µ almost surely as n → ∞. [sent-209, score-0.107]
</p><p>71 5  5  Severe inconsistency  In the previous section, we showed that p(Tn = 1 | X1:n ) does not converge to 1 for a standard normal DPM on any data with ﬁnite mean. [sent-214, score-0.344]
</p><p>72 In this section, we prove that in fact, it converges to 0, at least on standard normal data. [sent-215, score-0.147]
</p><p>73 This vividly illustrates that improperly using DPMs in this way can lead to entirely misleading results. [sent-216, score-0.03]
</p><p>74 The key step in the proof is an application of Hoeffding’s strong law of large numbers for U-statistics. [sent-217, score-0.093]
</p><p>75 then Pr  p(Tn = 1 | X1:n ) − 0 →  as n → ∞  under the standard normal DPM with concentration parameter α = 1. [sent-226, score-0.179]
</p><p>76 , n} with |S| ≥ 1, deﬁne h(xS ) by h(xS ) =  m(xS ) = p0 (xS )  1 |S| + 1  exp  1 1 2 |S| + 1  2  xj  ,  j∈S  where m√ the single-cluster marginal as in Equations 3. [sent-239, score-0.188]
</p><p>77 Note also that Eh(XS ) = 1 since Eh(XS ) =  h(xS ) p0 (xS ) dxS =  m(xS ) dxS = 1,  using the fact that m(xS ) is a density with respect to Lebesgue measure. [sent-243, score-0.113]
</p><p>78 By Hoeffding’s strong law of large numbers for U-statistics [19], a. [sent-251, score-0.093]
</p><p>79 k=1  1 HK log K = > 2k 2 2  th  where HK is the K harmonic number, and the last inequality follows from the standard bounds [20] on harmonic numbers: log K < HK ≤ log K + 1. [sent-268, score-0.07]
</p><p>80 4, we have R1 (X1:n ) = n3/2 √  =√  p(X1:n , Tn = 1) √ m(X1:n ) = n p0 (X1:n ) p0 (X1:n )  1 n n exp 2n+1 n+1  1 √ n  n  2  Xi  2 ≤ exp(Zn /2)  i=1  √ n where Zn = (1/ n) i=1 Xi ∼ N (0, 1) for each n ∈ {1, 2, . [sent-277, score-0.076]
</p><p>81 Acknowledgments We would like to thank Stu Geman for raising this question, and the anonymous referees for several helpful suggestions that improved the quality of this manuscript. [sent-283, score-0.028]
</p><p>82 Inference of population structure under a Dirichlet process model. [sent-300, score-0.048]
</p><p>83 Bayesian inﬁnite mixture model based clustering of gene expression proﬁles. [sent-305, score-0.154]
</p><p>84 A nonparametric Bayesian approach to detect the number of regimes in Markov switching models. [sent-311, score-0.038]
</p><p>85 Bayesian multi-population haplotype inference via a hierarchical Dirichlet process mixture. [sent-322, score-0.118]
</p><p>86 Particle ﬁlters for mixture models with an unknown number of components. [sent-326, score-0.123]
</p><p>87 Inconsistency of Pitman–Yor process mixtures for the number of components. [sent-333, score-0.137]
</p><p>88 Hierarchical priors and mixture models, with applicau tion in regression and density estimation. [sent-341, score-0.264]
</p><p>89 Characterization of a Bayesian genetic clustering algorithm based on a Dirichlet process prior and comparison among Bayesian clustering methods. [sent-347, score-0.153]
</p><p>90 On Bayesian analysis of mixtures with an unknown number of components. [sent-357, score-0.089]
</p><p>91 Bayesian ﬁnite mixtures with an unknown number of components: The allocation sampler. [sent-370, score-0.089]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dpm', 0.543), ('tn', 0.413), ('xs', 0.273), ('dpms', 0.21), ('inconsistency', 0.197), ('clusters', 0.167), ('components', 0.151), ('normal', 0.147), ('posterior', 0.131), ('mixture', 0.123), ('dirichlet', 0.123), ('density', 0.113), ('ai', 0.089), ('mixtures', 0.089), ('bayesian', 0.084), ('xj', 0.084), ('concentrate', 0.077), ('exp', 0.076), ('eh', 0.074), ('op', 0.072), ('partitions', 0.063), ('ller', 0.062), ('dxs', 0.06), ('law', 0.059), ('harrison', 0.055), ('sweeps', 0.055), ('tiny', 0.053), ('univariate', 0.052), ('normals', 0.052), ('miller', 0.051), ('uk', 0.05), ('inferences', 0.05), ('providence', 0.049), ('hk', 0.048), ('process', 0.048), ('surely', 0.048), ('component', 0.047), ('hoeffding', 0.047), ('jeffrey', 0.045), ('researchers', 0.045), ('zn', 0.045), ('misspeci', 0.044), ('crp', 0.044), ('prior', 0.043), ('mass', 0.042), ('hierarchical', 0.04), ('heterogeneity', 0.039), ('severe', 0.039), ('nonparametric', 0.038), ('brevity', 0.037), ('draw', 0.037), ('populations', 0.037), ('denominator', 0.037), ('elementary', 0.037), ('division', 0.036), ('harmonic', 0.035), ('numbers', 0.034), ('editors', 0.034), ('therein', 0.034), ('bioinformatics', 0.033), ('matthew', 0.033), ('coming', 0.032), ('concentration', 0.032), ('extra', 0.032), ('pr', 0.032), ('brown', 0.032), ('xn', 0.031), ('sn', 0.031), ('clustering', 0.031), ('backed', 0.03), ('escobar', 0.03), ('exj', 0.03), ('ferguson', 0.03), ('haplotype', 0.03), ('improperly', 0.03), ('inconvenient', 0.03), ('medvedovic', 0.03), ('overestimation', 0.03), ('sohn', 0.03), ('stu', 0.03), ('urge', 0.03), ('xai', 0.03), ('marginal', 0.028), ('gamma', 0.028), ('nite', 0.028), ('nitely', 0.028), ('equation', 0.028), ('applicau', 0.028), ('carolina', 0.028), ('dey', 0.028), ('diminish', 0.028), ('hjort', 0.028), ('pertinent', 0.028), ('pitman', 0.028), ('raising', 0.028), ('reminder', 0.028), ('suggestive', 0.028), ('uous', 0.028), ('vanishingly', 0.028), ('yor', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="18-tfidf-1" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>Author: Jeffrey W. Miller, Matthew T. Harrison</p><p>Abstract: For data assumed to come from a ﬁnite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of clusters — that is, the posterior on the number of components represented in the observed data. However, it turns out that this posterior is not consistent — it does not concentrate at the true number of components. In this note, we give an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a “mixture” with one standard normal component. Further, we show that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster converges (in probability) to 0. 1</p><p>2 0.136917 <a title="18-tfidf-2" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>Author: Dahua Lin</p><p>Abstract: Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm – random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the ﬂy when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efﬁciency – orders of magnitude speed-up compared to the state-of-the-art. 1</p><p>3 0.12712795 <a title="18-tfidf-3" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>Author: Samory Kpotufe, Francesco Orabona</p><p>Abstract: We consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time. We prove that it is possible to maintain such a structure in time O (log n) at ˜ any time step n while achieving a nearly-optimal regression rate of O n−2/(2+d) in terms of the unknown metric dimension d. Finally we prove a new regression lower-bound which is independent of a given data size, and hence is more appropriate for the streaming setting. 1</p><p>4 0.12471783 <a title="18-tfidf-4" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>5 0.12328844 <a title="18-tfidf-5" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu</p><p>Abstract: Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its inﬁnite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution. While this model can accommodate a wider range of conditional dependencies, some limitations still remain. To address this, we investigate two additional novel variants of the Poisson distribution and their corresponding joint graphical model distributions. Our three novel approaches provide classes of Poisson-like graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our models via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-sequencing data. 1</p><p>6 0.12229028 <a title="18-tfidf-6" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>7 0.10140637 <a title="18-tfidf-7" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>8 0.089154281 <a title="18-tfidf-8" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>9 0.087085381 <a title="18-tfidf-9" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>10 0.085651442 <a title="18-tfidf-10" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<p>11 0.081387222 <a title="18-tfidf-11" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>12 0.079898819 <a title="18-tfidf-12" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>13 0.072862633 <a title="18-tfidf-13" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>14 0.071528621 <a title="18-tfidf-14" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>15 0.06949351 <a title="18-tfidf-15" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>16 0.063263677 <a title="18-tfidf-16" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>17 0.06119312 <a title="18-tfidf-17" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>18 0.060276058 <a title="18-tfidf-18" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>19 0.058520734 <a title="18-tfidf-19" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>20 0.058275275 <a title="18-tfidf-20" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.154), (1, 0.051), (2, -0.015), (3, -0.007), (4, -0.007), (5, 0.119), (6, 0.146), (7, 0.021), (8, 0.029), (9, -0.022), (10, -0.034), (11, 0.043), (12, -0.014), (13, 0.067), (14, 0.011), (15, 0.03), (16, 0.106), (17, -0.067), (18, -0.023), (19, 0.101), (20, 0.042), (21, 0.174), (22, -0.026), (23, -0.017), (24, -0.116), (25, 0.056), (26, -0.044), (27, -0.054), (28, -0.047), (29, -0.018), (30, -0.036), (31, -0.056), (32, 0.068), (33, 0.002), (34, -0.033), (35, -0.002), (36, 0.114), (37, -0.003), (38, -0.177), (39, 0.051), (40, -0.044), (41, -0.082), (42, 0.036), (43, -0.035), (44, -0.0), (45, 0.02), (46, 0.01), (47, -0.079), (48, 0.012), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9530018 <a title="18-lsi-1" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>Author: Jeffrey W. Miller, Matthew T. Harrison</p><p>Abstract: For data assumed to come from a ﬁnite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of clusters — that is, the posterior on the number of components represented in the observed data. However, it turns out that this posterior is not consistent — it does not concentrate at the true number of components. In this note, we give an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a “mixture” with one standard normal component. Further, we show that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster converges (in probability) to 0. 1</p><p>2 0.63515741 <a title="18-lsi-2" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu</p><p>Abstract: Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its inﬁnite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution. While this model can accommodate a wider range of conditional dependencies, some limitations still remain. To address this, we investigate two additional novel variants of the Poisson distribution and their corresponding joint graphical model distributions. Our three novel approaches provide classes of Poisson-like graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our models via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-sequencing data. 1</p><p>3 0.62841272 <a title="18-lsi-3" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>4 0.62180704 <a title="18-lsi-4" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>Author: Dahua Lin</p><p>Abstract: Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm – random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the ﬂy when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efﬁciency – orders of magnitude speed-up compared to the state-of-the-art. 1</p><p>5 0.61260211 <a title="18-lsi-5" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>Author: Jason Lee, Ran Gilad-Bachrach, Rich Caruana</p><p>Abstract: In the mixture models problem it is assumed that there are K distributions θ1 , . . . , θK and one gets to observe a sample from a mixture of these distributions with unknown coeﬃcients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with diﬀerent mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the diﬀerences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data. 1</p><p>6 0.60221082 <a title="18-lsi-6" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>7 0.59139109 <a title="18-lsi-7" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>8 0.57895499 <a title="18-lsi-8" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>9 0.56967473 <a title="18-lsi-9" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>10 0.54687583 <a title="18-lsi-10" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>11 0.54228324 <a title="18-lsi-11" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>12 0.52295268 <a title="18-lsi-12" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>13 0.51722586 <a title="18-lsi-13" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>14 0.50704885 <a title="18-lsi-14" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<p>15 0.47092667 <a title="18-lsi-15" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>16 0.45899948 <a title="18-lsi-16" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>17 0.45473057 <a title="18-lsi-17" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>18 0.4519771 <a title="18-lsi-18" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>19 0.44503394 <a title="18-lsi-19" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>20 0.43604022 <a title="18-lsi-20" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.014), (8, 0.218), (16, 0.09), (33, 0.184), (34, 0.143), (36, 0.011), (41, 0.015), (49, 0.017), (56, 0.103), (70, 0.027), (85, 0.048), (89, 0.031), (93, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87872112 <a title="18-lda-1" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>Author: Jie Wang, Jiayu Zhou, Peter Wonka, Jieping Ye</p><p>Abstract: Lasso is a widely used regression technique to ﬁnd sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efﬁciency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efﬁcient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no “exact” screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso. 1</p><p>same-paper 2 0.852364 <a title="18-lda-2" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>Author: Jeffrey W. Miller, Matthew T. Harrison</p><p>Abstract: For data assumed to come from a ﬁnite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of clusters — that is, the posterior on the number of components represented in the observed data. However, it turns out that this posterior is not consistent — it does not concentrate at the true number of components. In this note, we give an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a “mixture” with one standard normal component. Further, we show that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster converges (in probability) to 0. 1</p><p>3 0.76797056 <a title="18-lda-3" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>Author: Dahua Lin</p><p>Abstract: Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm – random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the ﬂy when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efﬁciency – orders of magnitude speed-up compared to the state-of-the-art. 1</p><p>4 0.76649046 <a title="18-lda-4" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>Author: Francesca Petralia, Joshua T. Vogelstein, David Dunson</p><p>Abstract: Nonparametric estimation of the conditional distribution of a response given highdimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change ﬂexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efﬁciently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. 1</p><p>5 0.76155812 <a title="18-lda-5" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>Author: Michael Hughes, Erik Sudderth</p><p>Abstract: Variational inference algorithms provide the most effective framework for largescale training of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima. We present a new algorithm, memoized online variational inference, which scales to very large (yet ﬁnite) datasets while avoiding the complexities of stochastic gradient. Our algorithm maintains ﬁnite-dimensional sufﬁcient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples. Exploiting nested families of variational bounds for inﬁnite nonparametric models, we develop principled birth and merge moves allowing non-local optimization. Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed. Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.</p><p>6 0.7607969 <a title="18-lda-6" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>7 0.75872147 <a title="18-lda-7" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>8 0.75729007 <a title="18-lda-8" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>9 0.75722128 <a title="18-lda-9" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>10 0.75695777 <a title="18-lda-10" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>11 0.75356179 <a title="18-lda-11" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>12 0.7528652 <a title="18-lda-12" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>13 0.75279635 <a title="18-lda-13" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>14 0.75265557 <a title="18-lda-14" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>15 0.75218856 <a title="18-lda-15" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>16 0.75169086 <a title="18-lda-16" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>17 0.75161493 <a title="18-lda-17" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>18 0.75152558 <a title="18-lda-18" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>19 0.75128084 <a title="18-lda-19" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>20 0.75026947 <a title="18-lda-20" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
