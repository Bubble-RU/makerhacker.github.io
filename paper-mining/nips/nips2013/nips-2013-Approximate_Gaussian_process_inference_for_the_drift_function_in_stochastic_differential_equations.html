<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-39" href="#">nips2013-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</h1>
<br/><p>Source: <a title="nips-2013-39-pdf" href="http://papers.nips.cc/paper/4967-approximate-gaussian-process-inference-for-the-drift-function-in-stochastic-differential-equations.pdf">pdf</a></p><p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>Reference: <a title="nips-2013-39-reference" href="../nips2013_reference/nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Approximate Gaussian process inference for the drift of stochastic differential equations  Andreas Ruttor Computer Science, TU Berlin andreas. [sent-1, score-0.857]
</p><p>2 de  Abstract We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. [sent-7, score-0.955]
</p><p>3 Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. [sent-8, score-0.759]
</p><p>4 The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. [sent-9, score-0.849]
</p><p>5 1  Introduction  Gaussian process (GP) inference methods have been successfully applied to models for dynamical systems, see e. [sent-10, score-0.156]
</p><p>6 Usually, these studies have dealt with discrete time dynamics, where one uses a GP prior for modeling transition function and the measurement function of the system. [sent-13, score-0.08]
</p><p>7 On the other hand, many dynamical systems in the physical world evolve in continuous time and the noisy dynamics is described naturally in terms of stochastic differential equations (SDE). [sent-14, score-0.301]
</p><p>8 So far most inference approaches have dealt with the posterior prediction of state variables between observations (smoothing) and the estimation of parameters contained in the drift function, which governs the deterministic part of the microscopic time evolution. [sent-16, score-0.954]
</p><p>9 Since the drift is usually a nonlinear function of the state vector, a nonparametric estimation using Gaussian process priors would be a natural choice, when a large number of data is available. [sent-17, score-0.71]
</p><p>10 In fact, if an entire path of dense observations of the state dynamics is observed, the posterior process over the drift is exactly a GP. [sent-20, score-1.005]
</p><p>11 In [8] this sparse, incomplete observation case has been treated by a Gibbs sampler, which alternates between sampling complete state paths of the SDE and creating GP samples for the drift. [sent-22, score-0.101]
</p><p>12 Second, the densely sampled hidden paths are equivalent to a large number of imputed observations, for which the matrix inversions required by the GP posterior predictions can become computationally costly. [sent-24, score-0.181]
</p><p>13 It was shown in [8] that in the univariate case for GP priors with precision operators (the inverses of covariance kernels) which are differential operators efﬁcient predictions can be realized in terms of the solutions of differential equations. [sent-25, score-0.31]
</p><p>14 In this paper, we develop an alternative approximate expectation maximization (EM) method for inference from sparse observations, which is faster than the sampling approach and can also be applied to arbitrary kernels and multivariate SDEs. [sent-26, score-0.162]
</p><p>15 In the E-Step we approximate expectations over 1  state paths by those of a locally ﬁtted Ornstein-Uhlenbeck model. [sent-27, score-0.169]
</p><p>16 The M-step for computing the maximum posterior GP prediction of the drift depends on a continuum of function values and is thus approximated by a sparse GP. [sent-28, score-0.745]
</p><p>17 Section 2 introduces stochastic differential equations and section 3 discusses GP based inference for completely observed paths. [sent-30, score-0.218]
</p><p>18 2  Stochastic differential equations  We consider continuous-time univariate Markov processes of the diffusion type, where the dynamics of a d-dimensional state vector Xt ∈ Rd is given by the stochastic differential equation (SDE) dXt = f (Xt )dt + D1/2 dW. [sent-33, score-0.525]
</p><p>19 , f (x)) deﬁnes the deterministic drift and W is a Wiener process, which models additive white noise. [sent-37, score-0.587]
</p><p>20 D is the diffusion matrix, which we assume to be independent of x. [sent-38, score-0.135]
</p><p>21 where expectations over paths have been carried out and can be replaced by ordinary integrals. [sent-49, score-0.138]
</p><p>22 3  Bayesian Inference for dense observations  Suppose we observe a path of n d-dimensional observations X0:T = (Xt )t∈G over the time interval [0, T ]. [sent-50, score-0.233]
</p><p>23 Since for ∆t → 0, the transition probabilities of the process are Gaussian, pf (X0:T |f ) ∝ exp −  1 2∆t  t  ||Xt+∆t − Xt − f (Xt )∆t||  2  ,  (3)  . [sent-51, score-0.125]
</p><p>24 the probability density for the path with a given drift function f = (f (Xt ))t∈G at these observations can be written as the product where  pf (X0:T |f ) = p0 (X0:T )L(X0:T |f ), p0 (X0:T ) ∝ exp −  1 2∆t  t  ||Xt+∆t − Xt ||  (4) 2  (5)  is the measure over paths without drift, i. [sent-52, score-0.866]
</p><p>25 To attempt a nonparametric Bayesian estimate of the drift function f (x), we note that the exponent in (6) contains the drift f at most quadratically. [sent-58, score-1.206]
</p><p>26 Hence it becomes clear that a conjugate prior to the drift for this model is given by a Gaussian process, i. [sent-59, score-0.587]
</p><p>27 We denote probabilities over the drift f by upper case symbols in order to avoid confusion with path probabilities. [sent-62, score-0.668]
</p><p>28 Although a more general model is possible, we will restrict ourselves to the case where the 2  Figure 1: The left ﬁgure shows a snippet of the double well sample path in black and observations as red dots. [sent-63, score-0.294]
</p><p>29 The right picture displays the estimated drift function for the double well model after initialization, where the red line denotes the true drift function and the black line the mean function with corresponding 95%-conﬁdence bounds (twice the standard deviation) in blue. [sent-64, score-1.307]
</p><p>30 , d of the drift are independent (with usually differ2 2 ent kernels) and we assume that we have a diagonal diffusion matrix D = diag(σ1 , . [sent-69, score-0.722]
</p><p>31 In this j case, the GP posteriors of f (x) are independent, too, and we can estimate drift components indej j pendently by ordinary GP regression. [sent-73, score-0.62]
</p><p>32 Then a standard calculation [11] shows that the posterior process over drift functions f has a posterior mean and a GP posterior variance at an arbitrary point x is given by ¯ f j (x) = kj (x)⊤  2 σj I K + ∆t j  −1 j  d ,  2 σf j (x)  j  j  = K (x, x)−k (x)  ⊤  2 σj I K + ∆t j  −1  kj (x). [sent-75, score-0.996]
</p><p>33 A possible way out of this problem—as suggested by [8]—could be a restriction to kernels for which the inverse kernel, the precision operator, is a differential operator. [sent-78, score-0.175]
</p><p>34 A well known machine learning approach, which is based on a sparse Gaussian process approximation, applies to arbitrary kernels and generalizes easily to multivariate SDE. [sent-79, score-0.148]
</p><p>35 We have resorted speciﬁcally to the optimal Kullback-Leibler sparsity [1,12], where the likelihood term of a GP model is replaced by another effective likelihood, which depends only on a smaller set of variables fs . [sent-80, score-0.225]
</p><p>36 4  MAP Inference for sparse observations  The simple GP regression approach outlined in the previous section cannot be applied when obser. [sent-81, score-0.115]
</p><p>37 In this setting, we assume that n observations yk = Xτk , k = 1, . [sent-83, score-0.345]
</p><p>38 , n are obtained at (for simplicity) regular intervals τk = kτ , where τ ≫ ∆t is much larger than the microscopic time scale. [sent-86, score-0.097]
</p><p>39 In this case, a discretization in (6), where the sum over the microscopic grid t ∈ G would be replaced by a sum over macroscopic times τk and ∆t by τ , would correspond to a discrete time dynamical model of the form (1) again replacing ∆t by τ . [sent-87, score-0.297]
</p><p>40 But this discretization would give a bad approximation to the true SDE dynamics. [sent-88, score-0.096]
</p><p>41 The estimator of the drift would give some (approximate) estimation of the mean of the transition kernel over macroscopic times τ . [sent-89, score-0.722]
</p><p>42 This can be seen in ﬁgure 1, where the red line corresponds to the true drift (of the so called double-well model [4]) and the black line to its prediction based on observations with τ = 0. [sent-91, score-0.702]
</p><p>43 Our goal is to use an EM algorithm to compute the maximum posterior (MAP) prediction for the drift function f (x). [sent-94, score-0.706]
</p><p>44 Unfortunately, exact posterior expectations are intractable and one needs to work with suitable approximations. [sent-95, score-0.157]
</p><p>45 In the E-step, we compute the expected negative logarithm of the complete data likelihood L(f , q) = −Eq [ln L(X0:T |f )] ,  (9)  fnew = arg min (L(f , q) − ln P0 (f )) . [sent-98, score-0.089]
</p><p>46 (10)  where q denotes a measure over paths which approximates the intractable posterior p(X0:T |y, fold ) for the previous estimate fold of the drift. [sent-99, score-0.181]
</p><p>47 In the M-Step, we recompute the drift function as f  To compute the expectation in the E-step, we use (6) and take the limit ∆t → 0 at the end, when expectations have been computed. [sent-101, score-0.625]
</p><p>48 As f (x) is a time-independent function, this yields 1 −Eq [ln L(X0:T |f )] = lim Eq ||f (Xt )||2 ∆t − 2 f (Xt ), Xt+∆t − Xt ∆t→0 2 t 1 T Eq ||f (Xt )||2 − 2 f (Xt ), gt (Xt ) dt 2 0 1 = ||f (x)||2 A(x)dx − f (x), y(x) dx. [sent-102, score-0.131]
</p><p>49 (11) 2 Here qt (x) is the marginal density of Xt computed from the approximate posterior path measure q. [sent-103, score-0.368]
</p><p>50 We have also deﬁned the corresponding approximate posterior drift 1 Eq [Xt+∆t − Xt |Xt = x], (12) gt (x) = lim ∆t→0 ∆t as well as the functions =  T  T  qt (x)dt  A(x) =  gt (x)qt (x)dt. [sent-104, score-0.928]
</p><p>51 We need to ﬁnd tractable path measures q, which lead to good approximations for marginal densities and posterior drifts given arbitrary prior drift functions f (x). [sent-106, score-0.891]
</p><p>52 This can be expressed by the transition densities of the homogeneous Markov diffusion process with drift f (x). [sent-113, score-0.848]
</p><p>53 Since exact computations are not feasible for general drift functions, we approximate the transition density ps (x|xk ) in each interval Ik by that of a process, where the drift f (x) is replaced by its local linearization f (x) ≈ fou (x, t) = f (xk ) − Γk (x − xk ) 4  with Γk = −∇f (xk ). [sent-118, score-1.474]
</p><p>54 Here the transition density is a multivariate Gaussian (k) qs (x|y) = N x|αk + e−Γk s (y − αk ); Ss (17)  −1 where αk = yk + Γ−1 f (yk ) is the stationary mean and the variance Ss = As Bs is calculated k using the matrix exponential  As = exp Bs  Γk 0  D −Γ⊤ k  s  0 . [sent-120, score-0.346]
</p><p>55 and  ⊤  By inspecting mean and variance we see that the distribution is a equivalent to a bridge between the points X = yk and X = yk+1 and collapses to point masses at these points. [sent-122, score-0.306]
</p><p>56 3  Sparse M-Step approximation  To cope with the functional optimization, we resort to a sparse approximation for replacing the inﬁnite set f by a sparse set fs . [sent-125, score-0.317]
</p><p>57 Following appendix B in the supplementary material, we ﬁnd that in the sparse approximation the likelihood (11) is replaced by 1 ||E0 [f (x)|fs ]||2 A(x) dx − E0 [f (x)|fs ], y(x) dx, (20) 2 where the conditional expectation is over the GP prior. [sent-127, score-0.224]
</p><p>58 In order to avoid cluttered notation, it should be noted that in the following results for a component f j , the quantities Λs , fs , ks , K−1 , y(x), σ 2 , s similar to (7) depend on the component j, but not A(x). [sent-128, score-0.265]
</p><p>59 Ls (f , q) =  This is easily computed as E0 [f (x)|fs ] = k⊤ (x)K−1 fs . [sent-129, score-0.151]
</p><p>60 s s  (21)  1 ⊤ ⊤ f Λ s f s − fs d s 2 s  (22)  Hence Ls (f , q) = with Λs =  1 −1 K σ2 s  ks (x) A(x) k⊤ (x)dx K−1 , s s 5  ds =  1 −1 K σ2 s  ks (x) y(x) dx. [sent-130, score-0.315]
</p><p>61 (23)  With these results, the approximate MAP estimate is ¯ fs (x) = k⊤ (x)(I + Λs Ks )−1 ds . [sent-131, score-0.181]
</p><p>62 s  (24)  The integrals over x in (23) can be computed analytically for many kernels of interest such as polynomial and RBF ones. [sent-132, score-0.095]
</p><p>63 2 A related expression for the variance σs (x) = K(x, x) − k⊤ (x)(I + ΛKs )−1 Λs ks (x) can only be s viewed as a crude estimate, because it does not include the impact of the GP ﬂuctuations on the path probabilities. [sent-135, score-0.205]
</p><p>64 5  A crude estimate of an approximation error  Unfortunately, there is no guarantee that this approximation to the EM algorithm will always increase the exact likelihood p(y|f ). [sent-136, score-0.166]
</p><p>65 from the the Ornstein-Uhlenbeck approximation (19) to lowest order in the difference δf (Xt , t) = f (Xt ) − fou (Xt , t) between drift function and its approximation. [sent-138, score-0.724]
</p><p>66 The OrnsteinUhlenbeck approximation (19) can expressed in a similar way: we just have to replace L(X0:T |f ) by a functional Lou (X0:T |f ) which in turn is obtained by replacing f (Xt ) with the linearized drift fou (Xt , t) in (6). [sent-140, score-0.724]
</p><p>67 The difference in free energies (negative log evidences) can be expressed exactly by an expectation over the posterior OU processes and then expanded (similar to a cumulant expansion) in a Taylor series in ∆L = − ln(L/Lou ). [sent-141, score-0.119]
</p><p>68 ∆F = − {ln p(y|f ) − ln pou (y)} = − ln Eq e−∆L ≈ Eq [∆L] − Varq [∆L] ± . [sent-143, score-0.152]
</p><p>69 2  (26)  The computation of the ﬁrst term is similar to (11) and requires only the marginal qt and the posterior gt . [sent-146, score-0.285]
</p><p>70 The second term contains the posterior variance and requires two-time covariances of the OU process. [sent-147, score-0.119]
</p><p>71 This yields T  ∆F ≈ Eq [∆L] ≈  0  Eq [ δf (Xt , t), fou (Xt , t) − gt (Xt ) ] dt. [sent-149, score-0.153]
</p><p>72 (27)  This expression could be evaluated in order to estimate the inﬂuence of nonlinear parts of the drift on the approximation error. [sent-150, score-0.631]
</p><p>73 Also, we determined the sparse points for the GP algorithm in each case by ﬁrst constructing a histogram over the observations and then selecting the set of histogram midpoints of each histogram bin which contained at least a certain number bmin of observations. [sent-152, score-0.161]
</p><p>74 6  Figure 2: The ﬁgures show the estimated drift functions for the double well model (left) and the periodic diffusion model (right) after completion of the EM algorithm. [sent-154, score-0.822]
</p><p>75 Again, the black and blue lines denote mean and 95%-conﬁdence bounds, while the red lines indicate the true drift functions. [sent-155, score-0.626]
</p><p>76 1  One-dimensional toy models  First we test our algorithm on two toy data sets, the double well model with dynamics given by the SDE dx = 4(x − x3 )dt + dW (29) and a diffusion model driven by a periodic drift dx = sin(x)dt + dW. [sent-157, score-1.071]
</p><p>77 (30)  For both models, we simulated a path of size M = 105 on a regular grid with width ∆t = 0. [sent-158, score-0.152]
</p><p>78 We initialized the EM Algorithm by running the sparse GP for the observation points without any imputation and subsequently computed the expectation operators by analytically evaluating the expressions on the same time grid as the simulated path and summing over the time steps. [sent-160, score-0.193]
</p><p>79 An alternative initialization strategy which consists of generating a full trajectory of the same size as the original path using Brownian bridge sampling between observations did not bring any noticeable performance improvements. [sent-161, score-0.194]
</p><p>80 5 and p = 5, whereas for the periodic model we used an RBF kernel (c = 1) with the same values for σRBF and lRBF . [sent-166, score-0.089]
</p><p>81 Speciﬁcally, this data set as shown in ﬁgure 3 contains 4918 observations of oxygen isotope concentration δ 18 O over a time period from the present to roughly 1. [sent-173, score-0.214]
</p><p>82 Since there are generally less isotopes in ice formed under cold conditions, the isotope concentration can be regarded as an indicator of past temperatures. [sent-175, score-0.171]
</p><p>83 Recent research [16] suggest to model the rapid paleoclimatic changes exhibited in the data set by a simple dynamical system with polynomial drift function of order p = 3 as canonical model which allows for bistability. [sent-176, score-0.693]
</p><p>84 For this particular problem we ﬁrst tried to determine the diffusion constant σ of the data. [sent-178, score-0.135]
</p><p>85 5 by running the EM algorithm with a polynomial kernel (c = 0) of order p = 3 for each value in turn. [sent-181, score-0.087]
</p><p>86 The resulting drift function with the highest likelihood is shown in ﬁgure 3. [sent-182, score-0.623]
</p><p>87 The result seems to conﬁrm the existence of a metastable state of oxygen isotope concentration and a stable state at lower values. [sent-183, score-0.248]
</p><p>88 7  Figure 3: The ﬁgure on the left displays the NGRIP data set, while the picture on the right shows the estimated drift in black with corresponding 95%-conﬁdence bounds denoting twice the standard deviation in blue for the optimal diffusion value σ = 2. [sent-184, score-0.795]
</p><p>89 ˆ  Figure 4: The left ﬁgure shows the empirical density for the two-dimensional model, together with the vector ﬁelds of the actual drift function given in blue and the estimated drift given in red. [sent-186, score-1.206]
</p><p>90 The right picture shows a snippet from the full sample in black together with the ﬁrst 20 observations denoted by red dots. [sent-187, score-0.187]
</p><p>91 3  Two-dimensional toy model  As an example of a two dimensional system, we simulated from a process with the following SDE: dx dy  = =  (x(1 − x2 − y 2 ) − y)dt + dW1 , 2  2  (y(1 − x − y ) + y)dt + dW2 . [sent-189, score-0.151]
</p><p>92 (31) (32)  For this model we simulated a path of size M = 106 on a regular grid with width ∆t = 0. [sent-190, score-0.152]
</p><p>93 In the inference shown in ﬁgure 4 we used a polynomial kernel (c = 0) of order p = 4. [sent-192, score-0.123]
</p><p>94 7  Discussion  It would be interesting to replace the ad hoc local linear approximation of the posterior drift by a more ﬂexible time dependent Gaussian model. [sent-193, score-0.75]
</p><p>95 Such a method could be extended to noisy observations and the case, where some components of the state vector are not observed. [sent-195, score-0.115]
</p><p>96 Finally, this method could be turned into a variational Bayesian approximation, where one optimizes posteriors over both drifts and over state paths. [sent-196, score-0.144]
</p><p>97 The path probabilities are then inﬂuenced by the uncertainties in the drift estimation, which would lead to more realistic predictions of error bars. [sent-197, score-0.668]
</p><p>98 The coloured noise expansion and parameter estimation of diffusion processes. [sent-256, score-0.135]
</p><p>99 Posterior consistency via precision operators for Bayesian nonparametric drift estimation in SDEs. [sent-278, score-0.656]
</p><p>100 Bayesian inference for change points in dynamical systems with reusable states—a Chinese restaurant process approach. [sent-320, score-0.156]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('drift', 0.587), ('xt', 0.289), ('yk', 0.269), ('sde', 0.231), ('gp', 0.228), ('fs', 0.151), ('diffusion', 0.135), ('posterior', 0.119), ('differential', 0.118), ('tk', 0.11), ('rbf', 0.104), ('eq', 0.094), ('fou', 0.093), ('ks', 0.082), ('em', 0.081), ('path', 0.081), ('opper', 0.077), ('observations', 0.076), ('qt', 0.072), ('dt', 0.071), ('isotope', 0.069), ('lrbf', 0.069), ('oxygen', 0.069), ('dynamical', 0.068), ('dx', 0.067), ('manfred', 0.067), ('paths', 0.062), ('microscopic', 0.062), ('xk', 0.062), ('gt', 0.06), ('ik', 0.06), ('double', 0.06), ('kernels', 0.057), ('stk', 0.056), ('ice', 0.056), ('editors', 0.054), ('ln', 0.053), ('discretization', 0.052), ('process', 0.052), ('dynamics', 0.051), ('pereira', 0.05), ('kernel', 0.049), ('gure', 0.047), ('bmin', 0.046), ('isotopes', 0.046), ('ngrip', 0.046), ('pokern', 0.046), ('pou', 0.046), ('yvo', 0.046), ('transition', 0.045), ('approximation', 0.044), ('wiener', 0.043), ('crude', 0.042), ('tu', 0.042), ('drifts', 0.041), ('macroscopic', 0.041), ('ruttor', 0.041), ('sdes', 0.041), ('periodic', 0.04), ('gaussian', 0.04), ('sparse', 0.039), ('state', 0.039), ('black', 0.039), ('expectations', 0.038), ('berlin', 0.038), ('replaced', 0.038), ('dxt', 0.038), ('snippet', 0.038), ('polynomial', 0.038), ('bridge', 0.037), ('operators', 0.037), ('likelihood', 0.036), ('grid', 0.036), ('inference', 0.036), ('dealt', 0.035), ('ou', 0.035), ('equations', 0.035), ('regular', 0.035), ('picture', 0.034), ('marginal', 0.034), ('zemel', 0.033), ('posteriors', 0.033), ('toy', 0.032), ('weinberger', 0.032), ('cluttered', 0.032), ('nonparametric', 0.032), ('stable', 0.032), ('andreas', 0.032), ('density', 0.032), ('variational', 0.031), ('pt', 0.031), ('approximate', 0.03), ('bs', 0.03), ('burges', 0.03), ('dw', 0.03), ('densities', 0.029), ('stochastic', 0.029), ('ls', 0.028), ('pf', 0.028), ('bartlett', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="39-tfidf-1" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>2 0.21802182 <a title="39-tfidf-2" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>3 0.17624664 <a title="39-tfidf-3" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>Author: Botond Cseke, Manfred Opper, Guido Sanguinetti</p><p>Abstract: We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and signiﬁcant computational savings compared to discrete-time approaches in a neural application. 1</p><p>4 0.16828489 <a title="39-tfidf-4" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>5 0.15596405 <a title="39-tfidf-5" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>Author: Jonas Peters, Dominik Janzing, Bernhard Schölkopf</p><p>Abstract: Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identiﬁability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufﬁcient or the model is misspeciﬁed, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artiﬁcial and real data and code is provided. 1</p><p>6 0.1435357 <a title="39-tfidf-6" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>7 0.11286372 <a title="39-tfidf-7" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>8 0.10998399 <a title="39-tfidf-8" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>9 0.10635737 <a title="39-tfidf-9" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>10 0.10490905 <a title="39-tfidf-10" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>11 0.098385267 <a title="39-tfidf-11" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>12 0.098172814 <a title="39-tfidf-12" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>13 0.09395805 <a title="39-tfidf-13" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>14 0.09025719 <a title="39-tfidf-14" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>15 0.090122007 <a title="39-tfidf-15" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>16 0.089929551 <a title="39-tfidf-16" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>17 0.08608298 <a title="39-tfidf-17" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>18 0.078135058 <a title="39-tfidf-18" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>19 0.077514529 <a title="39-tfidf-19" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>20 0.077270411 <a title="39-tfidf-20" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, -0.016), (2, 0.053), (3, -0.057), (4, -0.087), (5, 0.099), (6, 0.124), (7, 0.116), (8, 0.079), (9, -0.142), (10, -0.135), (11, -0.233), (12, -0.132), (13, 0.056), (14, -0.046), (15, -0.006), (16, -0.058), (17, 0.058), (18, -0.016), (19, -0.042), (20, 0.106), (21, -0.024), (22, -0.052), (23, 0.045), (24, -0.068), (25, 0.006), (26, -0.031), (27, -0.044), (28, -0.005), (29, 0.061), (30, -0.091), (31, 0.017), (32, -0.066), (33, 0.024), (34, -0.018), (35, -0.045), (36, 0.007), (37, 0.004), (38, -0.011), (39, -0.028), (40, -0.017), (41, -0.043), (42, -0.041), (43, 0.006), (44, 0.012), (45, 0.006), (46, -0.007), (47, 0.067), (48, 0.007), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96088946 <a title="39-lsi-1" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>2 0.81104338 <a title="39-lsi-2" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>3 0.79710186 <a title="39-lsi-3" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>Author: Botond Cseke, Manfred Opper, Guido Sanguinetti</p><p>Abstract: We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and signiﬁcant computational savings compared to discrete-time approaches in a neural application. 1</p><p>4 0.71491337 <a title="39-lsi-4" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>Author: Jonas Peters, Dominik Janzing, Bernhard Schölkopf</p><p>Abstract: Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identiﬁability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufﬁcient or the model is misspeciﬁed, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artiﬁcial and real data and code is provided. 1</p><p>5 0.68003535 <a title="39-lsi-5" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>6 0.6525045 <a title="39-lsi-6" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>7 0.61275345 <a title="39-lsi-7" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>8 0.60036236 <a title="39-lsi-8" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>9 0.59179384 <a title="39-lsi-9" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>10 0.56854564 <a title="39-lsi-10" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>11 0.56090742 <a title="39-lsi-11" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>12 0.55607104 <a title="39-lsi-12" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>13 0.55322325 <a title="39-lsi-13" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>14 0.49876866 <a title="39-lsi-14" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>15 0.48764297 <a title="39-lsi-15" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>16 0.48746821 <a title="39-lsi-16" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>17 0.47438708 <a title="39-lsi-17" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>18 0.47194105 <a title="39-lsi-18" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>19 0.41518685 <a title="39-lsi-19" href="./nips-2013-Solving_inverse_problem_of_Markov_chain_with_partial_observations.html">299 nips-2013-Solving inverse problem of Markov chain with partial observations</a></p>
<p>20 0.40227968 <a title="39-lsi-20" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.02), (16, 0.055), (33, 0.086), (34, 0.182), (41, 0.058), (49, 0.03), (56, 0.139), (70, 0.02), (73, 0.231), (85, 0.04), (89, 0.023), (93, 0.024), (95, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83549958 <a title="39-lda-1" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>2 0.76362491 <a title="39-lda-2" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>Author: Özgür1 Şimşek</p><p>Abstract: Several attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule. This research has identiﬁed three environmental structures that aid heuristics: dominance, cumulative dominance, and noncompensatoriness. This paper develops these ideas further and examines their empirical relevance in 51 natural environments. The results show that all three structures are prevalent, making it possible for simple rules to reach, and occasionally exceed, the accuracy of the linear decision rule, using less information and less computation. 1</p><p>3 0.75028616 <a title="39-lda-3" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Speciﬁcally, by using auxiliary natural images, we train a stacked denoising autoencoder ofﬂine to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from ofﬂine training to the online tracking process. Online tracking involves a classiﬁcation neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classiﬁcation layer. Both the feature extractor and the classiﬁer can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU). 1</p><p>4 0.73202628 <a title="39-lda-4" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>5 0.70953852 <a title="39-lda-5" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>Author: Nima Taghipour, Jesse Davis, Hendrik Blockeel</p><p>Abstract: Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the ﬁrst-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree. 1</p><p>6 0.70941246 <a title="39-lda-6" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>7 0.70775753 <a title="39-lda-7" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>8 0.70483756 <a title="39-lda-8" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>9 0.70146364 <a title="39-lda-9" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>10 0.70089465 <a title="39-lda-10" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>11 0.70007795 <a title="39-lda-11" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>12 0.69999528 <a title="39-lda-12" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>13 0.69951993 <a title="39-lda-13" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>14 0.69898432 <a title="39-lda-14" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>15 0.69752336 <a title="39-lda-15" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>16 0.69710726 <a title="39-lda-16" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>17 0.69465488 <a title="39-lda-17" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>18 0.69457352 <a title="39-lda-18" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>19 0.69443572 <a title="39-lda-19" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>20 0.69440633 <a title="39-lda-20" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
