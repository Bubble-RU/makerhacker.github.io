<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-13" href="#">nips2013-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</h1>
<br/><p>Source: <a title="nips-2013-13-pdf" href="http://papers.nips.cc/paper/4978-a-scalable-approach-to-probabilistic-latent-space-inference-of-large-scale-networks.pdf">pdf</a></p><p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>Reference: <a title="nips-2013-13-reference" href="../nips2013_reference/nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a scalable approach for making inference about latent spaces of large networks. [sent-8, score-0.214]
</p><p>2 When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. [sent-10, score-0.262]
</p><p>3 1  Introduction  In the context of network analysis, a latent space refers to a space of unobserved latent representations of individual entities (i. [sent-11, score-0.352]
</p><p>4 The problem of latent space inference amounts to learning the bases of such a space and reducing the high-dimensional network data to such a lower-dimensional space, in which each entity has a position vector. [sent-14, score-0.291]
</p><p>5 To perform latent space analysis on at least million-node (if not larger) real social networks with many distinct latent roles [24], one must design inferential mechanisms that scale in both the number of vertices N and the number of latent roles K. [sent-17, score-1.128]
</p><p>6 In this paper, we argue that the following three principles are crucial for successful large-scale inference: (1) succinct but informative representation of networks; (2) parsimonious statistical modeling; (3) scalable and parallel inference algorithms. [sent-18, score-0.223]
</p><p>7 Its batch variational inference algorithm has O(N 2 K 2 ) time complexity and hence cannot be scaled to large networks. [sent-21, score-0.277]
</p><p>8 The a-MMSB [5] improves upon MMSB by applying principles (2) and (3): it reduces the dimension of the parameter space from O(K 2 ) to O(K), and applies a stochastic variational algorithm for fast inference. [sent-22, score-0.271]
</p><p>9 These higher-order relations have motivated the development of the triangular representation of networks [8], in which each network is represented succinctly as a bag of triangular motifs with size typically much smaller than Θ(N 2 ). [sent-26, score-1.026]
</p><p>10 This succinct representation has proven effective in extracting informative mixed-membership roles from 1  networks with high ﬁdelity, thus achieving the ﬁrst principle (1). [sent-27, score-0.413]
</p><p>11 However, the corresponding statistical model, called the mixed-membership triangular model (MMTM), only scales well against the size of a network, but does not scale to large numbers of latent roles (i. [sent-28, score-0.648]
</p><p>12 To be precise, if there are K distinct latent roles, its tensor of triangle-generating parameters is of size O(K 3 ), and its blocked Gibbs sampler requires O(K 3 ) time per iteration. [sent-31, score-0.205]
</p><p>13 We now present a scalable approach to both latent space modeling and inference algorithm design that encompasses all three aforementioned principles for large networks. [sent-33, score-0.262]
</p><p>14 In Section 3, we propose the parsimonious triangular model (PTM), in which the dimension of the triangle-generating parameters only grows linearly in K. [sent-35, score-0.34]
</p><p>15 Then, in Section 4, we develop a fast stochastic natural gradient ascent algorithm for performing variational inference, where an unbiased estimate of the natural gradient is obtained by subsampling a “mini-batch” of triangular motifs. [sent-37, score-0.618]
</p><p>16 These new developments all combine to yield an efﬁcient inference algorithm that usually converges after 2 passes on each triangular motif (or up to 4-5 passes at worst), and achieves competitive or improved accuracy for latent space recovery and link prediction on synthetic and real networks. [sent-39, score-0.796]
</p><p>17 Finally, in Section 5, we demonstrate that our algorithm converges and infers a 100-role latent space on a 1M-node Youtube social network in just 4 hours, using a single machine with 8 threads. [sent-40, score-0.259]
</p><p>18 2  Triangular Representation of Networks  We take a scalable approach to network modeling by representing each network succinctly as a bag of triangular motifs [8]. [sent-41, score-0.765]
</p><p>19 Each triangular motif is a connected subgraph over a vertex triple containing 2 or 3 edges (called open triangle and closed triangle respectively). [sent-42, score-0.758]
</p><p>20 Although this triangular format does not preserve all network information found in an edge representation, these three-node connected subgraphs are able to capture a number of informative structural features in the network. [sent-44, score-0.379]
</p><p>21 For example, in social network theory, the notion of triadic closure [21, 6] is commonly measured by the relative number of closed triangles compared to the total number of connected triples, known as the global clustering coefﬁcient or transitivity [17]. [sent-45, score-0.423]
</p><p>22 Most importantly of all, triangular modeling requires much less computational cost compared to edge-based models, with little or no degradation of performance for latent space recovery [8]. [sent-48, score-0.45]
</p><p>23 In networks with N vertices and low maximum vertex degree D, the number of triangular motifs Θ(N D2 ) is normally much smaller than Θ(N 2 ), allowing us to construct more efﬁcient inference algorithms scalable to larger networks. [sent-49, score-0.847]
</p><p>24 For high-maximum-degree networks, the triangular motifs can be subsampled in a node-centric fashion as a local data reduction step. [sent-50, score-0.506]
</p><p>25 For each vertex i with δ degree higher than a user-chosen threshold δ, uniformly sample 2 triangles from the set composed of (a) its adjacent closed triangles, and (b) its adjacent open triangles that are centered on i. [sent-51, score-0.639]
</p><p>26 Vertices with degree ≤ δ keep all triangles from their set. [sent-52, score-0.263]
</p><p>27 In what follows, we assume that a preprocessing step has been performed — namely, extracting and δ-subsampling triangular motifs (which can be done in O(1) time per sample, and requires < 1% of the actual inference time) — to yield a bag-of-triangles representation of the input network. [sent-54, score-0.553]
</p><p>28 , N } , i < j < k, let Eijk denote the observed type of triangular motif formed among these three vertices: Eijk = 1, 2 and 3 represent an open triangle with i, j and k in the center respectively, and Eijk = 4 if a closed triangle is formed. [sent-58, score-0.697]
</p><p>29 Because empty and single-edge triples are discarded, the set of triples with triangular motifs formed, I = {(i, j, k) : i < j < k, Eijk = 1, 2, 3 or 4}, is of size O(N δ 2 ) after δ-subsampling [8]. [sent-59, score-0.574]
</p><p>30 3  Parsimonious Triangular Model  Given the input network, now represented as a bag of triangular motifs, our goal is to make inference about the latent position vector θi of each vertex i ∈ {1, . [sent-60, score-0.564]
</p><p>31 approach: each vertex i can take a mixture distribution over K latent roles governed by a mixedmembership vector θi ∈ ∆K−1 restricted to the (K − 1)-simplex. [sent-65, score-0.457]
</p><p>32 Following a design principle similar to the Mixed-Membership Triangular Model (MMTM) [8], our Parsimonious Triangular Model (PTM) is essentially a latent-space model that deﬁnes the generative process for a bag of triangular motifs. [sent-67, score-0.34]
</p><p>33 To form a triangular motif Eijk for each triplet of vertices (i, j, k), a triplet of role indices si,jk , sj,ik , sk,ij ∈ {1, . [sent-70, score-0.614]
</p><p>34 These indices designate the roles taken by each vertex participating in this triangular motif. [sent-74, score-0.643]
</p><p>35 There are O(K 3 ) distinct conﬁgurations of such latent role triplet, and the MMTM uses a tensor of trianglegenerating parameters of the same size to deﬁne the probability of Eijk , one entry Bxyz for each possible conﬁguration (x, y, z). [sent-75, score-0.239]
</p><p>36 , K}, allowing PTM to scale to far more latent roles than MMTM. [sent-81, score-0.374]
</p><p>37 Once the role triplet (si,jk , sj,ik , sk,ij ) is chosen, some of the triangular motifs can become indistinguishable. [sent-83, score-0.615]
</p><p>38 Formally, this conﬁguration induces a set of triangle equivalence classes {{1, 2}, {3}, {4}} of all possible triangular motifs {1, 2, 3, 4}. [sent-85, score-0.647]
</p><p>39 We treat the triangular motifs within the same equivalence class as stochastically equivalent; that is, the conditional probabilities of events Eijk = 1 and Eijk = 2 are the same if x = si,jk = sj,ik = sk,ij . [sent-86, score-0.53]
</p><p>40 If all three vertices have the same role x, all three open triangles are equivalent and the induced set of equivalence classes is {{1, 2, 3}, {4}}. [sent-88, score-0.446]
</p><p>41 The probability of Eijk is determined by Bxxx ∈ ∆1 , where Bxxx,1 represents the total probability of sampling an open triangle from {1, 2, 3} and Bxxx,2 represents the closed triangle probability. [sent-89, score-0.357]
</p><p>42 Here, Bxx,1 and Bxx,2 represent the open triangle probabilities (for open triangles centered at a vertex in majority and minority role respectively), and Bxx,3 represents the closed triangle probability. [sent-93, score-0.851]
</p><p>43 There are two possible open triangles with a vertex in majority role at the center, and hence each has probability Bxx,1 /2. [sent-94, score-0.447]
</p><p>44 If all three vertices have distinct roles, the probability of Eijk depends on B0 ∈ ∆1 , where B0,1 represents the total probability of sampling an open triangle from {1, 2, 3} (regardless of the center vertex’s role) and B0,2 represents the closed triangle probability. [sent-96, score-0.44]
</p><p>45 To summarize, the PTM assumes the following generative process for a bag of triangular motifs: • Choose B0 ∈ ∆1 , Bxx ∈ ∆2 and Bxxx ∈ ∆1 for each role x ∈ {1, . [sent-97, score-0.419]
</p><p>46 • For each triplet of vertices (i, j, k) , i < j < k, − Draw role indices si,jk ∼ Discrete (θi ), sj,ik ∼ Discrete (θj ), sk,ij ∼ Discrete (θk ). [sent-105, score-0.218]
</p><p>47 − Choose a triangular motif Eijk ∈ {1, 2, 3, 4} based on B0 , Bxx , Bxxx and the conﬁguration of (si,jk , sj,ik , sk,ij ) (see Table 1 for the conditional probabilities). [sent-106, score-0.34]
</p><p>48 However, given a bag of triangular motifs E extracted from a network, the above procedure deﬁnes a valid probabilistic model p(E | α, λ) and we can legitimately use it for performing posterior inference p(s, θ, B | E, α, λ). [sent-108, score-0.62]
</p><p>49 We stress that our goal is latent space inference, not network simulation. [sent-109, score-0.218]
</p><p>50 4  Scalable Stochastic Variational Inference  In this section, we present a stochastic variational inference algorithm [10] for performing approximate inference under our model. [sent-110, score-0.325]
</p><p>51 The strong dependencies among the per-triangle latent roles (si,jk , sj,ik , sk,ij ) suggest that we should model them as a group, rather than completely independent as in a naive mean-ﬁeld approximation1 . [sent-114, score-0.374]
</p><p>52 Thus, the variational posterior of (si,jk , sj,ik , sk,ij ) is the discrete distribution . [sent-115, score-0.206]
</p><p>53 q(si,jk = x, sj,ik = y, sk,ij = z) = qijk (x, y, z) = φxyz , ijk  x, y, z = 1, . [sent-116, score-0.287]
</p><p>54 (2)  To simplify the notation, we decompose the variational objective L(φ, η, γ) into a global term and a summation of local terms, one term for each triangular motif (see Appendix for details). [sent-124, score-0.577]
</p><p>55 (3)  (i,j,k)∈I  The global term g(η, γ) depends only on the global variational parameters η, which govern the posterior of the triangle-generating probabilities B, as well as the per-node mixed-membership parameters γ. [sent-126, score-0.307]
</p><p>56 Deﬁne L(η, γ) = maxφ L(φ, η, γ), which is the variational objective achieved by ﬁxing the global parameters η, γ and optimizing the local parameters φ. [sent-129, score-0.237]
</p><p>57 (i,j,k)∈I  φijk  (4)  Stochastic variational inference is a stochastic gradient ascent algorithm [3] that maximizes L(η, γ), based on noisy estimates of its gradient with respect to η and γ. [sent-131, score-0.374]
</p><p>58 Whereas computing the true gradient L(η, γ) involves a costly summation over all triangular motifs as in (4), an unbiased noisy approximation of the gradient can be obtained much more cheaply by summing over a small subsample of triangles. [sent-132, score-0.575]
</p><p>59 With this unbiased estimate of the gradient and a suitable adaptive step size, the algorithm is guaranteed to converge to a stationary point of the variational objective L(η, γ) [18]. [sent-133, score-0.241]
</p><p>60 (2) Optimize the local parameters qijk (x, y, z) for all sampled triangles in parallel by (6). [sent-140, score-0.345]
</p><p>61 In our setting, the most natural way to obtain an unbiased gradient of L(η, γ) is to sample a “minibatch” of triangular motifs at each iteration, and then average the gradient of local terms in (4) only for these sampled triangles. [sent-144, score-0.601]
</p><p>62 Formally, let m be the total number of triangles and deﬁne LS (η, γ) = g(η, γ) +  m |S|  max (φijk , η, γ), (i,j,k)∈S  φijk  (5)  where S is a mini-batch of triangles sampled uniformly at random. [sent-145, score-0.476]
</p><p>63 To obtain the gradient LS (η, γ), one needs to compute the optimal local variational parameters φijk (keeping η and γ ﬁxed) for each sampled triangle (i, j, k) in the minibatch S; these optimal φijk ’s are then used in equation (5) to compute LS (η, γ). [sent-148, score-0.42]
</p><p>64 Taking partial derivatives of (3) with respect to each local term φxyz and setting them to zero, we get for distinct ijk x, y, z ∈ {1, . [sent-149, score-0.258]
</p><p>65 ijk (6) xxy xxx  See Appendix for the update equations of φijk and φijk (x = y). [sent-153, score-0.246]
</p><p>66 In other words, we assume the probability mass of the variational posterior q(si,jk , sj,ik , sk,ij ) falls entirely on the K “diagonal” role combinations (a, a, a) as well as O(K) randomly chosen “off-diagonals” (a, b, c). [sent-157, score-0.285]
</p><p>67 Note that any choice of A yields a valid lower bound to the true log-likelihood; this follows from standard variational inference theory. [sent-162, score-0.234]
</p><p>68 The natural gradient ˜ LS (η, γ) is obtained by a premultiplication of the ordinary gradient LS (η, γ) with the inverse of the Fisher information of the variational posterior q. [sent-165, score-0.28]
</p><p>69 5  Experiments  We demonstrate that our stochastic variational algorithm achieves latent space recovery accuracy comparable to or better than prior work, but in only a fraction of the time. [sent-174, score-0.399]
</p><p>70 1 Generating Synthetic Data We use two latent space models as the simulator for our experiments — the MMSB model [1] (which the MMSB batch variational algorithm solves for), and a model that produces power-law networks from a latent space (see Appendix for details). [sent-179, score-0.616]
</p><p>71 Brieﬂy, the MMSB model produces networks with “blocks” of nodes characterized by high edge probabilities, whereas the Power-Law model produces “communities” centered around a high-degree hub node. [sent-180, score-0.207]
</p><p>72 We show that our algorithm rapidly and accurately recovers latent space roles based on these two notions of node-relatedness. [sent-181, score-0.396]
</p><p>73 We generated networks with N ∈ {500, 1000, 2000, 5000, 10000} nodes, with the number of roles growing as K = N/100, to simulate the fact that large networks can have more roles [24]. [sent-183, score-0.768]
</p><p>74 We generated “easy” networks where each θi contains 1 to 2 nonzero roles, and “hard” networks with 1 to 4 roles per θi . [sent-184, score-0.528]
</p><p>75 125 (because there are no more than 4 roles per θi ), and use Normalized Mutual Information (NMI) [12, ˆ 23], a commonly-used measure of overlapping cluster accuracy, to compare the θi ’s with the true θi ’s (thresholded similarly). [sent-190, score-0.309]
</p><p>76 We tested the following algorithms: • Our PTM stochastic variational algorithm. [sent-193, score-0.223]
</p><p>77 50 = 1225 triangles 2 per node), hyperparameters α = λ = 0. [sent-196, score-0.26]
</p><p>78 In particular, if nodes from two different roles are initialized to have the same role, the output is likely to merge all nodes in both roles into a single role. [sent-211, score-0.652]
</p><p>79 To ensure a meaningful comparison, we therefore provide the same ﬁxed initialization to all algorithms — for every role x, we provide 2 example nodes i, and initialize the remaining nodes to have random roles. [sent-212, score-0.207]
</p><p>80 NMI close to 1) across almost all networks, validating its ability to recover latent roles under a range of network sizes N and roles K. [sent-218, score-0.72]
</p><p>81 In contrast, as N (and thus K) increases, MMSB Variational exhibits degraded performance despite having converged, while MMTM/PTM Gibbs converge to and become stuck in local minima 2  We chose δ = 50 because almost all our synthetic networks have median degree ≤ 50. [sent-219, score-0.211]
</p><p>82 3 In general, one might not have any ground truth roles or labels to seed the algorithm with. [sent-221, score-0.287]
</p><p>83 For such cases, our algorithm can be initialized as follows: rank all nodes according to the number of 3-triangles they touch, and then seed the top K nodes with different roles x. [sent-222, score-0.415]
</p><p>84 The intuition is that “good” roles may be deﬁned as having a high ratio of 3-triangles to 2-triangles among participating nodes. [sent-223, score-0.282]
</p><p>85 51  2  5  0 0  10  10% mini−batches Full batch variational 2  4  6  8  10  Data passes  Runtime: Power−Law hard  1  0. [sent-237, score-0.313]
</p><p>86 51  2  5  10  0 0  10% mini−batches Full batch variational 2  4  6  8  10  Data passes  1,000s of nodes N  Figure 1: Synthetic Experiments. [sent-254, score-0.355]
</p><p>87 Right: Convergence of our stochastic variational algorithm (with 10% minibatches) versus a batch variational version of our algorithm. [sent-258, score-0.449]
</p><p>88 We believe our method maintains high accuracy due to its parsimonious O(K) parameter structure — compared to MMSB Variational’s O(K 2 ) block matrix and MMTM Gibbs’s O(K 3 ) tensor of triangle parameters. [sent-297, score-0.228]
</p><p>89 We also demonstrate that our stochastic variational algorithm with 10% mini-batches converges much faster to the correct solution than a nonstochastic, full-batch implementation. [sent-306, score-0.223]
</p><p>90 5 One data pass is deﬁned as performing variational inference on m triangles, where m is equal to the total number of triangles. [sent-315, score-0.234]
</p><p>91 All networks were taken from the Stanford Large Network Dataset Collection; directed networks were converted to undirected networks via symmetrization. [sent-341, score-0.366]
</p><p>92 5  x 10  Brightkite K=300, 4 threads  Slashdot K=100, 4 threads  7  6  −1  x 10 −1  x 10  6  x 10 0  7  −1  x 10  Slashdot K=300, 4 threads  −2  −2  −3  −4  Heldout LB  −1. [sent-345, score-0.303]
</p><p>93 Training and heldout variational lower bound (equivalent to perplexity) convergence plots for all experiments in Table 3. [sent-401, score-0.348]
</p><p>94 method, we ranked possible links i − j by the probability that the triangle (i, j, k) will include edge i − j, marginalizing over all choices of the third node k and over all possible role choices for nodes i, j, k. [sent-408, score-0.304]
</p><p>95 This matches what has been observed in the network literature [24], and further validates our triangle modeling assumptions. [sent-410, score-0.224]
</p><p>96 Figure 2 plots the training and heldout variational lower bound for several experiments, and shows that our algorithm always converges in 2-5 data passes. [sent-417, score-0.327]
</p><p>97 In summary, we have constructed a latent space network model with O(N K) parameters and devised a stochastic variational algorithm for O(N K) inference. [sent-424, score-0.441]
</p><p>98 Our implementation allows network analysis with millions of nodes N and hundreds of roles K in hours on a single multi-core machine, with competitive or improved accuracy for latent space recovery and link prediction. [sent-425, score-0.713]
</p><p>99 These results are orders of magnitude faster than recent work on scalable latent space network modeling [5, 8]. [sent-426, score-0.294]
</p><p>100 On triangular versus edge representations — towards scalable modeling of networks. [sent-476, score-0.346]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mmsb', 0.373), ('triangular', 0.274), ('mmtm', 0.27), ('roles', 0.262), ('eijk', 0.256), ('ptm', 0.256), ('triangles', 0.238), ('motifs', 0.206), ('ijk', 0.206), ('variational', 0.183), ('lb', 0.157), ('heldout', 0.144), ('nmi', 0.143), ('triangle', 0.14), ('networks', 0.122), ('latent', 0.112), ('bxx', 0.108), ('bxxx', 0.108), ('gibbs', 0.105), ('threads', 0.101), ('network', 0.084), ('brightkite', 0.083), ('ls', 0.082), ('qijk', 0.081), ('role', 0.079), ('motif', 0.066), ('parsimonious', 0.066), ('hours', 0.066), ('bag', 0.066), ('passes', 0.065), ('runtime', 0.065), ('nodes', 0.064), ('vertex', 0.061), ('link', 0.061), ('vertices', 0.057), ('triplet', 0.056), ('slashdot', 0.054), ('xyz', 0.054), ('inference', 0.051), ('scalable', 0.051), ('stanford', 0.049), ('youtube', 0.048), ('triples', 0.047), ('open', 0.045), ('batch', 0.043), ('recovery', 0.042), ('ho', 0.041), ('social', 0.041), ('stochastic', 0.04), ('xxx', 0.04), ('synthetic', 0.038), ('gradient', 0.037), ('minibatch', 0.034), ('closed', 0.032), ('eq', 0.031), ('succinct', 0.029), ('yin', 0.029), ('dirichlet', 0.028), ('global', 0.028), ('equivalence', 0.027), ('aaa', 0.027), ('ergm', 0.027), ('junming', 0.027), ('odlis', 0.027), ('roget', 0.027), ('distinct', 0.026), ('local', 0.026), ('indices', 0.026), ('principles', 0.026), ('ascent', 0.026), ('orders', 0.025), ('guration', 0.025), ('seed', 0.025), ('overlapping', 0.025), ('community', 0.025), ('degree', 0.025), ('minority', 0.024), ('personalization', 0.024), ('majority', 0.024), ('law', 0.023), ('blocked', 0.023), ('posterior', 0.023), ('pittsburgh', 0.023), ('probabilities', 0.023), ('space', 0.022), ('per', 0.022), ('tensor', 0.022), ('govern', 0.022), ('qirong', 0.022), ('sociology', 0.022), ('governed', 0.022), ('hard', 0.022), ('convergence', 0.021), ('parameterization', 0.021), ('edge', 0.021), ('unbiased', 0.021), ('abc', 0.021), ('yeast', 0.021), ('eld', 0.021), ('participating', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="13-tfidf-1" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>2 0.2325514 <a title="13-tfidf-2" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>3 0.10906962 <a title="13-tfidf-3" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>Author: Dae Il Kim, Prem Gopalan, David Blei, Erik Sudderth</p><p>Abstract: Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efﬁcient structured mean ﬁeld variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show signiﬁcantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government. 1</p><p>4 0.084660865 <a title="13-tfidf-4" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><p>5 0.081556901 <a title="13-tfidf-5" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>Author: Michael Hughes, Erik Sudderth</p><p>Abstract: Variational inference algorithms provide the most effective framework for largescale training of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima. We present a new algorithm, memoized online variational inference, which scales to very large (yet ﬁnite) datasets while avoiding the complexities of stochastic gradient. Our algorithm maintains ﬁnite-dimensional sufﬁcient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples. Exploiting nested families of variational bounds for inﬁnite nonparametric models, we develop principled birth and merge moves allowing non-local optimization. Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed. Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.</p><p>6 0.078309722 <a title="13-tfidf-6" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>7 0.072259828 <a title="13-tfidf-7" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>8 0.071747683 <a title="13-tfidf-8" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>9 0.069020145 <a title="13-tfidf-9" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>10 0.068246752 <a title="13-tfidf-10" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>11 0.06680233 <a title="13-tfidf-11" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>12 0.065716624 <a title="13-tfidf-12" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>13 0.064859249 <a title="13-tfidf-13" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>14 0.062913351 <a title="13-tfidf-14" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>15 0.062886722 <a title="13-tfidf-15" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>16 0.061230268 <a title="13-tfidf-16" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>17 0.060109239 <a title="13-tfidf-17" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>18 0.060057022 <a title="13-tfidf-18" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>19 0.057027724 <a title="13-tfidf-19" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>20 0.055284869 <a title="13-tfidf-20" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.059), (2, -0.052), (3, 0.021), (4, 0.014), (5, 0.049), (6, 0.103), (7, -0.01), (8, 0.149), (9, -0.048), (10, 0.046), (11, 0.031), (12, 0.101), (13, -0.031), (14, -0.008), (15, -0.041), (16, -0.002), (17, -0.016), (18, -0.016), (19, -0.088), (20, 0.108), (21, -0.091), (22, -0.002), (23, -0.031), (24, 0.006), (25, -0.014), (26, 0.024), (27, 0.036), (28, 0.114), (29, -0.013), (30, -0.019), (31, -0.066), (32, 0.079), (33, -0.013), (34, -0.029), (35, -0.021), (36, -0.051), (37, 0.02), (38, 0.061), (39, 0.05), (40, 0.061), (41, -0.089), (42, 0.023), (43, 0.032), (44, 0.102), (45, -0.051), (46, 0.047), (47, -0.02), (48, 0.06), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93037015 <a title="13-lsi-1" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>2 0.89538962 <a title="13-lsi-2" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>Author: Dae Il Kim, Prem Gopalan, David Blei, Erik Sudderth</p><p>Abstract: Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efﬁcient structured mean ﬁeld variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show signiﬁcantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government. 1</p><p>3 0.8845042 <a title="13-lsi-3" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>4 0.70603216 <a title="13-lsi-4" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>Author: Myunghwan Kim, Jure Leskovec</p><p>Abstract: unkown-abstract</p><p>5 0.59719169 <a title="13-lsi-5" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>Author: Michael Hughes, Erik Sudderth</p><p>Abstract: Variational inference algorithms provide the most effective framework for largescale training of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima. We present a new algorithm, memoized online variational inference, which scales to very large (yet ﬁnite) datasets while avoiding the complexities of stochastic gradient. Our algorithm maintains ﬁnite-dimensional sufﬁcient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples. Exploiting nested families of variational bounds for inﬁnite nonparametric models, we develop principled birth and merge moves allowing non-local optimization. Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed. Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.</p><p>6 0.54461133 <a title="13-lsi-6" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>7 0.54220682 <a title="13-lsi-7" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>8 0.53546524 <a title="13-lsi-8" href="./nips-2013-Bayesian_Hierarchical_Community_Discovery.html">47 nips-2013-Bayesian Hierarchical Community Discovery</a></p>
<p>9 0.51585388 <a title="13-lsi-9" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>10 0.51233423 <a title="13-lsi-10" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<p>11 0.51178843 <a title="13-lsi-11" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>12 0.50454414 <a title="13-lsi-12" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>13 0.49364513 <a title="13-lsi-13" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>14 0.49342415 <a title="13-lsi-14" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>15 0.48816854 <a title="13-lsi-15" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>16 0.47505298 <a title="13-lsi-16" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>17 0.46914637 <a title="13-lsi-17" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>18 0.45998457 <a title="13-lsi-18" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>19 0.45378229 <a title="13-lsi-19" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>20 0.45337331 <a title="13-lsi-20" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (9, 0.32), (16, 0.05), (33, 0.1), (34, 0.107), (36, 0.035), (41, 0.02), (49, 0.023), (56, 0.104), (70, 0.027), (85, 0.042), (89, 0.024), (93, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7505886 <a title="13-lda-1" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>2 0.657368 <a title="13-lda-2" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>3 0.61541659 <a title="13-lda-3" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>Author: Yu-Xiang Wang, Huan Xu, Chenlei Leng</p><p>Abstract: Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for subspace clustering. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of “Self-Expressiveness”. The main difference is that SSC minimizes the vector 1 norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank, we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the “Self-Expressiveness Property” and “Graph Connectivity” at the same time. 1</p><p>4 0.61053497 <a title="13-lda-4" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>5 0.52039522 <a title="13-lda-5" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>6 0.51934254 <a title="13-lda-6" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>7 0.51931036 <a title="13-lda-7" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>8 0.51572067 <a title="13-lda-8" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>9 0.51444936 <a title="13-lda-9" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>10 0.51385146 <a title="13-lda-10" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>11 0.51280189 <a title="13-lda-11" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>12 0.51249093 <a title="13-lda-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.51214367 <a title="13-lda-13" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>14 0.5119428 <a title="13-lda-14" href="./nips-2013-Spike_train_entropy-rate_estimation_using_hierarchical_Dirichlet_process_priors.html">308 nips-2013-Spike train entropy-rate estimation using hierarchical Dirichlet process priors</a></p>
<p>15 0.51137197 <a title="13-lda-15" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>16 0.51127756 <a title="13-lda-16" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>17 0.51077688 <a title="13-lda-17" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>18 0.51014376 <a title="13-lda-18" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>19 0.51000786 <a title="13-lda-19" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>20 0.5088141 <a title="13-lda-20" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
