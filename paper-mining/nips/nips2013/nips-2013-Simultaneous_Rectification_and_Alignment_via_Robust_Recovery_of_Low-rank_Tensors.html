<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-295" href="#">nips2013-295</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</h1>
<br/><p>Source: <a title="nips-2013-295-pdf" href="http://papers.nips.cc/paper/5040-simultaneous-rectification-and-alignment-via-robust-recovery-of-low-rank-tensors.pdf">pdf</a></p><p>Author: Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, Yi Ma</p><p>Abstract: In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efﬁcient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special cases of our work, and yet each only performs part of the function of our method.</p><p>Reference: <a title="nips-2013-295-reference" href="../nips2013_reference/nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. [sent-6, score-0.149]
</p><p>2 Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. [sent-7, score-1.014]
</p><p>3 To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. [sent-8, score-0.151]
</p><p>4 The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. [sent-11, score-0.219]
</p><p>5 Many revolutionary new tools have been developed that enable people to recover low-dimensional structures in the form of sparse vectors or low-rank matrices in high dimensional data. [sent-15, score-0.134]
</p><p>6 In the literature, it has been shown that for matrix data, if the data is a deformed or corrupted version of an intrinsically low-rank matrix, one can recover the rectiﬁed low-rank structure despite different types of deformation (linear or nonlinear) and severe corruptions. [sent-19, score-0.214]
</p><p>7 Such concepts and methods have been successfully applied to rectify the so-called low-rank textures [1] and to align multiple correlated images (such as video frames or human faces) [2, 3, 4, 5, 6]. [sent-20, score-0.244]
</p><p>8 However, when applied to the data of higher-order tensorial form, such as videos or 3D range data, these tools are only able to harness one type of low-dimensional structure at a time, and are not able to exploit the low-dimensional 1  tensorial structures in the data. [sent-21, score-0.372]
</p><p>9 For instance, the previous work of TILT rectiﬁes a low-rank textural region in a single image [1] while RASL aligns multiple correlated images [6]. [sent-22, score-0.111]
</p><p>10 A natural question arises: can we simultaneously harness all such low-dimensional structures in an image sequence by viewing it as a three-order tensor? [sent-24, score-0.15]
</p><p>11 Actually, many existing visual data can be naturally viewed as three-order (or even higher-order) tensors (e. [sent-25, score-0.214]
</p><p>12 For tensorial data, however, one major challenge lies in an appropriate deﬁnition of the rank of a tensor, which corresponds to the notion of intrinsic “dimension” or “degree of freedom” for the tensorial data. [sent-30, score-0.382]
</p><p>13 Traditionally, there are two deﬁnitions of tensor rank, which are based on PARAFAC decomposition [7] and Tucker decomposition [8] respectively. [sent-31, score-0.586]
</p><p>14 Similar to the deﬁnition of matrix rank, the rank of a tensor based on PARAFAC decomposition is deﬁned as the minimum number of rank-one decompositions of a given tensor. [sent-32, score-0.725]
</p><p>15 However, this deﬁnition rank is a nonconvex and nonsmooth function on the tensor space, and direct minimization of this function is an NP-hard problem. [sent-33, score-0.681]
</p><p>16 An alternative deﬁnition of tensor rank is based on the so-called Tucker decomposition, which results in a vector of the ranks of a set of matrices unfolded from the tensor. [sent-34, score-0.761]
</p><p>17 Due to the recent breakthroughs in the recovery of low-rank matrices [9], the latter deﬁnition has received increasing attention. [sent-35, score-0.14]
</p><p>18 [10] adopt the sum of the ranks of the different unfolding matrices as the rank of the tensor data, which is in turn approximated by the sum of their nuclear norms. [sent-37, score-1.06]
</p><p>19 They then apply the alternating direction method (ADM) to solve the tensor completion problem with Gaussian observation noise. [sent-38, score-0.583]
</p><p>20 Instead of directly adding up the ranks of the unfolding matrices, a weighted sum of the ranks of the unfolding matrices is introduced by Liu et al. [sent-39, score-0.64]
</p><p>21 [12] and they also proposed several optimization algorithms to estimate missing values for tensorial visual data (such as color images). [sent-40, score-0.183]
</p><p>22 In [13], three different strategies have been developed to extend the trace-norm regularization to tensors: (1) tensors treated as matrices; (2) traditional constrained optimization of low rank tensors as in [12]; (3) a mixture of low-rank tensors. [sent-41, score-0.531]
</p><p>23 The above-mentioned work all addresses the tensor completion problem in which the locations of the missing entries are known, and moreover, observation noise is assumed to be Gaussian. [sent-42, score-0.553]
</p><p>24 However, in practice, a fraction of the tensorial entries can be arbitrarily corrupted by some large errors, and the number and the locations of the corrupted entries are unknown. [sent-43, score-0.342]
</p><p>25 [14] have extended the Robust Principal Component Analysis [9] from recovering a low-rank matrix to the tensor case. [sent-45, score-0.521]
</p><p>26 More precisely, they have proposed a method to recover a low-rank tensor with sparse errors. [sent-46, score-0.513]
</p><p>27 However, there are two issues that limit the practicality of such methods: (1) The tensorial data are assumed to be well aligned and rectiﬁed. [sent-47, score-0.124]
</p><p>28 Inspired by the previous work and motivated by the above observations, we propose a more general method for the recovery of low-rank tensorial data, especially three-order tensorial data, since our main interests are visual data. [sent-49, score-0.356]
</p><p>29 The main contributions of our work are three-fold: (1) The data samples in the tensor do not need to be well-aligned or rectiﬁed, and can be arbitrarily corrupted with a small fraction of errors. [sent-50, score-0.572]
</p><p>30 (2) This framework can simultaneously perform rectiﬁcation and alignment when applied to imagery data such as image sequences and video frames. [sent-51, score-0.206]
</p><p>31 (3) To resolve the interdependence among the nuclear norms of the unfolding matrices, we introduce auxiliary variables and relax the hard equality constraints using the augmented Lagrange multiplier method. [sent-53, score-0.518]
</p><p>32 To further improve the efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. [sent-54, score-0.151]
</p><p>33 Lowercase letters (a, b, c · · · ) denote scalars; bold lowercase (a, b, c · · · ) letters denote vectors; capital letters (A, B, C · · · ) denote matrices; calligraphic letters (A, B, C · · · ) denote tensors. [sent-57, score-0.125]
</p><p>34 In the following subsections, the tensor algebra and the tensor rank are brieﬂy introduced. [sent-58, score-1.161]
</p><p>35 2  I3 I3 I1  I2  I1  I2  I2  I2  A(1)  I1 I3 I1  I2  I2  I3  I3  I3  A(2)  I2 I3 I1  I2  I3  I1  I1  I1  A(3)  Figure 1: Illustration of unfolding a 3-order tensor. [sent-59, score-0.25]
</p><p>36 1  Tensor Algebra  We denote an N -order tensor as A ∈ RI1 ×I2 ×···×IN , where In (n = 1, 2, . [sent-61, score-0.492]
</p><p>37 Each element in this tensor is represented as ai1 ···in ···iN , where 1 ≤ in ≤ In . [sent-65, score-0.513]
</p><p>38 Each order of a tensor is associated with a ‘mode’. [sent-66, score-0.492]
</p><p>39 By unfolding a tensor along a mode, a tensor’s unfolding matrix corresponding to this mode is obtained. [sent-67, score-1.077]
</p><p>40 For example, the mode-n unfolding matrix ∏ A(n) ∈ RIn ×( i̸=n Ii ) of A, represented as A(n) = unfoldn (A), consists of In -dimensional moden column vectors which are obtained by varying the nth-mode index in and keeping indices of the other modes ﬁxed. [sent-68, score-0.321]
</p><p>41 1 shows an illustration of unfolding a 3-order tensor. [sent-70, score-0.25]
</p><p>42 The inverse operation of the mode-n unfolding is the mode-n folding which restores the original tensor A from the mode-n unfolding matrix A(n) , represented as A = foldn (A(n) ). [sent-71, score-1.042]
</p><p>43 The mode-n rank rn of A is deﬁned as the rank of the mode-n unfolding matrix A(n) : rn = rank(A(n) ). [sent-72, score-0.603]
</p><p>44 The operation of mode-n product of a tensor and a matrix forms a new tensor. [sent-73, score-0.521]
</p><p>45 The mode-n product of tensor A and matrix U is denoted as A ×n U . [sent-74, score-0.521]
</p><p>46 (1) in ∑ ∑ The scalar product of two tensors A and B with the dimension is deﬁned as ⟨A, B⟩ = i1 i2 · · √ ∑ · iN ai1 ···iN bi1 ···iN . [sent-77, score-0.184]
</p><p>47 The l0 norm ||A||0 is deﬁned to be the number of non-zero entries in A and the l1 norm ||A||1 = ∑ i1 ,. [sent-79, score-0.081]
</p><p>48 2 Tensor Rank Traditionally, there are two deﬁnitions of tensor rank, which are based on PARAFAC decomposition [7] and Tucker decomposition [8], respectively. [sent-85, score-0.586]
</p><p>49 However, this rank deﬁnition is a highly nonconvex and discontinuous function on the tensor space. [sent-91, score-0.685]
</p><p>50 Another kind of rank deﬁnition considers the mode-n rank rn of tensors, which is inspired by the Tucker decomposition [8]. [sent-93, score-0.343]
</p><p>51 The tensor A can be decomposed as follows: A = G ×1 U (1) ×2 U (2) · · · ×N U (N ) , ⊤  ⊤  ⊤  (3)  where G = A ×1 U (1) ×2 U (2) · · · ×N U (N ) is the core tensor controlling the interaction between the N mode matrices U (1) , . [sent-94, score-1.102]
</p><p>52 In the sense of Tucker decomposition, an appropriate deﬁnition of tensor rank should satisfy the follow condition: a low-rank tensor is a low-rank matrix when unfolded appropriately. [sent-98, score-1.181]
</p><p>53 This means the rank of a tensor can be represented by the rank of the 3  tensor’s unfolding matrices. [sent-99, score-1.031]
</p><p>54 As illustrated in [8], the orthonormal column vectors of U (n) span the column space of the mode-n unfolding matrix A(n) , (1 ≤ n ≤ N ), so that if U (n) ∈ RIn ×rn , n = 1, . [sent-100, score-0.321]
</p><p>55 , N , then the rank of the mode-n unfolding matrix A(n) is rn . [sent-103, score-0.441]
</p><p>56 We adopt this tensor rank deﬁnition in this paper. [sent-108, score-0.651]
</p><p>57 3  Low-rank Structure Recovery for Tensors  In this section, we ﬁrst formulate the problem of recovering low-rank tensors despite deformation and corruption, and then introduce an iterative optimization method to solve the low-rank recovery problem. [sent-109, score-0.337]
</p><p>58 1  Problem Formulation  Without loss of generality, in this paper we focus on 3-order tensors to study the low-rank recovery problem. [sent-112, score-0.262]
</p><p>59 Consider a low-rank 3-order data tensor A ∈ RI1 ×I2 ×I3 . [sent-114, score-0.492]
</p><p>60 In real applications, the data are inevitably corrupted by noise or errors. [sent-115, score-0.08]
</p><p>61 Based on the above assumptions, the original tensor data A can be represented as A = L + E, (4) where L is a low-rank tensor. [sent-117, score-0.513]
</p><p>62 (4) is that it requires the tensor to be well aligned. [sent-121, score-0.492]
</p><p>63 For real data such as video and face images, the image frames (face images) should be well aligned to ensure that the three-order tensor of the image stack to have low-rank. [sent-122, score-0.661]
</p><p>64 , τI3 ∈ Rp (p is the dimension of the transformations) which act on the two-dimensional slices (matrices) of the tensor data1 . [sent-127, score-0.492]
</p><p>65 When both corruption and misalignment are modeled, the low-rank structure recovery for tensors can be formalized as follows. [sent-136, score-0.287]
</p><p>66 (6) L,E,Γ  The above optimization problem is not directly tractable for the following two reasons: (1) both rank and ℓ0 -norm are nonconvex and discontinuous; (2) the equality constraint A ◦ Γ = L + E is highly nonlinear due to the domain transformation Γ. [sent-140, score-0.252]
</p><p>67 To relax the limitation (1), we ﬁrst recall the tensor rank deﬁnition in Section 2. [sent-141, score-0.656]
</p><p>68 In our work, we adopt the rank deﬁnition based on the Tucker decomposition which can be represented as follows: L is a rank-(r1 , r2 , r3 ) tensor where ri is the rank of unfolding matrix L(i) . [sent-143, score-1.132]
</p><p>69 In this way, tensor rank can be converted to calculating a set of matrices’ rank. [sent-144, score-0.626]
</p><p>70 We know that the nuclear (or trace) norm is ∑m the convex envelop of the rank of matrix: ||L(i) ||∗ = k=1 σk (L(i) ), where σk (L(i) ) is kth singular value of matrix L(i) . [sent-145, score-0.247]
</p><p>71 Therefore, we deﬁne the nuclear norm of a three-order tensor as follows: ||L||∗ =  N ∑  αi ||L(i) ||∗ , N = 3. [sent-146, score-0.576]
</p><p>72 The rank of L is replaced by ||L||∗ to make a convex relaxation of the optimization problem. [sent-148, score-0.163]
</p><p>73 It is well know that 1 In most applications, a three-order tensor can be naturally partitioned into a set of matrices (such as image frames in a video) and transformations should be applied on these matrices  4  ℓ1 -norm is a good convex surrogate of the ℓ0 -norm. [sent-149, score-0.724]
</p><p>74 2  Optimization Algorithm  Although the problem in (9) is convex, it is still difﬁcult to solve due to the interdependent nuclear norm terms. [sent-159, score-0.109]
</p><p>75 To remove these interdependencies and to optimize these terms independently, we introduce three auxiliary matrices {Mi , i = 1, 2, 3} to replace {L(i) , i = 1, 2, 3}, and the optimization problem changes to 3 ∑ ˜ min αi ||Mi ||∗ + γ||E||1 s. [sent-160, score-0.112]
</p><p>76 Y and Qi are the Lagrange multiplier tensor and matrix respectively. [sent-165, score-0.593]
</p><p>77 n ∑ ˜ ∆Γk+1 = Ji+ (∆Γk+1 )⊤ ϵi ϵ⊤ , i (3) i=1  ˜ where Ji+ = (Ji⊤ Ji )−1 Ji⊤ is pseudo-inverse of Ji and (∆Γk+1 )(3) is the mode-3 unfolding ˜k+1 . [sent-179, score-0.25]
</p><p>78 matrix of tensor ∆Γ • For terms Y k+1 and Qk+1 : i Y k+1 = Y k − T k+1 /µ;  Qk+1 = Qk − (Lk+1 − Mik+1 )/µ, i i (i)  i = 1, 2, 3. [sent-180, score-0.521]
</p><p>79 Consider the mode3 unfolding matrix A(3) in the bottom row of Fig. [sent-189, score-0.279]
</p><p>80 Suppose the tensor is formed by stacking a set of images along the third mode. [sent-191, score-0.565]
</p><p>81 While for the mode-1 and mode-2 unfolding matrices (see Fig. [sent-193, score-0.312]
</p><p>82 RASL: In the image alignment applications, RASL treats each image as a vector and does not make use of any spatial structure within each image. [sent-200, score-0.213]
</p><p>83 1, in our work, the low-rank constraint on the mode-1 and mode-2 unfolding matrices effectively harnesses the spatial structures within images. [sent-202, score-0.453]
</p><p>84 TILT: TILT deals with only one image and harnesses spatial low-rank structures to rectify the image. [sent-205, score-0.225]
</p><p>85 Moreover, since RASL is applied to one mode of the tensor, to make it more competitive, we apply RASL to each mode of the tensor and take the mode that has the minimal reconstruction error. [sent-224, score-0.691]
</p><p>86 For synthetic data, we ﬁrst randomly generate two data tensors: (1) a pure low-rank tensor Lo ∈ R50×50×50 whose rank is (10,10,10); (2) an error tensor E ∈ R50×50×50 in which only a fraction c of entries are non-zero (To ensure the error to be sparse, the maximal value of c is set to 40%). [sent-225, score-1.174]
</p><p>87 Then the testing tensor A can be obtained as A = Lo + E. [sent-226, score-0.492]
</p><p>88 The above results demonstrate the effectiveness and efﬁciency of our proposed optimization method for low-rank tensor recovery. [sent-238, score-0.521]
</p><p>89 The ﬁrst dataset contains 16 images of the side of a building, taken from various viewpoints by a perspective camera, and with various occlusions due to tree branches. [sent-241, score-0.101]
</p><p>90 3 illustrates the low-rank recovery results on this data set, in which Fig. [sent-243, score-0.078]
</p><p>91 The second data set contains 100 images of the handwritten number “3”, with a fair amount of diversity. [sent-254, score-0.073]
</p><p>92 5, the face alignment results obtained by our work is signiﬁcantly better than those obtained by the other two algorithms. [sent-262, score-0.129]
</p><p>93 The reason is that human face has a rich spatial low-rank structures due to symmetry, and our method simultaneously harnesses both temporal and spatial low-rank structures for rectiﬁcation and alignment. [sent-263, score-0.28]
</p><p>94 5  Conclusion  We have in this paper proposed a general low-rank recovery framework for arbitrary tensor data, which can simultaneously realize rectiﬁcation and alignment. [sent-264, score-0.622]
</p><p>95 We have adopted a proximal gradient based alternating direction method to solve the optimization problem, and have shown that the convergence of our algorithm is guaranteed. [sent-265, score-0.158]
</p><p>96 Learned-Miller, “Unsupervised joint alignment of complex images”, International Conference on Computer Vision pp. [sent-278, score-0.104]
</p><p>97 Kruskal, “Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics”, Linear Algebra and its Applications, 18(2): 95-138, 1977. [sent-302, score-0.134]
</p><p>98 Kashima, “Estimation of low-rank tensors via convex optimization”, Technical report, arXiv:1010. [sent-329, score-0.184]
</p><p>99 Ma, “The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices”, Technical Report UILU-ENG-09-2215, UIUC Technical Report, 2009. [sent-341, score-0.362]
</p><p>100 Yuan, “Linearized augmented lagrangian and alternating direction methods for nuclear norm minimization”, Mathematics of Computation, 82(281): 301-329, 2013. [sent-344, score-0.23]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rasl', 0.57), ('tensor', 0.492), ('unfolding', 0.25), ('tensors', 0.184), ('tilt', 0.154), ('mik', 0.151), ('rank', 0.134), ('tensorial', 0.124), ('recti', 0.124), ('lk', 0.123), ('qk', 0.115), ('alignment', 0.104), ('mi', 0.086), ('alm', 0.083), ('corrupted', 0.08), ('recovery', 0.078), ('apgp', 0.076), ('ji', 0.074), ('images', 0.073), ('multiplier', 0.072), ('lagrange', 0.07), ('tucker', 0.067), ('li', 0.064), ('matrices', 0.062), ('augmented', 0.062), ('nuclear', 0.058), ('lo', 0.058), ('apg', 0.058), ('harnesses', 0.057), ('ialm', 0.057), ('mode', 0.056), ('structures', 0.051), ('proximal', 0.049), ('decomposition', 0.047), ('rectify', 0.046), ('parafac', 0.046), ('deformation', 0.046), ('algebra', 0.043), ('videos', 0.042), ('ranks', 0.039), ('deformed', 0.038), ('foldi', 0.038), ('misalignments', 0.038), ('rin', 0.038), ('image', 0.038), ('alternating', 0.036), ('transformations', 0.036), ('oi', 0.036), ('qi', 0.034), ('frames', 0.034), ('video', 0.034), ('linearized', 0.034), ('unfolded', 0.034), ('spatial', 0.033), ('nonconvex', 0.033), ('completion', 0.032), ('reconstruction', 0.031), ('harness', 0.031), ('transformation', 0.031), ('relax', 0.03), ('visual', 0.03), ('simultaneously', 0.03), ('entries', 0.029), ('gandy', 0.029), ('textures', 0.029), ('lowercase', 0.029), ('matrix', 0.029), ('optimization', 0.029), ('rn', 0.028), ('viewpoints', 0.028), ('align', 0.028), ('ganesh', 0.028), ('uj', 0.027), ('ciency', 0.027), ('nition', 0.027), ('synthetic', 0.027), ('ma', 0.027), ('jn', 0.026), ('discontinuous', 0.026), ('norm', 0.026), ('interdependent', 0.025), ('face', 0.025), ('equality', 0.025), ('lagrangian', 0.025), ('adopt', 0.025), ('corruption', 0.025), ('arg', 0.024), ('letters', 0.024), ('decompositions', 0.023), ('direction', 0.023), ('lr', 0.022), ('realize', 0.022), ('minimization', 0.022), ('auxiliary', 0.021), ('recover', 0.021), ('gradient', 0.021), ('validated', 0.021), ('represented', 0.021), ('vision', 0.021), ('column', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="295-tfidf-1" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>Author: Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, Yi Ma</p><p>Abstract: In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efﬁcient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special cases of our work, and yet each only performs part of the function of our method.</p><p>2 0.42121699 <a title="295-tfidf-2" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>Author: Bernardino Romera-Paredes, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves signiﬁcantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. 1</p><p>3 0.34286919 <a title="295-tfidf-3" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>Author: Shouyuan Chen, Michael R. Lyu, Irwin King, Zenglin Xu</p><p>Abstract: Tensor completion from incomplete observations is a problem of signiﬁcant practical interest. However, it is unlikely that there exists an efﬁcient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Speciﬁcally, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art results. 1</p><p>4 0.29635686 <a title="295-tfidf-4" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki</p><p>Abstract: We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (“overlapped” and “latent”) for convex-optimizationbased tensor decomposition. We analyze the performance of “latent” approach for tensor decomposition, which was empirically found to perform better than the “overlapped” approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a speciﬁc unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We conﬁrm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error. 1</p><p>5 0.29619852 <a title="295-tfidf-5" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>Author: Tzu-Kuo Huang, Jeff Schneider</p><p>Abstract: Learning dynamic models from observed data has been a central issue in many scientiﬁc studies or engineering tasks. The usual setting is that data are collected sequentially from trajectories of some dynamical system operation. In quite a few modern scientiﬁc modeling tasks, however, it turns out that reliable sequential data are rather difﬁcult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer’s, or certain biological processes. Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze. Inspired by recent advances in spectral learning methods, we propose to study this problem from a different perspective: moment matching and spectral decomposition. Under that framework, we identify reasonable assumptions on the generative process of non-sequence data, and propose learning algorithms based on the tensor decomposition method [2] to provably recover ﬁrstorder Markov models and hidden Markov models. To the best of our knowledge, this is the ﬁrst formal guarantee on learning from non-sequence data. Preliminary simulation results conﬁrm our theoretical ﬁndings. 1</p><p>6 0.27724063 <a title="295-tfidf-6" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>7 0.23268056 <a title="295-tfidf-7" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>8 0.18676026 <a title="295-tfidf-8" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>9 0.13511132 <a title="295-tfidf-9" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>10 0.099226303 <a title="295-tfidf-10" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>11 0.098197147 <a title="295-tfidf-11" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>12 0.07720273 <a title="295-tfidf-12" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>13 0.075251564 <a title="295-tfidf-13" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>14 0.074575201 <a title="295-tfidf-14" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>15 0.072479382 <a title="295-tfidf-15" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>16 0.071175121 <a title="295-tfidf-16" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>17 0.063787453 <a title="295-tfidf-17" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>18 0.061812565 <a title="295-tfidf-18" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>19 0.059615932 <a title="295-tfidf-19" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>20 0.059450451 <a title="295-tfidf-20" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, 0.124), (2, 0.124), (3, 0.352), (4, 0.021), (5, -0.377), (6, 0.059), (7, -0.049), (8, 0.167), (9, 0.048), (10, -0.054), (11, -0.046), (12, -0.023), (13, 0.028), (14, -0.031), (15, -0.003), (16, -0.055), (17, -0.097), (18, 0.043), (19, 0.029), (20, 0.01), (21, 0.142), (22, -0.02), (23, 0.042), (24, -0.056), (25, 0.047), (26, 0.008), (27, 0.106), (28, 0.045), (29, 0.01), (30, 0.036), (31, 0.072), (32, -0.029), (33, 0.01), (34, -0.024), (35, -0.032), (36, -0.023), (37, 0.008), (38, -0.048), (39, 0.0), (40, 0.025), (41, -0.032), (42, 0.049), (43, 0.045), (44, -0.007), (45, -0.001), (46, 0.006), (47, 0.025), (48, 0.004), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95863748 <a title="295-lsi-1" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>Author: Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, Yi Ma</p><p>Abstract: In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efﬁcient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special cases of our work, and yet each only performs part of the function of our method.</p><p>2 0.90491587 <a title="295-lsi-2" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>Author: Bernardino Romera-Paredes, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves signiﬁcantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. 1</p><p>3 0.90381855 <a title="295-lsi-3" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>Author: Mark Rogers, Lei Li, Stuart Russell</p><p>Abstract: Data in the sciences frequently occur as sequences of multidimensional arrays called tensors. How can hidden, evolving trends in such data be extracted while preserving the tensor structure? The model that is traditionally used is the linear dynamical system (LDS) with Gaussian noise, which treats the latent state and observation at each time slice as a vector. We present the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation–maximization (EM) algorithm to estimate the parameters. The MLDS models each tensor observation in the time series as the multilinear projection of the corresponding member of a sequence of latent tensors. The latent tensors are again evolving with respect to a multilinear projection. Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both artiﬁcial and real datasets. 1</p><p>4 0.85455352 <a title="295-lsi-4" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki</p><p>Abstract: We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (“overlapped” and “latent”) for convex-optimizationbased tensor decomposition. We analyze the performance of “latent” approach for tensor decomposition, which was empirically found to perform better than the “overlapped” approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a speciﬁc unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We conﬁrm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error. 1</p><p>5 0.84267288 <a title="295-lsi-5" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>Author: Shouyuan Chen, Michael R. Lyu, Irwin King, Zenglin Xu</p><p>Abstract: Tensor completion from incomplete observations is a problem of signiﬁcant practical interest. However, it is unlikely that there exists an efﬁcient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Speciﬁcally, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art results. 1</p><p>6 0.68456215 <a title="295-lsi-6" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>7 0.59979469 <a title="295-lsi-7" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>8 0.45329201 <a title="295-lsi-8" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>9 0.39838836 <a title="295-lsi-9" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>10 0.37088317 <a title="295-lsi-10" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>11 0.31740415 <a title="295-lsi-11" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>12 0.31203642 <a title="295-lsi-12" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>13 0.31083632 <a title="295-lsi-13" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>14 0.30655283 <a title="295-lsi-14" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>15 0.30229339 <a title="295-lsi-15" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>16 0.29674715 <a title="295-lsi-16" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>17 0.29571539 <a title="295-lsi-17" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>18 0.27925691 <a title="295-lsi-18" href="./nips-2013-On_Algorithms_for_Sparse_Multi-factor_NMF.html">214 nips-2013-On Algorithms for Sparse Multi-factor NMF</a></p>
<p>19 0.26123458 <a title="295-lsi-19" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>20 0.25017986 <a title="295-lsi-20" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.029), (33, 0.165), (34, 0.08), (41, 0.062), (49, 0.042), (56, 0.113), (70, 0.04), (80, 0.23), (85, 0.045), (89, 0.027), (93, 0.039), (95, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80913037 <a title="295-lda-1" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>Author: Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, Yi Ma</p><p>Abstract: In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efﬁcient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special cases of our work, and yet each only performs part of the function of our method.</p><p>2 0.80781949 <a title="295-lda-2" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>Author: Deepti Pachauri, Risi Kondor, Vikas Singh</p><p>Abstract: The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efﬁcient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods. 1</p><p>3 0.70977074 <a title="295-lda-3" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>Author: Bernardino Romera-Paredes, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves signiﬁcantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. 1</p><p>4 0.7075237 <a title="295-lda-4" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin</p><p>Abstract: In this paper, we are interested in the development of efﬁcient algorithms for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the ﬁrst-order information. We cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds. We ﬁrst examine a two stages exploration-exploitation based algorithm which ﬁrst approximates the stochastic objectives by sampling and then solves a constrained stochastic optimization problem by projected gradient method. This method attains a suboptimal convergence rate even under strong assumption on the objectives. Our second approach is an efﬁcient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method √ conin strained optimization and attains the optimal convergence rate of O(1/ T ) in high probability for general Lipschitz continuous objectives.</p><p>5 0.70439345 <a title="295-lda-5" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>Author: Tianbao Yang</p><p>Abstract: We present and study a distributed optimization algorithm by employing a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between computation and communication. We verify our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances. 1</p><p>6 0.70412487 <a title="295-lda-6" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>7 0.69989932 <a title="295-lda-7" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>8 0.69945443 <a title="295-lda-8" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>9 0.69904989 <a title="295-lda-9" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>10 0.69850892 <a title="295-lda-10" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>11 0.69837475 <a title="295-lda-11" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>12 0.69817042 <a title="295-lda-12" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>13 0.69713986 <a title="295-lda-13" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>14 0.69695318 <a title="295-lda-14" href="./nips-2013-Accelerating_Stochastic_Gradient_Descent_using_Predictive_Variance_Reduction.html">20 nips-2013-Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</a></p>
<p>15 0.69685364 <a title="295-lda-15" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>16 0.69684082 <a title="295-lda-16" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>17 0.69630241 <a title="295-lda-17" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>18 0.69584692 <a title="295-lda-18" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>19 0.69489193 <a title="295-lda-19" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>20 0.69439435 <a title="295-lda-20" href="./nips-2013-Annealing_between_distributions_by_averaging_moments.html">36 nips-2013-Annealing between distributions by averaging moments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
