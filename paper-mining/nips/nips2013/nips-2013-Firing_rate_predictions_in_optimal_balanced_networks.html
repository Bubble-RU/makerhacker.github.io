<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2013-Firing rate predictions in optimal balanced networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-121" href="#">nips2013-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2013-Firing rate predictions in optimal balanced networks</h1>
<br/><p>Source: <a title="nips-2013-121-pdf" href="http://papers.nips.cc/paper/5053-firing-rate-predictions-in-optimal-balanced-networks.pdf">pdf</a></p><p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>Reference: <a title="nips-2013-121-reference" href="../nips2013_reference/nips-2013-Firing_rate_predictions_in_optimal_balanced_networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Firing rate predictions in optimal balanced networks  Sophie Den` ve e Group for Neural Theory ´ Ecole Normale Sup´ rieure e Paris, France sophie. [sent-1, score-0.376]
</p><p>2 org  Abstract How are ﬁring rates in a spiking network related to neural input, connectivity and network function? [sent-11, score-0.948]
</p><p>3 This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. [sent-12, score-0.569]
</p><p>4 However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. [sent-13, score-0.576]
</p><p>5 We develop a new technique for calculating ﬁring rates in optimal balanced networks. [sent-14, score-0.404]
</p><p>6 These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. [sent-15, score-0.629]
</p><p>7 We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. [sent-16, score-0.748]
</p><p>8 Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. [sent-18, score-0.752]
</p><p>9 A large, sometimes bewildering, diversity of ﬁring rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6]. [sent-21, score-0.748]
</p><p>10 What is the computational role of these ﬁring rate responses and how are ﬁring rates determined by neuron dynamics, network connectivity and neural input? [sent-22, score-0.822]
</p><p>11 However, most approaches have struggled to deal with the non-linearity of neural spike-generation mechanisms and the strong interaction between neurons as mediated through network connectivity. [sent-24, score-0.383]
</p><p>12 These calculations have led to important insights into how neural network connectivity and input determine ﬁring rates. [sent-29, score-0.455]
</p><p>13 We develop a new technique for calculating ﬁring rates, by directly identifying the non-linear structure of tightly balanced networks. [sent-31, score-0.395]
</p><p>14 Balanced network theory has come to be regarded as the standard model of cortical activity [12, 13], accounting for a large proportion of observed activity through a dynamic balance of excitation and inhibition [14]. [sent-32, score-0.577]
</p><p>15 Recently, it was found that tightly balanced networks are synonymous with efﬁcient coding, in which a signal is represented optimally subject to metabolic costs [15]. [sent-33, score-0.514]
</p><p>16 This observation allows us, here, to interpret balanced network activity as an optimisation algorithm. [sent-34, score-0.581]
</p><p>17 We use this technique to calculate ﬁring rates in a variety of balanced network models, thereby exploring the computational role and underlying network mechanisms of monotonic ﬁring rate tuning curves, bump-shaped tuning curves and tuning curve inhomogeneity. [sent-36, score-1.669]
</p><p>18 2  Optimal balanced network models  We calculate ﬁring rates in a balanced network consisting of N recurrently connected leaky integrate-and-ﬁre neurons (Fig. [sent-37, score-1.165]
</p><p>19 , sN ), where si (t) = k δ(t − ti ) is the spike train of neuron i with spike times ti . [sent-52, score-0.566]
</p><p>20 A spike is produced k k whenever the membrane potential Vi exceeds the spiking threshold Ti of neuron i. [sent-53, score-0.746]
</p><p>21 The membrane potential has the following dynamics: dVi = −λVi + dt  M  N  Fij Ij ,  Ωik sk +  (1)  j=1  k=1  where λ is the neuron leak, Ωik is connection strength from neuron k to neuron i and Fij is the connection strength from input j to neuron i [16]. [sent-55, score-1.047]
</p><p>22 When a neuron spikes, the membrane potential is reset to Ri ≡ Ti + Ωii . [sent-56, score-0.364]
</p><p>23 We are interested in networks where a balance of excitation and inhibition coincides with optimal signal representation. [sent-59, score-0.363]
</p><p>24 Not all choices of network connectivity and spiking thresholds will give both [12, 13], but if certain conditions are satisﬁed, this can be possible. [sent-60, score-0.595]
</p><p>25 rk =  (4)  0  and xj is a temporal ﬁltering of the j th input ∞  xj = 0  All the excitatory and inhibitory inputs received by neuron i are included in this summation (Eqn. [sent-64, score-0.353]
</p><p>26 Now, we can use this expression to derive the conditions that connectivity must satisfy so that the network operates in an optimal balanced state. [sent-67, score-0.569]
</p><p>27 In balanced networks, excitation and inhibition cancel to produce an input that is the same order of magnitude as the spiking threshold. [sent-68, score-0.664]
</p><p>28 In tightly balanced networks, which we consider, this cancellation is so precise that Vi → 0 in the large network limit (for all active neurons) [15, 17, 18]. [sent-70, score-0.554]
</p><p>29 This has two implications for our choice of network connectivity and spiking thresholds. [sent-73, score-0.595]
</p><p>30 Secondly, the spiking threshold of each neuron must be chosen so that each spike acts to minimise the cost function. [sent-76, score-0.637]
</p><p>31 Finally,  (A)  (C)  x  x ˆ  x, x ˆ  (B)  time (sec)  Figure 1: Optimal balanced network example. [sent-79, score-0.394]
</p><p>32 (A) Schematic of a balanced neural network providing an optimal spike-based representation x of a signal x. [sent-80, score-0.554]
</p><p>33 (B) A tightly balanced network can ˆ produce an output x1 (blue, top panel) that closely matches the signal x1 (black, top panel). [sent-81, score-0.616]
</p><p>34 Popˆ ulation spiking activity is represented here using a raster plot (middle panel), where each spike is represented with a dot. [sent-82, score-0.455]
</p><p>35 For a randomly chosen neuron (red, middle panel), we plot the total excitatory input (green, bottom panel) and the total inhibitory input (red, bottom panel). [sent-83, score-0.456]
</p><p>36 The sum of excitation and inhibition (black, bottom panel) ﬂuctuates about the spiking threshold (thin black line, bottom panel) indicating that this network is tightly balanced. [sent-84, score-0.803]
</p><p>37 A spike is produced whenever this sum exceeds the spiking threshold. [sent-85, score-0.382]
</p><p>38 (C) Firing rate tuning curves are measured during simulations of our balanced network. [sent-86, score-0.632]
</p><p>39 Therefore, the spiking threshold for each neuron must be set to Tk ≡ −Ωkk /2, though this condition can be relaxed considerably if our loss function has an additional linear cost term1 . [sent-90, score-0.452]
</p><p>40 We are interested in networks that are both tightly balanced and optimal. [sent-92, score-0.39]
</p><p>41 This is an important result, because it relates balanced network dynamics to a neural computation. [sent-95, score-0.486]
</p><p>42 Speciﬁcally, it allows us to interpret the spiking activity of our tightly balanced network as an algorithm that optimises a loss function (Eqn. [sent-96, score-0.901]
</p><p>43 (8) The second term of equation 7 is a metabolic cost term that penalises neurons for spiking excessively, ˆ and the ﬁrst term quantiﬁes the difference between the signal value x and a linear read-out, x, where ˆ x is computed using the linear decoder FT (Eqn. [sent-103, score-0.572]
</p><p>44 Therefore, a network with this connectivity ˆ produces spike trains that optimise equation 7, thereby producing an output x that is close to the signal value x. [sent-105, score-0.661]
</p><p>45 We ﬁnd that our network produces spike trains (Fig. [sent-108, score-0.353]
</p><p>46 We measure ﬁring rate tuning curves using a ﬁxed value of x2 while varying x1 . [sent-114, score-0.416]
</p><p>47 We use this signal because it can produce interesting, non-linear tuning curves (Fig. [sent-115, score-0.426]
</p><p>48 In the next section, we will attempt to understand this tuning curve non-linearity by calculating ﬁring rates analytically. [sent-117, score-0.456]
</p><p>49 3  Firing rate analysis with quadratic programming  Our goal is to calculate the ﬁring rates f of all the neurons in these tightly balanced network models as a function of the network input, the recurrent network connectivity Ω, and the feedforward connectivity F. [sent-118, score-1.767]
</p><p>50 On the surface, this may seem to be a difﬁcult problem, because individual neurons have complicated non-linear integrate-and-ﬁre dynamics and they interact strongly through network connectivity. [sent-119, score-0.399]
</p><p>51 Then, we ﬁnd that the optimal spiking thresholds for this network are given by Ti ≡ (−Ωii + bi )/2 ≥ −Ωii /2. [sent-127, score-0.42]
</p><p>52 We can now calculate ﬁring rates using this relationship and by exploiting the algorithmic nature of tightly balanced networks. [sent-133, score-0.566]
</p><p>53 Therefore, the ﬁring rates of our network are those that minimise E(f /λ), under the constraint that ﬁring rates must be positive: {fi } = arg min E(f /λ) . [sent-136, score-0.497]
</p><p>54 When both neurons are active, we can solve equation 10 exactly, to see that ﬁring rates are related to network connectivity according to f = −λΩ−1 Fx. [sent-146, score-0.696]
</p><p>55 When one of the neurons becomes silent, the other neuron must compensate by adjusting its ﬁring rate slope. [sent-147, score-0.461]
</p><p>56 For example, when neuron 1 becomes silent, we have f1 = 0 and the ﬁring rate of neuron 2 increases to f2 = λF2 x/(F2 FT + βI), where F2 denotes the second row of F. [sent-148, score-0.504]
</p><p>57 Predicted ﬁring rates closely match measured ﬁring rates for both neurons, and for all signal values (right). [sent-153, score-0.368]
</p><p>58 We also measure the ﬁring rate trajectory (right panel) as the network evolves towards the minimum of the cost function E(x1 = 1) (blue cross, right panel), where neuron 2 is silent. [sent-157, score-0.472]
</p><p>59 In general, we can interpret tuning curve shape to be the solution of a quadratic programming problem, which can be written as a piece-wise linear function f = M (x) · x, where M(x) is a matrix whose entries depend on the region of signal space occupied by x. [sent-163, score-0.476]
</p><p>60 For example, in the two-neuron system that we just discussed, the signal space is partitioned into three regions: one region where neuron 1 is active and where neuron 2 is silent, a second region where both neurons are active and a third region where neuron 1 is silent and neuron 2 is active (Fig. [sent-164, score-1.342]
</p><p>61 The boundaries of these regions occur at points in signal space where an active neuron becomes silent (or where a silent neuron becomes active). [sent-167, score-0.786]
</p><p>62 We can also use quadratic programming to describe the spiking dynamics underlying these nonlinear networks. [sent-169, score-0.376]
</p><p>63 The step-size is λ because when neuron i spikes, ri → ri + 1, according to equation 3, and therefore, fi → fi +λ, according to equation 9. [sent-173, score-0.41]
</p><p>64 Eventually, when the ﬁring rate has decayed too far from the optimal solution, another spike is ﬁred and the network moves closer to the optimum. [sent-178, score-0.402]
</p><p>65 In this way, spiking dynamics can be interpreted as a quadratic programming algorithm. [sent-179, score-0.376]
</p><p>66 4  Analysing tuning curve shape with quadratic programming  Now that we have a framework for relating ﬁring rates to network connectivity and input, we can explore the computational function of tuning curve shapes and the network mechanisms that generate these tuning curves. [sent-183, score-1.484]
</p><p>67 We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of ﬁring rate observations [2, 3, 4, 5]. [sent-184, score-0.653]
</p><p>68 We begin by considering a system of monotonic tuning curves, similar to the examples that we have considered already where recurrent connectivity is given by Ω = −FFT − βI. [sent-185, score-0.469]
</p><p>69 In these systems, the recurrent connectivity and hence the tuning curve shape is largely determined by the form of the feedforward matrix F. [sent-186, score-0.475]
</p><p>70 This matrix also determines the contribution of tuning curves to computational function, through its role as a linear decoder for signal representation (Eqn. [sent-187, score-0.454]
</p><p>71 This system produces monotonically increasing and decreasing tuning curves (Fig. [sent-191, score-0.357]
</p><p>72 3, blue tuning curves), and neurons with negative F values have negative ﬁring rate slopes (Fig. [sent-194, score-0.519]
</p><p>73 If the values of F are regularly spaced, then the tuning curves of individual neurons are regularly spaced, and, if we manipulate this regularity by adding some random noise to the connectivity, we obtain inhomogeneous and highly irregular tuning curves (Fig. [sent-196, score-0.99]
</p><p>74 This inhomogeneous monotonic tuning is reminiscent of tuning in many neural systems, including the oculomotor system [4]. [sent-199, score-0.606]
</p><p>75 The oculomotor system represents eye position, using neurons with negative slopes to represent left side eye positions and neurons with positive slopes to represent right side eye positions. [sent-200, score-0.714]
</p><p>76 (A) Each dot represents the contribution of a neuron to a signal representation (when the ﬁring rate is 10 × 16 Hz) (1st column). [sent-202, score-0.416]
</p><p>77 We simulate a network of neurons and measure ﬁring rates (2nd column). [sent-204, score-0.482]
</p><p>78 These measurements closely match our algorithmically predicted ﬁring rates (3rd column), where each point in the 4th column represents the ﬁring rate of an individual neuron for a given stimulus. [sent-205, score-0.485]
</p><p>79 The representation error (bottom panels, column 2 and column 3) is similar to the network without connectivity noise. [sent-207, score-0.437]
</p><p>80 Each dot represents the contribution of a neuron to a signal representation (when the ﬁring rate is 20 × 16 Hz) (1st column). [sent-209, score-0.416]
</p><p>81 This signal produces bump-shaped tuning curves (2nd column), which we can also predict accurately (3rd and 4th column). [sent-210, score-0.426]
</p><p>82 (A)  (B)  leak  membrane potential noise ⌘  Figure 4: Performance of quadratic programming in ﬁring rate prediction. [sent-211, score-0.357]
</p><p>83 Now, we can use the relationship that we have developed between tuning curves and computational function to interpret oculomotor tuning as an attempt to represent eye positions optimally. [sent-218, score-0.719]
</p><p>84 Bump-shaped tuning curves can be produced by networks representing circular variables x1 = cos θ, x2 = sin θ, where θ is the orientation of the signal (Fig. [sent-219, score-0.472]
</p><p>85 As before, the tuning curves of individual neurons are regularly spaced if the values of F are regularly spaced. [sent-221, score-0.634]
</p><p>86 If we add some noise to the connectivity F, the tuning curves become inhomogeneous and highly irregular. [sent-222, score-0.562]
</p><p>87 The success of our algorithmic approach in calculating ﬁring rates depends on the success of spiking networks in algorithmically optimising a cost function. [sent-226, score-0.532]
</p><p>88 The resolution of this spiking algorithm is determined by the leak λ and membrane potential noise. [sent-227, score-0.435]
</p><p>89 5  Discussion and Conclusions  We have developed a new algorithmic technique for calculating ﬁring rates in tightly balanced networks. [sent-233, score-0.532]
</p><p>90 Identifying such relationships is a long-standing problem in systems neuroscience, largely because the mathematical language that we use to describe information representation is very different to the language that we use to describe neural network spiking statistics. [sent-236, score-0.486]
</p><p>91 For tightly balanced networks, we have essentially solved this problem, by matching the ﬁring rate statistics of neural activity to the structure of neural signal representation. [sent-237, score-0.671]
</p><p>92 Previous studies have also interpreted ﬁring rates to be the result of a constrained optimisation problem [21], but for a population coding model, not for a network of spiking neurons. [sent-239, score-0.691]
</p><p>93 In a more recent study, a spiking network was used to solve an optimisation problem, although this network required positive and negative spikes, which is difﬁcult to reconcile with biological spiking [22]. [sent-240, score-0.92]
</p><p>94 The ﬁring rate tuning curves that we calculate have allowed us to investigate poorly understood features of experimentally recorded tuning curves. [sent-241, score-0.655]
</p><p>95 In particular, we have been able to evaluate the impact of tuning curve inhomogeneity on neural computation. [sent-242, score-0.381]
</p><p>96 We ﬁnd that tuning curve inhomogeneity is not necessarily noise because it does not necessarily harm signal representation. [sent-244, score-0.437]
</p><p>97 Therefore, we propose that tuning curves are inhomogeneous simply because they can be. [sent-245, score-0.387]
</p><p>98 3 Membrane potential noise can be included in our network model by adding a Wiener process noise term to our membrane potential equation (Eqn. [sent-249, score-0.401]
</p><p>99 (2006) Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition. [sent-330, score-0.397]
</p><p>100 (2012) Predictive coding of dynamical variables in balanced spiking networks. [sent-345, score-0.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ring', 0.605), ('spiking', 0.242), ('balanced', 0.216), ('neuron', 0.21), ('tuning', 0.2), ('network', 0.178), ('connectivity', 0.175), ('neurons', 0.167), ('panel', 0.144), ('spike', 0.14), ('rates', 0.137), ('curves', 0.132), ('tightly', 0.128), ('membrane', 0.124), ('silent', 0.12), ('excitation', 0.102), ('signal', 0.094), ('rate', 0.084), ('optimisation', 0.08), ('inhibition', 0.077), ('inhomogeneity', 0.075), ('hz', 0.073), ('activity', 0.073), ('machens', 0.069), ('curve', 0.068), ('firing', 0.068), ('slopes', 0.068), ('eye', 0.056), ('inhomogeneous', 0.055), ('dynamics', 0.054), ('regularly', 0.052), ('oculomotor', 0.051), ('calculating', 0.051), ('networks', 0.046), ('relationship', 0.046), ('minimise', 0.045), ('spikes', 0.045), ('balance', 0.044), ('neuroscience', 0.043), ('quadratic', 0.043), ('inhibitory', 0.043), ('prediction', 0.042), ('deneve', 0.041), ('excitatory', 0.04), ('leak', 0.039), ('equation', 0.039), ('calculate', 0.039), ('plos', 0.038), ('ti', 0.038), ('neural', 0.038), ('bottom', 0.038), ('calculations', 0.037), ('monotonic', 0.037), ('timescale', 0.037), ('kk', 0.037), ('programming', 0.037), ('ik', 0.037), ('ft', 0.036), ('sompolinsky', 0.035), ('trains', 0.035), ('fij', 0.034), ('boerlin', 0.034), ('champalimaud', 0.034), ('ckm', 0.034), ('linearising', 0.034), ('recurrently', 0.034), ('uctuate', 0.034), ('vreeswijk', 0.034), ('zotterman', 0.034), ('interpret', 0.034), ('receptive', 0.033), ('middle', 0.033), ('fi', 0.033), ('temporal', 0.033), ('recurrent', 0.032), ('active', 0.032), ('spaced', 0.031), ('uctuations', 0.031), ('fx', 0.03), ('normale', 0.03), ('rieure', 0.03), ('fft', 0.03), ('metabolic', 0.03), ('optimises', 0.03), ('optimising', 0.03), ('cortical', 0.03), ('potential', 0.03), ('representation', 0.028), ('vi', 0.028), ('coding', 0.028), ('ri', 0.028), ('thin', 0.028), ('column', 0.028), ('input', 0.027), ('population', 0.026), ('algorithmically', 0.026), ('generalise', 0.026), ('adrian', 0.026), ('dt', 0.026), ('system', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="121-tfidf-1" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>2 0.31905791 <a title="121-tfidf-2" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>3 0.23052126 <a title="121-tfidf-3" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>4 0.17724694 <a title="121-tfidf-4" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>5 0.17611752 <a title="121-tfidf-5" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>6 0.17375542 <a title="121-tfidf-6" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>7 0.16921237 <a title="121-tfidf-7" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>8 0.16746035 <a title="121-tfidf-8" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>9 0.1619112 <a title="121-tfidf-9" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>10 0.13822277 <a title="121-tfidf-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.12294561 <a title="121-tfidf-11" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>12 0.11696489 <a title="121-tfidf-12" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>13 0.10979135 <a title="121-tfidf-13" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>14 0.10913055 <a title="121-tfidf-14" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>15 0.1009387 <a title="121-tfidf-15" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>16 0.097871095 <a title="121-tfidf-16" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>17 0.096595049 <a title="121-tfidf-17" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>18 0.095120125 <a title="121-tfidf-18" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>19 0.093580455 <a title="121-tfidf-19" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>20 0.087653272 <a title="121-tfidf-20" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, 0.095), (2, -0.133), (3, -0.114), (4, -0.381), (5, -0.072), (6, -0.065), (7, -0.095), (8, 0.033), (9, 0.065), (10, 0.044), (11, -0.022), (12, 0.043), (13, 0.007), (14, 0.035), (15, -0.034), (16, 0.014), (17, 0.042), (18, 0.02), (19, 0.032), (20, -0.001), (21, 0.01), (22, 0.05), (23, -0.032), (24, 0.025), (25, -0.043), (26, -0.022), (27, 0.022), (28, 0.068), (29, -0.028), (30, -0.08), (31, -0.068), (32, -0.015), (33, -0.043), (34, -0.021), (35, 0.018), (36, 0.012), (37, -0.016), (38, 0.038), (39, -0.032), (40, -0.025), (41, -0.016), (42, 0.005), (43, 0.047), (44, -0.009), (45, -0.008), (46, -0.017), (47, -0.01), (48, 0.009), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97174656 <a title="121-lsi-1" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>2 0.87641221 <a title="121-lsi-2" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>3 0.87350261 <a title="121-lsi-3" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>4 0.75094497 <a title="121-lsi-4" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive ﬁelds in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identiﬁcation of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identiﬁed. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identiﬁcation algorithms. 1</p><p>5 0.74302942 <a title="121-lsi-5" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>6 0.70428741 <a title="121-lsi-6" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>7 0.66152936 <a title="121-lsi-7" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>8 0.65309632 <a title="121-lsi-8" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>9 0.64949089 <a title="121-lsi-9" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>10 0.63536155 <a title="121-lsi-10" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>11 0.63294905 <a title="121-lsi-11" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>12 0.62194574 <a title="121-lsi-12" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>13 0.62010109 <a title="121-lsi-13" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>14 0.56708193 <a title="121-lsi-14" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>15 0.56363755 <a title="121-lsi-15" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>16 0.56164676 <a title="121-lsi-16" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>17 0.56103081 <a title="121-lsi-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.5574919 <a title="121-lsi-18" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>19 0.49804765 <a title="121-lsi-19" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>20 0.46238771 <a title="121-lsi-20" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.153), (16, 0.046), (33, 0.115), (34, 0.09), (36, 0.01), (41, 0.022), (49, 0.159), (56, 0.098), (70, 0.097), (85, 0.036), (89, 0.026), (93, 0.05), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88331407 <a title="121-lda-1" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>2 0.81471157 <a title="121-lda-2" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>3 0.80791157 <a title="121-lda-3" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>4 0.80661899 <a title="121-lda-4" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>Author: Tuan A. Nguyen, Subbarao Kambhampati, Minh Do</p><p>Abstract: Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we ﬁrst introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of ﬁnding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner. 1</p><p>5 0.80375236 <a title="121-lda-5" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>Author: James Martens, Arkadev Chattopadhya, Toni Pitassi, Richard Zemel</p><p>Abstract: This paper examines the question: What kinds of distributions can be efﬁciently represented by Restricted Boltzmann Machines (RBMs)? We characterize the RBM’s unnormalized log-likelihood function as a type of neural network, and through a series of simulation results relate these networks to ones whose representational properties are better understood. We show the surprising result that RBMs can efﬁciently capture any distribution whose density depends on the number of 1’s in their input. We also provide the ﬁrst known example of a particular type of distribution that provably cannot be efﬁciently represented by an RBM, assuming a realistic exponential upper bound on the weights. By formally demonstrating that a relatively simple distribution cannot be represented efﬁciently by an RBM our results provide a new rigorous justiﬁcation for the use of potentially more expressive generative models, such as deeper ones. 1</p><p>6 0.79839081 <a title="121-lda-6" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>7 0.79327166 <a title="121-lda-7" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>8 0.79296941 <a title="121-lda-8" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>9 0.79057837 <a title="121-lda-9" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>10 0.77490801 <a title="121-lda-10" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>11 0.76765794 <a title="121-lda-11" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>12 0.76202333 <a title="121-lda-12" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>13 0.75438511 <a title="121-lda-13" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>14 0.7474854 <a title="121-lda-14" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>15 0.74363512 <a title="121-lda-15" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>16 0.73679608 <a title="121-lda-16" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>17 0.73069465 <a title="121-lda-17" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>18 0.72639418 <a title="121-lda-18" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>19 0.72536337 <a title="121-lda-19" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>20 0.72292012 <a title="121-lda-20" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
