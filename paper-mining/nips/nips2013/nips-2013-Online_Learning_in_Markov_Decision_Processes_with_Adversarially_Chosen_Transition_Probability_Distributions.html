<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-227" href="#">nips2013-227</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</h1>
<br/><p>Source: <a title="nips-2013-227-pdf" href="http://papers.nips.cc/paper/4975-online-learning-in-markov-decision-processes-with-adversarially-chosen-transition-probability-distributions.pdf">pdf</a></p><p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>Reference: <a title="nips-2013-227-reference" href="../nips2013_reference/nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. [sent-13, score-0.482]
</p><p>2 We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. [sent-14, score-0.439]
</p><p>3 The regret is independent of the size of the state and action spaces. [sent-15, score-0.452]
</p><p>4 We also consider the episodic adversarial online shortest path problem. [sent-17, score-0.862]
</p><p>5 Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. [sent-18, score-0.446]
</p><p>6 The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. [sent-19, score-0.421]
</p><p>7 The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. [sent-21, score-0.543]
</p><p>8 We consider the problem of online learning Markov Decision Processes (MDPs) when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. [sent-32, score-0.627]
</p><p>9 The (oblivious) adversary chooses a sequence of transition kernels mt and loss functions `t . [sent-34, score-0.632]
</p><p>10 At time t: (a) The learner observes the state xt in state space X and chooses an action at in the action space A. [sent-36, score-0.653]
</p><p>11 (c) The learner observes the transition kernel mt and the loss function `t , and suffers the loss `t (xt , at ). [sent-38, score-0.655]
</p><p>12 The above game is played for a total of T rounds and the total PT loss suffered by the learner is t=1 `t (xt , at ). [sent-40, score-0.394]
</p><p>13 The difﬁculty with MDP problems is that, unlike full information online learning problems, the choice of a policy at each round changes the future states and losses. [sent-42, score-0.612]
</p><p>14 To evaluate the learner’s performance, we imagine a hypothetical game where at each round the action played is chosen according to a ﬁxed policy ⇡, and the transition kernels mt and loss functions `t are the same as those chosen by the oblivious adversary. [sent-45, score-1.092]
</p><p>15 Then the loss of the policy ⇡ is t=1 `t (x⇡ , a⇡ ). [sent-47, score-0.407]
</p><p>16 The regret of a learner A with respect to PT PT ⇡ ⇡ a policy ⇡ 2 ⇧ is deﬁned as the random variable RT (A, ⇡) = t=1 `t (xt , at ) t=1 `t (xt , at ). [sent-49, score-0.646]
</p><p>17 The goal in adversarial online learning is to design learning algorithms for which the regret with respect to any policy grows sublinearly with T , the total number of rounds played. [sent-50, score-0.955]
</p><p>18 We also study a special case of this problem: the episodic online adversarial shortest path problem. [sent-52, score-0.862]
</p><p>19 Here, in each episode the adversary chooses a layered directed acyclic graph with a unique start and ﬁnish node. [sent-53, score-0.464]
</p><p>20 At the end of the round the graph and the loss function are revealed to the learner. [sent-59, score-0.329]
</p><p>21 The goal, as in the case of the online MDP problem, is to minimize regret with respect to a class of policies for choosing the path. [sent-60, score-0.597]
</p><p>22 Note that the online shortest path problem is a special case of the online MDP problem; the states are the nodes in the graph and the transition dynamics is speciﬁed by the edges. [sent-61, score-1.049]
</p><p>23 Abbasi-Yadkori and Szepesv´ ri [5] and in a Abbasi-Yadkori [6] derive algorithms with O( T ) regret for linearly parameterized MDP problems, while Ortner and Ryabko [7] derive O(T (2d+1)/(2d+2) ) regret bounds under a Lipschitz assumption, where d is the dimensionality of the state space. [sent-66, score-0.587]
</p><p>24 [8] consider the problem of online learning MDPs with ﬁxed and known dynamics, but adversarially changing loss functions. [sent-69, score-0.497]
</p><p>25 They show that when the transition kernel satisﬁes a p mixing condition (see Section 3), there is an algorithm with regret bound O( T ). [sent-70, score-0.424]
</p><p>26 2  However, their regret bound scales with the amount of variation in the transition kernels and in the worst case may grow linearly with time. [sent-72, score-0.436]
</p><p>27 [11] give a no-regret algorithm for the episodic shortest path problem with adversarial losses but stochastic transition dynamics. [sent-74, score-0.901]
</p><p>28 2  Our Contributions  First, we study a general MDP problem with large (possibly continuous) state and action spaces and adversarially changing dynamics and loss functions. [sent-76, score-0.567]
</p><p>29 We present an algorithm that guarantees p O( T ) regret with respect to a suitably small (totally bounded) class of policies ⇧ for this online MDP problem. [sent-77, score-0.626]
</p><p>30 The regret grows with the metric entropy of ⇧, so that if the comparison class is the set of all policies (that is, the algorithm must compete with the optimal ﬁxed policy), it scales polynomially with the size of the state and action spaces. [sent-78, score-0.623]
</p><p>31 Next, we present efﬁcient no-regret algorithms for the episodic online shortest path problem for two cases: when the graphs and loss functions (edge weights) are chosen adversarially and the set of graphs is small; and when the graphs are chosen adversarially, but the loss is stochastic. [sent-83, score-1.454]
</p><p>32 Finally, we show that for the general adversarial online shortest path problem, designing an efﬁcient no-regret algorithm is at least as hard as learning parity with noise. [sent-84, score-1.114]
</p><p>33 Since the online shortest path problem is a special case of online MDP problem, the hardness result is also applicable there. [sent-85, score-0.864]
</p><p>34 Organization: In Section 3 we introduce an algorithm for MDP problems with adversarially chosen transition kernels and loss functions. [sent-87, score-0.51]
</p><p>35 Section 4 discusses how this algorithm can also be applied to the online episodic shortest path problem with adversarially varying graphs and loss functions and also considers the case of stochastic loss functions. [sent-88, score-1.217]
</p><p>36 2, we show the reduction from the adversarial online epsiodic shortest path problem to learning parity with noise. [sent-90, score-1.053]
</p><p>37 We use ⇡(a|x) to denote the probability of choosing an action a in state x under policy ⇡. [sent-95, score-0.459]
</p><p>38 A random action under policy ⇡ is denoted by ⇡(x). [sent-96, score-0.408]
</p><p>39 For ﬁnite X , let P (⇡, m) be the transition probability matrix of policy ⇡ under transition kernel m. [sent-99, score-0.529]
</p><p>40 The adversary can change the dynamics and the loss functions, but is restricted to choose dynamics that satisfy a mixing condition. [sent-106, score-0.453]
</p><p>41 Assumption A1 Uniform Mixing There exists a constant ⌧ > 0 such that for all distributions d and d0 over the state space, any deterministic policy ⇡, and any transition kernel m 2 M , kdP (⇡, m) d0 P (⇡, m)k1  e 1/⌧ kd d0 k1 . [sent-107, score-0.453]
</p><p>42 1 There was an error in the proof of a claimed hardness result for the online adversarial MDP problem [8]; this claim has since been retracted [12, 13]. [sent-108, score-0.428]
</p><p>43 The algorithm, called OMDP and shown in Figure 1, maintains a distribution over the policy class, but changes its policy with a small probability. [sent-128, score-0.578]
</p><p>44 Let the loss functions selected by the adversary be bounded in [0, 1], and the transition kernels selected by the adversary satisfy Assumption A1. [sent-132, score-0.7]
</p><p>45 This result, however, is not sufﬁcient to show that the average regret with respect to the optimal stationary policy converges to zero. [sent-141, score-0.543]
</p><p>46 To be more precise, let ↵t be the probability that algorithm A changes its policy at round t. [sent-154, score-0.454]
</p><p>47 In our MDP problem, the EWA algorithm chooses a policy ⇡ 2 ⇧ accord⇣ ⌘ Pt 1 ⇡ ing to distribution qt (⇡) / exp > 0. [sent-164, score-0.386]
</p><p>48 The policies that this s=1 E [`s (xs , ⇡)] for some EWA algorithm generates most likely are different in consecutive rounds and thus, the EWA algorithm might change its policy frequently. [sent-165, score-0.52]
</p><p>49 [17]), rarely changes its policy (see Lemma 8 in Appendix A). [sent-167, score-0.343]
</p><p>50 4  Adversarial Online Shortest Path Problem  We consider the following adversarial online shortest path problem with changing graphs. [sent-171, score-0.829]
</p><p>51 At each round t the adversary presents a directed acyclic graph gt on n nodes to the decision maker, with L layers indexed by {1, . [sent-173, score-0.647]
</p><p>52 2 The decision-maker must choose a path pt from the start to the ﬁnish node. [sent-178, score-0.462]
</p><p>53 The loss `t (gt , pt ) of the decision-maker is the weight along the path that the decision-maker took on that round. [sent-180, score-0.556]
</p><p>54 The path is interpreted as follows : if at a node v, the edge (v, ⇡(v)) exists then the next node is ⇡(v). [sent-189, score-0.381]
</p><p>55 We compete against the class of such policies for choosing the shortest path. [sent-191, score-0.378]
</p><p>56 The regret of a decision-maker A with respect to a policy ⇡ 2 ⇧ is deﬁned as: PT PT RT (A, ⇡) = t=1 `t (gt , pt ) t=1 `t (gt , ⇡(gt )), where ⇡(gt ) is the path obtained by following the policy ⇡ starting at the source node. [sent-193, score-1.213]
</p><p>57 Note that it is possible that there exists no policy that would result in an actual path that leads to the sink for some graph. [sent-194, score-0.468]
</p><p>58 In this case we say that the loss of the policy is inﬁnite. [sent-195, score-0.407]
</p><p>59 Thus, there may be adversarially chosen sequences of graphs for which the regret of a decision-maker is not well-deﬁned. [sent-196, score-0.546]
</p><p>60 This can be easily corrected by the adversary ensuring that the graph always has some ﬁxed set of edges which result in a (possibly high loss) s ! [sent-197, score-0.33]
</p><p>61 In fact, we show that the adversary can choose a sequence of graphs and loss functions that make this problem at least as hard as learning noisy parities. [sent-199, score-0.462]
</p><p>62 Interestingly, for the hardness result to hold, it is essential that the adversary have the ability to control both the sequence of graphs and losses. [sent-202, score-0.358]
</p><p>63 3 These algorithms are originally proposed for continuing problems, but we can use them in shortest path problems with small modiﬁcations. [sent-210, score-0.429]
</p><p>64 Thus, if the adversary is forced to choose graphs from some small set G, then we have an efﬁcient algorithm for solving the problem. [sent-216, score-0.359]
</p><p>65 Assume that the learner suffers the loss of c(p) for choosing path p, where E [c(p)] = h`, pi and the loss vector ` 2 Rn(n 1)/2 is ﬁxed. [sent-225, score-0.605]
</p><p>66 [24] applied to the shortest path problem with a changing action set Pt and the loss function p `. [sent-228, score-0.727]
</p><p>67 Then the regret with respect to the best path in each round is Cn3 T for a problem-independent constant C. [sent-229, score-0.583]
</p><p>68 The C ONFIDENCE BALL1 algorithm constructs a high probability o n 1/2 b norm-1 ball conﬁdence set, Ct = ` : Vt (` `t )  t for an appropriate t , and chooses 1  an action pt according to pt = argmin`2Ct ,p2Pt h`, pi. [sent-231, score-0.617]
</p><p>69 [24] prove that the regret of the p C ONFIDENCE BALL1 algorithm is bounded by O(m3/2 T ), where m is the dimensionality of the action set (in our case m = n(n 1)/2). [sent-233, score-0.43]
</p><p>70 Note that the regret in Theorem 4 is with respect to the best path in each round, which is a stronger result than competing with a ﬁxed policy. [sent-235, score-0.461]
</p><p>71 2  Hardness Result  In this section, we show that the setting when both the graphs and the losses are chosen by an adversary, the problem is at least as hard as the noisy parity problem. [sent-237, score-0.46]
</p><p>72 In the online setting, the learning algorithm is given xt 2 {0, 1}n , the learning algorithm then picks y t 2 {0, 1}, and then the true label y t is revealed. [sent-241, score-0.341]
</p><p>73 The goal is to t=1 I(ˆ 6= y ) design a learning algorithm that runs in time polynomial in n, T and suffers regret O(poly(n)T 1 ) for some constant > 0. [sent-244, score-0.342]
</p><p>74 It follows from prior work that online agnostic learning of parities is at least as hard as the ofﬂine version (see Littlestone [26], Kanade and Steinke [27]). [sent-245, score-0.364]
</p><p>75 As mentioned previously, the agnostic parity learning problem is notoriously difﬁcult. [sent-246, score-0.379]
</p><p>76 Suppose there is a no-regret algorithm for the online adversarial shortest path problem that runs in time poly(n, T ) and achieves regret O(poly(n)T 1 ) for any constant > 0. [sent-249, score-1.093]
</p><p>77 Then there is a polynomial-time algorithm for online agnostic parity learning that achieves regret O(poly(n)T 1 ). [sent-250, score-0.82]
</p><p>78 Suppose an algorithm with the stated regret bound for the online shortest path problem exists, call it U . [sent-287, score-0.913]
</p><p>79 We will use this algorithm to solve the online parity learning problem. [sent-288, score-0.473]
</p><p>80 ˆ Thus, in effect we are using algorithm U as a meta-algorithm for the online agnostic parity learning problem. [sent-296, score-0.552]
</p><p>81 First, it is easy to check that the loss suffered by the meta-algorithm on the parity problem is exactly the same as the loss of U on the online shortest path problem. [sent-297, score-1.177]
</p><p>82 Next, we claim that for any S ✓ [n], there is a policy ⇡S that achieves the same loss (on the online shortest path problem) as the parity PARS does (on the parity learning problem). [sent-299, score-1.537]
</p><p>83 The policy is as follows: (i) From node ia , if i 2 S and (ia , (i + 1)b ) 2 E(g t ), go to (i + 1)b , otherwise go to (i + 1)a . [sent-300, score-0.411]
</p><p>84 We can think of the path pt as being in type a nodes or type b nodes. [sent-304, score-0.395]
</p><p>85 For each i 2 S, such that xt = 1, the path pt switches types. [sent-305, score-0.491]
</p><p>86 Thus, the loss suffered by the policy ⇡S is 1 if PARS (xt ) 6= y t and 0 otherwise. [sent-314, score-0.447]
</p><p>87 This is exactly the loss of the parity function PARS on the agnostic parity learning problem. [sent-315, score-0.725]
</p><p>88 Thus, if the algorithm U has regret O(poly(n), T 1 ), then the meta-algorithm for the online agnostic parity learning problem also has regret O(poly(n), T 1 ). [sent-316, score-1.088]
</p><p>89 We observe that the online shortest path problem is a special case of online MDP learning. [sent-318, score-0.803]
</p><p>90 Thus, the above reduction also shows that, short of a major breakthrough, it is unlikely that there exists a computationally efﬁcient algorithm for the fully adversarial online MDP problem. [sent-319, score-0.396]
</p><p>91 3  Small Number of Graphs  p In this section, we design an efﬁcient algorithm and prove a O(|G| T ) regret bound, where G is the set of graphs played by the adversary up to round T . [sent-321, score-0.755]
</p><p>92 This regret bound holds even if the graphs are revealed at the end of the rounds. [sent-324, score-0.365]
</p><p>93 Notice that p the graphs are shown at the if beginning of the rounds, obtaining regret bounds that scale like O(|G| T ) is trivial; the learner only needs to run |G| copies of the MDP-E algorithm of Even-Dar et al. [sent-325, score-0.497]
</p><p>94 Let n⇡ denote the node at layer l of round t if we run policy ⇡. [sent-327, score-0.5]
</p><p>95 Let ct (n0 , a) be the loss incurred t,l for taking action a in node n0 at round t. [sent-328, score-0.52]
</p><p>96 Let g(n0 , a) be the next node under graph g if we take action a in node n0 . [sent-337, score-0.33]
</p><p>97 As the generated policies are always the same, the regret bound in the next theorem, that is proven for the MDP-E algorithm, also applies to the new algorithm. [sent-356, score-0.41]
</p><p>98 The adversarial stochastic shortest path problem a o a with unknown transition probabilities. [sent-405, score-0.736]
</p><p>99 The online loop-free stochastic shortest path proba o a lem. [sent-431, score-0.616]
</p><p>100 The adversarial stochastic shortest path problem a o a with unknown transition probabilities. [sent-440, score-0.736]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.275), ('regret', 0.268), ('parity', 0.257), ('shortest', 0.236), ('pt', 0.202), ('adversary', 0.2), ('omdp', 0.197), ('path', 0.193), ('online', 0.187), ('adversarial', 0.18), ('mdp', 0.178), ('pars', 0.158), ('adversarially', 0.145), ('policies', 0.142), ('action', 0.133), ('loss', 0.132), ('transition', 0.127), ('round', 0.122), ('neu', 0.12), ('ewa', 0.118), ('csaba', 0.112), ('gt', 0.111), ('learner', 0.103), ('parities', 0.098), ('graphs', 0.097), ('xt', 0.096), ('nish', 0.094), ('szepesv', 0.083), ('mt', 0.081), ('yasin', 0.079), ('agnostic', 0.079), ('graph', 0.075), ('ia', 0.075), ('ib', 0.075), ('poly', 0.074), ('decision', 0.072), ('nl', 0.072), ('ct', 0.072), ('losses', 0.07), ('gergely', 0.069), ('episodic', 0.066), ('edge', 0.066), ('hardness', 0.061), ('node', 0.061), ('rt', 0.061), ('andr', 0.06), ('onfidence', 0.059), ('rgy', 0.057), ('edges', 0.055), ('state', 0.051), ('chooses', 0.051), ('mdps', 0.049), ('kanade', 0.048), ('gy', 0.048), ('jaksch', 0.045), ('ortner', 0.045), ('dani', 0.045), ('cryptographic', 0.045), ('suffers', 0.045), ('rounds', 0.045), ('dynamics', 0.044), ('notoriously', 0.043), ('eyal', 0.043), ('colt', 0.042), ('layer', 0.042), ('kernels', 0.041), ('rarely', 0.04), ('suffered', 0.04), ('played', 0.039), ('bartlett', 0.039), ('acyclic', 0.039), ('yishay', 0.038), ('episode', 0.037), ('chosen', 0.036), ('game', 0.035), ('kalai', 0.035), ('oblivious', 0.035), ('burnetas', 0.035), ('observes', 0.035), ('start', 0.034), ('bandit', 0.033), ('choose', 0.033), ('changing', 0.033), ('markov', 0.033), ('xa', 0.033), ('xl', 0.033), ('varun', 0.032), ('designing', 0.032), ('bt', 0.031), ('qt', 0.031), ('algorithm', 0.029), ('weight', 0.029), ('nick', 0.029), ('littlestone', 0.029), ('queensland', 0.029), ('shie', 0.029), ('spaces', 0.029), ('expert', 0.028), ('directed', 0.028), ('changes', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="227-tfidf-1" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>2 0.39442518 <a title="227-tfidf-2" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>3 0.30755737 <a title="227-tfidf-3" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>4 0.29172292 <a title="227-tfidf-4" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>Author: Nicolò Cesa-Bianchi, Ofer Dekel, Ohad Shamir</p><p>Abstract: We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player’s performance using a new notion of regret, also known as policy regret, which better captures the adversary’s adaptiveness to the player’s behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switch� ing costs, the attainable rate with bandit feedback is Θ(T 2/3 ). Interestingly, this √ rate is signiﬁcantly worse than the Θ( T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also � show that a bounded memory adversary can force Θ(T 2/3 ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. 1</p><p>5 0.29019037 <a title="227-tfidf-5" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>Author: Navid Zolghadr, Gabor Bartok, Russell Greiner, András György, Csaba Szepesvari</p><p>Abstract: This paper introduces the online probing problem: In each round, the learner is able to purchase the values of a subset of feature values. After the learner uses this information to come up with a prediction for the given round, he then has the option of paying to see the loss function that he is evaluated against. Either way, the learner pays for both the errors of his predictions and also whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features. We consider two variations of this problem, depending on whether the learner can observe the label for free or not. We provide algorithms and upper and lower bounds on the regret for both variants. We show that a positive cost for observing the label signiﬁcantly increases the regret of the problem. 1</p><p>6 0.25317204 <a title="227-tfidf-6" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>7 0.24901403 <a title="227-tfidf-7" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>8 0.2398711 <a title="227-tfidf-8" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>9 0.23965041 <a title="227-tfidf-9" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>10 0.2387539 <a title="227-tfidf-10" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>11 0.21067835 <a title="227-tfidf-11" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>12 0.19760433 <a title="227-tfidf-12" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>13 0.1834221 <a title="227-tfidf-13" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>14 0.18106702 <a title="227-tfidf-14" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>15 0.17449042 <a title="227-tfidf-15" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>16 0.17154945 <a title="227-tfidf-16" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>17 0.16672313 <a title="227-tfidf-17" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>18 0.1652903 <a title="227-tfidf-18" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>19 0.16332451 <a title="227-tfidf-19" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>20 0.16212027 <a title="227-tfidf-20" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.31), (1, -0.437), (2, 0.098), (3, -0.096), (4, -0.002), (5, -0.071), (6, 0.001), (7, -0.001), (8, 0.001), (9, -0.023), (10, 0.012), (11, -0.19), (12, 0.161), (13, 0.019), (14, 0.187), (15, -0.13), (16, 0.076), (17, -0.119), (18, -0.07), (19, -0.075), (20, -0.047), (21, -0.044), (22, -0.031), (23, 0.011), (24, 0.035), (25, 0.015), (26, -0.044), (27, 0.013), (28, 0.016), (29, 0.051), (30, -0.086), (31, 0.088), (32, -0.033), (33, 0.047), (34, -0.071), (35, -0.009), (36, -0.012), (37, -0.027), (38, -0.072), (39, -0.036), (40, -0.005), (41, 0.029), (42, 0.011), (43, -0.013), (44, 0.048), (45, -0.017), (46, 0.007), (47, 0.01), (48, -0.042), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96983272 <a title="227-lsi-1" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>2 0.83970171 <a title="227-lsi-2" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>3 0.80157423 <a title="227-lsi-3" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>Author: Shiau Hong Lim, Huan Xu, Shie Mannor</p><p>Abstract: An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case. 1</p><p>4 0.7701031 <a title="227-lsi-4" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>Author: Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour</p><p>Abstract: We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir [14]. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph (which must be accessible before selecting an action). In the undirected case, we show that the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efﬁcient manner. 1</p><p>5 0.74121797 <a title="227-lsi-5" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>Author: Asrar Ahmed, Pradeep Varakantham, Yossiri Adulyasak, Patrick Jaillet</p><p>Abstract: In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.</p><p>6 0.7334218 <a title="227-lsi-6" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>7 0.67367274 <a title="227-lsi-7" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>8 0.67188495 <a title="227-lsi-8" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>9 0.64347267 <a title="227-lsi-9" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>10 0.64267129 <a title="227-lsi-10" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>11 0.60482359 <a title="227-lsi-11" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>12 0.56728816 <a title="227-lsi-12" href="./nips-2013-Learning_Prices_for_Repeated_Auctions_with_Strategic_Buyers.html">159 nips-2013-Learning Prices for Repeated Auctions with Strategic Buyers</a></p>
<p>13 0.56498307 <a title="227-lsi-13" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>14 0.55587661 <a title="227-lsi-14" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>15 0.53636855 <a title="227-lsi-15" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>16 0.51767898 <a title="227-lsi-16" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>17 0.51736158 <a title="227-lsi-17" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>18 0.51306248 <a title="227-lsi-18" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>19 0.51241207 <a title="227-lsi-19" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>20 0.4937394 <a title="227-lsi-20" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.033), (2, 0.062), (16, 0.02), (33, 0.118), (34, 0.091), (41, 0.031), (49, 0.013), (56, 0.189), (63, 0.167), (70, 0.027), (85, 0.085), (89, 0.04), (93, 0.021), (95, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87942141 <a title="227-lda-1" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><p>2 0.82594514 <a title="227-lda-2" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>Author: Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour</p><p>Abstract: We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir [14]. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph (which must be accessible before selecting an action). In the undirected case, we show that the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efﬁcient manner. 1</p><p>3 0.8232277 <a title="227-lda-3" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>Author: Christina E. Lee, Asuman Ozdaglar, Devavrat Shah</p><p>Abstract: Computing the stationary distribution of a large ﬁnite or countably inﬁnite state space Markov Chain (MC) has become central in many problems such as statistical inference and network analysis. Standard methods involve large matrix multiplications as in power iteration, or simulations of long random walks, as in Markov Chain Monte Carlo (MCMC). Power iteration is costly, as it involves computation at every state. For MCMC, it is difﬁcult to determine whether the random walks are long enough to guarantee convergence. In this paper, we provide a novel algorithm that answers whether a chosen state in a MC has stationary probability larger than some ∆ ∈ (0, 1), and outputs an estimate of the stationary probability. Our algorithm is constant time, using information from a local neighborhood of the state on the graph induced by the MC, which has constant size relative to the state space. The multiplicative error of the estimate is upper bounded by a function of the mixing properties of the MC. Simulation results show MCs for which this method gives tight estimates. 1</p><p>4 0.81423324 <a title="227-lda-4" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><p>5 0.808281 <a title="227-lda-5" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>6 0.8076964 <a title="227-lda-6" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>7 0.80749303 <a title="227-lda-7" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>8 0.80291909 <a title="227-lda-8" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>9 0.79997557 <a title="227-lda-9" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>10 0.79976255 <a title="227-lda-10" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>11 0.799052 <a title="227-lda-11" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>12 0.79740536 <a title="227-lda-12" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>13 0.79686892 <a title="227-lda-13" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>14 0.79573148 <a title="227-lda-14" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>15 0.79485875 <a title="227-lda-15" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>16 0.79429656 <a title="227-lda-16" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>17 0.79101384 <a title="227-lda-17" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>18 0.79058093 <a title="227-lda-18" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>19 0.79028434 <a title="227-lda-19" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>20 0.78653949 <a title="227-lda-20" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
