<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-102" href="#">nips2013-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</h1>
<br/><p>Source: <a title="nips-2013-102-pdf" href="http://papers.nips.cc/paper/5011-efficient-algorithm-for-privately-releasing-smooth-queries.pdf">pdf</a></p><p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>Reference: <a title="nips-2013-102-reference" href="../nips2013_reference/nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . [sent-11, score-0.963]
</p><p>2 A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. [sent-12, score-0.433]
</p><p>3 We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). [sent-13, score-0.798]
</p><p>4 The mechanism ﬁrst outputs a summary of the database. [sent-14, score-0.493]
</p><p>5 To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. [sent-15, score-0.276]
</p><p>6 Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d  K ˜ O(n 2d+K ). [sent-16, score-0.772]
</p><p>7 Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. [sent-17, score-0.779]
</p><p>8 But when releasing statistics of sensitive data, one must tradeoff between the accuracy and the amount of privacy loss of the individuals in the database. [sent-20, score-0.313]
</p><p>9 In this paper we consider differential privacy [9], which has become a standard concept of privacy. [sent-21, score-0.329]
</p><p>10 Roughly speaking, a mechanism which releases information about the database is said to preserve differential privacy, if the change of a single database element does not affect the probability distribution of the output signiﬁcantly. [sent-22, score-0.853]
</p><p>11 It ensures that the risk of any individual to submit her information to the database is very small. [sent-24, score-0.216]
</p><p>12 An adversary can discover almost nothing new from the database that contains the individual’s information compared with that from the database without the individual’s information. [sent-25, score-0.308]
</p><p>13 Recently there have been extensive studies of machine learning, statistical estimation, and data mining under the differential privacy framework [29, 5, 18, 17, 6, 30, 20, 4]. [sent-26, score-0.349]
</p><p>14 Accurately answering statistical queries is an important problem in differential privacy. [sent-27, score-0.534]
</p><p>15 A simple and efﬁcient method is the Laplace mechanism [9], which adds Laplace noise to the true answers. [sent-28, score-0.348]
</p><p>16 Laplace mechanism is especially useful for query functions with low sensitivity, which is the maximal difference of the query values of two databases that are different in only one item. [sent-29, score-1.164]
</p><p>17 A typical 1  class of queries that has low sensitivity is linear queries, whose sensitivity is O(1/n), where n is the size of the database. [sent-30, score-0.355]
</p><p>18 If the number of queries is substantially larger than n2 , Laplace mechanism is not able to provide differentially private answers with nontrivial accuracy. [sent-33, score-0.911]
</p><p>19 Considering that potentially there are many users and each user may submit a set of queries, limiting the number of total queries to be smaller than n2 is too restricted in some situations. [sent-34, score-0.343]
</p><p>20 A remarkable result due to Blum, Ligett and Roth [2] shows that information theoretically it is possible for a mechanism to answer far more than n2 linear queries while preserving differential privacy and nontrivial accuracy simultaneously. [sent-35, score-1.141]
</p><p>21 All these mechanisms are very powerful in the sense that they can answer general and adversely chosen queries. [sent-37, score-0.26]
</p><p>22 On the other hand, even the fastest algorithms [16, 14] run in time linear in the size of the data universe to answer a query. [sent-38, score-0.366]
</p><p>23 Often the size of the data universe is much larger than that of the database, so these mechanisms are inefﬁcient. [sent-39, score-0.332]
</p><p>24 Recently, [25] shows that there is no polynomial time algorithm that can answer n2+o(1) general queries while preserving privacy and accuracy (assuming the existence of one-way function). [sent-40, score-0.747]
</p><p>25 Given the hardness result, recently there are growing interests in studying efﬁcient and differentially private mechanisms for restricted class of queries. [sent-41, score-0.44]
</p><p>26 From a practical point of view, if there exists a class of queries which is rich enough to contain most queries used in applications and allows one to develop fast mechanisms, then the hardness result is not a serious barrier for differential privacy. [sent-42, score-0.687]
</p><p>27 One class of queries that attracts a lot of attentions is the k-way conjunctions. [sent-43, score-0.343]
</p><p>28 A k-way conjunction query is speciﬁed by k features. [sent-46, score-0.368]
</p><p>29 The query asks what fraction of the individual records in the database has all these k features being 1. [sent-47, score-0.546]
</p><p>30 Another class of queries that yields efﬁcient mechanisms is sparse query. [sent-50, score-0.396]
</p><p>31 A query is m-sparse if it takes non-zero values on at most m elements in the data universe. [sent-51, score-0.368]
</p><p>32 When the data universe is [−1, 1]d , where d is a constant, [2] considers rectangle queries. [sent-53, score-0.306]
</p><p>33 A rectangle query is speciﬁed by an axis-aligned rectangle. [sent-54, score-0.425]
</p><p>34 The answer to the query is the fraction of the data points that lie in the rectangle. [sent-55, score-0.524]
</p><p>35 [2] shows that if [−1, 1]d is discretized to poly(n) bits of precision, then there are efﬁcient mechanisms for the class of rectangle queries. [sent-56, score-0.187]
</p><p>36 There are also works studying related range queries [19]. [sent-57, score-0.287]
</p><p>37 In this paper we study smooth queries deﬁned also on data universe [−1, 1]d for constant d. [sent-58, score-0.593]
</p><p>38 A smooth query is speciﬁed by a smooth function, which has bounded partial derivatives up to a certain order. [sent-59, score-0.629]
</p><p>39 The answer to the query is the average of the function values on data points in the database. [sent-60, score-0.505]
</p><p>40 Our main result is an -differentially private mechanism for the class of K-smooth queries, which are speciﬁed by functions with bounded partial derivatives up to order K. [sent-63, score-0.6]
</p><p>41 The mechanism has d 2d+K  K  ) (α, β)-accuracy, where α = O(n− 2d+K / ) for β ≥ e−O(n . [sent-64, score-0.321]
</p><p>42 The mechanism ﬁrst outputs a summary of the database. [sent-65, score-0.493]
</p><p>43 To obtain an answer of a smooth query, the user runs a public evaluation procedure which contains no information of the database. [sent-66, score-0.374]
</p><p>44 Outputting the summary has running time d+2+ 2d  d K ˜ O n1+ 2d+K , and the evaluation procedure for answering a query runs in time O(n 2d+K ). [sent-67, score-0.785]
</p><p>45 The mechanism has the advantage that both the accuracy and the running time for answering a query improve quickly as K/d increases (see also Table 1 in Section 3). [sent-68, score-0.938]
</p><p>46 Our algorithm is a L∞ -approximation based mechanism and is motivated by [24], which considers approximation of k-way conjunctions by low degree polynomials. [sent-69, score-0.392]
</p><p>47 The basic idea is to approximate the whole query class by linear combination of a small set of basis functions. [sent-70, score-0.455]
</p><p>48 The technical difﬁculties lie in that in order that the approximation induces an efﬁcient and differentially private mechanism, all the linear coefﬁcients of the basis functions must be small and efﬁciently computable. [sent-71, score-0.357]
</p><p>49 To guarantee these properties, we ﬁrst transform the query function. [sent-72, score-0.368]
</p><p>50 The smoothness of the functions also allows us to use an efﬁcient numerical method to compute the coefﬁcients to a precision so that the accuracy of the mechanism is not affected signiﬁcantly. [sent-74, score-0.46]
</p><p>51 2  Background  Let D be a database containing n data points in the data universe X . [sent-75, score-0.383]
</p><p>52 Typically, we assume that the data universe X = [−1, 1]d . [sent-77, score-0.229]
</p><p>53 A sanitizer S which is an algorithm that maps input database into some range R is said to preserve ( , δ)-differential privacy, if for all pairs of neighbor databases D, D and for any subset A ⊂ R, it holds that P(S(D) ∈ A) ≤ P(S(D ) ∈ A) · e + δ. [sent-82, score-0.448]
</p><p>54 Each linear query qf is speciﬁed by a function f which maps data 1 universe [−1, 1]d to R, and qf is deﬁned by qf (D) := |D| x∈D f (x). [sent-85, score-1.444]
</p><p>55 The accuracy of a mechanism with respect to Q is deﬁned as follows. [sent-87, score-0.358]
</p><p>56 A sanitizer S is said to have (α, β)accuracy for size n databases with respect to Q, if for every database D with |D| = n the following holds P(∃q ∈ Q, |S(D, q) − q(D)| ≥ α) ≤ β, where S(D, q) is the answer to q given by S. [sent-91, score-0.537]
</p><p>57 We will make use of Laplace mechanism [9] in our algorithm. [sent-92, score-0.321]
</p><p>58 We will design a differentially private mechanism which is accurate with respect to a query set Q possibly consisting of inﬁnite number of queries. [sent-95, score-0.949]
</p><p>59 Given a database D, the sanitizer outputs a summary which preserves differential privacy. [sent-96, score-0.608]
</p><p>60 For any qf ∈ Q, the user makes use of an evaluation procedure to measure f on the summary and obtain an approximate answer of qf (D). [sent-97, score-0.899]
</p><p>61 Although we may think of the evaluation procedure as part of the mechanism, it does not contain any information of the database and therefore is public. [sent-98, score-0.188]
</p><p>62 We will study the running time for the sanitizer outputting the summary. [sent-99, score-0.383]
</p><p>63 For the evaluation procedure, the running time per query is the focus. [sent-101, score-0.445]
</p><p>64 In this work we will frequently use trigonometric polynomials. [sent-104, score-0.216]
</p><p>65 For the univariate case, a function m p(θ) is called a trigonometric polynomial of degree m if p(θ) = a0 + l=1 (al cos lθ + bl sin lθ), where al , bl are constants. [sent-105, score-0.654]
</p><p>66 If p(θ) is an even function, we say that it is an even trigonometm ric polynomial, and p(θ) = a0 + l=1 al cos lθ. [sent-106, score-0.27]
</p><p>67 cos(ld θd ), then p is said to be an even trigonometric polynomial (with respect to each variable), and the degree of θi is the upper limit of li . [sent-116, score-0.358]
</p><p>68 3  Efﬁcient differentially private mechanism  Let us ﬁrst describe the set of queries considered in this work. [sent-117, score-0.847]
</p><p>69 Since each query qf is speciﬁed by a function f , a set of queries QF can be speciﬁed by a set of functions F . [sent-118, score-0.948]
</p><p>70 , kd ) is a d-tuple with nonnegative integers, then we deﬁne k k Dk := D1 1 · · · Dd d :=  3  ∂ kd ∂ k1 · · · kd . [sent-126, score-0.234]
</p><p>71 , md ), where m 1 Compute: Sum (D) = n x∈D cos (m1 θ1 (x)) . [sent-139, score-0.263]
</p><p>72 cos (md θd (x)); Sum (D) ← Sum (D) + Lap Let Su(D) = Sum (D)  m  ∞ ≤t−1  td n  ∞  ≤t−1  ;  be a td dimensional vector;  Return: Su(D). [sent-142, score-0.307]
</p><p>73 K Input: A query qf , where f : [−1, 1]d → R and f ∈ CB , d Summary Su(D) (a t -dimensional vector). [sent-144, score-0.642]
</p><p>74 , θd ) ∈ [−π, π]d ; Compute a trigonometric polynomial approximation pt (θ) of gf (θ), where the degree of each θi is t; // see Section 4 for details of computation. [sent-152, score-0.474]
</p><p>75 Algorithm 2: Answering a query  Let |k| := k1 + . [sent-157, score-0.368]
</p><p>76 |k|≤K x∈[−1,1]d K We will study the set CB which contains all smooth functions whose derivatives up to order K have K ∞-norm upper bounded by a constant B > 0. [sent-162, score-0.183]
</p><p>77 The set K K of queries speciﬁed by CB , denoted as QCB , is our focus. [sent-164, score-0.266]
</p><p>78 It says that if the query class is speciﬁed by smooth functions, then there is a very efﬁcient mechanism which preserves -differential privacy and good accuracy. [sent-167, score-1.086]
</p><p>79 The mechanism consists of two parts: One for outputting a summary of the database, the other for answering a query. [sent-168, score-0.83]
</p><p>80 The second part of the mechanism contains no private information of the database. [sent-170, score-0.468]
</p><p>81 Let the query set be QCB = {qf = n x∈D f (x) : f ∈ CB }, where K ∈ N and B > 0 are constants. [sent-173, score-0.368]
</p><p>82 Let the data universe be [−1, 1]d , where d ∈ N is a constant. [sent-174, score-0.229]
</p><p>83 Then the mechanism S given in Algorithm 1 and Algorithm 2 satisﬁes that for any > 0, the following hold:  1) The mechanism is -differentially private. [sent-175, score-0.642]
</p><p>84 1  d 2d+K  ) 2) For any β ≥ 10 · e− 5 (n the mechanism is (α, β)-accurate, where α = O and the hidden constant depends only on d, K and B. [sent-176, score-0.321]
</p><p>85 4) The running time for S to answer a query is O(n  d+2+ 2d K 2d+K  polylog(n)). [sent-179, score-0.548]
</p><p>86 , the query functions only have the ﬁrst order derivatives. [sent-185, score-0.408]
</p><p>87 2) The running time for outputting the summary does not change too much, because reading through the database requires Ω(n) time. [sent-191, score-0.556]
</p><p>88 3) The running time for answering a query reduces signiﬁcantly from roughly O(n3/2 ) to nearly O(n 0 ) as K getting large. [sent-192, score-0.603]
</p><p>89 In practice, the speed for answering a query may be more important than that for outputting the summary since the sanitizer only output the summary once. [sent-194, score-1.179]
</p><p>90 Thus having an nc -time (c 1) algorithm for query answering will be appealing. [sent-195, score-0.565]
</p><p>91 It also transforms the data universe from [−1, 1]d to [−π, π]d . [sent-204, score-0.229]
</p><p>92 To compute the summary, the mechanism just gives noisy answers to queries speciﬁed by even trigonometric monomials cos(m1 θ1 ) . [sent-206, score-0.865]
</p><p>93 For each 1 trigonometric monomial, the highest degree of any variable is t := maxd md = O(n 2d+K ). [sent-210, score-0.354]
</p><p>94 To answer a query speciﬁed by a smooth function f , the mechanism computes a trigonometric polynomial approximation of gf . [sent-212, score-1.322]
</p><p>95 The answer to the query qf is a linear combination of the summary by the coefﬁcients of the approximation trigonometric polynomial. [sent-213, score-1.158]
</p><p>96 If these conditions hold, then the mechanism just outputs noisy answers to the set of queries speciﬁed by the basis functions as the summary. [sent-216, score-0.733]
</p><p>97 When answering a query, the mechanism computes the coefﬁcients with which the linear combination of the basis functions approximate the query function. [sent-217, score-0.958]
</p><p>98 The answer to the query is simply the inner product of the coefﬁcients and the summary vector. [sent-218, score-0.646]
</p><p>99 The following theorem guarantees that by change of variables and using even trigonometric polynomials as the basis functions, the class of smooth functions has all the three properties described above. [sent-219, score-0.491]
</p><p>100 Then, there is an even trigonometric polynomial p whose degree of each variable is t(γ) = p(θ1 , . [sent-230, score-0.32]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('query', 0.368), ('mechanism', 0.321), ('qf', 0.274), ('queries', 0.266), ('privacy', 0.23), ('universe', 0.229), ('trigonometric', 0.216), ('cos', 0.201), ('outputting', 0.199), ('answering', 0.169), ('database', 0.154), ('private', 0.147), ('laplace', 0.143), ('summary', 0.141), ('sanitizer', 0.141), ('answer', 0.137), ('gf', 0.129), ('differentially', 0.113), ('peking', 0.113), ('mechanisms', 0.103), ('cb', 0.1), ('differential', 0.099), ('smooth', 0.098), ('moe', 0.092), ('lap', 0.075), ('kd', 0.071), ('databases', 0.067), ('eecs', 0.065), ('smoothness', 0.062), ('md', 0.062), ('su', 0.061), ('coef', 0.059), ('rectangle', 0.057), ('qcb', 0.056), ('cients', 0.056), ('polynomial', 0.053), ('poly', 0.053), ('polynomials', 0.053), ('td', 0.053), ('perception', 0.052), ('degree', 0.051), ('laboratory', 0.048), ('al', 0.047), ('releasing', 0.046), ('derivatives', 0.045), ('school', 0.044), ('bl', 0.043), ('running', 0.043), ('xd', 0.042), ('preserves', 0.042), ('functions', 0.04), ('user', 0.039), ('said', 0.038), ('submit', 0.038), ('ld', 0.038), ('basis', 0.038), ('answers', 0.037), ('accuracy', 0.037), ('public', 0.036), ('evaluation', 0.034), ('outputs', 0.031), ('sensitivity', 0.031), ('integers', 0.031), ('runs', 0.03), ('hardness', 0.029), ('nc', 0.028), ('adds', 0.027), ('class', 0.027), ('nontrivial', 0.027), ('dk', 0.026), ('pt', 0.025), ('releases', 0.025), ('attracts', 0.025), ('monomials', 0.025), ('attentions', 0.025), ('maxd', 0.025), ('monomial', 0.025), ('maps', 0.025), ('performances', 0.024), ('ideally', 0.024), ('preserving', 0.024), ('individual', 0.024), ('preserve', 0.023), ('arccos', 0.023), ('roughly', 0.023), ('combination', 0.022), ('privately', 0.022), ('ric', 0.022), ('studying', 0.021), ('nonnegative', 0.021), ('adversely', 0.02), ('polylog', 0.02), ('partial', 0.02), ('output', 0.02), ('extensive', 0.02), ('considers', 0.02), ('sup', 0.019), ('change', 0.019), ('lie', 0.019), ('attack', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="102-tfidf-1" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>2 0.29226398 <a title="102-tfidf-2" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>Author: John Duchi, Martin J. Wainwright, Michael Jordan</p><p>Abstract: We provide a detailed study of the estimation of probability distributions— discrete and continuous—in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental trade-offs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efﬁciency continuum. One of the consequences of our results is that Warner’s classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents. 1</p><p>3 0.27167112 <a title="102-tfidf-3" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>4 0.18189195 <a title="102-tfidf-4" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>Author: Maria-Florina Balcan, Vitaly Feldman</p><p>Abstract: We describe a framework for designing efﬁcient active learning algorithms that are tolerant to random classiﬁcation noise and differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of ﬁltered random examples. It builds on the powerful statistical query framework of Kearns [30]. We show that any efﬁcient active statistical learning algorithm can be automatically converted to an efﬁcient active learning algorithm which is tolerant to random classiﬁcation noise as well as other forms of “uncorrelated” noise. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efﬁciently actively learned in our framework. These results combined with our generic conversion lead to the ﬁrst computationally-efﬁcient algorithms for actively learning some of these concept classes in the presence of random classiﬁcation noise that provide exponential improvement in the dependence on the error over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efﬁcient active differentially-private algorithms. This leads to the ﬁrst differentially-private active learning algorithms with exponential label savings over the passive case. 1</p><p>5 0.17887317 <a title="102-tfidf-5" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>6 0.13863096 <a title="102-tfidf-6" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>7 0.091186091 <a title="102-tfidf-7" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>8 0.082166761 <a title="102-tfidf-8" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>9 0.075679407 <a title="102-tfidf-9" href="./nips-2013-Learning_to_Prune_in_Metric_and_Non-Metric_Spaces.html">169 nips-2013-Learning to Prune in Metric and Non-Metric Spaces</a></p>
<p>10 0.073655762 <a title="102-tfidf-10" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>11 0.068626344 <a title="102-tfidf-11" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>12 0.064507127 <a title="102-tfidf-12" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>13 0.062676832 <a title="102-tfidf-13" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>14 0.060969759 <a title="102-tfidf-14" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<p>15 0.054017548 <a title="102-tfidf-15" href="./nips-2013-The_Power_of_Asymmetry_in_Binary_Hashing.html">326 nips-2013-The Power of Asymmetry in Binary Hashing</a></p>
<p>16 0.049002841 <a title="102-tfidf-16" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>17 0.047638267 <a title="102-tfidf-17" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>18 0.047472805 <a title="102-tfidf-18" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>19 0.042841487 <a title="102-tfidf-19" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>20 0.038771663 <a title="102-tfidf-20" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.023), (2, 0.052), (3, -0.076), (4, 0.074), (5, 0.046), (6, -0.023), (7, -0.033), (8, 0.027), (9, 0.36), (10, 0.106), (11, -0.063), (12, -0.155), (13, 0.268), (14, -0.039), (15, 0.036), (16, -0.018), (17, 0.019), (18, 0.034), (19, -0.017), (20, 0.088), (21, -0.069), (22, 0.026), (23, 0.026), (24, 0.057), (25, -0.09), (26, -0.004), (27, 0.028), (28, 0.025), (29, 0.031), (30, 0.005), (31, -0.011), (32, -0.009), (33, 0.075), (34, -0.065), (35, 0.006), (36, -0.008), (37, 0.083), (38, -0.026), (39, -0.079), (40, 0.015), (41, 0.037), (42, -0.0), (43, 0.007), (44, 0.031), (45, 0.033), (46, -0.036), (47, 0.012), (48, -0.024), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96214122 <a title="102-lsi-1" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>2 0.75723249 <a title="102-lsi-2" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><p>3 0.73353326 <a title="102-lsi-3" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>Author: John Duchi, Martin J. Wainwright, Michael Jordan</p><p>Abstract: We provide a detailed study of the estimation of probability distributions— discrete and continuous—in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental trade-offs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efﬁciency continuum. One of the consequences of our results is that Warner’s classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents. 1</p><p>4 0.51192933 <a title="102-lsi-4" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>Author: Krzysztof M. Choromanski, Tony Jebara, Kui Tang</p><p>Abstract: The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of k-anonymity to the b-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results conﬁrm improved utility on benchmark and social data-sets.</p><p>5 0.49984139 <a title="102-lsi-5" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>6 0.48536259 <a title="102-lsi-6" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>7 0.4537372 <a title="102-lsi-7" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>8 0.36574858 <a title="102-lsi-8" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>9 0.35384873 <a title="102-lsi-9" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<p>10 0.33666807 <a title="102-lsi-10" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>11 0.31758624 <a title="102-lsi-11" href="./nips-2013-The_Power_of_Asymmetry_in_Binary_Hashing.html">326 nips-2013-The Power of Asymmetry in Binary Hashing</a></p>
<p>12 0.31510553 <a title="102-lsi-12" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>13 0.29739138 <a title="102-lsi-13" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>14 0.28549665 <a title="102-lsi-14" href="./nips-2013-Learning_to_Prune_in_Metric_and_Non-Metric_Spaces.html">169 nips-2013-Learning to Prune in Metric and Non-Metric Spaces</a></p>
<p>15 0.28212613 <a title="102-lsi-15" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>16 0.25849569 <a title="102-lsi-16" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>17 0.25817308 <a title="102-lsi-17" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>18 0.2366606 <a title="102-lsi-18" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>19 0.22681102 <a title="102-lsi-19" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>20 0.21844883 <a title="102-lsi-20" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.012), (16, 0.022), (33, 0.11), (34, 0.116), (41, 0.038), (49, 0.047), (54, 0.215), (56, 0.104), (70, 0.03), (76, 0.031), (85, 0.039), (89, 0.022), (93, 0.03), (95, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85385269 <a title="102-lda-1" href="./nips-2013-How_to_Hedge_an_Option_Against_an_Adversary%3A_Black-Scholes_Pricing_is_Minimax_Optimal.html">139 nips-2013-How to Hedge an Option Against an Adversary: Black-Scholes Pricing is Minimax Optimal</a></p>
<p>Author: Jacob Abernethy, Peter Bartlett, Rafael Frongillo, Andre Wibisono</p><p>Abstract: We consider a popular problem in ﬁnance, option pricing, through the lens of an online learning game between Nature and an Investor. In the Black-Scholes option pricing model from 1973, the Investor can continuously hedge the risk of an option by trading the underlying asset, assuming that the asset’s price ﬂuctuates according to Geometric Brownian Motion (GBM). We consider a worst-case model, in which Nature chooses a sequence of price ﬂuctuations under a cumulative quadratic volatility constraint, and the Investor can make a sequence of hedging decisions. Our main result is to show that the value of our proposed game, which is the “regret” of hedging strategy, converges to the Black-Scholes option price. We use signiﬁcantly weaker assumptions than previous work—for instance, we allow large jumps in the asset price—and show that the Black-Scholes hedging strategy is near-optimal for the Investor even in this non-stochastic framework. 1</p><p>same-paper 2 0.80369651 <a title="102-lda-2" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>Author: Ziteng Wang, Kai Fan, Jiaqi Zhang, Liwei Wang</p><p>Abstract: We study differentially private mechanisms for answering smooth queries on databases consisting of data points in Rd . A K-smooth query is speciﬁed by a function whose partial derivatives up to order K are all bounded. We develop an -differentially private mechanism which for the class of K-smooth queries has K accuracy O(n− 2d+K / ). The mechanism ﬁrst outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time d O(n1+ 2d+K ), and the evaluation algorithm for answering a query runs in time d+2+ 2d K ˜ O(n 2d+K ). Our mechanism is based on L∞ -approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efﬁciently computable coefﬁcients. 1</p><p>3 0.71166867 <a title="102-lda-3" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>4 0.69660217 <a title="102-lda-4" href="./nips-2013-On_the_Complexity_and_Approximation_of_Binary_Evidence_in_Lifted_Inference.html">220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</a></p>
<p>Author: Guy van den Broeck, Adnan Darwiche</p><p>Abstract: Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model’s symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efﬁcient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically. 1</p><p>5 0.6812855 <a title="102-lda-5" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>6 0.67886609 <a title="102-lda-6" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>7 0.67109001 <a title="102-lda-7" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>8 0.66866249 <a title="102-lda-8" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>9 0.6646713 <a title="102-lda-9" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>10 0.65965974 <a title="102-lda-10" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>11 0.65932941 <a title="102-lda-11" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>12 0.6571511 <a title="102-lda-12" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>13 0.65638644 <a title="102-lda-13" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>14 0.65491974 <a title="102-lda-14" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>15 0.65442812 <a title="102-lda-15" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>16 0.65385228 <a title="102-lda-16" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>17 0.65306062 <a title="102-lda-17" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>18 0.65218985 <a title="102-lda-18" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>19 0.65213674 <a title="102-lda-19" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>20 0.65184945 <a title="102-lda-20" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
