<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-124" href="#">nips2013-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</h1>
<br/><p>Source: <a title="nips-2013-124-pdf" href="http://papers.nips.cc/paper/5180-forgetful-bayes-and-myopic-planning-human-learning-and-decision-making-in-a-bandit-setting.pdf">pdf</a></p><p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><p>Reference: <a title="nips-2013-124-reference" href="../nips2013_reference/nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kg', 0.568), ('dbm', 0.38), ('reward', 0.281), ('arm', 0.256), ('bandit', 0.179), ('fbm', 0.161), ('policy', 0.16), ('tri', 0.152), ('wsls', 0.152), ('head', 0.145), ('hum', 0.134), ('eg', 0.134), ('tk', 0.129), ('fb', 0.11), ('subject', 0.106), ('gam', 0.095), ('agr', 0.077), ('db', 0.075), ('ush', 0.074), ('decid', 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="124-tfidf-1" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>2 0.33511409 <a title="124-tfidf-2" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>3 0.28494483 <a title="124-tfidf-3" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>4 0.26106602 <a title="124-tfidf-4" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>5 0.24180152 <a title="124-tfidf-5" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>6 0.19078346 <a title="124-tfidf-6" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>7 0.17886585 <a title="124-tfidf-7" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>8 0.17485282 <a title="124-tfidf-8" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>9 0.17455769 <a title="124-tfidf-9" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>10 0.17104793 <a title="124-tfidf-10" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>11 0.16771166 <a title="124-tfidf-11" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>12 0.16537398 <a title="124-tfidf-12" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>13 0.15724036 <a title="124-tfidf-13" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>14 0.15553318 <a title="124-tfidf-14" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>15 0.1509652 <a title="124-tfidf-15" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>16 0.14877632 <a title="124-tfidf-16" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>17 0.14318892 <a title="124-tfidf-17" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>18 0.14259481 <a title="124-tfidf-18" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>19 0.13290963 <a title="124-tfidf-19" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>20 0.1320193 <a title="124-tfidf-20" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, -0.271), (2, -0.075), (3, 0.003), (4, 0.025), (5, -0.146), (6, 0.044), (7, 0.231), (8, -0.061), (9, 0.087), (10, -0.029), (11, 0.082), (12, -0.17), (13, 0.022), (14, -0.056), (15, -0.067), (16, -0.074), (17, 0.039), (18, -0.001), (19, -0.053), (20, 0.046), (21, -0.042), (22, 0.047), (23, -0.007), (24, 0.011), (25, 0.016), (26, 0.03), (27, 0.032), (28, 0.004), (29, -0.024), (30, -0.024), (31, -0.0), (32, 0.1), (33, 0.038), (34, 0.026), (35, 0.046), (36, 0.101), (37, -0.025), (38, -0.01), (39, 0.103), (40, -0.06), (41, -0.037), (42, 0.013), (43, 0.084), (44, 0.118), (45, -0.042), (46, 0.047), (47, 0.088), (48, -0.047), (49, -0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93377805 <a title="124-lsi-1" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>2 0.75874257 <a title="124-lsi-2" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>3 0.68054152 <a title="124-lsi-3" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>4 0.65002036 <a title="124-lsi-4" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>5 0.63088691 <a title="124-lsi-5" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>6 0.61896747 <a title="124-lsi-6" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>7 0.58270156 <a title="124-lsi-7" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>8 0.52908009 <a title="124-lsi-8" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>9 0.51303339 <a title="124-lsi-9" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>10 0.49484515 <a title="124-lsi-10" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>11 0.48983935 <a title="124-lsi-11" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>12 0.45523122 <a title="124-lsi-12" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>13 0.45098141 <a title="124-lsi-13" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>14 0.4362275 <a title="124-lsi-14" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>15 0.43539786 <a title="124-lsi-15" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>16 0.4214032 <a title="124-lsi-16" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>17 0.41365108 <a title="124-lsi-17" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>18 0.40820819 <a title="124-lsi-18" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>19 0.40097848 <a title="124-lsi-19" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>20 0.40075094 <a title="124-lsi-20" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.025), (20, 0.122), (23, 0.016), (24, 0.021), (25, 0.089), (37, 0.115), (66, 0.238), (70, 0.028), (80, 0.137), (86, 0.069), (87, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79895329 <a title="124-lda-1" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>2 0.70722729 <a title="124-lda-2" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>3 0.70181519 <a title="124-lda-3" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>4 0.70048225 <a title="124-lda-4" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>5 0.69862223 <a title="124-lda-5" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>6 0.69830769 <a title="124-lda-6" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>7 0.69789809 <a title="124-lda-7" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>8 0.69376606 <a title="124-lda-8" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>9 0.69312871 <a title="124-lda-9" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>10 0.6930123 <a title="124-lda-10" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>11 0.69276673 <a title="124-lda-11" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>12 0.69242352 <a title="124-lda-12" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>13 0.69139123 <a title="124-lda-13" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>14 0.69029802 <a title="124-lda-14" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>15 0.68952972 <a title="124-lda-15" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>16 0.68888342 <a title="124-lda-16" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>17 0.68887669 <a title="124-lda-17" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>18 0.68727744 <a title="124-lda-18" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>19 0.68684608 <a title="124-lda-19" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>20 0.68562007 <a title="124-lda-20" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
