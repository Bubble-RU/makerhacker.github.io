<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-124" href="#">nips2013-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</h1>
<br/><p>Source: <a title="nips-2013-124-pdf" href="http://papers.nips.cc/paper/5180-forgetful-bayes-and-myopic-planning-human-learning-and-decision-making-in-a-bandit-setting.pdf">pdf</a></p><p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><p>Reference: <a title="nips-2013-124-reference" href="../nips2013_reference/nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting  Angela J. [sent-1, score-0.222]
</p><p>2 We investigate this behavior in the context of a multi-armed bandit task. [sent-5, score-0.197]
</p><p>3 Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. [sent-7, score-0.252]
</p><p>4 These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. [sent-10, score-0.172]
</p><p>5 A classic task used to study such sequential decision making problems is the multi-arm bandit paradigm [15]. [sent-13, score-0.281]
</p><p>6 In a standard bandit setting, people are given a limited number of trials to choose among a set of alternatives, or arms. [sent-14, score-0.286]
</p><p>7 After each choice, an outcome is generated based on a hidden reward distribution speciﬁc to the arm chosen, and the objective is to maximize the total reward after all trials. [sent-15, score-0.725]
</p><p>8 The reward gained on each trial both has intrinsic value and informs the decision maker about the relative desirability of the arm, which can help with future decisions. [sent-16, score-0.525]
</p><p>9 In order to be successful, decision makers have to balance their decisions between general exploration (selecting an arm about which one is ignorant) and exploitation (selecting an arm that is known to have relatively high expected reward). [sent-17, score-0.538]
</p><p>10 Because bandit problem elegantly capture the tension between exploration and exploitation that is manifest in real-world decision-making situations, they have received attention in many ﬁelds, including statistics [10], reinforcement learning [11, 19], economics [1, e. [sent-18, score-0.309]
</p><p>11 There is no known analytical optimal solution to the general bandit problem, though properties about the optimal solution of special cases are known [10]. [sent-21, score-0.248]
</p><p>12 For relatively simple, ﬁnite-horizon problems, the optimal solution can be computed numerically via dynamic program1  ming [11], though its computational complexity grows exponentially with the number of arms and trials. [sent-22, score-0.215]
</p><p>13 In the psychology literature, a number of heuristic policies, with varying levels of complexity in the learning and control processes, have been proposed as possible strategies used by human subjects [5, 4, 18, 12]. [sent-23, score-0.289]
</p><p>14 win-stay-lose-shift and ε-greedy), or switch between an exploration and exploitation mode either randomly [5] or discretely over time as more is learned about the environment [18]. [sent-26, score-0.175]
</p><p>15 In this work, we analyze a new model for human bandit choice behavior, whose learning component is based on the dynamic belief model (DBM) [21], and whose control component is based on the knowledge gradient (KG) algorithm [7]. [sent-27, score-0.448]
</p><p>16 KG is a myopic approximation to the optimal policy for sequential informational control problem, originally developed for operations research applications [7]; KG is known to be exactly optimal in some special cases of bandit problems, such as when there are only two arms. [sent-30, score-0.445]
</p><p>17 KG has been shown to outperform several established models, including the optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift, for human decision-making in bandit problems, under two certain learning scenarios other than DBM [22]. [sent-34, score-0.322]
</p><p>18 We then compare the performance of the models both in terms of agreement with human behavior on a trial-to-trial basis, and in terms of computational optimality. [sent-36, score-0.234]
</p><p>19 In the experiment, each participant completed 20 bandit problems in sequence, all problems had 4 arms and 15 trials. [sent-38, score-0.303]
</p><p>20 The reward rates were ﬁxed for all arms in each game, and were generated, prior to the start of data collection, independently from a Beta(2, 2) distribution. [sent-39, score-0.472]
</p><p>21 All participants played the same reward rates, but the order of the games was randomized. [sent-40, score-0.352]
</p><p>22 Participants were instructed that the reward rates in all games were drawn from the same environment, and that the reward rates were drawn only once; participants were not told the exact form of the Beta environment, i. [sent-41, score-0.683]
</p><p>23 3  Models  There exist multiple levels of complexity and optimality in both the learning and the decision components of decision making models of bandit problems. [sent-45, score-0.336]
</p><p>24 The top of the interface provides the count of the total number of successes to the current trial, index of the current trial and index of the current game. [sent-55, score-0.202]
</p><p>25 (b) Bayesian graphical model of FBM, assuming ﬁxed reward probabilities. [sent-56, score-0.277]
</p><p>26 The inset shows an example of the Beta prior for the reward probabilities. [sent-58, score-0.31]
</p><p>27 (c) Bayesian graphical model of DBM, assuming reward probabilities change from trial to trial. [sent-60, score-0.441]
</p><p>28 differ in complexity in at least two respects: the objective the decision policy tries to optimize (e. [sent-62, score-0.198]
</p><p>29 reward versus information), and the time-horizon over which the decision policy optimizes its objective (e. [sent-64, score-0.475]
</p><p>30 1  Dynamic Belief Model  Under the dynamic belief model (DBM), the reward probabilities can undergo discrete changes at times during the experimental session, such that at any trial, the subject’s prior belief is a mixture of the posterior belief from the previous trial and a generic prior. [sent-74, score-0.829]
</p><p>31 The subject’s implicit task is then to track the evolving reward probability of each arm over the course of the experiment. [sent-75, score-0.448]
</p><p>32 Suppose on each game, we have K arms with reward rates, θk , k = 1, · · · , K, which are iid generated from Beta (α, β). [sent-76, score-0.412]
</p><p>33 Let Stk and Fkt be the numbers of successes and failures obtained from the kth arm on the trial t. [sent-77, score-0.413]
</p><p>34 The estimated reward probability of arm k at trial t is θtk . [sent-78, score-0.612]
</p><p>35 The Bayesian ideal observer combines the sequentially developed prior belief about reward probabilities, with the incoming stream of observations (successes and failures on each arm), to infer the new posterior distributions. [sent-80, score-0.502]
</p><p>36 all bandit arms have ﬁxed probabilities of giving a reward throughout the game. [sent-87, score-0.58]
</p><p>37 In the Bayesian update rule, the prior on each trial is simply the posterior on the previous trial. [sent-90, score-0.224]
</p><p>38 Due to the low dimensionality of the bandit problem here (i. [sent-98, score-0.168]
</p><p>39 small number of arms and number of trials per game), the optimal policy, up to a discretization of the belief state, can be computed numerically using Bellman’s dynamic programming principle [2]. [sent-100, score-0.383]
</p><p>40 Let V t (qt ) be the expected total future reward on trial t. [sent-101, score-0.441]
</p><p>41 In the simulations, we compute the optimal policy off-line, for any conceivable setting of belief state on each trial (up to a ﬁne discretization of the belief state space), and then apply the computed policy for each sequence of choice and observations that each subject experiences. [sent-103, score-0.659]
</p><p>42 It assumes that the decision-maker will keep choosing the same arm as long as it continues to produce a reward, but shifts to other arms (with equal probabilities) following a failure to gain reward. [sent-108, score-0.369]
</p><p>43 It starts off on the ﬁrst trial randomly (equal probability at all arms). [sent-109, score-0.164]
</p><p>44 On each trial, with probability ε, the decision-maker chooses randomly (exploration), otherwise chooses the arm with the greatest estimated reward rate (exploitation). [sent-113, score-0.538]
</p><p>45 ε-Greedy keeps simple estimates of the reward rates, but does not track the uncertainty of the estimates. [sent-114, score-0.277]
</p><p>46 4  More concretely, ε-greedy adopts a stochastic policy: (1 − ε) /Mt ε/ (K − Mt )  Pr Dt = k | ε, θt =  if k ∈ argmaxk θtk otherwise  where Mt is the number of arms with the greatest estimated value at the tth trial. [sent-116, score-0.198]
</p><p>47 It evaluates the expected change in each estimated reward rate, if a certain arm were to be chosen, based on the current belief state. [sent-120, score-0.544]
</p><p>48 The KG decision rule is DKG,t = arg max θtk + (T − t − 1) vKG,t k k  (6)  The ﬁrst term of Equation 6 denotes the expected immediate reward by choosing the kth arm on trial t, whereas the second term reﬂects the expected knowledge gain. [sent-122, score-0.752]
</p><p>49 The formula for calculating vKG,t k for the binary bandit problems can be found in Chapter 5 of [14]. [sent-123, score-0.168]
</p><p>50 For WSLS, it is 1 for a win-stay decision, 1/3 for a lose-shift decision (because the model predicts shifting to the other three arms with equal probability), and 0 otherwise. [sent-129, score-0.219]
</p><p>51 The reparameterization has psychological interpretation as the mean reward probability and the certainty. [sent-135, score-0.277]
</p><p>52 We ﬁt each model across all subjects, assuming that every subject shared the same prior belief of the environment (α and β), rate of exploration (ε), and rate of change (γ). [sent-141, score-0.292]
</p><p>53 For further analyses to be shown in the result section, we also ﬁt the ε-greedy policy and the KG policy together with both learning models for each individual subject. [sent-142, score-0.228]
</p><p>54 9  FBM DBM  Individually−fit Model agreement  057 058  1  Model agreement with subjects  056  a  Model agreement with optimal  054 055  DBM DBM ind. [sent-157, score-0.463]
</p><p>55 5  10 Trial  15  Figure 1: Average reward achieved by the KG model forward playing the bandit problems with the Figure 2: (a) agreement same reward rates. [sent-171, score-0.859]
</p><p>56 Model All modelswith data simulated by the optimal simulatedmeasured as the solution KG KG achieves similar reward distribution as solution, by the optimal average the human performance, with per-trial likelihood. [sent-172, score-0.471]
</p><p>57 (except the optimal) are ﬁt to data playing at its maximum beta prior Beta(2, 2). [sent-173, score-0.189]
</p><p>58 For ε-greedy reward trials and games) of decision policy coupled with a learning the correct prior knowledge (eG) and KG, of the environment. [sent-178, score-0.61]
</p><p>59 (b) Model agreement with human data based on a leave-one(game)-out cross-validation, where the preferred typeface game from each subject for are separated train the model on New Roman iswe randomly withhold one throughout. [sent-181, score-0.322]
</p><p>60 (d) Trialwise agreement of eG and KG under individually-ﬁt MAP per-trial likelihood is calculated across all subjects for each trial, with the error bars showing the For the ﬁnal version, authors’ names are set in boldface, tests. [sent-194, score-0.272]
</p><p>61 We ﬁrst examine how well each of the decision policies agrees with the optimal policy on a trialto-trial basis. [sent-200, score-0.289]
</p><p>62 KG algorithm, under either learning framework, is most consistent (over 90%) with the optimal algorithm (separately under FBM and proper nouns), ﬂush is not surFirst level headings are lower case (except for ﬁrst word and DBM assumptions). [sent-202, score-0.314]
</p><p>63 Thisleft, bold and in prising given that KG is before the ﬁrst level heading and 1/2 line The inferred prior is point size 12. [sent-203, score-0.214]
</p><p>64 1 Headings: second level from an arm that the optimal policy would otherwise stay with. [sent-211, score-0.424]
</p><p>65 Second level headings are lower case (except for ﬁrst word and proper nouns), ﬂush left, bold and in point size 10. [sent-213, score-0.318]
</p><p>66 One line space before the second level heading and 1/2 line space after the second 4. [sent-214, score-0.165]
</p><p>67 122, respecThird level headings aremeans for α and β are . [sent-224, score-0.218]
</p><p>68 81 implies that the third level heading and 1/2 line space after the third level heading. [sent-230, score-0.184]
</p><p>69 2  3  15 Trial  3  15 Trial  Figure 1: Average reward in the human data model simulated data the bandit problems with the Figure 3: Behavioral patterns achieved by the KGand theforward playing from all models. [sent-247, score-0.605]
</p><p>70 Probabilities the optimal solution on simulated data from each model at their chosen, same reward distribution as are calculated basedwhen playing with the correct prior knowledge 067 of the environment. [sent-253, score-0.426]
</p><p>71 078 079 Figure 2c shows the model agreement with human data, of ε-greedy and KG, when their parameters Please pay special attention to the instructions in section 3 regarding ﬁgures, tables, acknowledg080 are individuallyreferences. [sent-285, score-0.242]
</p><p>72 2 Headings: ﬁrst systematic difference between the two models in their agreement with 083 However, Figure 2d shows alevel human data on a trial-by-trial base: during early trials, subjects’ behavior is more consistent with 084 First whereas during later trials, it is more consistent with KG. [sent-290, score-0.234]
</p><p>73 085 ε-greedy, level headings are lower case (except for ﬁrst word and proper nouns), ﬂush left, bold and in point size 12. [sent-291, score-0.318]
</p><p>74 One line space before the ﬁrst level heading and 1/2 line space after the ﬁrst level 086 We next break down the overall behavioral performance into four ﬁner measures: how often people heading. [sent-292, score-0.308]
</p><p>75 Second level headings are lower case (except for ﬁrst word and proper nouns), ﬂush left, bold and 091 in point size 10. [sent-297, score-0.318]
</p><p>76 One line space before the second level heading and 1/2 the same after the second The ﬁrst panel, for example, shows the trialwise probability of staying withline space arm following 092 level heading. [sent-298, score-0.443]
</p><p>77 People do not stay with the same arm after an immediate reward, which is 093 always the case for the optimal algorithm. [sent-300, score-0.289]
</p><p>78 In fact, subjects explore more during early trials, and become more exploitative later 2. [sent-302, score-0.225]
</p><p>79 As implied by Equation 5, KG calculates the probability of an arm surpassing Third best headings are and weights the knowledge gain more heavily in the early stage and in 096 the known level upon chosen,lower case (except for ﬁrst word and proper nouns), ﬂush left, boldof the point size 10. [sent-305, score-0.507]
</p><p>80 During the early trials, it sometimes chooses the second-best arm line space after the third level to maximize the knowledge heading. [sent-307, score-0.304]
</p><p>81 064  103  the horizon is approaching, it becomes increasingly important to stay with the arm that is known to 3. [sent-312, score-0.261]
</p><p>82 The fourth panels shows how often subjects choose to explore the least known option when they shift away from the choice with the highest expected reward. [sent-323, score-0.226]
</p><p>83 5  Discussion  Our analysis suggests that human behavior in the multi-armed bandit task is best captured by a knowledge gradient decision policy supported by a dynamic belief model learning process. [sent-327, score-0.675]
</p><p>84 Human subjects tend to explore more often than policies that optimize the speciﬁc utility of the bandit problems, and KG with DBM attributes this tendency to the belief of a stochastically changing environment, causing the sequential effects due to recent trial history. [sent-328, score-0.658]
</p><p>85 Concretely, we ﬁnd that people adopt a learning process that (erroneously) assumes the world to be non-stationary, and that they employ a semi-myopic choice policy that is sensitive to the horizon but assumes one-step exploration when comparing action values. [sent-329, score-0.325]
</p><p>86 Our results indicate that all decision policies considered here capture human data much better under the dynamic belief model than the ﬁxed belief model. [sent-330, score-0.481]
</p><p>87 Indeed, there is previous work suggesting that people approach bandit problems as if expecting a changing world [17]. [sent-335, score-0.214]
</p><p>88 This is despite informing the subjects that the arms have ﬁxed reward probabilities. [sent-336, score-0.562]
</p><p>89 So far, our results also favor the knowledge gradient policy as the best model for human decisionmaking in the bandit task. [sent-337, score-0.451]
</p><p>90 It optimizes the semi-myopic goal of maximizing future cumulative reward while assuming only one more time step of exploration and strict exploitation thereafter. [sent-338, score-0.389]
</p><p>91 The KG model under the more general DBM has the largest proportion of correct predictions of human data, and can capture the trial-wise dynamics of human behavioral reasonably well. [sent-339, score-0.278]
</p><p>92 This result implies that humans may use a normative way, as captured by KG, to explore by combining immediate reward expectation and long-term knowledge gain, compared to the previously proposed behavioral models that typically assumes that exploration is random or arbitrary. [sent-340, score-0.51]
</p><p>93 In addition, KG achieves similar behavioral patterns as the optimal model, and is computationally much less expensive (in particular being online and incurring a constant cost), making it a more plausible algorithm for human learning and decision-making. [sent-341, score-0.204]
</p><p>94 We observed that decision policies vary systematically in their abilities to predict human behavior on different kinds of trials. [sent-342, score-0.278]
</p><p>95 In the real world, people might use hybrid policies to solve the bandit problems; they might also use some smart heuristics, which dynamically adjusts the weight of the knowledge gain to the immediate reward gain. [sent-343, score-0.63]
</p><p>96 Figure 2d suggests that subjects may be adopting a strategy that is aggressively greedy at the beginning of the game, and then switches to a policy that is both sensitive to the value of exploration and the impending horizon as the end of the game approaches. [sent-344, score-0.424]
</p><p>97 One possibility is that subjects discount future rewards, which would result in a more exploitative behavior than non-discounted KG, especially at the beginning of the game. [sent-345, score-0.254]
</p><p>98 Psychological models of human and optimal performance in bandit problems. [sent-448, score-0.322]
</p><p>99 A bayesian analysis of human decisionmaking on bandit problems. [sent-483, score-0.336]
</p><p>100 Cheap but clever: Human active learning in a bandit setting. [sent-512, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kg', 0.533), ('dbm', 0.357), ('reward', 0.277), ('headings', 0.171), ('arm', 0.171), ('bandit', 0.168), ('trial', 0.164), ('eg', 0.152), ('fbm', 0.151), ('subjects', 0.15), ('wsls', 0.143), ('arms', 0.135), ('tk', 0.121), ('policy', 0.114), ('human', 0.114), ('beta', 0.11), ('fb', 0.104), ('belief', 0.096), ('agreement', 0.091), ('ush', 0.086), ('decision', 0.084), ('exploitative', 0.075), ('trials', 0.072), ('db', 0.071), ('op', 0.068), ('qt', 0.067), ('exploration', 0.065), ('environment', 0.063), ('heading', 0.062), ('nouns', 0.062), ('game', 0.057), ('citations', 0.057), ('fkt', 0.057), ('trialwise', 0.057), ('myopic', 0.054), ('stay', 0.052), ('policies', 0.051), ('inch', 0.05), ('games', 0.05), ('behavioral', 0.05), ('exploitation', 0.047), ('level', 0.047), ('title', 0.046), ('people', 0.046), ('playing', 0.046), ('cognitive', 0.045), ('option', 0.045), ('bold', 0.044), ('rtk', 0.043), ('dynamic', 0.04), ('optimal', 0.04), ('failures', 0.04), ('successes', 0.038), ('horizon', 0.038), ('instructions', 0.037), ('subject', 0.035), ('powell', 0.035), ('greatest', 0.034), ('prior', 0.033), ('gain', 0.032), ('names', 0.031), ('assumes', 0.031), ('highest', 0.031), ('staying', 0.031), ('humans', 0.031), ('exploratory', 0.031), ('knowledge', 0.03), ('bernoulli', 0.03), ('sequential', 0.029), ('bayesian', 0.029), ('behavior', 0.029), ('observer', 0.029), ('argmaxk', 0.029), ('formatter', 0.029), ('picas', 0.029), ('qtk', 0.029), ('screenshot', 0.029), ('tension', 0.029), ('proper', 0.028), ('line', 0.028), ('word', 0.028), ('author', 0.028), ('options', 0.028), ('chooses', 0.028), ('panel', 0.028), ('posterior', 0.027), ('rates', 0.027), ('pr', 0.027), ('tests', 0.027), ('immediate', 0.026), ('participants', 0.025), ('decisionmaking', 0.025), ('typeface', 0.025), ('psychology', 0.025), ('tables', 0.025), ('rt', 0.024), ('choices', 0.024), ('stk', 0.023), ('everyone', 0.023), ('forgetful', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="124-tfidf-1" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><p>2 0.26005393 <a title="124-tfidf-2" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>Author: Thomas Bonald, Alexandre Proutiere</p><p>Abstract: We consider an inﬁnite-armed bandit problem with Bernoulli rewards. The mean rewards are independent, uniformly distributed over [0, 1]. Rewards 0 and 1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit any arm is based on two successive targets, namely, the total number of successes until the ﬁrst failure and until the ﬁrst m failures, respectively, where m is a ﬁxed parameter. This two-target algorithm achieves a √ long-term average regret in 2n for a large parameter m and a known time horizon n. This regret is optimal and strictly less than the regret achieved by the best √ known algorithms, which is in 2 n. The results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons. Numerical experiments show the performance of the algorithm for ﬁnite time horizons. 1</p><p>3 0.25986087 <a title="124-tfidf-3" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>Author: Ian Goodfellow, Mehdi Mirza, Aaron Courville, Yoshua Bengio</p><p>Abstract: We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classiﬁcation tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classiﬁcation, classiﬁcation with missing inputs, and mean ﬁeld prediction tasks.1 1</p><p>4 0.22454648 <a title="124-tfidf-4" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>5 0.17794815 <a title="124-tfidf-5" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>Author: Eshcar Hillel, Zohar S. Karnin, Tomer Koren, Ronny Lempel, Oren Somekh</p><p>Abstract: We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an ε-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to com√ municate only once, they are able to learn k times faster than a single player. √ That is, distributing learning to k players gives rise to a factor k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε. 1</p><p>6 0.14960781 <a title="124-tfidf-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.14943182 <a title="124-tfidf-7" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>8 0.1464891 <a title="124-tfidf-8" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>9 0.14312734 <a title="124-tfidf-9" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>10 0.13282834 <a title="124-tfidf-10" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>11 0.13214423 <a title="124-tfidf-11" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>12 0.12852246 <a title="124-tfidf-12" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>13 0.1232517 <a title="124-tfidf-13" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>14 0.12119826 <a title="124-tfidf-14" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>15 0.12114535 <a title="124-tfidf-15" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>16 0.11632919 <a title="124-tfidf-16" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>17 0.11533692 <a title="124-tfidf-17" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>18 0.1150832 <a title="124-tfidf-18" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>19 0.11442775 <a title="124-tfidf-19" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>20 0.10697913 <a title="124-tfidf-20" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.235), (2, -0.088), (3, -0.019), (4, -0.006), (5, -0.082), (6, 0.116), (7, -0.092), (8, -0.076), (9, -0.045), (10, 0.011), (11, 0.155), (12, -0.123), (13, -0.044), (14, -0.116), (15, 0.06), (16, -0.115), (17, 0.071), (18, -0.017), (19, 0.017), (20, 0.126), (21, -0.053), (22, -0.006), (23, -0.046), (24, -0.004), (25, 0.008), (26, -0.014), (27, 0.066), (28, -0.088), (29, 0.053), (30, -0.059), (31, 0.036), (32, -0.029), (33, -0.021), (34, 0.131), (35, -0.103), (36, -0.077), (37, 0.006), (38, -0.156), (39, 0.085), (40, -0.027), (41, 0.123), (42, 0.006), (43, 0.038), (44, -0.053), (45, 0.14), (46, -0.06), (47, -0.009), (48, 0.09), (49, -0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95290393 <a title="124-lsi-1" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><p>2 0.6288901 <a title="124-lsi-2" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>Author: Thomas Bonald, Alexandre Proutiere</p><p>Abstract: We consider an inﬁnite-armed bandit problem with Bernoulli rewards. The mean rewards are independent, uniformly distributed over [0, 1]. Rewards 0 and 1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit any arm is based on two successive targets, namely, the total number of successes until the ﬁrst failure and until the ﬁrst m failures, respectively, where m is a ﬁxed parameter. This two-target algorithm achieves a √ long-term average regret in 2n for a large parameter m and a known time horizon n. This regret is optimal and strictly less than the regret achieved by the best √ known algorithms, which is in 2 n. The results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons. Numerical experiments show the performance of the algorithm for ﬁnite time horizons. 1</p><p>3 0.62538195 <a title="124-lsi-3" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>Author: Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles Isbell, Andrea L. Thomaz</p><p>Abstract: A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.</p><p>4 0.56980693 <a title="124-lsi-4" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>5 0.55138147 <a title="124-lsi-5" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>Author: Mohammad Gheshlaghi azar, Alessandro Lazaric, Emma Brunskill</p><p>Abstract: Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may signiﬁcantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi–armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it. 1</p><p>6 0.5425877 <a title="124-lsi-6" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>7 0.5203104 <a title="124-lsi-7" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>8 0.51233917 <a title="124-lsi-8" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>9 0.47399557 <a title="124-lsi-9" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>10 0.42605084 <a title="124-lsi-10" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>11 0.42486084 <a title="124-lsi-11" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>12 0.40951815 <a title="124-lsi-12" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>13 0.40450624 <a title="124-lsi-13" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>14 0.39573002 <a title="124-lsi-14" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>15 0.39267558 <a title="124-lsi-15" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>16 0.39064652 <a title="124-lsi-16" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>17 0.38808134 <a title="124-lsi-17" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>18 0.38316879 <a title="124-lsi-18" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>19 0.37533545 <a title="124-lsi-19" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>20 0.3566125 <a title="124-lsi-20" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.023), (16, 0.026), (27, 0.016), (33, 0.144), (34, 0.099), (36, 0.02), (41, 0.053), (49, 0.025), (56, 0.129), (70, 0.034), (77, 0.22), (85, 0.062), (89, 0.036), (93, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85322249 <a title="124-lda-1" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>Author: Beomjoon Kim, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: We propose a Learning from Demonstration (LfD) algorithm which leverages expert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error interactions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert’s suggestions are used to deﬁne linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-ﬁnding task. 1</p><p>2 0.84166926 <a title="124-lda-2" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>Author: Dae Il Kim, Prem Gopalan, David Blei, Erik Sudderth</p><p>Abstract: Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efﬁcient structured mean ﬁeld variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show signiﬁcantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government. 1</p><p>same-paper 3 0.83451366 <a title="124-lda-3" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><p>4 0.8096776 <a title="124-lda-4" href="./nips-2013-The_Power_of_Asymmetry_in_Binary_Hashing.html">326 nips-2013-The Power of Asymmetry in Binary Hashing</a></p>
<p>Author: Behnam Neyshabur, Nati Srebro, Ruslan Salakhutdinov, Yury Makarychev, Payman Yadollahpour</p><p>Abstract: When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x as the hamming distance between f (x) and g(x ), for two distinct binary codes f, g, rather than as the hamming distance between f (x) and f (x ). 1</p><p>5 0.7188611 <a title="124-lda-5" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>Author: Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, Guillermo Sapiro</p><p>Abstract: Graph matching is a challenging problem with very important applications in a wide range of ﬁelds, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efﬁciently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. 1</p><p>6 0.71489269 <a title="124-lda-6" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>7 0.71475303 <a title="124-lda-7" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>8 0.71424586 <a title="124-lda-8" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>9 0.7132529 <a title="124-lda-9" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>10 0.71226746 <a title="124-lda-10" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>11 0.71212804 <a title="124-lda-11" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>12 0.71136093 <a title="124-lda-12" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>13 0.71131748 <a title="124-lda-13" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>14 0.71109617 <a title="124-lda-14" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>15 0.71031237 <a title="124-lda-15" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>16 0.71006644 <a title="124-lda-16" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>17 0.70980978 <a title="124-lda-17" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>18 0.70960188 <a title="124-lda-18" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>19 0.70924193 <a title="124-lda-19" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>20 0.7090528 <a title="124-lda-20" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
