<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2013-Learning with Noisy Labels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-171" href="#">nips2013-171</a> knowledge-graph by maker-knowledge-mining</p><h1>171 nips-2013-Learning with Noisy Labels</h1>
<br/><p>Source: <a title="nips-2013-171-pdf" href="http://papers.nips.cc/paper/5073-learning-with-noisy-labels.pdf">pdf</a></p><p>Author: Nagarajan Natarajan, Inderjit Dhillon, Pradeep Ravikumar, Ambuj Tewari</p><p>Abstract: In this paper, we theoretically study the problem of binary classiﬁcation in the presence of random classiﬁcation noise — the learner, instead of seeing the true labels, sees labels that have independently been ﬂipped with some small probability. Moreover, random label noise is class-conditional — the ﬂip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisﬁes a simple symmetry condition, we show that the method leads to an efﬁcient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classiﬁcation with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence — methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.</p><p>Reference: <a title="nips-2013-171-reference" href="../nips2013_reference/nips-2013-Learning_with_Noisy_Labels_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stempfel', 0.315), ('loss', 0.25), ('risk', 0.247), ('noisy', 0.246), ('nois', 0.22), ('cram', 0.217), ('cle', 0.209), ('label', 0.197), ('nherd', 0.189), ('ralaivol', 0.167), ('unbias', 0.163), ('biconjug', 0.139), ('hin', 0.139), ('surrog', 0.137), ('convex', 0.135), ('proxy', 0.122), ('scot', 0.122), ('corrupt', 0.115), ('hing', 0.1), ('banan', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="171-tfidf-1" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>2 0.16850375 <a title="171-tfidf-2" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>3 0.14282468 <a title="171-tfidf-3" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>4 0.1350868 <a title="171-tfidf-4" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>5 0.13248013 <a title="171-tfidf-5" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>6 0.12611786 <a title="171-tfidf-6" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>7 0.1254282 <a title="171-tfidf-7" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>8 0.12410441 <a title="171-tfidf-8" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>9 0.1235441 <a title="171-tfidf-9" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>10 0.11035246 <a title="171-tfidf-10" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>11 0.10477336 <a title="171-tfidf-11" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>12 0.10434505 <a title="171-tfidf-12" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>13 0.10164054 <a title="171-tfidf-13" href="./nips-2013-Robust_Bloom_Filters_for_Large_MultiLabel_Classification_Tasks.html">279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</a></p>
<p>14 0.098375738 <a title="171-tfidf-14" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>15 0.097126298 <a title="171-tfidf-15" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>16 0.09633936 <a title="171-tfidf-16" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>17 0.095902123 <a title="171-tfidf-17" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>18 0.095182315 <a title="171-tfidf-18" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>19 0.094344035 <a title="171-tfidf-19" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>20 0.089939788 <a title="171-tfidf-20" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.039), (2, 0.106), (3, 0.052), (4, -0.044), (5, 0.028), (6, -0.024), (7, -0.066), (8, -0.016), (9, 0.075), (10, -0.004), (11, -0.024), (12, 0.033), (13, -0.073), (14, 0.037), (15, -0.083), (16, 0.125), (17, 0.013), (18, 0.026), (19, 0.017), (20, -0.104), (21, 0.189), (22, 0.105), (23, -0.023), (24, -0.028), (25, -0.101), (26, -0.011), (27, 0.087), (28, 0.149), (29, 0.047), (30, 0.038), (31, -0.033), (32, -0.039), (33, 0.042), (34, -0.056), (35, -0.007), (36, -0.012), (37, 0.024), (38, -0.045), (39, -0.034), (40, -0.048), (41, 0.044), (42, -0.153), (43, -0.015), (44, 0.093), (45, 0.033), (46, 0.021), (47, 0.048), (48, -0.03), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9539417 <a title="171-lsi-1" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>2 0.79772586 <a title="171-lsi-2" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>3 0.69598007 <a title="171-lsi-3" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>4 0.66669595 <a title="171-lsi-4" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>5 0.665977 <a title="171-lsi-5" href="./nips-2013-Robust_Bloom_Filters_for_Large_MultiLabel_Classification_Tasks.html">279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</a></p>
<p>6 0.66008461 <a title="171-lsi-6" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>7 0.65224206 <a title="171-lsi-7" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>8 0.64951921 <a title="171-lsi-8" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>9 0.64104617 <a title="171-lsi-9" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>10 0.62702972 <a title="171-lsi-10" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>11 0.62423587 <a title="171-lsi-11" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>12 0.62367713 <a title="171-lsi-12" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>13 0.60800999 <a title="171-lsi-13" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>14 0.59269172 <a title="171-lsi-14" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>15 0.58837956 <a title="171-lsi-15" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>16 0.57748932 <a title="171-lsi-16" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>17 0.57731092 <a title="171-lsi-17" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>18 0.57716513 <a title="171-lsi-18" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>19 0.57424313 <a title="171-lsi-19" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>20 0.57359552 <a title="171-lsi-20" href="./nips-2013-A_Latent_Source_Model_for_Nonparametric_Time_Series_Classification.html">10 nips-2013-A Latent Source Model for Nonparametric Time Series Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(20, 0.165), (25, 0.129), (37, 0.041), (42, 0.257), (70, 0.037), (80, 0.067), (86, 0.146), (87, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80687737 <a title="171-lda-1" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>2 0.78986651 <a title="171-lda-2" href="./nips-2013-On_Algorithms_for_Sparse_Multi-factor_NMF.html">214 nips-2013-On Algorithms for Sparse Multi-factor NMF</a></p>
<p>3 0.73039538 <a title="171-lda-3" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>4 0.72891593 <a title="171-lda-4" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>5 0.72842628 <a title="171-lda-5" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>6 0.72780353 <a title="171-lda-6" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>7 0.72762936 <a title="171-lda-7" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>8 0.72598195 <a title="171-lda-8" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>9 0.72595835 <a title="171-lda-9" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>10 0.72576267 <a title="171-lda-10" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>11 0.72550929 <a title="171-lda-11" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>12 0.72548985 <a title="171-lda-12" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>13 0.7247932 <a title="171-lda-13" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>14 0.72367895 <a title="171-lda-14" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>15 0.72330105 <a title="171-lda-15" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>16 0.7231645 <a title="171-lda-16" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>17 0.72290134 <a title="171-lda-17" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>18 0.7225281 <a title="171-lda-18" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>19 0.72220689 <a title="171-lda-19" href="./nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</a></p>
<p>20 0.72189641 <a title="171-lda-20" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
