<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-222" href="#">nips2013-222</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</h1>
<br/><p>Source: <a title="nips-2013-222-pdf" href="http://papers.nips.cc/paper/4936-on-the-linear-convergence-of-the-proximal-gradient-method-for-trace-norm-regularization.pdf">pdf</a></p><p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>Reference: <a title="nips-2013-222-reference" href="../nips2013_reference/nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. [sent-8, score-0.68]
</p><p>2 Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. [sent-9, score-0.254]
</p><p>3 In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. [sent-10, score-0.213]
</p><p>4 Our result is established without any strong convexity assumption on the loss function. [sent-11, score-0.149]
</p><p>5 A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest. [sent-12, score-0.368]
</p><p>6 Due to the combinatorial nature of the rank function, the task of recovering a matrix with the desired rank and properties is generally intractable. [sent-15, score-0.261]
</p><p>7 To circumvent this, a popular approach is to use the trace norm1 (also known as the nuclear norm) as a surrogate for the rank function. [sent-16, score-0.372]
</p><p>8 Such an approach is quite natural, as the trace norm is the tightest convex lower bound of the rank function over the set of matrices with spectral norm at most one [13]. [sent-17, score-0.936]
</p><p>9 In the context of machine learning, the trace norm is typically used as a regularizer in the minimization of certain convex loss function. [sent-18, score-0.626]
</p><p>10 This gives rise to convex optimization problems of the form min  X∈Rm×n  {F (X) = f (X) + τ X  ∗} ,  (1)  where f : Rm×n → R is the convex loss function, X ∗ denotes the trace norm of X, and τ > 0 is a regularization parameter. [sent-19, score-0.818]
</p><p>11 By standard results in convex optimization [4], the above formulation is tractable (i. [sent-20, score-0.138]
</p><p>12 , polynomial–time solvable) for many choices of the loss function f . [sent-22, score-0.085]
</p><p>13 In practice, 1  Recall that the trace norm of a matrix is deﬁned as the sum of its singular values. [sent-23, score-0.559]
</p><p>14 Currently, a popular method for solving (1) is the proximal gradient method (PGM), which exploits the composite nature of the objective function F and certain smoothness properties of the loss function f [8, 19, 11]. [sent-26, score-0.357]
</p><p>15 The attractiveness of PGM lies not only in its excellent numerical performance, but also in its strong theoretical convergence rate guarantees. [sent-27, score-0.128]
</p><p>16 Indeed, for the trace norm–regularized problem (1) with f being convex and continuously differentiable and ∇f being Lipschitz continuous, the standard PGM will achieve an additive error of O(1/k) in the optimal value after k iterations. [sent-28, score-0.469]
</p><p>17 The sublinear O(1/k 2 ) convergence rate is known to be optimal if f is simply given by a ﬁrst–order oracle [12]. [sent-32, score-0.2]
</p><p>18 On the other hand, if f is strongly convex, then the convergence rate can be improved to O(ck ) for some c ∈ (0, 1) (i. [sent-33, score-0.17]
</p><p>19 However, in machine learning, the loss functions of interest are often highly structured and hence not just given by an oracle, but they are not necessarily strongly convex either. [sent-36, score-0.265]
</p><p>20 For instance, in matrix completion, a commonly used loss function is the square loss f (·) = A(·) − b 2 /2, where A : Rm×n → Rp is a linear measurement 2 operator and b ∈ Rp is a given set of observations. [sent-37, score-0.368]
</p><p>21 Clearly, f is not strongly convex when A has a non–trivial nullspace (or equivalently, when A is not injective). [sent-38, score-0.18]
</p><p>22 In view of this, it is natural to ask whether linear convergence of the PGM can be established for a larger class of loss functions. [sent-39, score-0.215]
</p><p>23 Speciﬁcally, we show that when the loss function f takes the form f (X) = h(A(X)), where A : Rm×n → Rp is an arbitrary linear operator and h : Rp → R is strictly convex with certain smoothness and curvature properties, the PGM for solving (1) has an asymptotic linear rate of convergence. [sent-41, score-0.377]
</p><p>24 Note that f need not be strictly convex even if h is, as A is arbitrary. [sent-42, score-0.138]
</p><p>25 Our result covers a wide range of loss functions used in the literature, such as square loss and logistic loss. [sent-43, score-0.215]
</p><p>26 Moreover, to the best of our knowledge, it is the ﬁrst linear convergence result concerning the application of a ﬁrst–order method to the trace norm–regularized problem (1) that does not require the strong convexity of f . [sent-44, score-0.383]
</p><p>27 The key to our convergence analysis is a new Lipschitzian error bound for problem (1). [sent-45, score-0.185]
</p><p>28 Roughly, it says that the distance between a point X ∈ Rm×n and the optimal solution set of (1) is on the order of the residual norm proxτ (X − ∇f (X)) − X F , where proxτ is the proximity operator associated with the regularization term τ X ∗ . [sent-46, score-0.543]
</p><p>29 Once we have such a bound, a routine application of the powerful analysis framework developed by Luo and Tseng [10] will yield the desired linear convergence result. [sent-47, score-0.101]
</p><p>30 Prior to this work, Lipschitzian error bounds for composite function minimization are available for cases where the non–smooth part either has a polyhedral epigraph (such as the ℓ1 –norm) [23] or is the (sparse) group LASSO regularization [22, 25]. [sent-48, score-0.231]
</p><p>31 However, the question of whether a similar bound holds for trace norm regularization has remained open, despite its apparent similarity to ℓ1 –norm regularization. [sent-49, score-0.494]
</p><p>32 Indeed, unlike the ℓ1 –norm, the trace norm has a non– polyhedral epigraph; see, e. [sent-50, score-0.444]
</p><p>33 Moreover, the existing approach for establishing error bounds for ℓ1 –norm or (sparse) group LASSO regularization is based on splitting the decision variables into groups, where variables from different groups do not interfere with one another, so that each group can be analyzed separately. [sent-53, score-0.101]
</p><p>34 However, the trace norm of a matrix is determined by its singular values, and each of them depends on every single entry of the matrix. [sent-54, score-0.559]
</p><p>35 To overcome the above difﬁculties, we make the ¯ ¯ ¯ crucial observation that if X is an optimal solution to (1), then both X and −∇f (X) have the same set of left and right singular vectors; see Proposition 4. [sent-56, score-0.2]
</p><p>36 As a result, we can use matrix perturbation theory to get hold of the spectral structure of the points that are close to the optimal solution set. [sent-58, score-0.198]
</p><p>37 This in turn allows us to establish a Lipschitzian error bound for the trace norm–regularized problem (1), thereby resolving the aforementioned open question in the afﬁrmative. [sent-59, score-0.359]
</p><p>38 1 Basic Setup We consider the trace norm–regularized optimization problem (1), in which the loss function f : Rm×n → R takes the form f (X) = h(A(X)), (2) where A : Rm×n → Rp is a linear operator and h : Rp → R is a function satisfying the following assumptions: 2  Assumption 2. [sent-61, score-0.397]
</p><p>39 (b) The function h is continuously differentiable with Lipschitz–continuous gradient on dom(h) and is strongly convex on any convex compact subset of dom(h). [sent-63, score-0.451]
</p><p>40 1(b) implies the strict convexity of h on dom(h) and the Lipschitz continuity of ∇f . [sent-65, score-0.131]
</p><p>41 For instance, in matrix completion, the square loss f (·) = A(·) − b 2 /2 induced by the linear measurement operator A and the set 2 of observations b ∈ Rp is of the form (2), with h(·) = (·) − b 2 /2. [sent-70, score-0.283]
</p><p>42 , AT (X)), and h : RT p → R is T given by h(z) = t=1 ℓ(zt , yt ) with zt ∈ Rp for t = 1, . [sent-78, score-0.144]
</p><p>43 Moreover, in the case where ℓ is, say, the square loss (i. [sent-85, score-0.13]
</p><p>44 , ℓ(zt , yt ) = zt − yt 2 /2) or the logistic loss (i. [sent-87, score-0.296]
</p><p>45 2 Some Facts about the Optimal Solution Set Since f (·) = h(A(·)) by (2) and h(·) is strictly convex on dom(h) by Assumption 2. [sent-93, score-0.138]
</p><p>46 1(b), it is easy to verify that the map X → A(X) is invariant over the optimal solution set X . [sent-94, score-0.101]
</p><p>47 In particular, X is a non–empty convex compact set. [sent-97, score-0.184]
</p><p>48 ¯ This implies that every X ∈ Rm×n has a unique projection X ∈ X onto X , which is given by the solution to the following optimization problem: dist(X, X ) = min X − Y Y ∈X  F. [sent-98, score-0.166]
</p><p>49 3 Proximal Gradient Method and the Residual Map To motivate the PGM for solving (1), we recall an alternative characterization of the optimal solution set X . [sent-103, score-0.132]
</p><p>50 Consider the proximity operator proxτ : Rm×n → Rm×n , which is deﬁned as τ Z  proxτ (X) = arg min  Z∈Rm×n  ∗  +  1 X −Z 2  2 F  . [sent-104, score-0.148]
</p><p>51 (3)  By comparing the optimality conditions for (1) and (3), it is immediate that a solution X ∗ ∈ Rm×n is optimal for (1) if and only if it satisﬁes the following ﬁxed–point equation: X ∗ = proxτ (X ∗ − ∇f (X ∗ )). [sent-105, score-0.14]
</p><p>52 As is well–known, the proximity operator deﬁned above can be expressed in terms of the so–called matrix 3  shrinkage operator. [sent-112, score-0.257]
</p><p>53 The non–negative vector shrinkage operator sµ : Rp → Rp is deﬁned as (sµ (z))i = max{0, zi − µ}, + + where i = 1, . [sent-115, score-0.148]
</p><p>54 The matrix shrinkage operator Sµ : Rm×n → Rm×n is deﬁned as Sµ (X) = U Σµ V T , where X = U ΣV T is the singular value decomposition of X with Σ = Diag(σ(X)) and σ(X) being the vector of singular values of X, and Σµ = Diag(sµ (σ(X))). [sent-119, score-0.437]
</p><p>55 Our goal in this paper is to study the convergence rate of the PGM (5). [sent-123, score-0.128]
</p><p>56 One natural candidate would be dist(·, X ), the distance to the optimal solution set X . [sent-125, score-0.101]
</p><p>57 In view of (4) and (6), a reasonable alternative would be the norm of the residual map R : Rm×n → Rm×n , which is deﬁned as R(X) = Sτ (X − ∇f (X)) − X. [sent-127, score-0.24]
</p><p>58 (7) m×n  Intuitively, the residual map measures how much a solution X ∈ R violates the optimality condition (4). [sent-128, score-0.153]
</p><p>59 In particular, X is an optimal solution to (1) if and only if R(X) = 0. [sent-129, score-0.101]
</p><p>60 This motivates the development of a so–called error bound for problem (1). [sent-131, score-0.084]
</p><p>61 3 Main Results Key to our convergence analysis of the PGM (5) is the following error bound for problem (1), which constitutes the main contribution of this paper: Theorem 3. [sent-132, score-0.185]
</p><p>62 1 and some standard properties of the PGM (5), we can apply the convergence analysis framework developed by Luo and Tseng [10] to establish the linear convergence of (5). [sent-138, score-0.231]
</p><p>63 R– linearly) to a vector w∞ if there exist an index K ≥ 0 and a constant ρ ∈ (0, 1) such that wk+1 − w∞ 2 / wk − w∞ 2 ≤ ρ for all k ≥ K (resp. [sent-140, score-0.079]
</p><p>64 if there exist constants γ > 0 and ρ ∈ (0, 1) such that wk − w∞ 2 ≤ γ · ρk for all k ≥ 0). [sent-141, score-0.079]
</p><p>65 Then, the sequence of solutions {X k }k≥0 generated by the PGM (5) converges R–linearly to an element in the optimal solution set X , and the associated sequence of objective values {F (X k )}k≥0 converges Q–linearly to the optimal value v ∗ . [sent-150, score-0.386]
</p><p>66 F κ1 4  It follows that  κ4 (F (X k ) − v ∗ ), (12) κ1 + κ4 which establishes the Q–linear convergence of {F (X k )}k≥0 to v ∗ . [sent-157, score-0.136]
</p><p>67 Using (9) and (12), we can show that { X k+1 − X k 2 }k≥0 converges R–linearly to 0, which, together with (11), implies that F {X k }k≥0 converges R–linearly to a point in X ; see the supplementary material. [sent-158, score-0.126]
</p><p>68 However, as explained in Section 1, some new ingredients are needed in order to analyze the spectral properties of a point that is close to the optimal solution set X . [sent-161, score-0.141]
</p><p>69 Given a sequence {X k }k≥0 ∈ Rm×n \X , deﬁne ¯ ¯ Rk = R(X k ), X k = arg minY ∈X X k − Y F , δk = X k − X k F , k k k k ∗ k ¯ = A∗ (∇h(¯)), z = A(X ), G = ∇f (X ) = A (∇h(z )), G z  (13)  ∗  where A is the adjoint operator of A. [sent-164, score-0.138]
</p><p>70 (b) (Bounded Iterates) There exists a convex compact subset Z of dom(h) such that z k , z ∈ Z ¯ for all k ≥ 0. [sent-170, score-0.184]
</p><p>71 (16)  ¯ ¯ It is clear that A(X k − X k ) 2 ≤ A · X k − X k F , where A = sup Y F =1 A(Y ) 2 is the spectral norm of A. [sent-173, score-0.227]
</p><p>72 2), by passing to a subsequence if ¯ necessary, we may assume that {X k }k≥0 converges to some X. [sent-185, score-0.177]
</p><p>73 In particular, this implies that σ ¯ ¯ X k − X k 2 ≤ (L A 2 + 1) X k − X k F Rk F F κ2 ¯ for all k ≥ 0, which, upon dividing both sides by X k − X k F , yields a contradiction to (14). [sent-196, score-0.112]
</p><p>74 Hence, the sequence {A(X k )}k≥0 is also bounded, which implies ¯ that the points z k = A(X k ) and z = A(X) lie in a convex compact subset Z of dom(h) for all ¯ k ≥ 0. [sent-204, score-0.27]
</p><p>75 Then, by further passing to a subsequence if necessary, we may assume that ¯ (20) A(X k ) − z 2 X k − X k F → 0. [sent-210, score-0.136]
</p><p>76 ¯ In the sequel, we will also assume without loss of generality that m ≤ n. [sent-211, score-0.085]
</p><p>77 The following proposition establishes a property of the optimal solution set X that will play a crucial role in our proof. [sent-212, score-0.266]
</p><p>78 Let X − G = U [Diag(¯ ) 0] V T be the singular ¯ ¯ ¯ ¯ value decomposition of X − G, where U ∈ Rm×m , V ∈ Rn×n are orthogonal matrices and σ ¯ ¯ ¯ ¯ ¯ is the vector of singular values of X − G. [sent-215, score-0.261]
</p><p>79 Moreover, the set Xc ⊂ X , which is deﬁned as ¯ ¯ Xc = X ∈ X : X = U [Diag(σ(X)) 0] V T , is a non–empty convex compact set. [sent-217, score-0.184]
</p><p>80 By further passing to a subsequence if necessary, ¯ ¯ ¯ we will assume that {Qk }k≥0 converges to some Q. [sent-225, score-0.177]
</p><p>81 To begin, let σ k be the vector of singular values of k k ¯ ¯ X − G . [sent-231, score-0.099]
</p><p>82 , m, k by passing to a subsequence if necessary, we can classify the sequence {σi }k≥0 into one of the k k ˜ k ) > 0 for all k ≥ 0; (C) following three cases: (A) σi ≤ τ for all k ≥ 0; (B) σi > τ and σi (X k ˜ σi > τ and σi (X k ) = 0 for all k ≥ 0. [sent-236, score-0.178]
</p><p>83 The following proposition gives the key structural properties ¯ of Q that will lead to the desired contradiction: 6  ¯ ¯ ¯ ¯ Proposition 4. [sent-237, score-0.13]
</p><p>84 3 The matrix Q admits the decomposition Q = U [Diag(λ) 0] V T , where  ˜k  = − lim σi (X ) ≤ 0 in Case (A),    k→∞ γk λi for i = 1, . [sent-238, score-0.091]
</p><p>85 3 is not necessarily the singular ¯ value decomposition of Q, as λ could have negative components. [sent-243, score-0.133]
</p><p>86 7  5 Numerical Experiments In this section, we complement our theoretical results by testing the numerical performance of the PGM (5) on two problems: matrix completion and matrix classiﬁcation. [sent-316, score-0.193]
</p><p>87 Matrix Completion: We randomly generate an n × n matrix M with a prescribed rank r. [sent-317, score-0.159]
</p><p>88 This induces a sampling operator P : Rm×n → Rp and an observation vector b ∈ Rp . [sent-319, score-0.096]
</p><p>89 In our experiments, we ﬁx the rank r = 3 and use the square loss f (·) = P(·) − b 2 /2 with regularization parameter 2 µ = 1 in problem (1). [sent-320, score-0.286]
</p><p>90 Figure 1 shows the semi–log plots of the error in objective value and the error in solution against the number of iterations. [sent-323, score-0.204]
</p><p>91 It can be seen that as long as the iterates are close enough to the optimal set, both the objective values and the solutions converge linearly. [sent-324, score-0.149]
</p><p>92 3  Convergence Performance of Solution  4  150  Convergence Performance of Objective Value  5  10  Iterations  Log(Error of Solution)  10  Convergence Performance of Objective Value  5  Log(error of objective value)  Log(error of objective value)  θ=0. [sent-336, score-0.098]
</p><p>93 Speciﬁcally, we ﬁrst randomly generate a low-rank matrix classiﬁer X ∗ , which is an n × n symmetric matrix of rank r. [sent-340, score-0.216]
</p><p>94 In our experiments, p we ﬁx the rank r = 3, the dimension n = 40, and use the logistic loss f (·) = i=1 log(1 + exp(−yi ·, Wi )) with regularization parameter µ = 1 in problem (1). [sent-346, score-0.241]
</p><p>95 Figure 2 shows the convergence performance of the PGM (5) as θ varies. [sent-351, score-0.101]
</p><p>96 Again, it can be seen that both the objective values and the solutions converge linearly. [sent-352, score-0.079]
</p><p>97 6 Conclusion In this paper, we have established the linear convergence of the PGM for solving a class of trace norm–regularized problems. [sent-353, score-0.377]
</p><p>98 Our convergence result does not require the objective function to be strongly convex and is applicable to many settings in machine learning. [sent-354, score-0.33]
</p><p>99 The key technical tool in the proof is a Lipschitzian error bound for trace norm–regularized problems, which could be of independent interest. [sent-355, score-0.338]
</p><p>100 A future direction is to study error bounds for more general matrix norm– regularized problems and their implications on the convergence rates of ﬁrst–order methods. [sent-356, score-0.279]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pgm', 0.487), ('xc', 0.278), ('trace', 0.216), ('rk', 0.209), ('norm', 0.187), ('rm', 0.186), ('rp', 0.17), ('dom', 0.149), ('prox', 0.144), ('convex', 0.138), ('proposition', 0.13), ('gk', 0.127), ('lipschitzian', 0.125), ('dist', 0.125), ('non', 0.116), ('proximal', 0.105), ('rank', 0.102), ('convergence', 0.101), ('lipschitz', 0.101), ('multi', 0.099), ('singular', 0.099), ('operator', 0.096), ('subsequence', 0.089), ('loss', 0.085), ('qk', 0.085), ('luo', 0.082), ('completion', 0.079), ('zt', 0.077), ('tseng', 0.075), ('regularized', 0.074), ('xk', 0.07), ('yt', 0.067), ('lf', 0.062), ('solution', 0.061), ('epigraph', 0.061), ('invertibility', 0.061), ('gradient', 0.059), ('matrix', 0.057), ('diag', 0.057), ('regularization', 0.054), ('nuclear', 0.054), ('lemma', 0.054), ('residual', 0.053), ('continuity', 0.052), ('proximity', 0.052), ('shrinkage', 0.052), ('wk', 0.051), ('objective', 0.049), ('passing', 0.047), ('error', 0.047), ('compact', 0.046), ('satis', 0.046), ('princeton', 0.045), ('square', 0.045), ('implies', 0.044), ('linearly', 0.042), ('moreover', 0.042), ('sequence', 0.042), ('strongly', 0.042), ('polyhedral', 0.041), ('converges', 0.041), ('optimal', 0.04), ('spectral', 0.04), ('tomioka', 0.04), ('optimality', 0.039), ('contradiction', 0.038), ('proof', 0.038), ('armed', 0.037), ('kong', 0.037), ('claim', 0.037), ('bound', 0.037), ('th', 0.036), ('fazel', 0.035), ('establishes', 0.035), ('convexity', 0.035), ('nonsmooth', 0.035), ('decomposition', 0.034), ('iterations', 0.033), ('hong', 0.033), ('sublinear', 0.032), ('lectures', 0.032), ('classi', 0.032), ('onto', 0.032), ('solving', 0.031), ('concerning', 0.031), ('solutions', 0.03), ('sides', 0.03), ('aforementioned', 0.03), ('theorem', 0.03), ('iterates', 0.03), ('establish', 0.029), ('assumptions', 0.029), ('projection', 0.029), ('matrices', 0.029), ('established', 0.029), ('exist', 0.028), ('composite', 0.028), ('continuously', 0.028), ('wi', 0.027), ('rate', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="222-tfidf-1" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>2 0.20849305 <a title="222-tfidf-2" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>Author: Bernardino Romera-Paredes, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves signiﬁcantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. 1</p><p>3 0.1984849 <a title="222-tfidf-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.17586575 <a title="222-tfidf-4" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar</p><p>Abstract: We provide a uniﬁed framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M -estimators that minimize the sum of any loss function, and an instance of what we call a “hybrid” regularization, that is the inﬁmal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures. 1</p><p>5 0.16299047 <a title="222-tfidf-5" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>6 0.12481964 <a title="222-tfidf-6" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>7 0.123478 <a title="222-tfidf-7" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>8 0.12027103 <a title="222-tfidf-8" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>9 0.1178127 <a title="222-tfidf-9" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>10 0.11468464 <a title="222-tfidf-10" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>11 0.11192288 <a title="222-tfidf-11" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>12 0.1107255 <a title="222-tfidf-12" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>13 0.10410114 <a title="222-tfidf-13" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>14 0.10165843 <a title="222-tfidf-14" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>15 0.1010721 <a title="222-tfidf-15" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>16 0.1002944 <a title="222-tfidf-16" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>17 0.10009585 <a title="222-tfidf-17" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>18 0.098602086 <a title="222-tfidf-18" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>19 0.094959982 <a title="222-tfidf-19" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>20 0.094730705 <a title="222-tfidf-20" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.246), (1, 0.062), (2, 0.189), (3, 0.169), (4, -0.022), (5, -0.023), (6, -0.149), (7, 0.025), (8, 0.016), (9, 0.024), (10, 0.043), (11, 0.066), (12, 0.01), (13, -0.075), (14, -0.058), (15, -0.022), (16, -0.029), (17, 0.055), (18, 0.01), (19, 0.034), (20, 0.02), (21, 0.036), (22, 0.105), (23, 0.037), (24, -0.017), (25, -0.035), (26, 0.03), (27, -0.133), (28, 0.071), (29, 0.146), (30, -0.131), (31, -0.003), (32, -0.055), (33, -0.11), (34, -0.047), (35, -0.018), (36, 0.031), (37, 0.051), (38, -0.012), (39, -0.001), (40, 0.007), (41, 0.06), (42, -0.049), (43, -0.014), (44, 0.038), (45, -0.014), (46, 0.05), (47, -0.051), (48, 0.032), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95578408 <a title="222-lsi-1" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>2 0.77736163 <a title="222-lsi-2" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>3 0.77090508 <a title="222-lsi-3" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>4 0.69305837 <a title="222-lsi-4" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>5 0.66878682 <a title="222-lsi-5" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar</p><p>Abstract: We provide a uniﬁed framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M -estimators that minimize the sum of any loss function, and an instance of what we call a “hybrid” regularization, that is the inﬁmal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures. 1</p><p>6 0.66480166 <a title="222-lsi-6" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>7 0.6467948 <a title="222-lsi-7" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>8 0.6407544 <a title="222-lsi-8" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>9 0.62459731 <a title="222-lsi-9" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>10 0.6180985 <a title="222-lsi-10" href="./nips-2013-On_Algorithms_for_Sparse_Multi-factor_NMF.html">214 nips-2013-On Algorithms for Sparse Multi-factor NMF</a></p>
<p>11 0.60117185 <a title="222-lsi-11" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>12 0.58474976 <a title="222-lsi-12" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>13 0.57842606 <a title="222-lsi-13" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>14 0.57728529 <a title="222-lsi-14" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>15 0.55917335 <a title="222-lsi-15" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>16 0.55229449 <a title="222-lsi-16" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>17 0.55015969 <a title="222-lsi-17" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>18 0.54241067 <a title="222-lsi-18" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>19 0.53901386 <a title="222-lsi-19" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>20 0.53243744 <a title="222-lsi-20" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.016), (33, 0.584), (34, 0.054), (41, 0.042), (49, 0.016), (56, 0.104), (70, 0.027), (85, 0.023), (89, 0.028), (93, 0.024), (95, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99410254 <a title="222-lda-1" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>Author: Liming Wang, David Carlson, Miguel Rodrigues, David Wilcox, Robert Calderbank, Lawrence Carin</p><p>Abstract: We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate, X ∈ Rn , and the + observed data are a vector of counts, Y ∈ Zm . The projection matrix is designed + by maximizing mutual information between Y and X, I(Y ; X). When there is a latent class label C ∈ {1, . . . , L} associated with X, we consider the mutual information with respect to Y and C, I(Y ; C). New analytic expressions for the gradient of I(Y ; X) and I(Y ; C) are presented, with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classiﬁcation (photon counting). 1</p><p>2 0.99232143 <a title="222-lda-2" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu</p><p>Abstract: Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its inﬁnite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution. While this model can accommodate a wider range of conditional dependencies, some limitations still remain. To address this, we investigate two additional novel variants of the Poisson distribution and their corresponding joint graphical model distributions. Our three novel approaches provide classes of Poisson-like graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our models via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-sequencing data. 1</p><p>3 0.99146438 <a title="222-lda-3" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vamsi Ithapu, Qinyuan Sun, Sterling C. Johnson, Vikas Singh</p><p>Abstract: Multiple hypothesis testing is a signiﬁcant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given α-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub–sampled — on the order of 0.5% — matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacriﬁcing the ﬁdelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly 50× can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated α-threshold is also recovered faithfully, and is stable. 1</p><p>4 0.9892934 <a title="222-lda-4" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>Author: Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, Tomas Mikolov</p><p>Abstract: Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model. 1</p><p>5 0.98276114 <a title="222-lda-5" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>Author: Yichuan Tang, Ruslan Salakhutdinov</p><p>Abstract: Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classiﬁcation tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are not efﬁcient and unsuitable for modeling real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables. A new Generalized EM training procedure using importance sampling allows us to efﬁciently learn complicated conditional distributions. Our model achieves superior performance on synthetic and facial expressions datasets compared to conditional Restricted Boltzmann Machines and Mixture Density Networks. In addition, the latent features of our model improves classiﬁcation and can learn to generate colorful textures of objects. 1</p><p>6 0.9781456 <a title="222-lda-6" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>7 0.97806883 <a title="222-lda-7" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<p>8 0.97447014 <a title="222-lda-8" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>same-paper 9 0.97402853 <a title="222-lda-9" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>10 0.94110864 <a title="222-lda-10" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>11 0.92588693 <a title="222-lda-11" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>12 0.9089058 <a title="222-lda-12" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>13 0.90826428 <a title="222-lda-13" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>14 0.90538383 <a title="222-lda-14" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>15 0.90334076 <a title="222-lda-15" href="./nips-2013-Accelerating_Stochastic_Gradient_Descent_using_Predictive_Variance_Reduction.html">20 nips-2013-Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</a></p>
<p>16 0.90087974 <a title="222-lda-16" href="./nips-2013-Conditional_Random_Fields_via_Univariate_Exponential_Families.html">67 nips-2013-Conditional Random Fields via Univariate Exponential Families</a></p>
<p>17 0.90053123 <a title="222-lda-17" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>18 0.89875489 <a title="222-lda-18" href="./nips-2013-Stochastic_Ratio_Matching_of_RBMs_for_Sparse_High-Dimensional_Inputs.html">315 nips-2013-Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs</a></p>
<p>19 0.89552617 <a title="222-lda-19" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>20 0.8939209 <a title="222-lda-20" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
