<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-52" href="#">nips2013-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</h1>
<br/><p>Source: <a title="nips-2013-52-pdf" href="http://papers.nips.cc/paper/4886-bayesian-inference-as-iterated-random-functions-with-applications-to-sequential-inference-in-graphical-models.pdf">pdf</a></p><p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>Reference: <a title="nips-2013-52-reference" href="../nips2013_reference/nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bayesian inference as iterated random functions with applications to sequential inference in graphical models XuanLong Nguyen Department of Statistics University of Michigan Ann Arbor, Michigan 48109 xuanlong@umich. [sent-1, score-0.588]
</p><p>2 edu  Abstract We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. [sent-4, score-0.69]
</p><p>3 A convergence theory for iterated random functions is presented. [sent-5, score-0.305]
</p><p>4 As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. [sent-6, score-0.754]
</p><p>5 The sequential inference algorithm and its supporting theory are illustrated by simulated examples. [sent-7, score-0.161]
</p><p>6 1 Introduction The sequential posterior updates play a central role in many Bayesian inference procedures. [sent-8, score-0.317]
</p><p>7 As an example, in Bayesian inference one is interested in the posterior probability of variables of interest given the data observed sequentially up to a given time point. [sent-9, score-0.205]
</p><p>8 As a more speciﬁc example which provides the motivation for this work, in a sequential change point detection problem [1], the key quantity is the posterior probability that a change has occurred given the data observed up to present time. [sent-10, score-0.726]
</p><p>9 , a large-scale graphical model, the calculation of such quantities in a fast and online manner is a formidable challenge. [sent-13, score-0.125]
</p><p>10 In such situations approximate inference methods are required – for graphical models, message-passing variational inference algorithms present a viable option [2, 3]. [sent-14, score-0.344]
</p><p>11 In this paper we propose to treat Bayesian inference in a complex model as a speciﬁc instance of an abstract system of iterated random functions (IRF), a concept that originally arises in the study of Markov chains and stochastic systems [4]. [sent-15, score-0.302]
</p><p>12 The key technical property of the proposed IRF formalism that enables the connection to Bayesian inference under conditionally independent sampling is the semigroup property, which shall be deﬁned shortly in the sequel. [sent-16, score-0.283]
</p><p>13 It turns out that most exact and approximate Bayesian inference algorithms may be viewed as speciﬁc instances of an IRF system. [sent-17, score-0.213]
</p><p>14 The goal of this paper is to present a general convergence theory for the IRF with semigroup property. [sent-18, score-0.2]
</p><p>15 The theory is then applied to the analysis of exact and approximate message-passing inference algorithms, which arise in the context of distributed sequential change point problems using latent variable and directed graphical model as the underlying modeling framework. [sent-19, score-0.689]
</p><p>16 We wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8]. [sent-20, score-0.286]
</p><p>17 On the other hand, convergence and error analysis of message-passing algorithms in graphical models is quite rare and challenging, especially for approximate algorithms, and they are typically conﬁned to the speciﬁc form of belief propagation (sum-product) algorithm [9, 10, 11]. [sent-21, score-0.328]
</p><p>18 To the best of our knowledge, there is no existing work on the analysis of messagepassing inference algorithms for calculating conditional (posterior) probabilities for latent random 1  variables present in a graphical model. [sent-22, score-0.267]
</p><p>19 While such an analysis is a byproduct of this work, the viewpoint we put forward here that equates Bayesian posterior updates to a system of iterated random functions with semigroup property seems to be new and may be of general interest. [sent-23, score-0.539]
</p><p>20 As an example of the application of the result, we will provide a convergence analysis for an approximate sequential inference algorithm for the problem of multiple change point detection using graphical models. [sent-27, score-0.689]
</p><p>21 2 Bayesian posterior updates as iterated random functions In this paper we shall restrict ourselves to multivariate distributions of binary random variables. [sent-29, score-0.382]
</p><p>22 The iteration under consideration recursively produces a random sequence of elements of d Pd , starting from some initial value. [sent-31, score-0.132]
</p><p>23 We think of Pd as a subset of R2 equipped with the ℓ1 norm (that is, the total variation norm for discrete probability measures). [sent-32, score-0.106]
</p><p>24 For θ ∈ Rm , consider the function + qθ : Pd → Pd , deﬁned by qθ (x) :=  x⊙ θ xT θ  (1)  i i m and x ⊙ θ is pointwise multiplication where xT θ = i x θ is the usual inner product on R i i i with coordinates [x ⊙ θ] := x θ , for i = 0, 1, . [sent-37, score-0.133]
</p><p>25 One can think of θ as the likelihood and x as the prior distribution (or the posterior in the previous stage) and qθ (x) as the (new) posterior based on the two. [sent-42, score-0.258]
</p><p>26 Our goal is to ﬁnd sufﬁcient conditions on T and {θn } for the convergence of the iteration to an extreme point of Pd , which without loss of generality is taken to be e(0) := (1, 0, 0, . [sent-50, score-0.15]
</p><p>27 Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn (T (·)). [sent-54, score-0.374]
</p><p>28 If T is the identity, this property allows us to write Qn (x) = q⊙ n θi (x) — this is nothing but the Bayesian posterior update equation, under condii=1 tionally independent sampling, while modifying T results in an approximate Bayesian inference n procedure. [sent-58, score-0.334]
</p><p>29 2  3 General convergence theory Consider a sequence {θn }n≥1 ⊂ Rm of i. [sent-64, score-0.142]
</p><p>30 (4)  0 The normalization θn = 1 is convenient for showing convergence to e(0) . [sent-75, score-0.106]
</p><p>31 The sub-Gaussian norm in can be taken to be the ψ2 Orlicz norm (cf. [sent-81, score-0.106]
</p><p>32 Consider the sequence {Qn (x)}n≥0 deﬁned in (2) based on {θn } as above, an initial point x = (x0 , . [sent-86, score-0.101]
</p><p>33 Our main application of the theorem will be to the study of convergence of stopping rules for a distributed multiple change point problem endowed with latent variable graphical models. [sent-96, score-0.589]
</p><p>34 Before stating that problem, let us consider the classical (single) change point problem ﬁrst, and show how the theorem can be applied to analyze the convergence of the optimal Bayes rule. [sent-97, score-0.387]
</p><p>35 In the classical Bayesian change point problem [1], one observes a sequence {X 1 , X 2 , X 3 . [sent-99, score-0.329]
</p><p>36 } of independent data points whose distributions change at some random time λ. [sent-102, score-0.204]
</p><p>37 The goal is to ﬁnd a stopping rule τ which can predict λ based on the data points observed so far. [sent-112, score-0.185]
</p><p>38 It is well-known that a rule based on thresholding the posterior probability of λ is optimal (in a Neyman-Pearson sense). [sent-113, score-0.188]
</p><p>39 , X n ) collect the data up to time n and let γ n [n] := P(λ ≤ n|Xn ) be the posterior probability of λ having occurred before (or at) time n. [sent-117, score-0.226]
</p><p>40 Then, the Shiryayev rule τ := inf{n ∈ N : γ n [n] ≥ 1 − α}  (6)  is known to asymptotically have the least expected delay, among all stopping rules with false alarm probability bounded by α. [sent-118, score-0.237]
</p><p>41 Theorem 1 provides a way to quantify how fast the posterior γ n [n] approaches 1, once the change point has occurred, hence providing an estimate of the detection delay, even for ﬁnite number of samples. [sent-119, score-0.386]
</p><p>42 We should note that our approach here is somewhat independent of the classical techniques normally used for analyzing stopping rule (6). [sent-120, score-0.182]
</p><p>43 Since γ n [n] = P(Z = 1|Xn ), convergence of γ n [n] to 1 is equivalent to the convergence of Qn to e(0) = (1, 0). [sent-123, score-0.158]
</p><p>44 Then, (7) implies that Qn can be obtained by pointwise multiplication of Rn−1 by f (X n )θn and normalization to make a probability vector. [sent-127, score-0.127]
</p><p>45 We can now use Theorem 1 to analyze the convergence of γ n [n]. [sent-141, score-0.109]
</p><p>46 Let us condition on λ = k + 1, that is, we assume that the change point has occurred at time k + 1. [sent-142, score-0.293]
</p><p>47 Then, the sequence {X n }n≥k+1 g ∗ is distributed according to f , and we have Eθn = f log f = −I, where I is the KL divergence between densities f and g. [sent-143, score-0.17]
</p><p>48 4 Multiple change point problem via latent variable graphical models We now turn to our main application for Theorem 1, in the context of a multiple change point problem. [sent-149, score-0.558]
</p><p>49 In [18], graphical model formalism is used to extend the classical change point problem (cf. [sent-150, score-0.414]
</p><p>50 Example 1) to cases where multiple distributed latent change points are present. [sent-151, score-0.274]
</p><p>51 One starts with a network G = (V, E) of d sensors or nodes, each associated with a change point λj . [sent-153, score-0.196]
</p><p>52 1 2 Each node j observes a private sequence of measurements Xj = (Xj , Xj , . [sent-154, score-0.125]
</p><p>53 ) which undergoes a change in distribution at time λj , that is, iid  k−1 1 2 Xj , Xj , . [sent-157, score-0.253]
</p><p>54 , Xj | λj = k ∼ gj ,  iid  k+1 k Xj , Xj , · · · | λj = k ∼ fj ,  for densities gj and fj (w. [sent-160, score-0.238]
</p><p>55 Each connected pair of nodes share an additional sequence of measurements. [sent-164, score-0.104]
</p><p>56 The shared sequence undergoes a change in distribution at some point depending on λs1 and λs2 . [sent-169, score-0.332]
</p><p>57 More speciﬁcally, it is assumed that the earlier of the two change points causes a change in the shared sequence, that is, the distribution of Xe conditioned on (λs1 , λs2 ) only depends on λe := λs1 ∧ λs2 , the minimum of the two, i. [sent-170, score-0.424]
</p><p>58 , Xe | λe = k ∼ ge ,  k+1 k+2 Xe , Xe , · · · | λe = k ∼ fe . [sent-175, score-0.232]
</p><p>59 Network G induces a graphical model [2] which encodes the factorization (9) of the joint density. [sent-178, score-0.125]
</p><p>60 1) Suppose now that each node j wants to detect its change point λj , with minimum expected delay, while maintaining a false alarm probability at most α. [sent-181, score-0.294]
</p><p>61 Inspired by the classical change point problem, one is interested in computing the posterior probability that the change point has occurred up to now, that is, n γj [n] := P(λj ≤ n | Xn ). [sent-182, score-0.661]
</p><p>62 It is easy to verify that the natural stopping rule n τj = inf{n ∈ N : γj [n] ≥ 1 − α}  satisfy the false alarm constraint. [sent-184, score-0.237]
</p><p>63 It has also been shown that this rule is asymptotically optimal in terms of expected detection delay. [sent-185, score-0.12]
</p><p>64 Moreover, an algorithm based on the well-known sum-product [2] has been proposed, which allows the nodes to compute their posterior probabilities 10 by messagepassing. [sent-186, score-0.195]
</p><p>65 The drawback is the linear dependence on n, which makes the algorithm practically infeasible if the change points model rare events (where n could grow large before detecting the change. [sent-189, score-0.261]
</p><p>66 ) In the next section, we propose an approximate message passing algorithm which has computational complexity O(d), at each time step. [sent-190, score-0.114]
</p><p>67 We then show how the theory developed in Section 3 can be used to provide convergence guarantees for this approximate algorithm, as well as the exact one. [sent-192, score-0.216]
</p><p>68 1 Fast approximate message-passing (MP) We now turn to an approximate message-passing algorithm which, at each time step, has computational complexity O(d). [sent-194, score-0.134]
</p><p>69 In practice, however, the exponential complexity makes the exact recursion of little use for large networks. [sent-213, score-0.129]
</p><p>70 Inserting this RHS n n−1 n into (12) in place of P (Z∗ |X∗ ), we obtain a graphical model in variables Z∗ (instead of λ∗ ) n−1 n which has the same form as (9) with ν(Zj ; γj [n]) playing the role of the prior π(λj ). [sent-220, score-0.125]
</p><p>71 In order to make a meaningful comparison, n we also look at the algorithm which computes the exact sequence {P (Z∗ |Xn )}n∈N , recursively. [sent-228, score-0.133]
</p><p>72 ∗ ∗ To make this correspondence formal and the notation simpliﬁed, we use the symbol :≡ as follows n n yn :≡ P (Z∗ |Xn ), yn :≡ P (Z∗ |Xn ) (16) ∗ ∗ where now yn , yn ∈ Pd . [sent-233, score-0.56]
</p><p>73 Note that yn and yn are random elements of Pd , due the randomness of Xn . [sent-234, score-0.316]
</p><p>74 The exact and approximate sequences, {yn } and {yn }, follow general iteration (2) with the same random sequence {θn }, but with different deterministic operators T , denoted respectively with Tex and Tap . [sent-237, score-0.233]
</p><p>75 As suggested by Theorem 1, a key assumption for the convergence of the approximate algorithm will be Kρ ≤ 1. [sent-242, score-0.146]
</p><p>76 Recall that {λj } are the change points and their priors are geometric with parameters {ρj }. [sent-244, score-0.238]
</p><p>77 We analyze the algorithms, once all the change points have happened. [sent-245, score-0.234]
</p><p>78 Then, one expects the (joint) posterior of Z∗ to ∞ contract to the point Zj = 1, for all j ∈ V . [sent-247, score-0.167]
</p><p>79 Theorem 2 below quantiﬁes this convergence in ℓ1 norm (equivalently, total variation for measures). [sent-249, score-0.132]
</p><p>80 Recall pre-change and post-change densities ge and fe , and let Ie denote their KL divergence, that is, Ie := fe log(fe /ge ). [sent-250, score-0.444]
</p><p>81 We will assume that Ye := log(ge (X)/fe (X)) with X ∼ fe (18) is sub-Gaussian, for all e ∈ E, where E is extended edge notation introduced in Section 4. [sent-251, score-0.161]
</p><p>82 The choice X ∼ fe is in accordance with conditioning on Mn0 . [sent-253, score-0.161]
</p><p>83 ) 6  X23  λ3  X23 λ3 λ3 X24 X45  λ1  λ1 λ2  λ2 λ4  X12  λ5  λ4  mn 12 λ2  λ1  mn 32 X24 X45 mn 24  X12  λ5  1  1  1  0. [sent-260, score-0.153]
</p><p>84 2  MP APPROX 10  20  30  40  50  60  70  0  MP APPROX 10  20  30  40  50  60  70  Figure 1: Top row illustrates a network (left), which induces a graphical model (middle). [sent-276, score-0.164]
</p><p>85 Right panel n illustrates one stage of message-passing to compute posterior probabilities γj [n]. [sent-277, score-0.193]
</p><p>86 Bottom row illustrates typical n examples of posterior paths, n → γj [n], obtained by EXACT and approximate (APPROX) message passing, for the subgraph on nodes {1, 2, 3, 4}. [sent-278, score-0.354]
</p><p>87 The change points are designated with vertical dashed lines. [sent-279, score-0.204]
</p><p>88 There exists an absolute constant κ > 0, such that if I∗ (κ) > 0, the exact algorithm converges at least geometrically w. [sent-281, score-0.136]
</p><p>89 If in addition, Kρ ≤ 1, the approximate algorithm also converges at least geometrically w. [sent-285, score-0.133]
</p><p>90 3 Simulation results We present some simulation results to verify the effectiveness of the proposed approximation aln gorithm in estimating the posterior probabilities γj [n]. [sent-292, score-0.154]
</p><p>91 Conditioned on the change points λ∗ , all data sequences X∗ are assumed Gaussian with variance 1, pre-change mean 1 and post-change mean zero. [sent-296, score-0.204]
</p><p>92 1 illustrates n typical examples of posterior paths n → γj [n], for both the exact and approximate MP algorithms. [sent-301, score-0.305]
</p><p>93 One can observe that the approximate path often closely follows the exact one. [sent-302, score-0.137]
</p><p>94 In some cases, they might deviate for a while, but as suggested by Theorem 2, they approach one another quickly, once the change points have occurred. [sent-303, score-0.204]
</p><p>95 From the theorem and triangle inequality, it follows that under I∗ (κ) > 0 and Kρ ≤ 1, yn − yn converges to zero, at least geometrically w. [sent-304, score-0.385]
</p><p>96 Distributed fusion in sensor networks: A graphical models perspective. [sent-381, score-0.125]
</p><p>97 Message-passing sequential detection of multiple change points in networks. [sent-388, score-0.35]
</p><p>98 A graphical model representation of the track-oriented multiple hypothesis tracker. [sent-394, score-0.125]
</p><p>99 Bayesian inference as iterated random functions with applications to sequential inference in graphical models. [sent-461, score-0.588]
</p><p>100 Sequential detection of multiple change points in networks: a graphical model approach. [sent-467, score-0.39]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qn', 0.459), ('pd', 0.374), ('iterated', 0.226), ('xe', 0.207), ('xn', 0.206), ('zj', 0.177), ('fe', 0.161), ('change', 0.158), ('irf', 0.151), ('yn', 0.14), ('posterior', 0.129), ('graphical', 0.125), ('ie', 0.121), ('semigroup', 0.121), ('mp', 0.114), ('occurred', 0.097), ('lipt', 0.09), ('xj', 0.089), ('amini', 0.088), ('approx', 0.087), ('sequential', 0.085), ('tex', 0.08), ('stopping', 0.08), ('convergence', 0.079), ('rm', 0.077), ('inference', 0.076), ('lipschitz', 0.074), ('tap', 0.074), ('ge', 0.071), ('exact', 0.07), ('michigan', 0.07), ('approximate', 0.067), ('geometrically', 0.066), ('sequence', 0.063), ('alarm', 0.063), ('bayesian', 0.061), ('detection', 0.061), ('pointwise', 0.061), ('imin', 0.06), ('rule', 0.059), ('recursion', 0.059), ('norm', 0.053), ('arbor', 0.053), ('xuanlong', 0.053), ('densities', 0.051), ('mn', 0.051), ('formalism', 0.05), ('iid', 0.049), ('delay', 0.049), ('recall', 0.048), ('message', 0.047), ('points', 0.046), ('undergoes', 0.046), ('xij', 0.045), ('ann', 0.044), ('contractive', 0.044), ('classical', 0.043), ('xm', 0.041), ('nodes', 0.041), ('latent', 0.041), ('theorem', 0.039), ('multiplication', 0.039), ('rhs', 0.039), ('illustrates', 0.039), ('point', 0.038), ('rn', 0.038), ('ihler', 0.038), ('ye', 0.037), ('elements', 0.036), ('property', 0.036), ('nguyen', 0.036), ('gj', 0.036), ('false', 0.035), ('private', 0.035), ('conditioned', 0.035), ('bayes', 0.034), ('geometric', 0.034), ('coordinates', 0.033), ('fj', 0.033), ('inf', 0.033), ('iteration', 0.033), ('division', 0.032), ('operator', 0.032), ('subgraph', 0.031), ('analyze', 0.03), ('rare', 0.03), ('zi', 0.029), ('distributed', 0.029), ('drawback', 0.027), ('normalization', 0.027), ('log', 0.027), ('updates', 0.027), ('observes', 0.027), ('belief', 0.027), ('shared', 0.027), ('lemma', 0.026), ('write', 0.026), ('fisher', 0.026), ('july', 0.026), ('probabilities', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="52-tfidf-1" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>2 0.17180488 <a title="52-tfidf-2" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>3 0.105696 <a title="52-tfidf-3" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal, Ambuj Tewari</p><p>Abstract: The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. 1</p><p>4 0.10301715 <a title="52-tfidf-4" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>5 0.081511661 <a title="52-tfidf-5" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>6 0.080683425 <a title="52-tfidf-6" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>7 0.079742931 <a title="52-tfidf-7" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>8 0.07860516 <a title="52-tfidf-8" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>9 0.07597506 <a title="52-tfidf-9" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>10 0.074612394 <a title="52-tfidf-10" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>11 0.074084565 <a title="52-tfidf-11" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>12 0.071208611 <a title="52-tfidf-12" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>13 0.07110291 <a title="52-tfidf-13" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>14 0.068914257 <a title="52-tfidf-14" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>15 0.068110362 <a title="52-tfidf-15" href="./nips-2013-Online_Variational_Approximations_to_non-Exponential_Family_Change_Point_Models%3A_With_Application_to_Radar_Tracking.html">234 nips-2013-Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking</a></p>
<p>16 0.067711599 <a title="52-tfidf-16" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>17 0.066930763 <a title="52-tfidf-17" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>18 0.06520009 <a title="52-tfidf-18" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>19 0.065126665 <a title="52-tfidf-19" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>20 0.064183138 <a title="52-tfidf-20" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, 0.038), (2, 0.021), (3, 0.016), (4, 0.009), (5, 0.118), (6, 0.068), (7, -0.011), (8, 0.036), (9, 0.023), (10, 0.073), (11, 0.004), (12, -0.018), (13, 0.023), (14, -0.037), (15, -0.013), (16, -0.02), (17, -0.01), (18, -0.022), (19, 0.03), (20, -0.013), (21, 0.012), (22, 0.069), (23, 0.041), (24, -0.102), (25, 0.04), (26, -0.015), (27, 0.025), (28, 0.099), (29, 0.056), (30, -0.021), (31, -0.045), (32, -0.096), (33, 0.023), (34, 0.058), (35, -0.045), (36, 0.01), (37, 0.018), (38, 0.076), (39, 0.044), (40, 0.057), (41, -0.034), (42, -0.072), (43, -0.179), (44, 0.057), (45, 0.007), (46, -0.028), (47, -0.048), (48, 0.02), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9347195 <a title="52-lsi-1" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>2 0.7200377 <a title="52-lsi-2" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>3 0.6673221 <a title="52-lsi-3" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>Author: Tamir Hazan, Subhransu Maji, Joseph Keshet, Tommi Jaakkola</p><p>Abstract: In this work we develop efﬁcient methods for learning random MAP predictors for structured label problems. In particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods. We show that any smooth posterior distribution would sufﬁce to deﬁne a smooth PAC-Bayesian risk bound suitable for gradient methods. In addition, we relate the posterior distributions to computational properties of the MAP predictors. We suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized MAP predictors such as graph-cuts. We also describe label-augmented posterior models that can use efﬁcient MAP approximations, such as those arising from linear program relaxations. 1</p><p>4 0.63742948 <a title="52-lsi-4" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>Author: Nicolas Heess, Daniel Tarlow, John Winn</p><p>Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex nonGaussian factors, there is still a signiﬁcant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difﬁcult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising. 1</p><p>5 0.63180804 <a title="52-lsi-5" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>Author: Khaled Refaat, Arthur Choi, Adnan Darwiche</p><p>Abstract: EDML is a recently proposed algorithm for learning parameters in Bayesian networks. It was originally derived in terms of approximate inference on a metanetwork, which underlies the Bayesian approach to parameter estimation. While this initial derivation helped discover EDML in the ﬁrst place and provided a concrete context for identifying some of its properties (e.g., in contrast to EM), the formal setting was somewhat tedious in the number of concepts it drew on. In this paper, we propose a greatly simpliﬁed perspective on EDML, which casts it as a general approach to continuous optimization. The new perspective has several advantages. First, it makes immediate some results that were non-trivial to prove initially. Second, it facilitates the design of EDML algorithms for new graphical models, leading to a new algorithm for learning parameters in Markov networks. We derive this algorithm in this paper, and show, empirically, that it can sometimes learn estimates more efﬁciently from complete data, compared to commonly used optimization methods, such as conjugate gradient and L-BFGS. 1</p><p>6 0.61160076 <a title="52-lsi-6" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>7 0.58537394 <a title="52-lsi-7" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>8 0.58452815 <a title="52-lsi-8" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>9 0.58027756 <a title="52-lsi-9" href="./nips-2013-Online_Variational_Approximations_to_non-Exponential_Family_Change_Point_Models%3A_With_Application_to_Radar_Tracking.html">234 nips-2013-Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking</a></p>
<p>10 0.57687479 <a title="52-lsi-10" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>11 0.55274475 <a title="52-lsi-11" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>12 0.54115522 <a title="52-lsi-12" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>13 0.53696823 <a title="52-lsi-13" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>14 0.53096753 <a title="52-lsi-14" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>15 0.51817924 <a title="52-lsi-15" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>16 0.51798612 <a title="52-lsi-16" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>17 0.50819856 <a title="52-lsi-17" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>18 0.50706255 <a title="52-lsi-18" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>19 0.50641447 <a title="52-lsi-19" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>20 0.5041458 <a title="52-lsi-20" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.026), (16, 0.041), (33, 0.162), (34, 0.127), (36, 0.012), (41, 0.037), (49, 0.031), (56, 0.114), (70, 0.023), (85, 0.077), (89, 0.033), (93, 0.03), (95, 0.037), (96, 0.186)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89708579 <a title="52-lda-1" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>2 0.87478018 <a title="52-lda-2" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<p>Author: Ulrike Von Luxburg, Morteza Alamgir</p><p>Abstract: Consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d. from some unknown density p on Rd . We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or any distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate a local function of the gradient of p, and that integrating this function along shortest paths leads to an estimate of the underlying density. 1</p><p>same-paper 3 0.86107492 <a title="52-lda-3" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>4 0.80928963 <a title="52-lda-4" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin</p><p>Abstract: In this paper, we are interested in the development of efﬁcient algorithms for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the ﬁrst-order information. We cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds. We ﬁrst examine a two stages exploration-exploitation based algorithm which ﬁrst approximates the stochastic objectives by sampling and then solves a constrained stochastic optimization problem by projected gradient method. This method attains a suboptimal convergence rate even under strong assumption on the objectives. Our second approach is an efﬁcient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method √ conin strained optimization and attains the optimal convergence rate of O(1/ T ) in high probability for general Lipschitz continuous objectives.</p><p>5 0.79535741 <a title="52-lda-5" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>6 0.79035699 <a title="52-lda-6" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>7 0.78956461 <a title="52-lda-7" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>8 0.78942925 <a title="52-lda-8" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>9 0.78789866 <a title="52-lda-9" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>10 0.78726619 <a title="52-lda-10" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>11 0.78425312 <a title="52-lda-11" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>12 0.7840336 <a title="52-lda-12" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>13 0.78323406 <a title="52-lda-13" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>14 0.78293461 <a title="52-lda-14" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>15 0.78288883 <a title="52-lda-15" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>16 0.7817058 <a title="52-lda-16" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>17 0.7809611 <a title="52-lda-17" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>18 0.78082985 <a title="52-lda-18" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>19 0.78051811 <a title="52-lda-19" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>20 0.78034246 <a title="52-lda-20" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
