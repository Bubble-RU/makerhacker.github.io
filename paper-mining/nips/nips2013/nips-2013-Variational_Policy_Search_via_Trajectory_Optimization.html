<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>348 nips-2013-Variational Policy Search via Trajectory Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-348" href="#">nips2013-348</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>348 nips-2013-Variational Policy Search via Trajectory Optimization</h1>
<br/><p>Source: <a title="nips-2013-348-pdf" href="http://papers.nips.cc/paper/5178-variational-policy-search-via-trajectory-optimization.pdf">pdf</a></p><p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>Reference: <a title="nips-2013-348-reference" href="../nips2013_reference/nips-2013-Variational_Policy_Search_via_Trajectory_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. [sent-5, score-0.747]
</p><p>2 We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. [sent-7, score-0.913]
</p><p>3 A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. [sent-8, score-1.581]
</p><p>4 1  Introduction  Direct policy search methods have the potential to scale gracefully to complex, high-dimensional control tasks [12]. [sent-10, score-0.674]
</p><p>5 We propose to decouple policy optimization from exploration by using a variational decomposition of a maximum likelihood policy objective. [sent-13, score-1.389]
</p><p>6 Since direct model-based trajectory optimization is usually much easier than policy search, this method can discover low cost regions much more easily. [sent-15, score-0.884]
</p><p>7 Intuitively, the trajectory optimization “guides” the policy search toward regions of low cost. [sent-16, score-0.863]
</p><p>8 The trajectory optimization can be performed by a variant of the differential dynamic programming algorithm [4], and the policy is optimized with respect to a standard maximum likelihood objective. [sent-17, score-0.952]
</p><p>9 We show that this alternating optimization maximizes a well-deﬁned policy objective, and demonstrate experimentally that it can learn complex tasks in high-dimensional domains that are infeasible for methods that rely on random exploration. [sent-18, score-0.64]
</p><p>10 2  Preliminaries  In standard policy search, we seek to ﬁnd a distribution over actions ut in each state xt , denoted T πθ (ut |xt ), so as to minimize the sum of expected costs E[c(ζ)] = E[ t=1 c(xt , ut )], where ζ is a sequence of states and actions. [sent-20, score-1.692]
</p><p>11 The expectation is taken with respect to the system dynamics p(xt+1 |xt , ut ) and the policy πθ (ut |xt ), which is typically parameterized by a vector θ. [sent-21, score-1.046]
</p><p>12 ” 1  We follow prior work and deﬁne the probability of Ot as p(Ot = 1|xt , ut ) ∝ exp(−c(xt , ut )) [19]. [sent-23, score-0.778]
</p><p>13 Using the dynamics distribution p(xt+1 |xt , ut ) and the policy πθ (ut |xt ), we can deﬁne a dynamic Bayesian network that relates states, actions, and the optimality indicator. [sent-24, score-1.078]
</p><p>14 By setting Ot = 1 at all time steps and learning the maximum likelihood values for θ, we can perform policy optimization [20]. [sent-25, score-0.641]
</p><p>15 The corresponding optimization problem has the objective T  p(O|θ) = p(O|ζ)p(ζ|θ)dζ ∝ exp −  T  πθ (ut |xt )p(xt+1 |xt , ut )dζ. [sent-26, score-0.461]
</p><p>16 (1)  c(xt , ut ) p(x1 ) t=1  t=1  Although this objective differs from the classical minimum average cost objective, previous work showed that it is nonetheless useful for policy optimization and planning [20, 19]. [sent-27, score-1.092]
</p><p>17 This is the standard formulation for expectation maximization [9], and has been applied to policy optimization in previous work [8, 21, 3, 11]. [sent-31, score-0.636]
</p><p>18 However, prior policy optimization methods typically represent q(ζ) by sampling trajectories from the current policy πθ (ut |xt ) and reweighting them, for example by the exponential of their cost. [sent-32, score-1.257]
</p><p>19 We propose instead to directly optimize q(ζ) to minimize both its expected cost and its divergence from the current policy πθ (ut |xt ) when a model of the dynamics is available. [sent-34, score-0.812]
</p><p>20 We build off of a variant of DDP called iterative LQR, which linearizes the dynamics around the current trajectory, computes the optimal linear policy under linear-quadratic assumptions, executes this policy, and repeats the process around the new trajectory until convergence [17]. [sent-37, score-0.935]
</p><p>21 Because of the linear-quadratic assumptions, the value function is always quadratic, and the dynamics are Gaussian with the mean at f (xt , ut ) and noise . [sent-41, score-0.472]
</p><p>22 uut  The deterministic optimal policy is then given by ¯ ¯ g(xt ) = ut + kt + Kt (xt − xt ). [sent-47, score-1.399]
</p><p>23 ¯ ¯ By repeatedly computing the optimal policy around the current trajectory and updating xt and ut based on the new policy, iterative LQR converges to a locally optimal solution [17]. [sent-48, score-1.518]
</p><p>24 In order to use this algorithm to minimize the KL divergence in Equation 2, we introduce a modiﬁed cost function c(xt , ut ) = c(xt , ut ) − log πθ (ut |xt ). [sent-49, score-0.901]
</p><p>25 The optimal trajectory for this cost function approximately1 ¯ minimizes the KL divergence when q(ζ) is a Dirac delta function, since T  DKL (q(ζ) p(ζ|O, θ)) =  c(xt , ut ) − log πθ (ut |xt ) − log p(xt+1 |xt , ut ) dζ + const. [sent-50, score-1.117]
</p><p>26 The optimal policy πG under this framework minimizes an augmented cost function, given by c(xt , ut ) = c(xt , ut ) − H(πG ), ˜ ¯ where H(πG ) is the entropy of a stochastic policy πG (ut |xt ), and c(xt , ut ) includes log πθ (ut |xt ) ¯ as above. [sent-52, score-2.396]
</p><p>27 Ziebart [23] showed that the optimal policy can be written as πG (ut |xt ) = exp(−Qt (xt , ut ) + Vt (xt )), where V is a “softened” value function given by Vt (xt ) = log  exp (Qt (xt , ut )) dut . [sent-53, score-1.398]
</p><p>28 1 The minimization is not exact if the dynamics p(xt+1 |xt , ut ) are not deterministic, but the result is very close if the dynamics have much lower entropy than the policy and exponentiated cost, which is often the case. [sent-59, score-1.129]
</p><p>29 3  Algorithm 1 Variational Guided Policy Search 1: Initialize q(ζ) using DDP with cost c(xt , ut ) = α0 c(xt , ut ) ¯ 2: for iteration k = 1 to K do 3: Compute marginals (µ1 , Σt ), . [sent-60, score-0.903]
</p><p>30 In practice, the approximation is quite good when the dynamics and the cost c(xt , ut ) are smooth. [sent-64, score-0.529]
</p><p>31 Unfortunately, the policy term log πθ (ut |xt ) in the modiﬁed cost c(xt , ut ) can be quite jagged early on in the optimization, particularly for ¯ nonlinear policies. [sent-65, score-1.074]
</p><p>32 To mitigate this issue, we compute the derivatives of the policy not only along the current trajectory, but also at samples drawn from the current marginals q(xt ), and average them together. [sent-66, score-0.742]
</p><p>33 5  Variational Guided Policy Search  The variational guided policy search (variational GPS) algorithm alternates between minimizing the KL divergence in Equation 2 with respect to q(ζ) as described in the previous section, and maximizing the bound L(q, θ) with respect to the policy parameters θ. [sent-69, score-1.386]
</p><p>34 The gradient is given by T  L(q, θ) =  log πθ (ut |xt )dζ ≈  q(ζ) t=1  1 M  M  T  log πθ (ui |xi ), t t  (3)  i=1 t=1  where the samples (xi , ui ) are drawn from the marginals q(xt , ut ). [sent-73, score-0.517]
</p><p>35 When using SGD, new samt t ples can be drawn at every iteration, since sampling from q(xt , ut ) only requires the precomputed marginals from the preceding section. [sent-74, score-0.434]
</p><p>36 When choosing the policy class, it is common to use deterministic policies with additive Gaussian noise. [sent-79, score-0.652]
</p><p>37 In this case, we can optimize the policy more quickly and with many fewer samples by only sampling states and evaluating the integral over actions analytically. [sent-80, score-0.68]
</p><p>38 xt xt xt xt xi xt t 2 2  Two additional details should be taken into account in order to obtain the best results. [sent-82, score-1.45]
</p><p>39 First, although model-based trajectory optimization is more powerful than random exploration, complex tasks such as bipedal locomotion, which we address in the following section, are too difﬁcult to solve entirely with trajectory optimization. [sent-83, score-0.475]
</p><p>40 Since our method produces both a parameterized policy πθ (ut |xt ) and a DDP solution πG (ut |xt ), one might wonder why the DDP policy itself is not a suitable controller. [sent-93, score-1.168]
</p><p>41 This has three major advantages: ﬁrst, only the learned policy may be usable at runtime if the information available at runtime differs from the information during training, for example if the policy is trained in simulation and executed on a physical system with limited sensors. [sent-96, score-1.171]
</p><p>42 Second, if the policy class is chosen carefully, we might hope that the learned policy would generalize better than the DDP solution, as shown in previous work [10]. [sent-97, score-1.171]
</p><p>43 Third, multiple trajectories can be used to train a single policy from different initial states, creating a single controller that can succeed in a variety of situations. [sent-98, score-0.655]
</p><p>44 For both tasks, the policy sets joint torques on a simulated robot consisting of rigid links. [sent-100, score-0.593]
</p><p>45 Due to the high dimensionality and nonlinear dynamics, these tasks represent a signiﬁcant challenge for direct policy learning. [sent-103, score-0.628]
</p><p>46 The cost function for the walker was given by c(x, u) = wu u  2  + (vx − vx )2 + (py − py )2 ,  where vx and vx are the current and desired horizontal velocities, py and py are the current and desired heights of the hips, and the torque penalty was set to wu = 10−4 . [sent-104, score-0.446]
</p><p>47 Following previous work [10], the trajectory for the walker was initialized with a demonstration from a hand-crafted locomotion system [22]. [sent-107, score-0.348]
</p><p>48 The policy was represented by a neural network with one hidden layer and a soft rectifying nonlinearity of the form a = log(1 + exp(z)), with Gaussian noise at the output. [sent-108, score-0.602]
</p><p>49 Both the weights of the neural network and the diagonal covariance of the output noise were learned as part of the policy optimization. [sent-109, score-0.597]
</p><p>50 The number of policy parameters ranged from 63 for the 5-unit swimmer to 246 for the 10-unit walker. [sent-110, score-0.674]
</p><p>51 Due to its complexity and nonlinearity, this policy class presents a challenge to traditional policy search algorithms, which often focus on compact, linear policies [8]. [sent-111, score-1.261]
</p><p>52 Methods that sample from the current policy use 10 samples per iteration, unless noted otherwise. [sent-113, score-0.644]
</p><p>53 The cost was evaluated for both the actual stochastic policy (solid line), and a deterministic policy obtained by setting the variance of the Gaussian noise to zero (dashed line). [sent-115, score-1.225]
</p><p>54 The average cost of the stochastic policy is shown with a solid line, and the average cost of the deterministic policy without Gaussian noise is shown with a dashed line. [sent-121, score-1.282]
</p><p>55 The bottom-right panel shows plots of the swimmer and walker, with the center of mass trajectory under the learned policy shown in blue, and the initial DDP solution shown in black. [sent-122, score-0.94]
</p><p>56 The ﬁrst method we compare to is guided policy search (GPS), which uses importance sampling to introduce samples from the DDP solution into a likelihood ratio policy search [10]. [sent-123, score-1.411]
</p><p>57 Like our method, GPS uses DDP to explore regions of low cost, but the policy optimization is done using importance sampling, which can be susceptible to degenerate weights in high dimensions. [sent-125, score-0.636]
</p><p>58 Since standard GPS only samples from the initial DDP solution, these samples are only useful if they can be reproduced by the policy class. [sent-126, score-0.675]
</p><p>59 On the easier swimmer task, the GPS policy can reproduce the initial trajectory and succeeds immediately. [sent-128, score-0.897]
</p><p>60 However, GPS is unable to ﬁnd a successful walking policy with only 5 hidden units, which requires modiﬁcations to the initial trajectory. [sent-129, score-0.665]
</p><p>61 In addition, although the deterministic GPS policy performs well on the walker with 10 hidden units, the stochastic policy fails more often. [sent-130, score-1.286]
</p><p>62 However, samples from this adapted DDP solution are then included in the policy optimization with importance sampling, while our approach optimizes the variational bound L(q, θ). [sent-133, score-0.798]
</p><p>63 Because of this, although the adaptive variant of GPS improves on the standard variant, it is still unable to learn a walking policy with 5 hidden units, while our method quickly discovers an effective policy. [sent-136, score-0.667]
</p><p>64 DAGGER aims to learn a policy that imitates an oracle [14], which in our case is the DDP solution. [sent-138, score-0.596]
</p><p>65 At each iteration, DAGGER adds samples from the current policy to a dataset, and then optimizes the policy to take the oracle action at each dataset state. [sent-139, score-1.24]
</p><p>66 This variant succeeded on the swimming tasks and eventually found a good deterministic policy for the walker with 10 hidden units, though the learned stochastic policy performed very poorly. [sent-142, score-1.396]
</p><p>67 We also implemented an adapted variant, where the DDP solution is reoptimized at each iteration to match the policy (in addition to weighting), but this variant performed 6  worse. [sent-143, score-0.669]
</p><p>68 Finally, we compare to an alternative variational policy search algorithm analogous to PoWER [8]. [sent-146, score-0.717]
</p><p>69 Although PoWER requires a linear policy parameterization and a speciﬁc exploration strategy, we can construct an analogous non-linear algorithm by replacing the analytic M-step with nonlinear optimization, as in our method. [sent-147, score-0.71]
</p><p>70 This algorithm is identical to ours, except that instead of using DDP to optimize q(ζ), the variational distribution is formed by taking samples from the current policy and reweighting them by the exponential of their cost. [sent-148, score-0.753]
</p><p>71 ” The policy is still initialized with supervised training to resemble the initial DDP solution, but otherwise this method does not beneﬁt from trajectory optimization and relies entirely on random exploration. [sent-150, score-0.839]
</p><p>72 However, our work is the ﬁrst to our knowledge to show how trajectory optimization can be used to guide policy learning within the control-as-inference framework. [sent-159, score-0.808]
</p><p>73 Our variational decomposition follows prior work on policy search with variational inference [3, 11] and expectation maximization [8, 21]. [sent-160, score-0.825]
</p><p>74 This can be an advantage in applications where running an unstable, incompletely optimized policy can be costly or dangerous. [sent-165, score-0.604]
</p><p>75 Our use of DDP to guide the policy search parallels our previous Guided Policy Search (GPS) algorithm [10]. [sent-166, score-0.629]
</p><p>76 These samples are therefore only useful when the policy class can reproduce them effectively. [sent-168, score-0.609]
</p><p>77 As shown in the evaluation of the walker with 5 hidden units, GPS may be unable to discover a good policy when the policy class cannot reproduce the initial DDP solution. [sent-169, score-1.316]
</p><p>78 Adaptive GPS addresses this issue by reoptimizing the trajectory to resemble the current policy, but the policy is still optimized with respect to an importance-sampled return estimate, which leaves it highly prone to local optima, and the theoretical justiﬁcation for adaptation is unclear. [sent-170, score-0.831]
</p><p>79 The proposed method justiﬁes the reoptimization of the trajectory under a variational framework, and uses standard maximum likelihood in place of the complex importance-sampled objective. [sent-171, score-0.305]
</p><p>80 We also compared our method to DAGGER [14], which uses a general-purpose supervised training algorithm to train the current policy to match an oracle, which in our case is the DDP solution. [sent-172, score-0.609]
</p><p>81 DAGGER matches actions from the oracle policy at states visited by the current policy, under the 7  assumption that the oracle can provide good actions in all states. [sent-173, score-0.728]
</p><p>82 Unlike DAGGER, our approach is relatively insensitive to the instability of the learned policy, since the learned policy is not sampled. [sent-176, score-0.62]
</p><p>83 Several prior methods also propose to improve policy search by using a distribution over high-value states, which might come from a DDP solution [6, 1]. [sent-177, score-0.649]
</p><p>84 Such methods generally use this “restart” distribution as a new initial state distribution, and show that optimizing a policy from such a restart distribution also optimizes the expected return. [sent-178, score-0.63]
</p><p>85 8  Discussion and Future Work  We presented a policy search algorithm that employs a variational decomposition of a maximum likelihood objective to combine trajectory optimization with policy search. [sent-180, score-1.58]
</p><p>86 The variational distribution is obtained using differential dynamic programming (DDP), and the policy can be optimized with a standard nonlinear optimization algorithm. [sent-181, score-0.82]
</p><p>87 Model-based trajectory optimization effectively takes the place of random exploration, providing a much more effective means for ﬁnding low cost regions that the policy is then trained to visit. [sent-182, score-0.865]
</p><p>88 Our evaluation shows that this algorithm outperforms prior variational methods and prior methods that use trajectory optimization to guide policy search. [sent-183, score-0.896]
</p><p>89 First, the policy search does not need to sample the learned policy. [sent-185, score-0.652]
</p><p>90 By sampling directly from the Gaussian marginals of the DDP-induced distribution over trajectories, our approach also avoids some of the issues associated with unstable dynamics, requiring only that the task permit effective trajectory optimization. [sent-188, score-0.286]
</p><p>91 Obtaining good best-case performance is often the hardest part of policy search, since a policy that achieves good results occasionally is easier to improve with standard on-policy search methods than one that fails outright. [sent-190, score-1.203]
</p><p>92 While we mitigate this by averaging the policy derivatives over multiple samples from the DDP marginals, this approach could still break down in the presence of highly nonsmooth dynamics or policies. [sent-193, score-0.71]
</p><p>93 An interesting avenue for future work is to extend the trajectory optimization method to nonsmooth domains by using samples rather than linearization, perhaps analogously to the unscented Kalman ﬁlter [5, 18]. [sent-194, score-0.269]
</p><p>94 This could also avoid the need to differentiate the policy with respect to the inputs, allowing for richer policy classes to be used. [sent-195, score-1.148]
</p><p>95 Another interesting avenue for future work is to apply model-free trajectory optimization techniques [7], which would avoid the need for a model of the system dynamics, or to learn the dynamics from data, for example by using Gaussian processes [2]. [sent-196, score-0.317]
</p><p>96 It would also be straightforward to use multiple trajectories optimized from different initial states to learn a single policy that is able to succeed under a variety of initial conditions. [sent-197, score-0.741]
</p><p>97 Overall, we believe that trajectory optimization is a very useful tool for policy search. [sent-198, score-0.808]
</p><p>98 By separating the policy optimization and exploration problems into two separate phases, we can employ simpler algorithms such as SGD and DDP that are better suited for each phase, and can achieve superior performance on complex tasks. [sent-199, score-0.702]
</p><p>99 We believe that additional research into augmenting policy learning with trajectory optimization can further advance the performance of policy search techniques. [sent-200, score-1.437]
</p><p>100 PILCO: a model-based and data-efﬁcient approach to policy search. [sent-213, score-0.574]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.574), ('ddp', 0.425), ('ut', 0.389), ('xt', 0.29), ('trajectory', 0.192), ('gps', 0.141), ('dagger', 0.132), ('fut', 0.112), ('swimmer', 0.1), ('fxt', 0.099), ('walker', 0.09), ('variational', 0.088), ('exploration', 0.086), ('dynamics', 0.083), ('uut', 0.066), ('locomotion', 0.066), ('kt', 0.06), ('policies', 0.058), ('cost', 0.057), ('search', 0.055), ('guided', 0.053), ('vxxt', 0.05), ('marginals', 0.045), ('kl', 0.045), ('lqr', 0.044), ('divergence', 0.042), ('units', 0.042), ('optimization', 0.042), ('vx', 0.041), ('ot', 0.041), ('vxt', 0.037), ('samples', 0.035), ('current', 0.035), ('todorov', 0.033), ('variant', 0.033), ('walking', 0.032), ('trajectories', 0.032), ('dynamic', 0.032), ('initial', 0.031), ('swimming', 0.03), ('toussaint', 0.03), ('levine', 0.03), ('nonlinear', 0.03), ('optimized', 0.03), ('objective', 0.03), ('unstable', 0.029), ('py', 0.028), ('hidden', 0.028), ('qt', 0.026), ('sgd', 0.025), ('likelihood', 0.025), ('actions', 0.025), ('restart', 0.025), ('bipedal', 0.025), ('cuut', 0.025), ('cuxt', 0.025), ('cxxt', 0.025), ('nonquadratic', 0.025), ('quxt', 0.025), ('qxt', 0.025), ('qxxt', 0.025), ('uxt', 0.025), ('vladlen', 0.025), ('states', 0.025), ('differential', 0.024), ('tasks', 0.024), ('log', 0.024), ('reinforcement', 0.024), ('magnitude', 0.024), ('learned', 0.023), ('iteration', 0.023), ('dkl', 0.023), ('linearized', 0.022), ('oracle', 0.022), ('sergey', 0.022), ('torque', 0.022), ('dut', 0.022), ('qut', 0.022), ('optimize', 0.021), ('control', 0.021), ('solution', 0.02), ('deterministic', 0.02), ('executions', 0.02), ('importance', 0.02), ('task', 0.02), ('maximization', 0.02), ('parameterization', 0.02), ('adapted', 0.019), ('discover', 0.019), ('guides', 0.019), ('cxt', 0.019), ('gaussian', 0.019), ('robot', 0.019), ('equation', 0.018), ('mitigate', 0.018), ('succeed', 0.018), ('lbfgs', 0.018), ('imitation', 0.018), ('iterative', 0.018), ('robotics', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="348-tfidf-1" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>2 0.46420044 <a title="348-tfidf-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.42253816 <a title="348-tfidf-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>4 0.31804726 <a title="348-tfidf-4" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>Author: Prashanth L.A., Mohammad Ghavamzadeh</p><p>Abstract: In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in ﬁnance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we ﬁrst deﬁne a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a trafﬁc signal control application. 1</p><p>5 0.28625843 <a title="348-tfidf-5" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>6 0.28525385 <a title="348-tfidf-6" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>7 0.25272939 <a title="348-tfidf-7" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>8 0.24694222 <a title="348-tfidf-8" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>9 0.23839213 <a title="348-tfidf-9" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>10 0.22654878 <a title="348-tfidf-10" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>11 0.22338587 <a title="348-tfidf-11" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>12 0.21067835 <a title="348-tfidf-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.18623094 <a title="348-tfidf-13" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>14 0.17437656 <a title="348-tfidf-14" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>15 0.17268811 <a title="348-tfidf-15" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>16 0.16417363 <a title="348-tfidf-16" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>17 0.15875049 <a title="348-tfidf-17" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>18 0.15376025 <a title="348-tfidf-18" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>19 0.15108097 <a title="348-tfidf-19" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>20 0.15090032 <a title="348-tfidf-20" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.266), (1, -0.429), (2, -0.18), (3, 0.187), (4, -0.085), (5, 0.075), (6, -0.107), (7, 0.222), (8, 0.061), (9, -0.083), (10, -0.051), (11, -0.181), (12, -0.087), (13, 0.087), (14, -0.013), (15, -0.069), (16, -0.02), (17, -0.004), (18, -0.031), (19, 0.103), (20, 0.074), (21, -0.067), (22, 0.063), (23, 0.011), (24, -0.046), (25, -0.053), (26, 0.04), (27, 0.043), (28, 0.018), (29, -0.035), (30, 0.058), (31, -0.02), (32, 0.098), (33, -0.018), (34, -0.023), (35, 0.001), (36, 0.002), (37, -0.048), (38, 0.01), (39, 0.005), (40, 0.002), (41, 0.025), (42, -0.014), (43, 0.018), (44, 0.014), (45, 0.047), (46, 0.012), (47, -0.014), (48, 0.073), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97945184 <a title="348-lsi-1" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>2 0.84068322 <a title="348-lsi-2" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>3 0.82419717 <a title="348-lsi-3" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>4 0.80984151 <a title="348-lsi-4" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>Author: Prashanth L.A., Mohammad Ghavamzadeh</p><p>Abstract: In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in ﬁnance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we ﬁrst deﬁne a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a trafﬁc signal control application. 1</p><p>5 0.76109415 <a title="348-lsi-5" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>6 0.75749028 <a title="348-lsi-6" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>7 0.75109172 <a title="348-lsi-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.73014772 <a title="348-lsi-8" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>9 0.71978706 <a title="348-lsi-9" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>10 0.63210213 <a title="348-lsi-10" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>11 0.57715005 <a title="348-lsi-11" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>12 0.57433933 <a title="348-lsi-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.56636208 <a title="348-lsi-13" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>14 0.56525952 <a title="348-lsi-14" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>15 0.55231375 <a title="348-lsi-15" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>16 0.54162854 <a title="348-lsi-16" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>17 0.50908852 <a title="348-lsi-17" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>18 0.48971832 <a title="348-lsi-18" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>19 0.45247033 <a title="348-lsi-19" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>20 0.44479477 <a title="348-lsi-20" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.014), (16, 0.025), (33, 0.117), (34, 0.221), (41, 0.039), (49, 0.029), (56, 0.127), (70, 0.028), (85, 0.023), (86, 0.197), (89, 0.023), (92, 0.01), (93, 0.042), (95, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85975158 <a title="348-lda-1" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>2 0.85691279 <a title="348-lda-2" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>3 0.83652389 <a title="348-lda-3" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>Author: Remi Gribonval, Pierre Machart</p><p>Abstract: There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efﬁciency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization. 1</p><p>4 0.83612055 <a title="348-lda-4" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>5 0.81024587 <a title="348-lda-5" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>Author: Nima Taghipour, Jesse Davis, Hendrik Blockeel</p><p>Abstract: Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the ﬁrst-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree. 1</p><p>6 0.80945933 <a title="348-lda-6" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>7 0.8078739 <a title="348-lda-7" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>8 0.80597633 <a title="348-lda-8" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>9 0.80540764 <a title="348-lda-9" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>10 0.80481708 <a title="348-lda-10" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>11 0.80206704 <a title="348-lda-11" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>12 0.79645157 <a title="348-lda-12" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>13 0.79241562 <a title="348-lda-13" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>14 0.79201055 <a title="348-lda-14" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>15 0.79116482 <a title="348-lda-15" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>16 0.79103184 <a title="348-lda-16" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>17 0.78782058 <a title="348-lda-17" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>18 0.78545398 <a title="348-lda-18" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>19 0.78536737 <a title="348-lda-19" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>20 0.78228313 <a title="348-lda-20" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
