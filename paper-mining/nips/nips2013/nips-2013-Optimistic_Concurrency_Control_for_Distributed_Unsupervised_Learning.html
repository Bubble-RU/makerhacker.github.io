<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-238" href="#">nips2013-238</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</h1>
<br/><p>Source: <a title="nips-2013-238-pdf" href="http://papers.nips.cc/paper/5038-optimistic-concurrency-control-for-distributed-unsupervised-learning.pdf">pdf</a></p><p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>Reference: <a title="nips-2013-238-reference" href="../nips2013_reference/nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. [sent-4, score-0.317]
</p><p>2 We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. [sent-6, score-0.287]
</p><p>3 We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. [sent-7, score-0.175]
</p><p>4 We evaluate our methods via large-scale experiments in a cluster computing environment. [sent-8, score-0.096]
</p><p>5 1  Introduction  The desire to apply machine learning to increasingly larger datasets has pushed the machine learning community to address the challenges of distributed algorithm design: partitioning and coordinating computation across the processing resources. [sent-9, score-0.075]
</p><p>6 For these embarrassingly parallel tasks, the machine learning community has embraced the map-reduce paradigm, which provides a template for constructing distributed algorithms that are fault tolerant, scalable, and easy to study. [sent-11, score-0.119]
</p><p>7 , collapsed Gibbs sampling or coordinate ascent) which were developed and studied in the serial setting. [sent-14, score-0.181]
</p><p>8 The mutual exclusion approach, adopted by [1] and [2], guarantees a serializable execution preserving the theoretical properties of the serial algorithm but at the expense of parallelism and costly locking overhead. [sent-17, score-0.434]
</p><p>9 In this paper we explore a third approach, optimistic concurrency control (OCC) [5] which offers the performance gains of the coordination-free approach while at the same time ensuring a serializable execution and preserving the theoretical properties of the serial algorithm. [sent-19, score-0.692]
</p><p>10 1  We apply OCC to distributed nonparametric unsupervised learning—including but not limited to clustering—and implement distributed versions of the DP-Means [6], BP-Means [7], and online facility location (OFL) algorithms. [sent-23, score-0.375]
</p><p>11 Reinterpretation of online nonparametric clustering in the form of facility location with approximation guarantees. [sent-28, score-0.234]
</p><p>12 , model parameters or variable assignment) giving the illusion of serial dependencies between each operation. [sent-35, score-0.181]
</p><p>13 This opportunity for serializable concurrency forms the foundation of distributed database systems. [sent-37, score-0.421]
</p><p>14 For example, two customers may concurrently make purchases exhausting the inventory of unrelated products, but if they try to purchase the same product then we may need to serialize their purchases to ensure sufﬁcient inventory. [sent-38, score-0.21]
</p><p>15 This might work for an unpopular, rare product but if we are interested in selling a popular product for which we have a large inventory the serialization overhead could lead to unnecessarily slow response times. [sent-40, score-0.188]
</p><p>16 To address this problem, the database community has adopted optimistic concurrency control (OCC) [5] in which the system tries to satisfy the customers requests without locking and corrects transactions that could lead to negative inventory (e. [sent-41, score-0.497]
</p><p>17 Optimistic concurrency control exploits situations where most operations can execute concurrently without conﬂicting or violating serialization invariants. [sent-44, score-0.452]
</p><p>18 For example, given sufﬁcient inventory the order in which customers are satisﬁed is immaterial and concurrent operations can be executed serially to yield the same ﬁnal result. [sent-45, score-0.297]
</p><p>19 However, in the rare event that inventory is nearly depleted two concurrent purchases may not be serializable since the inventory can never be negative. [sent-46, score-0.254]
</p><p>20 By shifting the cost of concurrency control to rare events we can admit more costly concurrency control mechanisms (e. [sent-47, score-0.571]
</p><p>21 , operations or collections of operations), a mechanism to detect when a transaction violates serialization invariants (i. [sent-52, score-0.215]
</p><p>22 Optimistic concurrency control is most effective when the cost of validating concurrent transactions is small and conﬂicts occur infrequently. [sent-57, score-0.304]
</p><p>23 Machine learning algorithms are ideal for optimistic concurrency control. [sent-58, score-0.331]
</p><p>24 Similarly, symmetry in our models often provides the ﬂexibility to reorder serial operations while preserving algorithm invariants. [sent-60, score-0.277]
</p><p>25 Because the models encode the dependency structure, we can easily detect when an operation violates serial invariants and correct by rejecting the change and rerunning the computation. [sent-61, score-0.24]
</p><p>26 As a consequence OCC allows us to easily construct provably correct and efﬁcient distributed algorithms without the need to develop new theoretical tools to analyze complex non-deterministic distributed behavior. [sent-63, score-0.15]
</p><p>27 1  The OCC Pattern for Machine Learning  Optimistic concurrency control can be distilled to a simple pattern (meta-algorithm) for the design and implementation of distributed machine learning systems. [sent-65, score-0.35]
</p><p>28 Each processor maintains a replicated view of the global state and serially applies the learning algorithm as a sequence of operations on its assigned data and the global state. [sent-67, score-0.241]
</p><p>29 If an operation mutates the global state in a way that preserves the serialization invariants then the operation is accepted locally and its effect on the global state, if any, is eventually replicated to other processors. [sent-68, score-0.316]
</p><p>30 However, if an operation could potentially conﬂict with operations on other processors then it is sent to a unique serializing processor where it is rejected or corrected and the resulting global state change is eventually replicated to the rest of the processors. [sent-69, score-0.23]
</p><p>31 Meanwhile the originating processor either tentatively accepts the state change (if a rollback operator is deﬁned) or proceeds as though the operation has been deferred to some point in the future. [sent-70, score-0.114]
</p><p>32 Within an epoch t, b data points B(p, t) are evenly assigned to each of the P processors. [sent-72, score-0.187]
</p><p>33 Any state changes or serialization operations are transmitted at the end of the epoch and processed before the next epoch. [sent-73, score-0.277]
</p><p>34 In this paper we apply the OCC pattern to machine learning problems that have a more discrete, combinatorial ﬂavor—in particular unsupervised clustering and latent feature learning problems. [sent-76, score-0.09]
</p><p>35 These problems exhibit symmetry via their invariance to both data permutation and cluster or feature permutation. [sent-77, score-0.125]
</p><p>36 Together with the sparsity of interacting operations in their existing serial algorithms, these problems offer a unique opportunity to develop OCC algorithms. [sent-78, score-0.26]
</p><p>37 The algorithms considered to date in this literature have been developed and analyzed in the serial setting; our goal is to explore distributed algorithms for optimizing these cost functions that preserve the structure and analysis of their serial counterparts. [sent-81, score-0.437]
</p><p>38 Like the K-means algorithm, DP-Means alternates between updating the cluster assignment zi for each point xi and recomputing K the centroids C = {µk }k=1 associated with each clusters. [sent-85, score-0.206]
</p><p>39 However, DP-Means differs in that the number of clusters is not ﬁxed a priori. [sent-86, score-0.092]
</p><p>40 Instead, if the distance from a given data point to all existing cluster centroids is greater than a parameter λ, then a new cluster is created. [sent-87, score-0.192]
</p><p>41 While the second phase is trivially parallel, the process of introducing clusters in the ﬁrst phase is inherently serial. [sent-88, score-0.152]
</p><p>42 However, clusters tend to be introduced infrequently, and thus DP-Means provides an opportunity for OCC. [sent-89, score-0.118]
</p><p>43 3 we present an OCC parallelization of the DP-Means algorithm in which each iteration of the serial DP-Means algorithm is divided into N/(P b) bulk-synchronous epochs. [sent-91, score-0.215]
</p><p>44 During each epoch t, each processor p evaluates the cluster membership of its assigned data {xi }i∈B(p,t) using the cluster centers C from the previous epoch and optimistically proposes a new set of cluster ˆ ˆ centers C. [sent-93, score-0.835]
</p><p>45 At the end of each epoch the proposed cluster centers, C, are serially validated using Alg. [sent-94, score-0.369]
</p><p>46 The validation process accepts cluster centers that are not covered by (i. [sent-100, score-0.262]
</p><p>47 When a cluster center is rejected we update its reference to point to the already accepted center, thereby correcting the original point assignment. [sent-103, score-0.224]
</p><p>48 However, while DP-Means allows the clusters to be arbitrary points (e. [sent-106, score-0.141]
</p><p>49 , C ∈ RD ), FL constrains the clusters to be points C ⊆ F in a set of candidate locations F. [sent-108, score-0.141]
</p><p>50 We build on the online facility location (OFL) algorithm described by Meyerson [10]. [sent-112, score-0.179]
</p><p>51 The OFL algorithm processes each data point x serially in a single pass 2 by either adding x to the set of clusters with probability min(1, minµ∈C x − µ /λ2 ) or assigning x to the nearest existing cluster. [sent-113, score-0.22]
</p><p>52 The OCC OFL algorithm differs only in that clusters are introduced and validated stochastically—the validation process ensures that the new clusters are accepted with probability equal to the serial algorithm. [sent-117, score-0.518]
</p><p>53 4  Algorithm 4: Parallel OFL  Algorithm 5: OFLValidate  Input: Same as DP-Means ˆ for epoch t = 1 to N/(P b) do C ← ∅ for p ∈ {1, . [sent-120, score-0.113]
</p><p>54 As with serial DP-means, there are two phases in serial BP-means (Alg. [sent-124, score-0.362]
</p><p>55 In the ﬁrst phase, each data point xi is labeled with binary assignments from a collection of features (zik = 0 if xi doesn’t belong to feature k; otherwise zik = 1) to construct a representation xi ≈ k zik fk . [sent-126, score-0.452]
</p><p>56 In the ˆ second phase, parameter values (the feature means fk ∈ C) are updated based on the assignments. [sent-127, score-0.108]
</p><p>57 While the second phase is trivially parallel, the inherently serial nature of the ﬁrst phase combined with the infrequent introduction of new features points to the usefulness of OCC in this domain. [sent-129, score-0.313]
</p><p>58 Each transaction operates on a data point xi in two phases. [sent-131, score-0.083]
</p><p>59 In the ﬁrst, analysis phase, the optimal representation k zik fk is found. [sent-132, score-0.172]
</p><p>60 , xi − k zik fk > λ), the difference is proposed as a new feature in the second validation phase. [sent-135, score-0.267]
</p><p>61 At the end of epoch t, ˜ the proposed features {finew } are serially validated to obtain a set of accepted features C. [sent-136, score-0.419]
</p><p>62 For each proposed feature finew , the validation process ﬁrst ﬁnds the optimal representation finew ≈ new is not well represented, the difference finew − ˜ fk ∈C zik fk using newly accepted features. [sent-137, score-0.572]
</p><p>63 If fi ˜ ˜ fk ∈C zik fk is added to C and accepted as a new feature. [sent-138, score-0.351]
</p><p>64 The feature means update F ← (Z T Z)−1 Z T X can be evaluated as a single transaction by computing the T T sums Z T Z = i zi zi (where zi is a K × 1 column vector so zi zi is a K × K matrix) and Z T X = i zi xT in parallel. [sent-140, score-0.457]
</p><p>65 4  Analysis of Correctness and Scalability  In contrast to the coordination-free pattern in which scalability is trivial and correctness often requires strong assumptions or holds only in expectation, the OCC pattern leads to simple proofs of correctness and challenging scalability analysis. [sent-142, score-0.206]
</p><p>66 The distributed DP-means, OFL, and BP-means algorithms are serially equivalent to DP-means, OFL and BP-means, respectively. [sent-146, score-0.203]
</p><p>67 1 is relatively straightforward and is obtained by constructing a permutation function that describes an equivalent serial execution for each distributed execution. [sent-148, score-0.303]
</p><p>68 Serializability allows us to easily extend important theoretical properties of the serial algorithm to the distributed setting. [sent-150, score-0.256]
</p><p>69 For example, by invoking serializability, we can establish the following result for the OCC version of the online facility location (OFL) algorithm: 5  Theorem 4. [sent-151, score-0.179]
</p><p>70 2 is ﬁrst derived in the serial setting then extended to the distributed setting through serializability. [sent-156, score-0.256]
</p><p>71 2 is unaffected by distributed processing and has no communication or coarsening tradeoffs. [sent-158, score-0.098]
</p><p>72 Furthermore, to retain the same factors as a batch algorithm on the full data, divide-and-conquer schemes need a large number of preliminary centers at lower levels [11, 12]. [sent-159, score-0.126]
</p><p>73 In that case, the communication cost can be high, since all proposed clusters are sent at the same time, as opposed to the OCC approach. [sent-160, score-0.149]
</p><p>74 Scalability The scalability of the OCC algorithms depends on the number of transactions that are rejected during validation (i. [sent-162, score-0.118]
</p><p>75 To illustrate the techniques employed in OCC scalability analysis we study the DP-Means algorithm, whose scalability limiting factor is determined by the number of points that must be serially validated. [sent-167, score-0.315]
</p><p>76 We show that the communication cost only depends on the number of clusters and processing resources and does not directly depend on the number of data points. [sent-168, score-0.115]
</p><p>77 Assume N data points are generated iid to form a random number (KN ) of well-spaced clusters of diameter λ: λ is an upper bound on the distances within clusters and a lower bound on the distance between clusters. [sent-172, score-0.233]
</p><p>78 Then the expected number of serially validated points is bounded above by P b + E [KN ] for P processors and b points per epoch. [sent-173, score-0.292]
</p><p>79 Under the separation assumptions of the theorem, the number of clusters present in N data points, KN , is exactly equal to the number of clusters found by DP-Means in N data points; call this latter quantity kN . [sent-174, score-0.184]
</p><p>80 Since the master must process at least kN points, the overhead caused by rejections is P b and independent of N . [sent-176, score-0.096]
</p><p>81 The cluster and feature proportions were generated nonparametrically as described below. [sent-178, score-0.125]
</p><p>82 Clustering: The cluster proportions and indicators were generated simultaneously using the stickbreaking procedure for Dirichlet processes—‘sticks’ are ‘broken’ on-the-ﬂy to generate new clusters as necessary. [sent-181, score-0.188]
</p><p>83 Cluster means 1 were sampled µk ∼ N (0, I16 ), and data points were generated at xi ∼ N (µzi , 4 I16 ). [sent-183, score-0.094]
</p><p>84 We generated feature means fk ∼ N (0, I16 ) and data points xi ∼ N ( k zik fk , 1 I16 ). [sent-190, score-0.374]
</p><p>85 1  Simulated experiments  To test the efﬁciency of our algorithms, we simulated the ﬁrst iteration (one complete pass over all the data, where most clusters / features are created and thus greatest coordination is needed) of each algorithm in MATLAB. [sent-192, score-0.155]
</p><p>86 features, and MN , the number of proposed clusters / features. [sent-196, score-0.092]
</p><p>87 2  Distributed implementation and experiments  We also implemented1 the distributed algorithms in Spark [9], an open-source cluster computing system. [sent-206, score-0.171]
</p><p>88 The DP-means and BP-means algorithms were initialized by pre-processing a small number of data points (1/16 of the ﬁrst P b points)—this reduces the number of data points sent to the master on the ﬁrst epoch, while still preserving serializability of the algorithms. [sent-207, score-0.295]
</p><p>89 Ideally, to process the same amount of data, an algorithm and implementation with perfect scaling would take half the runtime on 8 machines as it would on 4, and so on. [sent-210, score-0.077]
</p><p>90 We were able to get perfect scaling (Figure 4a) in all but the ﬁrst iteration, when the master has to perform the most synchronization of proposed centers. [sent-215, score-0.077]
</p><p>91 Figure 4b shows that we get no scaling in the ﬁrst epoch, where all P b data points are sent to the master. [sent-219, score-0.105]
</p><p>92 6  Related work  Others have proposed alternatives to mutual exclusion and coordination-free parallelism for machine learning algorithm design. [sent-225, score-0.081]
</p><p>93 [14] proposed transforming the underlying model to expose additional parallelism while preserving the marginal posterior. [sent-226, score-0.108]
</p><p>94 7  (a) OCC DP-means  (b) OCC OFL  (c) OCC BP-means  Figure 4: Normalized runtime for distributed algorithms. [sent-233, score-0.109]
</p><p>95 Runtime of each iteration / epoch is divided by that  using 1 machine (P = 8). [sent-234, score-0.113]
</p><p>96 7  Discussion  In this paper we have shown how optimistic concurrency control can be usefully employed in the design of distributed machine learning algorithms. [sent-245, score-0.439]
</p><p>97 We established the equivalence of our distributed OCC DPmeans, OFL and BP-means algorithms to their serial counterparts, thus preserving their theoretical properties. [sent-247, score-0.299]
</p><p>98 In particular, the strong approximation guarantees of serial OFL translate immediately to the distributed algorithm. [sent-248, score-0.256]
</p><p>99 We implemented and evaluated all three OCC algorithms on a distributed computing platform and demonstrate strong scalability in practice. [sent-250, score-0.144]
</p><p>100 In particular we may be able to partially or probabilistically accept non-serializable operations in a way that preserves underlying algorithm invariants. [sent-253, score-0.096]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('occ', 0.727), ('ofl', 0.314), ('concurrency', 0.242), ('serial', 0.181), ('serially', 0.128), ('facility', 0.126), ('centers', 0.126), ('epoch', 0.113), ('accepted', 0.1), ('cluster', 0.096), ('zik', 0.093), ('clusters', 0.092), ('optimistic', 0.089), ('serializability', 0.086), ('serialization', 0.086), ('fk', 0.079), ('distributed', 0.075), ('scalability', 0.069), ('zi', 0.065), ('kn', 0.06), ('inventory', 0.057), ('finew', 0.057), ('serializable', 0.057), ('operations', 0.053), ('spark', 0.05), ('points', 0.049), ('execution', 0.047), ('xi', 0.045), ('parallel', 0.044), ('parallelism', 0.043), ('rollback', 0.043), ('preserving', 0.043), ('coordination', 0.04), ('fl', 0.04), ('concurrently', 0.038), ('transaction', 0.038), ('icts', 0.038), ('exclusion', 0.038), ('rejections', 0.038), ('invariants', 0.038), ('optimistically', 0.038), ('clustering', 0.035), ('granada', 0.035), ('runtime', 0.034), ('correctness', 0.034), ('sent', 0.034), ('parallelization', 0.034), ('master', 0.034), ('processors', 0.034), ('location', 0.033), ('control', 0.033), ('purchases', 0.033), ('validated', 0.032), ('processor', 0.031), ('vancouver', 0.031), ('customers', 0.03), ('phase', 0.03), ('feature', 0.029), ('ict', 0.029), ('replicated', 0.029), ('concurrent', 0.029), ('dpvalidate', 0.029), ('oflvalidate', 0.029), ('epochs', 0.028), ('rejected', 0.028), ('gonzalez', 0.027), ('opportunity', 0.026), ('unsupervised', 0.026), ('meyerson', 0.025), ('locking', 0.025), ('istanbul', 0.025), ('processed', 0.025), ('scalable', 0.025), ('evenly', 0.025), ('overhead', 0.024), ('features', 0.023), ('communication', 0.023), ('ref', 0.023), ('workload', 0.023), ('bnp', 0.023), ('scaling', 0.022), ('streaming', 0.022), ('joseph', 0.022), ('accept', 0.022), ('expose', 0.022), ('dirichlet', 0.021), ('preserves', 0.021), ('validation', 0.021), ('perfect', 0.021), ('rare', 0.021), ('database', 0.021), ('tamara', 0.021), ('broderick', 0.021), ('operation', 0.021), ('online', 0.02), ('nonparametric', 0.02), ('michael', 0.019), ('paradigm', 0.019), ('accepts', 0.019), ('purchase', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="238-tfidf-1" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>2 0.072680853 <a title="238-tfidf-2" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>3 0.065260969 <a title="238-tfidf-3" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>Author: Sinead A. Williamson, Steve N. MacEachern, Eric Xing</p><p>Abstract: Distributions over matrices with exchangeable rows and inﬁnitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution. 1</p><p>4 0.058912609 <a title="238-tfidf-4" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><p>5 0.058187485 <a title="238-tfidf-5" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>Author: Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman</p><p>Abstract: unkown-abstract</p><p>6 0.053483825 <a title="238-tfidf-6" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>7 0.046653375 <a title="238-tfidf-7" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>8 0.044646535 <a title="238-tfidf-8" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>9 0.044510327 <a title="238-tfidf-9" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>10 0.044344325 <a title="238-tfidf-10" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>11 0.043973129 <a title="238-tfidf-11" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>12 0.042493429 <a title="238-tfidf-12" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>13 0.041436147 <a title="238-tfidf-13" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>14 0.038957175 <a title="238-tfidf-14" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>15 0.037940167 <a title="238-tfidf-15" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>16 0.037585676 <a title="238-tfidf-16" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>17 0.036279917 <a title="238-tfidf-17" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>18 0.03615487 <a title="238-tfidf-18" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>19 0.03554843 <a title="238-tfidf-19" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>20 0.035017364 <a title="238-tfidf-20" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.015), (2, -0.003), (3, -0.006), (4, 0.016), (5, 0.057), (6, 0.028), (7, -0.012), (8, 0.039), (9, 0.002), (10, -0.01), (11, 0.055), (12, 0.024), (13, 0.012), (14, 0.015), (15, 0.02), (16, 0.007), (17, -0.043), (18, 0.029), (19, 0.023), (20, 0.035), (21, 0.027), (22, -0.021), (23, -0.029), (24, -0.027), (25, -0.034), (26, -0.026), (27, -0.031), (28, -0.019), (29, -0.075), (30, -0.023), (31, 0.034), (32, 0.159), (33, -0.007), (34, -0.004), (35, -0.017), (36, -0.019), (37, 0.059), (38, 0.003), (39, 0.019), (40, -0.042), (41, -0.088), (42, 0.051), (43, 0.016), (44, -0.056), (45, -0.045), (46, 0.012), (47, -0.107), (48, -0.068), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89647347 <a title="238-lsi-1" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>2 0.61391097 <a title="238-lsi-2" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>3 0.61327994 <a title="238-lsi-3" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><p>4 0.59977412 <a title="238-lsi-4" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>5 0.5706051 <a title="238-lsi-5" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>Author: Sinead A. Williamson, Steve N. MacEachern, Eric Xing</p><p>Abstract: Distributions over matrices with exchangeable rows and inﬁnitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution. 1</p><p>6 0.53972042 <a title="238-lsi-6" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>7 0.53466004 <a title="238-lsi-7" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>8 0.52510488 <a title="238-lsi-8" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>9 0.51557213 <a title="238-lsi-9" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>10 0.5005495 <a title="238-lsi-10" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<p>11 0.47184375 <a title="238-lsi-11" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>12 0.46194378 <a title="238-lsi-12" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>13 0.4554444 <a title="238-lsi-13" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>14 0.44794202 <a title="238-lsi-14" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>15 0.44735101 <a title="238-lsi-15" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>16 0.44049063 <a title="238-lsi-16" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>17 0.43783757 <a title="238-lsi-17" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>18 0.43772155 <a title="238-lsi-18" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>19 0.42601559 <a title="238-lsi-19" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>20 0.41874418 <a title="238-lsi-20" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (3, 0.013), (16, 0.097), (17, 0.23), (33, 0.083), (34, 0.1), (36, 0.016), (41, 0.035), (49, 0.036), (56, 0.082), (70, 0.024), (85, 0.045), (89, 0.022), (93, 0.064), (95, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75022316 <a title="238-lda-1" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>2 0.71504974 <a title="238-lda-2" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>Author: Wojciech Samek, Duncan Blythe, Klaus-Robert Müller, Motoaki Kawanabe</p><p>Abstract: The efﬁciency of Brain-Computer Interfaces (BCI) largely depends upon a reliable extraction of informative features from the high-dimensional EEG signal. A crucial step in this protocol is the computation of spatial ﬁlters. The Common Spatial Patterns (CSP) algorithm computes ﬁlters that maximize the difference in band power between two conditions, thus it is tailored to extract the relevant information in motor imagery experiments. However, CSP is highly sensitive to artifacts in the EEG data, i.e. few outliers may alter the estimate drastically and decrease classiﬁcation performance. Inspired by concepts from the ﬁeld of information geometry we propose a novel approach for robustifying CSP. More precisely, we formulate CSP as a divergence maximization problem and utilize the property of a particular type of divergence, namely beta divergence, for robustifying the estimation of spatial ﬁlters in the presence of artifacts in the data. We demonstrate the usefulness of our method on toy data and on EEG recordings from 80 subjects. 1</p><p>3 0.6692217 <a title="238-lda-3" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>4 0.61658639 <a title="238-lda-4" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>Author: Francesca Petralia, Joshua T. Vogelstein, David Dunson</p><p>Abstract: Nonparametric estimation of the conditional distribution of a response given highdimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change ﬂexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efﬁciently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. 1</p><p>5 0.6077683 <a title="238-lda-5" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><p>6 0.60576296 <a title="238-lda-6" href="./nips-2013-Pass-efficient_unsupervised_feature_selection.html">245 nips-2013-Pass-efficient unsupervised feature selection</a></p>
<p>7 0.59827161 <a title="238-lda-7" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>8 0.59645885 <a title="238-lda-8" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>9 0.59336549 <a title="238-lda-9" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>10 0.58979243 <a title="238-lda-10" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>11 0.58950126 <a title="238-lda-11" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>12 0.58926809 <a title="238-lda-12" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>13 0.58719921 <a title="238-lda-13" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>14 0.58643848 <a title="238-lda-14" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>15 0.5861724 <a title="238-lda-15" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>16 0.58583355 <a title="238-lda-16" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>17 0.58512133 <a title="238-lda-17" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>18 0.58408117 <a title="238-lda-18" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>19 0.5840649 <a title="238-lda-19" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>20 0.58344853 <a title="238-lda-20" href="./nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
