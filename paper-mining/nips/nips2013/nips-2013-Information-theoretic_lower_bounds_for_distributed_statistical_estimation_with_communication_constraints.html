<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-142" href="#">nips2013-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</h1>
<br/><p>Source: <a title="nips-2013-142-pdf" href="http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints.pdf">pdf</a></p><p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>Reference: <a title="nips-2013-142-reference" href="../nips2013_reference/nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Information-theoretic lower bounds for distributed statistical estimation with communication constraints Yuchen Zhang1 John C. [sent-1, score-0.51]
</p><p>2 edu  Abstract We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. [sent-6, score-0.823]
</p><p>3 Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. [sent-7, score-0.602]
</p><p>4 We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. [sent-8, score-0.626]
</p><p>5 We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. [sent-9, score-0.341]
</p><p>6 1  Introduction  Rapid growth in the size and scale of datasets has fueled increasing interest in statistical estimation in distributed settings [see, e. [sent-10, score-0.204]
</p><p>7 An essential design parameter in such methods is the amount of communication required between machines or chips. [sent-14, score-0.367]
</p><p>8 The focus of the current paper is the communication complexity of various classes of statistical estimation problems. [sent-16, score-0.291]
</p><p>9 In the classical setting, one considers centralized estimators that have access to all N samples, and for a given estimation problem, the optimal performance over all centralized schemes can be characterized by the minimax rate. [sent-21, score-0.854]
</p><p>10 Each machine may perform arbitrary operations m on its own subset of data, and it then communicates results of these intermediate computations to the other processors or to a central fusion node. [sent-23, score-0.213]
</p><p>11 In this paper, we try to answer the following question: what is the minimal number of bits that must be exchanged in order to achieve the optimal estimation error achievable by centralized schemes? [sent-24, score-0.646]
</p><p>12 For instance, Luo [15] considers architectures in which machines may send only a single bit to a centralized processor; for certain problems, he shows that if each machine receives a single one-dimensional sample, it is possible to achieve the optimal centralized rate up to constant factors. [sent-30, score-0.879]
</p><p>13 [2] study Probably Approximately Correct (PAC) learning in the distributed setting; however, their stated lower bounds do not involve the number of machines. [sent-32, score-0.219]
</p><p>14 In contrast, our work focuses on scaling issues, both in terms of the number of machines as well as the dimensionality of the underlying data, and we formalize the problem in terms of statistical minimax theory. [sent-33, score-0.458]
</p><p>15 1  More precisely, we study the following problem: given a budget B of the total number of bits that may be communicated from the m distributed datasets, what is the minimax risk of any estimator based on the communicated messages? [sent-34, score-0.96]
</p><p>16 In this paper, we present some minimax lower bounds for distributed statistical estimation. [sent-38, score-0.527]
</p><p>17 By comparing our lower bounds with results in statistical estimation, we can identify the minimal communication cost that a distributed estimator must pay to have performance comparable to classical centralized estimators. [sent-39, score-0.768]
</p><p>18 We consider a class of distributed protocols Π, in which at each round t = 1, 2, . [sent-52, score-0.348]
</p><p>19 , machine i sends a message Yt,i that is a measurable function of the local data X (i) , and potentially of past messages. [sent-55, score-0.256]
</p><p>20 It is convenient to model this message as being sent to a central fusion center. [sent-56, score-0.375]
</p><p>21 Let Y t = {Yt,i }i∈[m] denote the collection of all messages sent at round t. [sent-57, score-0.311]
</p><p>22 The length Lt,i of T m message Yt,i is the minimal number of bits required to encode it, and the total L = t=1 i=1 Lt,i of all messages sent corresponds to the total communication cost of the protocol. [sent-65, score-0.811]
</p><p>23 Note that the communication cost is a random variable, since the length of the messages may depend on the data, and the protocol may introduce auxiliary randomness. [sent-66, score-0.499]
</p><p>24 An independent protocol Π is based on a single round (T = 1) of communication, in which machine i sends message Y1,i to the fusion center. [sent-68, score-0.523]
</p><p>25 Given a family P, the class of independent protocols with budget B ≥ 0 is given by m  Aind (B, P) =  independent protocols Π such that  sup EP  P ∈P  i=1  Li ≤ B . [sent-70, score-0.741]
</p><p>26 (1)  (For simplicity, we use Yi to indicate the message sent from processor i and Li to denote its length in the independent case. [sent-71, score-0.227]
</p><p>27 , Bm ) and deﬁne Aind (B1:m , P) =  independent protocols Π such that  sup EP [Li ] ≤ Bi for i ∈ [m] . [sent-76, score-0.295]
</p><p>28 (2)  P ∈P  In contrast to independent protocols, the class of interactive protocols allows for interaction at different stages of the message passing process. [sent-77, score-0.523]
</p><p>29 In particular, suppose that machine i sends message Yt,i to the fusion center at time t, who then posts it on a “public blackboard,” where all machines can read Yt,i . [sent-78, score-0.579]
</p><p>30 We think of this as a global broadcast system, which may be natural in settings in which processors have limited power or upstream capacity, but the centralized fusion center can send messages without limit. [sent-79, score-0.763]
</p><p>31 In the interactive setting, the message Yt,i should be viewed as a measurable function of the local data X (i) , and the past messages Y 1:t−1 . [sent-80, score-0.561]
</p><p>32 The family of interactive protocols with budget B ≥ 0 is given by Ainter (B, P) =  interactive protocols Π such that  sup EP [L] ≤ B . [sent-81, score-1.111]
</p><p>33 2  We conclude this section by deﬁning the minimax framework used throughout this paper. [sent-83, score-0.269]
</p><p>34 We wish to characterize the best achievable performance of estimators θ that are functions of only the messages (Y 1 , . [sent-84, score-0.302]
</p><p>35 Given a class of distributions P, parameter θ : P → Θ, and communication budget B, the minimax risk for independent protocols is Mind (θ, P, B) :=  inf  inf sup EP,Π  Π∈Aind (B,P) θ P ∈P  θ(Y1 , . [sent-95, score-1.01]
</p><p>36 (4)  Here, the inﬁmum is taken jointly over all independent procotols Π that satisfy the budget constraint B, and over all estimators θ that are measurable functions of the messages in the protocol. [sent-99, score-0.415]
</p><p>37 This minimax risk should also be understood to depend on both the number of machines m and the individual sample size n. [sent-100, score-0.515]
</p><p>38 The minimax risk for interactive protocols, denoted by Minter , is deﬁned analogously, where the inﬁmum is instead taken over the class of interactive protocols. [sent-101, score-0.698]
</p><p>39 These communicationdependent minimax risks are the central objects in this paper: they provide a sharp characterization of the optimal rate of statistical estimation as a function of the communication budget B. [sent-102, score-0.868]
</p><p>40 (5)  The function θ → log MΘ (δ) is left-continuous and non-increasing in δ, so we may deﬁne the −1 inverse function log MΘ (B) := sup{δ | log MΘ (δ) ≥ B}. [sent-114, score-0.264]
</p><p>41 Proposition 1 For any family of distributions P and parameter set Θ = θ(P), the interactive minimax risk is lower bounded as 2 1 −1 Minter (θ, P, B) ≥ log MΘ (2B + 2) . [sent-115, score-0.778]
</p><p>42 (6) 4 Of course, the same lower bound also holds for Mind (θ, P, B), since any independent protocol is a special case of an interactive protocol. [sent-116, score-0.432]
</p><p>43 Since the packing entropy is lower bounded as log MΘ (δ) ≥ log(1/δ), the lower bound (6) implies e−2 −2B Mind (θ, P, B) ≥ Minter (θ, P, B) ≥ e . [sent-124, score-0.38]
</p><p>44 4 −2 1 Thus, setting B = 2 log n yields the lower bound Mind (θ, P([0, 1]), B) ≥ e4n . [sent-125, score-0.21]
</p><p>45 This lower bound is sharp up to the constant pre-factor, since it can be achieved by a simple method. [sent-126, score-0.235]
</p><p>46 Let us study how the budget B—meaning the of bits required to achieve the minimax rate—scales with the number of machines m. [sent-132, score-0.804]
</p><p>47 observations per machine: (a) Whenever the communication budget is upper bounded as B ≤ log(mn), there is a universal constant c such that c Minter (θ, U , B) ≥ . [sent-138, score-0.487]
</p><p>48 (mn)2 (b) Conversely, given a budget of B = 2 + 2 ln m log(mn) bits, there is a universal constant c such that c Minter (θ, U , B) ≤ . [sent-139, score-0.282]
</p><p>49 (mn)2 If each of m machines receives n observations, we have a total sample size of mn, so the minimax rate over all centralized procedures scales as 1/(mn)2 (for instance, see [14]). [sent-140, score-0.826]
</p><p>50 Consequently, Corollary 1(b) shows that the number of bits required to achieve the centralized rate has only logarithmic dependence on the number m of machines. [sent-141, score-0.589]
</p><p>51 In particular, we consider estimation in a normal location family model, where each machine receives an i. [sent-145, score-0.307]
</p><p>52 Theorem 1 For the univariate normal family N = {N(θ, σ 2 ) | θ ∈ [−1, 1]}, there is a universal constant c such that Minter (θ, N , B) ≥ c  σ2 min mn  m m mn , , 2 σ log m B log m  . [sent-149, score-1.066]
</p><p>53 (7)  The centralized minimax rate for estimating a univariate normal mean based on mn observations σ2 m is mn ; consequently, the lower bound (7) shows that at least B = Ω log m bits are required for a decentralized procedure to match the centralized rate in this case. [sent-150, score-1.941]
</p><p>54 This type of scaling is dramatically different than the logarithmic scaling for the uniform family, showing that establishing sharp communication-based lower bounds requires careful study of the underlying family of distributions. [sent-151, score-0.397]
</p><p>55 3  Independent protocols in multi-machine settings  Departing from the interactive setting, in this section we focus on independent protocols, providing somewhat more general results than those for interactive protocols. [sent-153, score-0.596]
</p><p>56 We ﬁrst provide lower bounds for the problem of mean estimation in the parameter for a d-dimensional normal location family Nd = {N(θ, σ 2 Id×d ) | θ ∈ Θ = [−1, 1]d },  (8)  Theorem 2 For i = 1, . [sent-154, score-0.387]
</p><p>57 , m, assume that each machine has communication budget Bi , and receives an i. [sent-157, score-0.404]
</p><p>58 There exists a universal (numerical) constant c such that Mind (θ, Nd , B1:m ) ≥ c  σ2 d min mn  m mn , , σ 2 log m  m i=1  m min{1, Bi } log m d  . [sent-161, score-0.954]
</p><p>59 (9)  Given centralized access to the full mn-sized sample, a reasonable procedure would be to compute 2 the sample mean, leading to an estimate with mean-squared error σ d , which is minimax optimal. [sent-162, score-0.532]
</p><p>60 mn 4  Consequently, Theorem 2 shows that to achieve an order-optimal mean-squared error, the total number of bits communicated must (nearly) scale with the product of the dimension d and number of machines m, that is, as dm/ log m. [sent-163, score-0.836]
</p><p>61 If we ignore logarithmic factors, this lower bound is achievable by a simple procedure: each machine computes the sample mean of its local data and quantizes each coordinate to precision σ 2 /n using O(d log(n/σ 2 )) bits. [sent-164, score-0.348]
</p><p>62 These quantized sample averages are communicated to the fusion center using B = O(dm log(n/σ 2 )) total bits. [sent-165, score-0.38]
</p><p>63 The fusion center averages them, obtaining an estimate with mean-squared error of optimal order σ 2 d/(mn) as required. [sent-166, score-0.212]
</p><p>64 It is a minimax lower bound for mean estimation over the family Pd = {P supported on [−1, 1]d }. [sent-168, score-0.569]
</p><p>65 Proposition 2 Assume that each of m machines receives a single sample (n = 1) from a distribution in Pd . [sent-169, score-0.266]
</p><p>66 There exists a universal (numerical) constant c such that Mind (θ, Pd , B1:m ) ≥ c  d min m, m  m i=1  where Bi is the budget for machine i. [sent-170, score-0.347]
</p><p>67 m min{1, Bi } d  ,  (10)  The standard minimax rate for d-dimensional mean estimation scales as d/m. [sent-171, score-0.439]
</p><p>68 The lower bound (10) m shows that in order to achieve this scaling, we must have i=1 min{1, Bi } m, showing that each d machine must send Bi d bits. [sent-172, score-0.248]
</p><p>69 Moreover, this lower bound is achievable by a simple scheme. [sent-173, score-0.196]
</p><p>70 Machine i uses d bits to send the vector Zi ∈ {0, 1}d to the fusion center. [sent-179, score-0.481]
</p><p>71 m 1 The fusion center then computes the average θ = m i=1 (2Zi − 1). [sent-180, score-0.212]
</p><p>72 4  Consequences for regression  In this section, we turn to identifying the minimax rates for a pair of important estimation problems: linear regression and probit regression. [sent-182, score-0.723]
</p><p>73 1  Linear regression  We consider a distributed instantiation of linear regression with ﬁxed design matrices. [sent-184, score-0.324]
</p><p>74 Concretely, suppose that each of m machines has stored a ﬁxed design matrix A(i) ∈ Rn×d and then observes a response vector b(i) ∈ Rd from the standard linear regression model b(i) = A(i) θ + ε(i) ,  (11)  where ε(i) ∼ N(0, σ 2 In×n ) is a noise vector. [sent-185, score-0.357]
</p><p>75 Our goal is to estimate unknown regression vector θ ∈ Θ = [−1, 1]d , shared across all machines, in a distributed manner, To state our result, we assume uniform upper and lower bounds on the eigenvalues of the rescaled design matrices, namely 0 < λmin ≤  ηmin (A(i) ) √ and n i∈{1,. [sent-186, score-0.39]
</p><p>76 (a) Then there is a universal positive constant c such that Mind (θ, P, B1:m ) ≥ c  σ2 d min mn  m mn , 2 , 2 σ λmax log m λ2 max  m i=1  m min{1, Bi } log m d  (b) Conversely, given budgets Bi ≥ d log(mn) for i = 1, . [sent-194, score-1.074]
</p><p>77 [14]) that the minimax rate for d-dimensional linear regression scales as dσ 2 /(nm). [sent-201, score-0.431]
</p><p>78 Part (a) of Corollary 2 shows this optimal rate is attainable only if the budget Bi at each m dm machine is of the order d/ log(m), meaning that the total budget B = i=1 Bi must grow as log m . [sent-202, score-0.45]
</p><p>79 Part (b) of the corollary shows that the minimax rate is achievable with budgets that match the lower bound up to logarithmic factors. [sent-203, score-0.701]
</p><p>80 [23], who show that solving each regression problem separately and then performing a form of approximate averaging, in which each machine uses Bi = d log(mn) bits, achieves the minimax rate up to constant prefactors. [sent-205, score-0.438]
</p><p>81 To prove part (a), we show that solving an arbitrary Gaussian mean estimation problem can be reduced to solving a specially constructed linear regression problem. [sent-206, score-0.24]
</p><p>82 Now consider any protocol Π ∈ Aind (B, P) that can solve any regression problem to within accuracy δ, so that E[ θ − θ 2 ] ≤ δ 2 . [sent-213, score-0.222]
</p><p>83 By the previously described reduction, the protocol Π can also 2 solve the mean estimation problem to accuracy δ, in particular via the pair (A(i) , b(i) ) constructed via expression (13). [sent-214, score-0.23]
</p><p>84 2  Probit regression  We now turn to the problem of binary classiﬁcation, in particular considering the probit regression model. [sent-217, score-0.38]
</p><p>85 Under condition (12) on the design matrices, we have: Corollary 3 Consider the probit model (14) under condition (12). [sent-225, score-0.225]
</p><p>86 Then (a) There is a universal constant c such that Mind (θ, P, B1:m ) ≥ c  d m min mn, 2 , mn λmax log m λ2 max  m i=1  m min{1, Bi } log m d  . [sent-226, score-0.702]
</p><p>87 Mind (θ, P, B1:m ) ≤ 2 λmin mn Proof: As in the previous case with linear regression, Zhang et al. [sent-231, score-0.289]
</p><p>88 ’s study of distributed convex optimization [23] gives part (b): each machine solves the local probit regression separately, after which each machine sends Bi = d log(mn) bits to average its local solution. [sent-232, score-0.713]
</p><p>89 To prove part (a), we show that linear regression problems can be solved via estimation in a specially constructed probit model. [sent-233, score-0.395]
</p><p>90 We construct the binary responses for our probit regression (Z (i,1) , . [sent-235, score-0.283]
</p><p>91 By inspection, any protocol Π ∈ Aind (B, P) solving the probit regression problem provides an estimator with the same error (risk) as the original linear regression problem via the construction (15). [sent-240, score-0.55]
</p><p>92 1  Broad outline  Most of our lower bounds follow the same basic strategy of reducing an estimation problem to a testing problem. [sent-244, score-0.239]
</p><p>93 Following this reduction, we then develop inequalities relating the probability of error in the test to the number of bits contained in the messages Yi sent from each machine. [sent-245, score-0.487]
</p><p>94 Lemma 1 shows that minimax lower lower bound can be derived by showing that, for some t > 0 to be chosen, it is difﬁcult to identify V within a radius of t. [sent-261, score-0.468]
</p><p>95 For any t ∈ R, we have P(dham (V , V ) > t) ≥ 1 −  I(V ; X) + log 2 log  |V| Nt  ,  where Nt := max |{ν ∈ V : dham (ν, ν ) ≤ t}| is the size of the largest t-neighborhood in V. [sent-263, score-0.356]
</p><p>96 ν∈V  Lemma 2 allows ﬂexibility in the application of the minimax bounds from Lemma 1. [sent-264, score-0.32]
</p><p>97 7  In a distributed protocol, we have a Markov chain V → X → Y , where Y denotes the messages the different machines send. [sent-268, score-0.437]
</p><p>98 Based on the messages Y , we consider an arbitrary estimator θ(Y ). [sent-269, score-0.241]
</p><p>99 Since d ≤ (de/t)t , for t ≤ d/6 we have t t log  |V| d ≥ d log 2 − log 2 Nt t  ≥ d log 2 −  2 d d √ > log(6e) − log 2 = d log 6 6 21/d 6 6e  for d ≥ 12 (the case d < 12 can be checked directly). [sent-271, score-0.528]
</p><p>100 In particular, inequality (18) establishes a link between the initial distribution (17) and the number of bits used to transmit information, that is, I(V ; Yi ) ≤ cδ 2 min{d, Bi }. [sent-281, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mn', 0.289), ('minimax', 0.269), ('protocols', 0.226), ('centralized', 0.226), ('bi', 0.216), ('bits', 0.207), ('messages', 0.196), ('probit', 0.186), ('interactive', 0.185), ('fusion', 0.179), ('communication', 0.178), ('minter', 0.171), ('machines', 0.15), ('budget', 0.147), ('mind', 0.146), ('aind', 0.143), ('dham', 0.143), ('protocol', 0.125), ('message', 0.112), ('universal', 0.101), ('regression', 0.097), ('send', 0.095), ('distributed', 0.091), ('log', 0.088), ('sent', 0.084), ('budgets', 0.083), ('ep', 0.08), ('decentralized', 0.079), ('sharp', 0.079), ('receives', 0.079), ('lower', 0.077), ('sends', 0.076), ('achievable', 0.074), ('estimation', 0.074), ('family', 0.073), ('communicated', 0.071), ('yi', 0.071), ('fano', 0.07), ('sup', 0.069), ('duchi', 0.069), ('min', 0.065), ('quantized', 0.06), ('risk', 0.059), ('corollary', 0.059), ('nt', 0.059), ('proposition', 0.056), ('logarithmic', 0.056), ('bounds', 0.051), ('mum', 0.051), ('pd', 0.048), ('estimator', 0.045), ('bound', 0.045), ('risks', 0.044), ('conversely', 0.043), ('stored', 0.042), ('location', 0.042), ('lemma', 0.04), ('measurable', 0.04), ('packing', 0.04), ('design', 0.039), ('normal', 0.039), ('statistical', 0.039), ('rate', 0.038), ('luo', 0.038), ('specially', 0.038), ('sample', 0.037), ('army', 0.037), ('max', 0.037), ('outline', 0.037), ('balcan', 0.036), ('consequently', 0.036), ('uniform', 0.035), ('minimal', 0.034), ('inequality', 0.034), ('processors', 0.034), ('constant', 0.034), ('center', 0.033), ('estimators', 0.032), ('processor', 0.031), ('achieve', 0.031), ('round', 0.031), ('mean', 0.031), ('dependence', 0.031), ('inf', 0.031), ('reduction', 0.03), ('dm', 0.03), ('px', 0.03), ('url', 0.03), ('statement', 0.029), ('suppose', 0.029), ('member', 0.028), ('local', 0.028), ('scales', 0.027), ('classical', 0.027), ('bounded', 0.027), ('bandwidth', 0.027), ('numerical', 0.026), ('establishing', 0.026), ('entropy', 0.026), ('proof', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="142-tfidf-1" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>2 0.18141557 <a title="142-tfidf-2" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>Author: John Duchi, Martin J. Wainwright, Michael Jordan</p><p>Abstract: We provide a detailed study of the estimation of probability distributions— discrete and continuous—in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental trade-offs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efﬁciency continuum. One of the consequences of our results is that Warner’s classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents. 1</p><p>3 0.1591212 <a title="142-tfidf-3" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>Author: Nicolas Heess, Daniel Tarlow, John Winn</p><p>Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex nonGaussian factors, there is still a signiﬁcant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difﬁcult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising. 1</p><p>4 0.12585416 <a title="142-tfidf-4" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><p>5 0.1096499 <a title="142-tfidf-5" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><p>6 0.10637575 <a title="142-tfidf-6" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>7 0.10554302 <a title="142-tfidf-7" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>8 0.10100085 <a title="142-tfidf-8" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>9 0.090867832 <a title="142-tfidf-9" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>10 0.087054417 <a title="142-tfidf-10" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>11 0.081884325 <a title="142-tfidf-11" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>12 0.079742931 <a title="142-tfidf-12" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>13 0.079444699 <a title="142-tfidf-13" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>14 0.077730358 <a title="142-tfidf-14" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>15 0.077052876 <a title="142-tfidf-15" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>16 0.077029034 <a title="142-tfidf-16" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>17 0.074696451 <a title="142-tfidf-17" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>18 0.071653374 <a title="142-tfidf-18" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>19 0.071449332 <a title="142-tfidf-19" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>20 0.070331447 <a title="142-tfidf-20" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, 0.022), (2, 0.083), (3, -0.02), (4, 0.036), (5, 0.1), (6, -0.009), (7, -0.05), (8, -0.049), (9, 0.066), (10, 0.109), (11, 0.016), (12, -0.052), (13, 0.042), (14, -0.033), (15, 0.007), (16, 0.02), (17, 0.011), (18, -0.013), (19, 0.084), (20, -0.087), (21, 0.02), (22, 0.003), (23, 0.016), (24, -0.068), (25, 0.019), (26, -0.072), (27, 0.009), (28, 0.06), (29, -0.127), (30, -0.02), (31, 0.005), (32, 0.012), (33, -0.022), (34, 0.133), (35, 0.048), (36, -0.154), (37, 0.051), (38, 0.041), (39, -0.049), (40, -0.03), (41, -0.115), (42, -0.072), (43, 0.051), (44, -0.037), (45, 0.087), (46, 0.073), (47, -0.002), (48, 0.049), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94208008 <a title="142-lsi-1" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>2 0.71110952 <a title="142-lsi-2" href="./nips-2013-Estimation_Bias_in_Multi-Armed_Bandit_Algorithms_for_Search_Advertising.html">112 nips-2013-Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising</a></p>
<p>Author: Min Xu, Tao Qin, Tie-Yan Liu</p><p>Abstract: In search advertising, the search engine needs to select the most proﬁtable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit (MAB) problem. In this paper, we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and “estimation of the largest mean” (ELM) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with beneﬁts to both the search engine and the advertisers. 1</p><p>3 0.68697602 <a title="142-lsi-3" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>4 0.60853004 <a title="142-lsi-4" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>Author: Nicolas Heess, Daniel Tarlow, John Winn</p><p>Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex nonGaussian factors, there is still a signiﬁcant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difﬁcult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising. 1</p><p>5 0.60395664 <a title="142-lsi-5" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>6 0.595635 <a title="142-lsi-6" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>7 0.594486 <a title="142-lsi-7" href="./nips-2013-Estimation%2C_Optimization%2C_and_Parallelism_when_Data_is_Sparse.html">111 nips-2013-Estimation, Optimization, and Parallelism when Data is Sparse</a></p>
<p>8 0.58339685 <a title="142-lsi-8" href="./nips-2013-Pass-efficient_unsupervised_feature_selection.html">245 nips-2013-Pass-efficient unsupervised feature selection</a></p>
<p>9 0.57974786 <a title="142-lsi-9" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>10 0.56296396 <a title="142-lsi-10" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>11 0.5620898 <a title="142-lsi-11" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>12 0.5566268 <a title="142-lsi-12" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>13 0.5548414 <a title="142-lsi-13" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>14 0.55316436 <a title="142-lsi-14" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>15 0.55204308 <a title="142-lsi-15" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>16 0.5475747 <a title="142-lsi-16" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>17 0.54216981 <a title="142-lsi-17" href="./nips-2013-Accelerated_Mini-Batch_Stochastic_Dual_Coordinate_Ascent.html">19 nips-2013-Accelerated Mini-Batch Stochastic Dual Coordinate Ascent</a></p>
<p>18 0.53805 <a title="142-lsi-18" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>19 0.53317547 <a title="142-lsi-19" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>20 0.53283638 <a title="142-lsi-20" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.048), (5, 0.029), (16, 0.039), (19, 0.014), (33, 0.157), (34, 0.077), (41, 0.025), (49, 0.026), (56, 0.133), (67, 0.016), (70, 0.047), (75, 0.067), (85, 0.108), (89, 0.049), (93, 0.052), (95, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91825479 <a title="142-lda-1" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>2 0.91367263 <a title="142-lda-2" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>Author: Erich Kummerfeld, David Danks</p><p>Abstract: Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and signiﬁcantly better when it is only locally stationary. 1</p><p>3 0.91143966 <a title="142-lda-3" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>Author: Dimitris Achlioptas, Zohar S. Karnin, Edo Liberty</p><p>Abstract: We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes A B 2 . For large m n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O 1 computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal ofﬂine distribution. Note that the probabilities in the optimal ofﬂine distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model. 1</p><p>4 0.90765446 <a title="142-lda-4" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>5 0.90567935 <a title="142-lda-5" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>6 0.90163118 <a title="142-lda-6" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>7 0.8994751 <a title="142-lda-7" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>8 0.89818537 <a title="142-lda-8" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>9 0.89814317 <a title="142-lda-9" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>10 0.89347363 <a title="142-lda-10" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>11 0.89307642 <a title="142-lda-11" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>12 0.89126176 <a title="142-lda-12" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>13 0.89038312 <a title="142-lda-13" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>14 0.89022487 <a title="142-lda-14" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>15 0.88959932 <a title="142-lda-15" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>16 0.88851291 <a title="142-lda-16" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>17 0.88777882 <a title="142-lda-17" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>18 0.88735366 <a title="142-lda-18" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>19 0.88699096 <a title="142-lda-19" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>20 0.88634002 <a title="142-lda-20" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
