<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-114" href="#">nips2013-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</h1>
<br/><p>Source: <a title="nips-2013-114-pdf" href="http://papers.nips.cc/paper/5167-extracting-regions-of-interest-from-biological-images-with-convolutional-sparse-block-coding.pdf">pdf</a></p><p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>Reference: <a title="nips-2013-114-reference" href="../nips2013_reference/nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Extracting regions of interest from biological images with convolutional sparse block coding  Marius Pachitariu1 , Adam Packer2 , Noah Pettit2 , Henry Dagleish2 , Michael Hausser2 and Maneesh Sahani1 1 Gatsby Unit, UCL, UK {marius, maneesh}@gatsby. [sent-1, score-0.793]
</p><p>2 uk  Abstract Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. [sent-12, score-1.04]
</p><p>3 Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. [sent-13, score-0.392]
</p><p>4 Formally, the model can be described as convolutional sparse block coding. [sent-14, score-0.448]
</p><p>5 For inference we use a variant of convolutional matching pursuit adapted to block-based representations. [sent-15, score-0.566]
</p><p>6 We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. [sent-18, score-0.675]
</p><p>7 We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. [sent-19, score-1.073]
</p><p>8 The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. [sent-20, score-0.347]
</p><p>9 1  Introduction  For evolutionary reasons, biological tissue at all spatial scales is composed of repeating patterns. [sent-21, score-0.49]
</p><p>10 At a small spatial scale eukaryotic cells contain only a few types of major organelles like mitochondria and vacuoles and several dozen minor organelles like vesicles and ribosomes. [sent-23, score-0.492]
</p><p>11 Each of the organelles is replicated a large number of times within each cell and has a distinctive visual appearance. [sent-24, score-0.316]
</p><p>12 At the scale of whole cells, most tissue types like muscle and epithelium are composed primarily of single cell types. [sent-25, score-0.51]
</p><p>13 Given the stereotypical nature of biological motifs, these images often appear as collections of similar elements over a noisy background, as shown in ﬁgure 1(a). [sent-32, score-0.37]
</p><p>14 We developed a generative image model that automatically discovers the repeating motifs, and segments biological images into the most common elements that form them. [sent-33, score-0.635]
</p><p>15 We apply the model to two-dimensional images composed of several hundred cells of possibly different types, such as 1  (a)  (b)  Figure 1: a. [sent-34, score-0.425]
</p><p>16 images of cortical tissue expressing ﬂuorescent GCaMP6, a calcium indicator, taken with a twophoton microscope in vivo. [sent-38, score-0.535]
</p><p>17 Our main aim is to automate the cell detection stage, because tracing cell contours by hand can be a laborious and inexact process, especially given the multitude of confounds usually present in these images. [sent-41, score-0.522]
</p><p>18 Assigning pixels to the correct cell can be difﬁcult. [sent-44, score-0.322]
</p><p>19 A third confound is that calcium, the marker which the ﬂuorescent images report, is present in the entire neuropil (in the dendrites and axons of the cells). [sent-45, score-0.348]
</p><p>20 Activation of calcium in the neuropil makes a noisy background for the estimation of cell somata. [sent-46, score-0.416]
</p><p>21 1 Background on automated extraction of cell somata Histological examination of biological tissue with light-microscopy is an important application for techniques of cell identiﬁcation and segmentation. [sent-49, score-0.877]
</p><p>22 Most algorithms for identifying cell somata from such images are based on hand-crafted ﬁltering and thresholding techniques. [sent-50, score-0.438]
</p><p>23 Our approach is to instead propose a fully generative model of the biological tissue which encapsulates our beliefs about the stereotypical structure of such images. [sent-52, score-0.444]
</p><p>24 The authors propose an independent components analysis (ICA) model of the movies which expresses their beliefs that all the pixels belonging to a cell should brighten together, but only rarely. [sent-56, score-0.322]
</p><p>25 Both of these studies are different from our approach, because we aim to recover cell bodies from single images alone. [sent-58, score-0.397]
</p><p>26 2 Background on convolutional image models Our proposed image model is a novel extension of a family of recent algorithms based on sparse coding that are commonly used in object recognition experiments [4], [5], [6], [7], [8]. [sent-68, score-0.662]
</p><p>27 A starting point for our model was the convolutional matching pursuit (MP) implementation of [5] (but see [6] for more details). [sent-69, score-0.505]
</p><p>28 The authors show that convolutional MP learns a diverse set of basis functions from natural images. [sent-70, score-0.328]
</p><p>29 Their implied generative model of an image is to pick out randomly a few basis functions and place them at random locations. [sent-72, score-0.341]
</p><p>30 While this is a poor generative model for natural images, it is much better suited to biological images which are composed of many repeating and seemingly randomly distributed elements of a few different types. [sent-73, score-0.528]
</p><p>31 In general, we expect the repeating elements in a biological image to have similar appearances to a ﬁrst approximation, but patterned variability is unavoidable. [sent-76, score-0.466]
</p><p>32 A better model of the image of a single cell might be to assume it was generated by combining a few different prototypes with different coefﬁcients, effectively interpolating between the prototypes. [sent-77, score-0.432]
</p><p>33 We group the prototypes related to a single object into blocks and every image is formed by activating a small number of such blocks. [sent-78, score-0.313]
</p><p>34 This property of sparse block coding makes it valuable in making hard assignments of inferred cell locations, rather than giving a continuous coefﬁcient for each location. [sent-83, score-0.52]
</p><p>35 Closer to our formulation, [8] have used a similar sparse block coding model on natural movie patches and added a temporal smoothness prior on the activation probabilities of blocks in consecutive movie frames. [sent-84, score-0.487]
</p><p>36 The expensive variational iterative techniques used by [8] for inference and learning in small image patches are computationally infeasible for the convolutional model of large images we present here. [sent-85, score-0.692]
</p><p>37 Instead, we use a convolutional block pursuit technique which is an extension of standard matching pursuit and has similarly low computational complexity even for arbitrarily large blocks and arbitrarily large images. [sent-86, score-0.989]
</p><p>38 1 Convolutional sparse block coding Following [8], we distinguish between identity and attribute variables in the generative model of each object in an image. [sent-88, score-0.429]
</p><p>39 An object can be a cell, a cell fragment or any other spatially-localized object. [sent-89, score-0.312]
</p><p>40 Identity variables hk , where (x, y) is the location of the object and k the type of object, xy are Bernoulli-distributed with very small prior probabilities. [sent-90, score-0.443]
</p><p>41 In the generative model xy these attributes are given a broad uniform probability and specify the coefﬁcients with which a set of basis functions Akl are combined at spatial location (x, y) before being linearly combined with objects generated at other locations. [sent-92, score-0.592]
</p><p>42 2 Inference by convolutional block pursuit Given a set of basis functions Akl and an image y, we would like to infer the most likely locations of objects of each type in an image. [sent-99, score-1.051]
</p><p>43 This inference is generally NP-hard but good solutions can nonetheless be obtained with greedy methods like matching pursuit (MP). [sent-100, score-0.34]
</p><p>44 Due to the quadratic nature of equation 1, for a proposal hk = 1 we can easily compute the MAP estimate for each xk given the current residual xy xy Akl ∗ xkl ◦ hk . [sent-103, score-0.89]
</p><p>45 Here we understand xk as a vector concatenating xkl for xy xy  image yres = y − k,l  all l. [sent-104, score-0.932]
</p><p>46 The MAP estimate for xk is xy ˆ xy xk = (Ak )T Ak k ¯ v (l) = Akl ∗ yres xy  −1  k vxy  xy  ¯ where Akl is the basis function Akl rotated by 180 degrees and the matrix Ak contains as columns the vectorized basis functions Akl . [sent-105, score-1.409]
</p><p>47 The corresponding increase in likelihood in equation 1 is T  δLk = xy  k ˆ xy vxy xkl p . [sent-106, score-0.716]
</p><p>48 Inference stops when the activation penalty log  A simple trick common to all matching pursuit algorithms [9], [6] allows us to save computation ¯ when sequentially calculating vklxy = Akl ∗ yres by keeping track of v and updating it after each new coefﬁcient is turned on: ˆ xy vnew = v − G(. [sent-108, score-0.724]
</p><p>49 xy) xk , where G is the grand Gram matrix of all basis functions Akl at all positions (x, y), and the indexing xy means that every dot runs over all possible values of that index. [sent-113, score-0.397]
</p><p>50 We also keep ˆ track during inference of x and δLk and only need to update these quantities at positions (x, y) xy around the extracted object. [sent-116, score-0.41]
</p><p>51 These caching techniques make the complexity of the inference scale linearly with the number of objects in each image, regardless of image or object size. [sent-117, score-0.353]
</p><p>52 Every iteration of block pursuit requires updating v, x and δLk locally around the extracted xy 2  In other words, the convolution uses “zero-padding”. [sent-121, score-0.748]
</p><p>53 However, this cost is also negligible compared to the cost of ﬁnding the best block at each iteration: the single most intensive operation during inference is the loop through all the elements in all the convolutional maps to ﬁnd the block which most increases the likelihood if activated. [sent-123, score-0.77]
</p><p>54 In practice for the datasets we use (for example, 18 images of 256 by 256 pixels each), a model can be learned in minutes on a modern CPU and inference on a single large image takes under one second. [sent-125, score-0.45]
</p><p>55 3  Learning with block K-SVD  Given the inferred active blocks and their coefﬁcients, we would like to adapt the parameters of the basis functions Akl so as to maximize the cost function in eq 1. [sent-127, score-0.423]
</p><p>56 We considered the option of estimating the subspaces in each Ak sequentially where we run a couple of iterations of learning with a single subspace in each Ak and then every couple of iterations we increase the number of subspaces we estimate for Ak . [sent-131, score-0.327]
</p><p>57 In every iteration of K-SVD, coefﬁcients are extracted for all the image patches in the training set. [sent-137, score-0.314]
</p><p>58 At every iteration of K-SVD, given a set of active basis functions per image obtained with an inference method, the objective is to minimize the reconstruction cost with respect to the basis functions and coefﬁcients simultaneously [10]. [sent-140, score-0.502]
</p><p>59 We consider each basis function Akl sequentially, extract all image patches {yi }i where that basis function is active and assume all coefﬁcients for the other basis functions are ﬁxed. [sent-141, score-0.601]
</p><p>60 In the convolutional setting, these patches are extracted from locations in the images where each basis function is active [6]. [sent-142, score-0.755]
</p><p>61 The new reconstructions for each patch yi are yi − Akl (Akl )T yi and with this new residual we move on to the next basis function to be reestimated. [sent-145, score-0.355]
</p><p>62 By analogy, in block K-SVD we are given a set of active blocks per image, each block consisting of K basis functions. [sent-146, score-0.682]
</p><p>63 We consider each block Ak sequentially, extract all image patches {yi }i where that block is active and assume all coefﬁcients for the other blocks are ﬁxed. [sent-147, score-0.794]
</p><p>64 On a more technical note, after each iteration of K-SVD we centered the parameters spatially so that the center of mass of the ﬁrst direction of variability in each block was aligned to the center of its window, otherwise the basis functions did not center by themselves. [sent-151, score-0.425]
</p><p>65 This is because at the step of re-estimating a block Ak from a set of patches {yi }i , some of these patches may be spatially overlapping in the full image. [sent-153, score-0.445]
</p><p>66 Typical failure modes of learning with non-incremental gradient descent and block K-SVD, respectively. [sent-164, score-0.34]
</p><p>67 but in our images many cell pairs touch and would in fact require overlapping windows. [sent-168, score-0.397]
</p><p>68 Instead, we decided to ﬁne-tune the parameters returned by block K-SVD with a few iterations of gradient descent which worked well in practice and in simulations recovered good model parameters with little further computational effort. [sent-169, score-0.337]
</p><p>69 1 Qualitative results on ﬂuorescent images of neurons The main applications of our work are to nissl-stained slices and to ﬁelds of neurons and neuropil imaged with a two-photon microscope (ﬁgure 1(a)). [sent-171, score-0.4]
</p><p>70 While the mice were either anesthetized or awake, their whiskers were stimulated which activated corresponding barrel cortex neurons, leading to an inﬂux of calcium into the cells and consequently an increase in ﬂuorescence which was reported by the two-photon microscope. [sent-173, score-0.413]
</p><p>71 Although cell somas receive a large inﬂux of calcium, dendrites and axons can also be seen. [sent-174, score-0.329]
</p><p>72 In practice, cell locations can be identiﬁed based on the mean images recorded over the duration of an entire experiment, in our case 1000 or 5000 frames. [sent-177, score-0.468]
</p><p>73 Note how within a block each of the two objects includes dimensions of variability that capture anisotropies in the shape of the cell or dendritic fragments. [sent-181, score-0.6]
</p><p>74 Figure 3(a) shows in alternating odd rows patches from the training set identiﬁed by the algorithm to contain cells and the respective reconstructions in the even rows. [sent-182, score-0.377]
</p><p>75 Figure 2(c) shows a typical failure for gradient based learning that motivated us to use incremental block learning. [sent-184, score-0.338]
</p><p>76 2 Simulated data We ran extensive experiments on simulated data to assess the algorithm’s ability to learn and infer cell locations. [sent-187, score-0.319]
</p><p>77 Patches from the GCaMP6 training images (odd rows) and their reconstructions (even rows) with the subspaces shown in ﬁgure 2(b). [sent-193, score-0.359]
</p><p>78 Supplemental ﬁgure 6 shows a simulated image and it can be seen to resemble images in the training set. [sent-198, score-0.386]
</p><p>79 1 Inference quality of convolutional block pursuit We kept the ground truths for the simulated dataset and investigated how well we can recover cell locations when we know perfectly what the simulation parameters were. [sent-203, score-1.045]
</p><p>80 We varied this parameter and report ROC curves for true positives and false positives as we vary the number of extracted coefﬁcients. [sent-205, score-0.562]
</p><p>81 Sometimes we observed that cells were identiﬁed not exactly at the correct location but one or a few pixels away. [sent-206, score-0.331]
</p><p>82 Such small deviations are acceptable in practice, so we considered inferred cells as correctly identiﬁed if they were within four pixels of the correct location (cells were 8-16 pixels in diameter). [sent-207, score-0.41]
</p><p>83 Figure 4(a) reports the typical performance of convolutional block pursuit. [sent-210, score-0.448]
</p><p>84 Using a single subspace per object is equivalent to matching pursuit, achieved signiﬁcantly worse performance and saturated at a smaller number of true positives because the model could not recognize some of the variations in cell shape. [sent-212, score-0.623]
</p><p>85 We also asked how well the model recovers the parameters when the true number of objects per image is unknown, by running several experiments with different mean numbers of objects per image. [sent-222, score-0.431]
</p><p>86 Inference with block pursuit with all three subspaces per object (B3P) as well as block pursuit with only the ﬁrst or ﬁrst two principal subspaces (B1P and B2P). [sent-228, score-1.254]
</p><p>87 Notice the small number of false negatives when a large proportion of the cells are identiﬁed. [sent-230, score-0.317]
</p><p>88 The cells not identiﬁed were too dim to pick out even with a large number of false negative, hence the quick saturation of the ROC curve. [sent-231, score-0.317]
</p><p>89 3 Comparison with human segmentation on biological images We compare the segmentation of the model with manual segmentations on one example each of the GCaMP6 and Nissl-stained images (ﬁgures 4(d) and 4(e)). [sent-238, score-0.603]
</p><p>90 We found that using multiple templates per block helped the model agree more with the human segmentations. [sent-241, score-0.351]
</p><p>91 In the case of the Nissl-stain, block coding with four templates identiﬁed ﬁfty more cells than matching pursuit. [sent-242, score-0.621]
</p><p>92 In addition, a post-hoc analysis suggests that many of the model’s false positives are in fact cells that were not selected in the manual segmentations. [sent-244, score-0.552]
</p><p>93 As we anticipated in the introduction, a standard method based on thresholded and localized correlation maps only reached 25 true positives at 50 false positives and is not shown in ﬁgure 4(d). [sent-246, score-0.499]
</p><p>94 4  Conclusions  We have presented an image model that can be used to automatically and effectively infer the locations and shapes of cells from biological image data. [sent-247, score-0.741]
</p><p>95 This application of generative image models is to our knowledge novel and should allow automating many types of biological studies. [sent-248, score-0.409]
</p><p>96 Our contribution to the image modelling literature is to extend the sparse block coding model presented in [8] to the convolutional setting where each block is allowed to be present at any location in an image. [sent-249, score-0.911]
</p><p>97 We also derived convolutional block pursuit, a greedy inference algorithm which scales gracefully to images of large dimensions with many possible object types in the generative model. [sent-250, score-0.849]
</p><p>98 On simulated data, convolutional block pursuit recovers with good accuracy cell locations in simulated biological images and the learning rule recovers well and consistently the parameters of the generative model. [sent-253, score-1.628]
</p><p>99 Using the block pursuit algorithm recovers signiﬁcantly more cells than simple matching pursuit. [sent-254, score-0.79]
</p><p>100 On data from calcium imaging experiments and nissl-stained tissue, the model succeeds in recovering cell locations and learns good models of the variability among different cell shapes. [sent-255, score-0.78]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('akl', 0.33), ('xy', 0.256), ('cell', 0.243), ('convolutional', 0.226), ('cells', 0.222), ('block', 0.222), ('pursuit', 0.207), ('positives', 0.202), ('tissue', 0.184), ('xkl', 0.163), ('image', 0.156), ('images', 0.154), ('subspaces', 0.145), ('biological', 0.136), ('ak', 0.136), ('calcium', 0.119), ('gure', 0.112), ('basis', 0.102), ('uorescent', 0.1), ('false', 0.095), ('patches', 0.095), ('hk', 0.088), ('generative', 0.083), ('pixels', 0.079), ('coef', 0.077), ('cients', 0.076), ('simulated', 0.076), ('organelles', 0.073), ('matching', 0.072), ('locations', 0.071), ('object', 0.069), ('variability', 0.068), ('objects', 0.067), ('recovers', 0.067), ('mp', 0.067), ('repeating', 0.067), ('extracted', 0.063), ('yres', 0.062), ('inference', 0.061), ('svd', 0.061), ('reconstructions', 0.06), ('activation', 0.06), ('gd', 0.059), ('roc', 0.055), ('blocks', 0.055), ('coding', 0.055), ('confound', 0.054), ('neuropil', 0.054), ('spatial', 0.054), ('yi', 0.051), ('dendrites', 0.05), ('templates', 0.05), ('composed', 0.049), ('identi', 0.049), ('motifs', 0.047), ('uorescence', 0.047), ('cortical', 0.045), ('neurons', 0.045), ('active', 0.044), ('descent', 0.043), ('human', 0.042), ('segmentation', 0.042), ('klxy', 0.041), ('somata', 0.041), ('stereotypical', 0.041), ('vxy', 0.041), ('incremental', 0.041), ('patch', 0.04), ('xk', 0.039), ('failure', 0.039), ('elements', 0.039), ('slices', 0.038), ('sequentially', 0.037), ('per', 0.037), ('gradient', 0.036), ('imaging', 0.036), ('confounds', 0.036), ('dozen', 0.036), ('axons', 0.036), ('barrel', 0.036), ('dendrite', 0.036), ('activated', 0.036), ('recovered', 0.036), ('types', 0.034), ('prototypes', 0.033), ('isa', 0.033), ('marius', 0.033), ('microscope', 0.033), ('dictionary', 0.033), ('manual', 0.033), ('spatially', 0.033), ('imaged', 0.031), ('lk', 0.031), ('location', 0.03), ('track', 0.03), ('ucl', 0.03), ('gregor', 0.03), ('maneesh', 0.03), ('visible', 0.03), ('automated', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="114-tfidf-1" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>2 0.16620187 <a title="114-tfidf-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.13744418 <a title="114-tfidf-3" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>4 0.12820706 <a title="114-tfidf-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.1236204 <a title="114-tfidf-5" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>6 0.12150063 <a title="114-tfidf-6" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>7 0.12143356 <a title="114-tfidf-7" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>8 0.11618991 <a title="114-tfidf-8" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>9 0.1144105 <a title="114-tfidf-9" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>10 0.10491367 <a title="114-tfidf-10" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>11 0.09955541 <a title="114-tfidf-11" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>12 0.099168286 <a title="114-tfidf-12" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>13 0.092408448 <a title="114-tfidf-13" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>14 0.091326222 <a title="114-tfidf-14" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>15 0.091238871 <a title="114-tfidf-15" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>16 0.091064282 <a title="114-tfidf-16" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>17 0.085448325 <a title="114-tfidf-17" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>18 0.083239093 <a title="114-tfidf-18" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>19 0.08039619 <a title="114-tfidf-19" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>20 0.080230705 <a title="114-tfidf-20" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.115), (2, -0.132), (3, -0.061), (4, -0.029), (5, -0.079), (6, -0.086), (7, 0.004), (8, -0.049), (9, 0.015), (10, -0.1), (11, 0.068), (12, 0.042), (13, 0.023), (14, -0.095), (15, 0.015), (16, -0.072), (17, -0.139), (18, -0.106), (19, -0.015), (20, 0.012), (21, 0.011), (22, -0.001), (23, -0.056), (24, -0.006), (25, -0.043), (26, 0.046), (27, 0.037), (28, -0.004), (29, 0.028), (30, 0.039), (31, 0.13), (32, 0.037), (33, 0.162), (34, -0.055), (35, 0.067), (36, 0.098), (37, 0.06), (38, 0.128), (39, -0.045), (40, -0.01), (41, -0.008), (42, 0.036), (43, 0.035), (44, -0.043), (45, -0.084), (46, -0.022), (47, 0.003), (48, 0.114), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96727574 <a title="114-lsi-1" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>2 0.75356978 <a title="114-lsi-2" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>3 0.71947706 <a title="114-lsi-3" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>4 0.68014497 <a title="114-lsi-4" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>Author: Haichao Zhang, David Wipf</p><p>Abstract: Typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point. Consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatiallyvarying or non-uniform blur operator. Using ideas from Bayesian inference and convex analysis, this paper derives a simple non-uniform blind deblurring algorithm with a spatially-adaptive image penalty. Through an implicit normalization process, this penalty automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted. Remaining regions with modest blur and revealing edges therefore dominate on average without explicitly incorporating structureselection heuristics. The algorithm can be implemented using an optimization strategy that is virtually tuning-parameter free and simpler than existing methods, and likely can be applied in other settings such as dictionary learning. Detailed theoretical analysis and empirical comparisons on real images serve as validation.</p><p>5 0.66822469 <a title="114-lsi-5" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>Author: Vikash Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Josh Tenenbaum</p><p>Abstract: The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difﬁcult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that deﬁne ﬂexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs (GPGP) consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer’s output and the data, and latent variables that adjust the ﬁdelity of the renderer and the tolerance of the likelihood. Representations and algorithms from computer graphics are used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on generalpurpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and yields accurate, approximately Bayesian inferences about real-world images. 1</p><p>6 0.66325343 <a title="114-lsi-6" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>7 0.65407109 <a title="114-lsi-7" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>8 0.63212007 <a title="114-lsi-8" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>9 0.6126501 <a title="114-lsi-9" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>10 0.59716505 <a title="114-lsi-10" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>11 0.58511597 <a title="114-lsi-11" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>12 0.57253814 <a title="114-lsi-12" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>13 0.57196206 <a title="114-lsi-13" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>14 0.57045579 <a title="114-lsi-14" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>15 0.55516517 <a title="114-lsi-15" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>16 0.55139852 <a title="114-lsi-16" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>17 0.53882837 <a title="114-lsi-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.52939773 <a title="114-lsi-18" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>19 0.52778172 <a title="114-lsi-19" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>20 0.52359945 <a title="114-lsi-20" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (16, 0.042), (33, 0.136), (34, 0.131), (36, 0.011), (41, 0.04), (49, 0.054), (56, 0.092), (70, 0.085), (73, 0.167), (85, 0.032), (89, 0.04), (93, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88966364 <a title="114-lda-1" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>Author: Özgür1 Şimşek</p><p>Abstract: Several attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule. This research has identiﬁed three environmental structures that aid heuristics: dominance, cumulative dominance, and noncompensatoriness. This paper develops these ideas further and examines their empirical relevance in 51 natural environments. The results show that all three structures are prevalent, making it possible for simple rules to reach, and occasionally exceed, the accuracy of the linear decision rule, using less information and less computation. 1</p><p>2 0.88964856 <a title="114-lda-2" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Speciﬁcally, by using auxiliary natural images, we train a stacked denoising autoencoder ofﬂine to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from ofﬂine training to the online tracking process. Online tracking involves a classiﬁcation neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classiﬁcation layer. Both the feature extractor and the classiﬁer can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU). 1</p><p>same-paper 3 0.86278003 <a title="114-lda-3" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>4 0.83155882 <a title="114-lda-4" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>5 0.80158478 <a title="114-lda-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.79884702 <a title="114-lda-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.79847038 <a title="114-lda-7" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>8 0.79643637 <a title="114-lda-8" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>9 0.79275537 <a title="114-lda-9" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>10 0.79171568 <a title="114-lda-10" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>11 0.79008204 <a title="114-lda-11" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>12 0.78986067 <a title="114-lda-12" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>13 0.78870362 <a title="114-lda-13" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>14 0.78793097 <a title="114-lda-14" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>15 0.78361011 <a title="114-lda-15" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>16 0.78217959 <a title="114-lda-16" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>17 0.78056329 <a title="114-lda-17" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>18 0.77997279 <a title="114-lda-18" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>19 0.77889651 <a title="114-lda-19" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>20 0.77775967 <a title="114-lda-20" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
