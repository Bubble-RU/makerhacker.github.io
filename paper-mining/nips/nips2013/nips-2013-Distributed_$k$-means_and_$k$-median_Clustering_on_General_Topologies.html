<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-94" href="#">nips2013-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</h1>
<br/><p>Source: <a title="nips-2013-94-pdf" href="http://papers.nips.cc/paper/5096-distributed-k-means-and-k-median-clustering-on-general-topologies.pdf">pdf</a></p><p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>Reference: <a title="nips-2013-94-reference" href="../nips2013_reference/nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. [sent-6, score-1.139]
</p><p>2 We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. [sent-7, score-1.326]
</p><p>3 1  Introduction  Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12]. [sent-9, score-0.458]
</p><p>4 Additionally, most of these algorithms assume that the distributed nodes can communicate with all other sites or that there is a central coordinator that communicates with all other sites. [sent-16, score-0.364]
</p><p>5 In this paper, we study the problem of distributed clustering where the data is distributed across nodes whose communication is restricted to the edges of an arbitrary graph. [sent-17, score-0.554]
</p><p>6 We provide algorithms with small communication cost and provable guarantees on the clustering quality. [sent-18, score-0.473]
</p><p>7 Our technique for reducing communication in general graphs is based on the construction of a small set of points which act as a proxy for the entire data set. [sent-19, score-0.355]
</p><p>8 An -coreset is a weighted set of points whose cost on any set of centers is approximately the cost of the original data on those same centers up to accuracy . [sent-20, score-0.623]
</p><p>9 Thus an approximate solution for the coreset is also an approximate solution for the original data. [sent-21, score-0.831]
</p><p>10 [23]  5  (b) Our Construction  Figure 1: (a) Each node computes a coreset on the weighted pointset for its own data and its subtrees’ coresets. [sent-24, score-0.867]
</p><p>11 Communicating the approximate cost of a global solution to each node is enough for the local construction, leading to low communication cost overall. [sent-27, score-0.718]
</p><p>12 More precisely, in Section 3, we propose a distributed coreset construction algorithm based on local approximate solutions. [sent-29, score-1.051]
</p><p>13 Each node computes an approximate solution for its local data, and then constructs the local portion of a coreset using only its local data and the total cost of each node’s ap˜ proximation. [sent-30, score-1.306]
</p><p>14 For constant, this builds a coreset of size O(kd+nk) for k-median and k-means when the data lies in d dimensions and is distributed over n sites. [sent-31, score-0.896]
</p><p>15 If there is a central coordinator among the n sites, then clustering can be performed on the coordinator by collecting the local portions of ˜ the coreset with a communication cost equal to the coreset size O(kd + nk). [sent-32, score-2.337]
</p><p>16 For distributed clustering over general connected topologies, we propose an algorithm based on the distributed coreset construction and a message-passing approach, whose communication cost improves over previous coreset-based algorithms. [sent-33, score-1.545]
</p><p>17 For a ﬁxed amount of communication, our algorithm outperforms other coreset construction algorithms. [sent-36, score-0.852]
</p><p>18 Comparison to Other Coreset Algorithms: Since coresets summarize local information they are a natural tool to use when trying to reduce communication complexity. [sent-37, score-0.517]
</p><p>19 If each node constructs an coreset on its local data, then the union of these coresets is clearly an -coreset for the entire data set. [sent-38, score-1.217]
</p><p>20 Unfortunately the size of the coreset in this approach increases greatly with the number of nodes. [sent-39, score-0.785]
</p><p>21 Its main idea is to approximate the union of local coresets with another coreset. [sent-41, score-0.351]
</p><p>22 They assume nodes communicate over a rooted tree, with each node passing its coreset to its parent. [sent-42, score-0.94]
</p><p>23 Because the approximation factor of the constructed coreset depends on the quality of its component coresets, the accuracy a coreset needs (and thus the overall communication complexity) scales with the height of this tree. [sent-43, score-1.798]
</p><p>24 Although it is possible to ﬁnd a spanning tree in any communication network, when the graph has large diameter every tree has large height. [sent-44, score-0.352]
</p><p>25 We show that it is possible to construct a global coreset with low communication overhead. [sent-46, score-1.008]
</p><p>26 This is done by distributing the coreset construction procedure rather than combining local coresets. [sent-47, score-0.913]
</p><p>27 The communication needed to construct this coreset is negligible – just a single value from each data set representing the approximate cost of their local optimal clustering. [sent-48, score-1.23]
</p><p>28 Since the sampled global -coreset is the same size as any local -coreset, this leads to an improvement of the communication cost over the other approaches. [sent-49, score-0.474]
</p><p>29 The constructed coreset is smaller by a factor of n in general graphs, and is independent of the communication topology. [sent-51, score-0.979]
</p><p>30 [9] also merge coresets using coreset construction, but they do so in a model of parallel computation and ignore communication costs. [sent-53, score-1.213]
</p><p>31 Here we measure the communication cost in number of points transmitted, and assume for simplicity that there is no latency in the communication. [sent-70, score-0.415]
</p><p>32 The goal is to ﬁnd a set of k centers x which optimize cost(P, x) while keeping the computation efﬁcient and the communication cost as low as possible. [sent-72, score-0.475]
</p><p>33 Our focus is to reduce the communication cost while preserving theoretical guarantees for approximating clustering cost. [sent-73, score-0.473]
</p><p>34 In the centralized setting, the idea of summarization with respect to the clustering task is captured by the concept of coresets [13, 8]. [sent-76, score-0.399]
</p><p>35 A coreset is a set of weighted points whose cost approximates the cost of the original data for any set of k centers. [sent-77, score-1.184]
</p><p>36 An -coreset for a set of points P with respect to a center-based cost function is a set of points S and a set of weights w : S → R such that for any set of centers x, we have (1 − )cost(P, x) ≤ p∈S w(p)cost(p, x) ≤ (1 + )cost(P, x). [sent-79, score-0.373]
</p><p>37 In the centralized setting, many coreset construction algorithms have been proposed for k-median, k-means and some other cost functions. [sent-80, score-1.062]
</p><p>38 For example, for points in Rd , algorithms in [8] construct ˜ ˜ coresets of size O(kd/ 4 ) for k-means and coresets of size O(kd/ 2 ) for k-median. [sent-81, score-0.513]
</p><p>39 In the distributed setting, it is natural to ask whether there exists an algorithm that constructs a small coreset for the entire point set but still has low communication cost. [sent-82, score-1.16]
</p><p>40 Note that the union of coresets for multiple data sets is a coreset for the union of the data sets. [sent-83, score-1.061]
</p><p>41 The immediate construction of combining the local coresets from each node would produce a global coreset whose size was larger by a factor of n, greatly increasing the communication complexity. [sent-84, score-1.45]
</p><p>42 We present a distributed algorithm which constructs a global coreset the same size as the centralized construction and only needs a single value1 communicated to each node. [sent-85, score-1.109]
</p><p>43 3  Distributed Coreset Construction  Here we design a distributed coreset construction algorithm for k-means and k-median. [sent-87, score-0.962]
</p><p>44 To gain some intuition on the distributed coreset construction algorithm, we brieﬂy review the construction algorithm in [8] in the centralized setting. [sent-89, score-1.091]
</p><p>45 The coreset is constructed by computing a constant approximation solution for the entire data set, and then sampling points proportional to their contributions to the cost of this solution. [sent-90, score-1.05]
</p><p>46 3  Algorithm 1 Communication aware distributed coreset construction Input: Local datasets {Pi , 1 ≤ i ≤ n}, parameter t (number of points to be sampled). [sent-95, score-0.993]
</p><p>47 We show that a global coreset can be constructed in a distributed fashion by estimating the weight of the entire data set with the sum of local approximations. [sent-104, score-1.002]
</p><p>48 The size of the coreset is O( 1 (kd+log 1 )+nk log nk ) for k-means, and O( 1 (kd+ 4 2 δ δ 1 log δ ) + nk) for k-median. [sent-108, score-0.893]
</p><p>49 As described below, the distributed coreset construction can be achieved by using Algorithm 1 with appropriate t, namely O( 1 (kd + log 1 ) + nk log nk ) for k-means and O( 1 (kd + log 1 )) for k4 2 δ δ δ median. [sent-110, score-1.174]
</p><p>50 Then for with probability z∈P mz any f ∈ F , the expectation of the weighted cost of S equals the cost of the original data P , since E q∈S wq f (q) = q∈S E[wq f (q)] = q∈S p∈P Pr[q = p]wp f (p) = p∈P f (p). [sent-119, score-0.641]
</p><p>51 We ﬁrst consider the centralized setting and review how [8] applied the lemma to construct a coreset for k-median as in Deﬁnition 1. [sent-132, score-0.864]
</p><p>52 A natural approach is to apply this lemma directly to the cost fx (p) := cost(p, x). [sent-133, score-0.343]
</p><p>53 Aiming to approximate 4  the error p [cost(p, x) − cost(bp , x)] rather than to approximate p cost(p, x) directly, we deﬁne fx (p) := cost(p, x) − cost(bp , x) + cost(p, bp ), where cost(p, bp ) is added so that fx (p) ≥ 0. [sent-137, score-0.662]
</p><p>54 Since 0 ≤ fx (p) ≤ 2cost(p, bp ), we can apply the lemma with mp = 2cost(p, bp ). [sent-138, score-0.645]
</p><p>55 It bounds the difference | p∈P fx (p) − q∈S wq fx (q)| by 2 p∈P cost(p, bp ), so we have an O( )-approximation. [sent-139, score-0.689]
</p><p>56 Note that p∈P fx (p) − q∈S wq fx (q) does not equal p∈P cost(p, x) − q∈S wq cost(q, x). [sent-140, score-0.744]
</p><p>57 However, it equals the difference between p∈P cost(p, x) and a weighted cost of the sampled points and the centers in the approximation solution. [sent-141, score-0.381]
</p><p>58 To get a coreset as in Deﬁnition 1, we need to add the centers of the approximation solution with speciﬁc weights to the coreset. [sent-142, score-0.933]
</p><p>59 Our key contribution in this paper is to show that in the distributed setting, it sufﬁces to choose bp from the local approximation solution for the local dataset containing p, rather than from an approximation solution for the global dataset. [sent-144, score-0.538]
</p><p>60 Furthermore, the sampling and the weighting of the coreset points can be done in a local manner. [sent-145, score-0.891]
</p><p>61 We have the following lemma for k-median with F = {fx : fx (p) = d(p, x) − d(bp , x) + d(p, bp ), x ∈ (Rd )k }. [sent-147, score-0.348]
</p><p>62 2 δ Proof Sketch of Lemma 2: We want to show that for any set of centers x the true cost for using these centers is well approximated by the cost on the weighted coreset. [sent-150, score-0.578]
</p><p>63 Note that our coreset has two z∈P m types of points: sampled points q ∈ S = ∪n Si with weight wq := mq |S| z and local solution i=1 centers b ∈ B = ∪n Bi with weight wb := |Pb | − q∈S∩Pb wq . [sent-151, score-1.509]
</p><p>64 As mentioned above, we construct fx (p) to be the difference between the cost of p and the cost of bp so that Lemma 1 can be applied. [sent-154, score-0.641]
</p><p>65 Note that the centers are weighted such that b∈B wb d(b, x) = b∈B |Pb |d(b, x) − b∈B q∈S∩Pb wq d(b, x) = p∈P d(bp , x) − wq d(bq , x). [sent-155, score-0.624]
</p><p>66 Taken together with the fact that p∈P mp = wq mq , we can show q∈S q∈S that  p∈P  d(p, x) −  q∈S∪B  wq d(q, x) =  p∈P  fx (p) −  q∈S  wq fx (q) . [sent-156, score-1.118]
</p><p>67 Note that 0 ≤  fx (p) ≤ 2d(p, bp ) by triangle inequality, and S is sufﬁciently large and chosen according to weights mp = d(p, bp ), so the conditions of Lemma 1 are met. [sent-157, score-0.65]
</p><p>68 Therefore, by Lemma 2, when |S| ≥ O 1 (kd + log 1 ) , the weighted cost of S ∪ B approximates the k-median cost of P for any set 2 δ of centers, then (S ∪ B, w) becomes an -coreset for P . [sent-160, score-0.386]
</p><p>69 The total communication cost is bounded by O(mn), since even in the most general case that every node only knows its neighbors, we can broadcast the local costs with O(mn) communication (see Algorithm 3). [sent-161, score-0.738]
</p><p>70 It also bounds (C), noting that E[ q∈Pb ∩S wq ] = |Pb |, and thus q∈Pb ∩S wq ≤ 2|Pb | when t ≥ O(nk log nk ). [sent-181, score-0.548]
</p><p>71 4  Effect of Network Topology on Communication Cost  If there is a central coordinator in the communication graph, then we can run distributed coreset construction algorithm and send the local portions of the coreset to the coordinator, which can perform the clustering task. [sent-183, score-2.297]
</p><p>72 The total communication cost is just the size of the coreset. [sent-184, score-0.37]
</p><p>73 We propose to use a message passing approach for collecting information for coreset construction and sharing the local portions of the coreset. [sent-186, score-1.01]
</p><p>74 Since each piece of the coreset is shared at most twice across any particular edge in message passing, we have Theorem 2. [sent-188, score-0.796]
</p><p>75 The communication cost is O(m( 1 (kd + log 1 ) + nk log nk )) for k-means, and O(m( 1 (kd + log 1 ) + nk)) for k-median. [sent-190, score-0.596]
</p><p>76 4 2 δ δ δ In contrast, an approach where each node constructs an -coreset for k-means and sends it to the ˜ other nodes incurs communication cost of O( mnkd ). [sent-191, score-0.485]
</p><p>77 4 Our algorithm can also be applied on a rooted tree: we can send the coreset portions to the root which then applies an approximation algorithm. [sent-193, score-0.91]
</p><p>78 Given an α-approximation algorithm for weighted k-means (k-median respectively) as a subroutine, there exists an algorithm that with probability at least 1 − δ outputs a (1 + )αapproximation solution for distributed k-means (k-median respectively) clustering on a rooted tree of height h. [sent-195, score-0.38]
</p><p>79 The total communication cost is O(h( 1 (kd + log 1 ) + nk log nk )) for k-means, and 4 δ δ O(h( 1 (kd + log 1 ) + nk)) for k-median. [sent-196, score-0.596]
</p><p>80 2 δ 4 2 ˜ ˜ Our approach improves the cost of O( nh 4kd ) for k-means and the cost of O( nh 2kd ) for k-median in [23] 2 . [sent-197, score-0.354]
</p><p>81 The algorithm in [23] builds on each node a coreset for the union of coresets from its  2 Their algorithm used coreset construction as a subroutine. [sent-198, score-1.966]
</p><p>82 The construction algorithm they used builds ˜ d coreset of size O( nkh log |P |). [sent-199, score-0.885]
</p><p>83 Throughout this paper, when we compare to [23] we assume they use the coreset construction technique of [8] to reduce their coreset size and communication cost. [sent-200, score-1.817]
</p><p>84 Since the coreset construction subroutine has quadratic dependence on 1/ for k-median (quartic for k-means), the algorithm then has quadratic dependence on h (quartic for k-means). [sent-202, score-0.874]
</p><p>85 Our algorithm does not build coreset on top of coresets, resulting in a better dependence on the height of the tree h. [sent-203, score-0.835]
</p><p>86 5  Experiments  Here we evaluate the effectiveness of our algorithm and compare it to other distributed coreset algorithms. [sent-207, score-0.895]
</p><p>87 We present the k-means cost of the solution by our algorithm with varying communication cost, and compare to those of other algorithms when they use the same amount of communication. [sent-208, score-0.4]
</p><p>88 Experimental Methodology: We ﬁrst generate a communication graph connecting local sites, and then partition the data into local data sets. [sent-211, score-0.434]
</p><p>89 To measure the quality of the coreset generated, we run Lloyd’s algorithm on the coreset and the global data respectively to get two solutions, and compute the ratio between the costs of the two solutions over the global data. [sent-219, score-1.682]
</p><p>90 We compare our algorithm with COMBINE, the method of combining coresets from local data sets, and with the algorithm of [23] (Zhang et al. [sent-221, score-0.337]
</p><p>91 We observe that the algorithms perform well with much smaller coreset sizes than predicted by the theoretical bounds. [sent-226, score-0.771]
</p><p>92 1 cost ratio, the coreset size and thus the communication needed is only 0. [sent-228, score-1.141]
</p><p>93 Our algorithm then saves 10% − 20% communication cost to achieve the same approximation ratio. [sent-237, score-0.409]
</p><p>94 This is due to the fact that their algorithm needs larger coresets to prevent the accumulation of errors when constructing coresets from component coresets, and thus needs higher communication cost to achieve the same approximation ratio. [sent-241, score-0.898]
</p><p>95 4  communication cost  (d) grid graph, similarity-based  2. [sent-296, score-0.397]
</p><p>96 8  ×106  communication cost  (f) preferential graph, degree-based  Figure 2: k-means cost (normalized by baseline) v. [sent-301, score-0.563]
</p><p>97 6  communication cost  (e) grid graph, weighted  2. [sent-361, score-0.441]
</p><p>98 8  communication cost  ×106  (f) preferential graph, degree-based  Figure 3: k-means cost (normalized by baseline) v. [sent-366, score-0.563]
</p><p>99 communication cost over the spanning trees of the graphs. [sent-368, score-0.396]
</p><p>100 An effective coreset compression algorithm for large scale sensor networks. [sent-434, score-0.824]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreset', 0.771), ('coresets', 0.234), ('wq', 0.222), ('communication', 0.208), ('bp', 0.167), ('cost', 0.162), ('fx', 0.15), ('mp', 0.13), ('kd', 0.111), ('distributed', 0.11), ('centers', 0.105), ('clustering', 0.103), ('pb', 0.095), ('coordinator', 0.088), ('nk', 0.086), ('local', 0.075), ('construction', 0.067), ('centralized', 0.062), ('sites', 0.06), ('pi', 0.058), ('node', 0.052), ('mz', 0.051), ('points', 0.045), ('communicate', 0.045), ('weighted', 0.044), ('graph', 0.044), ('constructs', 0.04), ('sensor', 0.039), ('quartic', 0.038), ('vi', 0.038), ('portions', 0.037), ('bi', 0.036), ('ni', 0.034), ('send', 0.034), ('costs', 0.033), ('partition', 0.032), ('lemma', 0.031), ('preferential', 0.031), ('wb', 0.031), ('rooted', 0.029), ('site', 0.029), ('global', 0.029), ('dim', 0.029), ('union', 0.028), ('grid', 0.027), ('tree', 0.027), ('wp', 0.026), ('portion', 0.026), ('symposium', 0.026), ('spanning', 0.026), ('gx', 0.025), ('approximation', 0.025), ('titles', 0.025), ('ri', 0.025), ('message', 0.025), ('balcan', 0.024), ('nodes', 0.023), ('si', 0.023), ('height', 0.023), ('topologies', 0.022), ('mq', 0.022), ('subroutine', 0.022), ('accumulation', 0.021), ('diameter', 0.02), ('triangle', 0.02), ('sketch', 0.02), ('passing', 0.02), ('feldman', 0.019), ('communicates', 0.019), ('zhang', 0.019), ('central', 0.019), ('combine', 0.019), ('objectives', 0.019), ('round', 0.018), ('algo', 0.018), ('topology', 0.018), ('log', 0.018), ('graphs', 0.018), ('transmitted', 0.018), ('solutions', 0.018), ('networks', 0.017), ('ratio', 0.017), ('kannan', 0.017), ('entire', 0.017), ('acm', 0.016), ('solution', 0.016), ('mn', 0.016), ('communicating', 0.016), ('communicated', 0.016), ('proceedings', 0.016), ('weights', 0.016), ('nh', 0.015), ('builds', 0.015), ('collecting', 0.015), ('algorithm', 0.014), ('approximate', 0.014), ('network', 0.014), ('proportional', 0.014), ('annual', 0.014), ('greatly', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="94-tfidf-1" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>2 0.10637575 <a title="94-tfidf-2" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>3 0.08473888 <a title="94-tfidf-3" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>4 0.071161285 <a title="94-tfidf-4" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>Author: Jason Chang, John W. Fisher III</p><p>Abstract: We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve signiﬁcant computational gains. We combine a nonergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two subclusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for ﬁnite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods. 1</p><p>5 0.067817599 <a title="94-tfidf-5" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>6 0.063080288 <a title="94-tfidf-6" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>7 0.0605672 <a title="94-tfidf-7" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>8 0.060044199 <a title="94-tfidf-8" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>9 0.056217127 <a title="94-tfidf-9" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>10 0.048854977 <a title="94-tfidf-10" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>11 0.047528725 <a title="94-tfidf-11" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>12 0.047387797 <a title="94-tfidf-12" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>13 0.044026069 <a title="94-tfidf-13" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>14 0.043973129 <a title="94-tfidf-14" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>15 0.043742966 <a title="94-tfidf-15" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>16 0.043713946 <a title="94-tfidf-16" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>17 0.041735619 <a title="94-tfidf-17" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>18 0.039213695 <a title="94-tfidf-18" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>19 0.038760904 <a title="94-tfidf-19" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>20 0.038141623 <a title="94-tfidf-20" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.022), (2, 0.01), (3, -0.015), (4, 0.043), (5, 0.07), (6, 0.022), (7, -0.046), (8, -0.005), (9, 0.027), (10, 0.037), (11, 0.001), (12, 0.078), (13, 0.038), (14, -0.014), (15, 0.014), (16, -0.025), (17, 0.02), (18, 0.009), (19, 0.034), (20, -0.002), (21, 0.027), (22, -0.002), (23, -0.016), (24, -0.067), (25, -0.039), (26, -0.05), (27, 0.04), (28, -0.034), (29, -0.107), (30, -0.05), (31, 0.001), (32, 0.108), (33, 0.002), (34, 0.083), (35, 0.033), (36, -0.077), (37, 0.001), (38, -0.028), (39, -0.088), (40, -0.006), (41, -0.051), (42, -0.089), (43, 0.042), (44, -0.0), (45, -0.013), (46, 0.016), (47, -0.058), (48, -0.036), (49, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92776173 <a title="94-lsi-1" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>2 0.61671931 <a title="94-lsi-2" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>3 0.60790414 <a title="94-lsi-3" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>4 0.5820505 <a title="94-lsi-4" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>Author: Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman</p><p>Abstract: unkown-abstract</p><p>5 0.57499373 <a title="94-lsi-5" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>6 0.52515018 <a title="94-lsi-6" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>7 0.52301359 <a title="94-lsi-7" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>8 0.52009749 <a title="94-lsi-8" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>9 0.51054692 <a title="94-lsi-9" href="./nips-2013-Accelerated_Mini-Batch_Stochastic_Dual_Coordinate_Ascent.html">19 nips-2013-Accelerated Mini-Batch Stochastic Dual Coordinate Ascent</a></p>
<p>10 0.50366759 <a title="94-lsi-10" href="./nips-2013-Estimation_Bias_in_Multi-Armed_Bandit_Algorithms_for_Search_Advertising.html">112 nips-2013-Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising</a></p>
<p>11 0.5022198 <a title="94-lsi-11" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>12 0.48287034 <a title="94-lsi-12" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>13 0.47630784 <a title="94-lsi-13" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>14 0.47396082 <a title="94-lsi-14" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>15 0.46659574 <a title="94-lsi-15" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>16 0.45538831 <a title="94-lsi-16" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>17 0.44812194 <a title="94-lsi-17" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>18 0.444682 <a title="94-lsi-18" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>19 0.43605775 <a title="94-lsi-19" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>20 0.43530655 <a title="94-lsi-20" href="./nips-2013-Understanding_variable_importances_in_forests_of_randomized_trees.html">340 nips-2013-Understanding variable importances in forests of randomized trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.22), (10, 0.011), (16, 0.063), (33, 0.085), (34, 0.076), (41, 0.048), (49, 0.029), (56, 0.119), (70, 0.032), (85, 0.053), (89, 0.021), (93, 0.1), (95, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80735642 <a title="94-lda-1" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>same-paper 2 0.78704429 <a title="94-lda-2" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>3 0.70643401 <a title="94-lda-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.68935251 <a title="94-lda-4" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>Author: Yu-Xiang Wang, Huan Xu, Chenlei Leng</p><p>Abstract: Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for subspace clustering. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of “Self-Expressiveness”. The main difference is that SSC minimizes the vector 1 norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank, we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the “Self-Expressiveness Property” and “Graph Connectivity” at the same time. 1</p><p>5 0.66436356 <a title="94-lda-5" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>6 0.64295483 <a title="94-lda-6" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>7 0.63369012 <a title="94-lda-7" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>8 0.63317525 <a title="94-lda-8" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>9 0.63285333 <a title="94-lda-9" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>10 0.63208395 <a title="94-lda-10" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>11 0.63113791 <a title="94-lda-11" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>12 0.62940103 <a title="94-lda-12" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>13 0.62783945 <a title="94-lda-13" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>14 0.62764704 <a title="94-lda-14" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>15 0.62530875 <a title="94-lda-15" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>16 0.62521458 <a title="94-lda-16" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>17 0.62451196 <a title="94-lda-17" href="./nips-2013-Accelerated_Mini-Batch_Stochastic_Dual_Coordinate_Ascent.html">19 nips-2013-Accelerated Mini-Batch Stochastic Dual Coordinate Ascent</a></p>
<p>18 0.62414461 <a title="94-lda-18" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>19 0.62231195 <a title="94-lda-19" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>20 0.62170798 <a title="94-lda-20" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
