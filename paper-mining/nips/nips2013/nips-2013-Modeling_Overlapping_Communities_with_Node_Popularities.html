<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 nips-2013-Modeling Overlapping Communities with Node Popularities</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-196" href="#">nips2013-196</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 nips-2013-Modeling Overlapping Communities with Node Popularities</h1>
<br/><p>Source: <a title="nips-2013-196-pdf" href="http://papers.nips.cc/paper/4977-modeling-overlapping-communities-with-node-popularities.pdf">pdf</a></p><p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>Reference: <a title="nips-2013-196-reference" href="../nips2013_reference/nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mmsb', 0.382), ('amp', 0.379), ('za', 0.36), ('nod', 0.353), ('yab', 0.296), ('commun', 0.214), ('ab', 0.176), ('link', 0.17), ('rab', 0.138), ('memb', 0.134), ('elbo', 0.12), ('popul', 0.108), ('netscy', 0.104), ('xab', 0.104), ('network', 0.099), ('blockmodel', 0.095), ('blog', 0.093), ('nonlink', 0.087), ('sab', 0.087), ('assort', 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="196-tfidf-1" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>2 0.2693764 <a title="196-tfidf-2" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>Author: Ryosuke Matsushita, Toshiyuki Tanaka</p><p>Abstract: We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efﬁcient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by reformulating it as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd’s K-means algorithm. 1</p><p>3 0.25068867 <a title="196-tfidf-3" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>4 0.20050949 <a title="196-tfidf-4" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>Author: Myunghwan Kim, Jure Leskovec</p><p>Abstract: unkown-abstract</p><p>5 0.17012614 <a title="196-tfidf-5" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>Author: Tai Qin, Karl Rohe</p><p>Abstract: Spectral clustering is a fast and popular algorithm for ﬁnding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artiﬁcially inﬂate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the “star shape” in the eigenvectors–a common feature of empirical networks–can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justiﬁes several of the variations of the spectral clustering algorithm in terms of these models. 1</p><p>6 0.16348171 <a title="196-tfidf-6" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>7 0.15125576 <a title="196-tfidf-7" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>8 0.125774 <a title="196-tfidf-8" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>9 0.12126192 <a title="196-tfidf-9" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>10 0.110192 <a title="196-tfidf-10" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>11 0.10748068 <a title="196-tfidf-11" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>12 0.10379863 <a title="196-tfidf-12" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>13 0.091863893 <a title="196-tfidf-13" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>14 0.086821318 <a title="196-tfidf-14" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>15 0.08267194 <a title="196-tfidf-15" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>16 0.081705689 <a title="196-tfidf-16" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>17 0.080603421 <a title="196-tfidf-17" href="./nips-2013-Blind_Calibration_in_Compressed_Sensing_using_Message_Passing_Algorithms.html">59 nips-2013-Blind Calibration in Compressed Sensing using Message Passing Algorithms</a></p>
<p>18 0.07947053 <a title="196-tfidf-18" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>19 0.078181364 <a title="196-tfidf-19" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>20 0.075509697 <a title="196-tfidf-20" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.154), (1, 0.04), (2, 0.025), (3, -0.051), (4, -0.088), (5, 0.02), (6, 0.088), (7, 0.092), (8, 0.044), (9, -0.141), (10, 0.108), (11, 0.174), (12, 0.169), (13, -0.085), (14, -0.066), (15, 0.022), (16, -0.172), (17, 0.165), (18, 0.161), (19, -0.079), (20, -0.031), (21, -0.016), (22, -0.055), (23, 0.031), (24, -0.074), (25, -0.092), (26, 0.133), (27, 0.047), (28, 0.054), (29, 0.239), (30, 0.004), (31, 0.066), (32, 0.021), (33, -0.082), (34, -0.045), (35, -0.064), (36, 0.046), (37, -0.012), (38, -0.01), (39, 0.021), (40, -0.109), (41, 0.02), (42, 0.004), (43, 0.051), (44, -0.002), (45, 0.093), (46, 0.021), (47, -0.106), (48, 0.042), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93919009 <a title="196-lsi-1" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>2 0.7971732 <a title="196-lsi-2" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>Author: Dae Il Kim, Prem Gopalan, David Blei, Erik Sudderth</p><p>Abstract: Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efﬁcient structured mean ﬁeld variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show signiﬁcantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government. 1</p><p>3 0.74805915 <a title="196-lsi-3" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>Author: Myunghwan Kim, Jure Leskovec</p><p>Abstract: unkown-abstract</p><p>4 0.68496197 <a title="196-lsi-4" href="./nips-2013-A_Scalable_Approach_to_Probabilistic_Latent_Space_Inference_of_Large-Scale_Networks.html">13 nips-2013-A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks</a></p>
<p>Author: Junming Yin, Qirong Ho, Eric Xing</p><p>Abstract: We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efﬁcient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. 1</p><p>5 0.66469944 <a title="196-lsi-5" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>Author: Tai Qin, Karl Rohe</p><p>Abstract: Spectral clustering is a fast and popular algorithm for ﬁnding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artiﬁcially inﬂate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the “star shape” in the eigenvectors–a common feature of empirical networks–can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justiﬁes several of the variations of the spectral clustering algorithm in terms of these models. 1</p><p>6 0.64837426 <a title="196-lsi-6" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>7 0.57369101 <a title="196-lsi-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.55529308 <a title="196-lsi-8" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>9 0.54199809 <a title="196-lsi-9" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>10 0.54029131 <a title="196-lsi-10" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>11 0.53921425 <a title="196-lsi-11" href="./nips-2013-Bayesian_Hierarchical_Community_Discovery.html">47 nips-2013-Bayesian Hierarchical Community Discovery</a></p>
<p>12 0.52938533 <a title="196-lsi-12" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>13 0.51327765 <a title="196-lsi-13" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>14 0.50965172 <a title="196-lsi-14" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>15 0.48883569 <a title="196-lsi-15" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>16 0.48340246 <a title="196-lsi-16" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>17 0.47261903 <a title="196-lsi-17" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>18 0.46089101 <a title="196-lsi-18" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>19 0.45150435 <a title="196-lsi-19" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>20 0.43357176 <a title="196-lsi-20" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.024), (20, 0.166), (25, 0.107), (37, 0.029), (41, 0.331), (70, 0.036), (80, 0.065), (86, 0.086), (87, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76027566 <a title="196-lda-1" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>same-paper 2 0.72433895 <a title="196-lda-2" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>Author: Prem Gopalan, Chong Wang, David Blei</p><p>Abstract: We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts signiﬁcantly better than the MMSB. 1</p><p>3 0.71383685 <a title="196-lda-3" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>Author: Jie Wang, Jiayu Zhou, Peter Wonka, Jieping Ye</p><p>Abstract: Lasso is a widely used regression technique to ﬁnd sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efﬁciency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efﬁcient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no “exact” screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso. 1</p><p>4 0.70473444 <a title="196-lda-4" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>Author: Ying Liu, Alan Willsky</p><p>Abstract: Gaussian Graphical Models (GGMs) or Gauss Markov random ﬁelds are widely used in many applications, and the trade-off between the modeling capacity and the efﬁciency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity O(k 2 n) using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efﬁcient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or ﬂight networks where the FVS nodes often correspond to a small number of highly inﬂuential nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate with complexity O(kn2 + n2 log n) if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing an inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efﬁcient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank corrections with complexity O(kn2 + n2 log n) per iteration. We perform experiments using both synthetic data as well as real data of ﬂight delays to demonstrate the modeling capacity with FVSs of various sizes. 1</p><p>5 0.63331687 <a title="196-lda-5" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>Author: Josh S. Merel, Roy Fox, Tony Jebara, Liam Paninski</p><p>Abstract: In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user’s neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modiﬁed decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.</p><p>6 0.57445252 <a title="196-lda-6" href="./nips-2013-Robust_Bloom_Filters_for_Large_MultiLabel_Classification_Tasks.html">279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</a></p>
<p>7 0.57375157 <a title="196-lda-7" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>8 0.57325369 <a title="196-lda-8" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>9 0.57287681 <a title="196-lda-9" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>10 0.5728156 <a title="196-lda-10" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>11 0.57209939 <a title="196-lda-11" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>12 0.57198793 <a title="196-lda-12" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>13 0.57163227 <a title="196-lda-13" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>14 0.57112795 <a title="196-lda-14" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>15 0.57103854 <a title="196-lda-15" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>16 0.57055646 <a title="196-lda-16" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>17 0.57012439 <a title="196-lda-17" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>18 0.56994385 <a title="196-lda-18" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>19 0.56950521 <a title="196-lda-19" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>20 0.56873089 <a title="196-lda-20" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
