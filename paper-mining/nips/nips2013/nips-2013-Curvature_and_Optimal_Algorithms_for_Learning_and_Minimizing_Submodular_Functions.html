<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-78" href="#">nips2013-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</h1>
<br/><p>Source: <a title="nips-2013-78-pdf" href="http://papers.nips.cc/paper/4989-curvature-and-optimal-algorithms-for-learning-and-minimizing-submodular-functions.pdf">pdf</a></p><p>Author: Rishabh K. Iyer, Stefanie Jegelka, Jeff A. Bilmes</p><p>Abstract: We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. 1</p><p>Reference: <a title="nips-2013-78-reference" href="../nips2013_reference/nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu †  Abstract We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. [sent-13, score-2.437]
</p><p>2 We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. [sent-14, score-0.926]
</p><p>3 Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. [sent-17, score-1.253]
</p><p>4 , n} is submodular if for all subsets S, T ⊆ V , it holds that f (S) + f (T ) ≥ f (S ∪ T ) + / f (S ∩ T ). [sent-24, score-0.764]
</p><p>5 A function f is submodular if it satisﬁes diminishing marginal returns, namely f (j|S) ≥ f (j|T ) for all S ⊆ T, j ∈ T , and is monotone if f (j|S) ≥ 0 for all j ∈ S, S ⊆ V . [sent-26, score-0.908]
</p><p>6 Given the increasing importance of submodular functions in machine learning, these observations beg the question of qualifying and quantifying properties that make sub-classes of submodular functions more amenable to learning and optimization. [sent-29, score-1.644]
</p><p>7 Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [2, 25], concave functions [7, 18, 24], label cost or covering functions [9, 31]. [sent-30, score-1.101]
</p><p>8 Previously, the concept of curvature has been used to 1  tighten bounds for submodular maximization problems [3, 29]. [sent-33, score-1.283]
</p><p>9 Hence, our results complete a unifying picture of the effect of curvature on submodular problems. [sent-34, score-1.166]
</p><p>10 Moreover, curvature is still a fairly generic concept, as it only depends on the marginal gains of the submodular function. [sent-35, score-1.114]
</p><p>11 It allows a smooth transition between the ‘easy’ functions and the ‘really hard’ subclasses of submodular functions. [sent-36, score-0.822]
</p><p>12 2  Problem statements, deﬁnitions and background  Before stating our main results, we provide some necessary deﬁnitions and introduce a new concept, the curve normalized version of a submodular function. [sent-37, score-0.764]
</p><p>13 Throughout this paper, we assume that the submodular function f is deﬁned on a ground set V of n elements, that it is nonnegative and f (∅) = 0. [sent-38, score-0.791]
</p><p>14 We also use normalized modular (or additive) functions w : 2V → R which are those that can be written as a sum of weights, w(S) = i∈S w(i). [sent-39, score-0.247]
</p><p>15 (Approximation [8]) Given a submodular function f in form of a value oracle, ﬁnd an ˆ approximation f (within polynomial time and representable within polynomial space), such that for ˆ ˆ all X ⊆ V , it holds that f (X) ≤ f (X) ≤ α1 (n)f (X) for a polynomial α1 (n). [sent-41, score-1.02]
</p><p>16 (Constrained optimization [27, 7, 10, 16]) Minimize a submodular function f over a family C of feasible sets, i. [sent-47, score-0.764]
</p><p>17 [8], who approx√ imate any monotone submodular function to within a factor of O( n log n), with a lower bound √ of α1 (n) = Ω( n/ log n). [sent-51, score-1.14]
</p><p>18 Building on this result, Balcan and Harvey [2] show how to PMAC-learn √ a monotone submodular function within a factor of α2 (n) = O( n), and prove a lower bound of Ω(n1/3 ) for the learning problem. [sent-52, score-1.076]
</p><p>19 Better learning results are possible for the subclass of submodular shells [23] and Fourier sparse set functions [26]. [sent-54, score-0.851]
</p><p>20 Constrained submodular minimization arises in applications such as power assignment or transportation problems [19, 30, 13]. [sent-56, score-0.846]
</p><p>21 Here, we will focus on the constraint of imposing a lower bound on the cardinality, and on combinatorial constraints where C is the set of all s-t paths, s-t cuts, spanning trees, or perfect matchings in a graph. [sent-59, score-0.226]
</p><p>22 A central concept in this work is the total curvature κf of a submodular function f and the curvature κf (S) with respect to a set S ⊆ V , deﬁned as [3, 29] κf = 1 − min j∈V  f (j | V \ j) , f (j)  κf (S) = 1 − min j∈S  f (j|S\j) . [sent-60, score-1.486]
</p><p>23 A modular function has curvature κf = 0, and a matroid rank function has maximal curvature κf = 1. [sent-63, score-0.943]
</p><p>24 Conceptually, curvature is distinct from the recently proposed submodularity ratio [5] that measures how far a function is from being submodular. [sent-65, score-0.41]
</p><p>25 Curvature has 1 served to tighten bounds for submodular maximization problems, e. [sent-66, score-0.888]
</p><p>26 , from (1−1/e) to κf (1−e−κf ) for monotone submodular maximization subject to a cardinality constraint [3] or matroid constraints [29], and these results are tight. [sent-68, score-1.037]
</p><p>27 For submodular minimization, learning, and approximation, however, the role of curvature has not yet been addressed (an exception are the upper bounds in [13] for minimization). [sent-69, score-1.217]
</p><p>28 In the following sections, we complete the picture of how curvature affects the complexity of submodular maximization and minimization, approximation, and learning. [sent-70, score-1.163]
</p><p>29 The above-cited lower bounds for Problems 1–3 were established with functions of maximal curvature (κf = 1) which, as we will see, is the worst case. [sent-71, score-0.526]
</p><p>30 An example for functions with κf < 1 is the class of concave over modular functions that have been used in speech processing [22] and computer k vision [17]. [sent-73, score-0.346]
</p><p>31 We call f κ the curve-normalized version of f because its curvature is κf κ = 1. [sent-79, score-0.35]
</p><p>32 The function f κ allows us to decompose a submodular function f into a “difﬁcult” polymatroid function and an “easy” modular part as f (X) = fdifﬁcult (X) + measy (X) where fdifﬁcult (X) = κf f κ (X) and measy (X) = (1 − κf ) j∈X f (j). [sent-80, score-1.07]
</p><p>33 Moreover, we may modulate the curvature of given any function g with κg = 1, by constructing a function f (X) cg(X) + (1 − c)|X| with curvature κf = c but otherwise the same polymatroidal structure as g. [sent-81, score-0.758]
</p><p>34 If f is monotone submodular with κf > 0, then f (X) ≤ (1 − κf ) j∈X f (j). [sent-85, score-0.908]
</p><p>35 The function f κ will be our tool for analyzing the hardness of submodular problems. [sent-91, score-0.787]
</p><p>36 Previous information-theoretic lower bounds for Problems 1–3 [6, 8, 10, 27] are independent of curvature and use functions with κf = 1. [sent-92, score-0.511]
</p><p>37 (3)  Both of these functions have curvature κf . [sent-98, score-0.408]
</p><p>38 This construction enables us to explicitly introduce the effect of curvature into information-theoretic bounds for all monotone submodular functions. [sent-99, score-1.34]
</p><p>39 The curve normalization (2) leads to reﬁned upper bounds for Problems 1–3, while the curvature modulation (3) provides matching lower bounds. [sent-101, score-0.509]
</p><p>40 The following are some of our main results: for approximating submodular functions (Problem 1), we replace the known bound √ √ n log n of α1 (n) = O( n log n) [8] by an improved curvature-dependent O( 1+(√n log n−1)(1−κf ) ). [sent-102, score-1.014]
</p><p>41 For learning submodular functions √ (Problem 2), we reﬁne the known bound of α2 (n) = O( n) [2] in the PMAC setting to a curvature √ n n1/3 ˜ dependent bound of O( 1+(√n−1)(1−κf ) ), with a lower bound of Ω( 1+(n1/3 −1)(1−κf ) ). [sent-104, score-1.358]
</p><p>42 Finally, Table 1 summarizes our curvature-dependent approximation bounds for constrained minimization (Problem 3). [sent-105, score-0.242]
</p><p>43 In general, our new curvaturedependent upper and lower bounds reﬁne known theoretical results whenever κf < 1, in many cases replacing known polynomial bounds by a curvature-dependent constant factor 1/(1 − κf ). [sent-107, score-0.356]
</p><p>44 1  A polymatroid function is a monotone increasing, nonnegative, submodular function satisfying f (∅) = 0. [sent-109, score-0.967]
</p><p>45 3  Approximating submodular functions everywhere  We ﬁrst address improved bounds for the problem of approximating a monotone submodular function everywhere. [sent-114, score-1.857]
</p><p>46 Previous work established α-approximations g to a submodular function f satisfying g(S) ≤ f (S) ≤ αg(S) for all S ⊆ V [8]. [sent-115, score-0.764]
</p><p>47 Note that the curvature of a monotone submodular function can be obtained within 2n + 1 queries to f . [sent-117, score-1.278]
</p><p>48 Given a polymatroid function f with κf < 1, let f κ be its curve-normalized version ˆ ˆ deﬁned in Equation (2), and let f κ be a submodular function satisfying f κ (X) ≤ f κ (X) ≤ κ ˆκ (X), for some X ⊆ V . [sent-123, score-0.823]
</p><p>49 1 may be directly applied to tighten recent results on approximating submodular functions everywhere. [sent-126, score-0.881]
</p><p>50 [8] computes an approximation to a polymatroid function f in polynomial time by approximating the submodular polyhedron via an ellipsoid. [sent-128, score-0.987]
</p><p>51 This √ approximation (which we call the ellipsoidal approximation) satisﬁes α(n) = O( n log n), and has the form wf (X) for a certain weight vector wf . [sent-129, score-0.268]
</p><p>52 Let f be a polymatroid function with κf < 1, and let wf κ (X) be the ellipsoidal approximation to the κ-curve-normalized version f κ (X) of f . [sent-134, score-0.244]
</p><p>53 Then the function f ea (X) = κf wf κ (X) + (1 − κf ) j∈X f (j) satisﬁes √ n log n √ f ea (X) ≤ f (X) ≤ O f ea (X). [sent-135, score-0.632]
</p><p>54 This is not surprising since a modular function can be inferred exactly within O(n) oracle calls. [sent-137, score-0.224]
</p><p>55 Given a submodular function f with curvature κf , there does not exist a (possibly randomized) polynomial-time algorithm that computes an approximation to f within a factor of n1/2− , for any > 0. [sent-143, score-1.279]
</p><p>56 1+(n1/2− −1)(1−κf ) ˆ The simplest alternative approximation to f one might conceive is the modular function f m (X) f (j) which can easily be computed by querying the n values f (j). [sent-144, score-0.272]
</p><p>57 Given a monotone submodular function f , it holds that2 ˆ f (X) ≤ f m (X) =  f (j) ≤ j∈X  2  |X| f (X) 1 + (|X| − 1)(1 − κf (X))  In [12], we show this result with a stronger notion of curvature: κf (X) = 1 − ˆ  4  P  j∈X P  f (j|X\j) . [sent-147, score-0.908]
</p><p>58 1 for the modular approximation is complewe get that f (X) ≤ f √ mentary to Corollary 3. [sent-153, score-0.272]
</p><p>59 2: First, the modular approximation is better whenever |X| ≤ n. [sent-154, score-0.292]
</p><p>60 1 depends on the curvature κf (X) with respect to the set X, which is stronger ˆ than κf . [sent-156, score-0.35]
</p><p>61 1 is tight for any modular approximation to a submodular function: Lemma 3. [sent-162, score-1.081]
</p><p>62 For any κ > 0, there exists a monotone submodular function f with curvature κ such |X| that no modular upper bound on f can approximate f (X) to a factor better than 1+(|X|−1)(1−κf ) . [sent-164, score-1.595]
</p><p>63 The improved curvature dependent bounds immediately imply better bounds for the class of concave over modular functions used in [22, 17, 11]. [sent-165, score-0.802]
</p><p>64 4  Learning Submodular functions  We next address the problem of learning submodular functions in a PMAC setting [2]. [sent-169, score-0.88]
</p><p>65 Balcan and Harvey [2] propose an algorithm that PMAC-learns √ any monotone, nonnegative submodular function within a factor α(n) = n + 1 by reducing the problem to that of learning a binary classiﬁer. [sent-177, score-0.873]
</p><p>66 If we assume that we have an upper bound on the curvature κf , or that we can estimate it 3 , and have access to the value of the singletons f (j), j ∈ V , then we can obtain better learning results with non-maximal curvature: Lemma 4. [sent-178, score-0.436]
</p><p>67 Let f be a monotone submodular function for which we know an upper bound on its curvature and the singleton weights f (j) for all j ∈ V . [sent-180, score-1.359]
</p><p>68 1, here we only need an upper bound on the curvature and do not need to know the singleton weights {f (j), j ∈ V }. [sent-201, score-0.451]
</p><p>69 Note also that, while κf itself is an upper bound of κf (X), often one does have an upper bound on κf (X) if one ˆ ˆ knows the function class of f (for example, say concave over modular). [sent-202, score-0.213]
</p><p>70 In particular, an immediate k corollary is that the class of concave over modular functions f (X) = i=1 λi [wi (X)]a , λi ≥ 0, for √ a ∈ (0, 1) can be learnt within a factor of min{ n + 1, 1 + |X|1−a }. [sent-203, score-0.445]
</p><p>71 5  Constrained submodular minimization  Next, we apply our results to the minimization of submodular functions under constraints. [sent-204, score-1.704]
</p><p>72 Given a submodular function f , let f1 be an approximation of f such that f1 (X) ≤ ˆ (X), for all X ⊆ V . [sent-211, score-0.847]
</p><p>73 The ﬁrst approach uses a simple modular upper bound (MUB) and the second relies on the Ellipsoidal approximation (EA) we used in Section 3. [sent-222, score-0.358]
</p><p>74 MUB: The simplest approximation to a submodular function is the modular approximation m ˆ ˆκ f m (X) j∈X f (j) ≥ f (X). [sent-223, score-1.119]
</p><p>75 1 implies improved approximation bounds for practically relevant concave over modular functions, such k as those used in [17]. [sent-237, score-0.43]
</p><p>76 This is signiﬁcantly better than the worst case factor of |X ∗ | for general submodular functions. [sent-239, score-0.826]
</p><p>77 EA: Instead of employing a modular upper bound, we can approximate f κ using the construction ˆ by Goemans et al. [sent-240, score-0.225]
</p><p>78 In that case, f (X) = κf wf κ (X) + (1 − κf )f m (X) has a special form: a weighted sum of a concave function and a modular function. [sent-243, score-0.281]
</p><p>79 Minimizing such a function over constraints C is harder than minimizing a merely modular function, but with ˆ the algorithm in [24] we obtain an FPTAS5 for minimizing f over C whenever we can minimize a nonnegative linear function over C. [sent-244, score-0.307]
</p><p>80 For a submodular function with curvature κf < 1, algorithm EA will return a solution X that satisﬁes √ n log n f (X) ≤ O √ f (X ∗ ). [sent-248, score-1.146]
</p><p>81 Svitkina and Fleischer [27] prove that for monotone submodular functions of arbitrary curvature, it is impossible to ﬁnd a polynomial-time algorithm with an approximation factor better than n/ log n. [sent-255, score-1.143]
</p><p>82 Moreover, the bound of EA is tight up to poly-log factors, in that no polynomial time algorithm can achieve a n1/2− general approximation factor better than 1+(n1/2− −1)(1−κf ) for any > 0. [sent-262, score-0.291]
</p><p>83 n2/3− , 1+(n2/3− −1)(1−κf )  Minimum submodular s-t cut (SSC): This problem, also known as the cooperative cut problem [16, 17], asks to minimize a monotone submodular function f such that the solution X ⊆ E is a set of edges whose removal disconnects s from t in G. [sent-274, score-1.791]
</p><p>84 Using curvature reﬁnes the We can also show a n1/2− lower bound of [16] to 1+(n1/2− −1)(1−κf ) , for any > 0. [sent-275, score-0.436]
</p><p>85 1 implies an approximation √  m log m m factor of O( (√m log m−1)(1−κf )+1 ) for EA and a factor of 1+(m−1)(1−κf ) for MUB, where m = |E| is the number of edges in the graph. [sent-277, score-0.286]
</p><p>86 Algorithm PNA achieves a worst-case approximation factor of 2+(n−2)(1−κf ) for the cooperative cut problem. [sent-285, score-0.211]
</p><p>87 Minimum submodular perfect matching (SPM): Here, we aim to ﬁnd a perfect matching in a graph that minimizes a monotone submodular function. [sent-296, score-1.744]
</p><p>88 1 implies that an MUB n approximation will achieve an approximation factor of at most 2+(n−2)(1−κf ) . [sent-298, score-0.228]
</p><p>89 We focus on cardinality lower bound constraints, C = {X ⊆ V : |X| ≥ α} and the “worst-case” class of functions that has been used throughout this paper to prove lower ¯ ¯ bounds, f R (X) = min{|X ∩ R| + β, |X|, α} where R = V \R and R ⊆ V is random set such that 1/2+ 2 |R| = α. [sent-328, score-0.225]
</p><p>90 To obtain a function with speciﬁc curvature κ, we R deﬁne fκ (X) = κf (X) + (1 − κ)|X| as in Equation (3). [sent-332, score-0.35]
</p><p>91 Overall, we see that the empirical results quite closely follow our theoretical results, and that, as the theory suggests, curvature signiﬁcantly affects the approximation factors. [sent-348, score-0.464]
</p><p>92 6  Conclusion and Discussion  In this paper, we study the effect of curvature on the problems of approximating, learning and minimizing submodular functions under constraints. [sent-349, score-1.229]
</p><p>93 These results complement known results for submodular maximization [3, 29]. [sent-351, score-0.792]
</p><p>94 Another open question is whether a quantity similar to curvature can be deﬁned for subadditive functions, thus reﬁning the results in [1] for learning subadditive functions. [sent-353, score-0.408]
</p><p>95 Finally it also seems that the techniques in this paper could be used to provide improved curvature-dependent regret bounds for constrained online submodular minimization [15]. [sent-354, score-0.939]
</p><p>96 Approximability of combinatorial problems with multi-agent submodular cost functions. [sent-394, score-0.835]
</p><p>97 Algorithms for approximate minimization of the difference between submodular functions, with applications. [sent-424, score-0.823]
</p><p>98 Combinatorial Problems with submodular coupling in machine learning and computer vision. [sent-439, score-0.778]
</p><p>99 Submodularity beyond submodular energies: coupling edges in graph cuts. [sent-456, score-0.793]
</p><p>100 Learning mixtures of submodular shells with application to document summarization. [sent-492, score-0.793]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('submodular', 0.764), ('curvature', 0.35), ('mub', 0.233), ('modular', 0.189), ('ea', 0.183), ('monotone', 0.144), ('jegelka', 0.089), ('approximation', 0.083), ('corollary', 0.075), ('bounds', 0.067), ('factor', 0.062), ('submodularity', 0.06), ('polymatroid', 0.059), ('minimization', 0.059), ('pmac', 0.058), ('polymatroidal', 0.058), ('functions', 0.058), ('ellipsoidal', 0.051), ('goel', 0.051), ('polynomial', 0.051), ('wf', 0.051), ('bound', 0.05), ('lemma', 0.049), ('combinatorial', 0.048), ('iyer', 0.047), ('balcan', 0.046), ('cardinality', 0.045), ('tight', 0.045), ('harvey', 0.045), ('goemans', 0.045), ('concave', 0.041), ('matroid', 0.039), ('cut', 0.038), ('spanning', 0.037), ('upper', 0.036), ('lower', 0.036), ('bilmes', 0.036), ('practically', 0.034), ('constrained', 0.033), ('iwata', 0.033), ('log', 0.032), ('approximating', 0.03), ('tighten', 0.029), ('fdif', 0.029), ('measy', 0.029), ('pna', 0.029), ('shells', 0.029), ('subadditive', 0.029), ('svitkina', 0.029), ('tripathi', 0.029), ('cooperative', 0.028), ('maximization', 0.028), ('nonnegative', 0.027), ('wi', 0.026), ('surrogate', 0.026), ('factors', 0.025), ('minx', 0.025), ('tighter', 0.024), ('focs', 0.024), ('problems', 0.023), ('hardness', 0.023), ('matchings', 0.022), ('pervasive', 0.022), ('stefanie', 0.022), ('concept', 0.022), ('varying', 0.022), ('picture', 0.021), ('ssp', 0.021), ('re', 0.02), ('whenever', 0.02), ('within', 0.02), ('matching', 0.02), ('argminx', 0.02), ('minimizing', 0.019), ('wireless', 0.019), ('constraints', 0.017), ('theoretical', 0.017), ('improved', 0.016), ('perfect', 0.016), ('harder', 0.016), ('lin', 0.016), ('unifying', 0.016), ('oracle', 0.015), ('singleton', 0.015), ('maximal', 0.015), ('edges', 0.015), ('path', 0.015), ('effect', 0.015), ('uai', 0.014), ('imply', 0.014), ('empirical', 0.014), ('everywhere', 0.014), ('logarithmic', 0.014), ('coupling', 0.014), ('economics', 0.014), ('covering', 0.014), ('fourier', 0.013), ('game', 0.013), ('constantin', 0.013), ('hitherto', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="78-tfidf-1" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>Author: Rishabh K. Iyer, Stefanie Jegelka, Jeff A. Bilmes</p><p>Abstract: We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. 1</p><p>2 0.70540166 <a title="78-tfidf-2" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>Author: Rishabh K. Iyer, Jeff A. Bilmes</p><p>Abstract: We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 25] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms. 1</p><p>3 0.42490038 <a title="78-tfidf-3" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>4 0.3716996 <a title="78-tfidf-4" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>Author: Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause</p><p>Abstract: Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable yet representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol G REE D I, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar-based clustering, on tens of millions of data points using Hadoop. 1</p><p>5 0.27758539 <a title="78-tfidf-5" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>6 0.12292311 <a title="78-tfidf-6" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>7 0.088334151 <a title="78-tfidf-7" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>8 0.061126608 <a title="78-tfidf-8" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>9 0.051538788 <a title="78-tfidf-9" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>10 0.049254701 <a title="78-tfidf-10" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>11 0.042060532 <a title="78-tfidf-11" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>12 0.040681645 <a title="78-tfidf-12" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>13 0.040519297 <a title="78-tfidf-13" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>14 0.040354561 <a title="78-tfidf-14" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>15 0.040038612 <a title="78-tfidf-15" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>16 0.03980583 <a title="78-tfidf-16" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>17 0.039254237 <a title="78-tfidf-17" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>18 0.037643857 <a title="78-tfidf-18" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>19 0.036800459 <a title="78-tfidf-19" href="./nips-2013-Embed_and_Project%3A_Discrete_Sampling_with_Universal_Hashing.html">107 nips-2013-Embed and Project: Discrete Sampling with Universal Hashing</a></p>
<p>20 0.035125725 <a title="78-tfidf-20" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.009), (2, 0.061), (3, 0.034), (4, 0.097), (5, 0.179), (6, -0.367), (7, -0.569), (8, 0.163), (9, -0.236), (10, -0.152), (11, -0.016), (12, -0.157), (13, 0.11), (14, -0.01), (15, 0.062), (16, 0.131), (17, -0.031), (18, 0.021), (19, -0.053), (20, 0.013), (21, 0.001), (22, -0.063), (23, 0.023), (24, -0.007), (25, 0.034), (26, -0.043), (27, -0.031), (28, 0.003), (29, -0.063), (30, 0.076), (31, -0.065), (32, -0.006), (33, 0.004), (34, -0.007), (35, -0.033), (36, 0.002), (37, -0.041), (38, 0.015), (39, -0.015), (40, 0.009), (41, 0.039), (42, 0.015), (43, -0.003), (44, 0.034), (45, -0.007), (46, -0.021), (47, 0.032), (48, 0.073), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96240598 <a title="78-lsi-1" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>Author: Rishabh K. Iyer, Stefanie Jegelka, Jeff A. Bilmes</p><p>Abstract: We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. 1</p><p>2 0.96228057 <a title="78-lsi-2" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>Author: Rishabh K. Iyer, Jeff A. Bilmes</p><p>Abstract: We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 25] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms. 1</p><p>3 0.81849015 <a title="78-lsi-3" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>Author: Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause</p><p>Abstract: Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable yet representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol G REE D I, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar-based clustering, on tens of millions of data points using Hadoop. 1</p><p>4 0.77233577 <a title="78-lsi-4" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>5 0.57344896 <a title="78-lsi-5" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>6 0.32856759 <a title="78-lsi-6" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>7 0.23686837 <a title="78-lsi-7" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>8 0.20667164 <a title="78-lsi-8" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>9 0.1783652 <a title="78-lsi-9" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>10 0.16562778 <a title="78-lsi-10" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>11 0.16038924 <a title="78-lsi-11" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<p>12 0.16033083 <a title="78-lsi-12" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>13 0.15782896 <a title="78-lsi-13" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>14 0.15683742 <a title="78-lsi-14" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>15 0.15136199 <a title="78-lsi-15" href="./nips-2013-Estimation_Bias_in_Multi-Armed_Bandit_Algorithms_for_Search_Advertising.html">112 nips-2013-Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising</a></p>
<p>16 0.15079676 <a title="78-lsi-16" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>17 0.15020722 <a title="78-lsi-17" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>18 0.14548087 <a title="78-lsi-18" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>19 0.1453688 <a title="78-lsi-19" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>20 0.14436288 <a title="78-lsi-20" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (13, 0.256), (16, 0.023), (29, 0.019), (33, 0.088), (34, 0.072), (41, 0.024), (49, 0.027), (56, 0.132), (70, 0.027), (85, 0.04), (89, 0.025), (93, 0.025), (95, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75196093 <a title="78-lda-1" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>Author: Rishabh K. Iyer, Stefanie Jegelka, Jeff A. Bilmes</p><p>Abstract: We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the “curvature” of the submodular function, and provide lower and upper bounds that reﬁne and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to inﬂuence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. 1</p><p>2 0.6443553 <a title="78-lda-2" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>Author: Rishabh K. Iyer, Jeff A. Bilmes</p><p>Abstract: We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 25] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms. 1</p><p>3 0.64381236 <a title="78-lda-3" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>Author: Xinhua Zhang, Wee Sun Lee, Yee Whye Teh</p><p>Abstract: Incorporating invariance information is important for many learning problems. To exploit invariances, most existing methods resort to approximations that either lead to expensive optimization problems such as semi-deﬁnite programming, or rely on separation oracles to retain tractability. Some methods further limit the space of functions and settle for non-convex models. In this paper, we propose a framework for learning in reproducing kernel Hilbert spaces (RKHS) using local invariances that explicitly characterize the behavior of the target function around data instances. These invariances are compactly encoded as linear functionals whose value are penalized by some loss function. Based on a representer theorem that we establish, our formulation can be efﬁciently optimized via a convex program. For the representer theorem to hold, the linear functionals are required to be bounded in the RKHS, and we show that this is true for a variety of commonly used RKHS and invariances. Experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art. 1</p><p>4 0.63749415 <a title="78-lda-4" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>Author: Maria-Florina Balcan, Vitaly Feldman</p><p>Abstract: We describe a framework for designing efﬁcient active learning algorithms that are tolerant to random classiﬁcation noise and differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of ﬁltered random examples. It builds on the powerful statistical query framework of Kearns [30]. We show that any efﬁcient active statistical learning algorithm can be automatically converted to an efﬁcient active learning algorithm which is tolerant to random classiﬁcation noise as well as other forms of “uncorrelated” noise. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efﬁciently actively learned in our framework. These results combined with our generic conversion lead to the ﬁrst computationally-efﬁcient algorithms for actively learning some of these concept classes in the presence of random classiﬁcation noise that provide exponential improvement in the dependence on the error over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efﬁcient active differentially-private algorithms. This leads to the ﬁrst differentially-private active learning algorithms with exponential label savings over the passive case. 1</p><p>5 0.63643664 <a title="78-lda-5" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>6 0.62555444 <a title="78-lda-6" href="./nips-2013-On_the_Complexity_and_Approximation_of_Binary_Evidence_in_Lifted_Inference.html">220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</a></p>
<p>7 0.62349117 <a title="78-lda-7" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>8 0.61935729 <a title="78-lda-8" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>9 0.60293859 <a title="78-lda-9" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>10 0.59535998 <a title="78-lda-10" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>11 0.59156293 <a title="78-lda-11" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>12 0.59112787 <a title="78-lda-12" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>13 0.58874977 <a title="78-lda-13" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>14 0.58594429 <a title="78-lda-14" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>15 0.58096409 <a title="78-lda-15" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>16 0.57714331 <a title="78-lda-16" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>17 0.56433553 <a title="78-lda-17" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>18 0.5616855 <a title="78-lda-18" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>19 0.56143409 <a title="78-lda-19" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>20 0.56117254 <a title="78-lda-20" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
