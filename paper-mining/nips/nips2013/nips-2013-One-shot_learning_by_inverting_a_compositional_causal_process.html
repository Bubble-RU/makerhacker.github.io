<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>226 nips-2013-One-shot learning by inverting a compositional causal process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-226" href="#">nips2013-226</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>226 nips-2013-One-shot learning by inverting a compositional causal process</h1>
<br/><p>Source: <a title="nips-2013-226-pdf" href="http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf">pdf</a></p><p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>Reference: <a title="nips-2013-226-reference" href="../nips2013_reference/nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. [sent-10, score-0.203]
</p><p>2 Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. [sent-11, score-0.242]
</p><p>3 1  Introduction  People can acquire a new concept from only the barest of experience – just one or a handful of examples in a high-dimensional space of raw perceptual input. [sent-14, score-0.185]
</p><p>4 Although machine learning has tackled some of the same classiﬁcation and recognition problems that people solve so effortlessly, the standard algorithms require hundreds or thousands of examples to reach good performance. [sent-15, score-0.346]
</p><p>5 While the standard MNIST benchmark dataset for digit recognition has 6000 training examples per class [19], people can classify new images of a foreign handwritten character from just one example (Figure 1b) [23, 16, 17]. [sent-16, score-0.693]
</p><p>6 Similarly, while classiﬁers are generally trained on hundreds of images per class, using benchmark datasets such as ImageNet [4] and CIFAR-10/100 [14], people can learn a a)  b)  c)  Human drawers  3  3 canonical  Figure 1: Can you learn a new concept from just one example? [sent-17, score-0.468]
</p><p>7 c) The learned concepts also support many other abilities such as generating examples and parsing. [sent-20, score-0.183]
</p><p>8 1  1  2  1  Figure 2: Four alphabets from Omniglot, each with ﬁve characters drawn by four different people. [sent-25, score-0.296]
</p><p>9 Additionally, while classiﬁcation has received most of the attention in machine learning, people can generalize in a variety of other ways after learning a new concept. [sent-30, score-0.241]
</p><p>10 Equipped with the concept “Segway” or a new handwritten character (Figure 1c), people can produce new examples, parse an object into its critical parts, and ﬁll in a missing part of an image. [sent-31, score-0.671]
</p><p>11 Given that people seem to succeed at both sides of the tradeoff, a central challenge is to explain this remarkable ability: What types of representations can be learned from just one or a few examples, and how can these representations support such ﬂexible generalizations? [sent-36, score-0.241]
</p><p>12 We selected simple visual concepts from the domain of handwritten characters, which offers a large number of novel, high-dimensional, and cognitively natural stimuli (Figure 2). [sent-39, score-0.212]
</p><p>13 These characters are signiﬁcantly more complex than the simple artiﬁcial stimuli most often modeled in psychological studies of concept learning (e. [sent-40, score-0.274]
</p><p>14 , [6, 13]), yet they remain simple enough to hope that a computational model could see all the structure that people do, unlike domains such as natural scenes. [sent-42, score-0.241]
</p><p>15 While similar in spirit to MNIST, rather than having 10 characters with 6000 examples each, it has over 1600 character with 20 examples each – making it more like the “transpose” of MNIST. [sent-44, score-0.558]
</p><p>16 These characters were selected from 50 different alphabets on www. [sent-45, score-0.296]
</p><p>17 Since it was produced on Amazon’s Mechanical Turk, each image is paired with a movie ([x,y,time] coordinates) showing how that drawing was produced. [sent-52, score-0.185]
</p><p>18 In addition to introducing new one-shot learning challenge problems, this paper also introduces Hierarchical Bayesian Program Learning (HBPL), a model that exploits the principles of compositionality and causality to learn a wide range of simple visual concepts from just a single example. [sent-53, score-0.308]
</p><p>19 We compared the model with people and other competitive computational models for character recognition, including Deep Boltzmann Machines [25] and their Hierarchical Deep extension for learning with very few examples [26]. [sent-54, score-0.523]
</p><p>20 In this test, both people and the model performed the same task side by side, and then other human participants judged which result was from a person and which was from a machine. [sent-57, score-0.476]
</p><p>21 2  Hierarchical Bayesian Program Learning  We introduce a new computational approach called Hierarchical Bayesian Program Learning (HBPL) that utilizes the principles of compositionality and causality to build a probabilistic generative model of handwritten characters. [sent-58, score-0.225]
</p><p>22 It is compositional because characters are represented as stochastic motor programs where primitive structure is shared and re-used across characters at multiple levels, including strokes and sub-strokes. [sent-59, score-0.855]
</p><p>23 character type 1 ( = 2)  (m) b}  I (m)  I (m)  Figure 3: An illustration of the HBPL model generating two character types (left and right), where the dotted line separates the type-level from the token-level variables. [sent-63, score-0.471]
</p><p>24 Legend: number of strokes κ, relations R, primitive id z (color-coded to highlight sharing), control points x (open circles), scale y, start locations L, trajectories T , transformation A, noise and θb , and image I. [sent-64, score-0.587]
</p><p>25 “structural description” to explain the image by freely combining these elementary parts and their spatial relations. [sent-65, score-0.185]
</p><p>26 Unlike classic structural description models [27, 2], HBPL also reﬂects abstract causal structure about how characters are actually produced. [sent-66, score-0.258]
</p><p>27 This type of causal representation is psychologically plausible, and it has been previously theorized to explain both behavioral and neuro-imaging data regarding human character perception and learning (e. [sent-67, score-0.324]
</p><p>28 As in most previous “analysis by synthesis” models of characters, strokes are not modeled at the level of muscle movements, so that they are abstract enough to be completed by a hand, a foot, or an airplane writing in the sky. [sent-70, score-0.253]
</p><p>29 But HBPL also learns a signiﬁcantly more complex representation than earlier models, which used only one stroke (unless a second was added manually) [24, 10] or received on-line input data [9], sidestepping the challenging parsing problem needed to interpret complex characters. [sent-71, score-0.285]
</p><p>30 The model distinguishes between character types (an ‘A’, ‘B’, etc. [sent-72, score-0.212]
</p><p>31 1  Generating a character type  A character type ψ = {κ, S, R} is deﬁned by a set of κ strokes S = {S1 , . [sent-83, score-0.677]
</p><p>32 (2)  The number of strokes is sampled from a multinomial P (κ) estimated from the empirical frequencies (Figure 4b), and the other conditional distributions are deﬁned in the sections below. [sent-94, score-0.282]
</p><p>33 All hyperparameters, including the library of primitives (top of Figure 3), were learned from a large “background set” of character drawings as described in Sections 2. [sent-95, score-0.359]
</p><p>34 Each stroke is initiated by pressing the pen down and terminated by lifting the pen up. [sent-98, score-0.376]
</p><p>35 In between, a stroke is a motor routine composed of simple movements called substrokes Si = {si1 , . [sent-99, score-0.356]
</p><p>36 The discrete class zij ∈ N is an index into the library of primini tive motor elements (top of Figure 3), and its distribution P (zi ) = P (zi1 ) j=2 P (zij |zi(j−1) ) is a ﬁrst-order Markov Process that adds sub-strokes at each step until a special “stop” state is sampled that ends the stroke. [sent-104, score-0.3]
</p><p>37 The ﬁve control points xij ∈ R10 (small open circles in Figure 3) are sampled from a Gaussian P (xij |zij ) = N (µzij , Σzij ) , but they live in an abstract space not yet embedded in the image frame. [sent-105, score-0.324]
</p><p>38 The type-level scale yij of this space, relative to the image frame, is sampled from P (yij |zij ) = Gamma(αzij , βzij ). [sent-106, score-0.299]
</p><p>39 The spatial relation Ri speciﬁes how the beginning of stroke Si connects to the previous strokes {S1 , . [sent-108, score-0.503]
</p><p>40 Relations can come in four types with probabilities θR , and each type has different sub-variables and dimensionalities: • Independent relations, Ri = {Ji , Li }, where the position of stroke i does not depend on previous strokes. [sent-119, score-0.278]
</p><p>41 The variable Ji ∈ N is drawn from P (Ji ), a multinomial over a 2D image grid that depends on index i (Figure 4c). [sent-120, score-0.217]
</p><p>42 Since the position Li ∈ R2 has to be real-valued, P (Li |Ji ) is then sampled uniformly at random from within the image cell Ji . [sent-121, score-0.242]
</p><p>43 • Start or End relations, Ri = {ui }, where stroke i starts at either the beginning or end of a previous stroke ui , sampled uniformly at random from ui ∈ {1, . [sent-122, score-0.603]
</p><p>44 • Along relations, Ri = {ui , vi , τi }, where stroke i begins along previous stroke ui ∈ {1, . [sent-126, score-0.537]
</p><p>45 2  Generating a character token (m)  The token-level variables, θ(m) = {L(m) , x(m) , y (m) , R(m) , A(m) , σb (m)  P (θ(m) |ψ) = P (L(m) |θ\L(m) , ψ)  (m)  P (Ri i  (m)  |Ri )P (yi  ,  (m)  |yi )P (xi  (m)  }, are distributed as (m)  |xi )P (A(m) , σb  ,  (m)  )  (3) with details below. [sent-134, score-0.266]
</p><p>46 A stroke trajectory Ti (Figure 3) is a sequence of points in the image plane (m) (m) (m) (m) that represents the path of the pen. [sent-138, score-0.467]
</p><p>47 To construct the trajectory Ti (see illustration in Figure 3), the spline deﬁned by the scaled (m) (m) 10 control points y1 x1 ∈ R is evaluated to form a trajectory,1 which is shifted in the image plane (m) (m) (m) to begin at Li . [sent-141, score-0.321]
</p><p>48 (m)  Token-level relations must be exactly equal to their type-level counterparts, P (Ri |Ri ) = (m) δ(Ri − Ri ), except for the “along” relation which allows for token-level variability for (m) 2 the attachment along the spline using a truncated Gaussian P (τi |τi ) ∝ N (τi , στ ). [sent-143, score-0.209]
</p><p>49 is start or end, and  1 The number of spline evaluations is computed to be approximately 2 points for every 3 pixels of distance along the spline (with a minimum of 10 evaluations). [sent-152, score-0.244]
</p><p>50 4  a)  library of motor primitives  b)  number of of strokes Number strokes  frequency  6000 1  1  2  Figure 4: Learned hyperparameters. [sent-153, score-0.698]
</p><p>51 b&c;) Empirical distributions where the heatmap c) show how starting point differs by stroke number. [sent-156, score-0.25]
</p><p>52 2  4000 2000 0  0  2  c)  4  6  8  stroke start positions  1  1  3  3  2  2  1  2  3  3  4  4  ≥4  4  4  (m)  3  Image. [sent-157, score-0.25]
</p><p>53 An image transformation A ∈ R is sampled from P (A(m) ) = N ([1, 1, 0, 0], ΣA ), where the ﬁrst two elements control a global re-scaling and the second two control a global translation of the center of mass of T (m) . [sent-158, score-0.214]
</p><p>54 This grayscale image is then perturbed by two noise processes, which make the gradient more robust during optimization and encourage partial solutions during classiﬁcation. [sent-160, score-0.225]
</p><p>55 3  4  Learning high-level knowledge of motor programs  The Omniglot dataset was randomly split into a 30 alphabet “background” set and a 20 alphabet “evaluation” set, constrained such that the background set included the six most common alphabets as determined by Google hits. [sent-164, score-0.318]
</p><p>56 Background images, paired with their motor data, were used to learn the hyperparameters of the HBPL model, including a set of 1000 primitive motor elements (Figure 4a) and position models for a drawing’s ﬁrst, second, and third stroke, etc. [sent-165, score-0.316]
</p><p>57 Details are provided in Section SI-4 for learning the models of primitives, positions, relations, token variability, and image transformations. [sent-168, score-0.239]
</p><p>58 4  Inference  Posterior inference in this model is very challenging, since parsing an image I (m) requires exploring a large combinatorial space of different numbers and types of strokes, relations, and sub-strokes. [sent-170, score-0.22]
</p><p>59 , ψ [K] , θ(m)[K] , which are the most promising candidates proposed by a fast, bottom-up image analysis, shown in Figure 5a and detailed in Section SI-5. [sent-174, score-0.185]
</p><p>60 (6)  Image  Thinned  Binary image  a) i  Binary image  b)  1  2  train train train traintrain train  22 222 11 112 11  Traced graph (raw)  0  train  train  2 2 1 1 2 22 11 222 111 2 1 1 −59. [sent-183, score-0.64]
</p><p>61 6 0  test 00000 0  ii Thinned image  test test test test test  test 22 1 1 2 2 1 22 11 222 111 1  0  Thinned image  1 2 test  22 222 12 11 111  traced graph (cleaned)  −831  planning  2 1 2  −2. [sent-184, score-0.808]
</p><p>62 6  Thinned image  iii  train  train  Binary image  −159 −159 −159 −159 −159 −88. [sent-214, score-0.46]
</p><p>63 12e+03  planning  planning  -1273  -831  -2041  Figure 5: Parsing a raw image. [sent-251, score-0.178]
</p><p>64 a) The raw image (i) is processed by a thinning algorithm [18] (ii) and then analyzed as an undirected graph [20] (iii) where parses are guided random walks (Section SI-5). [sent-252, score-0.439]
</p><p>65 b) The ﬁve best parses found for that image (top row) are shown with their log wj (Eq. [sent-253, score-0.33]
</p><p>66 5), where numbers inside circles denote stroke order and starting position, and smaller open circles denote sub-stroke breaks. [sent-254, score-0.326]
</p><p>67 These ﬁve parses were re-ﬁt to three different raw images of characters (left in image triplets), where the best parse (top right) and its associated image reconstruction (bottom right) are shown above its score (Eq. [sent-255, score-0.988]
</p><p>68 planning cleaned  planning cleaned  planning cleaned  Given an approximate posterior for a particular image, the model can evaluate the posterior predictive score of a new image by re-ﬁtting the token-level variables (bottom Figure 5b), as explained in Section 3. [sent-257, score-0.536]
</p><p>69 Each trial (of 400 total) consists of a single test image of a new character compared to 20 new characters from the same alphabet, given just one image each produced by a typical drawer of that alphabet. [sent-262, score-0.832]
</p><p>70 On each trial, as in Figure 1b, participants were shown an image of a new character and asked to click on another image that shows the same character. [sent-266, score-0.752]
</p><p>71 To ensure classiﬁcation was indeed “one shot,” participants completed just one randomly selected trial from each of the 10 within-alphabet classiﬁcation tasks, so that characters never repeated across trials. [sent-267, score-0.376]
</p><p>72 For a test image I (T ) and 20 training images I (c) for c = 1, . [sent-270, score-0.318]
</p><p>73 While inference so far involves parses of I (c) reﬁt to I (T ) , it also seems desirable to include parses of I (T ) reﬁt to I (c) , namely P (I (c) |I (T ) ). [sent-278, score-0.29]
</p><p>74 The full HBPL model is compared to a transformation-based approach that models the variance in image tokens as just global scales, translations, and blur, which relates to congealing models [23]. [sent-287, score-0.226]
</p><p>75 This HBPL model “without strokes” still beneﬁts from good bottom-up image analysis (Figure 5) and a learned transformation model. [sent-288, score-0.185]
</p><p>76 The Afﬁne model is identical to HBPL during search, (m) but during classiﬁcation, only the warp A(m) , blur σb , and noise (m) are re-optimized to a new (T ) image (change the argument of “max” in Eq. [sent-289, score-0.213]
</p><p>77 To evaluate classiﬁcation performance, ﬁrst the approximate posterior distribution over the DBMs top-level features was inferred for each image in the evaluation set, followed by performing 1-nearest neighbor in this feature space using cosine similarity. [sent-293, score-0.185]
</p><p>78 To speed up learning of the DBM and HD models, the original images were down-sampled, so that each image was represented by 28x28 pixels with greyscale values from [0,1]. [sent-294, score-0.31]
</p><p>79 To further reduce overﬁtting and learn more about the 2D image topology, which is built in to some deep models like convolution networks [19], the set of background characters was artiﬁcially enhanced by generating slight image translations (+/- 3 pixels), rotations (+/- 5 degrees), and scales (0. [sent-295, score-0.781]
</p><p>80 As predicted, people were skilled one-shot learners, with an average error rate of 4. [sent-308, score-0.241]
</p><p>81 3%  One-shot generation of new examples  Not only can people classify new examples, they can generate new examples – even from just one image. [sent-330, score-0.381]
</p><p>82 While all generative classiﬁers can produce examples, it can be difﬁcult to synthesize a range of compelling new examples in their raw form, especially since many models generate only features of raw stimuli (e. [sent-331, score-0.267]
</p><p>83 We ran another Mechanical Turk task to produce nine new examples of 50 randomly selected handwritten character images from the evaluation set. [sent-334, score-0.516]
</p><p>84 After correctly answering comprehension questions, 18 participants in the USA were asked to “draw a new example” of 25 characters, resulting in nine examples per character. [sent-336, score-0.335]
</p><p>85 To simulate drawings from nine different people, each of the models generated nine samples after seeing exactly the same images people did, as described in Section SI-8 and shown in Figure 6. [sent-337, score-0.519]
</p><p>86 Low-level image differences were minimized by re-rendering stroke trajectories in the same way for the models and people. [sent-338, score-0.47]
</p><p>87 7  Example  People  HBPL  Afﬁne  HD  Figure 6: Generating new examples from just a single “target” image (left). [sent-340, score-0.255]
</p><p>88 Each grid shows nine new examples synthesized by people and the three computational models. [sent-341, score-0.407]
</p><p>89 To compare the examples generated by people and the models, we ran a visual Turing test using 50 new participants in the USA on Mechanical Turk. [sent-343, score-0.59]
</p><p>90 Participants were told that they would see a target image and two grids of 9 images (Figure 6), where one grid was drawn by people with their computer mice and the other grid was drawn by a computer program that “simulates how people draw a new character. [sent-344, score-0.862]
</p><p>91 Participants who tried to label drawings from people vs. [sent-352, score-0.302]
</p><p>92 HBPL were only 56% percent correct, while those who tried to label people vs. [sent-353, score-0.241]
</p><p>93 While both group means were signiﬁcantly better than chance, a subject analysis revealed only 2 of 21 participants were better than chance for people vs. [sent-357, score-0.455]
</p><p>94 HBPL, while 24 of 25 were signiﬁcant for people vs. [sent-358, score-0.241]
</p><p>95 Likewise, 8 of 50 items were above chance for people vs. [sent-360, score-0.285]
</p><p>96 HBPL, while 48 of 50 items were above chance for people vs. [sent-361, score-0.285]
</p><p>97 Since participants could easily detect the overly consistent Afﬁne model, it seems the difﬁculty participants had in detecting HBPL’s exemplars was not due to task confusion. [sent-363, score-0.371]
</p><p>98 Interestingly, participants did not signiﬁcantly improve over the trials, even after seeing hundreds of images from the model. [sent-364, score-0.294]
</p><p>99 If one were to incorporate this compositional and causal structure into a deep learning model, it could lead to better performance on our tasks. [sent-369, score-0.182]
</p><p>100 Thus, we do not see our model as the ﬁnal word on how humans learn concepts, but rather, as a suggestion for the type of structure that best captures how people learn rich concepts from very sparse data. [sent-370, score-0.373]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hbpl', 0.512), ('strokes', 0.253), ('stroke', 0.25), ('people', 0.241), ('character', 0.212), ('characters', 0.206), ('image', 0.185), ('participants', 0.17), ('parses', 0.145), ('zij', 0.137), ('ri', 0.122), ('motor', 0.106), ('spline', 0.104), ('parse', 0.1), ('alphabets', 0.09), ('deep', 0.089), ('images', 0.089), ('yij', 0.085), ('handwritten', 0.081), ('compositionality', 0.078), ('turing', 0.078), ('raw', 0.078), ('hd', 0.075), ('xij', 0.072), ('relations', 0.071), ('examples', 0.07), ('thinned', 0.067), ('cleaned', 0.067), ('concepts', 0.066), ('causality', 0.066), ('visual', 0.065), ('af', 0.065), ('nine', 0.064), ('pen', 0.063), ('drawings', 0.061), ('omniglot', 0.061), ('classi', 0.058), ('primitives', 0.058), ('wi', 0.055), ('token', 0.054), ('dbms', 0.054), ('li', 0.054), ('hierarchical', 0.053), ('causal', 0.052), ('boltzmann', 0.05), ('planning', 0.05), ('salakhutdinov', 0.049), ('mechanical', 0.049), ('generating', 0.047), ('train', 0.045), ('ji', 0.044), ('test', 0.044), ('chance', 0.044), ('si', 0.043), ('primitive', 0.043), ('alphabet', 0.043), ('program', 0.042), ('compelling', 0.041), ('compositional', 0.041), ('tokens', 0.041), ('brenden', 0.041), ('tui', 0.041), ('grayscale', 0.04), ('argmax', 0.039), ('circles', 0.038), ('ti', 0.038), ('lake', 0.037), ('ui', 0.037), ('concept', 0.037), ('traced', 0.036), ('segway', 0.036), ('background', 0.036), ('pixels', 0.036), ('psychology', 0.036), ('hundreds', 0.035), ('trajectories', 0.035), ('parsing', 0.035), ('variability', 0.034), ('judged', 0.033), ('greek', 0.033), ('shot', 0.033), ('learn', 0.033), ('trajectory', 0.032), ('cognitive', 0.032), ('imagenet', 0.032), ('bayesian', 0.032), ('grid', 0.032), ('human', 0.032), ('thinning', 0.031), ('comprehension', 0.031), ('exemplars', 0.031), ('psychological', 0.031), ('sampled', 0.029), ('scripts', 0.028), ('dbm', 0.028), ('blur', 0.028), ('turk', 0.028), ('perception', 0.028), ('position', 0.028), ('library', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="226-tfidf-1" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>2 0.19966991 <a title="226-tfidf-2" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>3 0.11374186 <a title="226-tfidf-3" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>Author: Michiel Hermans, Benjamin Schrauwen</p><p>Abstract: Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difﬁcult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales. 1</p><p>4 0.10532411 <a title="226-tfidf-4" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>Author: Vikash Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Josh Tenenbaum</p><p>Abstract: The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difﬁcult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that deﬁne ﬂexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs (GPGP) consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer’s output and the data, and latent variables that adjust the ﬁdelity of the renderer and the tolerance of the likelihood. Representations and algorithms from computer graphics are used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on generalpurpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and yields accurate, approximately Bayesian inferences about real-world images. 1</p><p>5 0.09781982 <a title="226-tfidf-5" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>6 0.094575495 <a title="226-tfidf-6" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>7 0.09170568 <a title="226-tfidf-7" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>8 0.091233872 <a title="226-tfidf-8" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>9 0.08039619 <a title="226-tfidf-9" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>10 0.079348795 <a title="226-tfidf-10" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>11 0.079266444 <a title="226-tfidf-11" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>12 0.07911969 <a title="226-tfidf-12" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>13 0.075725675 <a title="226-tfidf-13" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>14 0.074662231 <a title="226-tfidf-14" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>15 0.07245931 <a title="226-tfidf-15" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>16 0.071726196 <a title="226-tfidf-16" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>17 0.070502706 <a title="226-tfidf-17" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>18 0.069680706 <a title="226-tfidf-18" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>19 0.068243891 <a title="226-tfidf-19" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>20 0.06663651 <a title="226-tfidf-20" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.074), (2, -0.14), (3, -0.065), (4, 0.104), (5, -0.072), (6, -0.009), (7, 0.013), (8, -0.025), (9, 0.017), (10, -0.076), (11, -0.002), (12, -0.017), (13, 0.011), (14, -0.091), (15, 0.01), (16, -0.035), (17, -0.066), (18, -0.088), (19, 0.003), (20, 0.021), (21, -0.042), (22, -0.038), (23, 0.006), (24, -0.046), (25, 0.099), (26, 0.061), (27, 0.011), (28, 0.002), (29, -0.043), (30, -0.031), (31, 0.049), (32, -0.108), (33, -0.044), (34, -0.056), (35, -0.008), (36, -0.032), (37, 0.082), (38, 0.047), (39, 0.037), (40, -0.009), (41, 0.135), (42, -0.074), (43, 0.033), (44, 0.017), (45, -0.003), (46, -0.008), (47, -0.036), (48, 0.06), (49, -0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93583846 <a title="226-lsi-1" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>2 0.81764531 <a title="226-lsi-2" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>3 0.73999745 <a title="226-lsi-3" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>Author: Chen-Ping Yu, Wen-Yu Hua, Dimitris Samaras, Greg Zelinsky</p><p>Abstract: Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects deﬁned as groupings of superpixels that are similar in intensity, color, and gradient orientation features. We introduce a novel parametric method of clustering superpixels by modeling mixture of Weibulls on Earth Mover’s Distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception. We validated this model using a new 90-image dataset of real world scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (Spearman’s ρ = 0.8038, p < 0.001), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth. We conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features. 1</p><p>4 0.71766108 <a title="226-lsi-4" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>Author: Vikash Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Josh Tenenbaum</p><p>Abstract: The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difﬁcult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that deﬁne ﬂexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs (GPGP) consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer’s output and the data, and latent variables that adjust the ﬁdelity of the renderer and the tolerance of the likelihood. Representations and algorithms from computer graphics are used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on generalpurpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and yields accurate, approximately Bayesian inferences about real-world images. 1</p><p>5 0.71039927 <a title="226-lsi-5" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>6 0.70116496 <a title="226-lsi-6" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>7 0.67568254 <a title="226-lsi-7" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>8 0.65092218 <a title="226-lsi-8" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>9 0.64365572 <a title="226-lsi-9" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>10 0.60952401 <a title="226-lsi-10" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>11 0.605811 <a title="226-lsi-11" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>12 0.60417169 <a title="226-lsi-12" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>13 0.60297805 <a title="226-lsi-13" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>14 0.58591592 <a title="226-lsi-14" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>15 0.57766491 <a title="226-lsi-15" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>16 0.5666827 <a title="226-lsi-16" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>17 0.55529553 <a title="226-lsi-17" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>18 0.53753835 <a title="226-lsi-18" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>19 0.53625518 <a title="226-lsi-19" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>20 0.53322905 <a title="226-lsi-20" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.037), (33, 0.193), (34, 0.107), (41, 0.035), (49, 0.041), (56, 0.084), (70, 0.066), (71, 0.228), (85, 0.036), (89, 0.021), (93, 0.051), (95, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86450189 <a title="226-lda-1" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>Author: David Lopez-Paz, Philipp Hennig, Bernhard Schölkopf</p><p>Abstract: We introduce the Randomized Dependence Coefﬁcient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R´ nyi Maximum Correlation Coefﬁcient. RDC is deﬁned in e terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just ﬁve lines of R code, included at the end of the paper. 1</p><p>same-paper 2 0.83551848 <a title="226-lda-2" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>3 0.80276132 <a title="226-lda-3" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato</p><p>Abstract: A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-ﬁtting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modiﬁcations. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefﬁcients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data. 1</p><p>4 0.7378853 <a title="226-lda-4" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>5 0.7354477 <a title="226-lda-5" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>6 0.73429149 <a title="226-lda-6" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>7 0.73176193 <a title="226-lda-7" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>8 0.73165596 <a title="226-lda-8" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>9 0.73097283 <a title="226-lda-9" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>10 0.73034942 <a title="226-lda-10" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>11 0.72861028 <a title="226-lda-11" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>12 0.72855079 <a title="226-lda-12" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>13 0.7282266 <a title="226-lda-13" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>14 0.72772247 <a title="226-lda-14" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>15 0.72709441 <a title="226-lda-15" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>16 0.72677696 <a title="226-lda-16" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>17 0.72632366 <a title="226-lda-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.72627777 <a title="226-lda-18" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>19 0.72572833 <a title="226-lda-19" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>20 0.72568995 <a title="226-lda-20" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
