<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-354" href="#">nips2013-354</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</h1>
<br/><p>Source: <a title="nips-2013-354-pdf" href="http://papers.nips.cc/paper/5003-when-in-doubt-swap-high-dimensional-sparse-recovery-from-correlated-measurements.pdf">pdf</a></p><p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><p>Reference: <a title="nips-2013-354-reference" href="../nips2013_reference/nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. [sent-3, score-0.142]
</p><p>2 It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. [sent-4, score-0.477]
</p><p>3 We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. [sent-5, score-0.145]
</p><p>4 We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. [sent-7, score-0.35]
</p><p>5 Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications. [sent-10, score-0.219]
</p><p>6 The simplest, but still very useful, setting is when the observations can be approximated as a sparse linear combination of the columns in a measurement matrix X weighted by the non-zero entries of the unknown sparse vector. [sent-11, score-0.36]
</p><p>7 In the literature, this problem is often to referred to as the sparse recovery or the support recovery problem. [sent-13, score-0.614]
</p><p>8 Although several tractable sparse recovery algorithms have been proposed in the literature, statistical guarantees for accurately estimating S ∗ can only be provided under conditions that limit how correlated the columns of X can be. [sent-14, score-0.496]
</p><p>9 For example, if there exists a column, say Xi , that is nearly linearly dependent on the columns indexed by S ∗ , some sparse recovery algorithms may falsely select Xi . [sent-15, score-0.435]
</p><p>10 However, in many applications, X cannot be speciﬁed by a practitioner, and correlated measurement matrices are inevitable. [sent-17, score-0.143]
</p><p>11 For example, when the columns in X correspond to gene expression values, it has been observed that genes in the same pathway produce correlated values [1]. [sent-18, score-0.24]
</p><p>12 In this paper, we develop new sparse recovery algorithms that can accurately recover S ∗ for measurement matrices that exhibit strong correlations. [sent-20, score-0.429]
</p><p>13 We propose a greedy algorithm, called SWAP, that iteratively swaps variables starting from an initial estimate of S ∗ until a desired loss function cannot be decreased any further. [sent-21, score-0.145]
</p><p>14 We prove that SWAP can accurately identify the true signal support 1  under relatively mild conditions on the restricted eigenvalues of the matrix X T X and under certain conditions on the correlations between the columns of X. [sent-22, score-0.361]
</p><p>15 A novel aspect of our theory is that the conditions we derive are only needed when conventional sparse recovery algorithms fail to recover S ∗ . [sent-23, score-0.349]
</p><p>16 This motivates the use of SWAP as a wrapper around sparse recovery algorithms for improved performance. [sent-24, score-0.35]
</p><p>17 Finally, using numerical simulations, we show that SWAP consistently outperforms many state of the art algorithms on both synthetic and real data corresponding to gene expression values. [sent-25, score-0.14]
</p><p>18 SWAP is a greedy algorithm with novel guarantees for sparse recovery and we make appropriate comparisons in the text. [sent-29, score-0.338]
</p><p>19 Another line of research when dealing with correlated measurements is to estimate a superset of S ∗ ; see [14–18] for examples. [sent-30, score-0.115]
</p><p>20 Section 4 presents theoretical results on the conditions needed for provably correct sparse recovery. [sent-34, score-0.109]
</p><p>21 Thus, we mainly focus on the sparse recovery problem of estimating S ∗ . [sent-53, score-0.31]
</p><p>22 A classical strategy for sparse recovery is to search for a support of size k that minimizes a suitable loss function. [sent-54, score-0.432]
</p><p>23 In this paper, we design a sparse recovery algorithm that provably, and efﬁciently, ﬁnds the true support for a broad class of measurement matrices that includes matrices with high correlations. [sent-56, score-0.537]
</p><p>24 Recall that our main goal is to ﬁnd a support S that minimizes the loss deﬁned in (2). [sent-58, score-0.122]
</p><p>25 Suppose that we are given an estimate, say S (1) , of the true support and let L(1) be the corresponding least-squares loss (see (2)). [sent-59, score-0.134]
</p><p>26 Our main idea to transition from S (1) to an appropriate S (2) is to swap variables as follows: (1)  Swap every i ∈ S (1) with i ∈ (S (1) )c and compute the resulting loss Li,i = L({S (1) \i} ∪i ; y, X). [sent-61, score-0.927]
</p><p>27 (1)  If mini,i Li,i < L(1) , there exists a support that has a lower loss than the original one. [sent-62, score-0.108]
</p><p>28 (a) Histogram of sparse eigenvalues of X over 10, 000 random sets of size 10; (b) legend; (c) mean true positive rate vs. [sent-71, score-0.148]
</p><p>29 Algorithm 1: SWAP(y, X, S) Inputs: Measurements y, design matrix X, and initial support S. [sent-74, score-0.118]
</p><p>30 Figure 1 illustrates the performance of SWAP for a matrix X that corresponds to 83 samples of 2308 gene expression values for patients with small round blue cell tumors [19]. [sent-85, score-0.135]
</p><p>31 5 and randomly chosen sparse vectors with non-zero entries between 1 and 2. [sent-87, score-0.112]
</p><p>32 This means that the columns of X are highly correlated with each other. [sent-90, score-0.123]
</p><p>33 Figure 1(c) shows the mean fraction of variables estimated to be in the true support over 100 different trials. [sent-91, score-0.146]
</p><p>34 Although SWAP can be used with a random initialization S, we recommend using SWAP in combination with another sparse recovery algorithm. [sent-97, score-0.327]
</p><p>35 The dashed lines represent standard sparse recovery algorithms, while the solid lines with markers represent SWAP algorithms. [sent-99, score-0.329]
</p><p>36 Intuitively, since many sparse recovery algorithms can perform partial support recovery, using such an initialization results in a smaller search space when searching for the true support. [sent-101, score-0.451]
</p><p>37 Other competitive algorithms, such as forward-backwards (FoBa) [20] or CoSaMP [21], usually estimate a signal with higher sparsity level and iteratively remove variables until k variables are selected. [sent-124, score-0.139]
</p><p>38 Intuitively, as we shall see in Section 4, by maintaining a support of size k, the performance of SWAP only depends on correlations among the columns of the matrix XA , where A is of size at most 2k and it includes the true support. [sent-126, score-0.257]
</p><p>39 In contrast, for other sparse recovery algorithms, |A| ≥ 2k. [sent-127, score-0.31]
</p><p>40 Smaller values of ρk+ correspond to correlated columns in the matrix X. [sent-134, score-0.146]
</p><p>41 ∗ i∈S  A smaller βmin will evidently require more number of observations for exact recovery of the support. [sent-136, score-0.24]
</p><p>42 Finally, we deﬁne a parameter that characterizes the correlations between the columns of the matrix XS ∗ and the columns of the matrix X(S ∗ )c , where recall that S ∗ is the true support of the unknown sparse vector β ∗ . [sent-137, score-0.412]
</p><p>43 For a set Ωk,d that contains all supports of size k with atleast k − d active variables from S ∗ , deﬁne γd as S\i  2 γd :=  max  min  S\i  Σi,S ΣS,S ¯ ¯ ¯ S\i Σi,i  S∈Ωk,d \S ∗ i∈(S ∗ )c ∩S  −1 2 1  ¯ , S = S ∗ \S ,  (5)  where ΣB = X T Π⊥ [B]X/n. [sent-138, score-0.118]
</p><p>44 Popular sparse regression algorithms, such as the Lasso and the OMP, can perform accurate support recovery when ζ 2 = maxi∈(S ∗ )c Σi,S ∗ Σ−1,S ∗ 2 < 1. [sent-139, score-0.425]
</p><p>45 2 that SWAP can perform accurate support recovery when γd < 1. [sent-141, score-0.318]
</p><p>46 , accurate support recovery is possible using the Lasso, then SWAP can be initialized by the output of the Lasso. [sent-146, score-0.34]
</p><p>47 In this case, γ(Ω) = 0 and SWAP also outputs the true support as long as S ∗ minimizes the loss function. [sent-147, score-0.148]
</p><p>48 • The parameter ζ directly computes correlations between the columns of X. [sent-151, score-0.114]
</p><p>49 In contrast, γd computes correlations between the columns of X when projected onto the null space of a matrix XB , where |B| = d − 1. [sent-152, score-0.151]
</p><p>50 The reason that the minimum appears in γd is because we choose to swap variables that result in the smallest loss. [sent-154, score-0.9]
</p><p>51 Throughout this Section, we assume the following: (A1) The observations y and the measurement matrix X follow the linear model in (1), where the noise is sub-Gaussian with parameter σ, and the columns of X have been normalized. [sent-158, score-0.161]
</p><p>52 The condition on n is mainly enforced to guarantee that the true support S ∗ minimizes the loss function. [sent-170, score-0.165]
</p><p>53 This condition is weaker than the sufﬁcient conditions required for other computationally tractable sparse recovery algorithms. [sent-171, score-0.349]
</p><p>54 As shown in [20], FoBa requires that n = Ω(log(p)/(ρ3 βmin )) k+ for high-dimensional consistent support recovery, where the choice of , which is greater than k, depends on the correlations in the matrix X. [sent-173, score-0.16]
</p><p>55 k+ This shows that if a sparse recovery algorithm can accurately estimate the true support, then SWAP does not introduce any false positives and also outputs the true support. [sent-176, score-0.386]
</p><p>56 Furthermore, if a sparse regression algorithm falsely detects one variable, then SWAP can potentially recover the correct support. [sent-177, score-0.182]
</p><p>57 Thus, using SWAP with other algorithms does not harm the sparse recovery performance of other algorithms. [sent-178, score-0.327]
</p><p>58 We now consider the more interesting case when SWAP is initialized by a support S (1) that falsely detects more than one variable. [sent-179, score-0.178]
</p><p>59 Furthermore, to ensure that the true support can be recovered, we need to impose some additional assumptions on the measurement matrix X. [sent-181, score-0.193]
</p><p>60 1, γk captures the correlations between the columns of XS ∗ and the columns of X(S ∗ )c . [sent-184, score-0.172]
</p><p>61 2 says that if SWAP is initialized with any support of size k, and γk satisﬁes the condition stated in the theorem, then SWAP will output the true support when given a sufﬁcient number of observations. [sent-191, score-0.227]
</p><p>62 , when σ = 0, the condition required for accurate support recovery reduces to γk < 1. [sent-194, score-0.335]
</p><p>63 2, outlined in [27], relies on imposing conditions on each support of size k such that that there exists a swap so that the loss can be necessarily decreased. [sent-196, score-1.006]
</p><p>64 Clearly, if such a property holds for each support, except S ∗ , then SWAP will output the true support since (i) there are only a ﬁnite number of possible supports, and (ii) each iteration of p SWAP results in a different support. [sent-197, score-0.107]
</p><p>65 The dependence on k in the expression for the number of observations n arises from applying the union bound over all supports of size k. [sent-198, score-0.106]
</p><p>66 We  5 Numerical Simulations In this section, we show how SWAP compares to other sparse recovery algorithms. [sent-207, score-0.31]
</p><p>67 The true support is chosen so that each variable in the support is assigned to a different block. [sent-216, score-0.188]
</p><p>68 The only difference is that the true support is chosen so that ﬁve different blocks contain active variables and each chosen block contains four active variables. [sent-227, score-0.212]
</p><p>69 In both (A1) and (A2), as a increases, the strength of correlations between the columns increases. [sent-229, score-0.114]
</p><p>70 We use the following sparse recovery algorithms to initialize SWAP: (i) Lasso, (ii) Thresholded Lasso (TLasso) [25], (iii) Forward-Backward (FoBa) [20], (iv) CoSaMP [21], (v) Marginal Regression (MaR), and (vi) Random. [sent-231, score-0.327]
</p><p>71 TLasso ﬁrst applies Lasso to select a superset of the support and then selects the largest k as the estimated support. [sent-232, score-0.114]
</p><p>72 MaR selects the support by choosing the largest k variables in |X T y|. [sent-237, score-0.119]
</p><p>73 When the number of observations increase to n = 200, we observe that all SWAP based algorithms perform better than standard sparse recovery algorithms. [sent-251, score-0.344]
</p><p>74 For (A1), we have exact support recovery for SWAP when a ≤ 0. [sent-252, score-0.304]
</p><p>75 For (A2), we have exact support recovery when a < 0. [sent-254, score-0.304]
</p><p>76 Figures 2(a) and 2(b) shows the mean number of iterations required by SWAP based algorithms as the correlations in the matrix X increase. [sent-257, score-0.139]
</p><p>77 For algorithms that estimate a large fraction of the true support (TLasso, FoBa, and CoSaMP), the number of iterations is generally very small. [sent-259, score-0.152]
</p><p>78 2 Gene Expression Data We now present results on two gene expression cancer datasets. [sent-325, score-0.124]
</p><p>79 The second dataset2 contains expression levels from patients with and without prostate cancer. [sent-327, score-0.111]
</p><p>80 The matrix X contains the gene expression values and the vector y is an indictor of the type of cancer a patient has. [sent-328, score-0.163]
</p><p>81 Although this is a classiﬁcation problem, we treat it as a recovery problem. [sent-329, score-0.223]
</p><p>82 We use leave-one-out crossvalidation and apply the sparse recovery algorithms described in Section 5. [sent-335, score-0.327]
</p><p>83 For each level of sparsity, we choose the sparse recovery algorithm (labeled as standard) and the SWAP based algorithm that results in the minimum least-squares loss over the training data. [sent-337, score-0.357]
</p><p>84 6 Summary and Future Work We studied the sparse recovery problem of estimating the support of a high-dimensional sparse vector when given a measurement matrix that contains correlated columns. [sent-370, score-0.645]
</p><p>85 We presented a simple algorithm, called SWAP, that iteratively swaps variables starting from an initial estimate of the support until an appropriate loss function can no longer be decreased further. [sent-371, score-0.198]
</p><p>86 We showed that SWAP is surprising effective in situations where the measurement matrix contains correlated columns. [sent-372, score-0.167]
</p><p>87 We theoretically quantiﬁed the conditions on the measurement matrix that guarantee accurate support recovery. [sent-373, score-0.218]
</p><p>88 Our theoretical results show that if SWAP is initialized with a support that contains some active variables, then SWAP can tolerate even higher correlations in the measurement matrix. [sent-374, score-0.261]
</p><p>89 Using numerical simulations on synthetic and real data, we showed how SWAP outperformed several sparse recovery algorithms. [sent-375, score-0.363]
</p><p>90 The second is a detailed analysis of SWAP when used with other sparse recovery algorithms. [sent-378, score-0.31]
</p><p>91 Thirion, “Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering,” in Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. [sent-439, score-0.39]
</p><p>92 Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Transactions Information Theory, vol. [sent-450, score-0.273]
</p><p>93 Wainwright, “Sharp thresholds for noisy and high-dimensional recovery of sparsity using 1 -constrained quadratic programming (Lasso),” IEEE Transactions Information Theory, vol. [sent-456, score-0.263]
</p><p>94 Yu, “Lasso-type recovery of sparse representations for highdimensional data,” Annals of Statistics, vol. [sent-461, score-0.338]
</p><p>95 Zhang, “Correlated variables in regresu u sion: clustering and sparse estimation,” Journal of Statistical Planning and Inference, vol. [sent-513, score-0.111]
</p><p>96 , “Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks,” Nature medicine, vol. [sent-531, score-0.111]
</p><p>97 Zhang, “Adaptive forward-backward greedy algorithm for learning sparse representations,” IEEE Transactions Information Theory, vol. [sent-536, score-0.115]
</p><p>98 Tropp, “CoSaMP: Iterative signal recovery from incomplete and inaccurate samples,” Applied and Computational Harmonic Analysis, vol. [sent-543, score-0.239]
</p><p>99 Zhou, “The adaptive and the thresholded lasso for potenu tially misspeciﬁed models (and a lower bound for the lasso),” Electronic Journal of Statistics, vol. [sent-566, score-0.104]
</p><p>100 Baraniuk, “Swapping variables for high-dimensional sparse regression with correlated measurements,” arXiv:1312. [sent-578, score-0.196]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('swap', 0.876), ('recovery', 0.223), ('foba', 0.141), ('cosamp', 0.127), ('tlasso', 0.113), ('tpr', 0.112), ('lasso', 0.088), ('sparse', 0.087), ('support', 0.081), ('xs', 0.069), ('correlated', 0.065), ('measurement', 0.063), ('mar', 0.062), ('columns', 0.058), ('correlations', 0.056), ('supports', 0.055), ('gene', 0.052), ('falsely', 0.05), ('hlmann', 0.046), ('vats', 0.042), ('sparsity', 0.04), ('cancer', 0.038), ('prostate', 0.035), ('cv', 0.034), ('expression', 0.034), ('degree', 0.033), ('correlation', 0.031), ('geer', 0.031), ('swaps', 0.031), ('measurements', 0.031), ('genes', 0.031), ('baraniuk', 0.03), ('iterations', 0.028), ('greedy', 0.028), ('loss', 0.027), ('true', 0.026), ('patients', 0.026), ('clearly', 0.026), ('eigenvalue', 0.026), ('meinshausen', 0.025), ('cancers', 0.025), ('houston', 0.025), ('leukemia', 0.025), ('omp', 0.025), ('entries', 0.025), ('detects', 0.025), ('accurately', 0.024), ('variables', 0.024), ('matrix', 0.023), ('xa', 0.023), ('wrapper', 0.023), ('active', 0.023), ('conditions', 0.022), ('initialized', 0.022), ('annals', 0.022), ('decreased', 0.02), ('level', 0.02), ('eigenvalues', 0.02), ('regression', 0.02), ('numerical', 0.02), ('blocks', 0.019), ('rice', 0.019), ('markers', 0.019), ('superset', 0.019), ('tropp', 0.019), ('orthogonal', 0.019), ('ik', 0.018), ('selection', 0.018), ('zhang', 0.018), ('condition', 0.017), ('algorithms', 0.017), ('legend', 0.017), ('initialization', 0.017), ('remark', 0.017), ('observations', 0.017), ('synthetic', 0.017), ('inactive', 0.017), ('tx', 0.017), ('simulations', 0.016), ('contains', 0.016), ('thresholded', 0.016), ('signal', 0.016), ('matrices', 0.015), ('iteratively', 0.015), ('mean', 0.015), ('designs', 0.015), ('theoretically', 0.015), ('statement', 0.014), ('accurate', 0.014), ('discusses', 0.014), ('minimizes', 0.014), ('recovering', 0.014), ('design', 0.014), ('null', 0.014), ('highdimensional', 0.014), ('selects', 0.014), ('representations', 0.014), ('restricted', 0.013), ('throughout', 0.013), ('includes', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="354-tfidf-1" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><p>2 0.092932239 <a title="354-tfidf-2" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>3 0.088555105 <a title="354-tfidf-3" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>Author: Yanshuai Cao, Marcus A. Brubaker, David Fleet, Aaron Hertzmann</p><p>Abstract: We propose an efﬁcient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case. 1</p><p>4 0.08769083 <a title="354-tfidf-4" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>5 0.087452553 <a title="354-tfidf-5" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>Author: Shouyuan Chen, Michael R. Lyu, Irwin King, Zenglin Xu</p><p>Abstract: Tensor completion from incomplete observations is a problem of signiﬁcant practical interest. However, it is unlikely that there exists an efﬁcient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Speciﬁcally, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art results. 1</p><p>6 0.08529526 <a title="354-tfidf-6" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>7 0.074231155 <a title="354-tfidf-7" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>8 0.059891045 <a title="354-tfidf-8" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>9 0.058107395 <a title="354-tfidf-9" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>10 0.04996255 <a title="354-tfidf-10" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>11 0.047901861 <a title="354-tfidf-11" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>12 0.047695525 <a title="354-tfidf-12" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>13 0.047432691 <a title="354-tfidf-13" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>14 0.046929833 <a title="354-tfidf-14" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>15 0.04245384 <a title="354-tfidf-15" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>16 0.040894445 <a title="354-tfidf-16" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>17 0.04060176 <a title="354-tfidf-17" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>18 0.039184235 <a title="354-tfidf-18" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>19 0.037809476 <a title="354-tfidf-19" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>20 0.037179101 <a title="354-tfidf-20" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.105), (1, 0.048), (2, 0.043), (3, 0.062), (4, -0.015), (5, 0.013), (6, -0.016), (7, 0.004), (8, -0.067), (9, 0.009), (10, 0.029), (11, -0.026), (12, -0.039), (13, -0.061), (14, -0.073), (15, -0.054), (16, 0.021), (17, -0.022), (18, -0.004), (19, -0.073), (20, 0.019), (21, 0.041), (22, -0.047), (23, -0.0), (24, 0.036), (25, 0.004), (26, -0.01), (27, 0.016), (28, -0.041), (29, -0.008), (30, -0.006), (31, 0.012), (32, 0.036), (33, 0.085), (34, -0.003), (35, 0.046), (36, 0.08), (37, -0.02), (38, -0.034), (39, -0.011), (40, -0.032), (41, -0.023), (42, 0.081), (43, 0.018), (44, 0.03), (45, -0.047), (46, 0.056), (47, -0.021), (48, -0.01), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88837469 <a title="354-lsi-1" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><p>2 0.76538241 <a title="354-lsi-2" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>3 0.66562366 <a title="354-lsi-3" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>4 0.63996923 <a title="354-lsi-4" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>5 0.61623353 <a title="354-lsi-5" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>6 0.60828263 <a title="354-lsi-6" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>7 0.5510115 <a title="354-lsi-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.51655173 <a title="354-lsi-8" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>9 0.48603782 <a title="354-lsi-9" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>10 0.48255309 <a title="354-lsi-10" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>11 0.48088825 <a title="354-lsi-11" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>12 0.47624624 <a title="354-lsi-12" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>13 0.47543758 <a title="354-lsi-13" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>14 0.45696849 <a title="354-lsi-14" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>15 0.45447499 <a title="354-lsi-15" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>16 0.45174351 <a title="354-lsi-16" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>17 0.44903505 <a title="354-lsi-17" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>18 0.43564755 <a title="354-lsi-18" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>19 0.41772941 <a title="354-lsi-19" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>20 0.41539422 <a title="354-lsi-20" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.015), (6, 0.224), (16, 0.058), (19, 0.019), (33, 0.161), (34, 0.121), (41, 0.016), (49, 0.042), (56, 0.084), (70, 0.022), (85, 0.035), (89, 0.048), (93, 0.034), (95, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80290985 <a title="354-lda-1" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><p>2 0.79650956 <a title="354-lda-2" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>Author: Michiel Hermans, Benjamin Schrauwen</p><p>Abstract: Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difﬁcult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales. 1</p><p>3 0.77440208 <a title="354-lda-3" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>Author: Corinna Cortes, Marius Kloft, Mehryar Mohri</p><p>Abstract: We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby beneﬁt from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efﬁcient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classiﬁcation tasks. 1</p><p>4 0.71769553 <a title="354-lda-4" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>Author: Fabian Sinz, Anna Stockl, January Grewe, January Benda</p><p>Abstract: We present a novel non-parametric method for ﬁnding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest. 1</p><p>5 0.71542555 <a title="354-lda-5" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>Author: Kevin Swersky, Jasper Snoek, Ryan P. Adams</p><p>Abstract: Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost. 1</p><p>6 0.71462512 <a title="354-lda-6" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>7 0.71271777 <a title="354-lda-7" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>8 0.7124573 <a title="354-lda-8" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>9 0.71147507 <a title="354-lda-9" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>10 0.71004021 <a title="354-lda-10" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>11 0.70885831 <a title="354-lda-11" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>12 0.70746434 <a title="354-lda-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.70677489 <a title="354-lda-13" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>14 0.70535058 <a title="354-lda-14" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>15 0.70456302 <a title="354-lda-15" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>16 0.70450968 <a title="354-lda-16" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>17 0.70411211 <a title="354-lda-17" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>18 0.70348275 <a title="354-lda-18" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>19 0.70305783 <a title="354-lda-19" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>20 0.70260698 <a title="354-lda-20" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
