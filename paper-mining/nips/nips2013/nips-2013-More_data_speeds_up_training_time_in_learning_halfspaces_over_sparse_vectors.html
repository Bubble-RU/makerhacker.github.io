<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-199" href="#">nips2013-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</h1>
<br/><p>Source: <a title="nips-2013-199-pdf" href="http://papers.nips.cc/paper/4905-more-data-speeds-up-training-time-in-learning-halfspaces-over-sparse-vectors.pdf">pdf</a></p><p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>Reference: <a title="nips-2013-199-reference" href="../nips2013_reference/nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 More data speeds up training time in learning halfspaces over sparse vectors  Amit Daniely Department of Mathematics The Hebrew University Jerusalem, Israel  Nati Linial School of CS and Eng. [sent-1, score-0.302]
</p><p>2 We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . [sent-5, score-0.502]
</p><p>3 Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. [sent-7, score-0.635]
</p><p>4 We further show that under stronger hardness assumptions, even O n1. [sent-8, score-0.19]
</p><p>5 A basic question is how to learn from “big data”. [sent-19, score-0.124]
</p><p>6 An intriguing question in this vein, studied recently by several researchers ([Decatur et al. [sent-25, score-0.116]
</p><p>7 The main contributions of this work are: 1  • Conditioning on the hardness of refuting random 3CNF formulas, we give the ﬁrst example of a natural supervised learning problem for which the answer to Question 1 is positive. [sent-29, score-0.597]
</p><p>8 • To prove this, we present a novel technique to establish computational-statistical tradeoffs in supervised learning problems. [sent-30, score-0.127]
</p><p>9 Additional contributions are non trivial efﬁcient algorithms for learning halfspaces over 2-sparse ˜ 2 ˜ 2 and 3-sparse vectors using O n and O n2 examples respectively. [sent-32, score-0.302]
</p><p>10 The natural learning problem we consider is the task of learning the class of halfspaces over k-sparse vectors. [sent-33, score-0.313]
</p><p>11 We consider the standard setting of agnostic PAC learning, which models the realistic scenario where the labels are not necessarily fully determined by some hypothesis from Hn,k . [sent-35, score-0.233]
</p><p>12 when some hypothesis from Hn,k has zero error, the problem of learning halfspaces is easy even over Rn . [sent-38, score-0.415]
</p><p>13 representation independent learning), namely, the learning algorithm is not restricted to output a hypothesis from Hn,k , but only should output a hypothesis whose error is not much larger than the error of the best hypothesis in Hn,k . [sent-42, score-0.531]
</p><p>14 Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [sent-45, score-0.26]
</p><p>15 Therefore, every upper bound for Hn,2 implies an upper bound for Pn , while every lower bound for Pn implies a lower bound for Hn,2 . [sent-59, score-0.202]
</p><p>16 Since VC(Pn ) = n and VC(Hn,2 ) = n + 1, the information theoretic barrier to learn these classes is Θ n . [sent-60, score-0.191]
</p><p>17 Indeed, since Hn,3 ⊂ Hn,k for every k ≥ 3, it is trivial that item 3 below holds for every k ≥ 3. [sent-66, score-0.228]
</p><p>18 The upper bound given in item 1 holds for every k. [sent-67, score-0.189]
</p><p>19 It is impossible to efﬁciently learn Hn,3 , if we are only provided with a training set of size O n under Feige’s assumption regarding the hardness of refuting random 3CNF 2 formulas [Feige, 2002]. [sent-75, score-0.767]
</p><p>20 5), it is impossible to learn efﬁciently with a training set of size O is formalized in Theorem 4. [sent-77, score-0.116]
</p><p>21 5  2  n n The proof of item 1 above is easy – simply note that Hn,3 has VC dimension n + 1. [sent-81, score-0.117]
</p><p>22 We note, however, that a weaker result, that still sufﬁces for answering Question 1 in the afﬁrmative, can be proven using a naive improper learning algorithm. [sent-84, score-0.138]
</p><p>23 In addition, we can efﬁciently ﬁnd a function f that minimizes the empirical training error over a training set S as follows: For every x ∈ Cn,k , if x does not appear at all in the training set we will set f (x) arbitrarily to 1. [sent-88, score-0.175]
</p><p>24 Our main result gives a positive answer to Question 1 for the task of improperly learning Hn,k for k ≥ 3. [sent-96, score-0.169]
</p><p>25 Since VC(Hn,1 ) = VC(Hn,2 ) = n + 1, the information theoretic barrier for learning these classes is Θ n . [sent-98, score-0.15]
</p><p>26 In section 4, we prove that Hn,2 (and, consequently, Hn,1 ⊂ Hn,2 ) can be learnt using 2 3  O n log2 (n) examples, indicating that signiﬁcant computational-statistical tradeoffs start to manifest themselves only for k ≥ 3. [sent-99, score-0.126]
</p><p>27 Under cryptographic assumptions, they showed that there exist binary learning problems, in which more data can provably be used to speed up training time. [sent-104, score-0.121]
</p><p>28 In all of these papers, the main idea is to construct a hypothesis class based on a one-way function. [sent-107, score-0.218]
</p><p>29 However, the constructed k  it is not hard to show that Hn,k can be learnt using a sample of Ω n2 examples by a naive improper learning algorithm, similar to the algorithm we describe in this section for k = 3. [sent-108, score-0.341]
</p><p>30 In this work, instead of using cryptographic assumptions, we rely on the hardness of refuting random 3CNF formulas. [sent-111, score-0.558]
</p><p>31 The simplicity and ﬂexibility of 3CNF formulas enable us to derive lower bounds for natural classes such as halfspaces. [sent-112, score-0.245]
</p><p>32 Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. [sent-115, score-0.317]
</p><p>33 The main reason for the difference is that in supervised learning problems, the learner is allowed to employ improper learning, which gives it a lot of power in choosing an adequate representation of the data. [sent-121, score-0.26]
</p><p>34 For example, the upper bound we have derived for the class of sparse halfspaces switched from representing hypotheses as halfspaces to representation of hypotheses as tables over Cn,3 , which made the learning problem easy from the computational perspective. [sent-122, score-0.635]
</p><p>35 The crux of the difﬁculty in constructing lower bounds is due to this freedom of the learner in choosing a convenient representation. [sent-123, score-0.127]
</p><p>36 This difﬁculty does not arise in the problem of sparse PCA detection, since there the learner must output a good sparse vector. [sent-124, score-0.115]
</p><p>37 2  Background and notation  For hypothesis class H ⊂ {±1}X and a set Y ⊂ X, we deﬁne the restriction of H to Y by H|Y = {h|Y | h ∈ H}. [sent-126, score-0.218]
</p><p>38 m  A learning algorithm, L, receives a sample S ∈ (Cn,3 × {±1}) and return a hypothesis L(S) : Cn,3 → {±1}. [sent-144, score-0.286]
</p><p>39 We say that L learns Hn,3 using m(n, ) examples if,2 for every distribution D on Cn,3 × {±1} and a sample S of more than m(n, ) i. [sent-145, score-0.236]
</p><p>40 examples drawn from D, 1 10 The algorithm L is efﬁcient if it runs in polynomial time in the sample size and returns a hypothesis that can be evaluated in polynomial time. [sent-148, score-0.288]
</p><p>41 2  Refuting random 3SAT formulas  We frequently view a boolean assignment to variables x1 , . [sent-150, score-0.329]
</p><p>42 An n-variables 3CNF clause is a boolean formula of the form C(x) = (−1)j1 xi1 ∨ (−1)j2 xi2 ∨ (−1)j1 xi3 , x ∈ {±1}n An n-variables 3CNF formula is a boolean formula of the form φ(x) = ∧m Ci (x) , i=1 2 For simplicity, we require the algorithm to succeed with probability of at least 9/10. [sent-155, score-0.765]
</p><p>43 Deﬁne the value, Val(φ), of φ as the maximal fraction of clauses that can be simultaneously satisﬁed. [sent-158, score-0.161]
</p><p>44 By 3CNFn,m we denote the set of 3CNF formulas with n variables and m clauses. [sent-160, score-0.179]
</p><p>45 Refuting random 3CNF formulas has been studied extensively (see e. [sent-161, score-0.179]
</p><p>46 It is known that for large enough ∆ (∆ = 6 will sufﬁce) a random formula in 3CNFn,∆n is not satisﬁable with probability 1 − o(1). [sent-165, score-0.119]
</p><p>47 Moreover, for every 0 ≤ < 1 , and a large 4 enough ∆ = ∆( ), the value of a random formula 3CNFn,∆n is ≤ 1 − with probability 1 − o(1). [sent-166, score-0.189]
</p><p>48 The problem of refuting random 3CNF concerns efﬁcient algorithms that provide a proof that a random 3CNF is not satisﬁable, or far from being satisﬁable. [sent-167, score-0.339]
</p><p>49 It can either produce a satisﬁable formula, or, produce a formula uniformly at random. [sent-170, score-0.119]
</p><p>50 The algorithm should identify whether the produced formula is random or satisﬁable. [sent-171, score-0.119]
</p><p>51 coins of A  (A(φ) = “exceptional”) ≥  3 4  • Completeness: For every n, (A(φ) = “typical”) ≥ 1 − o(1)  Pr Rand. [sent-174, score-0.208]
</p><p>52 coins of A, φ∼Uni(3CNFn,n∆(n) )  By a standard repetition argument, the probability of 3 can be ampliﬁed to 1 − 2−n , while efﬁciency 4 is preserved. [sent-175, score-0.138]
</p><p>53 Since for random φ ∈ 3CNFn,n∆(n) , A(φ) = “typical” with probability 1 − o(1), such an algorithm provides, for most 3CNF formulas a proof that their value is less that 1 − . [sent-177, score-0.208]
</p><p>54 Note that an algorithm that -refutes random 3CNF with ratio ∆ also -refutes random 3CNF with ratio ∆ for every 0 ≤ ≤ . [sent-178, score-0.136]
</p><p>55 Thus, the task of refuting random 3CNF’s gets easier as gets smaller. [sent-179, score-0.316]
</p><p>56 For every > 0 and for every large enough integer ∆ > ∆0 ( ) there exists no efﬁcient algorithm that -refutes random 3CNF formulas with ratio ∆. [sent-185, score-0.38]
</p><p>57 In fact, for all we know, the following conjecture may be true for every 0 ≤ µ ≤ 0. [sent-186, score-0.206]
</p><p>58 For every > 0 and for every integer ∆ > ∆0 ( ) there exists no efﬁcient algorithm that -refutes random 3CNF with ratio ∆ · nµ . [sent-190, score-0.201]
</p><p>59 Note that Feige’s conjecture is equivalent to the 0-R3SAT hardness assumption. [sent-191, score-0.326]
</p><p>60 2) is 1+µ true, then there exists no efﬁcient learning algorithm that learns the class Hn,3 using O n 2 examples. [sent-197, score-0.124]
</p><p>61 1 we rely on the validity of a conjecture, similar to conjecture 2. [sent-199, score-0.18]
</p><p>62 2) the validity of the conjecture on which we rely for majority formulas follows the validity of conjecture 2. [sent-202, score-0.569]
</p><p>63 By 3MAJn,m we denote the set of 3MAJ formulas with n variables and m clauses. [sent-205, score-0.179]
</p><p>64 If the µ-R3SAT hardness assumption is true, then for every > 0 and for every large enough integer ∆ > ∆0 ( ) there exists no efﬁcient algorithm with the following properties. [sent-210, score-0.358]
</p><p>65 coins of A  (A(φ) = “exceptional”) ≥  3 4  • For every n, (A(φ) = “typical”) ≥ 1 − o(1)  Pr Rand. [sent-213, score-0.208]
</p><p>66 coins of A, φ∼Uni(3MAJn,∆n1+µ )  Next, we prove Theorem 3. [sent-214, score-0.138]
</p><p>67 Namely, deﬁne d d the subclass Hn,3 ⊂ Hn,3 , of homogenous halfspaces with binary weights, given by Hn,3 = n {hw,0 | w ∈ {±1} }. [sent-217, score-0.273]
</p><p>68 As we show, under the µ-R3SAT hardness assumption, it is impossible to efﬁciently learn this subclass using only O  n1+µ 2  examples. [sent-218, score-0.306]
</p><p>69 Proof idea: We will reduce the task of refuting random 3MAJ formulas with linear number of d clauses to the task of (improperly) learning Hn,3 with linear number of samples. [sent-219, score-0.69]
</p><p>70 The ﬁrst step will be to construct a transformation that associates every 3MAJ clause with two examples in Cn,3 × {±1}, d and every assignment with a hypothesis in Hn,3 . [sent-220, score-0.603]
</p><p>71 As we will show, the hypothesis corresponding to an assignment ψ is correct on the two examples corresponding to a clause C if and only if ψ satisﬁes C. [sent-221, score-0.463]
</p><p>72 With that interpretation at hand, every 3MAJ formula φ can be thought of as a distribution Dφ on Cn,3 × {±1}, which is the empirical distribution induced by ψ’s clauses. [sent-222, score-0.189]
</p><p>73 To construct an efﬁcient algorithm for refuting 3MAJ-formulas, we simply feed n the learning algorithm with κ 0. [sent-225, score-0.282]
</p><p>74 012 examples drawn from Dφ and answer “exceptional” if the error of the hypothesis returned by the algorithm is small. [sent-226, score-0.301]
</p><p>75 If φ is (almost) satisﬁable, the algorithm is guaranteed to return a hypothesis with a small error. [sent-227, score-0.239]
</p><p>76 If the learning algorithm is proper, then it must return a hypothesis d from Hn,3 and therefore it would necessarily return a hypothesis with a large error. [sent-229, score-0.478]
</p><p>77 However, here we want to rule out improper algorithms as well. [sent-231, score-0.138]
</p><p>78 The crux of the construction is that if φ is random, no algorithm (even improper and even inefﬁcient) can return a hypothesis with a small error. [sent-232, score-0.418]
</p><p>79 012 samples, the algorithm won’t see most of ψ’s clauses, and, consequently, the produced hypothesis h will be independent of them. [sent-234, score-0.177]
</p><p>80 Since these clauses are random, h is likely to err on about half of them, so that ErrDφ (h) will be close to half! [sent-235, score-0.161]
</p><p>81 Indeed, if the learning algorithm obtains more than n1+µ examples, then it will see most of ψ’s clauses, and therefore it might succeed in “learning” even when the source of the formula is random. [sent-238, score-0.119]
</p><p>82 1) Assume by way of contradiction that the µ-R3SAT hardness assumption is 1+µ true and yet there exists an efﬁcient learning algorithm that learns the class Hn,3 using O n 2 1 examples. [sent-242, score-0.314]
</p><p>83 Setting = 100 , we conclude that there exists an efﬁcient algorithm L and a constant κ > 0 such that given a sample S of more than κ · n1+µ examples drawn from a distribution D on Cn,3 × {±1}, returns a classiﬁer L(S) : Cn,3 → {±1} such that  • L(S) can be evaluated efﬁciently. [sent-243, score-0.139]
</p><p>84 On input φ ∈ 3MAJn,∆n1+µ consisting of the 3MAJ clauses C1 , . [sent-251, score-0.161]
</p><p>85 For every clause, Ck = MAJ((−1)j1 xi1 , (−1)j2 xi2 , (−1)j3 xi3 ), generate an example (xk , yk ) ∈ Cn,3 × {±1} by choosing b ∈ {±1} at random and letting 3  (−1)jl eil , 1  (xk , yk ) = b ·  ∈ Cn,3 × {±1} . [sent-256, score-0.308]
</p><p>86 l=1  For example, if n = 6, the clause is MAJ(−x2 , x3 , x6 ) and b = −1, we generate the example ((0, 1, −1, 0, 0, −1), −1) 1+µ  2. [sent-257, score-0.184]
</p><p>87 coins of A  (A(φ) = “exceptional”) ≥  3 4  • For every n, (A(φ) = “typical”) ≥ 1 − o(1)  Pr Rand. [sent-267, score-0.208]
</p><p>88 coins of A, φ∼Uni(3MAJn,∆n1+µ )  Assume ﬁrst that φ ∈ 3MAJn,∆n1+µ is chosen at random. [sent-268, score-0.138]
</p><p>89 Given the sample S1 , the sample S2 := S \ S1 is a sample of |S2 | i. [sent-269, score-0.141]
</p><p>90 Moreover, for every example (xk , yk ) ∈ S2 , yk is a Bernoulli random variable 1 with parameter 2 which is independent of xk . [sent-273, score-0.394]
</p><p>91 To see that, note that an example whose instance is xk can be generated by exactly two clauses – one corresponds to yk = 1, while the other corresponds to yk = −1 (e. [sent-274, score-0.485]
</p><p>92 , the instance (1, −1, 0, 1) can be generated from the clause MAJ(x1 , −x2 , x4 ) and b = 1 or the clause MAJ(−x1 , x2 , −x4 ) and b = −1). [sent-276, score-0.368]
</p><p>93 Thus, given the instance xk , the probability 1 that yk = 1 is 2 , independent of xk . [sent-277, score-0.291]
</p><p>94 Let Ψ ∈ Hn,3 be the hypothesis Ψ(x) = sign ( ψ, x ). [sent-282, score-0.206]
</p><p>95 It can be easily checked that Ψ(xk ) = yk if and 1 3 only if ψ satisﬁes Ck . [sent-283, score-0.119]
</p><p>96 4  Upper bounds for learning Hn,2 and Hn,3  The following theorem derives upper bounds for learning Hn,2 and Hn,3 . [sent-287, score-0.119]
</p><p>97 An obvious open question is to close the gap between the lower and upper bounds. [sent-296, score-0.114]
</p><p>98 5 We conjecture that Hn,3 can be learnt efﬁciently using a sample of O n 2 examples. [sent-298, score-0.247]
</p><p>99 Also, we believe that our new proof technique can be used for establishing computational-sample complexity tradeoffs for other natural learning problems. [sent-299, score-0.143]
</p><p>100 Complexity theoretic lower bounds for sparse principal component detection. [sent-310, score-0.124]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('errd', 0.282), ('refuting', 0.282), ('halfspaces', 0.238), ('exceptional', 0.23), ('val', 0.224), ('feige', 0.207), ('errs', 0.198), ('hardness', 0.19), ('clause', 0.184), ('formulas', 0.179), ('hypothesis', 0.177), ('maj', 0.169), ('clauses', 0.161), ('coins', 0.138), ('improper', 0.138), ('conjecture', 0.136), ('yk', 0.119), ('formula', 0.119), ('boolean', 0.112), ('decatur', 0.1), ('pac', 0.093), ('vc', 0.093), ('berthet', 0.092), ('item', 0.088), ('cryptographic', 0.086), ('xk', 0.086), ('question', 0.083), ('improperly', 0.075), ('pn', 0.072), ('pr', 0.071), ('every', 0.07), ('typical', 0.069), ('rigollet', 0.069), ('theoretic', 0.066), ('supervised', 0.065), ('examples', 0.064), ('learnt', 0.064), ('tradeoffs', 0.062), ('return', 0.062), ('declare', 0.062), ('hazan', 0.061), ('answer', 0.06), ('jerusalem', 0.059), ('hebrew', 0.057), ('learner', 0.057), ('applebaum', 0.056), ('daniely', 0.056), ('dubios', 0.056), ('agnostic', 0.056), ('learns', 0.055), ('establishing', 0.052), ('ampli', 0.052), ('eran', 0.05), ('satis', 0.049), ('shai', 0.049), ('sample', 0.047), ('barrier', 0.047), ('israel', 0.047), ('validity', 0.044), ('nati', 0.043), ('linial', 0.043), ('ci', 0.043), ('class', 0.041), ('learn', 0.041), ('crux', 0.041), ('impossible', 0.04), ('radically', 0.039), ('assignment', 0.038), ('gave', 0.038), ('classes', 0.037), ('rn', 0.036), ('realizable', 0.035), ('gaps', 0.035), ('subclass', 0.035), ('training', 0.035), ('task', 0.034), ('th', 0.033), ('ratio', 0.033), ('chandrasekaran', 0.033), ('et', 0.033), ('reductions', 0.032), ('namely', 0.031), ('upper', 0.031), ('amit', 0.031), ('theorem', 0.03), ('majority', 0.03), ('hypotheses', 0.029), ('sign', 0.029), ('bounds', 0.029), ('sparse', 0.029), ('proof', 0.029), ('adversary', 0.029), ('preferences', 0.029), ('exists', 0.028), ('concerns', 0.028), ('concretely', 0.028), ('inef', 0.028), ('hard', 0.028), ('uni', 0.028), ('ciently', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="199-tfidf-1" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>2 0.10150943 <a title="199-tfidf-2" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>Author: Maria-Florina Balcan, Vitaly Feldman</p><p>Abstract: We describe a framework for designing efﬁcient active learning algorithms that are tolerant to random classiﬁcation noise and differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of ﬁltered random examples. It builds on the powerful statistical query framework of Kearns [30]. We show that any efﬁcient active statistical learning algorithm can be automatically converted to an efﬁcient active learning algorithm which is tolerant to random classiﬁcation noise as well as other forms of “uncorrelated” noise. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efﬁciently actively learned in our framework. These results combined with our generic conversion lead to the ﬁrst computationally-efﬁcient algorithms for actively learning some of these concept classes in the presence of random classiﬁcation noise that provide exponential improvement in the dependence on the error over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efﬁcient active differentially-private algorithms. This leads to the ﬁrst differentially-private active learning algorithms with exponential label savings over the passive case. 1</p><p>3 0.081892252 <a title="199-tfidf-3" href="./nips-2013-On_the_Complexity_and_Approximation_of_Binary_Evidence_in_Lifted_Inference.html">220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</a></p>
<p>Author: Guy van den Broeck, Adnan Darwiche</p><p>Abstract: Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model’s symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efﬁcient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically. 1</p><p>4 0.074861459 <a title="199-tfidf-4" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>Author: Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, S. Muthukrishnan</p><p>Abstract: Maximization of submodular functions has wide applications in machine learning and artiﬁcial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efﬁcient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem. 1</p><p>5 0.072295628 <a title="199-tfidf-5" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>Author: Cosma Shalizi, Aryeh Kontorovitch</p><p>Abstract: We informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with ﬁnite VC-dimension (IID processes are the simplest example). A mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate. In this paper, we argue that it is natural in predictive PAC to condition not on the past observations but on the mixture component of the sample path. This deﬁnition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data. In particular, we give a novel PAC generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component. We also provide a characterization of mixtures of absolutely regular (β-mixing) processes, of independent probability-theoretic interest. 1</p><p>6 0.067655273 <a title="199-tfidf-6" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>7 0.059080098 <a title="199-tfidf-7" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>8 0.059010405 <a title="199-tfidf-8" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>9 0.053219549 <a title="199-tfidf-9" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>10 0.052722502 <a title="199-tfidf-10" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>11 0.050286498 <a title="199-tfidf-11" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>12 0.048268985 <a title="199-tfidf-12" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>13 0.047968272 <a title="199-tfidf-13" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>14 0.046433941 <a title="199-tfidf-14" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>15 0.046196345 <a title="199-tfidf-15" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>16 0.04561758 <a title="199-tfidf-16" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>17 0.04543801 <a title="199-tfidf-17" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>18 0.045412615 <a title="199-tfidf-18" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>19 0.044517644 <a title="199-tfidf-19" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>20 0.043602195 <a title="199-tfidf-20" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.01), (2, 0.035), (3, -0.015), (4, 0.044), (5, 0.018), (6, -0.023), (7, -0.04), (8, -0.022), (9, 0.035), (10, 0.035), (11, -0.018), (12, -0.025), (13, 0.008), (14, 0.006), (15, -0.049), (16, -0.029), (17, 0.017), (18, 0.028), (19, 0.026), (20, -0.043), (21, -0.004), (22, 0.028), (23, -0.059), (24, -0.01), (25, 0.015), (26, -0.042), (27, 0.016), (28, -0.053), (29, 0.003), (30, -0.048), (31, 0.026), (32, -0.051), (33, -0.018), (34, -0.068), (35, -0.053), (36, 0.006), (37, 0.076), (38, 0.03), (39, 0.022), (40, -0.102), (41, 0.003), (42, 0.038), (43, -0.077), (44, 0.076), (45, 0.006), (46, -0.054), (47, -0.019), (48, -0.011), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88633686 <a title="199-lsi-1" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>2 0.6833958 <a title="199-lsi-2" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>Author: Cosma Shalizi, Aryeh Kontorovitch</p><p>Abstract: We informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with ﬁnite VC-dimension (IID processes are the simplest example). A mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate. In this paper, we argue that it is natural in predictive PAC to condition not on the past observations but on the mixture component of the sample path. This deﬁnition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data. In particular, we give a novel PAC generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component. We also provide a characterization of mixtures of absolutely regular (β-mixing) processes, of independent probability-theoretic interest. 1</p><p>3 0.62863082 <a title="199-lsi-3" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>Author: Xiaojin Zhu</p><p>Abstract: What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for ﬁnding the optimal teaching set. Our algorithm optimizes the aggregate sufﬁcient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. 1</p><p>4 0.58849227 <a title="199-lsi-4" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>Author: Karthika Mohan, Judea Pearl, Jin Tian</p><p>Abstract: We address the problem of recoverability i.e. deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called ‘Missingness Graphs’ to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we derive conditions that the graph should satisfy to ensure recoverability and devise algorithms to detect the presence of these conditions in the graph. 1</p><p>5 0.5883711 <a title="199-lsi-5" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>Author: Sivan Sabato, Anand D. Sarwate, Nati Srebro</p><p>Abstract: We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classiﬁcation in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be signiﬁcantly lower than the active label complexity. We also show a general competitive approach for learning with outcome-dependent costs. 1</p><p>6 0.57843339 <a title="199-lsi-6" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>7 0.55070162 <a title="199-lsi-7" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>8 0.54148477 <a title="199-lsi-8" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>9 0.5287618 <a title="199-lsi-9" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>10 0.50625741 <a title="199-lsi-10" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>11 0.49117127 <a title="199-lsi-11" href="./nips-2013-What_do_row_and_column_marginals_reveal_about_your_dataset%3F.html">352 nips-2013-What do row and column marginals reveal about your dataset?</a></p>
<p>12 0.48993549 <a title="199-lsi-12" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>13 0.48735535 <a title="199-lsi-13" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>14 0.48366049 <a title="199-lsi-14" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>15 0.48194775 <a title="199-lsi-15" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>16 0.47930139 <a title="199-lsi-16" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>17 0.47488263 <a title="199-lsi-17" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>18 0.47196108 <a title="199-lsi-18" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>19 0.47141552 <a title="199-lsi-19" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>20 0.47121546 <a title="199-lsi-20" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.012), (16, 0.013), (33, 0.084), (34, 0.049), (41, 0.021), (56, 0.665), (70, 0.013), (85, 0.02), (89, 0.016), (93, 0.017), (95, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98866093 <a title="199-lda-1" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>Author: Samory Kpotufe, Vikas Garg</p><p>Abstract: We present the ﬁrst result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown H¨ lder-continuity of the regression function at x. The result holds with o high probability simultaneously at all points x in a general metric space X of unknown structure. 1</p><p>same-paper 2 0.98184878 <a title="199-lda-2" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>3 0.95973855 <a title="199-lda-3" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>4 0.94786692 <a title="199-lda-4" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>Author: Tzu-Kuo Huang, Jeff Schneider</p><p>Abstract: Learning dynamic models from observed data has been a central issue in many scientiﬁc studies or engineering tasks. The usual setting is that data are collected sequentially from trajectories of some dynamical system operation. In quite a few modern scientiﬁc modeling tasks, however, it turns out that reliable sequential data are rather difﬁcult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer’s, or certain biological processes. Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze. Inspired by recent advances in spectral learning methods, we propose to study this problem from a different perspective: moment matching and spectral decomposition. Under that framework, we identify reasonable assumptions on the generative process of non-sequence data, and propose learning algorithms based on the tensor decomposition method [2] to provably recover ﬁrstorder Markov models and hidden Markov models. To the best of our knowledge, this is the ﬁrst formal guarantee on learning from non-sequence data. Preliminary simulation results conﬁrm our theoretical ﬁndings. 1</p><p>5 0.94614881 <a title="199-lda-5" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><p>6 0.92620575 <a title="199-lda-6" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>7 0.91498572 <a title="199-lda-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.90373605 <a title="199-lda-8" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>9 0.89225245 <a title="199-lda-9" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>10 0.86148155 <a title="199-lda-10" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>11 0.85215437 <a title="199-lda-11" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>12 0.84989834 <a title="199-lda-12" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>13 0.82666612 <a title="199-lda-13" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>14 0.82235074 <a title="199-lda-14" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>15 0.82085454 <a title="199-lda-15" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>16 0.81905502 <a title="199-lda-16" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>17 0.81265718 <a title="199-lda-17" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>18 0.80214548 <a title="199-lda-18" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>19 0.80183548 <a title="199-lda-19" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>20 0.79133189 <a title="199-lda-20" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
