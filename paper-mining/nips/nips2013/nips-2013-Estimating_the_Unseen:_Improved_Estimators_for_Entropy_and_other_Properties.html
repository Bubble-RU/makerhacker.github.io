<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-110" href="#">nips2013-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</h1>
<br/><p>Source: <a title="nips-2013-110-pdf" href="http://papers.nips.cc/paper/5170-estimating-the-unseen-improved-estimators-for-entropy-and-other-properties.pdf">pdf</a></p><p>Author: Paul Valiant, Gregory Valiant</p><p>Abstract: Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. 1</p><p>Reference: <a title="nips-2013-110-reference" href="../nips2013_reference/nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. [sent-3, score-0.138]
</p><p>2 Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). [sent-4, score-0.417]
</p><p>3 Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. [sent-6, score-0.17]
</p><p>4 This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. [sent-7, score-0.216]
</p><p>5 Additionally, many database management tasks employ sampling techniques to optimize query execution; improved estimators would allow for either smaller sample sizes or increased accuracy, leading to improved efﬁciency of the database system (see, e. [sent-20, score-0.161]
</p><p>6 We introduce a general and robust approach for using a sample to characterize the “unseen” portion of the distribution. [sent-23, score-0.17]
</p><p>7 Without any a priori assumptions about the distribution, one cannot know what the unseen domain elements are. [sent-24, score-0.362]
</p><p>8 Nevertheless, one can still hope to estimate the “shape” or histogram of the unseen portion of the distribution—essentially, we estimate how many unseen domain elements occur in various probability ranges. [sent-25, score-0.935]
</p><p>9 Given such a reconstruction, one can then use it to estimate any property of the distribution which only depends on the shape/histogram; such properties are termed symmetric and include entropy and support size. [sent-26, score-0.261]
</p><p>10 In light of the long history of work on estimating entropy by the neuroscience, statistics, computer science, and information theory communities, it is compelling that our approach (which is agnostic to the property in question) outperforms these entropy-speciﬁc estimators. [sent-27, score-0.175]
</p><p>11 Additionally, we extend this intuition to develop estimators for properties of pairs of distributions, the most important of which are the distance metrics. [sent-28, score-0.16]
</p><p>12 We demonstrate that our approach can accurately estimate the total variational distance (also known as statistical distance or ￿1 distance) between distributions using small samples. [sent-29, score-0.214]
</p><p>13 To illustrate the challenge of estimating variational distance (between distributions over discrete domains) given small samples, consider drawing two samples, each consisting of 1000 draws from a uniform distribution over 10,000 distinct elements. [sent-30, score-0.364]
</p><p>14 Each sample can contain at most 10% of the domain elements, and their intersection will likely contain only 1% of the domain elements; yet from this, one would like to conclude that these two samples must have been drawn from nearly identical distributions. [sent-31, score-0.287]
</p><p>15 1  Previous work: estimating distributions, and estimating properties  There is a long line of work on inferring information about the unseen portion of a distribution, beginning with independent contributions from both R. [sent-33, score-0.404]
</p><p>16 Fisher was presented with data on butterﬂies collected over a 2 year expedition in Malaysia, and sought to estimate the number of new species that would be discovered if a second 2 year expedition were conducted [8]. [sent-36, score-0.293]
</p><p>17 Good were working on the related problem of estimating the total probability mass accounted for by the unseen portion of a distribution [9]. [sent-40, score-0.393]
</p><p>18 posed the following natural question: given a sample, what distribution maximizes the likelihood of seeing the observed species frequencies, that is, the number of species observed once, twice, etc. [sent-45, score-0.285]
</p><p>19 There is also a large literature on estimating support size (also known as the “species problem”, and the related “distinct elements” problem), and we refer the reader to [16] and to [17] for several hundred references. [sent-55, score-0.155]
</p><p>20 Over the past 15 years, the theoretical computer science community has spent signiﬁcant effort developing estimators and establishing worst-case information theoretic lower bounds on the sample size required for various distribution estimation tasks, including entropy and support size (e. [sent-56, score-0.457]
</p><p>21 2  The algorithm we present here is based on the intuition of the estimator described in our theoretical work [1]. [sent-60, score-0.146]
</p><p>22 That estimator is not practically viable, and additionally, requires as input an accurate upper bound on the support size of the distribution in question. [sent-61, score-0.279]
</p><p>23 See Appendix A for further theoretical and practical comparisons with the estimator of [1]. [sent-64, score-0.146]
</p><p>24 Formally, F is the vector whose ith component, Fi , is the number of elements in the domain that occur exactly i times in sample X. [sent-76, score-0.303]
</p><p>25 For estimating entropy, or any other property whose value is invariant to relabeling the distribution support, the ﬁngerprint of a sample contains all the relevant information (see [21], for a formal proof of this fact). [sent-77, score-0.181]
</p><p>26 We note that in some of the literature, the ﬁngerprint is alternately termed the pattern, histogram, histogram of the histogram or collision statistics of the sample. [sent-78, score-0.416]
</p><p>27 In analogy with the ﬁngerprint of a sample, we deﬁne the histogram of a distribution, a representation in which the labels of the domain have been removed. [sent-79, score-0.293]
</p><p>28 The histogram of a distribution D is a mapping hD : (0, 1] → N ∪ {0}, where hD (x) is equal to the number of domain elements that each occur in distribution D with probability x. [sent-81, score-0.484]
</p><p>29 Formally, hD (x) = |{α : D(α) = x}|, where D(α) is the probability mass that distribution D assigns to domain element α. [sent-82, score-0.198]
</p><p>30 ￿ Since h(x) denotes the number of elements that have probability x, we have x:h(x)￿=0 x·h(x) = 1, as the total probability mass of a distribution is 1. [sent-84, score-0.152]
</p><p>31 Any symmetric property is a function of only the histogram of the distribution: • The Shannon entropy H(D) of a distribution D is deﬁned to be ￿ ￿ H(D) := − D(α) log2 D(α) = − hD (x)x log2 x. [sent-85, score-0.361]
</p><p>32 α∈sup(D)  x:hD (x)￿=0  • The support size is the number of domain elements that occur with positive probability: ￿ |sup(D)| := |{α : D(α) > 0}| = hD (x). [sent-86, score-0.31]
</p><p>33 Consider a sequence of animals, obtained as a sample from the distribution of animals on a certain island, X = (mouse, mouse, bird, cat, mouse, bird, bird, mouse, dog, mouse). [sent-88, score-0.159]
</p><p>34 We have F = (2, 0, 1, 0, 1), indicating that two species occurred exactly once (cat and dog), one species occurred exactly three times (bird), and one species occurred exactly ﬁve times (mouse). [sent-89, score-0.486]
</p><p>35 The associated histogram of this distribution is h : (0, 1] → Z deﬁned by h(1/16) = 4, h(1/4) = 1, h(1/2) = 1, and for all x ￿∈ {1/16, 1/4, 1/2}, h(x) = 0. [sent-91, score-0.241]
</p><p>36 Let D be the uniform distribution with support size 1000. [sent-96, score-0.159]
</p><p>37 Let X be a sample consisting of 500 independent draws from D. [sent-98, score-0.178]
</p><p>38 Each element of the domain, in expectation, will occur 1/2 times in X, and thus the number of occurrences of each domain element in the sample X will be roughly distributed as P oi(1/2). [sent-99, score-0.219]
</p><p>39 Thus we expect to see roughly 303 elements once, 76 elements twice, 13 elements three times, etc. [sent-102, score-0.252]
</p><p>40 , and in expectation 607 domain elements will not be seen at all. [sent-103, score-0.169]
</p><p>41 As an illustration of this approach, suppose we are given a sample of size k = 500, with ﬁngerprint F = (301, 78, 13, 1, 0, 0, . [sent-106, score-0.136]
</p><p>42 ); recalling Example 5, we recognize that F is very similar to the expected ﬁngerprint that we would obtain if the sample had been drawn from the uniform distribution over support 1000. [sent-109, score-0.233]
</p><p>43 Although the sample only contains 391 unique domain elements, we might be justiﬁed in concluding that the entropy of the true distribution from which the sample was drawn is close to H(U nif (1000)) = log2 (1000). [sent-110, score-0.509]
</p><p>44 In general, how does one obtain a “plausible” histogram from a ﬁngerprint in a principled fashion? [sent-111, score-0.239]
</p><p>45 Given a distribution D, and some domain element α occurring with probability x = D(α), the probability that it will be drawn exactly i times in k independent draws from D is P r[Binomial(k, x) = i] ≈ poi(kx, i). [sent-113, score-0.195]
</p><p>46 We will, roughly, invert the linear map from histograms to expected ﬁngerprint entries, to yield a map from observed ﬁngerprints, to plausible histograms h￿ . [sent-118, score-0.163]
</p><p>47 ), and consider the two histograms given by the uniform distributions with respective support sizes 10,000, and 100,000. [sent-124, score-0.186]
</p><p>48 99, yet these distributions are quite different and have very different entropy values and support sizes. [sent-126, score-0.228]
</p><p>49 To resolve this issue in a principled fashion, we strengthen our initial goal of “returning a histogram that could have plausibly generated the observed ﬁngerprint”: we instead return the simplest histogram that could have plausibly generated the observed ﬁngerprint. [sent-128, score-0.501]
</p><p>50 Recall the example above, where we observed only 10 distinct elements, but to explain the data we could either infer an additional 9,900 unseen elements, or an additional 99,000. [sent-129, score-0.239]
</p><p>51 In this sense, inferring “only” 9,900 additional unseen elements is the simplest explanation that ﬁts the data, in the spirit of Occam’s razor. [sent-130, score-0.277]
</p><p>52 1  The algorithm  We pose this problem of ﬁnding the simplest plausible histogram as a pair of linear programs. [sent-132, score-0.267]
</p><p>53 The constraint that h￿ corresponds to a histogram simply means that the total probability mass is 1, and all probability values are nonnegative. [sent-135, score-0.243]
</p><p>54 The second linear program will then ﬁnd the histogram h￿￿ of minimal support size, subject to the constraint that the distance between its expected ﬁngerprint, and the observed ﬁngerprint, is not much worse than that of the histogram found by the ﬁrst linear program. [sent-136, score-0.608]
</p><p>55 To make the linear programs ﬁnite, we consider a ﬁne mesh of values x1 , . [sent-137, score-0.14]
</p><p>56 , h￿ will correspond to the histogram values at these mesh points, with variable h￿ 1 i ￿ representing the number of domain elements that occur with probability xi , namely h￿ (xi ). [sent-144, score-0.534]
</p><p>57 A minor complicating issue is that this approach is designed for the challenging “rare events” regime, where there are many domain elements each seen only a handful of times. [sent-145, score-0.169]
</p><p>58 Hence we will split the ﬁngerprint into the “easy” and “hard” portions, and use the empirical estimator for the easy portion, and our linear programming approach for the hard portion. [sent-147, score-0.146]
</p><p>59 , Fm , derived from a sample of size k, vector x = x1 , . [sent-153, score-0.136]
</p><p>60 • Let vopt be the objective function value returned by running Linear Program 1 on input F ￿ , x. [sent-168, score-0.152]
</p><p>61 • Let h be the histogram returned by running Linear Program 2 on input F ￿ , x, vopt , α. [sent-169, score-0.36]
</p><p>62 , Fm , derived from a sample of size k, vector x = x1 , . [sent-177, score-0.136]
</p><p>63 , x￿ consisting of a ﬁne mesh of points in the interval (0, 1]. [sent-180, score-0.148]
</p><p>64 , h￿ : 1 ￿ ￿ ￿ ￿ m ￿ ￿ ￿ ￿ ￿ 1 ￿ ￿ √ hj · poi(kxj , i)￿ Minimize: ￿ Fi − ￿ 1 + Fi ￿ j=1 i=1 ￿￿ ￿ ￿ ￿ Subject to: j=1 xj hj = i Fi /k, and ∀j, hj ≥ 0. [sent-191, score-0.18]
</p><p>65 , Fm , derived from a sample of size k, vector x = x1 , . [sent-196, score-0.136]
</p><p>66 , x￿ consisting of a ﬁne mesh of points in the interval (0, 1], optimal objective function value vopt from Linear Program 1, and error parameter α > 0. [sent-199, score-0.261]
</p><p>67 , h￿ : 1 1 ￿ ￿ ￿ ￿ ￿￿ ￿m ￿￿ ￿ ￿ ￿ √1 Minimize: Subject to: ￿Fi − j=1 h￿ · poi(kxj , i)￿ ≤ vopt +α, j j=1 hj i=1 1+Fi ￿￿ ￿ ￿ ￿ j=1 xj hj = i Fi /k, and ∀j, hj ≥ 0. [sent-210, score-0.293]
</p><p>68 , x￿ , returns a histogram h￿ such that |H(D) − H(h￿ )| ≤ √0 . [sent-215, score-0.208]
</p><p>69 The information theoretic lower bounds of [1] show that there is some constant C1 such that for sufﬁciently large k, no algorithm can estimate the entropy of (worst-case) distributions of support size n to within ±0. [sent-220, score-0.298]
</p><p>70 We begin by brieﬂy discussing the ﬁve entropy estimators to which we compare our estimator in Figure 1. [sent-225, score-0.334]
</p><p>71 The “naive” estimator: the entropy of the empirical distribution, namely, given a ﬁngerprint F ￿ i i derived from a set of k samples, H naive (F) := − i Fi k | log2 k |. [sent-228, score-0.208]
</p><p>72 ￿k The jackknifed naive estimator [25, 26]: H JK (F) := k · H naive (F) − k−1 j=1 H naive (F −j ), k where F −j is the ﬁngerprint given by removing the contribution of the jth sample. [sent-230, score-0.448]
</p><p>73 The coverage adjusted estimator (CAE) [27]: Chao and Shen proposed the CAE, which is specifically designed to apply to settings in which there is a signiﬁcant component of the distribution that is unseen, and was shown to perform well in practice in [22]. [sent-231, score-0.179]
</p><p>74 4 Given a ﬁngerprint F derived from a set of k samples, let Ps := 1 − F1 /k be the Good–Turing estimate of the probability mass of the “seen” portion of the distribution [9]. [sent-232, score-0.172]
</p><p>75 The CAE adjusts the empirical probabilities according to Ps , then applies the Horvitz–Thompson estimator for population totals [28] to take into account the probability that the elements were seen. [sent-233, score-0.253]
</p><p>76 H CAE (F) := − Fi k 1 − (1 − (i/k)Ps ) i The Best Upper Bound estimator [15]: The ﬁnal estimator to which we compare ours is the Best Upper Bound (BUB) estimator of Paninski. [sent-235, score-0.438]
</p><p>77 This estimator is obtained by searching for a minimax linear estimator, with respect to a certain error metric. [sent-236, score-0.146]
</p><p>78 The linear estimators of [2] can be viewed as a variant of this estimator with provable performance bounds. [sent-237, score-0.214]
</p><p>79 5 The BUB estimator requires, as input, an upper bound on the support size of the distribution from which the samples are drawn; if the bound provided is inaccurate, the performance degrades considerably, as was also remarked in [22]. [sent-238, score-0.279]
</p><p>80 In our experiments, we used Paninski’s implementation of the BUB estimator (publicly available on his website), with default parameters. [sent-239, score-0.146]
</p><p>81 For the distributions with ﬁnite support, we gave the true support size as input, and thus we are arguably comparing our estimator to the best–case performance of the BUB estimator. [sent-240, score-0.297]
</p><p>82 Given a sample of size k from a uniform distribution over k elements, it is not hard to show that the bias of the CAE is Ω(log k). [sent-243, score-0.195]
</p><p>83 For comparison, even the naive estimator has error bounded by a constant in the limit as k → ∞ in this setting. [sent-245, score-0.234]
</p><p>84 5 We also implemented the linear estimators of [2], though found that the BUB estimator performed better. [sent-247, score-0.214]
</p><p>85 5  3  10  4  10 Sample Size  5  10  0  4  5  10 10 Sample Size  6  10  Figure 1: Plots depicting the square root of the mean squared error (RMSE) of each entropy estimator over  500 trials, plotted as a function of the sample size; note the logarithmic scaling of the x-axis. [sent-278, score-0.388]
</p><p>86 The samples are drawn from six classes of distributions: the uniform distribution, U nif [n] that assigns probability pi = 1/n 5 for i = 1, 2, . [sent-279, score-0.196]
</p><p>87 , n; an even mixture of U nif [ n ] and U nif [ 4n ], which assigns probability pi = 2n for 5 5 n 5 n i = 1, . [sent-282, score-0.207]
</p><p>88 , n; the geometric distribution j=1  Geom[n], which has inﬁnite support and assigns probability pi = (1/n)(1 − 1/n)i , for i = 1, 2 . [sent-297, score-0.175]
</p><p>89 used as the support of the returned histogram was chosen to be a coarse geometric mesh, with x1 = 1/k2 , and xi = 1. [sent-310, score-0.304]
</p><p>90 The estimate of the distance is plotted along with error bars at plus and minus one standard deviation; our results are compared with those for the naive estimator (the distance between the empirical distributions). [sent-331, score-0.397]
</p><p>91 The unseen estimator can be seen to reliably distinguish between the d = 0, d = 1 , and d = 1 2 cases even for samples as small as several hundred. [sent-332, score-0.339]
</p><p>92 1  Estimating ￿1 distance and number of words in Hamlet  The other two properties that we consider do not have such widely-accepted estimators as entropy, and thus our evaluation of the unseen estimator will be more qualitative. [sent-334, score-0.499]
</p><p>93 Figure 2 shows the results of estimating the total variation distance (￿1 distance). [sent-336, score-0.148]
</p><p>94 Because total variation distance is a property of two distributions instead of one, ﬁngerprints and histograms are two-dimensional objects in this setting (see Section 4. [sent-337, score-0.196]
</p><p>95 We use this example to showcase the ﬂexibility of our linear programming approach—our estimator can be customized to particular domains in powerful and principled ways by adding or modifying the constraints of the linear program. [sent-340, score-0.177]
</p><p>96 To estimate the histogram of word frequencies in Hamlet, we note that the play is of length ≈ 25, 000, and thus the 1 minimum probability with which any word can occur is 25,000 . [sent-341, score-0.303]
</p><p>97 Estimating the unseen: an n/ log(n)–sample estimator for entropy and support size, shown optimal via new CLTs. [sent-357, score-0.323]
</p><p>98 The relation between the number of species and the number of individuals in a random sample of an animal population. [sent-401, score-0.219]
</p><p>99 The population frequencies of species and the estimation of population parameters. [sent-406, score-0.199]
</p><p>100 Nonparametric estimation of shannons index of diversity when there are unseen species in sample. [sent-516, score-0.319]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ngerprint', 0.602), ('rmse', 0.212), ('histogram', 0.208), ('unseen', 0.193), ('zipf', 0.188), ('cae', 0.166), ('fi', 0.147), ('estimator', 0.146), ('bub', 0.133), ('hamlet', 0.132), ('orlitsky', 0.132), ('species', 0.126), ('hd', 0.126), ('entropy', 0.12), ('mesh', 0.116), ('valiant', 0.116), ('ngerprints', 0.113), ('vopt', 0.113), ('poi', 0.1), ('sample', 0.093), ('naive', 0.088), ('domain', 0.085), ('elements', 0.084), ('geom', 0.083), ('mouse', 0.08), ('portion', 0.077), ('distance', 0.068), ('estimators', 0.068), ('program', 0.067), ('nif', 0.061), ('bird', 0.061), ('hj', 0.06), ('plausible', 0.059), ('turing', 0.058), ('support', 0.057), ('acharya', 0.056), ('fingerprint', 0.056), ('mixgeomzipf', 0.056), ('mixunif', 0.056), ('rare', 0.055), ('estimating', 0.055), ('draws', 0.053), ('histograms', 0.052), ('distributions', 0.051), ('poisson', 0.051), ('ps', 0.051), ('distinct', 0.046), ('assigns', 0.045), ('size', 0.043), ('occur', 0.041), ('pi', 0.04), ('returned', 0.039), ('unif', 0.038), ('batu', 0.038), ('dreamt', 0.038), ('expedition', 0.038), ('istogram', 0.038), ('jackknifed', 0.038), ('lausible', 0.038), ('mutations', 0.038), ('shakespeare', 0.038), ('fm', 0.036), ('occurred', 0.036), ('oi', 0.035), ('mass', 0.035), ('dog', 0.034), ('animals', 0.033), ('distribution', 0.033), ('chao', 0.033), ('consisting', 0.032), ('year', 0.032), ('kxj', 0.031), ('horvitz', 0.031), ('exceptionally', 0.031), ('ind', 0.031), ('santhanam', 0.031), ('principled', 0.031), ('binomial', 0.03), ('websites', 0.029), ('passage', 0.029), ('depicting', 0.029), ('genetic', 0.029), ('concentrated', 0.029), ('cat', 0.028), ('appendix', 0.028), ('website', 0.027), ('plausibly', 0.027), ('frequencies', 0.027), ('estimate', 0.027), ('uniform', 0.026), ('ecological', 0.026), ('append', 0.026), ('variation', 0.025), ('british', 0.025), ('programs', 0.024), ('fisher', 0.024), ('les', 0.024), ('drawn', 0.024), ('properties', 0.024), ('population', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="110-tfidf-1" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>Author: Paul Valiant, Gregory Valiant</p><p>Abstract: Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. 1</p><p>2 0.13896562 <a title="110-tfidf-2" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>3 0.10058603 <a title="110-tfidf-3" href="./nips-2013-Spike_train_entropy-rate_estimation_using_hierarchical_Dirichlet_process_priors.html">308 nips-2013-Spike train entropy-rate estimation using hierarchical Dirichlet process priors</a></p>
<p>Author: Karin C. Knudson, Jonathan W. Pillow</p><p>Abstract: Entropy rate quantiﬁes the amount of disorder in a stochastic process. For spiking neurons, the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information, and a large literature has focused on the problem of estimating entropy rate from spike train data. Here we present Bayes least squares and empirical Bayesian entropy rate estimators for binary spike trains using hierarchical Dirichlet process (HDP) priors. Our estimator leverages the fact that the entropy rate of an ergodic Markov Chain with known transition probabilities can be calculated analytically, and many stochastic processes that are non-Markovian can still be well approximated by Markov processes of sufﬁcient depth. Choosing an appropriate depth of Markov model presents challenges due to possibly long time dependencies and short data sequences: a deeper model can better account for long time dependencies, but is more difﬁcult to infer from limited data. Our approach mitigates this difﬁculty by using a hierarchical prior to share statistical power across Markov chains of different depths. We present both a fully Bayesian and empirical Bayes entropy rate estimator based on this model, and demonstrate their performance on simulated and real neural spike train data. 1</p><p>4 0.085247494 <a title="110-tfidf-4" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>Author: Evan W. Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: Shannon’s entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difﬁcult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefﬁcient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to ﬂexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We deﬁne two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efﬁcient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.</p><p>5 0.071933843 <a title="110-tfidf-5" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><p>6 0.069156706 <a title="110-tfidf-6" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>7 0.065215118 <a title="110-tfidf-7" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>8 0.063875787 <a title="110-tfidf-8" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>9 0.059541605 <a title="110-tfidf-9" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>10 0.056219373 <a title="110-tfidf-10" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>11 0.04968505 <a title="110-tfidf-11" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>12 0.048686244 <a title="110-tfidf-12" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>13 0.047835715 <a title="110-tfidf-13" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>14 0.045794118 <a title="110-tfidf-14" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>15 0.044545222 <a title="110-tfidf-15" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>16 0.04369529 <a title="110-tfidf-16" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>17 0.042952418 <a title="110-tfidf-17" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>18 0.042503174 <a title="110-tfidf-18" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>19 0.042439535 <a title="110-tfidf-19" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>20 0.042128012 <a title="110-tfidf-20" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.048), (2, -0.011), (3, -0.004), (4, 0.006), (5, 0.034), (6, 0.019), (7, -0.003), (8, -0.019), (9, 0.073), (10, -0.02), (11, 0.002), (12, -0.063), (13, -0.003), (14, -0.009), (15, -0.011), (16, 0.048), (17, 0.028), (18, -0.039), (19, 0.068), (20, -0.031), (21, 0.032), (22, -0.081), (23, -0.066), (24, -0.003), (25, 0.043), (26, 0.015), (27, -0.045), (28, 0.001), (29, -0.025), (30, -0.02), (31, 0.03), (32, -0.051), (33, 0.046), (34, 0.042), (35, 0.035), (36, -0.037), (37, 0.097), (38, -0.032), (39, 0.082), (40, 0.002), (41, -0.034), (42, -0.01), (43, -0.013), (44, 0.066), (45, -0.035), (46, 0.036), (47, -0.014), (48, 0.01), (49, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89493507 <a title="110-lsi-1" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>Author: Paul Valiant, Gregory Valiant</p><p>Abstract: Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. 1</p><p>2 0.65007246 <a title="110-lsi-2" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>Author: Evan W. Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: Shannon’s entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difﬁcult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefﬁcient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to ﬂexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We deﬁne two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efﬁcient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.</p><p>3 0.63903487 <a title="110-lsi-3" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>Author: Isik B. Fidaner, Taylan Cemgil</p><p>Abstract: Inﬁnite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or ﬁnd its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based deﬁnition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various inﬁnite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.</p><p>4 0.62982929 <a title="110-lsi-4" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>Author: Il M. Park, Evan W. Archer, Kenneth Latimer, Jonathan W. Pillow</p><p>Abstract: Probabilistic models for binary spike patterns provide a powerful tool for understanding the statistical dependencies in large-scale neural recordings. Maximum entropy (or “maxent”) models, which seek to explain dependencies in terms of low-order interactions between neurons, have enjoyed remarkable success in modeling such patterns, particularly for small groups of neurons. However, these models are computationally intractable for large populations, and low-order maxent models have been shown to be inadequate for some datasets. To overcome these limitations, we propose a family of “universal” models for binary spike patterns, where universality refers to the ability to model arbitrary distributions over all 2m binary patterns. We construct universal models using a Dirichlet process centered on a well-behaved parametric base measure, which naturally combines the ﬂexibility of a histogram and the parsimony of a parametric model. We derive computationally efﬁcient inference methods using Bernoulli and cascaded logistic base measures, which scale tractably to large populations. We also establish a condition for equivalence between the cascaded logistic and the 2nd-order maxent or “Ising” model, making cascaded logistic a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data. 1</p><p>5 0.62835062 <a title="110-lsi-5" href="./nips-2013-Spike_train_entropy-rate_estimation_using_hierarchical_Dirichlet_process_priors.html">308 nips-2013-Spike train entropy-rate estimation using hierarchical Dirichlet process priors</a></p>
<p>Author: Karin C. Knudson, Jonathan W. Pillow</p><p>Abstract: Entropy rate quantiﬁes the amount of disorder in a stochastic process. For spiking neurons, the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information, and a large literature has focused on the problem of estimating entropy rate from spike train data. Here we present Bayes least squares and empirical Bayesian entropy rate estimators for binary spike trains using hierarchical Dirichlet process (HDP) priors. Our estimator leverages the fact that the entropy rate of an ergodic Markov Chain with known transition probabilities can be calculated analytically, and many stochastic processes that are non-Markovian can still be well approximated by Markov processes of sufﬁcient depth. Choosing an appropriate depth of Markov model presents challenges due to possibly long time dependencies and short data sequences: a deeper model can better account for long time dependencies, but is more difﬁcult to infer from limited data. Our approach mitigates this difﬁculty by using a hierarchical prior to share statistical power across Markov chains of different depths. We present both a fully Bayesian and empirical Bayes entropy rate estimator based on this model, and demonstrate their performance on simulated and real neural spike train data. 1</p><p>6 0.55238718 <a title="110-lsi-6" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>7 0.54528308 <a title="110-lsi-7" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>8 0.53514576 <a title="110-lsi-8" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>9 0.5120815 <a title="110-lsi-9" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>10 0.50874168 <a title="110-lsi-10" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>11 0.50624448 <a title="110-lsi-11" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>12 0.4938722 <a title="110-lsi-12" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>13 0.49053699 <a title="110-lsi-13" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>14 0.47656742 <a title="110-lsi-14" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>15 0.46443185 <a title="110-lsi-15" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>16 0.46102399 <a title="110-lsi-16" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>17 0.45851076 <a title="110-lsi-17" href="./nips-2013-Transportability_from_Multiple_Environments_with_Limited_Experiments.html">337 nips-2013-Transportability from Multiple Environments with Limited Experiments</a></p>
<p>18 0.45728645 <a title="110-lsi-18" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>19 0.45719212 <a title="110-lsi-19" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>20 0.45467049 <a title="110-lsi-20" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.037), (16, 0.032), (30, 0.24), (33, 0.158), (34, 0.091), (41, 0.035), (49, 0.035), (56, 0.101), (70, 0.035), (85, 0.041), (89, 0.039), (93, 0.06), (95, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84095949 <a title="110-lda-1" href="./nips-2013-Accelerated_Mini-Batch_Stochastic_Dual_Coordinate_Ascent.html">19 nips-2013-Accelerated Mini-Batch Stochastic Dual Coordinate Ascent</a></p>
<p>Author: Shai Shalev-Shwartz, Tong Zhang</p><p>Abstract: Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007]. 1</p><p>same-paper 2 0.78025645 <a title="110-lda-2" href="./nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</a></p>
<p>Author: Paul Valiant, Gregory Valiant</p><p>Abstract: Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. 1</p><p>3 0.73542625 <a title="110-lda-3" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>Author: Shouyuan Chen, Michael R. Lyu, Irwin King, Zenglin Xu</p><p>Abstract: Tensor completion from incomplete observations is a problem of signiﬁcant practical interest. However, it is unlikely that there exists an efﬁcient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Speciﬁcally, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art results. 1</p><p>4 0.73173475 <a title="110-lda-4" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>Author: Alessandro Rudi, Guillermo D. Canas, Lorenzo Rosasco</p><p>Abstract: A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples. In this paper we introduce a general formulation of this problem and derive novel learning error estimates. Our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribution, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods. 1</p><p>5 0.68290639 <a title="110-lda-5" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><p>6 0.68194878 <a title="110-lda-6" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>7 0.68140537 <a title="110-lda-7" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>8 0.68007839 <a title="110-lda-8" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>9 0.67998952 <a title="110-lda-9" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>10 0.67905027 <a title="110-lda-10" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>11 0.67881382 <a title="110-lda-11" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>12 0.67808497 <a title="110-lda-12" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>13 0.67804712 <a title="110-lda-13" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>14 0.67792279 <a title="110-lda-14" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>15 0.67764699 <a title="110-lda-15" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>16 0.67670506 <a title="110-lda-16" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>17 0.67632532 <a title="110-lda-17" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>18 0.67620432 <a title="110-lda-18" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>19 0.67610407 <a title="110-lda-19" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>20 0.67560315 <a title="110-lda-20" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
