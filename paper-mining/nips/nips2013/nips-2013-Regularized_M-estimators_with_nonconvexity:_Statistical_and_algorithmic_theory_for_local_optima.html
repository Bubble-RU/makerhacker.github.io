<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-271" href="#">nips2013-271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</h1>
<br/><p>Source: <a title="nips-2013-271-pdf" href="http://papers.nips.cc/paper/4904-regularized-m-estimators-with-nonconvexity-statistical-and-algorithmic-theory-for-local-optima.pdf">pdf</a></p><p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>Reference: <a title="nips-2013-271-reference" href="../nips2013_reference/nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Regularized M -estimators with nonconvexity: Statistical and algorithmic theory for local optima  Martin J. [sent-1, score-0.264]
</p><p>2 edu  Abstract We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. [sent-5, score-0.533]
</p><p>3 Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. [sent-6, score-0.705]
</p><p>4 Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. [sent-7, score-1.237]
</p><p>5 On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. [sent-8, score-1.175]
</p><p>6 1  Introduction  Optimization of nonconvex functions is known to be computationally intractable in general [11, 12]. [sent-10, score-0.417]
</p><p>7 Unlike convex functions, nonconvex functions may possess local optima that are not global optima, and standard iterative methods such as gradient descent and coordinate descent are only guaranteed to converge to local optima. [sent-11, score-1.128]
</p><p>8 Although statistical results regarding nonconvex M -estimation often only provide guarantees about the accuracy of global optima, it is observed empirically that the local optima obtained by various estimation algorithms seem to be well-behaved. [sent-12, score-0.816]
</p><p>9 In this paper, we study the question of whether it is possible to certify “good” behavior, in both a statistical and computational sense, for various nonconvex M -estimators. [sent-13, score-0.454]
</p><p>10 On the statistical level, we provide an abstract result, applicable to a broad class of (potentially nonconvex) M -estimators, which bounds the distance between any local optimum and the unique minimum of the population risk. [sent-14, score-0.332]
</p><p>11 Although local optima of nonconvex objectives may not coincide with global optima, our theory shows that any local optimum is essentially as good as a global optimum from a statistical perspective. [sent-15, score-1.146]
</p><p>12 In addition to nonconvex loss functions, our theory also applies to nonconvex regularizers, shedding new light on a long line of recent work involving the nonconvex SCAD and MCP regularizers [3, 2, 13, 14]. [sent-17, score-1.41]
</p><p>13 Various methods have been proposed for optimizing convex loss functions with nonconvex penalties [3, 4, 15], but these methods are only guaranteed to generate local optima of the composite objective, which have not been proven to be well-behaved. [sent-18, score-1.059]
</p><p>14 In fact, we establish that under suitable conditions, a modiﬁed form of composite gradient descent only requires log(1/ stat ) iterations to obtain a solution that is accurate up to the statistical precision stat . [sent-20, score-1.011]
</p><p>15 We ﬁrst establish notation, then discuss assumptions for nonconvex regularizers and losses studied in our paper. [sent-27, score-0.573]
</p><p>16 Our goal is to estimate the parameter vector β ∗ := arg min L(β) that minimizes the population p β∈R  risk, assumed to be unique. [sent-35, score-0.148]
</p><p>17 To this end, we consider a regularized M -estimator of the form n β ∈ arg min {Ln (β; Z1 ) + ρλ (β)} , g(β)≤R  (1)  where ρλ : Rp → R is a regularizer, depending on a tuning parameter λ > 0, which serves to enforce a certain type of structure on the solution. [sent-36, score-0.141]
</p><p>18 In all cases, we consider regularizers that are p separable across coordinates, and with a slight abuse of notation, we write ρλ (β) = j=1 ρλ (βj ). [sent-37, score-0.169]
</p><p>19 Our theory allows for possible nonconvexity in both the loss function Ln and the regularizer ρλ . [sent-38, score-0.267]
</p><p>20 Consequently, any feasible point for the optimization problem (1) satisﬁes the constraint β 1 ≤ R, and as long as the empirical loss and regularizer are continuous, the Weierstrass extreme value theorem guarantees that a global minimum β exists. [sent-40, score-0.374]
</p><p>21 2  Nonconvex regularizers  We now state and discuss conditions on the regularizer, deﬁned in terms of ρλ : R → R. [sent-42, score-0.187]
</p><p>22 3  Nonconvex loss functions and restricted strong convexity  Throughout this paper, we require the loss function Ln to be differentiable, but we do not require it to be convex. [sent-56, score-0.268]
</p><p>23 On the other hand, if Ln is convex (but not strongly convex), the left-hand expression in inequality (4) is always nonnegative, so inequalities (4a) and (4b) hold ∆ 1 α1 n α2 n trivially for ∆ 1 ≥ ∆ 2 τ1 log p and ∆ 2 ≥ τ2 log p , respectively. [sent-62, score-0.316]
</p><p>24 Hence, the RSC inequalities only enforce a type of strong convexity condition over a cone set of the form  3  ∆ ∆  1 2  ≤c  n log p  . [sent-63, score-0.242]
</p><p>25 Statistical guarantees and consequences  We now turn to our main statistical guarantees and some consequences for various statistical models. [sent-64, score-0.268]
</p><p>26 Our theory applies to any vector β ∈ Rp that satisﬁes the ﬁrst-order necessary conditions to be a local minimum of the program (1): Ln (β) +  ρλ (β), β − β ≥ 0,  for all feasible β ∈ Rp . [sent-65, score-0.286]
</p><p>27 1  Main statistical results  Our main theorem is deterministic in nature, and speciﬁes conditions on the regularizer, loss function, and parameters, which guarantee that any local optimum β lies close to the target vector β ∗ = arg min L(β). [sent-68, score-0.497]
</p><p>28 Suppose the regularizer ρλ satisﬁes Assumption 1, Ln satisﬁes the RSC conditions (4) with α1 > µ, and β ∗ is feasible for the objective. [sent-72, score-0.151]
</p><p>29 Consider any choice of λ such that 2 · max L  Ln (β ∗ )  ∞,  α2  3  log p n  ≤ λ ≤  α2 , 6RL  (6)  16R2 max(τ 2 ,τ 2 )  1 2 log p. [sent-73, score-0.242]
</p><p>30 for  many statistical models, in which case we have a squared  2 -error  that scales as  k log p n ,  as expected. [sent-79, score-0.185]
</p><p>31 Recall that α1 measures the level of curvature of the loss function Ln , while µ measures the level of nonconvexity of the penalty ρλ . [sent-82, score-0.32]
</p><p>32 We now develop corollaries for various nonconvex loss functions and regularizers of interest. [sent-84, score-0.632]
</p><p>33 2  Corrected linear regression  We begin by considering the case of high-dimensional linear regression with systematically corrupted observations. [sent-86, score-0.352]
</p><p>34 Recall that in the framework of ordinary linear regression, we have the model yi =  β ∗ , xi p j=1  +  i,  for i = 1, . [sent-87, score-0.143]
</p><p>35 Following Loh i=1 and Wainwright [6], assume we instead observe pairs {(zi , yi )}n , where the zi ’s are systematically i=1 corrupted versions of the corresponding xi ’s. [sent-91, score-0.267]
</p><p>36 Some examples include the following: (a) Additive noise: Observe zi = xi + wi , where wi ⊥ xi , E[wi ] = 0, and cov[wi ] = Σw . [sent-92, score-0.215]
</p><p>37 ⊥ (b) Missing data: For ϑ ∈ [0, 1), observe zi ∈ Rp such that for each component j, we independently observe zij = xij with probability 1 − ϑ, and zij = ∗ with probability ϑ. [sent-93, score-0.146]
</p><p>38 We use the population and empirical loss functions L(β) =  1 T β Σx β − β ∗T Σx β, 2  and  Ln (β) =  1 T β Γβ − γ T β, 2  (9)  where (Γ, γ) are estimators for (Σx , Σx β ∗ ) depending on {(zi , yi )}n . [sent-94, score-0.249]
</p><p>39 i=1 From the formulation (1), the corrected linear regression estimator is given by β ∈ arg min g(β)≤R  1 T β Γβ − γ T β + ρλ (β) . [sent-96, score-0.362]
</p><p>40 Other choices of Γ are applicable to missing data (model (b)), and also lead to nonconvex programs [6]. [sent-99, score-0.39]
</p><p>41 observations {(zi , yi )}n from a corrupted linear model with i=1 sub-Gaussian additive noise. [sent-104, score-0.172]
</p><p>42 Suppose (λ, R) are chosen such that β ∗ is feasible and log p c ≤λ≤ . [sent-105, score-0.157]
</p><p>43 n R Then given a sample size n ≥ C max{R2 , k} log p, any local optimum β of the nonconvex program (10) satisﬁes the estimation error bounds √ c0 λk c0 λ k ∗ , and β − β∗ 1 ≤ , β−β 2 ≤ λmin (Σx ) − 2µ λmin (Σx ) − 2µ with probability at least 1 − c1 exp(−c2 log p), where β ∗ 0 = k. [sent-106, score-0.921]
</p><p>44 Note, however, that the latter results are stated only for a global minimum β of the program (10), whereas Corollary 1 is a much stronger result holding for any local minimum β. [sent-110, score-0.283]
</p><p>45 Theorem 2 of our earlier paper [6] provides an indirect route for establishing similar bounds on β − β ∗ 1 and β − β ∗ 2 , since the projected gradient descent algorithm may become stuck in local minima. [sent-111, score-0.225]
</p><p>46 c  Corollary 1 also has important consequences in the case where pairs {(xi , yi )}n from the linear i=1 model (8) are observed without corruption and ρλ is nonconvex. [sent-113, score-0.137]
</p><p>47 Much existing work [3, 14] only establishes statistical consistency of global minima and then provides specialized algorithms for obtaining speciﬁc local optima that are provably close to global optima. [sent-115, score-0.47]
</p><p>48 In contrast, our results demonstrate that any optimization algorithm converging to a local optimum sufﬁces. [sent-116, score-0.218]
</p><p>49 Recall that a GLM is characterized by the conditional distribution yi β, xi − ψ(xT β) i P(yi | xi , β, σ) = exp , c(σ) where σ > 0 is a scale parameter and ψ is the cumulant function. [sent-119, score-0.165]
</p><p>50 The bound is required to establish both statistical consistency results in the present section and fast global convergence guarantees for our optimization algorithms in Section 4. [sent-123, score-0.239]
</p><p>51 We will assume that β ∗ is sparse and optimize the penalized maximum likelihood program β ∈ arg min g(β)≤R  1 n  n  ψ(xT β) − yi xT β + ρλ (β) . [sent-124, score-0.271]
</p><p>52 Suppose (λ, R) are chosen such that β ∗ is feasible and c log p ≤λ≤ . [sent-130, score-0.157]
</p><p>53 n R 2 Given a sample size n ≥ CR log p, any local optimum β of the nonconvex program (12) satisﬁes √ c0 λ k c0 λk ∗ β−β 2 ≤ , and β − β∗ 1 ≤ , λmin (Σx ) − 2µ λmin (Σx ) − 2µ with probability at least 1 − c1 exp(−c2 log p), where β ∗ 0 = k. [sent-131, score-0.878]
</p><p>54 c  5  4  Optimization algorithm  We now describe how a version of composite gradient descent may be applied to efﬁciently optimize the nonconvex program (1). [sent-132, score-0.778]
</p><p>55 We may then write the program (1) as β ∈ arg  Ln (β) − µ β  min gλ,µ (β)≤R  2 2  +λgλ,µ (β) . [sent-134, score-0.189]
</p><p>56 (14)  ¯ Ln  The objective function then decomposes nicely into a sum of a differentiable but nonconvex function and a possibly nonsmooth but convex penalty. [sent-135, score-0.497]
</p><p>57 Applied to the representation (14), the composite gradient descent procedure of Nesterov [10] produces a sequence of iterates {β t }∞ via the updates t=0 β  t+1  ∈ arg  min gλ,µ (β)≤R  where  1 η  2  Ln (β t ) η  1 β − βt − 2  + 2  λ gλ,µ (β) , η  (15)  is the stepsize. [sent-136, score-0.45]
</p><p>58 (16) For all vectors β2 ∈ B2 (3) ∩ B1 (R), we require the following form of restricted strong convexity:   α β − β 2 − τ log p β − β 2 ,  1 1 ∀ β1 − β2 2 ≤ 3, (17a) 1 2 1 2 2 1  n T (β1 , β2 ) ≥  log p  α β −β  2 1 β1 − β2 1 , ∀ β1 − β2 2 ≥ 3. [sent-138, score-0.272]
</p><p>59 We also assume an upper bound: log p T (β1 , β2 ) ≤ α3 β1 − β2 2 + τ3 β1 − β2 2 , for all β1 , β2 ∈ Rp , (18) 2 1 n a condition referred to as restricted smoothness in past work [1]. [sent-141, score-0.18]
</p><p>60 The following theorem applies to any population loss function L for which the population minimizer β ∗ is k-sparse and β ∗ 2 ≤ 1, and under the scaling n > Ck log p, for a constant C depending on the αi ’s and τi ’s. [sent-144, score-0.35]
</p><p>61 We show that the composite gradient updates (15) exhibit a type of globally geometric convergence in terms of the quantity α−µ 4η  128τ k log p n . [sent-145, score-0.378]
</p><p>62 Let κ :=  1−  T ∗ (δ) :=  2 log  + ϕ(n, p, k)  ,  φ(β 0 )−φ(β) δ2  log(1/κ)  where ϕ(n, p, k) :=  + 1+  log 2 log(1/κ)  λRL δ2  log log  (19)  ,  (20)  where φ(β) := Ln (β) + ρλ (β), and deﬁne stat := β − β ∗ 2 . [sent-147, score-0.773]
</p><p>63 Suppose β is any global minimum of the program (14), with R  log p ≤ c, n  and  λ≥  4 · max L  Ln (β ∗ )  Then for any stepsize η ≥ 2 · max{α3 − µ, µ} and tolerance δ 2 ≥ βt − β  2 2  ≤  2 α−µ  ∞,  τ  log p n  . [sent-150, score-0.437]
</p><p>64 4  δ2 +  δ k log p + 128τ τ n 6  2 stat  (21)  Remark 3. [sent-152, score-0.41]
</p><p>65 Note that for the optimal choice of tolerance parameter δ stat , the bound in inequalc 2 stat ity (21) takes the form α−µ , meaning successive iterates are guaranteed to converge to a region within statistical accuracy of the true global optimum β. [sent-153, score-0.908]
</p><p>66 Combining Theorems 1 and 2, we have βt − β  max  5  βt − β∗  2,  k log p n  =O  2  ,  ∀t ≥ T (c  stat  ). [sent-154, score-0.41]
</p><p>67 Simulations  In this section, we report the results of simulations for two versions of the loss function Ln , corresponding to linear and logistic regression, and three penalty functions: Lasso, SCAD, and MCP. [sent-155, score-0.33]
</p><p>68 Since ψ(t) = log(1 + exp(t)), the program (12) becomes β ∈ arg  min gλ,µ (β)≤R  1 n  n  {log(1 + exp( β, xi ) − yi β, xi } + ρλ (β) . [sent-170, score-0.325]
</p><p>69 (23)  i=1  We optimized the programs (22) and (23) using the composite gradient updates (15). [sent-171, score-0.257]
</p><p>70 Figure 1 shows the results of corrected linear regression with Lasso, SCAD, and MCP regularizers for three √ different problem sizes p. [sent-172, score-0.404]
</p><p>71 4 l2 norm error  comparing penalties for logistic regression 1  p=128 p=256 p=512  0. [sent-184, score-0.277]
</p><p>72 Plots showing statistical consistency of (a) linear and (b) logistic regression with Lasso, SCAD, and MCP. [sent-190, score-0.287]
</p><p>73 The estimation error β − β ∗ 2 n is plotted against the rescaled sample size k log p . [sent-192, score-0.21]
</p><p>74 Each panel shows two different families of curves, corresponding to statistical error (red) and optimization error 7  (blue). [sent-195, score-0.184]
</p><p>75 The vertical axis measures the 2 -error on a log scale, while the horizontal axis tracks the iteration number. [sent-196, score-0.15]
</p><p>76 The curves were obtained by running composite gradient descent from 10 random √ starting points. [sent-197, score-0.351]
</p><p>77 As predicted by our theory, the optimization error decreases at a linear rate until it falls to the level of statistical error. [sent-199, score-0.226]
</p><p>78 log error plot for corrected linear regression with SCAD, a = 3. [sent-202, score-0.489]
</p><p>79 7 2 opt err stat err 0  log error plot for corrected linear regression with MCP, b = 1. [sent-203, score-1.297]
</p><p>80 5 2 opt err stat err 0  log error plot for corrected linear regression with SCAD, a = 2. [sent-204, score-1.297]
</p><p>81 Plots illustrating linear rates of convergence for corrected linear regression with MCP and SCAD. [sent-206, score-0.291]
</p><p>82 Red lines depict statistical error log β − β ∗ 2 and blue lines depict optimization error log β t − β 2 . [sent-207, score-0.662]
</p><p>83 As predicted by Theorem 2, the optimization error decreases linearly up to statistical accuracy. [sent-208, score-0.199]
</p><p>84 Each plot shows the solution trajectory for 10 initializations of composite gradient descent. [sent-209, score-0.363]
</p><p>85 √ Figure 3 provides analogous results to Figure 2 for logistic regression, using p = 64, k = p , and n = 20k log p . [sent-211, score-0.204]
</p><p>86 The plot shows solution trajectories for 20 different initializations of composite gradient descent. [sent-212, score-0.363]
</p><p>87 Again, the log optimization error decreases at a linear rate up to the level of statistical error, as predicted by Theorem 2. [sent-213, score-0.347]
</p><p>88 Whereas the convex Lasso penalty yields a unique local/global optimum β, SCAD and MCP produce multiple local optima. [sent-214, score-0.308]
</p><p>89 log error plot for logistic regression with Lasso 1  log error plot for logistic regression with SCAD, a = 3. [sent-215, score-0.842]
</p><p>90 5  −1  −7 0  log error plot for logistic regression with MCP, b = 3 0. [sent-224, score-0.421]
</p><p>91 5  −4 0  500  1000 iteration count  (b)  1500  2000  −4 0  500  1000 iteration count  1500  2000  (c)  Figure 3. [sent-226, score-0.174]
</p><p>92 Plots showing linear rates of convergence on a log scale for logistic regression. [sent-227, score-0.231]
</p><p>93 Red lines depict statistical error and blue lines depict optimization error. [sent-228, score-0.377]
</p><p>94 Each plot shows the solution trajectory for 20 initializations of composite gradient descent. [sent-232, score-0.363]
</p><p>95 6  Discussion  We have analyzed theoretical properties of local optima of regularized M -estimators, where both the loss and penalty function are allowed to be nonconvex. [sent-233, score-0.463]
</p><p>96 Our results are the ﬁrst to establish that all local optima of such nonconvex problems are close to the truth, implying that any optimization method guaranteed to converge to a local optimum will provide statistically consistent solutions. [sent-234, score-0.954]
</p><p>97 We show that a variant of composite gradient descent may be used to obtain near-global optima in linear time, and verify our theoretical results with simulations. [sent-235, score-0.535]
</p><p>98 Fast global convergence of gradient methods for high-dimensional statistical recovery. [sent-244, score-0.209]
</p><p>99 Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. [sent-249, score-0.503]
</p><p>100 High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. [sent-272, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('scad', 0.405), ('nonconvex', 0.39), ('stat', 0.289), ('mcp', 0.243), ('ln', 0.225), ('err', 0.209), ('composite', 0.183), ('optima', 0.182), ('rsc', 0.144), ('regularizers', 0.14), ('nonconvexity', 0.124), ('corrected', 0.124), ('log', 0.121), ('regression', 0.113), ('loh', 0.103), ('rp', 0.103), ('optimum', 0.102), ('opt', 0.101), ('lasso', 0.089), ('depict', 0.085), ('logistic', 0.083), ('local', 0.082), ('penalty', 0.081), ('loss', 0.075), ('gradient', 0.074), ('global', 0.071), ('descent', 0.069), ('regularizer', 0.068), ('yi', 0.067), ('statistical', 0.064), ('program', 0.062), ('convexity', 0.061), ('plot', 0.061), ('count', 0.058), ('arg', 0.058), ('zi', 0.053), ('glm', 0.051), ('population', 0.05), ('xi', 0.049), ('corollary', 0.049), ('nonconcave', 0.048), ('conditions', 0.047), ('rescaled', 0.046), ('berkeley', 0.046), ('corrupted', 0.046), ('initializations', 0.045), ('penalized', 0.044), ('regularized', 0.043), ('convex', 0.043), ('error', 0.043), ('consequences', 0.043), ('establish', 0.043), ('min', 0.04), ('satis', 0.04), ('wainwright', 0.04), ('curvature', 0.04), ('guaranteed', 0.039), ('simulations', 0.038), ('penalties', 0.038), ('annals', 0.036), ('feasible', 0.036), ('suppose', 0.035), ('optimization', 0.034), ('minimum', 0.034), ('xt', 0.033), ('nicely', 0.033), ('zij', 0.033), ('lines', 0.033), ('wi', 0.032), ('taylor', 0.032), ('negahban', 0.032), ('additive', 0.032), ('inequalities', 0.031), ('nesterov', 0.031), ('differentiable', 0.031), ('restricted', 0.03), ('nonnegative', 0.03), ('es', 0.03), ('estimators', 0.03), ('pl', 0.03), ('decreases', 0.03), ('write', 0.029), ('fan', 0.029), ('panels', 0.029), ('condition', 0.029), ('iteration', 0.029), ('theorem', 0.029), ('predicted', 0.028), ('tolerance', 0.028), ('regularity', 0.027), ('guarantees', 0.027), ('linear', 0.027), ('interior', 0.027), ('xij', 0.027), ('functions', 0.027), ('versions', 0.026), ('systematically', 0.026), ('iterates', 0.026), ('applies', 0.025), ('curves', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="271-tfidf-1" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>2 0.14810802 <a title="271-tfidf-2" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><p>3 0.14633073 <a title="271-tfidf-3" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>Author: Tai Qin, Karl Rohe</p><p>Abstract: Spectral clustering is a fast and popular algorithm for ﬁnding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artiﬁcially inﬂate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the “star shape” in the eigenvectors–a common feature of empirical networks–can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justiﬁes several of the variations of the spectral clustering algorithm in terms of these models. 1</p><p>4 0.14505826 <a title="271-tfidf-4" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar</p><p>Abstract: We provide a uniﬁed framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M -estimators that minimize the sum of any loss function, and an instance of what we call a “hybrid” regularization, that is the inﬁmal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures. 1</p><p>5 0.14201462 <a title="271-tfidf-5" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><p>6 0.12127583 <a title="271-tfidf-6" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>7 0.10393975 <a title="271-tfidf-7" href="./nips-2013-Accelerating_Stochastic_Gradient_Descent_using_Predictive_Variance_Reduction.html">20 nips-2013-Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</a></p>
<p>8 0.10146113 <a title="271-tfidf-8" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>9 0.098602086 <a title="271-tfidf-9" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>10 0.096578203 <a title="271-tfidf-10" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>11 0.096172757 <a title="271-tfidf-11" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>12 0.094314046 <a title="271-tfidf-12" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>13 0.093685821 <a title="271-tfidf-13" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>14 0.086604275 <a title="271-tfidf-14" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>15 0.079932019 <a title="271-tfidf-15" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>16 0.077544972 <a title="271-tfidf-16" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>17 0.07708697 <a title="271-tfidf-17" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>18 0.077052876 <a title="271-tfidf-18" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>19 0.074346587 <a title="271-tfidf-19" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>20 0.07290516 <a title="271-tfidf-20" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.04), (2, 0.12), (3, 0.02), (4, 0.001), (5, 0.096), (6, -0.09), (7, 0.05), (8, -0.05), (9, 0.021), (10, 0.095), (11, 0.018), (12, -0.025), (13, -0.123), (14, -0.069), (15, -0.079), (16, 0.018), (17, -0.0), (18, 0.032), (19, 0.029), (20, -0.014), (21, 0.082), (22, 0.02), (23, 0.06), (24, -0.088), (25, -0.005), (26, -0.046), (27, 0.024), (28, -0.063), (29, 0.003), (30, -0.07), (31, -0.013), (32, -0.035), (33, -0.088), (34, -0.058), (35, 0.031), (36, -0.078), (37, -0.074), (38, 0.105), (39, -0.113), (40, 0.047), (41, -0.04), (42, -0.03), (43, 0.001), (44, -0.015), (45, -0.061), (46, -0.039), (47, 0.029), (48, 0.078), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95432502 <a title="271-lsi-1" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>2 0.71428543 <a title="271-lsi-2" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><p>3 0.70384347 <a title="271-lsi-3" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>Author: Samory Kpotufe, Vikas Garg</p><p>Abstract: We present the ﬁrst result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown H¨ lder-continuity of the regression function at x. The result holds with o high probability simultaneously at all points x in a general metric space X of unknown structure. 1</p><p>4 0.68832546 <a title="271-lsi-4" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>5 0.6804176 <a title="271-lsi-5" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>6 0.67626691 <a title="271-lsi-6" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>7 0.67288095 <a title="271-lsi-7" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>8 0.66699159 <a title="271-lsi-8" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>9 0.65600491 <a title="271-lsi-9" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>10 0.6188342 <a title="271-lsi-10" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>11 0.61744475 <a title="271-lsi-11" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>12 0.61064833 <a title="271-lsi-12" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>13 0.60219002 <a title="271-lsi-13" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>14 0.59999305 <a title="271-lsi-14" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>15 0.56073314 <a title="271-lsi-15" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>16 0.55739099 <a title="271-lsi-16" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>17 0.54609549 <a title="271-lsi-17" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>18 0.54512578 <a title="271-lsi-18" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>19 0.54356521 <a title="271-lsi-19" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>20 0.53106564 <a title="271-lsi-20" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.026), (33, 0.135), (34, 0.096), (41, 0.021), (49, 0.022), (56, 0.136), (70, 0.02), (85, 0.053), (89, 0.057), (93, 0.038), (95, 0.324)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9165163 <a title="271-lda-1" href="./nips-2013-On_the_Complexity_and_Approximation_of_Binary_Evidence_in_Lifted_Inference.html">220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</a></p>
<p>Author: Guy van den Broeck, Adnan Darwiche</p><p>Abstract: Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model’s symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efﬁcient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically. 1</p><p>2 0.87894249 <a title="271-lda-2" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>3 0.851933 <a title="271-lda-3" href="./nips-2013-Statistical_Active_Learning_Algorithms.html">309 nips-2013-Statistical Active Learning Algorithms</a></p>
<p>Author: Maria-Florina Balcan, Vitaly Feldman</p><p>Abstract: We describe a framework for designing efﬁcient active learning algorithms that are tolerant to random classiﬁcation noise and differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of ﬁltered random examples. It builds on the powerful statistical query framework of Kearns [30]. We show that any efﬁcient active statistical learning algorithm can be automatically converted to an efﬁcient active learning algorithm which is tolerant to random classiﬁcation noise as well as other forms of “uncorrelated” noise. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efﬁciently actively learned in our framework. These results combined with our generic conversion lead to the ﬁrst computationally-efﬁcient algorithms for actively learning some of these concept classes in the presence of random classiﬁcation noise that provide exponential improvement in the dependence on the error over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efﬁcient active differentially-private algorithms. This leads to the ﬁrst differentially-private active learning algorithms with exponential label savings over the passive case. 1</p><p>same-paper 4 0.8234899 <a title="271-lda-4" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>5 0.78371257 <a title="271-lda-5" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>Author: Ying Liu, Alan Willsky</p><p>Abstract: Gaussian Graphical Models (GGMs) or Gauss Markov random ﬁelds are widely used in many applications, and the trade-off between the modeling capacity and the efﬁciency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity O(k 2 n) using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efﬁcient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or ﬂight networks where the FVS nodes often correspond to a small number of highly inﬂuential nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate with complexity O(kn2 + n2 log n) if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing an inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efﬁcient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank corrections with complexity O(kn2 + n2 log n) per iteration. We perform experiments using both synthetic data as well as real data of ﬂight delays to demonstrate the modeling capacity with FVSs of various sizes. 1</p><p>6 0.74994612 <a title="271-lda-6" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>7 0.70931536 <a title="271-lda-7" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>8 0.70875734 <a title="271-lda-8" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>9 0.70503575 <a title="271-lda-9" href="./nips-2013-Curvature_and_Optimal_Algorithms_for_Learning_and_Minimizing_Submodular_Functions.html">78 nips-2013-Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a></p>
<p>10 0.67253143 <a title="271-lda-10" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>11 0.67094052 <a title="271-lda-11" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>12 0.67076999 <a title="271-lda-12" href="./nips-2013-%CE%A3-Optimality_for_Active_Learning_on_Gaussian_Random_Fields.html">359 nips-2013-Σ-Optimality for Active Learning on Gaussian Random Fields</a></p>
<p>13 0.66297007 <a title="271-lda-13" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>14 0.66250539 <a title="271-lda-14" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>15 0.66132987 <a title="271-lda-15" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>16 0.65249276 <a title="271-lda-16" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>17 0.64930087 <a title="271-lda-17" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>18 0.64530605 <a title="271-lda-18" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>19 0.64319825 <a title="271-lda-19" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>20 0.64222896 <a title="271-lda-20" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
