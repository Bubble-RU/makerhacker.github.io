<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-254" href="#">nips2013-254</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</h1>
<br/><p>Source: <a title="nips-2013-254-pdf" href="http://papers.nips.cc/paper/5005-probabilistic-low-rank-matrix-completion-with-adaptive-spectral-regularization-algorithms.pdf">pdf</a></p><p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>Reference: <a title="nips-2013-254-reference" href="../nips2013_reference/nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We propose a novel class of algorithms for low rank matrix completion. [sent-12, score-0.351]
</p><p>2 Our approach builds on novel penalty functions on the singular values of the low rank matrix. [sent-13, score-0.497]
</p><p>3 By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. [sent-14, score-0.35]
</p><p>4 The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. [sent-15, score-0.174]
</p><p>5 We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. [sent-17, score-0.351]
</p><p>6 1  Introduction  Matrix completion has attracted a lot of attention over the past few years. [sent-18, score-0.109]
</p><p>7 The objective is to “complete” a matrix of potentially large dimension based on a small (and potentially noisy) subset of its entries [1, 2, 3]. [sent-19, score-0.123]
</p><p>8 One popular application is to build automatic recommender systems, where the rows correspond to users, the columns to items and entries may be ratings or binary (like/dislike). [sent-20, score-0.149]
</p><p>9 In many cases, it is reasonable to assume that the unknown m × n matrix Z can be approximated by a matrix of low rank Z AB T where A and B are respectively of size m × k and n × k, with k min(m, n). [sent-22, score-0.418]
</p><p>10 In the recommender system application, the low rank assumption is sensible as it is commonly believed that only a few factors contribute to user’s preferences. [sent-23, score-0.306]
</p><p>11 The low rank structure thus implies some sort of collaboration between the different users/items [4]. [sent-24, score-0.284]
</p><p>12 Low rank matrix completion can be adressed by solving the following optimization problem minimize Z  1 2σ 2  2  (Xij − Zij ) + λ rank(Z) (i,j)∈Ω  1  (2)  where λ > 0 is some regularization parameter. [sent-33, score-0.513]
</p><p>13 [4] proposed an iterative algorithm, called Soft-Impute, for solving the nuclear norm regularized minimization (3). [sent-35, score-0.201]
</p><p>14 In this paper, we show that the solution to the objective function (3) can be interpreted as a Maximum A Posteriori (MAP) estimate when assuming that the singular values of Z are independently and identically drawn (iid) from an exponential distribution with rate λ. [sent-36, score-0.143]
</p><p>15 Using this Bayesian interpretation, we propose alternative concave penalties to the nuclear norm, obtained by considering that the singular values are iid from a mixture of exponential distributions. [sent-37, score-0.391]
</p><p>16 We show that this class of penalties bridges the gap between the nuclear norm and the rank penalty, and that a simple Expectation-Maximization (EM) algorithm can be derived to obtain MAP estimates. [sent-38, score-0.519]
</p><p>17 The resulting algorithm iteratively adapts the shrinkage coefﬁcients associated to the singular values. [sent-39, score-0.153]
</p><p>18 2  Complete matrix X  Consider ﬁrst that we observe the complete matrix X of size m × n. [sent-44, score-0.179]
</p><p>19 (4) in the complete case is a soft-thresholded singular value decomposition (SVD) of X [7, 4], i. [sent-48, score-0.154]
</p><p>20 X = U DV T is the singular value decomposition of X with D = diag(d1 , . [sent-54, score-0.109]
</p><p>21 , dr ) this can be further decomposed as p(Z) = p(U )p(V )p(D) where we assume a uniform Haar prior distribution on the unitary matrices U and V , and exponential priors on the singular values di , hence r  p(d1 , . [sent-61, score-0.501]
</p><p>22 , dr ) =  Exp (di ; λ)  (5)  i=1  where Exp(x; λ) = λ exp(−λx) is the probability density function (pdf) of the exponential distribution of parameter λ evaluated at x. [sent-64, score-0.139]
</p><p>23 We propose here alternative penalty/prior distributions, that bridge the gap between the rank and the nuclear norm penalties. [sent-66, score-0.46]
</p><p>24 Our penalties are based on hierarchical Bayes constructions and the related optimization problems to obtain MAP estimates can be solved by using an EM algorithm. [sent-67, score-0.125]
</p><p>25 1  Hierarchical adaptive spectral penalty  We consider the following hierarchical prior for the low rank matrix Z. [sent-69, score-0.615]
</p><p>26 We now assume that each singular value di has its own regularization parameter γi . [sent-74, score-0.365]
</p><p>27 1 0 0  1  2  3  4  5  di  Figure 1: Marginal distribution p(di ) with a = b = β for different values of the parameter β. [sent-85, score-0.196]
</p><p>28 The distribution becomes more concentrated around zero with heavier tails as β decreases. [sent-86, score-0.078]
</p><p>29 Figure 2: Thresholding rules on the singular values di of X for the soft thresholding rule (λ = 1), and hierarchical adaptive soft thresholding algorithm with a = b = β. [sent-88, score-0.672]
</p><p>30 γr ) =  r  p(di |γi ) = i=1  Exp(di ; γi ) i=1  We further assume that the regularization parameters are themselves iid from a gamma distribution r  r  p(γ1 , . [sent-95, score-0.147]
</p><p>31 , γr ) =  p(γi ) = i=1  Gamma(γi ; a, b) i=1  where Gamma(γi ; a, b) is the pdf of the gamma distribution of parameters a > 0 and b > 0 evaluated at γi . [sent-98, score-0.078]
</p><p>32 The marginal distribution over di is thus a continuous mixture of exponential distributions ∞ aba p(di ) = Exp(di ; γi ) Gamma(γi ; a, b)dγi = (6) (di + b)a+1 0 It is a Pareto distribution which has heavier tails than the exponential distribution. [sent-99, score-0.366]
</p><p>33 The lower β, the heavier the tails of the distribution. [sent-101, score-0.078]
</p><p>34 Let r  pen(Z) = − log p(Z) = −  r  log(p(di )) = C1 + i=1  (a + 1) log(b + di )  (7)  i=1  be the penalty induced by the prior p(Z). [sent-103, score-0.3]
</p><p>35 We call the penalty (7) the Hierarchical Adaptive Spectral Penalty (HASP). [sent-104, score-0.104]
</p><p>36 On Figure 3 (top) are represented the balls of constant penalties for a symmetric 2 × 2 matrix, for the HASP, nuclear norm and rank penalties. [sent-105, score-0.498]
</p><p>37 When the matrix is assumed to be diagonal, one recovers respectively the lasso, hierarchical adaptive lasso (HAL) [6, 8] and 0 penalties, as shown on Figure 3 (bottom). [sent-106, score-0.243]
</p><p>38 The penalty (7) admits as special cases the nuclear norm penalty λ||Z||∗ when a = λb and b → ∞. [sent-107, score-0.435]
</p><p>39 Another closely related penalty is the log-det heuristic [5, 9] penalty, deﬁned for a square matrice Z by log det(Z + δI) where δ is some small regularization constant. [sent-108, score-0.164]
</p><p>40 Both penalties agree on square matrices when a = b = 0 and δ = 0. [sent-109, score-0.09]
</p><p>41 2  EM algorithm for MAP estimation  Using the exponential mixture representation (6), we now show how to derive an EM algorithm [10] to obtain a MAP estimate Z = arg max [log p(X|Z) + log p(Z)] Z  i. [sent-111, score-0.1]
</p><p>42 to minimize L(Z) =  1 X −Z 2σ 2  r 2 F  +  (a + 1) log(b + di ) i=1  3  (8)  (b) HASP (β = 1)  (e)  1  norm  (c) HASP (β = 0. [sent-113, score-0.288]
</p><p>43 1)  (f) HAL (β = 1)  (a) Nuclear norm  (g) HAL (β = 0. [sent-114, score-0.071]
</p><p>44 1)  (d) Rank penalty  (h)  0  norm  Figure 3: Top: Manifold of constant penalty, for a symmetric 2 × 2 matrix Z = [x, y; y, z] for (a) the nuclear norm, (b-c) hierarchical adaptive spectral penalty with a = b = β (b) β = 1 and (c) β = 0. [sent-115, score-0.636]
</p><p>45 Bottom: contour of constant penalty for a diagonal matrix [x, 0; 0, z], where one recovers the classical (e) lasso, (f-g) hierarchical lasso and (h) 0 penalties. [sent-117, score-0.29]
</p><p>46 i  (9) is an adaptive nuclear norm regularized optimization problem, with weights ωi . [sent-123, score-0.281]
</p><p>47 ≤ ωr (10) The above weights will therefore penalize less heavily higher singular values, hence reducing bias. [sent-131, score-0.157]
</p><p>48 Figure 2 shows the thresholding rule applied to the singular values of X for the HAST algorithm (a = b = β, with β = 2 and β = 0. [sent-146, score-0.169]
</p><p>49 The bias term, which is equal to λ for the nuclear norm, goes to zero as di goes to inﬁnity. [sent-148, score-0.326]
</p><p>50 As λ is the mean value of the regularization parameter γi , we initialize the algorithm with the soft thresholded SVD with parameter σ 2 λ. [sent-150, score-0.232]
</p><p>51 4  Algorithm 1 Hierarchical Adaptive Soft Thresholded (HAST) algorithm for low rank estimation of complete matrices Initialize Z (0) . [sent-153, score-0.38]
</p><p>52 It relies on imputing missing values, similarly to the EM algorithm for SVD with missing data, see e. [sent-158, score-0.172]
</p><p>53 Algorithm 2 provides the resulting iterative procedure for matrix completion with the hierarchical adaptive spectral penalty. [sent-172, score-0.336]
</p><p>54 Algorithm 2 Hierarchical Adaptive Soft Impute (HASI) algorithm for matrix completion Initialize Z (0) . [sent-173, score-0.197]
</p><p>55 , r, compute the weights ωi =  a+1 (t−1) b+di  ⊥ • Set Z (t) = Sσ2 ω(t) PΩ (X) + PΩ (Z (t−1) )  • If  L(Z (t−1) )−L(Z (t) ) L(Z (t−1) )  < ε then return Z = Z (t)  Related algorithms Algorithm 2 admits the Soft-Impute algorithm of [4] as a special case when (t) a = λb and b = β → ∞. [sent-177, score-0.07]
</p><p>56 On the contrary, when β < ∞, our algorithm adaptively updates the weights so that to penalize less heavily higher singular values. [sent-179, score-0.178]
</p><p>57 Some authors have proposed related one-step adaptive spectral penalty algorithms [14, 11, 12]. [sent-180, score-0.199]
</p><p>58 As in the complete case, we suggest to set a = λb and b = β and to initialize the algorithm with the Soft-Impute algorithm with regularization parameter σ 2 λ. [sent-183, score-0.167]
</p><p>59 5  Scaling Similarly to the Soft-Impute algorithm, the computationally demanding part of Algo⊥ rithm 2 is Sσ2 ω(t) PΩ (X) + PΩ (Z (t−1) ) which requires calculating a low rank truncated SVD. [sent-184, score-0.284]
</p><p>60 This sophisticated linear algebra algorithm can efﬁciently compute the truncated SVD of the “sparse + low rank” matrix ⊥ PΩ (X) + PΩ (Z (t−1) ) = PΩ (X) − PΩ (Z (t−1) ) + Z (t−1) low rank  sparse  and can thus handle large matrices, as shown in [4]. [sent-186, score-0.419]
</p><p>61 We generate Gaussian matrices A and B respectively of size m × q and n × q, q ≤ r so that the matrix Z = AB T is of low rank q. [sent-190, score-0.381]
</p><p>62 A Gaussian noise of variance σ 2 is then added to the entries of Z to obtain the matrix X. [sent-191, score-0.123]
</p><p>63 We compute err, the relative error between the estimated matrix Z and the true matrix Z in the complete case, and errΩ⊥ in the incomplete case, where ⊥ ⊥ ||PΩ (Z) − PΩ (Z)||2 F ⊥ (Z)||2 ||PΩ F For the HASP penalty, we set a = λβ and b = β. [sent-195, score-0.211]
</p><p>64 We compute the solutions over a grid of 50 values of the regularization parameter λ linearly spaced from λ0 to 0, where λ0 = ||PΩ (X)||2 is the largest singular value of the input matrix X, padded with zeros. [sent-196, score-0.236]
</p><p>65 err =  ||Z − Z||2 F ||Z||2 F  errΩ⊥ =  and  Complete case We ﬁrst consider that the observed matrix is complete, with SNR = 1 and q = 10. [sent-199, score-0.122]
</p><p>66 The HAST algorithm 1 is compared to the soft thresholded (ST) and hard thresholded (HT) SVD. [sent-200, score-0.21]
</p><p>67 The HASP penalty provides a bridge/tradeoff between the nuclear norm and the rank penalty. [sent-202, score-0.542]
</p><p>68 For example, value of β = 10 show a minimum at the true rank q = 10 as HT, but with a lower error when the rank is overestimated. [sent-203, score-0.474]
</p><p>69 the rank obtained by varying the value of the regularization parameter λ. [sent-232, score-0.297]
</p><p>70 Results on simulated data are given for (a) complete matrix with SNR=1 (b) 50% missing and SNR=1 and (c) 80% missing and SNR=10. [sent-233, score-0.274]
</p><p>71 Incomplete case Then we consider the matrix completion problem, and remove uniformly a given percentage of the entries in X. [sent-234, score-0.232]
</p><p>72 Results, averaged over 50 replications, are reported in Figures 4(b-c) for a true rank q = 5, (b) 50% of missing data and 6  MMMF SoftImp SoftImp+ HardImp HASI 0. [sent-236, score-0.303]
</p><p>73 Figure 5: Boxplots of the test error and ranks obtained over 50 replications on simulated data. [sent-253, score-0.167]
</p><p>74 172 27  SNR = 1 and (c) 80% of missing data and SNR = 10. [sent-289, score-0.066]
</p><p>75 Similar behavior is observed, with the HASI algorithm attaining a minimum at the true rank q = 5. [sent-290, score-0.258]
</p><p>76 We then conduct the same experiments, but remove 20% of the observed entries as a validation set to estimate the regularization parameters (λ, β) for HASI, and λ for the other methods. [sent-291, score-0.135]
</p><p>77 We estimate Z on the whole observed matrix, and use the unobserved entries as a test set. [sent-292, score-0.095]
</p><p>78 Results on the test error and estimated ranks over 50 replications are reported in Figure 5. [sent-293, score-0.137]
</p><p>79 In both cases, it is able to recover very accurately the true rank of the matrix. [sent-296, score-0.237]
</p><p>80 The three datasets1 contain one hundred jokes, with user ratings between -10 and +10. [sent-300, score-0.107]
</p><p>81 We randomly select two ratings per user as a test set, and two other ratings per user as a validation set to select the parameters λ and β. [sent-301, score-0.272]
</p><p>82 Low values of β exhibit a bimodal behavior with two modes at low rank and full rank. [sent-309, score-0.284]
</p><p>83 1  Jester datasets can be downloaded from the url http://goldberg. [sent-311, score-0.077]
</p><p>84 the rank obtained by varying the value of the regularization parameter λ. [sent-329, score-0.297]
</p><p>85 Second, we conducted the same comparison on two MovieLens datasets2 , which contain ratings of movies by users. [sent-331, score-0.071]
</p><p>86 We randomly select 20% of the entries as a test set, and the remaining entries are split between a training set (80%) and a validation set (20%). [sent-332, score-0.17]
</p><p>87 For all the methods, we stop the regularization path as soon as the estimated rank exceeds rmax = 100. [sent-333, score-0.325]
</p><p>88 This is a practical consideration: given that the computations for high ranks demand more time and memory, we are interested in restricting ourselves to low rank solutions. [sent-334, score-0.317]
</p><p>89 For the MovieLens 100k dataset, HASI provides better NMAE than the other methods with a low rank solution. [sent-336, score-0.284]
</p><p>90 For the larger MovieLens 1M dataset, the precision, maximum number of iterations and maximum rank are decreased to = 10−6 , tmax = 100 and rmax = 30. [sent-337, score-0.297]
</p><p>91 5  Conclusion  The proposed class of methods has shown to provide good results compared to several alternative low rank matrix completion methods. [sent-340, score-0.46]
</p><p>92 It provides a bridge between nuclear norm and rank regularization algorithms. [sent-341, score-0.52]
</p><p>93 In this paper, we have focused on a gamma mixture of exponentials, as it leads to a simple and interpretable expression for the weights. [sent-343, score-0.077]
</p><p>94 Additionally, it is possible to derive an EM algorithm for low rank matrix completion for binary matrices. [sent-348, score-0.481]
</p><p>95 While we focus on point estimation in this paper, it would be of interest to investigate a fully Bayesian approach and derive a Gibbs sampler or variational algorithm to approximate the posterior distribution, and compare to other full Bayesian approaches to matrix completion [20, 21]. [sent-350, score-0.197]
</p><p>96 2  MovieLens datasets can be downloaded from the url http://www. [sent-355, score-0.077]
</p><p>97 A singular value thresholding algorithm for matrix complee tion. [sent-404, score-0.236]
</p><p>98 Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices. [sent-415, score-0.304]
</p><p>99 Fast variational Bayesian inference for non-conjugate matrix factorization models. [sent-484, score-0.067]
</p><p>100 Global analytic solution of fullyobserved variational Bayesian matrix factorization. [sent-493, score-0.067]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hasi', 0.593), ('jester', 0.254), ('nmae', 0.254), ('rank', 0.237), ('softimp', 0.212), ('snr', 0.198), ('di', 0.196), ('mmmf', 0.169), ('hast', 0.148), ('movielens', 0.131), ('nuclear', 0.13), ('hasp', 0.127), ('singular', 0.109), ('completion', 0.109), ('hardimp', 0.106), ('dr', 0.105), ('penalty', 0.104), ('svd', 0.094), ('em', 0.081), ('xij', 0.074), ('soft', 0.073), ('ratings', 0.071), ('norm', 0.071), ('matrix', 0.067), ('missing', 0.066), ('replications', 0.065), ('hierarchical', 0.065), ('bordeaux', 0.064), ('penalties', 0.06), ('regularization', 0.06), ('thresholded', 0.058), ('adaptive', 0.057), ('entries', 0.056), ('err', 0.055), ('gamma', 0.053), ('caron', 0.052), ('imp', 0.052), ('zij', 0.052), ('low', 0.047), ('dv', 0.047), ('complete', 0.045), ('heavier', 0.044), ('hal', 0.043), ('imb', 0.042), ('diag', 0.039), ('cand', 0.039), ('test', 0.039), ('thresholding', 0.039), ('spectral', 0.038), ('impute', 0.037), ('user', 0.036), ('marie', 0.034), ('francois', 0.034), ('exponential', 0.034), ('iid', 0.034), ('tails', 0.034), ('posteriori', 0.033), ('url', 0.033), ('ranks', 0.033), ('tmax', 0.032), ('incomplete', 0.032), ('lasso', 0.031), ('simulated', 0.03), ('matrices', 0.03), ('rennie', 0.028), ('reweighted', 0.028), ('rmax', 0.028), ('unitary', 0.027), ('ht', 0.027), ('admits', 0.026), ('initialization', 0.026), ('collaborative', 0.025), ('pdf', 0.025), ('penalize', 0.025), ('downloaded', 0.024), ('mixture', 0.024), ('map', 0.024), ('bayesian', 0.023), ('weights', 0.023), ('recovers', 0.023), ('adapts', 0.023), ('bridge', 0.022), ('recommender', 0.022), ('supplementary', 0.022), ('ab', 0.022), ('inria', 0.022), ('france', 0.022), ('completed', 0.021), ('minimize', 0.021), ('algorithm', 0.021), ('views', 0.02), ('initialize', 0.02), ('datasets', 0.02), ('srebro', 0.02), ('validation', 0.019), ('imputing', 0.019), ('lanczos', 0.019), ('propack', 0.019), ('bidiagonalization', 0.019), ('adressed', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="254-tfidf-1" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>2 0.13966045 <a title="254-tfidf-2" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>3 0.13136445 <a title="254-tfidf-3" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>Author: Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees. Our algorithms exploit adaptivity to identify entries that are highly informative for learning the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analyses. In the absence of noise, we show that one can exactly recover a n ⇥ n matrix of rank r from merely ⌦(nr3/2 log(r)) matrix entries. We also show that one can recover an order T tensor using ⌦(nrT 1/2 T 2 log(r)) entries. For noisy recovery, our algorithm consistently estimates a low rank matrix corrupted with noise using ⌦(nr3/2 polylog(n)) entries. We complement our study with simulations that verify our theory and demonstrate the scalability of our algorithms. 1</p><p>4 0.11608377 <a title="254-tfidf-4" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>Author: Shouyuan Chen, Michael R. Lyu, Irwin King, Zenglin Xu</p><p>Abstract: Tensor completion from incomplete observations is a problem of signiﬁcant practical interest. However, it is unlikely that there exists an efﬁcient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Speciﬁcally, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art results. 1</p><p>5 0.10974728 <a title="254-tfidf-5" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>6 0.1002944 <a title="254-tfidf-6" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>7 0.099441804 <a title="254-tfidf-7" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>8 0.099066079 <a title="254-tfidf-8" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>9 0.097184964 <a title="254-tfidf-9" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>10 0.09554287 <a title="254-tfidf-10" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>11 0.082120813 <a title="254-tfidf-11" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>12 0.074297838 <a title="254-tfidf-12" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>13 0.072571814 <a title="254-tfidf-13" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>14 0.069694959 <a title="254-tfidf-14" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>15 0.066866867 <a title="254-tfidf-15" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>16 0.065760083 <a title="254-tfidf-16" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>17 0.065523371 <a title="254-tfidf-17" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>18 0.064428397 <a title="254-tfidf-18" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>19 0.063787453 <a title="254-tfidf-19" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>20 0.063443936 <a title="254-tfidf-20" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.075), (2, 0.08), (3, 0.136), (4, -0.029), (5, -0.05), (6, -0.01), (7, -0.008), (8, -0.029), (9, -0.028), (10, 0.005), (11, 0.03), (12, 0.019), (13, 0.017), (14, -0.047), (15, -0.059), (16, -0.015), (17, 0.016), (18, 0.012), (19, -0.016), (20, -0.001), (21, -0.056), (22, 0.008), (23, 0.002), (24, 0.006), (25, 0.024), (26, 0.064), (27, -0.195), (28, 0.007), (29, -0.022), (30, 0.021), (31, -0.099), (32, -0.012), (33, -0.059), (34, 0.07), (35, 0.041), (36, 0.125), (37, -0.016), (38, -0.059), (39, 0.064), (40, -0.071), (41, 0.036), (42, 0.002), (43, -0.048), (44, -0.027), (45, 0.031), (46, -0.039), (47, -0.062), (48, -0.026), (49, -0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93400133 <a title="254-lsi-1" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>2 0.80940235 <a title="254-lsi-2" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>Author: Miao Xu, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: In standard matrix completion theory, it is required to have at least O(n ln2 n) observed entries to perfectly recover a low-rank matrix M of size n × n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(ln n). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning. 1</p><p>3 0.7647084 <a title="254-lsi-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.74142766 <a title="254-lsi-4" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>5 0.70420986 <a title="254-lsi-5" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>Author: Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees. Our algorithms exploit adaptivity to identify entries that are highly informative for learning the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analyses. In the absence of noise, we show that one can exactly recover a n ⇥ n matrix of rank r from merely ⌦(nr3/2 log(r)) matrix entries. We also show that one can recover an order T tensor using ⌦(nrT 1/2 T 2 log(r)) entries. For noisy recovery, our algorithm consistently estimates a low rank matrix corrupted with noise using ⌦(nr3/2 polylog(n)) entries. We complement our study with simulations that verify our theory and demonstrate the scalability of our algorithms. 1</p><p>6 0.66307139 <a title="254-lsi-6" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>7 0.64479738 <a title="254-lsi-7" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>8 0.60471225 <a title="254-lsi-8" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>9 0.56280655 <a title="254-lsi-9" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>10 0.55901951 <a title="254-lsi-10" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>11 0.52400595 <a title="254-lsi-11" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>12 0.51133353 <a title="254-lsi-12" href="./nips-2013-What_do_row_and_column_marginals_reveal_about_your_dataset%3F.html">352 nips-2013-What do row and column marginals reveal about your dataset?</a></p>
<p>13 0.50375444 <a title="254-lsi-13" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>14 0.50367486 <a title="254-lsi-14" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>15 0.50346172 <a title="254-lsi-15" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>16 0.50078517 <a title="254-lsi-16" href="./nips-2013-A_Novel_Two-Step_Method_for_Cross_Language_Representation_Learning.html">12 nips-2013-A Novel Two-Step Method for Cross Language Representation Learning</a></p>
<p>17 0.46738103 <a title="254-lsi-17" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>18 0.46002954 <a title="254-lsi-18" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>19 0.46002385 <a title="254-lsi-19" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>20 0.45951408 <a title="254-lsi-20" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.025), (33, 0.201), (34, 0.099), (41, 0.017), (49, 0.052), (56, 0.092), (64, 0.274), (70, 0.017), (85, 0.03), (89, 0.027), (93, 0.029), (95, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8431173 <a title="254-lda-1" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>2 0.81002223 <a title="254-lda-2" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>Author: Xiaojin Zhu</p><p>Abstract: What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for ﬁnding the optimal teaching set. Our algorithm optimizes the aggregate sufﬁcient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. 1</p><p>same-paper 3 0.78400218 <a title="254-lda-3" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>4 0.70151907 <a title="254-lda-4" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>5 0.66825593 <a title="254-lda-5" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>Author: Ian Goodfellow, Mehdi Mirza, Aaron Courville, Yoshua Bengio</p><p>Abstract: We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classiﬁcation tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classiﬁcation, classiﬁcation with missing inputs, and mean ﬁeld prediction tasks.1 1</p><p>6 0.66813725 <a title="254-lda-6" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>7 0.66731793 <a title="254-lda-7" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>8 0.66677594 <a title="254-lda-8" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>9 0.66671497 <a title="254-lda-9" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>10 0.66642195 <a title="254-lda-10" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>11 0.66577959 <a title="254-lda-11" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>12 0.66541457 <a title="254-lda-12" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>13 0.66393602 <a title="254-lda-13" href="./nips-2013-Annealing_between_distributions_by_averaging_moments.html">36 nips-2013-Annealing between distributions by averaging moments</a></p>
<p>14 0.6638242 <a title="254-lda-14" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>15 0.66363096 <a title="254-lda-15" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>16 0.6633606 <a title="254-lda-16" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>17 0.66328943 <a title="254-lda-17" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>18 0.662835 <a title="254-lda-18" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>19 0.66252887 <a title="254-lda-19" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>20 0.66241491 <a title="254-lda-20" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
