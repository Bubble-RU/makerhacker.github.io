<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-286" href="#">nips2013-286</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</h1>
<br/><p>Source: <a title="nips-2013-286-pdf" href="http://papers.nips.cc/paper/4995-robust-learning-of-low-dimensional-dynamics-from-large-neural-ensembles.pdf">pdf</a></p><p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>Reference: <a title="nips-2013-286-reference" href="../nips2013_reference/nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Robust learning of low-dimensional dynamics from large neural ensembles  David Pfau  Eftychios A. [sent-1, score-0.274]
</p><p>2 edu  Abstract Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. [sent-6, score-0.235]
</p><p>3 Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. [sent-7, score-0.252]
</p><p>4 The basic method extends PCA to the exponential family using nuclear norm minimization. [sent-10, score-0.501]
</p><p>5 We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. [sent-11, score-0.208]
</p><p>6 We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. [sent-12, score-0.638]
</p><p>7 We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. [sent-13, score-0.659]
</p><p>8 Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. [sent-14, score-0.823]
</p><p>9 1  Introduction  Progress in neural recording technology has made it possible to record spikes from ever larger populations of neurons [1]. [sent-15, score-0.41]
</p><p>10 Analysis of these large populations suggests that much of the activity can be explained by simple population-level dynamics [2]. [sent-16, score-0.498]
</p><p>11 Typically, this low-dimensional activity is extracted by principal component analysis (PCA) [3, 4, 5], but in recent years a number of extensions have been introduced in the neuroscience literature, including jPCA [6] and demixed principal component analysis (dPCA) [7]. [sent-17, score-0.542]
</p><p>12 A downside of these methods is that they do not treat either the discrete nature of spike data or the positivity of ﬁring rates in a statistically principled way. [sent-18, score-0.231]
</p><p>13 One alternative is to ﬁt a more complex statistical model directly from spike data, where temporal dependencies are attributed to latent low dimensional dynamics [8, 9]. [sent-20, score-0.477]
</p><p>14 State space models can include complex interactions such as switching linear dynamics [10] and direct coupling between neurons [11]. [sent-22, score-0.394]
</p><p>15 These methods have drawbacks too: they are typically ﬁt by approximate EM [12] or other methods that are prone to local minima, the number of latent dimensions is typically chosen ahead of time, and a certain class of possible dynamics must be chosen before doing dimensionality reduction. [sent-23, score-0.363]
</p><p>16 Our approach is convex and based on recent advances in system identiﬁcation using nuclear norm minimization [13, 14, 15], a convex relaxation of matrix rank minimization. [sent-25, score-0.761]
</p><p>17 Unlike the standard GLM, where the inputs driving the neurons are observed, we assume that the driving activity is unobserved, but lies on some low dimensional subspace. [sent-36, score-0.24]
</p><p>18 Thus, instead of ﬁtting a linear receptive ﬁeld, the goal of learning in low-dimensional GLMs is to accurately recover the latent subspace of activity. [sent-38, score-0.339]
</p><p>19 Let xt ∈ Rm be the value of the dynamics at time t. [sent-39, score-0.217]
</p><p>20 To turn this into spiking activity, we project this into the space of neurons: yt = Cxt + b is a vector in Rn , n m, where each dimension of yt corresponds to one neuron. [sent-40, score-0.594]
</p><p>21 C ∈ Rn×m denotes the subspace of the neural population and b ∈ Rn the bias vector for all the neurons. [sent-41, score-0.334]
</p><p>22 As yt can take on negative values, we cannot use this directly as a ﬁring rate, and so we pass each element of yt through some convex and log-concave increasing point-wise nonlinearity f : R → R+ . [sent-42, score-0.671]
</p><p>23 To account for biophysical effects such as refractory periods, bursting, and direct synaptic connections, we include a linear dependence on spike history before the nonlinearity. [sent-44, score-0.51]
</p><p>24 We can extend this simple model by adding dynamics to the low-dimensional latent state, including input-driven dynamics. [sent-47, score-0.311]
</p><p>25 In this case the model is closely related to the common input model used in neuroscience [11], the difference being that the observed input is added to xt rather than being directly mapped to yt . [sent-48, score-0.362]
</p><p>26 The case without history terms and with linear Gaussian dynamics is a wellstudied state space model for neural data, usually ﬁt by EM [19, 12, 20], though a consistent spectral method has been derived [16] for the case f (·) = exp(·). [sent-49, score-0.419]
</p><p>27 Unlike these methods, our approach largely decouples the problem of dimensionality reduction and learning dynamics: even in the case of nonstationary, non-Gaussian dynamics where A, B and Cov[ ] change over time, we can still robustly recover the latent subspace spanned by xt . [sent-50, score-0.613]
</p><p>28 1  Learning Nuclear norm minimization  In the case that the spike history terms D1:k are zero, the natural rate at time t is yt = Cxt + b, so all yt are elements of some m-dimensional afﬁne space given by the span of the columns of C offset by b. [sent-52, score-1.131]
</p><p>29 Ideally we would minimize λnT rank(A(Y )) − t=1 log p(st |yt ), where λ controls how much we trade off between a simple solution and the likelihood of the data, however general rank minimization is a hard non convex problem. [sent-59, score-0.248]
</p><p>30 Instead we replace the matrix rank with its convex envelope: the sum of singular values or nuclear norm · ∗ [13], which can be seen as the analogue of the 1 norm for vector sparsity. [sent-60, score-0.831]
</p><p>31 Our problem then becomes: √ min λ nT ||A(Y )||∗ − Y  T  log p(st |yt )  (3)  t=1  Since the log likelihood scales linearly with the size of the data, and the singular values scale with √ the square root of the size, we also add a factor of nT in front of the nuclear norm term. [sent-61, score-0.701]
</p><p>32 In the examples in this paper, we assume spikes are drawn from a Poisson distribution: N  log p(st |yt ) =  sit log f (yit ) − f (yit ) − log sit ! [sent-62, score-0.293]
</p><p>33 2  Stable principal component pursuit  The model above is appropriate for cases where the spike history terms Dτ are zero, that is the observed data can entirely be described by some low-dimensional global dynamics. [sent-66, score-0.502]
</p><p>34 In real data neurons exhibit history-dependent behavior like bursting and refractory periods. [sent-67, score-0.229]
</p><p>35 Moreover if the recorded neurons are close to each other some may have direct synaptic connections. [sent-68, score-0.311]
</p><p>36 1 it is clear that yt is no longer restricted to a lowdimensional afﬁne space. [sent-70, score-0.297]
</p><p>37 This is an extension of stable principal component pursuit [23], which separates sparse and low-rank components of a noise-corrupted matrix. [sent-78, score-0.288]
</p><p>38 One can also consider the use of a group sparsity penalty where each group collects a speciﬁc synaptic weight across all the k time lags. [sent-81, score-0.242]
</p><p>39 As we recover a subspace spanned by the columns of Y rather than a single parameter, this presents a challenge. [sent-84, score-0.302]
</p><p>40 For the case of PCA, we can project the held out data onto a subspace spanned by principal components and compute what fraction of total variance is explained by this subspace. [sent-86, score-0.59]
</p><p>41 For any exponential family with natural parameters θ, link function g, function F such that F = g −1 and sufﬁcient statistic T , the log likelihood can be written as DF [θ||g(T (x))] − h(x), where D· [·||·] is a Bregman divergence [24]: DF [x||y] = F (x) − F (y) − (x − y)T F (y). [sent-88, score-0.242]
</p><p>42 Given a matrix of natural rates recovered from training data, we compute the fraction of Bregman divergence explained by a sequence of subspaces as follows. [sent-92, score-0.545]
</p><p>43 Let ui be the ith singular vector of (q) the recovered natural rates. [sent-93, score-0.284]
</p><p>44 Let b be the mean natural rate, and let yt be the maximum likelihood natural rates restricted to the space spanned by u1 , . [sent-94, score-0.578]
</p><p>45 , uq : q (q)  yt  k (q)  =  Dτ st−τ + b  ui vit + τ =1  i=1  q (q)  vt (q)  =  arg max log p st  k  ui vit +  v  Dτ st−τ + b  (6)  τ =1  i=1  (q)  Here vt is the projection of yt onto the singular vectors. [sent-97, score-0.943]
</p><p>46 Then the divergence from the mean explained by the qth dimension is given by (q−1)  t  DF yt  (0)  t  DF yt  (q)  yt  (7)  g(st )  (0)  where yt is the bias b plus the spike history terms. [sent-98, score-1.654]
</p><p>47 For Gaussian noise g(x) = x and F (x) = 1 ||x||2 and this is exactly the variance explained by each principal component, while for 2 Poisson noise g(x) = log(x) and F (x) = i exp(xi ). [sent-100, score-0.246]
</p><p>48 5 is difﬁcult, because the nuclear and 1 norm are not differentiable everywhere. [sent-106, score-0.501]
</p><p>49 1  Nuclear norm minimization  To ﬁnd the optimal Y we alternate between minimizing an augmented Lagrangian with respect to Y , minimizing with respect to an auxiliary variable Z, and performing gradient ascent on a Lagrange multiplier Λ. [sent-111, score-0.229]
</p><p>50 2  Stable principal component pursuit  To extend ADMM to the problem in Eq. [sent-119, score-0.244]
</p><p>51 , Dk ), and stack the different time-shifted matrices of spike histories on top of one another to form a single spike history matrix H. [sent-124, score-0.461]
</p><p>52 First, we show in the absence of spike history terms that the true low dimensional subspace can be recovered in the limit of large data, even when the dynamics are nonstationary. [sent-129, score-0.906]
</p><p>53 Second, we show that spectral methods can accurately recover the transition matrix when dynamics are linear. [sent-130, score-0.403]
</p><p>54 Lastly, we show that nuclear-norm penalized subspace recovery leads to improved prediction on real neural data recorded from macaque motor cortex. [sent-132, score-0.322]
</p><p>55 For linear dynamical systems, the transition matrix was sampled from a Gaussian distribution, and the 5  0  −50  1600  1700  1800  1900  2000  2100  2200  2300  2400  Subspace Angle  1. [sent-134, score-0.227]
</p><p>56 While the subspace remains the same, the dynamics switch between 5 different linear systems. [sent-138, score-0.416]
</p><p>57 Left top: one dimension of the latent trajectory, switching from one set of dynamics to another (red line). [sent-139, score-0.349]
</p><p>58 Left middle: ﬁring rates of a subset of neurons during the same switch. [sent-140, score-0.204]
</p><p>59 Left bottom: covariance between spike counts for different neurons during each epoch of linear dynamics. [sent-141, score-0.305]
</p><p>60 Right top: Angle between the true subspace and top principal components directly from spike data, from natural rates recovered by nuclear norm minimization, and from the true natural rates. [sent-142, score-1.435]
</p><p>61 Right bottom: fraction of Bregman divergence explained by the top 1, 5 or 10 dimensions from nuclear norm minimization. [sent-143, score-0.761]
</p><p>62 Dotted lines are variance explained by the same number of principal components. [sent-144, score-0.246]
</p><p>63 1 the divergence explained by a given number of dimensions exceeds the variance explained by the same number of PCs. [sent-146, score-0.381]
</p><p>64 We ﬁrst sought to show that we could accurately recover the subspace in which the dynamics take place even when those dynamics are not stationary. [sent-152, score-0.679]
</p><p>65 We performed nuclear norm minimization on data generated from this model, varying the smoothing parameter λ from 10−3 to 10, and compared the subspace angle between the top 8 principal components and the true matrix C. [sent-154, score-1.029]
</p><p>66 We found that when smoothing was optimized the recovered subspace was signiﬁcantly closer to the true subspace than the top principal components taken directly from spike data. [sent-156, score-0.921]
</p><p>67 Increasing the amount of data from 1000 to 10000 time bins signiﬁcantly reduced the average subspace angle at the optimal λ. [sent-157, score-0.255]
</p><p>68 The top PCs of the true natural rates Y , while not spanning exactly the same space as C due to differences between the mean column and true bias b, was still closer to the true subspace than the result of nuclear norm minimization. [sent-158, score-0.956]
</p><p>69 We also computed the fraction of Bregman divergence explained by the sequence of spaces spanned by successive principal components, solving Eq. [sent-159, score-0.39]
</p><p>70 We did not ﬁnd a clear drop at the true dimensionality of the subspace, but we did ﬁnd that a larger share of the divergence could be explained by the top dimensions than by PCA directly on spikes. [sent-161, score-0.307]
</p><p>71 To show that the parameters of a latent dynamical system can be recovered, we investigated the performance of spectral methods on model data with linear Gaussian latent dynamics. [sent-164, score-0.424]
</p><p>72 After estimating natural rates by nuclear norm minimization with λ = 0. [sent-166, score-0.68]
</p><p>73 01 on 10 trials of 10000 time bins with unit-variance innovations t , we ﬁt the transition matrix A by subspace identiﬁcation (SSID) [26]. [sent-167, score-0.341]
</p><p>74 2  Imaginary Component  (b)  (c)  Figure 2: Recovered eigenvalues for the transition matrix of a linear dynamical system from model neural data. [sent-202, score-0.381]
</p><p>75 (2b) Eigenvalues recovered from subspace identiﬁcation directly on spike counts. [sent-206, score-0.55]
</p><p>76 (2c) Eigenvalues recovered from subspace identiﬁcation on the natural rates estimated by nuclear norm minimization. [sent-207, score-1.0]
</p><p>77 nuclear norm minimization had little bias, and seemed to perform almost as well as SSID directly on the true natural rates. [sent-208, score-0.662]
</p><p>78 We found that other methods for ﬁtting linear dynamical systems from the estimated natural rates were biased, as was SSID on the result of nuclear norm minimization without mean-centering (see the supplementary material for more details). [sent-209, score-0.82]
</p><p>79 We incorporated spike history terms into our model data to see whether local connectivity and global dynamics could be separated. [sent-210, score-0.559]
</p><p>80 We found that we could recover synaptic weights with an r2 up to . [sent-214, score-0.218]
</p><p>81 4 on this data by combining both a nuclear norm and 1 penalty, compared to at most . [sent-215, score-0.501]
</p><p>82 Somewhat surprisingly, at the extreme of either no nuclear norm penalty or a dominant nuclear norm penalty, increasing the 1 penalty never improved estimation. [sent-218, score-1.142]
</p><p>83 Left: r2 between true and recovered synaptic weights across a range of parameters. [sent-255, score-0.404]
</p><p>84 Right: scatter plot of true versus recovered synaptic weights, illustrating the effect of the nuclear norm term. [sent-258, score-0.905]
</p><p>85 We ﬁt linear dynamical systems by subspace identiﬁcation as in Fig. [sent-264, score-0.339]
</p><p>86 2, but as we did not have access to a “true” linear dynamical system for comparison, we evaluated our model ﬁts by approximating the held out log likelihood by Laplace-Gaussian ﬁltering [28]. [sent-265, score-0.376]
</p><p>87 We found that a strong nuclear norm penalty improved prediction by several hundred bits per second, and that fewer dimensions were needed for optimal prediction as the nuclear norm penalty was increased. [sent-267, score-1.194]
</p><p>88 The best ﬁt models predicted held out data nearly as well as models trained via EM, even though nuclear norm minimization is not directly maximizing the likelihood of a linear dynamical system. [sent-268, score-0.852]
</p><p>89 16e−02 EM −2000  0  5  10  15  20  25  30  35  40  45  50  Number of Latent Dimensions  6  Discussion  Figure 4: Log likelihood of held out motor cortex The method presented here has a number of straight- data versus number of latent dimensions for difforward extensions. [sent-273, score-0.418]
</p><p>90 If the dimensionality of the la- ferent latent linear dynamical systems. [sent-274, score-0.234]
</p><p>91 If there are also observed inputs ut then the term inside the nuclear norm should also include a projection orthogonal to the row space of the inputs. [sent-279, score-0.538]
</p><p>92 This could enable joint learning of dynamics and receptive ﬁelds for small populations of neurons with high dimensional inputs. [sent-280, score-0.452]
</p><p>93 It remains an open question what kinds of dynamics can be learned from the recovered natural parameters. [sent-284, score-0.452]
</p><p>94 In this paper we have focused on linear systems, but nuclear norm minimization could just as easily be combined with spectral methods for switching linear systems and general nonlinear systems. [sent-285, score-0.656]
</p><p>95 Harris, “Population rate dynamics and multineuron ﬁring patterns in sensory cortex,” The Journal of Neuroscience, vol. [sent-309, score-0.217]
</p><p>96 Sahani, “Dynamical segmentation of single trials from population neural data,” Advances in neural information processing systems, vol. [sent-400, score-0.247]
</p><p>97 Vandenberghe, “Interior-point method for nuclear norm approximation with application to system identiﬁcation,” SIAM Journal on Matrix Analysis and Applications, vol. [sent-425, score-0.544]
</p><p>98 Vandenberghe, “Nuclear norm system identiﬁcation with missing inputs and outputs,” Systems & Control Letters, vol. [sent-431, score-0.245]
</p><p>99 Sahani, “Spectral learning of linear dynamics from generalised-linear observations with application to neural population data,” Advances in neural information processing systems, vol. [sent-438, score-0.409]
</p><p>100 Schapire, “A generalization of principal component analysis to the exponential family,” Advances in neural information processing systems, vol. [sent-465, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nuclear', 0.336), ('yt', 0.297), ('dynamics', 0.217), ('subspace', 0.199), ('recovered', 0.185), ('ssid', 0.178), ('st', 0.172), ('synaptic', 0.172), ('spike', 0.166), ('norm', 0.165), ('dynamical', 0.14), ('neurons', 0.139), ('dh', 0.13), ('principal', 0.125), ('df', 0.121), ('explained', 0.121), ('bregman', 0.119), ('nt', 0.105), ('populations', 0.096), ('glm', 0.096), ('latent', 0.094), ('history', 0.092), ('paninski', 0.091), ('sahani', 0.089), ('held', 0.088), ('divergence', 0.087), ('connectivity', 0.084), ('spikes', 0.081), ('population', 0.078), ('shenoy', 0.078), ('pythagorean', 0.076), ('cunningham', 0.074), ('penalty', 0.07), ('ryu', 0.067), ('ring', 0.067), ('motor', 0.066), ('neuroscience', 0.065), ('rates', 0.065), ('activity', 0.064), ('minimization', 0.064), ('nonstationary', 0.062), ('pursuit', 0.06), ('cortex', 0.059), ('likelihood', 0.059), ('component', 0.059), ('cxt', 0.058), ('imaginary', 0.058), ('neural', 0.057), ('spanned', 0.057), ('angle', 0.056), ('trials', 0.055), ('admm', 0.055), ('eigenvalues', 0.054), ('spectral', 0.053), ('pca', 0.053), ('dimensions', 0.052), ('nonlinearities', 0.051), ('transition', 0.05), ('natural', 0.05), ('singular', 0.049), ('poisson', 0.049), ('true', 0.047), ('identi', 0.046), ('recover', 0.046), ('log', 0.046), ('hatsopoulos', 0.045), ('koyama', 0.045), ('buesing', 0.045), ('demixed', 0.045), ('pfau', 0.045), ('romo', 0.045), ('extensible', 0.045), ('confounding', 0.045), ('refractory', 0.045), ('bursting', 0.045), ('em', 0.045), ('stable', 0.044), ('dim', 0.043), ('system', 0.043), ('rank', 0.042), ('vit', 0.041), ('santhanam', 0.041), ('machens', 0.041), ('zk', 0.04), ('divergences', 0.04), ('newton', 0.04), ('nonlinearity', 0.04), ('yit', 0.039), ('switching', 0.038), ('inputs', 0.037), ('macke', 0.037), ('sit', 0.037), ('convex', 0.037), ('recording', 0.037), ('matrix', 0.037), ('multipliers', 0.036), ('nn', 0.036), ('simoncelli', 0.035), ('biophysical', 0.035), ('vandenberghe', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="286-tfidf-1" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>2 0.29188111 <a title="286-tfidf-2" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>3 0.20410848 <a title="286-tfidf-3" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>4 0.20235211 <a title="286-tfidf-4" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>5 0.18991442 <a title="286-tfidf-5" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>6 0.18705294 <a title="286-tfidf-6" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>7 0.18632701 <a title="286-tfidf-7" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>8 0.17724694 <a title="286-tfidf-8" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>9 0.15778527 <a title="286-tfidf-9" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>10 0.1553628 <a title="286-tfidf-10" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>11 0.15007554 <a title="286-tfidf-11" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>12 0.14879288 <a title="286-tfidf-12" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>13 0.14865881 <a title="286-tfidf-13" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>14 0.14042293 <a title="286-tfidf-14" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>15 0.1370241 <a title="286-tfidf-15" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>16 0.13683556 <a title="286-tfidf-16" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>17 0.13250209 <a title="286-tfidf-17" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>18 0.12692732 <a title="286-tfidf-18" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>19 0.12481964 <a title="286-tfidf-19" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>20 0.12468899 <a title="286-tfidf-20" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.287), (1, 0.126), (2, 0.023), (3, 0.028), (4, -0.389), (5, -0.069), (6, -0.075), (7, 0.035), (8, -0.049), (9, -0.08), (10, -0.066), (11, 0.013), (12, 0.057), (13, 0.131), (14, -0.045), (15, -0.037), (16, 0.014), (17, 0.075), (18, 0.058), (19, -0.01), (20, 0.037), (21, 0.002), (22, 0.096), (23, -0.004), (24, 0.087), (25, -0.015), (26, 0.078), (27, -0.079), (28, 0.028), (29, 0.037), (30, 0.036), (31, -0.058), (32, 0.052), (33, -0.043), (34, 0.017), (35, 0.136), (36, -0.056), (37, -0.016), (38, 0.02), (39, 0.04), (40, 0.023), (41, 0.091), (42, -0.025), (43, -0.018), (44, 0.014), (45, 0.028), (46, 0.126), (47, -0.016), (48, 0.046), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95538735 <a title="286-lsi-1" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>2 0.69240069 <a title="286-lsi-2" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>3 0.64796054 <a title="286-lsi-3" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>4 0.63803244 <a title="286-lsi-4" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>5 0.61661011 <a title="286-lsi-5" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>6 0.60860592 <a title="286-lsi-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.59908116 <a title="286-lsi-7" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>8 0.54871362 <a title="286-lsi-8" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>9 0.54802293 <a title="286-lsi-9" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>10 0.53309894 <a title="286-lsi-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.51834011 <a title="286-lsi-11" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>12 0.51529789 <a title="286-lsi-12" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>13 0.51279789 <a title="286-lsi-13" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>14 0.51229417 <a title="286-lsi-14" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>15 0.51153767 <a title="286-lsi-15" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>16 0.50263995 <a title="286-lsi-16" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>17 0.50175673 <a title="286-lsi-17" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>18 0.48899743 <a title="286-lsi-18" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>19 0.4875173 <a title="286-lsi-19" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>20 0.47970855 <a title="286-lsi-20" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.041), (33, 0.185), (34, 0.144), (41, 0.037), (49, 0.095), (56, 0.076), (70, 0.066), (85, 0.027), (89, 0.058), (93, 0.046), (94, 0.134), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90622574 <a title="286-lda-1" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>2 0.87017792 <a title="286-lda-2" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>3 0.86914849 <a title="286-lda-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.86759138 <a title="286-lda-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.86735994 <a title="286-lda-5" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>6 0.86629784 <a title="286-lda-6" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>7 0.86274987 <a title="286-lda-7" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>8 0.86227977 <a title="286-lda-8" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>9 0.85943437 <a title="286-lda-9" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>10 0.85547543 <a title="286-lda-10" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>11 0.85515255 <a title="286-lda-11" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>12 0.85510951 <a title="286-lda-12" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>13 0.85415876 <a title="286-lda-13" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>14 0.85392976 <a title="286-lda-14" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>15 0.85090476 <a title="286-lda-15" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>16 0.84949464 <a title="286-lda-16" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>17 0.84871733 <a title="286-lda-17" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>18 0.84823251 <a title="286-lda-18" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>19 0.84763485 <a title="286-lda-19" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>20 0.8472662 <a title="286-lda-20" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
