<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2013-Learning Multiple Models via Regularized Weighting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-158" href="#">nips2013-158</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2013-Learning Multiple Models via Regularized Weighting</h1>
<br/><p>Source: <a title="nips-2013-158-pdf" href="http://papers.nips.cc/paper/5098-learning-multiple-models-via-regularized-weighting.pdf">pdf</a></p><p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>Reference: <a title="nips-2013-158-reference" href="../nips2013_reference/nips-2013-Learning_Multiple_Models_via_Regularized_Weighting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. [sent-9, score-0.497]
</p><p>2 A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). [sent-10, score-0.307]
</p><p>3 However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. [sent-11, score-0.361]
</p><p>4 We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. [sent-12, score-0.376]
</p><p>5 We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i. [sent-15, score-0.691]
</p><p>6 Yet, in practice, it is common to encounter data that were generated by a mixture of several models rather than a single one, and the goal is to learn a number of models such that any given data can be explained by at least one of the learned models. [sent-21, score-0.376]
</p><p>7 It is also common for the data to contain outliers: data-points that are not well explained by any of the models to be learned, possibly inserted by external processes. [sent-22, score-0.207]
</p><p>8 At its center is the problem of assigning data points to models, with the main consideration that every model be consistent with many of the data points. [sent-24, score-0.326]
</p><p>9 Thus we seek for each model a distribution of weights over the data points, and encourage even weights by regularizing these distributions (hence our approach is called Regularized Weighting; abbreviated as RW). [sent-25, score-0.256]
</p><p>10 A data point that is inconsistent with all available models will receive lower weight and even sometimes be ignored. [sent-26, score-0.366]
</p><p>11 The value of ignoring difﬁcult points is illustrated by contrast with the common approach, which we consider next. [sent-27, score-0.249]
</p><p>12 This leaves the minimum loss approach vulnerable to outliers and corruptions: If one data point goes to inﬁnity, so must at least one model. [sent-30, score-0.67]
</p><p>13 Indeed, as we show later, the RW formulation is provably robust in the case of clustering, in the sense of having non-zero breakdown point [2]. [sent-32, score-0.561]
</p><p>14 A new formulation of the sub-task of associating data points to models as a convex optimization problem for setting weights. [sent-35, score-0.422]
</p><p>15 This problem favors broadly based models, and may ignore difﬁcult data points entirely. [sent-36, score-0.209]
</p><p>16 We show that the breakdown point of the proposed method is bounded away from zero for the clustering case. [sent-41, score-0.621]
</p><p>17 The breakdown point is a concept from robust statistics: it is the fraction of adversarial outliers that an algorithm can sustain without having its output arbitrarily changed. [sent-42, score-0.821]
</p><p>18 We show, empirically on a synthetic and real world datasets, that our formulation is more resistant to fat tailed additive noise. [sent-45, score-0.334]
</p><p>19 As almost every method to tackle the multiple model learning problem, we use alternating optimization of the models and the association (weights), i. [sent-53, score-0.291]
</p><p>20 Our formulation for optimizing the association requires solving a quadratic problem in kn variables, where k is the number of models and n is the number of points. [sent-56, score-0.295]
</p><p>21 Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details. [sent-61, score-0.57]
</p><p>22 Slightly closer to our approach is [12], whose formulation generalizes a common approach to different model types and permits for problem speciﬁc regularization, giving both generalization results and algorithmic iteration complexity results. [sent-64, score-0.247]
</p><p>23 Algorithms for dealing with outliers and multiple models together have been proposed in the context of clustering [14]. [sent-66, score-0.687]
</p><p>24 2  Formulation  In this section we show how multi-model learning problems can be formed from simple estimation problem (where we seek to explain weighted data points by a single model), and imposing a par2  ticular joint loss. [sent-69, score-0.445]
</p><p>25 We contrast the joint loss proposed here to a common one through the weights assigned by each and their effects on robustness. [sent-70, score-0.352]
</p><p>26 n  We refer throughout to n data points from X by (xi )i=1 = X ∈ X n , which we seek to explain k by k models from M denoted (mj )j=1 = M ∈ Mk . [sent-71, score-0.346]
</p><p>27 A base weighted learning problem is a tuple (X , M, ℓ, A), where ℓ : X × M → R+ is a non-negative convex function, which we call a base loss function and A : △n × X n → M deﬁnes an efﬁcient algorithm for choosing a model. [sent-74, score-0.438]
</p><p>28 Given the weight w and data X, A obtains n low weighted empirical loss i=1 wi ℓ (xi , m) (the weighted empirical loss need not be minimal, allowing for regularization which we do not discuss further). [sent-75, score-0.781]
</p><p>29 n  We will often denote the losses of a model m over X as a vector l = (ℓ(xi , m))i=1 . [sent-76, score-0.212]
</p><p>30 In the context of a set of models M , we similarly associate the loss vector lj and the weight vector wj with the ⊤ model mj ; this allows us to use the terse notation wj lj for the weighted loss of model j. [sent-77, score-1.433]
</p><p>31 • In subspace clustering, also known as subspace segmentation, the objective is to group the training samples into subsets, such that each subset can be well approximated by a low-dimensional afﬁne subspace. [sent-82, score-0.246]
</p><p>32 • Regression clustering [16] extends the standard linear regression problem in that the training samples cannot be explained by one linear function. [sent-84, score-0.377]
</p><p>33 The most common way to tackle the multiple model learning problem is the minimum loss approach, i. [sent-88, score-0.393]
</p><p>34 e, to minimize the following joint loss L (X, M ) =  1 n  min ℓ (x, m) . [sent-89, score-0.202]
</p><p>35 1)  In terms of weighted base learning problems, each model gives equal weight to all points for which 2 it is the best (lowest loss) model. [sent-91, score-0.56]
</p><p>36 For example, when M = X = Rn with ℓ(x, m) = x − m 2 the squared Euclidean distance loss yields k means clustering. [sent-92, score-0.256]
</p><p>37 In this context, alternating between choosing for each x its loss minimizing model, and adjusting each model to minimized the squared Euclidean loss, yields Lloyd’s algorithm (and its generalizations for other problems). [sent-93, score-0.327]
</p><p>38 The minimum loss approach requires that every point is assigned to a model, this can potentially cause problems in the presence of outliers. [sent-94, score-0.309]
</p><p>39 For example, consider the clustering case where the data contain a single outlier point xi . [sent-95, score-0.553]
</p><p>40 Let xi tend to inﬁnity; there will always be some mj that is closest to xi , and is therefore (at equilibrium) the average of xi and some other data points. [sent-96, score-0.502]
</p><p>41 We call this phenomenon mode I of sensitivity to outliers; it is common also 3  to such simple estimators as the mean. [sent-98, score-0.233]
</p><p>42 Mode II of sensitivity is more particular: as mj follows xi to inﬁnity, it stops being the closest to any other points, until the model is associated only to the outlier and thus matches it perfectly. [sent-99, score-0.567]
</p><p>43 Mode II of sensitivity is not clustering speciﬁc, and Fig. [sent-103, score-0.34]
</p><p>44 Neither mode is avoided by spreading a point’s weight among models as in mixture models [6]. [sent-106, score-0.523]
</p><p>45 A penalty term discourages the concentration of a model on few points and thus mode II sensitivity. [sent-108, score-0.324]
</p><p>46 For clustering this robustness is formalized in Theorem 2. [sent-110, score-0.424]
</p><p>47 1: Data is a mixture of two quadratics, with positive fat tailed noise. [sent-129, score-0.296]
</p><p>48 Under a minimum loss approach an off-the-chart high-noise point sufﬁces to prevent the top broken line from being close to many other data points. [sent-130, score-0.313]
</p><p>49 Given k weight vectors, we denote their averk age v (W ) = k −1 j=1 wj , and just v when W is clear from context. [sent-135, score-0.288]
</p><p>50 The Regularized Weighting k  multiple model learning loss is a function Lα : X n × Mk × (△n ) → R deﬁned as k  Lα (X, M, W ) = α u − v (W )  2 2  +k  l⊤ wj j  −1  (2. [sent-136, score-0.389]
</p><p>51 3)  As its name suggests, our formulation regularizes distributions of weight over data points; speciﬁcally, wj are controlled by forcing their average v to be close to the uniform distribution u. [sent-139, score-0.438]
</p><p>52 We avoid this by penalizing squared Euclidean distance from uniformity, which emphasizes points receiving weight much higher than the natural n−1 , and essentially ignores small variations around n−1 . [sent-141, score-0.47]
</p><p>53 In the following examples, we will consider a set of γnk −1 data points, recalling that nk −1 is the natural number of points per model. [sent-144, score-0.264]
</p><p>54 To avoid letting a few high loss outliers skew our models (mode I of sensitivity), we prefer instead to give them zero weight. [sent-145, score-0.526]
</p><p>55 Take γ ≪ k/2, then the cost of ignoring some γnk −1 points in all models is at most αn−1 · 2γk −1 ≪ αn−1 . [sent-146, score-0.263]
</p><p>56 335 model 1 weighs 21 points model 2 weighs 39 points  Weight assigned by model, scaled: wj,i · n/k  1. [sent-151, score-0.57]
</p><p>57 Within each model, weights are afﬁne in the loss (see Section 2. [sent-170, score-0.234]
</p><p>58 The gap allowed between the maximal weights of different models allows a point from the right cluster to be adopted by the left model, lowering overall penalty at a cost to weighted losses. [sent-172, score-0.272]
</p><p>59 If the jth model is ﬁt to only γnk −1 points for γ ≪ 1, the penalty from those points will be at least (approximately) αn−1 · γ −1 k −1 . [sent-174, score-0.377]
</p><p>60 We can make the ﬁrst situation cheap and the second expensive (per model) in comparison to the empirical weighted loss term by choosing k −1  αn  ≈k  −1  ⊤ w j lj . [sent-175, score-0.34]
</p><p>61 Consider the case where a model has low loss for fewer than n/(2k) points: spreading its weight only over them can incur very high costs due to the regularization term, which might be lowered by including some higher-loss points that are indeed better explained by another model (see Figure 2. [sent-178, score-0.768]
</p><p>62 This challenge might be solved by explicitly and separately estimating the relative frequencies of the classes, and penalizing deviations from the estimates rather than from equal frequencies, as is done in mixture models [6]; this is left for future study. [sent-180, score-0.203]
</p><p>63 1  Two properties of Regularized Weighting  Two properties of our formulation result from an analysis (in Appendix A for lack of space) of a dual problem of the weight setting problem (2. [sent-182, score-0.277]
</p><p>64 4): if outliers are present and αn−1 > 2B where B bounds losses on all points including outliers, weights will be almost uniform (enabling mode I of sensi5  tivity). [sent-189, score-0.858]
</p><p>65 One observation from this lemma is that if a particular model j gives weight to some point i, then every point with lower loss ℓ (xi′ , mj ) under that model will receive at least that much weight. [sent-200, score-0.881]
</p><p>66 This property plays a key role in the proof of robustness to outliers in clustering. [sent-201, score-0.431]
</p><p>67 3) is convex when we ﬁx the models, and an efﬁcient procedure A is assumed for solving a weighted base learning problem for a model, supporting an alternating optimization approach, as in Algorithm 1; see Section 5 for further discussion. [sent-205, score-0.247]
</p><p>68 1 on page 4 provides a positive example in regression clustering, and a more substantial empirical evaluation on subspace clustering is in Appendix B. [sent-208, score-0.476]
</p><p>69 In the particular case of clustering with the squared Euclidean loss, robustness beneﬁts can be proved. [sent-209, score-0.444]
</p><p>70 We use “breakdown point” – the standard robustness measure in the literature of robust statistics [2], see also [17, 18] and many others – to quantify the robustness property of the proposed formulation. [sent-210, score-0.35]
</p><p>71 The breakdown point of an estimator is the smallest fraction of bad observations that can cause the estimator to take arbitrarily aberrant values, i. [sent-211, score-0.388]
</p><p>72 , the smallest fraction of outliers needed to completely break an estimator. [sent-213, score-0.304]
</p><p>73 For the case of clustering with the squared Euclidean distance base loss, the min-loss approach corresponds to k-means clustering which is not robust in this sense; its breakdown point is 0. [sent-214, score-1.164]
</p><p>74 The non robustness of k-means has led to the development of many formulations of robust clustering, see a review by [14]. [sent-215, score-0.223]
</p><p>75 In contrast, we show that our joint loss yields an estimator that has a non-zero breakdown point, and is hence robust. [sent-216, score-0.502]
</p><p>76 In general, a squared loss clustering formulation that assigns equal weight to different data points cannot be robust – as one data point tends to inﬁnity so must at least one model. [sent-217, score-1.163]
</p><p>77 On the other hand if α is too low, it becomes possible 6  for each model to assign all of its weight to a single point, which may well be an outlier tending to inﬁnity. [sent-219, score-0.348]
</p><p>78 Let X = M be a Euclidean space in which we perform clustering with the loss 2 ℓ (xi , mj ) = mj − xi and k centers. [sent-222, score-0.999]
</p><p>79 Denote by R the radius of any ball containing the inliers, −2 and η < k /22 the proportion of outliers allowed to be outside the ball. [sent-223, score-0.347]
</p><p>80 Denote also by r a radius such that there exists M ′ = {m′ , · · · , m′ } such that each inlier is within a distance r of some 1 k model m′ and each mj approximates (i. [sent-224, score-0.466]
</p><p>81 mj − xi 2 ≤ 6R for every model mj and inlier xi . [sent-228, score-0.751]
</p><p>82 Then we have  Theorem 2 shows that when the number of outliers is not too high, then the learned model, regardless of the magnitude of the outliers, is close to the inliers and hence cannot be arbitrarily bad. [sent-229, score-0.487]
</p><p>83 In particular, the theorem implies a non-zero breakdown point for any α > nr2 ; taking too high an α merely forces a larger but still ﬁnite R. [sent-230, score-0.357]
</p><p>84 If the inliers are amenable to balanced clustering so that r ≪ R, the regime of non-zero breakdown is extended to smaller α. [sent-231, score-0.716]
</p><p>85 First, due to the regularization term, for any model, the total weight on the few outliers is at most 1/3. [sent-233, score-0.503]
</p><p>86 Second, an optimal model must thus be at least twice as close to the weighted average of its inlier as it is to the weighted average of its outliers. [sent-234, score-0.305]
</p><p>87 This step depends critically on squared Euclidean loss being used. [sent-235, score-0.218]
</p><p>88 Lastly, this gap in distances cannot be large in absolute terms, due to Lemma 2; an outlier that is much farther from the model than the inliers must receive weight zero. [sent-236, score-0.541]
</p><p>89 The current formulation seems to be particularly vulnerable since it allows data to be ignored, in contrast to most generalization bounds that assume equal weight is given to all data. [sent-239, score-0.46]
</p><p>90 Our loss Lα (X, M ) differs from common losses in allowing data points to be differently weighted. [sent-240, score-0.618]
</p><p>91 We shall endow Mk with the metric d∞ (M, M ′ ) = max ℓ (·, mj ) − ℓ ·, m′ j j∈[k]  ∞  and deﬁne its covering number Nε Mk as the minimal cardinality of a set Mk such that Mk ⊆ ε k B(M, ε). [sent-246, score-0.322]
</p><p>92 M ∈M ε  The bound depends on an upper bound on base losses denoted B; this should be viewed as ﬁxing a scale for the losses and is standard where losses are not naturally bounded (e. [sent-247, score-0.599]
</p><p>93 Let the base losses be bounded in the interval [0, B], let Mk have covering numdk bers Nε Mk ≤ (C/ε) and let γ = nB/ (2α). [sent-252, score-0.325]
</p><p>94 Given data and models (X, M ) there exists an algorithm that ﬁnds a weight matrix W such that Lα (X, M, W ) − Lα (X, M ) ≤ ε using O and memory. [sent-261, score-0.268]
</p><p>95 The ﬁrst bound might suggest that typical settings of α ∝ n requires iterations to increase with the number of points n; the second bounds shows this is not always necessary. [sent-263, score-0.233]
</p><p>96 6  Conclusion  In this paper, we proposed and analyzed, from a general perspective, a new formulation for learning multiple models that explain well much of the data. [sent-266, score-0.274]
</p><p>97 This is based on associating to each model a regularized weight distribution over the data it explains well. [sent-267, score-0.385]
</p><p>98 We further provided generalization bounds and explained an optimization procedure to solve the formulation in scale. [sent-269, score-0.261]
</p><p>99 Our main motivation comes from the fast growing attention to analyzing data using multiple models, under the names of k-means clustering, subspace segmentation, and Gaussian mixture models, to list a few. [sent-270, score-0.297]
</p><p>100 We believe general methods with desirable properties such as generalization and robustness will supply ready tools for new applications using other model types. [sent-272, score-0.223]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('outliers', 0.304), ('breakdown', 0.3), ('clustering', 0.264), ('mj', 0.258), ('weight', 0.169), ('losses', 0.169), ('points', 0.167), ('loss', 0.165), ('mk', 0.16), ('inliers', 0.152), ('outlier', 0.136), ('robustness', 0.127), ('subspace', 0.123), ('fat', 0.121), ('wj', 0.119), ('mode', 0.114), ('formulation', 0.108), ('tailed', 0.105), ('mml', 0.103), ('robust', 0.096), ('base', 0.092), ('weighted', 0.089), ('weighting', 0.087), ('lj', 0.086), ('rw', 0.084), ('inlier', 0.084), ('regularized', 0.083), ('nity', 0.079), ('sensitivity', 0.076), ('shie', 0.075), ('euclidean', 0.071), ('mixture', 0.07), ('weights', 0.069), ('alternating', 0.066), ('explained', 0.065), ('kn', 0.064), ('covering', 0.064), ('lloyd', 0.063), ('multiple', 0.062), ('singapore', 0.058), ('point', 0.057), ('models', 0.057), ('segmentation', 0.056), ('spreading', 0.056), ('weighs', 0.056), ('nk', 0.055), ('xi', 0.054), ('sons', 0.053), ('generalization', 0.053), ('squared', 0.053), ('fista', 0.053), ('vulnerable', 0.053), ('minimum', 0.049), ('lemma', 0.048), ('regression', 0.048), ('associating', 0.048), ('explain', 0.047), ('technion', 0.046), ('huan', 0.046), ('haifa', 0.044), ('af', 0.044), ('penalizing', 0.043), ('radius', 0.043), ('model', 0.043), ('common', 0.043), ('appendix', 0.042), ('data', 0.042), ('receive', 0.041), ('page', 0.041), ('partly', 0.041), ('tend', 0.04), ('ignoring', 0.039), ('mi', 0.039), ('assigned', 0.038), ('distance', 0.038), ('israel', 0.038), ('focs', 0.037), ('joint', 0.037), ('ieee', 0.036), ('bounds', 0.035), ('associate', 0.034), ('wiley', 0.034), ('solutions', 0.034), ('optimizing', 0.034), ('xu', 0.034), ('seek', 0.033), ('adversarial', 0.033), ('formalized', 0.033), ('ignored', 0.033), ('frequencies', 0.033), ('association', 0.032), ('consideration', 0.032), ('allowing', 0.032), ('iterations', 0.031), ('tackle', 0.031), ('arbitrarily', 0.031), ('seeks', 0.031), ('regularization', 0.03), ('lowered', 0.03), ('ticular', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="158-tfidf-1" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>2 0.2184713 <a title="158-tfidf-2" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>3 0.14473665 <a title="158-tfidf-3" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>Author: Mahito Sugiyama, Karsten Borgwardt</p><p>Abstract: Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efﬁciency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search. 1</p><p>4 0.13346362 <a title="158-tfidf-4" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>Author: Yu-Xiang Wang, Huan Xu, Chenlei Leng</p><p>Abstract: Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for subspace clustering. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of “Self-Expressiveness”. The main difference is that SSC minimizes the vector 1 norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank, we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the “Self-Expressiveness Property” and “Graph Connectivity” at the same time. 1</p><p>5 0.11778869 <a title="158-tfidf-5" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>Author: Eunho Yang, Pradeep Ravikumar</p><p>Abstract: We provide a uniﬁed framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M -estimators that minimize the sum of any loss function, and an instance of what we call a “hybrid” regularization, that is the inﬁmal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures. 1</p><p>6 0.11276466 <a title="158-tfidf-6" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>7 0.10859823 <a title="158-tfidf-7" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>8 0.10698813 <a title="158-tfidf-8" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>9 0.10617891 <a title="158-tfidf-9" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>10 0.095931336 <a title="158-tfidf-10" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>11 0.093383551 <a title="158-tfidf-11" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>12 0.093110286 <a title="158-tfidf-12" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>13 0.091375522 <a title="158-tfidf-13" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>14 0.089938872 <a title="158-tfidf-14" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>15 0.086326122 <a title="158-tfidf-15" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>16 0.085540891 <a title="158-tfidf-16" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>17 0.084502347 <a title="158-tfidf-17" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>18 0.080227479 <a title="158-tfidf-18" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>19 0.079891741 <a title="158-tfidf-19" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>20 0.079429559 <a title="158-tfidf-20" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, 0.064), (2, 0.077), (3, 0.008), (4, 0.035), (5, 0.057), (6, -0.048), (7, 0.05), (8, -0.096), (9, -0.008), (10, -0.044), (11, 0.125), (12, 0.111), (13, 0.098), (14, 0.096), (15, 0.01), (16, 0.048), (17, -0.028), (18, 0.1), (19, -0.043), (20, -0.035), (21, 0.078), (22, 0.018), (23, 0.017), (24, -0.026), (25, 0.022), (26, -0.081), (27, 0.003), (28, -0.028), (29, -0.018), (30, -0.098), (31, 0.067), (32, 0.145), (33, 0.008), (34, 0.03), (35, -0.011), (36, -0.108), (37, -0.068), (38, -0.018), (39, 0.033), (40, -0.058), (41, 0.038), (42, -0.076), (43, 0.054), (44, 0.066), (45, -0.146), (46, 0.079), (47, 0.019), (48, -0.015), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9484517 <a title="158-lsi-1" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>2 0.70300525 <a title="158-lsi-2" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>Author: Mahito Sugiyama, Karsten Borgwardt</p><p>Abstract: Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efﬁciency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search. 1</p><p>3 0.65738779 <a title="158-lsi-3" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>Author: Yu-Xiang Wang, Huan Xu, Chenlei Leng</p><p>Abstract: Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for subspace clustering. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of “Self-Expressiveness”. The main difference is that SSC minimizes the vector 1 norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank, we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the “Self-Expressiveness Property” and “Graph Connectivity” at the same time. 1</p><p>4 0.64225018 <a title="158-lsi-4" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>5 0.63323736 <a title="158-lsi-5" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>Author: Martin Mevissen, Emanuele Ragnoli, Jia Yuan Yu</p><p>Abstract: We consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions. This set is a ball around a density function estimated from data samples, i.e., it is data-driven and random. Polynomial optimization problems are inherently hard due to nonconvex objectives and constraints. However, we show that by employing polynomial and histogram density estimates, we can introduce robustness with respect to distributional uncertainty sets without making the problem harder. We show that the optimum to the distributionally robust problem is the limit of a sequence of tractable semideﬁnite programming relaxations. We also give ﬁnite-sample consistency guarantees for the data-driven uncertainty sets. Finally, we apply our model and solution method in a water network optimization problem. 1</p><p>6 0.62000525 <a title="158-lsi-6" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>7 0.61369205 <a title="158-lsi-7" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>8 0.60718912 <a title="158-lsi-8" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>9 0.60396284 <a title="158-lsi-9" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>10 0.59898919 <a title="158-lsi-10" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>11 0.58481181 <a title="158-lsi-11" href="./nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</a></p>
<p>12 0.57833183 <a title="158-lsi-12" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>13 0.57265902 <a title="158-lsi-13" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>14 0.56922984 <a title="158-lsi-14" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>15 0.56765187 <a title="158-lsi-15" href="./nips-2013-Parametric_Task_Learning.html">244 nips-2013-Parametric Task Learning</a></p>
<p>16 0.56455064 <a title="158-lsi-16" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>17 0.55100906 <a title="158-lsi-17" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>18 0.54494494 <a title="158-lsi-18" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>19 0.54211903 <a title="158-lsi-19" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>20 0.54141319 <a title="158-lsi-20" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.022), (10, 0.193), (16, 0.029), (33, 0.156), (34, 0.137), (36, 0.01), (41, 0.038), (49, 0.036), (56, 0.122), (70, 0.054), (85, 0.03), (89, 0.048), (93, 0.051), (95, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86933953 <a title="158-lda-1" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>2 0.86702269 <a title="158-lda-2" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>Author: Boqing Gong, Kristen Grauman, Fei Sha</p><p>Abstract: In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difﬁcult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identiﬁed domains to be different from each other to the maximum extent; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric formulation and efﬁcient optimization procedure that can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks. 1</p><p>same-paper 3 0.8603695 <a title="158-lda-3" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>4 0.79750657 <a title="158-lda-4" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>Author: Eshcar Hillel, Zohar S. Karnin, Tomer Koren, Ronny Lempel, Oren Somekh</p><p>Abstract: We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an ε-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to com√ municate only once, they are able to learn k times faster than a single player. √ That is, distributing learning to k players gives rise to a factor k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε. 1</p><p>5 0.79009891 <a title="158-lda-5" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>Author: Fabian Sinz, Anna Stockl, January Grewe, January Benda</p><p>Abstract: We present a novel non-parametric method for ﬁnding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest. 1</p><p>6 0.78769898 <a title="158-lda-6" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>7 0.78670341 <a title="158-lda-7" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>8 0.78500849 <a title="158-lda-8" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>9 0.78495705 <a title="158-lda-9" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>10 0.78463912 <a title="158-lda-10" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>11 0.78459692 <a title="158-lda-11" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>12 0.78405887 <a title="158-lda-12" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>13 0.78375161 <a title="158-lda-13" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>14 0.78309697 <a title="158-lda-14" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>15 0.78269851 <a title="158-lda-15" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>16 0.78267932 <a title="158-lda-16" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>17 0.78220981 <a title="158-lda-17" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>18 0.78140354 <a title="158-lda-18" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>19 0.78135753 <a title="158-lda-19" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>20 0.78125829 <a title="158-lda-20" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
