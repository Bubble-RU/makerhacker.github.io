<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-128" href="#">nips2013-128</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</h1>
<br/><p>Source: <a title="nips-2013-128-pdf" href="http://papers.nips.cc/paper/4997-generalized-method-of-moments-for-rank-aggregation.pdf">pdf</a></p><p>Author: Hossein Azari Soufiani, William Chen, David C. Parkes, Lirong Xia</p><p>Abstract: In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efﬁciency. 1</p><p>Reference: <a title="nips-2013-128-reference" href="../nips2013_reference/nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. [sent-11, score-0.158]
</p><p>2 Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. [sent-12, score-0.864]
</p><p>3 We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. [sent-13, score-0.103]
</p><p>4 1  Introduction  In many applications, we need to aggregate the preferences of agents over a set of alternatives to produce a joint ranking. [sent-15, score-0.124]
</p><p>5 For example, in systems for ranking the quality of products, restaurants, or other services, we can generate an aggregate rank through feedback from individual users. [sent-16, score-0.185]
</p><p>6 This idea of rank aggregation also plays an important role in multiagent systems, meta-search engines [4], belief merging [5], crowdsourcing [15], and many other e-commerce applications. [sent-17, score-0.1]
</p><p>7 A standard approach towards rank aggregation is to treat input rankings as data generated from a probabilistic model, and then learn the MLE of the input data. [sent-18, score-0.196]
</p><p>8 This line of research is sometimes referred to as learning to rank [11]. [sent-24, score-0.049]
</p><p>9 [16] proposed a rank aggregation algorithm, called Rank Centrality (RC), based on computing the stationary distribution of a Markov chain whose transition matrix is deﬁned according to the data (pairwise comparisons among alternatives). [sent-26, score-0.205]
</p><p>10 The authors describe the approach as being model independent, and prove that for data generated according to BTL, the output of RC converges to the ground truth, and the performance of RC is almost identical to the performance of 1  MLE for BTL. [sent-27, score-0.063]
</p><p>11 In this paper, we take a generalized method-of-moments (GMM) point of view towards rank aggregation. [sent-30, score-0.072]
</p><p>12 The main technical contribution of this paper is a class of GMMs for parameter estimation under the PL model, which generalizes BTL and the input consists of full rankings instead of pairwise comparisons as in the case of BTL and RC algorithm. [sent-32, score-0.315]
</p><p>13 Our algorithms ﬁrst break full rankings into pairwise comparisons, and then solve the generalized moment conditions to ﬁnd the parameters. [sent-33, score-0.27]
</p><p>14 Each of our GMMs is characterized by a way of breaking full rankings. [sent-34, score-0.675]
</p><p>15 We characterize conditions for the output of the algorithm to be unique, and we also obtain some general characterizations that help us to determine which method of breaking leads to a consistent GMM. [sent-35, score-0.716]
</p><p>16 Speciﬁcally, full breaking (which uses all pairwise comparisons in the ranking) is consistent, but adjacent breaking (which only uses pairwise comparisons in adjacent positions) is inconsistent. [sent-36, score-1.872]
</p><p>17 We also compare statistical efﬁciency and running time of these methods experimentally using both synthetic and real-world data, showing that all GMMs run much faster than the MM algorithm. [sent-38, score-0.064]
</p><p>18 For the synthetic data, we observe that many consistent GMMs converge as fast as the MM algorithm, while there exists a clear tradeoff between computational complexity and statistical efﬁciency among consistent GMMs. [sent-39, score-0.193]
</p><p>19 However, we note that our algorithms aggregate full rankings under PL, while the RC algorithm aggregates pairwise comparisons. [sent-41, score-0.274]
</p><p>20 Moreover, by taking a GMM point of view, we prove the consistency of our algorithms on top of theories for GMMs, while Negahban et al. [sent-43, score-0.027]
</p><p>21 , dn } denote the data, where each dj is a full ranking over C. [sent-51, score-0.172]
</p><p>22 The PL model is a parametric model where each alternative ci is m parameterized by γi ∈ (0, 1), such that i=1 γi = 1. [sent-52, score-0.117]
</p><p>23 Let Ω i=1 ∗ Given γ ∈ Ω, the probability for a ranking d = [ci1 ci2 · · · cim ] is deﬁned as follows. [sent-59, score-0.11]
</p><p>24 γim−1 γi γi PrPL (d|γ) = m 1 × m2 × ··· × γim−1 + γim l=1 γil l=2 γil In the BTL model, the data is composed of pairwise comparisons instead of rankings, and the γ i1 model is parameterized in the same way as PL, such that PrBTL (ci1 ci2 |γ) = . [sent-60, score-0.157]
</p><p>25 A GMM is consistent if and only if for any γ ∗ ∈ Ω, GMMg (D, W) converges in probability to γ ∗ as n → ∞ and the data is drawn i. [sent-67, score-0.085]
</p><p>26 2  It is well-known that GMMg (D, W) is consistent if it satisﬁes some regularity conditions plus the following condition [7]: Condition 1. [sent-72, score-0.102]
</p><p>27 MLE as a consistent GMM: Suppose the likelihood function is twice-differentiable, then the MLE is a consistent GMM where g(d, γ) = γ log Pr(d|γ) and Wn = I. [sent-75, score-0.13]
</p><p>28 [16] proposed the Rank Centrality (RC) algorithm that aggregates pairwise comparisons DP = {Y1 , . [sent-78, score-0.177]
</p><p>29 1 Let aij denote the number of ci cj in DP and it is assumed that for any i = j, aij + aji = k. [sent-82, score-0.285]
</p><p>30 Let dmax denote the maximum pairwise defeats for an alternative. [sent-83, score-0.07]
</p><p>31 Let X ci  cj  (Y ) =  1 0  if Y = [ci otherwise  cj ]  ∗ and PRC (Y ) =  X ci cl l=i X  −  cj ci  if i = j if i = j . [sent-85, score-0.743]
</p><p>32 It is not hard to check that the output of RC is the output of GMMgRC . [sent-87, score-0.038]
</p><p>33 Moreover, GMMgRC satisﬁes Condition 1 under the BTL model, and as we will show later in Corollary 4, GMMgRC is consistent for BTL. [sent-88, score-0.065]
</p><p>34 3  Generalized Method-of-Moments for the Plakett-Luce model  In this section we introduce our GMMs for rank aggregation under PL. [sent-89, score-0.1]
</p><p>35 For any full ranking d over C, we let • X ci  cj  (d) =  1 0  ci d cj otherwise  • P (d) be an m × m matrix where P (d)ij = • gF (d, γ) = P (d) · γ and P (D) =  1 n  d∈D  −  X ci cl l=i X  cj ci  (d) if i = j (d) if i = j  P (d)  For example, let m = 3, D = {[c1 c2 c3 ], [c2 c3 c1 ]}. [sent-93, score-1.032]
</p><p>36 Moreover, we will show in Corollary 3 that GMMgF is consistent for PL. [sent-99, score-0.065]
</p><p>37 In the above example, we count all pairwise comparisons in a full ranking d to build P (d), and deﬁne g = P (D) · γ to be linear in γ. [sent-100, score-0.329]
</p><p>38 In general, we may consider some subset of pairwise comparisons. [sent-101, score-0.07]
</p><p>39 Intuitively, a breaking is an undirected graph over the m positions in a ranking, such that for any full ranking d, the pairwise comparisons between alternatives in the ith position and jth position are counted to construct PG (d) if and only if {i, j} ∈ G. [sent-103, score-0.984]
</p><p>40 A breaking is a non-empty undirected graph G whose vertices are {1, . [sent-105, score-0.613]
</p><p>41 Given any breaking G, any full ranking d over C, and any ci , cj ∈ C, we let 1 The BTL model in [16] is slightly different from that in this paper. [sent-109, score-1.024]
</p><p>42 3  c  • XGi  cj  (d) =  1 0  {Pos(ci , d), Pos(cj , d)} ∈ G and ci otherwise  d  cj  , where Pos(ci , d) is the posi-  tion of ci in d. [sent-111, score-0.478]
</p><p>43 c  • PG (d) be an m × m matrix where PG (d)ij =  −  XGi cl l=i XG  cj ci  (d) if i = j (d) if i = j  • gG (d, γ) = PG (d) · γ • GMMG (D) be the GMM method that solves Equation (1) for gG and Wn = I. [sent-112, score-0.265]
</p><p>44 T P We are now ready to present our GMM algorithm (Algorithm 1) parameterized by a breaking G. [sent-131, score-0.613]
</p><p>45 B  4  Algorithm 1: GMMG (D) Input: A breaking G and data D = {d1 , . [sent-134, score-0.613]
</p><p>46 For any breaking G and any data D, there exists γ ∈ Ω such that PG (D) · γ = 0. [sent-145, score-0.613]
</p><p>47 Among the following three conditions, 1 and 2 are equivalent for any breaking G and any data D. [sent-154, score-0.63]
</p><p>48 Moreover, conditions 1 and 2 are equivalent to condition 3 if and only if G is connected. [sent-155, score-0.054]
</p><p>49 For the full breaking, adjacent breaking, and any top-k breaking, the three statements in Theorem 2 are equivalent for any data D. [sent-163, score-0.214]
</p><p>50 [6] identiﬁed a necessary and sufﬁcient condition on data D for the MLE under PL to be unique, which is equivalent to condition 1 in Theorem 2. [sent-166, score-0.053]
</p><p>51 For the full breaking GF , |GMMGF (D)| = 1 if and only if |MLEP L (D)| = 1. [sent-169, score-0.675]
</p><p>52 2  Consistency  We say a breaking G is consistent (for PL), if GMMG is consistent (for PL). [sent-171, score-0.743]
</p><p>53 Below, we show that some breakings deﬁned in the last subsection are consistent. [sent-172, score-0.238]
</p><p>54 A breaking G is consistent if and only if Ed|γ ∗ [g(d, γ ∗ )] = 0, which is equivalent to the following equalities: Pr(ci cj |{Pos(ci , d), Pos(cj , d)} ∈ G) γ∗ for all i = j, = i. [sent-175, score-0.817]
</p><p>55 (2) ∗ Pr(cj ci |{Pos(ci ), Pos(cj )} ∈ G) γj Theorem 4. [sent-176, score-0.117]
</p><p>56 Continuing, we show that position-k breakings are consistent, then use this and Theorem 4 as building blocks to prove additional consistency results. [sent-182, score-0.248]
</p><p>57 For any k ≥ 1, the position-k breaking Gk is consistent. [sent-184, score-0.613]
</p><p>58 The full breaking GF is consistent; for any k, Gk is consistent, and for any k ≥ 2, T Gk is consistent. [sent-188, score-0.675]
</p><p>59 Adjacent breaking GA is consistent if and only if all components in γ ∗ are the same. [sent-190, score-0.678]
</p><p>60 5  Lastly, the technique developed in this section can also provide an independent proof that the RC algorithm is consistent for BTL, which is implied by the main theorem in [16]: Corollary 4. [sent-191, score-0.065]
</p><p>61 By checking similar conditions as we did in the proof of Theorem 3, we can prove that GM MgRC is consistent for BTL. [sent-194, score-0.084]
</p><p>62 The results in this section suggest that if we want to learn the parameters of PL, we should use consistent breakings, including full breaking, top-k breakings, bottom-k breakings, and position-k breakings. [sent-195, score-0.127]
</p><p>63 The adjacent breaking seems quite natural, but it is not consistent, thus will not provide a good estimate to the parameters of PL. [sent-196, score-0.748]
</p><p>64 • GMM (Algorithm 1) with full breaking: O(m2 n + m2. [sent-203, score-0.062]
</p><p>65 In particular, the GMM with adjacent breaking and top-k breaking for constant k’s are the fastest. [sent-213, score-1.361]
</p><p>66 However, we recall that the GMM with adjacent breaking is not consistent, while the other algorithms are consistent. [sent-214, score-0.748]
</p><p>67 We would expect that as data size grows, the GMM with adjacent breaking will provide a relatively poor estimation to γ ∗ compared to the other methods. [sent-215, score-0.748]
</p><p>68 As it can be seen above the coefﬁcient for n is linear in m for top-k breaking and quadratic for full breaking while it is cubic in m for the MM algorithm. [sent-218, score-1.288]
</p><p>69 Therefore, it is natural to conjecture that for the same data, GMMGk with large k converges faster than GMMGk with small k. [sent-221, score-0.061]
</p><p>70 In other words, T T we expect to see the following time-efﬁciency tradeoff among GMMGk for different k’s, which is T veriﬁed by the experimental results in the next section. [sent-222, score-0.039]
</p><p>71 4  Experiments  The running time and statistical efﬁciency of MM and our GMMs are examined for both synthetic data and a real-world sushi dataset [9]. [sent-225, score-0.135]
</p><p>72 • Generating the ground truth: for m ≤ 300, the ground truth γ ∗ is generated from the Dirichlet distribution Dir(1). [sent-227, score-0.07]
</p><p>73 • Generating data: given a ground truth γ ∗ , we generate up to 1000 full rankings from PL. [sent-228, score-0.204]
</p><p>74 We implemented MM [8] for 1, 3, 10 iterations, as well as GMMs with full breaking, adjacent breaking, and top-k breaking for all k ≤ m − 1. [sent-229, score-0.81]
</p><p>75 2 • Kendall Rank Correlation Coefﬁcient: Let K(γ, γ ∗ ) denote the Kendall tau distance between the ranking over components in γ and the ranking over components in γ ∗ . [sent-233, score-0.22]
</p><p>76 The Kendall correlation K(γ,γ ∗ ) is 1 − 2 m(m−1)/2 . [sent-234, score-0.049]
</p><p>77 The multiple repetitions for the statistical efﬁciency experiments in Figure 3 and experiments for sushi data in Figure 5 have been done using the odyssey cluster. [sent-237, score-0.091]
</p><p>78 1 Synthetic Data In this subsection we focus on comparisons among MM, GMM-F (full breaking), and GMM-A (adjacent breaking). [sent-240, score-0.122]
</p><p>79 For the Kendall correlation criterion, GMM-F (full breaking) has the best performance and GMM-A (adjacent breaking) has the worst performance. [sent-245, score-0.049]
</p><p>80 The only exception is between GMM-F (full breaking) and MM for Kendall correlation at n = 1000. [sent-248, score-0.049]
</p><p>81 000 250  500  750  1000  n (agents)  250  500  750  1000  n (agents)  Figure 3: The MSE and Kendall correlation of MM (10 iterations), GMM-F (full breaking), and GMM-A (adjacent breaking). [sent-268, score-0.049]
</p><p>82 2  Time-Efﬁciency Tradeoff among Top-k Breakings  Results on the running time and statistical efﬁciency for top-k breakings are shown in Figure 4. [sent-271, score-0.259]
</p><p>83 We recall that top-1 is equivalent to position-1, and top-(m − 1) is equivalent to the full breaking. [sent-272, score-0.096]
</p><p>84 For n = 100, MSE comparisons between successive top-k breakings are statistically signiﬁcant at 95% level from (top-1, top-2) to (top-6, top-7). [sent-273, score-0.308]
</p><p>85 The comparisons in running time are all signiﬁcant at 95% conﬁdence level. [sent-274, score-0.107]
</p><p>86 On average, we observe that top-k breakings with smaller k run faster, while top-k breakings with larger k have higher statistical efﬁciency in both MSE and Kendall correlation. [sent-275, score-0.442]
</p><p>87 3  Experiments for Real Data  In the sushi dataset [9], there are 10 kinds of sushi (m = 10) and the amount of data n is varied, randomly sampling with replacement. [sent-278, score-0.182]
</p><p>88 We set the ground truth to be the output of MM applied to all 5000 data points. [sent-279, score-0.065]
</p><p>89 For the running time, we observe the same as for the synthetic data: GMM (adjacent breaking) runs faster than GMM (full breaking), which runs faster than MM (The results on running time can be found in supplementary material B). [sent-280, score-0.104]
</p><p>90 Comparisons for MSE and Kendall correlation are shown in Figure 5. [sent-281, score-0.049]
</p><p>91 0000 1  2  3  4  5  6  7  8  9  1  2  k (Top k Breaking)  3  4  5  6  7  8  9  1  k (Top k Breaking)  2  3  4  5  6  7  8  9  k (Top k Breaking)  Figure 4: Comparison of GMM with top-k breakings as k is varied. [sent-297, score-0.221]
</p><p>92 0000  0 1000 2000 3000 4000 5000  1000  2000  n (agents)  3000  n (agents)  4000  5000  1000  2000  3000  4000  5000  n (agents)  Figure 5: The MSE and Kendall correlation criteria and computation time for MM (10 iterations), GMM-F (full breaking), and GMM-A (adjacent breaking) on sushi data. [sent-308, score-0.14]
</p><p>93 Differences between performances are all statistically signiﬁcant with 95% conﬁdence (with exception of Kendall correlation and both GMM methods for n = 200, where p = 0. [sent-310, score-0.049]
</p><p>94 This is different from comparisons for synthetic data (Figure 3). [sent-312, score-0.111]
</p><p>95 We believe that the main reason is because PL does not ﬁt sushi data well, which is a fact recently observed by Azari et al. [sent-313, score-0.091]
</p><p>96 Therefore, we cannot expect that GMM converges to the output of MM on the sushi dataset, since the consistency results (Corollary 3) assumes that the data is generated under PL. [sent-315, score-0.157]
</p><p>97 5  Future Work  We plan to work on the connection between consistent breakings and preference elicitation. [sent-316, score-0.286]
</p><p>98 For example, even though the theory in this paper is developed for full ranks, the notion of top-k and bottom-k breaking are implicitly allowing some partial order settings. [sent-317, score-0.675]
</p><p>99 More speciﬁcally, top-k breaking can be achieved from partial orders that include full rankings for the top-k alternatives. [sent-318, score-0.771]
</p><p>100 Essai sur l’application de l’analyse a la probabilit´ des d´ cisions rendues a la e e ` ` pluralit´ des voix. [sent-339, score-0.046]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('breaking', 0.613), ('gmm', 0.359), ('gmmg', 0.251), ('breakings', 0.221), ('mm', 0.21), ('gmms', 0.168), ('btl', 0.162), ('rc', 0.145), ('kendall', 0.138), ('adjacent', 0.135), ('cj', 0.122), ('ci', 0.117), ('pl', 0.114), ('gf', 0.113), ('ranking', 0.11), ('rankings', 0.096), ('sushi', 0.091), ('pg', 0.089), ('comparisons', 0.087), ('gk', 0.084), ('mse', 0.078), ('pos', 0.075), ('pairwise', 0.07), ('consistent', 0.065), ('gm', 0.065), ('wn', 0.063), ('full', 0.062), ('prc', 0.059), ('agents', 0.056), ('aggregation', 0.051), ('rank', 0.049), ('correlation', 0.049), ('azari', 0.048), ('gmmgf', 0.044), ('gmmgk', 0.044), ('gmmgrc', 0.044), ('prpl', 0.044), ('alternatives', 0.042), ('lirong', 0.039), ('mle', 0.039), ('harvard', 0.037), ('parkes', 0.036), ('ciency', 0.035), ('negahban', 0.035), ('kdmax', 0.029), ('mgrc', 0.029), ('prbtl', 0.029), ('yiling', 0.029), ('consistency', 0.027), ('corollary', 0.027), ('cl', 0.026), ('ani', 0.026), ('centrality', 0.026), ('hossein', 0.026), ('sou', 0.026), ('xgi', 0.026), ('agent', 0.026), ('aggregate', 0.026), ('dence', 0.025), ('dp', 0.025), ('synthetic', 0.024), ('ground', 0.024), ('bellevue', 0.024), ('devavrat', 0.024), ('ford', 0.024), ('gg', 0.024), ('seas', 0.024), ('usa', 0.024), ('aij', 0.023), ('generalized', 0.023), ('des', 0.023), ('mallows', 0.023), ('im', 0.022), ('truth', 0.022), ('tahoe', 0.021), ('xia', 0.021), ('conjecture', 0.021), ('tradeoff', 0.021), ('gl', 0.02), ('faster', 0.02), ('running', 0.02), ('aggregates', 0.02), ('converges', 0.02), ('nv', 0.019), ('output', 0.019), ('conditions', 0.019), ('pr', 0.018), ('condition', 0.018), ('among', 0.018), ('lake', 0.018), ('reveals', 0.018), ('il', 0.017), ('equivalent', 0.017), ('intervals', 0.017), ('rq', 0.017), ('wa', 0.017), ('subsection', 0.017), ('calculated', 0.015), ('ij', 0.015), ('ga', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="128-tfidf-1" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>Author: Hossein Azari Soufiani, William Chen, David C. Parkes, Lirong Xia</p><p>Abstract: In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efﬁciency. 1</p><p>2 0.14165442 <a title="128-tfidf-2" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>Author: Dan Rosenbaum, Daniel Zoran, Yair Weiss</p><p>Abstract: Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical ﬂow to learn the local statistics of optical ﬂow and compare the learned models to prior models assumed by computer vision researchers. We ﬁnd that a Gaussian mixture model (GMM) with 64 components provides a signiﬁcantly better model for local ﬂow statistics when compared to commonly used models. We investigate the source of the GMM’s success and show it is related to an explicit representation of ﬂow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical ﬂow. In accordance with the assumptions often made in computer vision, the model learns that ﬂow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the beneﬁt of conditioning ﬂow estimation on the local intensity pattern is marginal. 1</p><p>3 0.121314 <a title="128-tfidf-3" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>4 0.064634323 <a title="128-tfidf-4" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>Author: Harikrishna Narasimhan, Shivani Agarwal</p><p>Abstract: We investigate the relationship between three fundamental problems in machine learning: binary classiﬁcation, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classiﬁcation model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classiﬁcation model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classiﬁcation model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model). 1</p><p>5 0.055567253 <a title="128-tfidf-5" href="./nips-2013-Bayesian_Estimation_of_Latently-grouped_Parameters_in_Undirected_Graphical_Models.html">46 nips-2013-Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models</a></p>
<p>Author: Jie Liu, David Page</p><p>Abstract: In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneﬁcial to group the parameters for more efﬁcient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with “stripped” Beta approximation (Gibbs SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs SBA’s performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs SBA also generalize better than the models learned by MLE on real-world Senate voting data. 1</p><p>6 0.049991395 <a title="128-tfidf-6" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>7 0.04962106 <a title="128-tfidf-7" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>8 0.047763739 <a title="128-tfidf-8" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>9 0.046020657 <a title="128-tfidf-9" href="./nips-2013-Adaptive_Market_Making_via_Online_Learning.html">26 nips-2013-Adaptive Market Making via Online Learning</a></p>
<p>10 0.044688862 <a title="128-tfidf-10" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>11 0.043933444 <a title="128-tfidf-11" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>12 0.043309789 <a title="128-tfidf-12" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>13 0.042880118 <a title="128-tfidf-13" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>14 0.040253174 <a title="128-tfidf-14" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>15 0.03859251 <a title="128-tfidf-15" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>16 0.03829721 <a title="128-tfidf-16" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>17 0.034914248 <a title="128-tfidf-17" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>18 0.034318954 <a title="128-tfidf-18" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>19 0.033288192 <a title="128-tfidf-19" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>20 0.031631283 <a title="128-tfidf-20" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.088), (1, 0.025), (2, 0.004), (3, 0.019), (4, 0.012), (5, 0.02), (6, 0.02), (7, -0.015), (8, -0.012), (9, 0.013), (10, 0.021), (11, 0.035), (12, 0.02), (13, -0.023), (14, -0.057), (15, 0.075), (16, 0.047), (17, -0.017), (18, 0.049), (19, 0.035), (20, -0.062), (21, -0.052), (22, -0.022), (23, -0.03), (24, -0.032), (25, -0.05), (26, 0.046), (27, 0.06), (28, 0.025), (29, 0.054), (30, -0.027), (31, 0.059), (32, 0.039), (33, -0.071), (34, -0.105), (35, 0.016), (36, 0.126), (37, 0.009), (38, -0.048), (39, 0.039), (40, 0.012), (41, 0.033), (42, 0.043), (43, -0.058), (44, -0.013), (45, 0.051), (46, -0.12), (47, 0.082), (48, -0.117), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92394966 <a title="128-lsi-1" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>Author: Hossein Azari Soufiani, William Chen, David C. Parkes, Lirong Xia</p><p>Abstract: In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efﬁciency. 1</p><p>2 0.57022965 <a title="128-lsi-2" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>3 0.46679264 <a title="128-lsi-3" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>Author: Dan Rosenbaum, Daniel Zoran, Yair Weiss</p><p>Abstract: Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical ﬂow to learn the local statistics of optical ﬂow and compare the learned models to prior models assumed by computer vision researchers. We ﬁnd that a Gaussian mixture model (GMM) with 64 components provides a signiﬁcantly better model for local ﬂow statistics when compared to commonly used models. We investigate the source of the GMM’s success and show it is related to an explicit representation of ﬂow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical ﬂow. In accordance with the assumptions often made in computer vision, the model learns that ﬂow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the beneﬁt of conditioning ﬂow estimation on the local intensity pattern is marginal. 1</p><p>4 0.44586498 <a title="128-lsi-4" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>5 0.42115876 <a title="128-lsi-5" href="./nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F.html">290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</a></p>
<p>Author: Qiang Liu, Alex Ihler, Mark Steyvers</p><p>Abstract: We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers’ performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. 1</p><p>6 0.39586517 <a title="128-lsi-6" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>7 0.36719576 <a title="128-lsi-7" href="./nips-2013-Convergence_of_Monte_Carlo_Tree_Search_in_Simultaneous_Move_Games.html">71 nips-2013-Convergence of Monte Carlo Tree Search in Simultaneous Move Games</a></p>
<p>8 0.34018803 <a title="128-lsi-8" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>9 0.33866826 <a title="128-lsi-9" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>10 0.33821869 <a title="128-lsi-10" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>11 0.33317143 <a title="128-lsi-11" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>12 0.32852739 <a title="128-lsi-12" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>13 0.3256785 <a title="128-lsi-13" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>14 0.32048106 <a title="128-lsi-14" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>15 0.31990132 <a title="128-lsi-15" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>16 0.31747434 <a title="128-lsi-16" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>17 0.31316561 <a title="128-lsi-17" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>18 0.31083047 <a title="128-lsi-18" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>19 0.30070692 <a title="128-lsi-19" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>20 0.29758459 <a title="128-lsi-20" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.022), (33, 0.109), (34, 0.158), (41, 0.036), (49, 0.024), (56, 0.083), (70, 0.017), (85, 0.023), (89, 0.033), (93, 0.035), (95, 0.013), (99, 0.344)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80479681 <a title="128-lda-1" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>Author: Hossein Azari Soufiani, William Chen, David C. Parkes, Lirong Xia</p><p>Abstract: In this paper we propose a class of efﬁcient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efﬁciency. 1</p><p>2 0.79533648 <a title="128-lda-2" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>Author: Jukka Corander, Tomi Janhunen, Jussi Rintanen, Henrik Nyman, Johan Pensar</p><p>Abstract: We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efﬁcient encodings, we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisﬁability and its extensions such as maximum satisﬁability, satisﬁability modulo theories, and answer set programming, enable us to prove optimal certain networks which have been previously found by stochastic search. 1</p><p>3 0.73133504 <a title="128-lda-3" href="./nips-2013-Learning_to_Prune_in_Metric_and_Non-Metric_Spaces.html">169 nips-2013-Learning to Prune in Metric and Non-Metric Spaces</a></p>
<p>Author: Leonid Boytsov, Bilegsaikhan Naidan</p><p>Abstract: Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-toprune approaches: density estimation through sampling and “stretching” of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efﬁcient for the same rank approximation quality. 1</p><p>4 0.7081185 <a title="128-lda-4" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><p>5 0.69258869 <a title="128-lda-5" href="./nips-2013-Robust_Sparse_Principal_Component_Regression_under_the_High_Dimensional_Elliptical_Model.html">283 nips-2013-Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are two folds. First, in low dimensions and under the Gaussian model, by borrowing the strength from recent development in minimax optimal principal component estimation, we ﬁrst time sharply characterize the potential advantage of classical principal component regression over least square estimation. Secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian, including many well known distributions such as multivariate Gaussian, rank-deﬁcient Gaussian, t, Cauchy, and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra ﬂexibilities make it very suitable for modeling ﬁnance and biomedical imaging data. Under the elliptical model, we prove that our method can estimate the regression coefﬁcients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method. 1</p><p>6 0.65498579 <a title="128-lda-6" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>7 0.53125429 <a title="128-lda-7" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>8 0.52927595 <a title="128-lda-8" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>9 0.52494621 <a title="128-lda-9" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>10 0.52404201 <a title="128-lda-10" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>11 0.52301323 <a title="128-lda-11" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>12 0.52295053 <a title="128-lda-12" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>13 0.52253234 <a title="128-lda-13" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>14 0.52200681 <a title="128-lda-14" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>15 0.52173287 <a title="128-lda-15" href="./nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</a></p>
<p>16 0.52086097 <a title="128-lda-16" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>17 0.52084792 <a title="128-lda-17" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>18 0.5206821 <a title="128-lda-18" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>19 0.5188669 <a title="128-lda-19" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>20 0.51846927 <a title="128-lda-20" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
